python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_2 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1540897543 --max_global_steps=133333 2>&1 | tee -a /logs/wmt_jax_02-06-2024-13-15-17.log
I0206 13:15:39.989489 140446903760704 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_2/wmt_jax.
I0206 13:15:41.046146 140446903760704 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0206 13:15:41.047482 140446903760704 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0206 13:15:41.047635 140446903760704 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0206 13:15:41.048937 140446903760704 submission_runner.py:542] Using RNG seed 1540897543
I0206 13:15:42.167707 140446903760704 submission_runner.py:551] --- Tuning run 1/5 ---
I0206 13:15:42.167908 140446903760704 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_1.
I0206 13:15:42.168210 140446903760704 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_1/hparams.json.
I0206 13:15:42.362506 140446903760704 submission_runner.py:206] Initializing dataset.
I0206 13:15:42.375751 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 13:15:42.379951 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 13:15:42.528204 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 13:15:44.524151 140446903760704 submission_runner.py:213] Initializing model.
I0206 13:15:53.640796 140446903760704 submission_runner.py:255] Initializing optimizer.
I0206 13:15:54.776690 140446903760704 submission_runner.py:262] Initializing metrics bundle.
I0206 13:15:54.776906 140446903760704 submission_runner.py:280] Initializing checkpoint and logger.
I0206 13:15:54.778092 140446903760704 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/wmt_jax/trial_1 with prefix checkpoint_
I0206 13:15:54.778248 140446903760704 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_1/meta_data_0.json.
I0206 13:15:54.778448 140446903760704 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 13:15:54.778508 140446903760704 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 13:15:55.148221 140446903760704 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 13:15:55.476652 140446903760704 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_1/flags_0.json.
I0206 13:15:55.487557 140446903760704 submission_runner.py:314] Starting training loop.
I0206 13:16:32.138969 140282003707648 logging_writer.py:48] [0] global_step=0, grad_norm=5.419407367706299, loss=11.158559799194336
I0206 13:16:32.156337 140446903760704 spec.py:321] Evaluating on the training split.
I0206 13:16:32.160111 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 13:16:32.163927 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 13:16:32.203531 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 13:16:39.781038 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 13:21:26.913658 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 13:21:26.926701 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 13:21:26.931062 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 13:21:26.970094 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 13:21:33.785482 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 13:26:11.228734 140446903760704 spec.py:349] Evaluating on the test split.
I0206 13:26:11.231651 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 13:26:11.234806 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 13:26:11.271919 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 13:26:14.115299 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 13:30:51.440663 140446903760704 submission_runner.py:408] Time since start: 895.95s, 	Step: 1, 	{'train/accuracy': 0.0005978592089377344, 'train/loss': 11.188164710998535, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 36.66873526573181, 'total_duration': 895.9530372619629, 'accumulated_submission_time': 36.66873526573181, 'accumulated_eval_time': 859.2842590808868, 'accumulated_logging_time': 0}
I0206 13:30:51.461229 140277229537024 logging_writer.py:48] [1] accumulated_eval_time=859.284259, accumulated_logging_time=0, accumulated_submission_time=36.668735, global_step=1, preemption_count=0, score=36.668735, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.191028, test/num_examples=3003, total_duration=895.953037, train/accuracy=0.000598, train/bleu=0.000000, train/loss=11.188165, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.190282, validation/num_examples=3000
I0206 13:31:26.359435 140277221144320 logging_writer.py:48] [100] global_step=100, grad_norm=0.4252822697162628, loss=8.962847709655762
I0206 13:32:01.315963 140277229537024 logging_writer.py:48] [200] global_step=200, grad_norm=0.16638250648975372, loss=8.640403747558594
I0206 13:32:36.336298 140277221144320 logging_writer.py:48] [300] global_step=300, grad_norm=0.20045772194862366, loss=8.420865058898926
I0206 13:33:11.380932 140277229537024 logging_writer.py:48] [400] global_step=400, grad_norm=0.2542458474636078, loss=8.01500129699707
I0206 13:33:46.440550 140277221144320 logging_writer.py:48] [500] global_step=500, grad_norm=0.3743516504764557, loss=7.683066368103027
I0206 13:34:21.506741 140277229537024 logging_writer.py:48] [600] global_step=600, grad_norm=0.48180320858955383, loss=7.466031551361084
I0206 13:34:56.587206 140277221144320 logging_writer.py:48] [700] global_step=700, grad_norm=0.5534127950668335, loss=7.244294166564941
I0206 13:35:31.690202 140277229537024 logging_writer.py:48] [800] global_step=800, grad_norm=0.5590087175369263, loss=7.012558460235596
I0206 13:36:06.808701 140277221144320 logging_writer.py:48] [900] global_step=900, grad_norm=0.48544761538505554, loss=6.849398136138916
I0206 13:36:41.901539 140277229537024 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5167056918144226, loss=6.555864334106445
I0206 13:37:17.007434 140277221144320 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6384467482566833, loss=6.400579929351807
I0206 13:37:52.130580 140277229537024 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7403523325920105, loss=6.199862003326416
I0206 13:38:27.240307 140277221144320 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7884055972099304, loss=6.103891849517822
I0206 13:39:02.338155 140277229537024 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.590291440486908, loss=5.970288276672363
I0206 13:39:37.503620 140277221144320 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6430501341819763, loss=5.813462257385254
I0206 13:40:12.629927 140277229537024 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6377041935920715, loss=5.681713104248047
I0206 13:40:47.741939 140277221144320 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7515249848365784, loss=5.615146160125732
I0206 13:41:22.878948 140277229537024 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7010016441345215, loss=5.4373579025268555
I0206 13:41:58.021311 140277221144320 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8048293590545654, loss=5.384374618530273
I0206 13:42:33.131986 140277229537024 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3536357879638672, loss=5.324806213378906
I0206 13:43:08.247207 140277221144320 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.108947992324829, loss=5.095524787902832
I0206 13:43:43.344719 140277229537024 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.9458640813827515, loss=5.06730318069458
I0206 13:44:18.459665 140277221144320 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6677107810974121, loss=4.962762832641602
I0206 13:44:51.544556 140446903760704 spec.py:321] Evaluating on the training split.
I0206 13:44:54.570096 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 13:48:39.115487 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 13:48:41.827332 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 13:52:25.882339 140446903760704 spec.py:349] Evaluating on the test split.
I0206 13:52:28.596492 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 13:56:10.543888 140446903760704 submission_runner.py:408] Time since start: 2415.06s, 	Step: 2396, 	{'train/accuracy': 0.41089096665382385, 'train/loss': 4.017301082611084, 'train/bleu': 14.135664486209082, 'validation/accuracy': 0.39612650871276855, 'validation/loss': 4.122348785400391, 'validation/bleu': 9.774756744618458, 'validation/num_examples': 3000, 'test/accuracy': 0.38062867522239685, 'test/loss': 4.3253560066223145, 'test/bleu': 7.894892890419983, 'test/num_examples': 3003, 'score': 876.663633108139, 'total_duration': 2415.0562007427216, 'accumulated_submission_time': 876.663633108139, 'accumulated_eval_time': 1538.2834823131561, 'accumulated_logging_time': 0.03058004379272461}
I0206 13:56:10.562268 140277229537024 logging_writer.py:48] [2396] accumulated_eval_time=1538.283482, accumulated_logging_time=0.030580, accumulated_submission_time=876.663633, global_step=2396, preemption_count=0, score=876.663633, test/accuracy=0.380629, test/bleu=7.894893, test/loss=4.325356, test/num_examples=3003, total_duration=2415.056201, train/accuracy=0.410891, train/bleu=14.135664, train/loss=4.017301, validation/accuracy=0.396127, validation/bleu=9.774757, validation/loss=4.122349, validation/num_examples=3000
I0206 13:56:12.334971 140277221144320 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9126591682434082, loss=4.868469715118408
I0206 13:56:47.311635 140277229537024 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8074228763580322, loss=4.807571887969971
I0206 13:57:22.374608 140277221144320 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7062898278236389, loss=4.6922807693481445
I0206 13:57:57.451479 140277229537024 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.1238913536071777, loss=4.683077335357666
I0206 13:58:32.552173 140277221144320 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8473365902900696, loss=4.485372066497803
I0206 13:59:07.667729 140277229537024 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9035133719444275, loss=4.550330638885498
I0206 13:59:42.890448 140277221144320 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7354457974433899, loss=4.463273048400879
I0206 14:00:17.985736 140277229537024 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9943550229072571, loss=4.344518661499023
I0206 14:00:53.091504 140277221144320 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8142909407615662, loss=4.224118709564209
I0206 14:01:28.209499 140277229537024 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7298564910888672, loss=4.178323745727539
I0206 14:02:03.327435 140277221144320 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5972297191619873, loss=4.1948041915893555
I0206 14:02:38.411825 140277229537024 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.768618106842041, loss=4.066330432891846
I0206 14:03:13.522089 140277221144320 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.7743766903877258, loss=4.082444190979004
I0206 14:03:48.660109 140277229537024 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6672372817993164, loss=3.9689033031463623
I0206 14:04:23.746610 140277221144320 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7248468399047852, loss=4.0259904861450195
I0206 14:04:58.827505 140277229537024 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5413582921028137, loss=4.073400020599365
I0206 14:05:33.948247 140277221144320 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8572595119476318, loss=3.989954710006714
I0206 14:06:09.073342 140277229537024 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8928012847900391, loss=3.9422519207000732
I0206 14:06:44.168311 140277221144320 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5906884074211121, loss=3.9767284393310547
I0206 14:07:19.281113 140277229537024 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5915396809577942, loss=3.9129204750061035
I0206 14:07:54.392449 140277221144320 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.667174220085144, loss=3.926551103591919
I0206 14:08:29.493395 140277229537024 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5354586839675903, loss=3.810255765914917
I0206 14:09:04.601042 140277221144320 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7556520700454712, loss=3.901944637298584
I0206 14:09:39.679526 140277229537024 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5723801851272583, loss=3.7799127101898193
I0206 14:10:10.631093 140446903760704 spec.py:321] Evaluating on the training split.
I0206 14:10:13.640604 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:12:41.965463 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 14:12:44.691655 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:15:18.900439 140446903760704 spec.py:349] Evaluating on the test split.
I0206 14:15:21.633802 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:17:49.574653 140446903760704 submission_runner.py:408] Time since start: 3714.09s, 	Step: 4790, 	{'train/accuracy': 0.542180597782135, 'train/loss': 2.732574224472046, 'train/bleu': 24.602004108236525, 'validation/accuracy': 0.5455852746963501, 'validation/loss': 2.7102763652801514, 'validation/bleu': 20.36131793008979, 'validation/num_examples': 3000, 'test/accuracy': 0.5423159599304199, 'test/loss': 2.7538504600524902, 'test/bleu': 18.71880292122399, 'test/num_examples': 3003, 'score': 1716.6430037021637, 'total_duration': 3714.0869665145874, 'accumulated_submission_time': 1716.6430037021637, 'accumulated_eval_time': 1997.2269372940063, 'accumulated_logging_time': 0.06126093864440918}
I0206 14:17:49.592190 140277221144320 logging_writer.py:48] [4790] accumulated_eval_time=1997.226937, accumulated_logging_time=0.061261, accumulated_submission_time=1716.643004, global_step=4790, preemption_count=0, score=1716.643004, test/accuracy=0.542316, test/bleu=18.718803, test/loss=2.753850, test/num_examples=3003, total_duration=3714.086967, train/accuracy=0.542181, train/bleu=24.602004, train/loss=2.732574, validation/accuracy=0.545585, validation/bleu=20.361318, validation/loss=2.710276, validation/num_examples=3000
I0206 14:17:53.456868 140277229537024 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5348055362701416, loss=3.7215566635131836
I0206 14:18:28.402404 140277221144320 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5471739172935486, loss=3.766753911972046
I0206 14:19:03.466620 140277229537024 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5498983860015869, loss=3.703624725341797
I0206 14:19:38.546227 140277221144320 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6524033546447754, loss=3.7250208854675293
I0206 14:20:13.616283 140277229537024 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5699418187141418, loss=3.6971030235290527
I0206 14:20:48.692743 140277221144320 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5472283363342285, loss=3.6671037673950195
I0206 14:21:23.779776 140277229537024 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5769872665405273, loss=3.7293009757995605
I0206 14:21:58.871256 140277221144320 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6858416199684143, loss=3.6577835083007812
I0206 14:22:33.990272 140277229537024 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5695802569389343, loss=3.7140908241271973
I0206 14:23:09.095971 140277221144320 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5260539054870605, loss=3.602900266647339
I0206 14:23:44.185053 140277229537024 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5058509111404419, loss=3.558176040649414
I0206 14:24:19.260574 140277221144320 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4942609369754791, loss=3.665172815322876
I0206 14:24:54.498568 140277229537024 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4657493829727173, loss=3.5054168701171875
I0206 14:25:29.621189 140277221144320 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4761345684528351, loss=3.5620577335357666
I0206 14:26:04.691995 140277229537024 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4818444550037384, loss=3.642939805984497
I0206 14:26:39.766123 140277221144320 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.522658109664917, loss=3.560152292251587
I0206 14:27:14.867354 140277229537024 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4521460235118866, loss=3.579402446746826
I0206 14:27:49.996352 140277221144320 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5410308241844177, loss=3.5853991508483887
I0206 14:28:25.105111 140277229537024 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.49857982993125916, loss=3.5734000205993652
I0206 14:29:00.183663 140277221144320 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.44248324632644653, loss=3.5846354961395264
I0206 14:29:35.280403 140277229537024 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4246102571487427, loss=3.4926066398620605
I0206 14:30:10.350569 140277221144320 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.3876531720161438, loss=3.529780864715576
I0206 14:30:45.465413 140277229537024 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.44581666588783264, loss=3.491657018661499
I0206 14:31:20.574339 140277221144320 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4662662148475647, loss=3.487107753753662
I0206 14:31:49.775200 140446903760704 spec.py:321] Evaluating on the training split.
I0206 14:31:52.783551 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:34:15.052502 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 14:34:17.766609 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:36:46.578601 140446903760704 spec.py:349] Evaluating on the test split.
I0206 14:36:49.287287 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:39:02.933384 140446903760704 submission_runner.py:408] Time since start: 4987.45s, 	Step: 7185, 	{'train/accuracy': 0.5838678479194641, 'train/loss': 2.3458170890808105, 'train/bleu': 27.234345079265907, 'validation/accuracy': 0.5861551761627197, 'validation/loss': 2.3145346641540527, 'validation/bleu': 23.179417644358757, 'validation/num_examples': 3000, 'test/accuracy': 0.5881703495979309, 'test/loss': 2.31225848197937, 'test/bleu': 21.78799174427264, 'test/num_examples': 3003, 'score': 2556.738527774811, 'total_duration': 4987.44571518898, 'accumulated_submission_time': 2556.738527774811, 'accumulated_eval_time': 2430.385052204132, 'accumulated_logging_time': 0.09026479721069336}
I0206 14:39:02.949291 140277229537024 logging_writer.py:48] [7185] accumulated_eval_time=2430.385052, accumulated_logging_time=0.090265, accumulated_submission_time=2556.738528, global_step=7185, preemption_count=0, score=2556.738528, test/accuracy=0.588170, test/bleu=21.787992, test/loss=2.312258, test/num_examples=3003, total_duration=4987.445715, train/accuracy=0.583868, train/bleu=27.234345, train/loss=2.345817, validation/accuracy=0.586155, validation/bleu=23.179418, validation/loss=2.314535, validation/num_examples=3000
I0206 14:39:08.551145 140277221144320 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.378927081823349, loss=3.4750494956970215
I0206 14:39:43.465602 140277229537024 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4453655779361725, loss=3.4435949325561523
I0206 14:40:18.455906 140277221144320 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.42507556080818176, loss=3.3938839435577393
I0206 14:40:53.507681 140277229537024 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4416424334049225, loss=3.45452880859375
I0206 14:41:28.562839 140277221144320 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.3990453779697418, loss=3.4314002990722656
I0206 14:42:03.630448 140277229537024 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.43114060163497925, loss=3.4200055599212646
I0206 14:42:38.675919 140277221144320 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.44015446305274963, loss=3.41947078704834
I0206 14:43:13.744861 140277229537024 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.407090425491333, loss=3.401553153991699
I0206 14:43:48.842043 140277221144320 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.38347575068473816, loss=3.3638041019439697
I0206 14:44:23.901863 140277229537024 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.42317095398902893, loss=3.4079716205596924
I0206 14:44:58.990447 140277221144320 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3909311592578888, loss=3.3104729652404785
I0206 14:45:34.108024 140277229537024 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.36498674750328064, loss=3.4571468830108643
I0206 14:46:09.199352 140277221144320 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3802759647369385, loss=3.3509368896484375
I0206 14:46:44.291987 140277229537024 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.41256147623062134, loss=3.3661162853240967
I0206 14:47:19.374944 140277221144320 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3622712194919586, loss=3.3794422149658203
I0206 14:47:54.464443 140277229537024 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3322342336177826, loss=3.389224052429199
I0206 14:48:29.597654 140277221144320 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.38269197940826416, loss=3.3245716094970703
I0206 14:49:04.711633 140277229537024 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.47024592757225037, loss=3.3904402256011963
I0206 14:49:39.771869 140277221144320 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.34822672605514526, loss=3.3969030380249023
I0206 14:50:14.845152 140277229537024 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.36236074566841125, loss=3.330256223678589
I0206 14:50:49.897071 140277221144320 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.32539212703704834, loss=3.4247665405273438
I0206 14:51:24.978877 140277229537024 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3095315992832184, loss=3.293910264968872
I0206 14:52:00.052750 140277221144320 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.30037495493888855, loss=3.3291497230529785
I0206 14:52:35.129193 140277229537024 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.341197669506073, loss=3.321258306503296
I0206 14:53:03.232135 140446903760704 spec.py:321] Evaluating on the training split.
I0206 14:53:06.235298 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:55:35.836032 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 14:55:38.546386 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 14:58:05.757275 140446903760704 spec.py:349] Evaluating on the test split.
I0206 14:58:08.471252 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 15:00:24.854923 140446903760704 submission_runner.py:408] Time since start: 6269.37s, 	Step: 9582, 	{'train/accuracy': 0.5891788601875305, 'train/loss': 2.2676243782043457, 'train/bleu': 27.89985486351619, 'validation/accuracy': 0.6069980263710022, 'validation/loss': 2.1352217197418213, 'validation/bleu': 24.639766045857712, 'validation/num_examples': 3000, 'test/accuracy': 0.6104235649108887, 'test/loss': 2.1066174507141113, 'test/bleu': 23.303168552867703, 'test/num_examples': 3003, 'score': 3396.931195497513, 'total_duration': 6269.367269515991, 'accumulated_submission_time': 3396.931195497513, 'accumulated_eval_time': 2872.0077636241913, 'accumulated_logging_time': 0.11701345443725586}
I0206 15:00:24.870574 140277221144320 logging_writer.py:48] [9582] accumulated_eval_time=2872.007764, accumulated_logging_time=0.117013, accumulated_submission_time=3396.931195, global_step=9582, preemption_count=0, score=3396.931195, test/accuracy=0.610424, test/bleu=23.303169, test/loss=2.106617, test/num_examples=3003, total_duration=6269.367270, train/accuracy=0.589179, train/bleu=27.899855, train/loss=2.267624, validation/accuracy=0.606998, validation/bleu=24.639766, validation/loss=2.135222, validation/num_examples=3000
I0206 15:00:31.521013 140277229537024 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.30918648838996887, loss=3.3186981678009033
I0206 15:01:06.429876 140277221144320 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.30058935284614563, loss=3.2548539638519287
I0206 15:01:41.431540 140277229537024 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.32736629247665405, loss=3.3013901710510254
I0206 15:02:16.491198 140277221144320 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.2909179925918579, loss=3.26159930229187
I0206 15:02:51.539840 140277229537024 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.29553937911987305, loss=3.2496519088745117
I0206 15:03:26.583379 140277221144320 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2773565948009491, loss=3.2455570697784424
I0206 15:04:01.660279 140277229537024 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3108408749103546, loss=3.274285316467285
I0206 15:04:36.716312 140277221144320 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.29429131746292114, loss=3.2880263328552246
I0206 15:05:11.768457 140277229537024 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.29746806621551514, loss=3.21482253074646
I0206 15:05:46.840544 140277221144320 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.29367056488990784, loss=3.339071273803711
I0206 15:06:21.908917 140277229537024 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.26340240240097046, loss=3.2966725826263428
I0206 15:06:56.960484 140277221144320 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3554486036300659, loss=3.383315324783325
I0206 15:07:32.037341 140277229537024 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3165813684463501, loss=3.306180000305176
I0206 15:08:07.139956 140277221144320 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.3316691815853119, loss=3.236732006072998
I0206 15:08:42.221937 140277229537024 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.25541993975639343, loss=3.315145492553711
I0206 15:09:17.302167 140277221144320 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.26415419578552246, loss=3.2197952270507812
I0206 15:09:52.338868 140277229537024 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.29156258702278137, loss=3.2983782291412354
I0206 15:10:27.420226 140277221144320 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.254996120929718, loss=3.2117714881896973
I0206 15:11:02.469231 140277229537024 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.27085670828819275, loss=3.206080913543701
I0206 15:11:37.557554 140277221144320 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3133297860622406, loss=3.2422802448272705
I0206 15:12:12.603522 140277229537024 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.2736489176750183, loss=3.235405445098877
I0206 15:12:47.660913 140277221144320 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.27029839158058167, loss=3.3683855533599854
I0206 15:13:22.713366 140277229537024 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2700740098953247, loss=3.199148416519165
I0206 15:13:57.774957 140277221144320 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.29270827770233154, loss=3.2553441524505615
I0206 15:14:25.187540 140446903760704 spec.py:321] Evaluating on the training split.
I0206 15:14:28.200562 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 15:17:45.574505 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 15:17:48.277482 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 15:21:08.750360 140446903760704 spec.py:349] Evaluating on the test split.
I0206 15:21:11.466016 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 15:24:03.773181 140446903760704 submission_runner.py:408] Time since start: 7688.29s, 	Step: 11980, 	{'train/accuracy': 0.6027758717536926, 'train/loss': 2.1443228721618652, 'train/bleu': 28.891173233400416, 'validation/accuracy': 0.6202526688575745, 'validation/loss': 2.0171313285827637, 'validation/bleu': 25.276258137532324, 'validation/num_examples': 3000, 'test/accuracy': 0.6257509589195251, 'test/loss': 1.974539041519165, 'test/bleu': 24.60483532445832, 'test/num_examples': 3003, 'score': 4237.160478591919, 'total_duration': 7688.285516738892, 'accumulated_submission_time': 4237.160478591919, 'accumulated_eval_time': 3450.5933167934418, 'accumulated_logging_time': 0.14389681816101074}
I0206 15:24:03.789260 140277229537024 logging_writer.py:48] [11980] accumulated_eval_time=3450.593317, accumulated_logging_time=0.143897, accumulated_submission_time=4237.160479, global_step=11980, preemption_count=0, score=4237.160479, test/accuracy=0.625751, test/bleu=24.604835, test/loss=1.974539, test/num_examples=3003, total_duration=7688.285517, train/accuracy=0.602776, train/bleu=28.891173, train/loss=2.144323, validation/accuracy=0.620253, validation/bleu=25.276258, validation/loss=2.017131, validation/num_examples=3000
I0206 15:24:11.144217 140277221144320 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2476942539215088, loss=3.274750232696533
I0206 15:24:46.058462 140277229537024 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.263623982667923, loss=3.140228509902954
I0206 15:25:21.042584 140277221144320 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2677620053291321, loss=3.1976335048675537
I0206 15:25:56.076890 140277229537024 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.25190266966819763, loss=3.2384579181671143
I0206 15:26:31.081183 140277221144320 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.23565365374088287, loss=3.1884899139404297
I0206 15:27:06.142358 140277229537024 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2627687454223633, loss=3.2187998294830322
I0206 15:27:41.226693 140277221144320 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.25096654891967773, loss=3.2067606449127197
I0206 15:28:16.307205 140277229537024 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.25492551922798157, loss=3.2115478515625
I0206 15:28:51.321148 140277221144320 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.25047433376312256, loss=3.1480679512023926
I0206 15:29:26.369777 140277229537024 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.2555573582649231, loss=3.216547727584839
I0206 15:30:01.382483 140277221144320 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3494240343570709, loss=3.2617440223693848
I0206 15:30:36.426666 140277229537024 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2365526556968689, loss=3.2216014862060547
I0206 15:31:11.459942 140277221144320 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2527793347835541, loss=3.2442421913146973
I0206 15:31:46.495074 140277229537024 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.23422497510910034, loss=3.150493621826172
I0206 15:32:21.513320 140277221144320 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.22672052681446075, loss=3.2124810218811035
I0206 15:32:56.520022 140277229537024 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2651156485080719, loss=3.163332223892212
I0206 15:33:31.533756 140277221144320 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2869572341442108, loss=3.152920722961426
I0206 15:34:06.539475 140277229537024 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.26070964336395264, loss=3.1828110218048096
I0206 15:34:41.549553 140277221144320 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.24832873046398163, loss=3.204739570617676
I0206 15:35:16.573444 140277229537024 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.26262998580932617, loss=3.19983172416687
I0206 15:35:51.571910 140277221144320 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2531718909740448, loss=3.1302902698516846
I0206 15:36:26.576436 140277229537024 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2916249632835388, loss=3.1390998363494873
I0206 15:37:01.611653 140277221144320 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.3059142529964447, loss=3.1734418869018555
I0206 15:37:36.630228 140277229537024 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.24919423460960388, loss=3.0673227310180664
I0206 15:38:04.001749 140446903760704 spec.py:321] Evaluating on the training split.
I0206 15:38:07.008301 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 15:40:37.764845 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 15:40:40.479573 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 15:43:07.463346 140446903760704 spec.py:349] Evaluating on the test split.
I0206 15:43:10.173064 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 15:45:25.224668 140446903760704 submission_runner.py:408] Time since start: 8969.74s, 	Step: 14380, 	{'train/accuracy': 0.6137219071388245, 'train/loss': 2.0440056324005127, 'train/bleu': 29.763904979834205, 'validation/accuracy': 0.6301347613334656, 'validation/loss': 1.9250251054763794, 'validation/bleu': 26.04915640493588, 'validation/num_examples': 3000, 'test/accuracy': 0.637022852897644, 'test/loss': 1.8806644678115845, 'test/bleu': 25.595357216731195, 'test/num_examples': 3003, 'score': 5077.2845776081085, 'total_duration': 8969.737039804459, 'accumulated_submission_time': 5077.2845776081085, 'accumulated_eval_time': 3891.816184282303, 'accumulated_logging_time': 0.17147159576416016}
I0206 15:45:25.241304 140277221144320 logging_writer.py:48] [14380] accumulated_eval_time=3891.816184, accumulated_logging_time=0.171472, accumulated_submission_time=5077.284578, global_step=14380, preemption_count=0, score=5077.284578, test/accuracy=0.637023, test/bleu=25.595357, test/loss=1.880664, test/num_examples=3003, total_duration=8969.737040, train/accuracy=0.613722, train/bleu=29.763905, train/loss=2.044006, validation/accuracy=0.630135, validation/bleu=26.049156, validation/loss=1.925025, validation/num_examples=3000
I0206 15:45:32.592043 140277229537024 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2607742249965668, loss=3.1573984622955322
I0206 15:46:07.463418 140277221144320 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.24482962489128113, loss=3.1194324493408203
I0206 15:46:42.548323 140277229537024 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2521364390850067, loss=3.1521356105804443
I0206 15:47:17.592332 140277221144320 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.25548967719078064, loss=3.0929548740386963
I0206 15:47:52.695169 140277229537024 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2446095049381256, loss=3.1427929401397705
I0206 15:48:27.770283 140277221144320 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.23545122146606445, loss=3.1423094272613525
I0206 15:49:02.844663 140277229537024 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.31463226675987244, loss=3.063410520553589
I0206 15:49:37.884454 140277221144320 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.29505306482315063, loss=3.0831186771392822
I0206 15:50:12.930650 140277229537024 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.24515190720558167, loss=3.240283966064453
I0206 15:50:47.984362 140277221144320 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2649700343608856, loss=3.152944803237915
I0206 15:51:23.035285 140277229537024 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2821865379810333, loss=3.039930582046509
I0206 15:51:58.102215 140277221144320 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.2611769437789917, loss=3.1575064659118652
I0206 15:52:33.151515 140277229537024 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2639353275299072, loss=3.161226511001587
I0206 15:53:08.232034 140277221144320 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3418629765510559, loss=3.0698537826538086
I0206 15:53:43.274217 140277229537024 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.28408217430114746, loss=3.1444895267486572
I0206 15:54:18.356999 140277221144320 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.28333181142807007, loss=3.184046506881714
I0206 15:54:53.447997 140277229537024 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2743452489376068, loss=3.1293208599090576
I0206 15:55:28.547857 140277221144320 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2658015191555023, loss=3.178889513015747
I0206 15:56:03.655967 140277229537024 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3116260766983032, loss=3.053880214691162
I0206 15:56:38.718365 140277221144320 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2843121588230133, loss=3.0812900066375732
I0206 15:57:13.768510 140277229537024 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.28361260890960693, loss=3.0849623680114746
I0206 15:57:48.848989 140277221144320 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.33224472403526306, loss=3.102071762084961
I0206 15:58:23.928164 140277229537024 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2537273168563843, loss=3.0719335079193115
I0206 15:58:58.956798 140277221144320 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.2652885317802429, loss=3.0423147678375244
I0206 15:59:25.242385 140446903760704 spec.py:321] Evaluating on the training split.
I0206 15:59:27.979782 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:01:57.823522 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 16:02:00.537306 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:04:24.609469 140446903760704 spec.py:349] Evaluating on the test split.
I0206 16:04:27.339280 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:06:45.113977 140446903760704 submission_runner.py:408] Time since start: 10249.63s, 	Step: 16776, 	{'train/accuracy': 0.6152881979942322, 'train/loss': 2.0378637313842773, 'train/bleu': 29.833339905367374, 'validation/accuracy': 0.6370410919189453, 'validation/loss': 1.8624820709228516, 'validation/bleu': 26.930205823477618, 'validation/num_examples': 3000, 'test/accuracy': 0.6476788520812988, 'test/loss': 1.806699275970459, 'test/bleu': 25.957416144448224, 'test/num_examples': 3003, 'score': 5917.195971488953, 'total_duration': 10249.626316785812, 'accumulated_submission_time': 5917.195971488953, 'accumulated_eval_time': 4331.687678337097, 'accumulated_logging_time': 0.1997363567352295}
I0206 16:06:45.130603 140277229537024 logging_writer.py:48] [16776] accumulated_eval_time=4331.687678, accumulated_logging_time=0.199736, accumulated_submission_time=5917.195971, global_step=16776, preemption_count=0, score=5917.195971, test/accuracy=0.647679, test/bleu=25.957416, test/loss=1.806699, test/num_examples=3003, total_duration=10249.626317, train/accuracy=0.615288, train/bleu=29.833340, train/loss=2.037864, validation/accuracy=0.637041, validation/bleu=26.930206, validation/loss=1.862482, validation/num_examples=3000
I0206 16:06:53.886460 140277221144320 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.34303018450737, loss=3.1366279125213623
I0206 16:07:28.814780 140277229537024 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2891845703125, loss=3.0609138011932373
I0206 16:08:03.830683 140277221144320 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3265472650527954, loss=3.141427993774414
I0206 16:08:38.854413 140277229537024 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3412094712257385, loss=3.1171488761901855
I0206 16:09:13.915625 140277221144320 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3782399594783783, loss=3.041304111480713
I0206 16:09:48.953273 140277229537024 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3172903060913086, loss=3.118267059326172
I0206 16:10:23.974142 140277221144320 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.314172625541687, loss=3.0422091484069824
I0206 16:10:59.027486 140277229537024 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.2947613298892975, loss=3.058239698410034
I0206 16:11:34.085958 140277221144320 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.28779271245002747, loss=3.1181745529174805
I0206 16:12:09.115069 140277229537024 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.34584328532218933, loss=3.0108909606933594
I0206 16:12:44.278435 140277221144320 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.37457022070884705, loss=3.0939135551452637
I0206 16:13:19.371608 140277229537024 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.4022359848022461, loss=3.07546067237854
I0206 16:13:54.412770 140277221144320 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3212311863899231, loss=3.076115369796753
I0206 16:14:29.478099 140277229537024 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3068346679210663, loss=3.096179723739624
I0206 16:15:04.534336 140277221144320 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.35429567098617554, loss=3.076437473297119
I0206 16:15:39.577866 140277229537024 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.29006195068359375, loss=3.056971311569214
I0206 16:16:14.624740 140277221144320 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2976939380168915, loss=3.0508852005004883
I0206 16:16:49.702944 140277229537024 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3523913621902466, loss=3.074181318283081
I0206 16:17:24.764045 140277221144320 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.38958612084388733, loss=3.0714833736419678
I0206 16:17:59.831832 140277229537024 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.34767431020736694, loss=3.048433780670166
I0206 16:18:34.924812 140277221144320 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3674534261226654, loss=3.078742742538452
I0206 16:19:10.004202 140277229537024 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.35960790514945984, loss=2.999114513397217
I0206 16:19:45.116107 140277221144320 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.33963191509246826, loss=3.071118116378784
I0206 16:20:20.204001 140277229537024 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.34334462881088257, loss=3.0733230113983154
I0206 16:20:45.116359 140446903760704 spec.py:321] Evaluating on the training split.
I0206 16:20:48.129059 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:23:38.050170 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 16:23:40.760136 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:26:02.120887 140446903760704 spec.py:349] Evaluating on the test split.
I0206 16:26:04.824811 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:28:21.654427 140446903760704 submission_runner.py:408] Time since start: 11546.17s, 	Step: 19173, 	{'train/accuracy': 0.6377730965614319, 'train/loss': 1.8624444007873535, 'train/bleu': 31.384704872693035, 'validation/accuracy': 0.6448649168014526, 'validation/loss': 1.8179594278335571, 'validation/bleu': 27.341544220468872, 'validation/num_examples': 3000, 'test/accuracy': 0.6542908549308777, 'test/loss': 1.7641087770462036, 'test/bleu': 26.770263755815385, 'test/num_examples': 3003, 'score': 6757.092479228973, 'total_duration': 11546.166801214218, 'accumulated_submission_time': 6757.092479228973, 'accumulated_eval_time': 4788.225700378418, 'accumulated_logging_time': 0.22781896591186523}
I0206 16:28:21.671864 140277221144320 logging_writer.py:48] [19173] accumulated_eval_time=4788.225700, accumulated_logging_time=0.227819, accumulated_submission_time=6757.092479, global_step=19173, preemption_count=0, score=6757.092479, test/accuracy=0.654291, test/bleu=26.770264, test/loss=1.764109, test/num_examples=3003, total_duration=11546.166801, train/accuracy=0.637773, train/bleu=31.384705, train/loss=1.862444, validation/accuracy=0.644865, validation/bleu=27.341544, validation/loss=1.817959, validation/num_examples=3000
I0206 16:28:31.439965 140277229537024 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3008092939853668, loss=3.0157077312469482
I0206 16:29:06.325879 140277221144320 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.293191522359848, loss=3.0189270973205566
I0206 16:29:41.311835 140277229537024 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3399398624897003, loss=3.1085128784179688
I0206 16:30:16.380158 140277221144320 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.36164167523384094, loss=3.1157913208007812
I0206 16:30:51.420449 140277229537024 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.35559019446372986, loss=3.0531883239746094
I0206 16:31:26.449588 140277221144320 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3559964597225189, loss=3.145446300506592
I0206 16:32:01.614369 140277229537024 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.29391586780548096, loss=3.0334970951080322
I0206 16:32:36.635409 140277221144320 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3321797251701355, loss=3.0606260299682617
I0206 16:33:11.668726 140277229537024 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.40121158957481384, loss=3.1088030338287354
I0206 16:33:46.694790 140277221144320 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.34815123677253723, loss=3.0003840923309326
I0206 16:34:21.744221 140277229537024 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.34517136216163635, loss=3.0677456855773926
I0206 16:34:56.761960 140277221144320 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.3541131317615509, loss=3.079580783843994
I0206 16:35:31.798341 140277229537024 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.309123694896698, loss=3.139852523803711
I0206 16:36:06.822263 140277221144320 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3898695111274719, loss=3.0303637981414795
I0206 16:36:41.888150 140277229537024 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3733378052711487, loss=3.065408229827881
I0206 16:37:16.900941 140277221144320 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.31239795684814453, loss=3.014065742492676
I0206 16:37:51.937881 140277229537024 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.3183072805404663, loss=3.0426106452941895
I0206 16:38:26.962135 140277221144320 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.33978015184402466, loss=2.969968795776367
I0206 16:39:02.032860 140277229537024 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.31035104393959045, loss=3.0354223251342773
I0206 16:39:37.082879 140277221144320 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.35241881012916565, loss=3.1043484210968018
I0206 16:40:12.101169 140277229537024 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.3692610263824463, loss=3.0920755863189697
I0206 16:40:47.115964 140277221144320 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.4997481107711792, loss=3.067263603210449
I0206 16:41:22.166432 140277229537024 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3507738709449768, loss=3.086310625076294
I0206 16:41:57.243849 140277221144320 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.34789854288101196, loss=3.108363628387451
I0206 16:42:21.817569 140446903760704 spec.py:321] Evaluating on the training split.
I0206 16:42:24.832344 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:45:05.508868 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 16:45:08.217734 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:47:26.982394 140446903760704 spec.py:349] Evaluating on the test split.
I0206 16:47:29.693976 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 16:49:45.800690 140446903760704 submission_runner.py:408] Time since start: 12830.31s, 	Step: 21572, 	{'train/accuracy': 0.6281996965408325, 'train/loss': 1.9296544790267944, 'train/bleu': 30.43993556295968, 'validation/accuracy': 0.647431492805481, 'validation/loss': 1.7907466888427734, 'validation/bleu': 27.470066942691965, 'validation/num_examples': 3000, 'test/accuracy': 0.6562082767486572, 'test/loss': 1.7340083122253418, 'test/bleu': 26.91141067893076, 'test/num_examples': 3003, 'score': 7597.14856171608, 'total_duration': 12830.313049316406, 'accumulated_submission_time': 7597.14856171608, 'accumulated_eval_time': 5232.208755493164, 'accumulated_logging_time': 0.2553091049194336}
I0206 16:49:45.821897 140277229537024 logging_writer.py:48] [21572] accumulated_eval_time=5232.208755, accumulated_logging_time=0.255309, accumulated_submission_time=7597.148562, global_step=21572, preemption_count=0, score=7597.148562, test/accuracy=0.656208, test/bleu=26.911411, test/loss=1.734008, test/num_examples=3003, total_duration=12830.313049, train/accuracy=0.628200, train/bleu=30.439936, train/loss=1.929654, validation/accuracy=0.647431, validation/bleu=27.470067, validation/loss=1.790747, validation/num_examples=3000
I0206 16:49:55.932684 140277221144320 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.42602553963661194, loss=2.986757278442383
I0206 16:50:30.826311 140277229537024 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.43310338258743286, loss=3.024439573287964
I0206 16:51:05.822410 140277221144320 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.41940733790397644, loss=3.0233819484710693
I0206 16:51:40.824938 140277229537024 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.40671485662460327, loss=3.1032519340515137
I0206 16:52:15.861515 140277221144320 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3894093930721283, loss=3.019192695617676
I0206 16:52:50.862396 140277229537024 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.34132125973701477, loss=3.113450288772583
I0206 16:53:25.897564 140277221144320 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3649492561817169, loss=3.071913719177246
I0206 16:54:00.923871 140277229537024 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3475775420665741, loss=2.9804277420043945
I0206 16:54:35.971166 140277221144320 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.4302176535129547, loss=2.9939804077148438
I0206 16:55:10.986452 140277229537024 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.37833794951438904, loss=3.0026535987854004
I0206 16:55:46.022507 140277221144320 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3342033922672272, loss=2.994927167892456
I0206 16:56:21.056979 140277229537024 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.4593944549560547, loss=3.1056771278381348
I0206 16:56:56.075625 140277221144320 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.3830973505973816, loss=3.01255202293396
I0206 16:57:31.116231 140277229537024 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.381670743227005, loss=3.0341973304748535
I0206 16:58:06.146393 140277221144320 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.3604409992694855, loss=3.0173182487487793
I0206 16:58:41.175216 140277229537024 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.34016552567481995, loss=3.0373387336730957
I0206 16:59:16.189221 140277221144320 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3326297998428345, loss=3.0430052280426025
I0206 16:59:51.229347 140277229537024 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.31476613879203796, loss=3.059140205383301
I0206 17:00:26.258358 140277221144320 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.38523316383361816, loss=3.084843635559082
I0206 17:01:01.268297 140277229537024 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.34270402789115906, loss=2.9561145305633545
I0206 17:01:36.315105 140277221144320 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.504490077495575, loss=3.093536376953125
I0206 17:02:11.353061 140277229537024 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.38257789611816406, loss=3.0552234649658203
I0206 17:02:46.387439 140277221144320 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.3195447623729706, loss=3.0053024291992188
I0206 17:03:21.425111 140277229537024 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.36296167969703674, loss=2.9764904975891113
I0206 17:03:46.015695 140446903760704 spec.py:321] Evaluating on the training split.
I0206 17:03:49.027489 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:06:26.922330 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 17:06:29.636424 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:08:57.854336 140446903760704 spec.py:349] Evaluating on the test split.
I0206 17:09:00.569859 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:11:18.934151 140446903760704 submission_runner.py:408] Time since start: 14123.45s, 	Step: 23972, 	{'train/accuracy': 0.6257244944572449, 'train/loss': 1.9461606740951538, 'train/bleu': 30.647498348593707, 'validation/accuracy': 0.6491302251815796, 'validation/loss': 1.7739145755767822, 'validation/bleu': 27.639039633589352, 'validation/num_examples': 3000, 'test/accuracy': 0.6590785384178162, 'test/loss': 1.7163658142089844, 'test/bleu': 26.735188246892026, 'test/num_examples': 3003, 'score': 8437.255508899689, 'total_duration': 14123.446489810944, 'accumulated_submission_time': 8437.255508899689, 'accumulated_eval_time': 5685.127123594284, 'accumulated_logging_time': 0.2881033420562744}
I0206 17:11:18.952394 140277221144320 logging_writer.py:48] [23972] accumulated_eval_time=5685.127124, accumulated_logging_time=0.288103, accumulated_submission_time=8437.255509, global_step=23972, preemption_count=0, score=8437.255509, test/accuracy=0.659079, test/bleu=26.735188, test/loss=1.716366, test/num_examples=3003, total_duration=14123.446490, train/accuracy=0.625724, train/bleu=30.647498, train/loss=1.946161, validation/accuracy=0.649130, validation/bleu=27.639040, validation/loss=1.773915, validation/num_examples=3000
I0206 17:11:29.074595 140277229537024 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.39676520228385925, loss=2.9890522956848145
I0206 17:12:04.005982 140277221144320 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.34007272124290466, loss=2.9972774982452393
I0206 17:12:38.960793 140277229537024 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3944684863090515, loss=3.034978151321411
I0206 17:13:13.941563 140277221144320 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3912773132324219, loss=3.0026237964630127
I0206 17:13:48.944321 140277229537024 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.35159191489219666, loss=2.994535446166992
I0206 17:14:23.943327 140277221144320 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3816070854663849, loss=2.9701411724090576
I0206 17:14:58.972605 140277229537024 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.3471480906009674, loss=2.9399654865264893
I0206 17:15:33.977584 140277221144320 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.47478172183036804, loss=2.9904251098632812
I0206 17:16:08.999542 140277229537024 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.35098302364349365, loss=2.973418712615967
I0206 17:16:44.027190 140277221144320 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.43152710795402527, loss=3.032893419265747
I0206 17:17:19.053094 140277229537024 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.3849290907382965, loss=3.028343677520752
I0206 17:17:54.090790 140277221144320 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3293890357017517, loss=3.0025830268859863
I0206 17:18:29.110779 140277229537024 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.3417472839355469, loss=2.952781915664673
I0206 17:19:04.100383 140277221144320 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4118843972682953, loss=3.13252592086792
I0206 17:19:39.092025 140277229537024 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.39089515805244446, loss=3.0291738510131836
I0206 17:20:14.072485 140277221144320 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4579003155231476, loss=3.0153238773345947
I0206 17:20:49.056996 140277229537024 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.3808634579181671, loss=2.9983086585998535
I0206 17:21:24.044127 140277221144320 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3764312267303467, loss=3.00280499458313
I0206 17:21:59.038802 140277229537024 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4384460747241974, loss=3.0809719562530518
I0206 17:22:34.052261 140277221144320 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3742508292198181, loss=3.0148069858551025
I0206 17:23:09.050749 140277229537024 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5537439584732056, loss=2.968207359313965
I0206 17:23:44.030525 140277221144320 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3161506652832031, loss=2.993870735168457
I0206 17:24:19.075006 140277229537024 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.4770168364048004, loss=3.002485513687134
I0206 17:24:54.076955 140277221144320 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.33360397815704346, loss=3.0011658668518066
I0206 17:25:18.985058 140446903760704 spec.py:321] Evaluating on the training split.
I0206 17:25:21.988011 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:28:26.602720 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 17:28:29.319685 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:31:02.806741 140446903760704 spec.py:349] Evaluating on the test split.
I0206 17:31:05.518414 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:33:43.754579 140446903760704 submission_runner.py:408] Time since start: 15468.27s, 	Step: 26373, 	{'train/accuracy': 0.6318857073783875, 'train/loss': 1.8993377685546875, 'train/bleu': 31.127981987220117, 'validation/accuracy': 0.6520439982414246, 'validation/loss': 1.7544596195220947, 'validation/bleu': 27.978040845419148, 'validation/num_examples': 3000, 'test/accuracy': 0.6620068550109863, 'test/loss': 1.7058236598968506, 'test/bleu': 27.445354021922327, 'test/num_examples': 3003, 'score': 9277.202247619629, 'total_duration': 15468.266919612885, 'accumulated_submission_time': 9277.202247619629, 'accumulated_eval_time': 6189.89656496048, 'accumulated_logging_time': 0.316986083984375}
I0206 17:33:43.772928 140277229537024 logging_writer.py:48] [26373] accumulated_eval_time=6189.896565, accumulated_logging_time=0.316986, accumulated_submission_time=9277.202248, global_step=26373, preemption_count=0, score=9277.202248, test/accuracy=0.662007, test/bleu=27.445354, test/loss=1.705824, test/num_examples=3003, total_duration=15468.266920, train/accuracy=0.631886, train/bleu=31.127982, train/loss=1.899338, validation/accuracy=0.652044, validation/bleu=27.978041, validation/loss=1.754460, validation/num_examples=3000
I0206 17:33:53.554193 140277221144320 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.33576932549476624, loss=3.0137341022491455
I0206 17:34:28.450200 140277229537024 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.45601263642311096, loss=2.9580156803131104
I0206 17:35:03.415424 140277221144320 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.36575937271118164, loss=2.9518277645111084
I0206 17:35:38.422737 140277229537024 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3554905652999878, loss=3.028303384780884
I0206 17:36:13.410959 140277221144320 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.41483843326568604, loss=3.045832872390747
I0206 17:36:48.386286 140277229537024 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4007379710674286, loss=3.000894546508789
I0206 17:37:23.395872 140277221144320 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3236546516418457, loss=2.972515106201172
I0206 17:37:58.408472 140277229537024 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.37877053022384644, loss=2.9655730724334717
I0206 17:38:33.411839 140277221144320 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.38281458616256714, loss=2.965461492538452
I0206 17:39:08.461428 140277229537024 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.319718599319458, loss=2.9327616691589355
I0206 17:39:43.468592 140277221144320 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.36019477248191833, loss=2.987102508544922
I0206 17:40:18.480617 140277229537024 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5293545126914978, loss=3.0010530948638916
I0206 17:40:53.481468 140277221144320 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.4142860770225525, loss=3.011721134185791
I0206 17:41:28.482815 140277229537024 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.35807791352272034, loss=2.9968883991241455
I0206 17:42:03.506835 140277221144320 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.33657020330429077, loss=3.0146303176879883
I0206 17:42:38.476307 140277229537024 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.3574115037918091, loss=2.942328691482544
I0206 17:43:13.471392 140277221144320 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.3939906060695648, loss=3.120650053024292
I0206 17:43:48.480090 140277229537024 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.3673827052116394, loss=2.9209110736846924
I0206 17:44:23.485174 140277221144320 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.37813717126846313, loss=2.967261552810669
I0206 17:44:58.485683 140277229537024 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.4072761833667755, loss=2.986769437789917
I0206 17:45:33.489449 140277221144320 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.37223753333091736, loss=2.966203212738037
I0206 17:46:08.485748 140277229537024 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.5226635932922363, loss=2.935749053955078
I0206 17:46:43.487903 140277221144320 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.4059371054172516, loss=3.0850870609283447
I0206 17:47:18.468777 140277229537024 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.45488619804382324, loss=4.881657123565674
I0206 17:47:44.011754 140446903760704 spec.py:321] Evaluating on the training split.
I0206 17:47:47.015325 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:50:29.672471 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 17:50:32.383024 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:53:11.759250 140446903760704 spec.py:349] Evaluating on the test split.
I0206 17:53:14.481830 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 17:55:50.470271 140446903760704 submission_runner.py:408] Time since start: 16794.98s, 	Step: 28775, 	{'train/accuracy': 0.33275213837623596, 'train/loss': 3.951723337173462, 'train/bleu': 0.5231126249610653, 'validation/accuracy': 0.2997359037399292, 'validation/loss': 4.32647705078125, 'validation/bleu': 0.11509172939415249, 'validation/num_examples': 3000, 'test/accuracy': 0.28972169756889343, 'test/loss': 4.496841907501221, 'test/bleu': 0.07429394634716048, 'test/num_examples': 3003, 'score': 10117.357129573822, 'total_duration': 16794.98263859749, 'accumulated_submission_time': 10117.357129573822, 'accumulated_eval_time': 6676.355022907257, 'accumulated_logging_time': 0.34547972679138184}
I0206 17:55:50.488874 140277221144320 logging_writer.py:48] [28775] accumulated_eval_time=6676.355023, accumulated_logging_time=0.345480, accumulated_submission_time=10117.357130, global_step=28775, preemption_count=0, score=10117.357130, test/accuracy=0.289722, test/bleu=0.074294, test/loss=4.496842, test/num_examples=3003, total_duration=16794.982639, train/accuracy=0.332752, train/bleu=0.523113, train/loss=3.951723, validation/accuracy=0.299736, validation/bleu=0.115092, validation/loss=4.326477, validation/num_examples=3000
I0206 17:55:59.558119 140277229537024 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.450226753950119, loss=4.792984485626221
I0206 17:56:34.373879 140277221144320 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.0211961269378662, loss=4.719323635101318
I0206 17:57:09.297422 140277229537024 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7067185044288635, loss=4.7025465965271
I0206 17:57:44.230696 140277221144320 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5418512225151062, loss=4.662822246551514
I0206 17:58:19.168538 140277229537024 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6714592576026917, loss=4.69064474105835
I0206 17:58:54.093372 140277221144320 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.071273922920227, loss=4.679332256317139
I0206 17:59:29.043047 140277229537024 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.6901116371154785, loss=4.619781494140625
I0206 18:00:03.995253 140277221144320 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.5590513944625854, loss=4.575545310974121
I0206 18:00:38.986277 140277229537024 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5558850169181824, loss=3.2224626541137695
I0206 18:01:14.003227 140277221144320 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.3856825828552246, loss=3.0743248462677
I0206 18:01:49.011948 140277229537024 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.396305650472641, loss=3.0385420322418213
I0206 18:02:24.027738 140277221144320 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3801916837692261, loss=3.0017359256744385
I0206 18:02:59.054052 140277229537024 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.33035653829574585, loss=3.0157241821289062
I0206 18:03:34.098723 140277221144320 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3526380956172943, loss=3.0251359939575195
I0206 18:04:09.119797 140277229537024 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.36079922318458557, loss=2.972285270690918
I0206 18:04:44.166021 140277221144320 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.34444737434387207, loss=2.974170684814453
I0206 18:05:19.181641 140277229537024 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.35648635029792786, loss=3.03701114654541
I0206 18:05:54.213963 140277221144320 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.39497968554496765, loss=3.020733118057251
I0206 18:06:29.242923 140277229537024 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.3465151786804199, loss=2.9398679733276367
I0206 18:07:04.269965 140277221144320 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.36744022369384766, loss=3.0160739421844482
I0206 18:07:39.299090 140277229537024 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.3625982403755188, loss=2.948286294937134
I0206 18:08:14.335031 140277221144320 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.359648197889328, loss=2.9506287574768066
I0206 18:08:49.368443 140277229537024 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.42260393500328064, loss=2.978977918624878
I0206 18:09:24.382508 140277221144320 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.43996745347976685, loss=2.981468915939331
I0206 18:09:50.705349 140446903760704 spec.py:321] Evaluating on the training split.
I0206 18:09:53.712969 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:12:48.424677 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 18:12:51.128810 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:15:29.725420 140446903760704 spec.py:349] Evaluating on the test split.
I0206 18:15:32.445071 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:18:05.838217 140446903760704 submission_runner.py:408] Time since start: 18130.35s, 	Step: 31177, 	{'train/accuracy': 0.6317818760871887, 'train/loss': 1.9045122861862183, 'train/bleu': 30.54583083674687, 'validation/accuracy': 0.6527383327484131, 'validation/loss': 1.7534050941467285, 'validation/bleu': 27.74894091426883, 'validation/num_examples': 3000, 'test/accuracy': 0.662657618522644, 'test/loss': 1.6983014345169067, 'test/bleu': 27.054145258705955, 'test/num_examples': 3003, 'score': 10957.487800359726, 'total_duration': 18130.350570201874, 'accumulated_submission_time': 10957.487800359726, 'accumulated_eval_time': 7171.487823009491, 'accumulated_logging_time': 0.3740403652191162}
I0206 18:18:05.857657 140277229537024 logging_writer.py:48] [31177] accumulated_eval_time=7171.487823, accumulated_logging_time=0.374040, accumulated_submission_time=10957.487800, global_step=31177, preemption_count=0, score=10957.487800, test/accuracy=0.662658, test/bleu=27.054145, test/loss=1.698301, test/num_examples=3003, total_duration=18130.350570, train/accuracy=0.631782, train/bleu=30.545831, train/loss=1.904512, validation/accuracy=0.652738, validation/bleu=27.748941, validation/loss=1.753405, validation/num_examples=3000
I0206 18:18:14.254514 140277221144320 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.36727890372276306, loss=2.9482946395874023
I0206 18:18:49.140344 140277229537024 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.39339402318000793, loss=3.0193374156951904
I0206 18:19:24.120279 140277221144320 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.3889860510826111, loss=2.9988443851470947
I0206 18:19:59.124039 140277229537024 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4172268807888031, loss=2.94535231590271
I0206 18:20:34.105222 140277221144320 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.43245795369148254, loss=2.9446005821228027
I0206 18:21:09.110957 140277229537024 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.37319961190223694, loss=3.0371832847595215
I0206 18:21:44.088462 140277221144320 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3780859708786011, loss=3.0029611587524414
I0206 18:22:19.067496 140277229537024 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.32564446330070496, loss=2.981527328491211
I0206 18:22:54.054943 140277221144320 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.36629602313041687, loss=2.980818510055542
I0206 18:23:29.118826 140277229537024 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.44342565536499023, loss=2.9913601875305176
I0206 18:24:04.112340 140277221144320 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.34402400255203247, loss=2.9791030883789062
I0206 18:24:39.098865 140277229537024 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.364469051361084, loss=2.9183568954467773
I0206 18:25:14.117364 140277221144320 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.37717750668525696, loss=3.0981838703155518
I0206 18:25:49.132339 140277229537024 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.41393354535102844, loss=2.9559335708618164
I0206 18:26:24.129989 140277221144320 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.42108064889907837, loss=2.9382734298706055
I0206 18:26:59.170725 140277229537024 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3654598891735077, loss=2.9606401920318604
I0206 18:27:34.149436 140277221144320 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.3802175223827362, loss=3.067762851715088
I0206 18:28:09.157376 140277229537024 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.4054197669029236, loss=2.9817605018615723
I0206 18:28:44.155984 140277221144320 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.4436115622520447, loss=2.9980311393737793
I0206 18:29:19.145802 140277229537024 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.3499152660369873, loss=3.010003089904785
I0206 18:29:54.156164 140277221144320 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.3651486933231354, loss=3.0656394958496094
I0206 18:30:29.148391 140277229537024 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.33111870288848877, loss=2.964108467102051
I0206 18:31:04.132918 140277221144320 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.3814193606376648, loss=2.9767141342163086
I0206 18:31:39.127940 140277229537024 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.39897048473358154, loss=3.0624992847442627
I0206 18:32:06.137907 140446903760704 spec.py:321] Evaluating on the training split.
I0206 18:32:09.152230 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:35:05.435242 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 18:35:08.165043 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:37:43.146406 140446903760704 spec.py:349] Evaluating on the test split.
I0206 18:37:45.861084 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:40:20.123150 140446903760704 submission_runner.py:408] Time since start: 19464.64s, 	Step: 33579, 	{'train/accuracy': 0.6354771852493286, 'train/loss': 1.8794300556182861, 'train/bleu': 30.563934941582254, 'validation/accuracy': 0.6564084887504578, 'validation/loss': 1.7356117963790894, 'validation/bleu': 28.160680413584807, 'validation/num_examples': 3000, 'test/accuracy': 0.6678984761238098, 'test/loss': 1.6765241622924805, 'test/bleu': 27.420616054276252, 'test/num_examples': 3003, 'score': 11797.682716608047, 'total_duration': 19464.63546180725, 'accumulated_submission_time': 11797.682716608047, 'accumulated_eval_time': 7665.472952365875, 'accumulated_logging_time': 0.40361714363098145}
I0206 18:40:20.142875 140277221144320 logging_writer.py:48] [33579] accumulated_eval_time=7665.472952, accumulated_logging_time=0.403617, accumulated_submission_time=11797.682717, global_step=33579, preemption_count=0, score=11797.682717, test/accuracy=0.667898, test/bleu=27.420616, test/loss=1.676524, test/num_examples=3003, total_duration=19464.635462, train/accuracy=0.635477, train/bleu=30.563935, train/loss=1.879430, validation/accuracy=0.656408, validation/bleu=28.160680, validation/loss=1.735612, validation/num_examples=3000
I0206 18:40:27.809376 140277229537024 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.4177941679954529, loss=2.9650661945343018
I0206 18:41:02.648330 140277221144320 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.33967480063438416, loss=2.9477717876434326
I0206 18:41:37.627372 140277229537024 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.3378898799419403, loss=2.9662680625915527
I0206 18:42:12.634604 140277221144320 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.35157299041748047, loss=3.0066561698913574
I0206 18:42:47.602980 140277229537024 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.37832552194595337, loss=2.9783143997192383
I0206 18:43:22.594453 140277221144320 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.4042162299156189, loss=2.956696033477783
I0206 18:43:57.583318 140277229537024 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.4225442409515381, loss=3.061561346054077
I0206 18:44:32.591456 140277221144320 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.3401273488998413, loss=3.0304994583129883
I0206 18:45:07.574492 140277229537024 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.41777512431144714, loss=2.925778388977051
I0206 18:45:42.575989 140277221144320 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.37270283699035645, loss=3.0228426456451416
I0206 18:46:17.554250 140277229537024 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.35844355821609497, loss=2.958134889602661
I0206 18:46:52.525923 140277221144320 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3289068639278412, loss=2.9371471405029297
I0206 18:47:27.533485 140277229537024 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.35167527198791504, loss=3.00661563873291
I0206 18:48:02.541043 140277221144320 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3863363265991211, loss=3.0463709831237793
I0206 18:48:37.578763 140277229537024 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.36603835225105286, loss=2.9616122245788574
I0206 18:49:12.590219 140277221144320 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.360860675573349, loss=3.0077502727508545
I0206 18:49:47.594594 140277229537024 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.4167996942996979, loss=3.0093390941619873
I0206 18:50:22.599499 140277221144320 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.3952207565307617, loss=2.959554672241211
I0206 18:50:57.593743 140277229537024 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.3545268774032593, loss=2.965526819229126
I0206 18:51:32.564978 140277221144320 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3608781397342682, loss=3.0394959449768066
I0206 18:52:07.584258 140277229537024 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.40504398941993713, loss=2.982499599456787
I0206 18:52:42.551371 140277221144320 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.37737035751342773, loss=3.01007080078125
I0206 18:53:17.538336 140277229537024 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.33525416254997253, loss=2.922900676727295
I0206 18:53:52.542914 140277221144320 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.37027040123939514, loss=2.969918727874756
I0206 18:54:20.247046 140446903760704 spec.py:321] Evaluating on the training split.
I0206 18:54:23.246750 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:57:12.287458 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 18:57:14.991760 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 18:59:53.065507 140446903760704 spec.py:349] Evaluating on the test split.
I0206 18:59:55.773355 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 19:02:30.841052 140446903760704 submission_runner.py:408] Time since start: 20795.35s, 	Step: 35981, 	{'train/accuracy': 0.6375195384025574, 'train/loss': 1.8644064664840698, 'train/bleu': 31.009726888604998, 'validation/accuracy': 0.6567308306694031, 'validation/loss': 1.7263554334640503, 'validation/bleu': 28.174935173674044, 'validation/num_examples': 3000, 'test/accuracy': 0.6671082377433777, 'test/loss': 1.6758179664611816, 'test/bleu': 27.54092155263876, 'test/num_examples': 3003, 'score': 12637.7005712986, 'total_duration': 20795.35339331627, 'accumulated_submission_time': 12637.7005712986, 'accumulated_eval_time': 8156.0668778419495, 'accumulated_logging_time': 0.43479251861572266}
I0206 19:02:30.860851 140277229537024 logging_writer.py:48] [35981] accumulated_eval_time=8156.066878, accumulated_logging_time=0.434793, accumulated_submission_time=12637.700571, global_step=35981, preemption_count=0, score=12637.700571, test/accuracy=0.667108, test/bleu=27.540922, test/loss=1.675818, test/num_examples=3003, total_duration=20795.353393, train/accuracy=0.637520, train/bleu=31.009727, train/loss=1.864406, validation/accuracy=0.656731, validation/bleu=28.174935, validation/loss=1.726355, validation/num_examples=3000
I0206 19:02:37.843764 140277221144320 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.33324292302131653, loss=2.922968864440918
I0206 19:03:12.710104 140277229537024 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.40767404437065125, loss=3.020433187484741
I0206 19:03:47.688822 140277221144320 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.43227052688598633, loss=2.958875894546509
I0206 19:04:22.691888 140277229537024 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.3777408003807068, loss=2.9527387619018555
I0206 19:04:57.690792 140277221144320 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.36283186078071594, loss=2.9484164714813232
I0206 19:05:32.786527 140277229537024 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.3736271560192108, loss=3.0104875564575195
I0206 19:06:07.800645 140277221144320 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.5132354497909546, loss=2.9564554691314697
I0206 19:06:42.835759 140277229537024 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.4546087384223938, loss=2.9413692951202393
I0206 19:07:17.822884 140277221144320 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3742425739765167, loss=2.94358491897583
I0206 19:07:52.843906 140277229537024 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.4075019657611847, loss=3.022834300994873
I0206 19:08:27.853276 140277221144320 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.39871689677238464, loss=2.998807907104492
I0206 19:09:02.898799 140277229537024 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.39673569798469543, loss=2.8843414783477783
I0206 19:09:37.899560 140277221144320 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.36741337180137634, loss=2.980842351913452
I0206 19:10:12.886032 140277229537024 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4201302230358124, loss=2.956385850906372
I0206 19:10:47.916181 140277221144320 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.41514793038368225, loss=2.9891653060913086
I0206 19:11:23.010088 140277229537024 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.35856708884239197, loss=3.0132036209106445
I0206 19:11:58.028961 140277221144320 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.38881006836891174, loss=2.9509201049804688
I0206 19:12:33.037869 140277229537024 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.3368067741394043, loss=2.9556941986083984
I0206 19:13:08.038606 140277221144320 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.36911261081695557, loss=2.994342803955078
I0206 19:13:43.052028 140277229537024 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.4016178250312805, loss=3.025238513946533
I0206 19:14:18.057559 140277221144320 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3449557423591614, loss=2.977288246154785
I0206 19:14:53.050047 140277229537024 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.4036335349082947, loss=2.979367971420288
I0206 19:15:28.029306 140277221144320 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.38599368929862976, loss=2.952505588531494
I0206 19:16:03.004440 140277229537024 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.3682495951652527, loss=2.980067491531372
I0206 19:16:31.100897 140446903760704 spec.py:321] Evaluating on the training split.
I0206 19:16:34.112180 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 19:19:19.695196 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 19:19:22.417764 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 19:21:51.447269 140446903760704 spec.py:349] Evaluating on the test split.
I0206 19:21:54.152516 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 19:24:21.021878 140446903760704 submission_runner.py:408] Time since start: 22105.53s, 	Step: 38382, 	{'train/accuracy': 0.6450705528259277, 'train/loss': 1.804628610610962, 'train/bleu': 32.15894108712716, 'validation/accuracy': 0.6593098640441895, 'validation/loss': 1.7143281698226929, 'validation/bleu': 28.36643041451481, 'validation/num_examples': 3000, 'test/accuracy': 0.6678752303123474, 'test/loss': 1.6616512537002563, 'test/bleu': 27.693778275595207, 'test/num_examples': 3003, 'score': 13477.855075120926, 'total_duration': 22105.53423690796, 'accumulated_submission_time': 13477.855075120926, 'accumulated_eval_time': 8625.987797498703, 'accumulated_logging_time': 0.46489620208740234}
I0206 19:24:21.042010 140277221144320 logging_writer.py:48] [38382] accumulated_eval_time=8625.987797, accumulated_logging_time=0.464896, accumulated_submission_time=13477.855075, global_step=38382, preemption_count=0, score=13477.855075, test/accuracy=0.667875, test/bleu=27.693778, test/loss=1.661651, test/num_examples=3003, total_duration=22105.534237, train/accuracy=0.645071, train/bleu=32.158941, train/loss=1.804629, validation/accuracy=0.659310, validation/bleu=28.366430, validation/loss=1.714328, validation/num_examples=3000
I0206 19:24:27.672204 140277229537024 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.3971867859363556, loss=2.9484434127807617
I0206 19:25:02.607190 140277221144320 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.44992268085479736, loss=2.9544973373413086
I0206 19:25:37.561121 140277229537024 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.5929089188575745, loss=3.027905225753784
I0206 19:26:12.536101 140277221144320 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.36774560809135437, loss=2.9034862518310547
I0206 19:26:47.530909 140277229537024 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3706589639186859, loss=2.9408557415008545
I0206 19:27:22.521656 140277221144320 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.3351842164993286, loss=2.9223549365997314
I0206 19:27:57.526622 140277229537024 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.4206254780292511, loss=3.001429796218872
I0206 19:28:32.533757 140277221144320 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.46152713894844055, loss=2.928746461868286
I0206 19:29:07.532320 140277229537024 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3585682511329651, loss=3.0141124725341797
I0206 19:29:42.540796 140277221144320 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.36664146184921265, loss=2.9518566131591797
I0206 19:30:17.552466 140277229537024 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.37603887915611267, loss=2.979393720626831
I0206 19:30:52.571340 140277221144320 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.3405197262763977, loss=2.921569585800171
I0206 19:31:27.573667 140277229537024 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.33228716254234314, loss=2.9518728256225586
I0206 19:32:02.619897 140277221144320 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.3291143774986267, loss=2.9361536502838135
I0206 19:32:37.643103 140277229537024 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.37532153725624084, loss=2.98175048828125
I0206 19:33:12.657668 140277221144320 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.4394739866256714, loss=2.943121910095215
I0206 19:33:47.629240 140277229537024 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.4004262387752533, loss=2.996793508529663
I0206 19:34:22.648888 140277221144320 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.3904091417789459, loss=2.9214751720428467
I0206 19:34:57.669260 140277229537024 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.34669604897499084, loss=2.9145097732543945
I0206 19:35:32.728871 140277221144320 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.33821505308151245, loss=2.9609575271606445
I0206 19:36:07.757260 140277229537024 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.391613632440567, loss=2.9545130729675293
I0206 19:36:42.773619 140277221144320 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3618124723434448, loss=2.881960153579712
I0206 19:37:17.801290 140277229537024 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.38166627287864685, loss=2.978532552719116
I0206 19:37:52.789873 140277221144320 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3707417845726013, loss=2.948668956756592
I0206 19:38:21.205213 140446903760704 spec.py:321] Evaluating on the training split.
I0206 19:38:24.208967 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 19:41:36.445301 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 19:41:39.144771 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 19:44:14.687535 140446903760704 spec.py:349] Evaluating on the test split.
I0206 19:44:17.396789 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 19:46:47.769243 140446903760704 submission_runner.py:408] Time since start: 23452.28s, 	Step: 40783, 	{'train/accuracy': 0.6396812200546265, 'train/loss': 1.8572800159454346, 'train/bleu': 31.35065259778344, 'validation/accuracy': 0.6599918007850647, 'validation/loss': 1.7028095722198486, 'validation/bleu': 28.272012813109214, 'validation/num_examples': 3000, 'test/accuracy': 0.6715008020401001, 'test/loss': 1.6438584327697754, 'test/bleu': 27.98273853137321, 'test/num_examples': 3003, 'score': 14317.929590463638, 'total_duration': 23452.281606912613, 'accumulated_submission_time': 14317.929590463638, 'accumulated_eval_time': 9132.55176949501, 'accumulated_logging_time': 0.49638891220092773}
I0206 19:46:47.790363 140277229537024 logging_writer.py:48] [40783] accumulated_eval_time=9132.551769, accumulated_logging_time=0.496389, accumulated_submission_time=14317.929590, global_step=40783, preemption_count=0, score=14317.929590, test/accuracy=0.671501, test/bleu=27.982739, test/loss=1.643858, test/num_examples=3003, total_duration=23452.281607, train/accuracy=0.639681, train/bleu=31.350653, train/loss=1.857280, validation/accuracy=0.659992, validation/bleu=28.272013, validation/loss=1.702810, validation/num_examples=3000
I0206 19:46:54.074882 140277221144320 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.38951408863067627, loss=2.8886959552764893
I0206 19:47:28.942946 140277229537024 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3799632787704468, loss=2.9756393432617188
I0206 19:48:03.865088 140277221144320 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.36510828137397766, loss=2.9603238105773926
I0206 19:48:38.896619 140277229537024 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.36199086904525757, loss=2.9776487350463867
I0206 19:49:13.934759 140277221144320 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.37651941180229187, loss=2.9675474166870117
I0206 19:49:48.932348 140277229537024 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3692030608654022, loss=2.9288978576660156
I0206 19:50:23.922795 140277221144320 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.4241061508655548, loss=2.8581132888793945
I0206 19:50:58.900788 140277229537024 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.35942864418029785, loss=2.9153223037719727
I0206 19:51:33.898223 140277221144320 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.3260799050331116, loss=2.9848623275756836
I0206 19:52:08.926116 140277229537024 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.41751348972320557, loss=2.9771854877471924
I0206 19:52:43.919634 140277221144320 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.3505670428276062, loss=2.919982671737671
I0206 19:53:18.907623 140277229537024 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3651758134365082, loss=2.966500997543335
I0206 19:53:53.886286 140277221144320 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.34394383430480957, loss=2.9350385665893555
I0206 19:54:28.897342 140277229537024 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.35337209701538086, loss=2.9186267852783203
I0206 19:55:03.918066 140277221144320 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3758947253227234, loss=3.061150550842285
I0206 19:55:38.930778 140277229537024 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.38320255279541016, loss=2.9872350692749023
I0206 19:56:13.925705 140277221144320 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3567073345184326, loss=2.921820878982544
I0206 19:56:48.906259 140277229537024 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3603802025318146, loss=2.974160671234131
I0206 19:57:23.901205 140277221144320 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.35843122005462646, loss=2.937769889831543
I0206 19:57:58.880301 140277229537024 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.34226399660110474, loss=2.9972620010375977
I0206 19:58:33.915743 140277221144320 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.349331259727478, loss=2.963372230529785
I0206 19:59:08.911061 140277229537024 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.38104838132858276, loss=2.9133403301239014
I0206 19:59:43.914780 140277221144320 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3385266363620758, loss=2.8763043880462646
I0206 20:00:18.911692 140277229537024 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.43945643305778503, loss=2.9473319053649902
I0206 20:00:48.039300 140446903760704 spec.py:321] Evaluating on the training split.
I0206 20:00:51.050111 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:03:59.777091 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 20:04:02.500068 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:06:42.122318 140446903760704 spec.py:349] Evaluating on the test split.
I0206 20:06:44.836688 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:09:25.291952 140446903760704 submission_runner.py:408] Time since start: 24809.80s, 	Step: 43185, 	{'train/accuracy': 0.6362661123275757, 'train/loss': 1.8771040439605713, 'train/bleu': 31.457801930764408, 'validation/accuracy': 0.6599298119544983, 'validation/loss': 1.7009010314941406, 'validation/bleu': 28.352906109577713, 'validation/num_examples': 3000, 'test/accuracy': 0.6713729500770569, 'test/loss': 1.6449847221374512, 'test/bleu': 27.856974105756258, 'test/num_examples': 3003, 'score': 15158.089678287506, 'total_duration': 24809.804277181625, 'accumulated_submission_time': 15158.089678287506, 'accumulated_eval_time': 9649.80432677269, 'accumulated_logging_time': 0.5288918018341064}
I0206 20:09:25.312826 140277221144320 logging_writer.py:48] [43185] accumulated_eval_time=9649.804327, accumulated_logging_time=0.528892, accumulated_submission_time=15158.089678, global_step=43185, preemption_count=0, score=15158.089678, test/accuracy=0.671373, test/bleu=27.856974, test/loss=1.644985, test/num_examples=3003, total_duration=24809.804277, train/accuracy=0.636266, train/bleu=31.457802, train/loss=1.877104, validation/accuracy=0.659930, validation/bleu=28.352906, validation/loss=1.700901, validation/num_examples=3000
I0206 20:09:30.883559 140277229537024 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.38787391781806946, loss=2.9231488704681396
I0206 20:10:05.737052 140277221144320 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.35083141922950745, loss=2.9055919647216797
I0206 20:10:40.652294 140277229537024 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3941015601158142, loss=2.948029041290283
I0206 20:11:15.623710 140277221144320 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.33478236198425293, loss=2.8930253982543945
I0206 20:11:50.599010 140277229537024 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.3822951316833496, loss=2.9706966876983643
I0206 20:12:25.550912 140277221144320 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.3690682351589203, loss=2.989495277404785
I0206 20:13:00.535510 140277229537024 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.43595489859580994, loss=2.9019765853881836
I0206 20:13:35.553469 140277221144320 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3568747639656067, loss=2.9238979816436768
I0206 20:14:10.516941 140277229537024 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3543957471847534, loss=2.8908207416534424
I0206 20:14:45.476704 140277221144320 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.40019580721855164, loss=2.959756374359131
I0206 20:15:20.446889 140277229537024 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.33911675214767456, loss=2.9433095455169678
I0206 20:15:55.447070 140277221144320 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.43122953176498413, loss=2.8506298065185547
I0206 20:16:30.446552 140277229537024 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.373818963766098, loss=2.891322135925293
I0206 20:17:05.467294 140277221144320 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.3536795377731323, loss=2.9588420391082764
I0206 20:17:40.427227 140277229537024 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2682958841323853, loss=4.930881500244141
I0206 20:18:15.383682 140277221144320 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.46775683760643005, loss=4.770074367523193
I0206 20:18:50.337944 140277229537024 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.7912571430206299, loss=4.704562187194824
I0206 20:19:25.276793 140277221144320 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.530586838722229, loss=4.66021203994751
I0206 20:20:00.214148 140277229537024 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.50506591796875, loss=4.622547149658203
I0206 20:20:35.162426 140277221144320 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.6287641525268555, loss=4.559379577636719
I0206 20:21:10.077123 140277229537024 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5787407755851746, loss=4.601517200469971
I0206 20:21:44.988869 140277221144320 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.718167781829834, loss=4.5493388175964355
I0206 20:22:19.924975 140277229537024 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.5630704164505005, loss=4.540360927581787
I0206 20:22:54.903390 140277221144320 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.118395447731018, loss=3.835280179977417
I0206 20:23:25.408822 140446903760704 spec.py:321] Evaluating on the training split.
I0206 20:23:28.408830 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:26:53.582810 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 20:26:56.305728 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:30:39.373604 140446903760704 spec.py:349] Evaluating on the test split.
I0206 20:30:42.076114 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:34:01.225353 140446903760704 submission_runner.py:408] Time since start: 26285.74s, 	Step: 45589, 	{'train/accuracy': 0.590805172920227, 'train/loss': 2.1539804935455322, 'train/bleu': 24.934241026593256, 'validation/accuracy': 0.5927143096923828, 'validation/loss': 2.137026071548462, 'validation/bleu': 20.67777602591015, 'validation/num_examples': 3000, 'test/accuracy': 0.5968043804168701, 'test/loss': 2.1455793380737305, 'test/bleu': 19.25898748849468, 'test/num_examples': 3003, 'score': 15998.09845662117, 'total_duration': 26285.73772263527, 'accumulated_submission_time': 15998.09845662117, 'accumulated_eval_time': 10285.620803833008, 'accumulated_logging_time': 0.5597467422485352}
I0206 20:34:01.245994 140277229537024 logging_writer.py:48] [45589] accumulated_eval_time=10285.620804, accumulated_logging_time=0.559747, accumulated_submission_time=15998.098457, global_step=45589, preemption_count=0, score=15998.098457, test/accuracy=0.596804, test/bleu=19.258987, test/loss=2.145579, test/num_examples=3003, total_duration=26285.737723, train/accuracy=0.590805, train/bleu=24.934241, train/loss=2.153980, validation/accuracy=0.592714, validation/bleu=20.677776, validation/loss=2.137026, validation/num_examples=3000
I0206 20:34:05.440122 140277221144320 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.4576570987701416, loss=3.230055570602417
I0206 20:34:40.280301 140277229537024 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.5631771683692932, loss=3.119079828262329
I0206 20:35:15.215273 140277221144320 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.4359685480594635, loss=2.9824347496032715
I0206 20:35:50.195876 140277229537024 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.48345956206321716, loss=2.932321786880493
I0206 20:36:25.173602 140277221144320 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.4580906927585602, loss=2.941263437271118
I0206 20:37:00.156299 140277229537024 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3813832700252533, loss=2.9891858100891113
I0206 20:37:35.137386 140277221144320 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.37775981426239014, loss=2.9106082916259766
I0206 20:38:10.148435 140277229537024 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.45130741596221924, loss=2.950134038925171
I0206 20:38:45.228587 140277221144320 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.4327605962753296, loss=3.02219820022583
I0206 20:39:20.212064 140277229537024 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3691527247428894, loss=3.009233236312866
I0206 20:39:55.189765 140277221144320 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.42065393924713135, loss=3.0121707916259766
I0206 20:40:30.197478 140277229537024 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.36746862530708313, loss=2.992128372192383
I0206 20:41:05.149068 140277221144320 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.388691246509552, loss=2.967592716217041
I0206 20:41:40.147700 140277229537024 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.37817779183387756, loss=2.9320228099823
I0206 20:42:15.147382 140277221144320 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.35793328285217285, loss=2.940373659133911
I0206 20:42:50.132323 140277229537024 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.3707011640071869, loss=2.9047656059265137
I0206 20:43:25.106997 140277221144320 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.4388415217399597, loss=2.954956531524658
I0206 20:44:00.145357 140277229537024 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.3887525498867035, loss=2.9102838039398193
I0206 20:44:35.129882 140277221144320 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3325592577457428, loss=2.9518721103668213
I0206 20:45:10.152444 140277229537024 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.4442290961742401, loss=2.91605544090271
I0206 20:45:45.118627 140277221144320 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.35000327229499817, loss=2.941833019256592
I0206 20:46:20.115689 140277229537024 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.49611449241638184, loss=2.9804060459136963
I0206 20:46:55.117342 140277221144320 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3803770840167999, loss=2.9399938583374023
I0206 20:47:30.097503 140277229537024 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3620345890522003, loss=2.942539691925049
I0206 20:48:01.264485 140446903760704 spec.py:321] Evaluating on the training split.
I0206 20:48:04.288311 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:50:49.418803 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 20:50:52.128361 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:53:17.712945 140446903760704 spec.py:349] Evaluating on the test split.
I0206 20:53:20.424340 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 20:55:50.944788 140446903760704 submission_runner.py:408] Time since start: 27595.46s, 	Step: 47991, 	{'train/accuracy': 0.6427844762802124, 'train/loss': 1.8317044973373413, 'train/bleu': 31.89963562097901, 'validation/accuracy': 0.6606737375259399, 'validation/loss': 1.6950204372406006, 'validation/bleu': 28.240349978262042, 'validation/num_examples': 3000, 'test/accuracy': 0.6726977229118347, 'test/loss': 1.6335320472717285, 'test/bleu': 27.737813681491367, 'test/num_examples': 3003, 'score': 16838.03125667572, 'total_duration': 27595.45713019371, 'accumulated_submission_time': 16838.03125667572, 'accumulated_eval_time': 10755.301027297974, 'accumulated_logging_time': 0.5908377170562744}
I0206 20:55:50.968386 140277221144320 logging_writer.py:48] [47991] accumulated_eval_time=10755.301027, accumulated_logging_time=0.590838, accumulated_submission_time=16838.031257, global_step=47991, preemption_count=0, score=16838.031257, test/accuracy=0.672698, test/bleu=27.737814, test/loss=1.633532, test/num_examples=3003, total_duration=27595.457130, train/accuracy=0.642784, train/bleu=31.899636, train/loss=1.831704, validation/accuracy=0.660674, validation/bleu=28.240350, validation/loss=1.695020, validation/num_examples=3000
I0206 20:55:54.456528 140277229537024 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.4051932394504547, loss=2.96986985206604
I0206 20:56:29.316056 140277221144320 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.40592238306999207, loss=2.941477060317993
I0206 20:57:04.263691 140277229537024 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3708486258983612, loss=2.994805097579956
I0206 20:57:39.296622 140277221144320 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.34636032581329346, loss=2.9461617469787598
I0206 20:58:14.267253 140277229537024 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.36708390712738037, loss=2.903393030166626
I0206 20:58:49.242500 140277221144320 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.36945974826812744, loss=2.9188647270202637
I0206 20:59:24.241004 140277229537024 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3747151792049408, loss=3.0028207302093506
I0206 20:59:59.246996 140277221144320 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3749321401119232, loss=2.9634602069854736
I0206 21:00:34.259528 140277229537024 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.39281225204467773, loss=2.889406442642212
I0206 21:01:09.248749 140277221144320 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3644246757030487, loss=2.8803610801696777
I0206 21:01:44.259150 140277229537024 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3867807388305664, loss=2.951871633529663
I0206 21:02:19.269778 140277221144320 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.44533929228782654, loss=2.919872760772705
I0206 21:02:54.275352 140277229537024 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.37992265820503235, loss=2.869417667388916
I0206 21:03:29.289553 140277221144320 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3272671103477478, loss=2.8729753494262695
I0206 21:04:04.307087 140277229537024 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.38794535398483276, loss=2.912698268890381
I0206 21:04:39.323167 140277221144320 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3472973108291626, loss=2.9758524894714355
I0206 21:05:14.354165 140277229537024 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.39915212988853455, loss=2.9155702590942383
I0206 21:05:49.366231 140277221144320 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3835209608078003, loss=2.997936725616455
I0206 21:06:24.369493 140277229537024 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.35762280225753784, loss=2.8877809047698975
I0206 21:06:59.380993 140277221144320 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.45690372586250305, loss=2.9640347957611084
I0206 21:07:34.392148 140277229537024 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3913766145706177, loss=2.9255034923553467
I0206 21:08:09.404431 140277221144320 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.41821667551994324, loss=3.033078908920288
I0206 21:08:44.429746 140277229537024 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.4296749532222748, loss=2.934048652648926
I0206 21:09:19.435975 140277221144320 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.43309569358825684, loss=2.8856072425842285
I0206 21:09:50.992598 140446903760704 spec.py:321] Evaluating on the training split.
I0206 21:09:54.003890 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 21:12:55.952061 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 21:12:58.644685 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 21:15:24.528875 140446903760704 spec.py:349] Evaluating on the test split.
I0206 21:15:27.259757 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 21:17:59.978077 140446903760704 submission_runner.py:408] Time since start: 28924.49s, 	Step: 50392, 	{'train/accuracy': 0.6582806706428528, 'train/loss': 1.7210654020309448, 'train/bleu': 32.246968132861625, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.6892980337142944, 'validation/bleu': 28.434917155790025, 'validation/num_examples': 3000, 'test/accuracy': 0.6735227704048157, 'test/loss': 1.623759388923645, 'test/bleu': 27.932242256488045, 'test/num_examples': 3003, 'score': 17677.970749616623, 'total_duration': 28924.49044728279, 'accumulated_submission_time': 17677.970749616623, 'accumulated_eval_time': 11244.286452054977, 'accumulated_logging_time': 0.6243085861206055}
I0206 21:17:59.999184 140277229537024 logging_writer.py:48] [50392] accumulated_eval_time=11244.286452, accumulated_logging_time=0.624309, accumulated_submission_time=17677.970750, global_step=50392, preemption_count=0, score=17677.970750, test/accuracy=0.673523, test/bleu=27.932242, test/loss=1.623759, test/num_examples=3003, total_duration=28924.490447, train/accuracy=0.658281, train/bleu=32.246968, train/loss=1.721065, validation/accuracy=0.662062, validation/bleu=28.434917, validation/loss=1.689298, validation/num_examples=3000
I0206 21:18:03.150846 140277221144320 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.37989604473114014, loss=2.9765162467956543
I0206 21:18:37.991235 140277229537024 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3785628080368042, loss=2.960726261138916
I0206 21:19:12.923879 140277221144320 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.34099191427230835, loss=2.8957152366638184
I0206 21:19:47.909834 140277229537024 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.37907448410987854, loss=2.9128940105438232
I0206 21:20:22.919308 140277221144320 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3602631688117981, loss=2.940250873565674
I0206 21:20:57.910754 140277229537024 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.47789472341537476, loss=2.9724957942962646
I0206 21:21:32.917379 140277221144320 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.38247522711753845, loss=2.931213140487671
I0206 21:22:07.939279 140277229537024 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.33034488558769226, loss=2.9068984985351562
I0206 21:22:42.926609 140277221144320 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.41551995277404785, loss=2.97513484954834
I0206 21:23:17.910500 140277229537024 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.34577009081840515, loss=2.949357271194458
I0206 21:23:52.941910 140277221144320 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.3939501941204071, loss=2.901803731918335
I0206 21:24:27.975802 140277229537024 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.34387046098709106, loss=2.9316842555999756
I0206 21:25:02.958422 140277221144320 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.39263224601745605, loss=3.0018930435180664
I0206 21:25:37.921008 140277229537024 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.42092248797416687, loss=2.930117607116699
I0206 21:26:12.960486 140277221144320 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3846052289009094, loss=3.0495951175689697
I0206 21:26:47.960254 140277229537024 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.37849387526512146, loss=2.874447822570801
I0206 21:27:22.956031 140277221144320 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.47246792912483215, loss=2.894116163253784
I0206 21:27:57.969868 140277229537024 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.39107951521873474, loss=2.9949839115142822
I0206 21:28:32.998607 140277221144320 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3505338430404663, loss=2.92083477973938
I0206 21:29:08.013928 140277229537024 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.3807419538497925, loss=2.918227195739746
I0206 21:29:43.055252 140277221144320 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.40070876479148865, loss=2.9714150428771973
I0206 21:30:18.052203 140277229537024 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3867173194885254, loss=2.9974706172943115
I0206 21:30:53.061093 140277221144320 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3626432716846466, loss=2.960334062576294
I0206 21:31:28.099960 140277229537024 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.38002318143844604, loss=2.956329107284546
I0206 21:32:00.046652 140446903760704 spec.py:321] Evaluating on the training split.
I0206 21:32:03.074374 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 21:35:21.920265 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 21:35:24.629549 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 21:38:01.201392 140446903760704 spec.py:349] Evaluating on the test split.
I0206 21:38:03.946909 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 21:41:06.042419 140446903760704 submission_runner.py:408] Time since start: 30310.55s, 	Step: 52793, 	{'train/accuracy': 0.6478515863418579, 'train/loss': 1.799898624420166, 'train/bleu': 31.518275366485874, 'validation/accuracy': 0.6649638414382935, 'validation/loss': 1.6738790273666382, 'validation/bleu': 28.84308670692472, 'validation/num_examples': 3000, 'test/accuracy': 0.6762535572052002, 'test/loss': 1.611672282218933, 'test/bleu': 28.25260805810742, 'test/num_examples': 3003, 'score': 18517.93385744095, 'total_duration': 30310.554755687714, 'accumulated_submission_time': 18517.93385744095, 'accumulated_eval_time': 11790.282139539719, 'accumulated_logging_time': 0.6552319526672363}
I0206 21:41:06.070414 140277221144320 logging_writer.py:48] [52793] accumulated_eval_time=11790.282140, accumulated_logging_time=0.655232, accumulated_submission_time=18517.933857, global_step=52793, preemption_count=0, score=18517.933857, test/accuracy=0.676254, test/bleu=28.252608, test/loss=1.611672, test/num_examples=3003, total_duration=30310.554756, train/accuracy=0.647852, train/bleu=31.518275, train/loss=1.799899, validation/accuracy=0.664964, validation/bleu=28.843087, validation/loss=1.673879, validation/num_examples=3000
I0206 21:41:08.867860 140277229537024 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.36140161752700806, loss=2.9699392318725586
I0206 21:41:43.718956 140277221144320 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.38097143173217773, loss=2.980851650238037
I0206 21:42:18.682875 140277229537024 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.37900468707084656, loss=2.9723453521728516
I0206 21:42:53.659113 140277221144320 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.3804584741592407, loss=2.9538373947143555
I0206 21:43:28.622137 140277229537024 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.3934962749481201, loss=2.915912389755249
I0206 21:44:03.658379 140277221144320 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.4105471968650818, loss=2.882951021194458
I0206 21:44:38.629032 140277229537024 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3545388877391815, loss=2.9444894790649414
I0206 21:45:13.632407 140277221144320 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3660028278827667, loss=2.9004745483398438
I0206 21:45:48.647097 140277229537024 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.3955702483654022, loss=2.9532313346862793
I0206 21:46:23.676442 140277221144320 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.4095609784126282, loss=2.928727149963379
I0206 21:46:58.680902 140277229537024 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.3452141582965851, loss=2.9615230560302734
I0206 21:47:33.703736 140277221144320 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.39269575476646423, loss=2.873159170150757
I0206 21:48:08.694784 140277229537024 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3561188578605652, loss=2.8880434036254883
I0206 21:48:43.675315 140277221144320 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3946596682071686, loss=2.8482718467712402
I0206 21:49:18.766962 140277229537024 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.41927558183670044, loss=2.9300994873046875
I0206 21:49:53.788300 140277221144320 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.42437630891799927, loss=2.8574466705322266
I0206 21:50:28.783740 140277229537024 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.35605114698410034, loss=2.8623313903808594
I0206 21:51:03.744765 140277221144320 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3532235324382782, loss=2.9137792587280273
I0206 21:51:38.775294 140277229537024 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3844006061553955, loss=2.9380743503570557
I0206 21:52:13.761161 140277221144320 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.39156264066696167, loss=2.9085476398468018
I0206 21:52:48.767164 140277229537024 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3616747558116913, loss=2.8615283966064453
I0206 21:53:23.764760 140277221144320 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.3654749393463135, loss=2.9324986934661865
I0206 21:53:58.730549 140277229537024 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3586495816707611, loss=2.887476682662964
I0206 21:54:33.716006 140277221144320 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.40848633646965027, loss=2.97227144241333
I0206 21:55:06.328739 140446903760704 spec.py:321] Evaluating on the training split.
I0206 21:55:09.344333 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 21:58:16.669454 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 21:58:19.394904 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:00:47.881469 140446903760704 spec.py:349] Evaluating on the test split.
I0206 22:00:50.601876 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:03:23.389633 140446903760704 submission_runner.py:408] Time since start: 31647.90s, 	Step: 55195, 	{'train/accuracy': 0.6423264145851135, 'train/loss': 1.8311876058578491, 'train/bleu': 31.520352554833806, 'validation/accuracy': 0.6630296111106873, 'validation/loss': 1.6784354448318481, 'validation/bleu': 28.5659265759097, 'validation/num_examples': 3000, 'test/accuracy': 0.6748591065406799, 'test/loss': 1.615303874015808, 'test/bleu': 28.13716320799326, 'test/num_examples': 3003, 'score': 19358.101722240448, 'total_duration': 31647.902009248734, 'accumulated_submission_time': 19358.101722240448, 'accumulated_eval_time': 12287.342983961105, 'accumulated_logging_time': 0.6947894096374512}
I0206 22:03:23.411308 140277229537024 logging_writer.py:48] [55195] accumulated_eval_time=12287.342984, accumulated_logging_time=0.694789, accumulated_submission_time=19358.101722, global_step=55195, preemption_count=0, score=19358.101722, test/accuracy=0.674859, test/bleu=28.137163, test/loss=1.615304, test/num_examples=3003, total_duration=31647.902009, train/accuracy=0.642326, train/bleu=31.520353, train/loss=1.831188, validation/accuracy=0.663030, validation/bleu=28.565927, validation/loss=1.678435, validation/num_examples=3000
I0206 22:03:25.507747 140277221144320 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.38432005047798157, loss=2.8544938564300537
I0206 22:04:00.334271 140277229537024 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.368796706199646, loss=2.9518775939941406
I0206 22:04:35.239657 140277221144320 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3705052435398102, loss=2.9173412322998047
I0206 22:05:10.191825 140277229537024 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.37369418144226074, loss=2.8858394622802734
I0206 22:05:45.147642 140277221144320 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3942878246307373, loss=2.947394847869873
I0206 22:06:20.112846 140277229537024 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.6292999982833862, loss=3.0187602043151855
I0206 22:06:55.099960 140277221144320 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.047851085662842, loss=3.424459934234619
I0206 22:07:30.102396 140277229537024 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.2278125286102295, loss=3.152265787124634
I0206 22:08:05.121244 140277221144320 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.4218151867389679, loss=2.9111971855163574
I0206 22:08:40.105778 140277229537024 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.365441232919693, loss=2.867246627807617
I0206 22:09:15.080513 140277221144320 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.40168872475624084, loss=2.9693028926849365
I0206 22:09:50.051678 140277229537024 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.39658230543136597, loss=2.9565136432647705
I0206 22:10:25.021977 140277221144320 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.33992794156074524, loss=2.88299822807312
I0206 22:11:00.013313 140277229537024 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3408612906932831, loss=2.930264949798584
I0206 22:11:35.010272 140277221144320 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.33819887042045593, loss=2.866051435470581
I0206 22:12:10.006404 140277229537024 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.3849892020225525, loss=2.971985101699829
I0206 22:12:44.999061 140277221144320 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3895444869995117, loss=2.978628635406494
I0206 22:13:20.010948 140277229537024 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.36169424653053284, loss=3.003580093383789
I0206 22:13:54.981815 140277221144320 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.4072013199329376, loss=2.876851797103882
I0206 22:14:29.960277 140277229537024 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.35354769229888916, loss=2.867100238800049
I0206 22:15:04.961841 140277221144320 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3693027198314667, loss=2.910675525665283
I0206 22:15:39.953450 140277229537024 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3828050196170807, loss=2.9115381240844727
I0206 22:16:14.923390 140277221144320 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.37738603353500366, loss=2.901881456375122
I0206 22:16:49.896601 140277229537024 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.33228006958961487, loss=3.0030741691589355
I0206 22:17:23.553611 140446903760704 spec.py:321] Evaluating on the training split.
I0206 22:17:26.556403 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:20:03.440362 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 22:20:06.144859 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:22:35.504808 140446903760704 spec.py:349] Evaluating on the test split.
I0206 22:22:38.216428 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:25:05.420853 140446903760704 submission_runner.py:408] Time since start: 32949.93s, 	Step: 57598, 	{'train/accuracy': 0.6510561108589172, 'train/loss': 1.7641416788101196, 'train/bleu': 31.744707806123117, 'validation/accuracy': 0.6652490496635437, 'validation/loss': 1.6621553897857666, 'validation/bleu': 28.623343659957573, 'validation/num_examples': 3000, 'test/accuracy': 0.6766951680183411, 'test/loss': 1.596364974975586, 'test/bleu': 28.10985823697703, 'test/num_examples': 3003, 'score': 20198.159299850464, 'total_duration': 32949.93321561813, 'accumulated_submission_time': 20198.159299850464, 'accumulated_eval_time': 12749.21016407013, 'accumulated_logging_time': 0.7266678810119629}
I0206 22:25:05.442868 140277221144320 logging_writer.py:48] [57598] accumulated_eval_time=12749.210164, accumulated_logging_time=0.726668, accumulated_submission_time=20198.159300, global_step=57598, preemption_count=0, score=20198.159300, test/accuracy=0.676695, test/bleu=28.109858, test/loss=1.596365, test/num_examples=3003, total_duration=32949.933216, train/accuracy=0.651056, train/bleu=31.744708, train/loss=1.764142, validation/accuracy=0.665249, validation/bleu=28.623344, validation/loss=1.662155, validation/num_examples=3000
I0206 22:25:06.517328 140277229537024 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3666735887527466, loss=2.920652389526367
I0206 22:25:41.379465 140277221144320 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.3584428131580353, loss=2.9758520126342773
I0206 22:26:16.312707 140277229537024 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.35261350870132446, loss=2.9658596515655518
I0206 22:26:51.331475 140277221144320 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.33515623211860657, loss=2.8122406005859375
I0206 22:27:26.320075 140277229537024 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.36178916692733765, loss=2.86799955368042
I0206 22:28:01.311272 140277221144320 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.37318381667137146, loss=2.880635976791382
I0206 22:28:36.338899 140277229537024 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3704618215560913, loss=2.827155351638794
I0206 22:29:11.382412 140277221144320 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3412272036075592, loss=2.8824405670166016
I0206 22:29:46.368929 140277229537024 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.34374019503593445, loss=2.885772228240967
I0206 22:30:21.375012 140277221144320 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.35865506529808044, loss=2.9259798526763916
I0206 22:30:56.389188 140277229537024 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3502507507801056, loss=2.9615533351898193
I0206 22:31:31.388546 140277221144320 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3711163401603699, loss=2.9056038856506348
I0206 22:32:06.432194 140277229537024 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.35524433851242065, loss=2.8822062015533447
I0206 22:32:41.447042 140277221144320 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3604382276535034, loss=2.8340351581573486
I0206 22:33:16.461596 140277229537024 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.34235745668411255, loss=2.94623064994812
I0206 22:33:51.463565 140277221144320 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.35315051674842834, loss=2.9021639823913574
I0206 22:34:26.502485 140277229537024 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.35908856987953186, loss=2.906231641769409
I0206 22:35:01.572950 140277221144320 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.3385356366634369, loss=2.937594175338745
I0206 22:35:36.602889 140277229537024 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.35902804136276245, loss=2.8911032676696777
I0206 22:36:11.606748 140277221144320 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3684256672859192, loss=2.9021544456481934
I0206 22:36:46.640669 140277229537024 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3903410732746124, loss=2.9109721183776855
I0206 22:37:21.650495 140277221144320 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3890138864517212, loss=2.872645378112793
I0206 22:37:56.652161 140277229537024 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.3403102457523346, loss=2.860434055328369
I0206 22:38:31.665435 140277221144320 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.41131868958473206, loss=2.86657977104187
I0206 22:39:05.664074 140446903760704 spec.py:321] Evaluating on the training split.
I0206 22:39:08.666697 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:42:12.664940 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 22:42:15.373796 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:44:55.985673 140446903760704 spec.py:349] Evaluating on the test split.
I0206 22:44:58.682604 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 22:47:38.478144 140446903760704 submission_runner.py:408] Time since start: 34302.99s, 	Step: 59999, 	{'train/accuracy': 0.644768476486206, 'train/loss': 1.8109740018844604, 'train/bleu': 31.910825778247982, 'validation/accuracy': 0.6663773655891418, 'validation/loss': 1.6560256481170654, 'validation/bleu': 28.56648746349024, 'validation/num_examples': 3000, 'test/accuracy': 0.6790773272514343, 'test/loss': 1.5836652517318726, 'test/bleu': 28.489317356034956, 'test/num_examples': 3003, 'score': 21038.290306568146, 'total_duration': 34302.990511894226, 'accumulated_submission_time': 21038.290306568146, 'accumulated_eval_time': 13262.024190664291, 'accumulated_logging_time': 0.7604336738586426}
I0206 22:47:38.500976 140277229537024 logging_writer.py:48] [59999] accumulated_eval_time=13262.024191, accumulated_logging_time=0.760434, accumulated_submission_time=21038.290307, global_step=59999, preemption_count=0, score=21038.290307, test/accuracy=0.679077, test/bleu=28.489317, test/loss=1.583665, test/num_examples=3003, total_duration=34302.990512, train/accuracy=0.644768, train/bleu=31.910826, train/loss=1.810974, validation/accuracy=0.666377, validation/bleu=28.566487, validation/loss=1.656026, validation/num_examples=3000
I0206 22:47:39.212215 140277221144320 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.37713623046875, loss=2.9261913299560547
I0206 22:48:14.032432 140277229537024 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.34631428122520447, loss=2.917078733444214
I0206 22:48:48.953442 140277221144320 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.4254745543003082, loss=2.8624415397644043
I0206 22:49:23.914407 140277229537024 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.3632206618785858, loss=2.9094247817993164
I0206 22:49:58.874333 140277221144320 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.36848214268684387, loss=2.928764581680298
I0206 22:50:33.866273 140277229537024 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3735448718070984, loss=2.945878028869629
I0206 22:51:08.843896 140277221144320 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.35094043612480164, loss=2.913949489593506
I0206 22:51:43.826766 140277229537024 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.33922356367111206, loss=2.9162871837615967
I0206 22:52:18.817853 140277221144320 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.3384719491004944, loss=2.91375732421875
I0206 22:52:53.808550 140277229537024 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3717345595359802, loss=2.962829113006592
I0206 22:53:28.788294 140277221144320 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.36831730604171753, loss=2.9163577556610107
I0206 22:54:03.795655 140277229537024 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.40514111518859863, loss=2.8568990230560303
I0206 22:54:38.759997 140277221144320 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3915669023990631, loss=2.8975830078125
I0206 22:55:13.747694 140277229537024 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.36378610134124756, loss=2.940197229385376
I0206 22:55:48.764969 140277221144320 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.4235798716545105, loss=2.8946688175201416
I0206 22:56:23.732578 140277229537024 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3614666163921356, loss=2.889467239379883
I0206 22:56:58.705310 140277221144320 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.36189189553260803, loss=2.8580546379089355
I0206 22:57:33.695655 140277229537024 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3572099804878235, loss=2.8740227222442627
I0206 22:58:08.703815 140277221144320 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3536238372325897, loss=2.9375686645507812
I0206 22:58:43.692901 140277229537024 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.35209566354751587, loss=2.8914754390716553
I0206 22:59:18.723379 140277221144320 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.36807361245155334, loss=2.952709197998047
I0206 22:59:53.728534 140277229537024 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.48881033062934875, loss=2.8468499183654785
I0206 23:00:28.693728 140277221144320 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.35450491309165955, loss=2.9113359451293945
I0206 23:01:03.683849 140277229537024 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.41217562556266785, loss=2.82684063911438
I0206 23:01:38.650498 140277221144320 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.35678040981292725, loss=2.85066294670105
I0206 23:01:38.656420 140446903760704 spec.py:321] Evaluating on the training split.
I0206 23:01:41.378616 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:04:19.639183 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 23:04:22.342654 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:06:51.913300 140446903760704 spec.py:349] Evaluating on the test split.
I0206 23:06:54.625417 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:09:23.857983 140446903760704 submission_runner.py:408] Time since start: 35608.37s, 	Step: 62401, 	{'train/accuracy': 0.6491748094558716, 'train/loss': 1.7910131216049194, 'train/bleu': 32.01965766582803, 'validation/accuracy': 0.669477105140686, 'validation/loss': 1.6406055688858032, 'validation/bleu': 29.233560910622813, 'validation/num_examples': 3000, 'test/accuracy': 0.6824356913566589, 'test/loss': 1.5777497291564941, 'test/bleu': 28.67643381041361, 'test/num_examples': 3003, 'score': 21878.360268354416, 'total_duration': 35608.370332956314, 'accumulated_submission_time': 21878.360268354416, 'accumulated_eval_time': 13727.225650072098, 'accumulated_logging_time': 0.7934637069702148}
I0206 23:09:23.881970 140277229537024 logging_writer.py:48] [62401] accumulated_eval_time=13727.225650, accumulated_logging_time=0.793464, accumulated_submission_time=21878.360268, global_step=62401, preemption_count=0, score=21878.360268, test/accuracy=0.682436, test/bleu=28.676434, test/loss=1.577750, test/num_examples=3003, total_duration=35608.370333, train/accuracy=0.649175, train/bleu=32.019658, train/loss=1.791013, validation/accuracy=0.669477, validation/bleu=29.233561, validation/loss=1.640606, validation/num_examples=3000
I0206 23:09:58.739834 140277221144320 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.4121175706386566, loss=2.876077175140381
I0206 23:10:33.648060 140277229537024 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.36446309089660645, loss=2.886228322982788
I0206 23:11:08.618065 140277221144320 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3727363348007202, loss=2.8462367057800293
I0206 23:11:43.584596 140277229537024 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.37902772426605225, loss=2.9120795726776123
I0206 23:12:18.550177 140277221144320 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.36067020893096924, loss=2.8194456100463867
I0206 23:12:53.597455 140277229537024 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3744095265865326, loss=2.9468398094177246
I0206 23:13:28.607320 140277221144320 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.36223888397216797, loss=2.847329616546631
I0206 23:14:03.603247 140277229537024 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.4484734535217285, loss=2.884866952896118
I0206 23:14:38.599786 140277221144320 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.3611280024051666, loss=2.9208686351776123
I0206 23:15:13.544845 140277229537024 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3841783404350281, loss=2.985687732696533
I0206 23:15:48.527699 140277221144320 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.34090057015419006, loss=2.860149383544922
I0206 23:16:23.508359 140277229537024 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3676925301551819, loss=2.844733715057373
I0206 23:16:58.488298 140277221144320 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.37147706747055054, loss=2.84067440032959
I0206 23:17:33.500183 140277229537024 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.36875295639038086, loss=2.9861090183258057
I0206 23:18:08.553156 140277221144320 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3728925287723541, loss=2.8890380859375
I0206 23:18:43.516326 140277229537024 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.42813941836357117, loss=2.9144997596740723
I0206 23:19:18.492980 140277221144320 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3916175961494446, loss=2.9722609519958496
I0206 23:19:53.507830 140277229537024 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.35443708300590515, loss=2.944492816925049
I0206 23:20:28.511515 140277221144320 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3574509918689728, loss=2.8483006954193115
I0206 23:21:03.501482 140277229537024 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.37771978974342346, loss=2.919114828109741
I0206 23:21:38.496535 140277221144320 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.40439310669898987, loss=2.9168214797973633
I0206 23:22:13.485462 140277229537024 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.364762544631958, loss=2.8279168605804443
I0206 23:22:48.443238 140277221144320 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.43115437030792236, loss=2.8163657188415527
I0206 23:23:23.421129 140277229537024 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.38481810688972473, loss=2.8794820308685303
I0206 23:23:24.189105 140446903760704 spec.py:321] Evaluating on the training split.
I0206 23:23:27.199312 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:26:45.461446 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 23:26:48.159695 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:29:57.758487 140446903760704 spec.py:349] Evaluating on the test split.
I0206 23:30:00.469157 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:33:00.346541 140446903760704 submission_runner.py:408] Time since start: 37024.86s, 	Step: 64804, 	{'train/accuracy': 0.6520285606384277, 'train/loss': 1.7609772682189941, 'train/bleu': 31.971719271704508, 'validation/accuracy': 0.6713989973068237, 'validation/loss': 1.6386438608169556, 'validation/bleu': 29.0899984850159, 'validation/num_examples': 3000, 'test/accuracy': 0.6805996298789978, 'test/loss': 1.5786640644073486, 'test/bleu': 28.657661996239458, 'test/num_examples': 3003, 'score': 22718.57821083069, 'total_duration': 37024.85886883736, 'accumulated_submission_time': 22718.57821083069, 'accumulated_eval_time': 14303.382984161377, 'accumulated_logging_time': 0.8290464878082275}
I0206 23:33:00.369909 140277221144320 logging_writer.py:48] [64804] accumulated_eval_time=14303.382984, accumulated_logging_time=0.829046, accumulated_submission_time=22718.578211, global_step=64804, preemption_count=0, score=22718.578211, test/accuracy=0.680600, test/bleu=28.657662, test/loss=1.578664, test/num_examples=3003, total_duration=37024.858869, train/accuracy=0.652029, train/bleu=31.971719, train/loss=1.760977, validation/accuracy=0.671399, validation/bleu=29.089998, validation/loss=1.638644, validation/num_examples=3000
I0206 23:33:34.167769 140277229537024 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.42018625140190125, loss=2.8605406284332275
I0206 23:34:09.108970 140277221144320 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.376186341047287, loss=2.9193427562713623
I0206 23:34:44.211390 140277229537024 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.37203994393348694, loss=2.8787460327148438
I0206 23:35:19.202860 140277221144320 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.348954439163208, loss=2.9234459400177
I0206 23:35:54.183340 140277229537024 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.39204418659210205, loss=2.8884646892547607
I0206 23:36:29.146190 140277221144320 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3673020303249359, loss=2.934689998626709
I0206 23:37:04.125552 140277229537024 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.35200509428977966, loss=2.8435730934143066
I0206 23:37:39.082039 140277221144320 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.41986915469169617, loss=2.9067442417144775
I0206 23:38:14.051187 140277229537024 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.35613247752189636, loss=2.8786890506744385
I0206 23:38:49.018885 140277221144320 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.39280882477760315, loss=2.87196946144104
I0206 23:39:24.017194 140277229537024 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3796238899230957, loss=2.8592751026153564
I0206 23:39:59.097088 140277221144320 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.37283188104629517, loss=2.8875677585601807
I0206 23:40:34.104848 140277229537024 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.38379380106925964, loss=2.8610281944274902
I0206 23:41:09.084102 140277221144320 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3587189018726349, loss=2.8566858768463135
I0206 23:41:44.021751 140277229537024 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.4306998550891876, loss=2.796461582183838
I0206 23:42:19.004966 140277221144320 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.35696151852607727, loss=2.82405686378479
I0206 23:42:53.969805 140277229537024 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.407172828912735, loss=2.905832529067993
I0206 23:43:28.928340 140277221144320 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.36975961923599243, loss=2.8385138511657715
I0206 23:44:03.890518 140277229537024 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3779642581939697, loss=2.854968786239624
I0206 23:44:38.852320 140277221144320 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3549588620662689, loss=2.7954654693603516
I0206 23:45:13.829479 140277229537024 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.36011379957199097, loss=2.8913769721984863
I0206 23:45:48.907384 140277221144320 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.373306542634964, loss=2.9084041118621826
I0206 23:46:23.934540 140277229537024 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3530785143375397, loss=2.7952232360839844
I0206 23:46:58.956526 140277221144320 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.36348411440849304, loss=2.8339946269989014
I0206 23:47:00.436215 140446903760704 spec.py:321] Evaluating on the training split.
I0206 23:47:03.476294 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:49:46.987186 140446903760704 spec.py:333] Evaluating on the validation split.
I0206 23:49:49.697167 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:52:19.171877 140446903760704 spec.py:349] Evaluating on the test split.
I0206 23:52:21.883508 140446903760704 workload.py:181] Translating evaluation dataset.
I0206 23:55:05.714682 140446903760704 submission_runner.py:408] Time since start: 38350.23s, 	Step: 67206, 	{'train/accuracy': 0.6519597768783569, 'train/loss': 1.7757725715637207, 'train/bleu': 31.91697570224685, 'validation/accuracy': 0.670419454574585, 'validation/loss': 1.638580083847046, 'validation/bleu': 29.23616891361002, 'validation/num_examples': 3000, 'test/accuracy': 0.6822613477706909, 'test/loss': 1.5671225786209106, 'test/bleu': 28.72514626657771, 'test/num_examples': 3003, 'score': 23558.55596637726, 'total_duration': 38350.22704553604, 'accumulated_submission_time': 23558.55596637726, 'accumulated_eval_time': 14788.661399126053, 'accumulated_logging_time': 0.8624668121337891}
I0206 23:55:05.738937 140277229537024 logging_writer.py:48] [67206] accumulated_eval_time=14788.661399, accumulated_logging_time=0.862467, accumulated_submission_time=23558.555966, global_step=67206, preemption_count=0, score=23558.555966, test/accuracy=0.682261, test/bleu=28.725146, test/loss=1.567123, test/num_examples=3003, total_duration=38350.227046, train/accuracy=0.651960, train/bleu=31.916976, train/loss=1.775773, validation/accuracy=0.670419, validation/bleu=29.236169, validation/loss=1.638580, validation/num_examples=3000
I0206 23:55:38.814067 140277221144320 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.4080074429512024, loss=2.8526389598846436
I0206 23:56:13.742896 140277229537024 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3622252941131592, loss=2.9234447479248047
I0206 23:56:48.714057 140277221144320 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3829619586467743, loss=2.8469300270080566
I0206 23:57:23.685457 140277229537024 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.37866196036338806, loss=2.8787667751312256
I0206 23:57:58.633787 140277221144320 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.4033782184123993, loss=2.939777374267578
I0206 23:58:33.648535 140277229537024 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3802037239074707, loss=2.910444974899292
I0206 23:59:08.625499 140277221144320 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.369548499584198, loss=2.840172529220581
I0206 23:59:43.624658 140277229537024 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3669757544994354, loss=2.801948070526123
I0207 00:00:18.653289 140277221144320 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.42137613892555237, loss=2.9182169437408447
I0207 00:00:53.664326 140277229537024 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.4024837911128998, loss=2.919332981109619
I0207 00:01:28.635499 140277221144320 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.37411120533943176, loss=2.8410181999206543
I0207 00:02:03.635731 140277229537024 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3812651038169861, loss=2.903903007507324
I0207 00:02:38.624679 140277221144320 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3795936405658722, loss=2.859926462173462
I0207 00:03:13.602000 140277229537024 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.37475642561912537, loss=2.869241714477539
I0207 00:03:48.568021 140277221144320 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.39343351125717163, loss=2.914909839630127
I0207 00:04:23.560060 140277229537024 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.37125056982040405, loss=2.944319248199463
I0207 00:04:58.553336 140277221144320 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.35630518198013306, loss=2.781379461288452
I0207 00:05:33.535667 140277229537024 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.4437898099422455, loss=2.9059553146362305
I0207 00:06:08.489349 140277221144320 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.38340428471565247, loss=2.880622625350952
I0207 00:06:43.495312 140277229537024 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.371274471282959, loss=2.8696436882019043
I0207 00:07:18.501835 140277221144320 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3777364194393158, loss=2.857740640640259
I0207 00:07:53.475488 140277229537024 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.38308870792388916, loss=2.814523220062256
I0207 00:08:28.496112 140277221144320 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.34612613916397095, loss=2.857712984085083
I0207 00:09:03.477524 140277229537024 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3602449297904968, loss=2.808431625366211
I0207 00:09:05.999987 140446903760704 spec.py:321] Evaluating on the training split.
I0207 00:09:09.007627 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:12:10.709721 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 00:12:13.417279 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:14:52.868858 140446903760704 spec.py:349] Evaluating on the test split.
I0207 00:14:55.580917 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:17:24.308153 140446903760704 submission_runner.py:408] Time since start: 39688.82s, 	Step: 69609, 	{'train/accuracy': 0.6634093523025513, 'train/loss': 1.6923391819000244, 'train/bleu': 32.762389538704355, 'validation/accuracy': 0.6716221570968628, 'validation/loss': 1.6273186206817627, 'validation/bleu': 29.347775662744024, 'validation/num_examples': 3000, 'test/accuracy': 0.6841090321540833, 'test/loss': 1.5585025548934937, 'test/bleu': 29.071967186401448, 'test/num_examples': 3003, 'score': 24398.73073887825, 'total_duration': 39688.82049059868, 'accumulated_submission_time': 24398.73073887825, 'accumulated_eval_time': 15286.969474315643, 'accumulated_logging_time': 0.8974158763885498}
I0207 00:17:24.332282 140277221144320 logging_writer.py:48] [69609] accumulated_eval_time=15286.969474, accumulated_logging_time=0.897416, accumulated_submission_time=24398.730739, global_step=69609, preemption_count=0, score=24398.730739, test/accuracy=0.684109, test/bleu=29.071967, test/loss=1.558503, test/num_examples=3003, total_duration=39688.820491, train/accuracy=0.663409, train/bleu=32.762390, train/loss=1.692339, validation/accuracy=0.671622, validation/bleu=29.347776, validation/loss=1.627319, validation/num_examples=3000
I0207 00:17:56.405101 140277229537024 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3605092167854309, loss=2.8567793369293213
I0207 00:18:31.299871 140277221144320 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.44470492005348206, loss=2.871262550354004
I0207 00:19:06.263053 140277229537024 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.3656522333621979, loss=2.900329828262329
I0207 00:19:41.253701 140277221144320 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3660956919193268, loss=2.8126235008239746
I0207 00:20:16.224593 140277229537024 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3838713765144348, loss=2.8335185050964355
I0207 00:20:51.205580 140277221144320 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.41301968693733215, loss=2.8654401302337646
I0207 00:21:26.182526 140277229537024 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.41617870330810547, loss=2.8527443408966064
I0207 00:22:01.154668 140277221144320 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.36983656883239746, loss=2.853660821914673
I0207 00:22:36.135159 140277229537024 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.37767913937568665, loss=2.8104355335235596
I0207 00:23:11.134029 140277221144320 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.413759708404541, loss=2.8811330795288086
I0207 00:23:46.109225 140277229537024 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3969799280166626, loss=2.8962104320526123
I0207 00:24:21.101401 140277221144320 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.38718506693840027, loss=2.8412346839904785
I0207 00:24:56.087562 140277229537024 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3834226429462433, loss=2.956925868988037
I0207 00:25:31.068958 140277221144320 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.36970055103302, loss=2.8728880882263184
I0207 00:26:06.059668 140277229537024 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.37862494587898254, loss=2.809443712234497
I0207 00:26:41.050238 140277221144320 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.3866683840751648, loss=2.86633563041687
I0207 00:27:16.009516 140277229537024 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.41526806354522705, loss=2.913196563720703
I0207 00:27:50.991947 140277221144320 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.38344699144363403, loss=2.810555934906006
I0207 00:28:25.985168 140277229537024 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3877315819263458, loss=2.8124942779541016
I0207 00:29:01.013029 140277221144320 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.41876092553138733, loss=2.8476688861846924
I0207 00:29:36.024963 140277229537024 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.36266401410102844, loss=2.889362096786499
I0207 00:30:11.027670 140277221144320 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.3467566967010498, loss=2.943155288696289
I0207 00:30:46.007086 140277229537024 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.35823026299476624, loss=2.8095433712005615
I0207 00:31:20.994326 140277221144320 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3789634108543396, loss=2.8846380710601807
I0207 00:31:24.550983 140446903760704 spec.py:321] Evaluating on the training split.
I0207 00:31:27.556674 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:34:38.868331 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 00:34:41.572286 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:37:10.635857 140446903760704 spec.py:349] Evaluating on the test split.
I0207 00:37:13.342735 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:39:39.423316 140446903760704 submission_runner.py:408] Time since start: 41023.94s, 	Step: 72012, 	{'train/accuracy': 0.6599642634391785, 'train/loss': 1.7183440923690796, 'train/bleu': 32.44681045615368, 'validation/accuracy': 0.6714857816696167, 'validation/loss': 1.6171070337295532, 'validation/bleu': 29.12857090191241, 'validation/num_examples': 3000, 'test/accuracy': 0.6856196522712708, 'test/loss': 1.5455368757247925, 'test/bleu': 28.909133725017643, 'test/num_examples': 3003, 'score': 25238.861780643463, 'total_duration': 41023.935676813126, 'accumulated_submission_time': 25238.861780643463, 'accumulated_eval_time': 15781.841740846634, 'accumulated_logging_time': 0.9326050281524658}
I0207 00:39:39.447757 140277229537024 logging_writer.py:48] [72012] accumulated_eval_time=15781.841741, accumulated_logging_time=0.932605, accumulated_submission_time=25238.861781, global_step=72012, preemption_count=0, score=25238.861781, test/accuracy=0.685620, test/bleu=28.909134, test/loss=1.545537, test/num_examples=3003, total_duration=41023.935677, train/accuracy=0.659964, train/bleu=32.446810, train/loss=1.718344, validation/accuracy=0.671486, validation/bleu=29.128571, validation/loss=1.617107, validation/num_examples=3000
I0207 00:40:10.428787 140277221144320 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3679782450199127, loss=2.87626051902771
I0207 00:40:45.353726 140277229537024 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.35211700201034546, loss=2.8351197242736816
I0207 00:41:20.338973 140277221144320 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.38330715894699097, loss=2.824922800064087
I0207 00:41:55.292574 140277229537024 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3540702164173126, loss=2.8283567428588867
I0207 00:42:30.279072 140277221144320 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3849950134754181, loss=2.8799140453338623
I0207 00:43:05.262486 140277229537024 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3571202754974365, loss=2.7329294681549072
I0207 00:43:40.247312 140277221144320 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.36439868807792664, loss=2.8914639949798584
I0207 00:44:15.253185 140277229537024 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.4126451909542084, loss=2.8426318168640137
I0207 00:44:50.347777 140277221144320 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.40285027027130127, loss=2.8579940795898438
I0207 00:45:25.374293 140277229537024 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3921104669570923, loss=2.8490593433380127
I0207 00:46:00.375737 140277221144320 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3779543936252594, loss=2.8126108646392822
I0207 00:46:35.355029 140277229537024 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3931421935558319, loss=2.860168933868408
I0207 00:47:10.345758 140277221144320 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.38513699173927307, loss=2.799855947494507
I0207 00:47:45.320998 140277229537024 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.41325461864471436, loss=2.864978075027466
I0207 00:48:20.315731 140277221144320 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.38100340962409973, loss=2.8204283714294434
I0207 00:48:55.322859 140277229537024 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.37754514813423157, loss=2.829030990600586
I0207 00:49:30.289918 140277221144320 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.39385831356048584, loss=2.867504596710205
I0207 00:50:05.270525 140277229537024 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.38477954268455505, loss=2.8300085067749023
I0207 00:50:40.270062 140277221144320 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.371357262134552, loss=2.86076021194458
I0207 00:51:15.242737 140277229537024 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.40215399861335754, loss=2.832571268081665
I0207 00:51:50.228915 140277221144320 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.4069019556045532, loss=2.8345203399658203
I0207 00:52:25.209248 140277229537024 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3449088931083679, loss=2.86932635307312
I0207 00:53:00.171671 140277221144320 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3756590187549591, loss=2.813124179840088
I0207 00:53:35.154896 140277229537024 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3930473029613495, loss=2.91880464553833
I0207 00:53:39.771227 140446903760704 spec.py:321] Evaluating on the training split.
I0207 00:53:42.786517 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:57:17.841265 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 00:57:20.571382 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 00:59:53.166201 140446903760704 spec.py:349] Evaluating on the test split.
I0207 00:59:55.906109 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 01:02:24.801236 140446903760704 submission_runner.py:408] Time since start: 42389.31s, 	Step: 74415, 	{'train/accuracy': 0.6568201780319214, 'train/loss': 1.7489362955093384, 'train/bleu': 32.56595782076231, 'validation/accuracy': 0.6759618520736694, 'validation/loss': 1.6061208248138428, 'validation/bleu': 29.63801741455089, 'validation/num_examples': 3000, 'test/accuracy': 0.6896287202835083, 'test/loss': 1.5347216129302979, 'test/bleu': 29.456325345785103, 'test/num_examples': 3003, 'score': 26079.096412420273, 'total_duration': 42389.313524484634, 'accumulated_submission_time': 26079.096412420273, 'accumulated_eval_time': 16306.871610403061, 'accumulated_logging_time': 0.9680724143981934}
I0207 01:02:24.831839 140277221144320 logging_writer.py:48] [74415] accumulated_eval_time=16306.871610, accumulated_logging_time=0.968072, accumulated_submission_time=26079.096412, global_step=74415, preemption_count=0, score=26079.096412, test/accuracy=0.689629, test/bleu=29.456325, test/loss=1.534722, test/num_examples=3003, total_duration=42389.313524, train/accuracy=0.656820, train/bleu=32.565958, train/loss=1.748936, validation/accuracy=0.675962, validation/bleu=29.638017, validation/loss=1.606121, validation/num_examples=3000
I0207 01:02:54.806879 140277229537024 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3793632686138153, loss=2.8720152378082275
I0207 01:03:29.718432 140277221144320 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.35641011595726013, loss=2.859595775604248
I0207 01:04:04.695848 140277229537024 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.4185909628868103, loss=2.8811094760894775
I0207 01:04:39.641664 140277221144320 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.40289512276649475, loss=2.8157713413238525
I0207 01:05:14.636178 140277229537024 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.39894095063209534, loss=2.7611212730407715
I0207 01:05:49.613828 140277221144320 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3905516564846039, loss=2.834991931915283
I0207 01:06:24.622258 140277229537024 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3752062916755676, loss=2.82200288772583
I0207 01:06:59.632954 140277221144320 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3940633237361908, loss=2.8852343559265137
I0207 01:07:34.621185 140277229537024 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.39307349920272827, loss=2.791623115539551
I0207 01:08:09.626413 140277221144320 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.35863929986953735, loss=2.736632823944092
I0207 01:08:44.620979 140277229537024 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3937472701072693, loss=2.7775018215179443
I0207 01:09:19.618411 140277221144320 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.3794093132019043, loss=2.881174325942993
I0207 01:09:54.628113 140277229537024 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3860946297645569, loss=2.797563314437866
I0207 01:10:29.634538 140277221144320 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.4132678806781769, loss=2.8338663578033447
I0207 01:11:04.624416 140277229537024 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.39487224817276, loss=2.864488363265991
I0207 01:11:39.626108 140277221144320 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.39395856857299805, loss=2.8537864685058594
I0207 01:12:14.634659 140277229537024 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.4805106818675995, loss=2.859614610671997
I0207 01:12:49.622843 140277221144320 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.38712698221206665, loss=2.855268716812134
I0207 01:13:24.600368 140277229537024 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.39477208256721497, loss=2.909064531326294
I0207 01:13:59.581025 140277221144320 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.38190826773643494, loss=2.9003679752349854
I0207 01:14:34.620099 140277229537024 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.39534643292427063, loss=2.8495991230010986
I0207 01:15:09.621040 140277221144320 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.4155343472957611, loss=2.8383991718292236
I0207 01:15:44.600786 140277229537024 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.37611639499664307, loss=2.70672345161438
I0207 01:16:19.578714 140277221144320 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.38069766759872437, loss=2.8568625450134277
I0207 01:16:24.901919 140446903760704 spec.py:321] Evaluating on the training split.
I0207 01:16:27.911283 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 01:19:14.394651 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 01:19:17.113933 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 01:21:41.944727 140446903760704 spec.py:349] Evaluating on the test split.
I0207 01:21:44.641755 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 01:24:22.997198 140446903760704 submission_runner.py:408] Time since start: 43707.51s, 	Step: 76817, 	{'train/accuracy': 0.661069929599762, 'train/loss': 1.7087233066558838, 'train/bleu': 32.60740521012439, 'validation/accuracy': 0.6762098073959351, 'validation/loss': 1.603691577911377, 'validation/bleu': 29.3602712239802, 'validation/num_examples': 3000, 'test/accuracy': 0.6898146867752075, 'test/loss': 1.5289326906204224, 'test/bleu': 29.37583258832015, 'test/num_examples': 3003, 'score': 26919.076508283615, 'total_duration': 43707.509570360184, 'accumulated_submission_time': 26919.076508283615, 'accumulated_eval_time': 16784.96683216095, 'accumulated_logging_time': 1.0114808082580566}
I0207 01:24:23.022815 140277229537024 logging_writer.py:48] [76817] accumulated_eval_time=16784.966832, accumulated_logging_time=1.011481, accumulated_submission_time=26919.076508, global_step=76817, preemption_count=0, score=26919.076508, test/accuracy=0.689815, test/bleu=29.375833, test/loss=1.528933, test/num_examples=3003, total_duration=43707.509570, train/accuracy=0.661070, train/bleu=32.607405, train/loss=1.708723, validation/accuracy=0.676210, validation/bleu=29.360271, validation/loss=1.603692, validation/num_examples=3000
I0207 01:24:52.285292 140277221144320 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.37276068329811096, loss=2.8652560710906982
I0207 01:25:27.256421 140277229537024 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.4368114471435547, loss=2.888127326965332
I0207 01:26:02.232200 140277221144320 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.39037275314331055, loss=2.837555170059204
I0207 01:26:37.192081 140277229537024 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.40291208028793335, loss=2.8621275424957275
I0207 01:27:12.160970 140277221144320 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.3883308470249176, loss=2.8895535469055176
I0207 01:27:47.121563 140277229537024 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.40276315808296204, loss=2.8675262928009033
I0207 01:28:22.084411 140277221144320 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.4143487215042114, loss=2.8225271701812744
I0207 01:28:57.024380 140277229537024 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.3821832239627838, loss=2.79240345954895
I0207 01:29:31.997718 140277221144320 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.37443292140960693, loss=2.804818868637085
I0207 01:30:06.972828 140277229537024 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3950103521347046, loss=2.7902143001556396
I0207 01:30:41.949713 140277221144320 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.3974877893924713, loss=2.9080681800842285
I0207 01:31:16.936833 140277229537024 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.40935254096984863, loss=2.8464972972869873
I0207 01:31:51.937773 140277221144320 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3970421552658081, loss=2.8041598796844482
I0207 01:32:26.979783 140277229537024 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.3755757510662079, loss=2.8313512802124023
I0207 01:33:01.998732 140277221144320 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.37265339493751526, loss=2.834801197052002
I0207 01:33:37.011662 140277229537024 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.4090016484260559, loss=2.7757060527801514
I0207 01:34:11.991255 140277221144320 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.4203958213329315, loss=2.7994773387908936
I0207 01:34:46.974544 140277229537024 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3939000070095062, loss=2.906858205795288
I0207 01:35:21.948015 140277221144320 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4013885259628296, loss=2.8688290119171143
I0207 01:35:57.055857 140277229537024 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.4140121340751648, loss=2.8395421504974365
I0207 01:36:32.053075 140277221144320 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.4125140905380249, loss=2.827562093734741
I0207 01:37:07.042616 140277229537024 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.4127027094364166, loss=2.8786425590515137
I0207 01:37:42.023307 140277221144320 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.39584699273109436, loss=2.815606117248535
I0207 01:38:17.028439 140277229537024 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.36425068974494934, loss=2.80934476852417
I0207 01:38:23.039755 140446903760704 spec.py:321] Evaluating on the training split.
I0207 01:38:26.046128 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 01:41:12.310599 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 01:41:15.017610 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 01:43:40.804783 140446903760704 spec.py:349] Evaluating on the test split.
I0207 01:43:43.512424 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 01:46:13.819207 140446903760704 submission_runner.py:408] Time since start: 45018.33s, 	Step: 79219, 	{'train/accuracy': 0.6592926383018494, 'train/loss': 1.7247998714447021, 'train/bleu': 32.536791630899906, 'validation/accuracy': 0.6757262945175171, 'validation/loss': 1.6033855676651, 'validation/bleu': 29.62497735225051, 'validation/num_examples': 3000, 'test/accuracy': 0.6900470852851868, 'test/loss': 1.5301181077957153, 'test/bleu': 29.416101277503813, 'test/num_examples': 3003, 'score': 27759.00503540039, 'total_duration': 45018.331510305405, 'accumulated_submission_time': 27759.00503540039, 'accumulated_eval_time': 17255.74615764618, 'accumulated_logging_time': 1.0472896099090576}
I0207 01:46:13.844212 140277221144320 logging_writer.py:48] [79219] accumulated_eval_time=17255.746158, accumulated_logging_time=1.047290, accumulated_submission_time=27759.005035, global_step=79219, preemption_count=0, score=27759.005035, test/accuracy=0.690047, test/bleu=29.416101, test/loss=1.530118, test/num_examples=3003, total_duration=45018.331510, train/accuracy=0.659293, train/bleu=32.536792, train/loss=1.724800, validation/accuracy=0.675726, validation/bleu=29.624977, validation/loss=1.603386, validation/num_examples=3000
I0207 01:46:42.411253 140277229537024 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.38443586230278015, loss=2.832496166229248
I0207 01:47:17.280850 140277221144320 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.4139483571052551, loss=2.815664052963257
I0207 01:47:52.252579 140277229537024 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.40273338556289673, loss=2.7736294269561768
I0207 01:48:27.244854 140277221144320 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.38358017802238464, loss=2.815584421157837
I0207 01:49:02.271896 140277229537024 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.39862024784088135, loss=2.8716747760772705
I0207 01:49:37.280789 140277221144320 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3958694338798523, loss=2.829510450363159
I0207 01:50:12.286366 140277229537024 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.41900157928466797, loss=2.847818374633789
I0207 01:50:47.280280 140277221144320 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.40871554613113403, loss=2.7759170532226562
I0207 01:51:22.255517 140277229537024 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.40062597393989563, loss=2.8631772994995117
I0207 01:51:57.244397 140277221144320 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.38116466999053955, loss=2.825519323348999
I0207 01:52:32.223698 140277229537024 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.42663758993148804, loss=2.8545048236846924
I0207 01:53:07.245313 140277221144320 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.4040517210960388, loss=2.8329882621765137
I0207 01:53:42.245352 140277229537024 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.383849561214447, loss=2.8241260051727295
I0207 01:54:17.206838 140277221144320 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.38576453924179077, loss=2.843872547149658
I0207 01:54:52.163711 140277229537024 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.40063345432281494, loss=2.846625804901123
I0207 01:55:27.171515 140277221144320 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.38474854826927185, loss=2.787245988845825
I0207 01:56:02.138125 140277229537024 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.394870787858963, loss=2.8836278915405273
I0207 01:56:37.140805 140277221144320 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.397134006023407, loss=2.8472139835357666
I0207 01:57:12.128562 140277229537024 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.39357292652130127, loss=2.780520439147949
I0207 01:57:47.153742 140277221144320 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.39201706647872925, loss=2.7757885456085205
I0207 01:58:22.160915 140277229537024 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.4377478361129761, loss=2.739999532699585
I0207 01:58:57.128961 140277221144320 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.39109155535697937, loss=2.9476284980773926
I0207 01:59:32.108278 140277229537024 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.3865183889865875, loss=2.787322998046875
I0207 02:00:07.085894 140277221144320 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3954295516014099, loss=2.862794876098633
I0207 02:00:14.134926 140446903760704 spec.py:321] Evaluating on the training split.
I0207 02:00:17.138096 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:03:33.035897 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 02:03:35.744313 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:06:50.922535 140446903760704 spec.py:349] Evaluating on the test split.
I0207 02:06:53.618869 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:09:43.656508 140446903760704 submission_runner.py:408] Time since start: 46428.17s, 	Step: 81622, 	{'train/accuracy': 0.6859181523323059, 'train/loss': 1.5658107995986938, 'train/bleu': 34.2604911187917, 'validation/accuracy': 0.6783672571182251, 'validation/loss': 1.5894643068313599, 'validation/bleu': 29.177389345403792, 'validation/num_examples': 3000, 'test/accuracy': 0.6895938515663147, 'test/loss': 1.5182905197143555, 'test/bleu': 29.071133285384303, 'test/num_examples': 3003, 'score': 28599.207001686096, 'total_duration': 46428.168867349625, 'accumulated_submission_time': 28599.207001686096, 'accumulated_eval_time': 17825.267672777176, 'accumulated_logging_time': 1.0836036205291748}
I0207 02:09:43.682856 140277229537024 logging_writer.py:48] [81622] accumulated_eval_time=17825.267673, accumulated_logging_time=1.083604, accumulated_submission_time=28599.207002, global_step=81622, preemption_count=0, score=28599.207002, test/accuracy=0.689594, test/bleu=29.071133, test/loss=1.518291, test/num_examples=3003, total_duration=46428.168867, train/accuracy=0.685918, train/bleu=34.260491, train/loss=1.565811, validation/accuracy=0.678367, validation/bleu=29.177389, validation/loss=1.589464, validation/num_examples=3000
I0207 02:10:11.188399 140277221144320 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.41977107524871826, loss=2.795591115951538
I0207 02:10:46.071844 140277229537024 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.39184728264808655, loss=2.854391098022461
I0207 02:11:21.065438 140277221144320 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.4028396010398865, loss=2.8145134449005127
I0207 02:11:56.064969 140277229537024 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4127318561077118, loss=2.7834410667419434
I0207 02:12:31.037389 140277221144320 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3967801630496979, loss=2.8039190769195557
I0207 02:13:06.014324 140277229537024 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.43331241607666016, loss=2.7731425762176514
I0207 02:13:40.999603 140277221144320 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.4343233108520508, loss=2.875748872756958
I0207 02:14:15.990282 140277229537024 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.38130712509155273, loss=2.807586669921875
I0207 02:14:50.965783 140277221144320 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.39515188336372375, loss=2.7786312103271484
I0207 02:15:25.954238 140277229537024 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.4016653895378113, loss=2.794520139694214
I0207 02:16:00.969626 140277221144320 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.4065222442150116, loss=2.810584545135498
I0207 02:16:35.971360 140277229537024 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.4477458596229553, loss=2.770756721496582
I0207 02:17:11.000162 140277221144320 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.3879810571670532, loss=2.8195242881774902
I0207 02:17:45.989930 140277229537024 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.40547651052474976, loss=2.76411771774292
I0207 02:18:20.966046 140277221144320 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.42649590969085693, loss=2.776887893676758
I0207 02:18:55.938251 140277229537024 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.4269472658634186, loss=2.804316997528076
I0207 02:19:30.918482 140277221144320 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.4020751416683197, loss=2.775402069091797
I0207 02:20:05.921566 140277229537024 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.44176048040390015, loss=2.796369791030884
I0207 02:20:40.880149 140277221144320 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.4170560836791992, loss=2.7970027923583984
I0207 02:21:15.865298 140277229537024 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.43025076389312744, loss=2.8208858966827393
I0207 02:21:50.861687 140277221144320 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.4441404938697815, loss=2.858898639678955
I0207 02:22:25.846770 140277229537024 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.40705665946006775, loss=2.870598316192627
I0207 02:23:00.850743 140277221144320 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.3959129750728607, loss=2.8465170860290527
I0207 02:23:35.874242 140277229537024 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.4118344485759735, loss=2.8012702465057373
I0207 02:23:44.001656 140446903760704 spec.py:321] Evaluating on the training split.
I0207 02:23:47.019635 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:26:51.863002 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 02:26:54.583620 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:29:33.503883 140446903760704 spec.py:349] Evaluating on the test split.
I0207 02:29:36.225026 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:32:03.164989 140446903760704 submission_runner.py:408] Time since start: 47767.68s, 	Step: 84025, 	{'train/accuracy': 0.664563000202179, 'train/loss': 1.696059226989746, 'train/bleu': 33.119290385594596, 'validation/accuracy': 0.6789996027946472, 'validation/loss': 1.582720160484314, 'validation/bleu': 29.565914486020333, 'validation/num_examples': 3000, 'test/accuracy': 0.6921852231025696, 'test/loss': 1.5084973573684692, 'test/bleu': 29.47494867004409, 'test/num_examples': 3003, 'score': 29439.43800854683, 'total_duration': 47767.677359580994, 'accumulated_submission_time': 29439.43800854683, 'accumulated_eval_time': 18324.430958986282, 'accumulated_logging_time': 1.1216096878051758}
I0207 02:32:03.192511 140277221144320 logging_writer.py:48] [84025] accumulated_eval_time=18324.430959, accumulated_logging_time=1.121610, accumulated_submission_time=29439.438009, global_step=84025, preemption_count=0, score=29439.438009, test/accuracy=0.692185, test/bleu=29.474949, test/loss=1.508497, test/num_examples=3003, total_duration=47767.677360, train/accuracy=0.664563, train/bleu=33.119290, train/loss=1.696059, validation/accuracy=0.679000, validation/bleu=29.565914, validation/loss=1.582720, validation/num_examples=3000
I0207 02:32:29.694494 140277229537024 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.4439016580581665, loss=2.810680389404297
I0207 02:33:04.603391 140277221144320 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.43011727929115295, loss=2.8201396465301514
I0207 02:33:39.552539 140277229537024 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.40642616152763367, loss=2.868513345718384
I0207 02:34:14.503504 140277221144320 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4280369281768799, loss=2.847126007080078
I0207 02:34:49.458844 140277229537024 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.41297802329063416, loss=2.861915111541748
I0207 02:35:24.450426 140277221144320 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.40086397528648376, loss=2.817138433456421
I0207 02:35:59.443023 140277229537024 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.39930543303489685, loss=2.756889581680298
I0207 02:36:34.438880 140277221144320 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.4489385485649109, loss=2.7980093955993652
I0207 02:37:09.444104 140277229537024 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4017055928707123, loss=2.8021812438964844
I0207 02:37:44.451302 140277221144320 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.4295746386051178, loss=2.8265419006347656
I0207 02:38:19.508582 140277229537024 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.42567333579063416, loss=2.8019707202911377
I0207 02:38:54.508542 140277221144320 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.4098253548145294, loss=2.806807279586792
I0207 02:39:29.479910 140277229537024 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.4074695110321045, loss=2.878065824508667
I0207 02:40:04.453295 140277221144320 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.42339611053466797, loss=2.8160176277160645
I0207 02:40:39.418066 140277229537024 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.42603084444999695, loss=2.881572723388672
I0207 02:41:14.403033 140277221144320 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.4287005662918091, loss=2.847668170928955
I0207 02:41:49.505178 140277229537024 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.4314001202583313, loss=2.8170931339263916
I0207 02:42:24.534011 140277221144320 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.4575309455394745, loss=2.8074023723602295
I0207 02:42:59.516205 140277229537024 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.40526482462882996, loss=2.8276424407958984
I0207 02:43:34.530840 140277221144320 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.45461422204971313, loss=2.858211040496826
I0207 02:44:09.504750 140277229537024 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.4472672939300537, loss=2.863858699798584
I0207 02:44:44.467405 140277221144320 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.456594854593277, loss=2.791219711303711
I0207 02:45:19.463900 140277229537024 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.41329434514045715, loss=2.8178632259368896
I0207 02:45:54.463617 140277221144320 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4304009675979614, loss=2.825685501098633
I0207 02:46:03.278044 140446903760704 spec.py:321] Evaluating on the training split.
I0207 02:46:06.292077 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:49:15.515861 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 02:49:18.237572 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:52:11.279647 140446903760704 spec.py:349] Evaluating on the test split.
I0207 02:52:13.986367 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 02:54:45.119639 140446903760704 submission_runner.py:408] Time since start: 49129.63s, 	Step: 86427, 	{'train/accuracy': 0.6629806756973267, 'train/loss': 1.7091134786605835, 'train/bleu': 32.61050182170082, 'validation/accuracy': 0.6798179745674133, 'validation/loss': 1.5794235467910767, 'validation/bleu': 29.637436093481103, 'validation/num_examples': 3000, 'test/accuracy': 0.6930103302001953, 'test/loss': 1.5063138008117676, 'test/bleu': 29.577308672706145, 'test/num_examples': 3003, 'score': 30279.434716939926, 'total_duration': 49129.6320040226, 'accumulated_submission_time': 30279.434716939926, 'accumulated_eval_time': 18846.272495031357, 'accumulated_logging_time': 1.1595826148986816}
I0207 02:54:45.145394 140277229537024 logging_writer.py:48] [86427] accumulated_eval_time=18846.272495, accumulated_logging_time=1.159583, accumulated_submission_time=30279.434717, global_step=86427, preemption_count=0, score=30279.434717, test/accuracy=0.693010, test/bleu=29.577309, test/loss=1.506314, test/num_examples=3003, total_duration=49129.632004, train/accuracy=0.662981, train/bleu=32.610502, train/loss=1.709113, validation/accuracy=0.679818, validation/bleu=29.637436, validation/loss=1.579424, validation/num_examples=3000
I0207 02:55:10.928518 140277221144320 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.41773149371147156, loss=2.8335628509521484
I0207 02:55:45.793853 140277229537024 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4350835084915161, loss=2.8415656089782715
I0207 02:56:20.774450 140277221144320 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.4497818052768707, loss=2.8156609535217285
I0207 02:56:55.760327 140277229537024 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.44733211398124695, loss=2.7514591217041016
I0207 02:57:30.742958 140277221144320 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4272473156452179, loss=2.7803447246551514
I0207 02:58:05.722121 140277229537024 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.44726788997650146, loss=2.764066219329834
I0207 02:58:40.726935 140277221144320 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4376947283744812, loss=2.8223671913146973
I0207 02:59:15.704087 140277229537024 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.4258434772491455, loss=2.7694666385650635
I0207 02:59:50.690743 140277221144320 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4141824245452881, loss=2.7668278217315674
I0207 03:00:25.671175 140277229537024 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.42820629477500916, loss=2.7816972732543945
I0207 03:01:00.644800 140277221144320 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.431627094745636, loss=2.7306439876556396
I0207 03:01:35.647383 140277229537024 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.45421814918518066, loss=2.8192172050476074
I0207 03:02:10.598534 140277221144320 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.4073742628097534, loss=2.771639585494995
I0207 03:02:45.591578 140277229537024 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.4122420847415924, loss=2.7332608699798584
I0207 03:03:20.593437 140277221144320 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.4420040547847748, loss=2.801888942718506
I0207 03:03:55.595117 140277229537024 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.41390883922576904, loss=2.790487289428711
I0207 03:04:30.562471 140277221144320 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.4378626048564911, loss=2.8139657974243164
I0207 03:05:05.544015 140277229537024 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.4313521385192871, loss=2.812933921813965
I0207 03:05:40.543845 140277221144320 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.421111136674881, loss=2.7883100509643555
I0207 03:06:15.536676 140277229537024 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4195963144302368, loss=2.761444330215454
I0207 03:06:50.583692 140277221144320 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.42955371737480164, loss=2.765857458114624
I0207 03:07:25.628948 140277229537024 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.42056670784950256, loss=2.8294618129730225
I0207 03:08:00.721839 140277221144320 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.45886215567588806, loss=2.836263418197632
I0207 03:08:35.700562 140277229537024 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.42337682843208313, loss=2.7964162826538086
I0207 03:08:45.205708 140446903760704 spec.py:321] Evaluating on the training split.
I0207 03:08:48.218463 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:11:59.165605 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 03:12:01.874805 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:14:31.118733 140446903760704 spec.py:349] Evaluating on the test split.
I0207 03:14:33.828288 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:17:06.643616 140446903760704 submission_runner.py:408] Time since start: 50471.16s, 	Step: 88829, 	{'train/accuracy': 0.6717724204063416, 'train/loss': 1.6399476528167725, 'train/bleu': 33.690016325414604, 'validation/accuracy': 0.6823846101760864, 'validation/loss': 1.570753574371338, 'validation/bleu': 30.108637798690488, 'validation/num_examples': 3000, 'test/accuracy': 0.6959851384162903, 'test/loss': 1.4950261116027832, 'test/bleu': 29.989663189851356, 'test/num_examples': 3003, 'score': 31119.40724849701, 'total_duration': 50471.155947208405, 'accumulated_submission_time': 31119.40724849701, 'accumulated_eval_time': 19347.71030664444, 'accumulated_logging_time': 1.1959412097930908}
I0207 03:17:06.670920 140277221144320 logging_writer.py:48] [88829] accumulated_eval_time=19347.710307, accumulated_logging_time=1.195941, accumulated_submission_time=31119.407248, global_step=88829, preemption_count=0, score=31119.407248, test/accuracy=0.695985, test/bleu=29.989663, test/loss=1.495026, test/num_examples=3003, total_duration=50471.155947, train/accuracy=0.671772, train/bleu=33.690016, train/loss=1.639948, validation/accuracy=0.682385, validation/bleu=30.108638, validation/loss=1.570754, validation/num_examples=3000
I0207 03:17:31.784608 140277229537024 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4336634576320648, loss=2.77978515625
I0207 03:18:06.717831 140277221144320 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.4442102909088135, loss=2.7513210773468018
I0207 03:18:41.681951 140277229537024 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4348769783973694, loss=2.767770528793335
I0207 03:19:16.663305 140277221144320 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.4333391785621643, loss=2.8161580562591553
I0207 03:19:51.662479 140277229537024 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.41318345069885254, loss=2.712597370147705
I0207 03:20:26.641920 140277221144320 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.42986971139907837, loss=2.7094318866729736
I0207 03:21:01.617181 140277229537024 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4498380720615387, loss=2.8398842811584473
I0207 03:21:36.639418 140277221144320 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.4449979364871979, loss=2.804731845855713
I0207 03:22:11.714802 140277229537024 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.41862767934799194, loss=2.818528413772583
I0207 03:22:46.702615 140277221144320 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.4475736618041992, loss=2.765327215194702
I0207 03:23:21.673888 140277229537024 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.45129233598709106, loss=2.8362655639648438
I0207 03:23:56.665062 140277221144320 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.464790940284729, loss=2.8110692501068115
I0207 03:24:31.668823 140277229537024 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.42138829827308655, loss=2.8359036445617676
I0207 03:25:06.653018 140277221144320 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.44590768218040466, loss=2.7980291843414307
I0207 03:25:41.637530 140277229537024 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.4410759508609772, loss=2.842433214187622
I0207 03:26:16.622363 140277221144320 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.48884478211402893, loss=2.8126609325408936
I0207 03:26:51.619171 140277229537024 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4409174919128418, loss=2.7550485134124756
I0207 03:27:26.597339 140277221144320 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.48537614941596985, loss=2.828677177429199
I0207 03:28:01.591642 140277229537024 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.423492968082428, loss=2.792384386062622
I0207 03:28:36.588626 140277221144320 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.43983107805252075, loss=2.841686725616455
I0207 03:29:11.601902 140277229537024 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4313826560974121, loss=2.775791883468628
I0207 03:29:46.639743 140277221144320 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.43479278683662415, loss=2.7758796215057373
I0207 03:30:21.663632 140277229537024 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.4483545124530792, loss=2.7214691638946533
I0207 03:30:56.674429 140277221144320 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.44051694869995117, loss=2.793461561203003
I0207 03:31:06.889794 140446903760704 spec.py:321] Evaluating on the training split.
I0207 03:31:09.891008 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:34:03.020884 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 03:34:05.738004 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:36:29.197515 140446903760704 spec.py:349] Evaluating on the test split.
I0207 03:36:31.898045 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:38:49.425562 140446903760704 submission_runner.py:408] Time since start: 51773.94s, 	Step: 91231, 	{'train/accuracy': 0.6652762293815613, 'train/loss': 1.6848218441009521, 'train/bleu': 33.516918895580865, 'validation/accuracy': 0.682136595249176, 'validation/loss': 1.5662659406661987, 'validation/bleu': 29.97718632659504, 'validation/num_examples': 3000, 'test/accuracy': 0.6975887417793274, 'test/loss': 1.4849611520767212, 'test/bleu': 30.002012447607637, 'test/num_examples': 3003, 'score': 31959.536118268967, 'total_duration': 51773.93786597252, 'accumulated_submission_time': 31959.536118268967, 'accumulated_eval_time': 19810.24594926834, 'accumulated_logging_time': 1.2348592281341553}
I0207 03:38:49.458685 140277229537024 logging_writer.py:48] [91231] accumulated_eval_time=19810.245949, accumulated_logging_time=1.234859, accumulated_submission_time=31959.536118, global_step=91231, preemption_count=0, score=31959.536118, test/accuracy=0.697589, test/bleu=30.002012, test/loss=1.484961, test/num_examples=3003, total_duration=51773.937866, train/accuracy=0.665276, train/bleu=33.516919, train/loss=1.684822, validation/accuracy=0.682137, validation/bleu=29.977186, validation/loss=1.566266, validation/num_examples=3000
I0207 03:39:13.854839 140277221144320 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.45473712682724, loss=2.7376811504364014
I0207 03:39:48.740977 140277229537024 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4637487828731537, loss=2.759697437286377
I0207 03:40:23.732688 140277221144320 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.43107032775878906, loss=2.7570414543151855
I0207 03:40:58.729175 140277229537024 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4618140459060669, loss=2.82851505279541
I0207 03:41:33.730000 140277221144320 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.46528327465057373, loss=2.770880699157715
I0207 03:42:08.730768 140277229537024 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.44179463386535645, loss=2.750282049179077
I0207 03:42:43.735346 140277221144320 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.47295427322387695, loss=2.7203967571258545
I0207 03:43:18.708816 140277229537024 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.48273903131484985, loss=2.810622215270996
I0207 03:43:53.681553 140277221144320 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.4467938542366028, loss=2.806036949157715
I0207 03:44:28.700863 140277229537024 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.47359976172447205, loss=2.773712158203125
I0207 03:45:03.696297 140277221144320 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4497721791267395, loss=2.7361013889312744
I0207 03:45:38.734872 140277229537024 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4789115786552429, loss=2.842759609222412
I0207 03:46:13.754137 140277221144320 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.45810532569885254, loss=2.7719218730926514
I0207 03:46:48.734396 140277229537024 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.43729367852211, loss=2.753629207611084
I0207 03:47:23.767050 140277221144320 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.4684244692325592, loss=2.740328073501587
I0207 03:47:58.773664 140277229537024 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.46642863750457764, loss=2.7677383422851562
I0207 03:48:33.740485 140277221144320 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.44674476981163025, loss=2.740208864212036
I0207 03:49:08.764565 140277229537024 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4506864547729492, loss=2.822376012802124
I0207 03:49:43.754202 140277221144320 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.4822658598423004, loss=2.839169979095459
I0207 03:50:18.762742 140277229537024 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.4623613953590393, loss=2.7999227046966553
I0207 03:50:53.736668 140277221144320 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4609280526638031, loss=2.7845585346221924
I0207 03:51:28.768946 140277229537024 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4748201370239258, loss=2.7766807079315186
I0207 03:52:03.764387 140277221144320 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.4888860881328583, loss=2.7289390563964844
I0207 03:52:38.753402 140277229537024 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.5012912750244141, loss=2.7521755695343018
I0207 03:52:49.661982 140446903760704 spec.py:321] Evaluating on the training split.
I0207 03:52:52.672901 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:56:03.336611 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 03:56:06.034355 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 03:58:56.156826 140446903760704 spec.py:349] Evaluating on the test split.
I0207 03:58:58.861635 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 04:01:35.047007 140446903760704 submission_runner.py:408] Time since start: 53139.56s, 	Step: 93633, 	{'train/accuracy': 0.6700052618980408, 'train/loss': 1.6654683351516724, 'train/bleu': 33.97726970739959, 'validation/accuracy': 0.6840088963508606, 'validation/loss': 1.5550308227539062, 'validation/bleu': 30.201155231440644, 'validation/num_examples': 3000, 'test/accuracy': 0.6991226673126221, 'test/loss': 1.4706521034240723, 'test/bleu': 30.245471742788, 'test/num_examples': 3003, 'score': 32799.64951753616, 'total_duration': 53139.55933403969, 'accumulated_submission_time': 32799.64951753616, 'accumulated_eval_time': 20335.630873441696, 'accumulated_logging_time': 1.280862808227539}
I0207 04:01:35.075292 140277221144320 logging_writer.py:48] [93633] accumulated_eval_time=20335.630873, accumulated_logging_time=1.280863, accumulated_submission_time=32799.649518, global_step=93633, preemption_count=0, score=32799.649518, test/accuracy=0.699123, test/bleu=30.245472, test/loss=1.470652, test/num_examples=3003, total_duration=53139.559334, train/accuracy=0.670005, train/bleu=33.977270, train/loss=1.665468, validation/accuracy=0.684009, validation/bleu=30.201155, validation/loss=1.555031, validation/num_examples=3000
I0207 04:01:58.783934 140277229537024 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.4570308029651642, loss=2.725709915161133
I0207 04:02:33.683579 140277221144320 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.45400673151016235, loss=2.696281671524048
I0207 04:03:08.652432 140277229537024 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.5051881074905396, loss=2.8050599098205566
I0207 04:03:43.647454 140277221144320 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.46741288900375366, loss=2.727074146270752
I0207 04:04:18.644015 140277229537024 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.45568206906318665, loss=2.7361581325531006
I0207 04:04:53.607470 140277221144320 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4586971402168274, loss=2.773808479309082
I0207 04:05:28.584479 140277229537024 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.48697301745414734, loss=2.7848236560821533
I0207 04:06:03.554446 140277221144320 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4614161550998688, loss=2.768244981765747
I0207 04:06:38.551288 140277229537024 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4706628620624542, loss=2.7884860038757324
I0207 04:07:13.542917 140277221144320 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.4912094473838806, loss=2.803831100463867
I0207 04:07:48.534372 140277229537024 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.46589457988739014, loss=2.754988193511963
I0207 04:08:23.529103 140277221144320 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.452705979347229, loss=2.7374954223632812
I0207 04:08:58.513826 140277229537024 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4768141210079193, loss=2.7510201930999756
I0207 04:09:33.470039 140277221144320 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.4693578779697418, loss=2.7224743366241455
I0207 04:10:08.467683 140277229537024 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.49864986538887024, loss=2.785388231277466
I0207 04:10:43.494408 140277221144320 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.47338446974754333, loss=2.8038172721862793
I0207 04:11:18.466948 140277229537024 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.48943892121315, loss=2.782942533493042
I0207 04:11:53.480044 140277221144320 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.46794819831848145, loss=2.685755491256714
I0207 04:12:28.488790 140277229537024 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.46443459391593933, loss=2.6971287727355957
I0207 04:13:03.494118 140277221144320 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.47730815410614014, loss=2.726870059967041
I0207 04:13:38.484257 140277229537024 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.4951261878013611, loss=2.8268539905548096
I0207 04:14:13.472949 140277221144320 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.4724001884460449, loss=2.803524971008301
I0207 04:14:48.469211 140277229537024 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.5505526065826416, loss=2.8417041301727295
I0207 04:15:23.445532 140277221144320 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.47696319222450256, loss=2.7327280044555664
I0207 04:15:35.062278 140446903760704 spec.py:321] Evaluating on the training split.
I0207 04:15:38.068010 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 04:18:50.909731 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 04:18:53.616192 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 04:21:31.491180 140446903760704 spec.py:349] Evaluating on the test split.
I0207 04:21:34.215250 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 04:24:10.934118 140446903760704 submission_runner.py:408] Time since start: 54495.45s, 	Step: 96035, 	{'train/accuracy': 0.6742818355560303, 'train/loss': 1.6293132305145264, 'train/bleu': 33.98130465498545, 'validation/accuracy': 0.6850255727767944, 'validation/loss': 1.5457754135131836, 'validation/bleu': 30.021522350598442, 'validation/num_examples': 3000, 'test/accuracy': 0.6990413069725037, 'test/loss': 1.4698675870895386, 'test/bleu': 30.285427126973314, 'test/num_examples': 3003, 'score': 33639.54993915558, 'total_duration': 54495.446476221085, 'accumulated_submission_time': 33639.54993915558, 'accumulated_eval_time': 20851.502648115158, 'accumulated_logging_time': 1.319817066192627}
I0207 04:24:10.962178 140277229537024 logging_writer.py:48] [96035] accumulated_eval_time=20851.502648, accumulated_logging_time=1.319817, accumulated_submission_time=33639.549939, global_step=96035, preemption_count=0, score=33639.549939, test/accuracy=0.699041, test/bleu=30.285427, test/loss=1.469868, test/num_examples=3003, total_duration=54495.446476, train/accuracy=0.674282, train/bleu=33.981305, train/loss=1.629313, validation/accuracy=0.685026, validation/bleu=30.021522, validation/loss=1.545775, validation/num_examples=3000
I0207 04:24:33.944803 140277221144320 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.49402445554733276, loss=2.7295374870300293
I0207 04:25:08.829269 140277229537024 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.4870203137397766, loss=2.7176995277404785
I0207 04:25:43.832787 140277221144320 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.48189544677734375, loss=2.794046640396118
I0207 04:26:18.825231 140277229537024 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5178636312484741, loss=2.8038747310638428
I0207 04:26:53.799720 140277221144320 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.5243856906890869, loss=2.724393606185913
I0207 04:27:28.791469 140277229537024 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.5034323930740356, loss=2.8110156059265137
I0207 04:28:03.804428 140277221144320 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.5009374022483826, loss=2.7147982120513916
I0207 04:28:38.803560 140277229537024 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.476845920085907, loss=2.8101282119750977
I0207 04:29:13.819899 140277221144320 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.5114731788635254, loss=2.760181427001953
I0207 04:29:48.824213 140277229537024 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.4819500148296356, loss=2.7601664066314697
I0207 04:30:23.840889 140277221144320 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.4737565815448761, loss=2.7048161029815674
I0207 04:30:58.853423 140277229537024 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.49035903811454773, loss=2.7031962871551514
I0207 04:31:33.866833 140277221144320 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.509728193283081, loss=2.7606101036071777
I0207 04:32:08.870825 140277229537024 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.47550293803215027, loss=2.747866153717041
I0207 04:32:43.880162 140277221144320 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.4860233962535858, loss=2.7585792541503906
I0207 04:33:18.908376 140277229537024 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.49724286794662476, loss=2.7642667293548584
I0207 04:33:53.915271 140277221144320 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.4982653856277466, loss=2.731635093688965
I0207 04:34:28.946875 140277229537024 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.49229589104652405, loss=2.7864255905151367
I0207 04:35:03.998115 140277221144320 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.4841691553592682, loss=2.725024938583374
I0207 04:35:39.007485 140277229537024 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.4938599765300751, loss=2.8127827644348145
I0207 04:36:14.050004 140277221144320 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.49305692315101624, loss=2.7432539463043213
I0207 04:36:49.057629 140277229537024 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.46978622674942017, loss=2.770244836807251
I0207 04:37:24.079190 140277221144320 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5262609720230103, loss=2.807713031768799
I0207 04:37:59.075849 140277229537024 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.4699183404445648, loss=2.6807703971862793
I0207 04:38:11.047475 140446903760704 spec.py:321] Evaluating on the training split.
I0207 04:38:14.052292 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 04:41:18.443083 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 04:41:21.147571 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 04:43:52.556007 140446903760704 spec.py:349] Evaluating on the test split.
I0207 04:43:55.272269 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 04:46:23.400748 140446903760704 submission_runner.py:408] Time since start: 55827.91s, 	Step: 98436, 	{'train/accuracy': 0.6713905930519104, 'train/loss': 1.6505440473556519, 'train/bleu': 33.99387977317795, 'validation/accuracy': 0.685149610042572, 'validation/loss': 1.544081687927246, 'validation/bleu': 30.28188354970151, 'validation/num_examples': 3000, 'test/accuracy': 0.7021788358688354, 'test/loss': 1.4656221866607666, 'test/bleu': 30.44932657753321, 'test/num_examples': 3003, 'score': 34479.54863166809, 'total_duration': 55827.9131128788, 'accumulated_submission_time': 34479.54863166809, 'accumulated_eval_time': 21343.85585975647, 'accumulated_logging_time': 1.3581314086914062}
I0207 04:46:23.428525 140277221144320 logging_writer.py:48] [98436] accumulated_eval_time=21343.855860, accumulated_logging_time=1.358131, accumulated_submission_time=34479.548632, global_step=98436, preemption_count=0, score=34479.548632, test/accuracy=0.702179, test/bleu=30.449327, test/loss=1.465622, test/num_examples=3003, total_duration=55827.913113, train/accuracy=0.671391, train/bleu=33.993880, train/loss=1.650544, validation/accuracy=0.685150, validation/bleu=30.281884, validation/loss=1.544082, validation/num_examples=3000
I0207 04:46:46.078825 140277229537024 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5071024894714355, loss=2.7562096118927
I0207 04:47:20.978007 140277221144320 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.4773843288421631, loss=2.7305469512939453
I0207 04:47:55.974611 140277229537024 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5210320949554443, loss=2.7108867168426514
I0207 04:48:30.926297 140277221144320 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.4769464433193207, loss=2.8105669021606445
I0207 04:49:05.891739 140277229537024 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5252653956413269, loss=2.7960994243621826
I0207 04:49:40.874933 140277221144320 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.4785102903842926, loss=2.743997812271118
I0207 04:50:15.855819 140277229537024 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.4612806439399719, loss=2.7252790927886963
I0207 04:50:50.828968 140277221144320 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5186166167259216, loss=2.706571340560913
I0207 04:51:25.819582 140277229537024 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.48531633615493774, loss=2.7287933826446533
I0207 04:52:00.806415 140277221144320 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.4841647744178772, loss=2.7095420360565186
I0207 04:52:35.782313 140277229537024 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5109407305717468, loss=2.727368116378784
I0207 04:53:10.767888 140277221144320 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5049053430557251, loss=2.733083724975586
I0207 04:53:45.728110 140277229537024 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.4826740622520447, loss=2.6626126766204834
I0207 04:54:20.751821 140277221144320 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5047987699508667, loss=2.751500129699707
I0207 04:54:55.784548 140277229537024 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5123803019523621, loss=2.6824231147766113
I0207 04:55:30.815221 140277221144320 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5051474571228027, loss=2.6907975673675537
I0207 04:56:05.794408 140277229537024 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5037123560905457, loss=2.773409843444824
I0207 04:56:40.808502 140277221144320 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5000900030136108, loss=2.7265586853027344
I0207 04:57:15.810951 140277229537024 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.47383442521095276, loss=2.692106008529663
I0207 04:57:50.767795 140277221144320 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5092716217041016, loss=2.753859519958496
I0207 04:58:25.766053 140277229537024 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5451765060424805, loss=2.7321126461029053
I0207 04:59:00.788820 140277221144320 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.49041661620140076, loss=2.7113921642303467
I0207 04:59:35.812494 140277229537024 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5086570382118225, loss=2.702960968017578
I0207 05:00:10.823099 140277221144320 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5253061652183533, loss=2.7059545516967773
I0207 05:00:23.478888 140446903760704 spec.py:321] Evaluating on the training split.
I0207 05:00:26.494634 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:03:34.857046 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 05:03:37.579607 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:06:32.499967 140446903760704 spec.py:349] Evaluating on the test split.
I0207 05:06:35.214796 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:09:08.561221 140446903760704 submission_runner.py:408] Time since start: 57193.07s, 	Step: 100838, 	{'train/accuracy': 0.6884165406227112, 'train/loss': 1.5480619668960571, 'train/bleu': 35.2754136989264, 'validation/accuracy': 0.6868482828140259, 'validation/loss': 1.538313865661621, 'validation/bleu': 30.528489437300816, 'validation/num_examples': 3000, 'test/accuracy': 0.7017489075660706, 'test/loss': 1.4601340293884277, 'test/bleu': 30.560988154441777, 'test/num_examples': 3003, 'score': 35319.508835315704, 'total_duration': 57193.07353281975, 'accumulated_submission_time': 35319.508835315704, 'accumulated_eval_time': 21868.93807411194, 'accumulated_logging_time': 1.398036003112793}
I0207 05:09:08.594267 140277229537024 logging_writer.py:48] [100838] accumulated_eval_time=21868.938074, accumulated_logging_time=1.398036, accumulated_submission_time=35319.508835, global_step=100838, preemption_count=0, score=35319.508835, test/accuracy=0.701749, test/bleu=30.560988, test/loss=1.460134, test/num_examples=3003, total_duration=57193.073533, train/accuracy=0.688417, train/bleu=35.275414, train/loss=1.548062, validation/accuracy=0.686848, validation/bleu=30.528489, validation/loss=1.538314, validation/num_examples=3000
I0207 05:09:30.575850 140277221144320 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.497965008020401, loss=2.742950439453125
I0207 05:10:05.494390 140277229537024 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5267691612243652, loss=2.7588415145874023
I0207 05:10:40.436091 140277221144320 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5436519384384155, loss=2.7390918731689453
I0207 05:11:15.400996 140277229537024 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5580231547355652, loss=2.7539684772491455
I0207 05:11:50.407712 140277221144320 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.5207651853561401, loss=2.786729335784912
I0207 05:12:25.426559 140277229537024 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5234225392341614, loss=2.749267816543579
I0207 05:13:00.509584 140277221144320 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5263262391090393, loss=2.736694097518921
I0207 05:13:35.559661 140277229537024 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5179420709609985, loss=2.713357448577881
I0207 05:14:10.544028 140277221144320 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.536298394203186, loss=2.7097487449645996
I0207 05:14:45.508485 140277229537024 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5092855095863342, loss=2.7451488971710205
I0207 05:15:20.491737 140277221144320 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5325656533241272, loss=2.792806625366211
I0207 05:15:55.455694 140277229537024 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5243476629257202, loss=2.7427010536193848
I0207 05:16:30.468355 140277221144320 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5199669003486633, loss=2.713942527770996
I0207 05:17:05.450589 140277229537024 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5427393913269043, loss=2.754349708557129
I0207 05:17:40.455279 140277221144320 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.512406587600708, loss=2.6912477016448975
I0207 05:18:15.487693 140277229537024 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5400535464286804, loss=2.7259082794189453
I0207 05:18:50.460612 140277221144320 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5205961465835571, loss=2.7177295684814453
I0207 05:19:25.466514 140277229537024 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5247467160224915, loss=2.7159228324890137
I0207 05:20:00.460529 140277221144320 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5586071610450745, loss=2.8239076137542725
I0207 05:20:35.449594 140277229537024 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.537858247756958, loss=2.7481284141540527
I0207 05:21:10.486019 140277221144320 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5368380546569824, loss=2.739130735397339
I0207 05:21:45.530472 140277229537024 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5289109349250793, loss=2.7247371673583984
I0207 05:22:20.512827 140277221144320 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5317012071609497, loss=2.7051961421966553
I0207 05:22:55.503973 140277229537024 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.535851776599884, loss=2.7800586223602295
I0207 05:23:08.860372 140446903760704 spec.py:321] Evaluating on the training split.
I0207 05:23:11.868539 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:25:59.522097 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 05:26:02.256805 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:28:38.149486 140446903760704 spec.py:349] Evaluating on the test split.
I0207 05:28:40.880941 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:31:15.108903 140446903760704 submission_runner.py:408] Time since start: 58519.62s, 	Step: 103240, 	{'train/accuracy': 0.6779972314834595, 'train/loss': 1.604690432548523, 'train/bleu': 34.50147922318023, 'validation/accuracy': 0.6882741451263428, 'validation/loss': 1.5299122333526611, 'validation/bleu': 30.41491065908115, 'validation/num_examples': 3000, 'test/accuracy': 0.7038870453834534, 'test/loss': 1.4475866556167603, 'test/bleu': 30.51891889328535, 'test/num_examples': 3003, 'score': 36159.6839966774, 'total_duration': 58519.62125611305, 'accumulated_submission_time': 36159.6839966774, 'accumulated_eval_time': 22355.186529397964, 'accumulated_logging_time': 1.44234037399292}
I0207 05:31:15.138656 140277221144320 logging_writer.py:48] [103240] accumulated_eval_time=22355.186529, accumulated_logging_time=1.442340, accumulated_submission_time=36159.683997, global_step=103240, preemption_count=0, score=36159.683997, test/accuracy=0.703887, test/bleu=30.518919, test/loss=1.447587, test/num_examples=3003, total_duration=58519.621256, train/accuracy=0.677997, train/bleu=34.501479, train/loss=1.604690, validation/accuracy=0.688274, validation/bleu=30.414911, validation/loss=1.529912, validation/num_examples=3000
I0207 05:31:36.401340 140277229537024 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5491669774055481, loss=2.768599033355713
I0207 05:32:11.291822 140277221144320 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5292759537696838, loss=2.7472903728485107
I0207 05:32:46.307823 140277229537024 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5282406210899353, loss=2.687784433364868
I0207 05:33:21.290357 140277221144320 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5755714774131775, loss=2.700850486755371
I0207 05:33:56.283124 140277229537024 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.562162458896637, loss=2.665410280227661
I0207 05:34:31.266454 140277221144320 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5447666049003601, loss=2.762132406234741
I0207 05:35:06.276399 140277229537024 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5419124960899353, loss=2.7205071449279785
I0207 05:35:41.255515 140277221144320 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5537866353988647, loss=2.7388503551483154
I0207 05:36:16.278858 140277229537024 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5385183095932007, loss=2.6555960178375244
I0207 05:36:51.295791 140277221144320 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.543947160243988, loss=2.7224929332733154
I0207 05:37:26.302493 140277229537024 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5447037220001221, loss=2.700167655944824
I0207 05:38:01.329756 140277221144320 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5709779262542725, loss=2.7694506645202637
I0207 05:38:36.331292 140277229537024 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5495364665985107, loss=2.721525192260742
I0207 05:39:11.324484 140277221144320 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5242893695831299, loss=2.6718525886535645
I0207 05:39:46.310853 140277229537024 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.5344486236572266, loss=2.694239616394043
I0207 05:40:21.282396 140277221144320 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.5547314286231995, loss=2.6926615238189697
I0207 05:40:56.276893 140277229537024 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5562687516212463, loss=2.7004477977752686
I0207 05:41:31.274299 140277221144320 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5446863174438477, loss=2.665498733520508
I0207 05:42:06.261899 140277229537024 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5512468218803406, loss=2.7189691066741943
I0207 05:42:41.261321 140277221144320 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.5729798674583435, loss=2.729123830795288
I0207 05:43:16.263194 140277229537024 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5595601797103882, loss=2.72284197807312
I0207 05:43:51.260045 140277221144320 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.597690224647522, loss=2.677442789077759
I0207 05:44:26.251429 140277229537024 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.5739712715148926, loss=2.709120988845825
I0207 05:45:01.236834 140277221144320 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5470267534255981, loss=2.6856577396392822
I0207 05:45:15.300644 140446903760704 spec.py:321] Evaluating on the training split.
I0207 05:45:18.314163 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:48:17.501538 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 05:48:20.215640 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:50:57.292243 140446903760704 spec.py:349] Evaluating on the test split.
I0207 05:51:00.004538 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 05:53:19.110297 140446903760704 submission_runner.py:408] Time since start: 59843.62s, 	Step: 105642, 	{'train/accuracy': 0.6837121248245239, 'train/loss': 1.5825554132461548, 'train/bleu': 34.452289799932295, 'validation/accuracy': 0.6886957287788391, 'validation/loss': 1.523957371711731, 'validation/bleu': 30.520724395170017, 'validation/num_examples': 3000, 'test/accuracy': 0.7051188349723816, 'test/loss': 1.4421643018722534, 'test/bleu': 30.512686812294906, 'test/num_examples': 3003, 'score': 36999.760825634, 'total_duration': 59843.622671842575, 'accumulated_submission_time': 36999.760825634, 'accumulated_eval_time': 22838.996133089066, 'accumulated_logging_time': 1.4821085929870605}
I0207 05:53:19.138805 140277229537024 logging_writer.py:48] [105642] accumulated_eval_time=22838.996133, accumulated_logging_time=1.482109, accumulated_submission_time=36999.760826, global_step=105642, preemption_count=0, score=36999.760826, test/accuracy=0.705119, test/bleu=30.512687, test/loss=1.442164, test/num_examples=3003, total_duration=59843.622672, train/accuracy=0.683712, train/bleu=34.452290, train/loss=1.582555, validation/accuracy=0.688696, validation/bleu=30.520724, validation/loss=1.523957, validation/num_examples=3000
I0207 05:53:39.683183 140277221144320 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.569727897644043, loss=2.6501128673553467
I0207 05:54:14.560381 140277229537024 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5629196166992188, loss=2.714012861251831
I0207 05:54:49.535265 140277221144320 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.5637109279632568, loss=2.7635114192962646
I0207 05:55:24.500976 140277229537024 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5529146790504456, loss=2.724961996078491
I0207 05:55:59.474701 140277221144320 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.575467586517334, loss=2.682772159576416
I0207 05:56:34.442950 140277229537024 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.558800220489502, loss=2.6922719478607178
I0207 05:57:09.436270 140277221144320 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.5614281892776489, loss=2.718207836151123
I0207 05:57:44.507774 140277229537024 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5724900364875793, loss=2.675973415374756
I0207 05:58:19.541923 140277221144320 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.5861710906028748, loss=2.696483612060547
I0207 05:58:54.574583 140277229537024 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.5579022765159607, loss=2.649177074432373
I0207 05:59:29.571367 140277221144320 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.5570318698883057, loss=2.6688404083251953
I0207 06:00:04.562398 140277229537024 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5471881628036499, loss=2.6917200088500977
I0207 06:00:39.554410 140277221144320 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.5701776742935181, loss=2.6627650260925293
I0207 06:01:14.558754 140277229537024 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.5901134610176086, loss=2.785785675048828
I0207 06:01:49.538019 140277221144320 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.5985378623008728, loss=2.6863536834716797
I0207 06:02:24.545620 140277229537024 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.5759992003440857, loss=2.6939117908477783
I0207 06:02:59.537178 140277221144320 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.5790328979492188, loss=2.718503713607788
I0207 06:03:34.541764 140277229537024 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.5878893733024597, loss=2.7401413917541504
I0207 06:04:09.545027 140277221144320 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.5676038861274719, loss=2.6171035766601562
I0207 06:04:44.567959 140277229537024 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.5784912109375, loss=2.6836657524108887
I0207 06:05:19.546018 140277221144320 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.5639329552650452, loss=2.6284122467041016
I0207 06:05:54.567515 140277229537024 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.579748809337616, loss=2.7093636989593506
I0207 06:06:29.600195 140277221144320 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.593381404876709, loss=2.6376991271972656
I0207 06:07:04.639804 140277229537024 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.5941820740699768, loss=2.6948864459991455
I0207 06:07:19.423615 140446903760704 spec.py:321] Evaluating on the training split.
I0207 06:07:22.461436 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:10:21.074881 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 06:10:23.790764 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:12:59.041865 140446903760704 spec.py:349] Evaluating on the test split.
I0207 06:13:01.776477 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:15:37.253158 140446903760704 submission_runner.py:408] Time since start: 61181.77s, 	Step: 108044, 	{'train/accuracy': 0.6904653310775757, 'train/loss': 1.5444891452789307, 'train/bleu': 35.465164598367224, 'validation/accuracy': 0.6911135315895081, 'validation/loss': 1.5180892944335938, 'validation/bleu': 30.564646606616183, 'validation/num_examples': 3000, 'test/accuracy': 0.7067108154296875, 'test/loss': 1.433881402015686, 'test/bleu': 30.808667488476935, 'test/num_examples': 3003, 'score': 37839.9554643631, 'total_duration': 61181.76550936699, 'accumulated_submission_time': 37839.9554643631, 'accumulated_eval_time': 23336.82561659813, 'accumulated_logging_time': 1.5219931602478027}
I0207 06:15:37.289104 140277221144320 logging_writer.py:48] [108044] accumulated_eval_time=23336.825617, accumulated_logging_time=1.521993, accumulated_submission_time=37839.955464, global_step=108044, preemption_count=0, score=37839.955464, test/accuracy=0.706711, test/bleu=30.808667, test/loss=1.433881, test/num_examples=3003, total_duration=61181.765509, train/accuracy=0.690465, train/bleu=35.465165, train/loss=1.544489, validation/accuracy=0.691114, validation/bleu=30.564647, validation/loss=1.518089, validation/num_examples=3000
I0207 06:15:57.149528 140277229537024 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.5807114839553833, loss=2.677466869354248
I0207 06:16:32.026634 140277221144320 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.5986567139625549, loss=2.6988518238067627
I0207 06:17:06.994214 140277229537024 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.6244122982025146, loss=2.7083992958068848
I0207 06:17:41.983710 140277221144320 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.5871437788009644, loss=2.695403814315796
I0207 06:18:16.981308 140277229537024 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.6283382773399353, loss=2.6171555519104004
I0207 06:18:51.994800 140277221144320 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.5897193551063538, loss=2.695591688156128
I0207 06:19:26.991196 140277229537024 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.6169514060020447, loss=2.6465301513671875
I0207 06:20:01.993826 140277221144320 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.5979446172714233, loss=2.6132898330688477
I0207 06:20:36.976335 140277229537024 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6146345734596252, loss=2.6679890155792236
I0207 06:21:11.960299 140277221144320 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6007468700408936, loss=2.6730353832244873
I0207 06:21:46.948330 140277229537024 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.6158117651939392, loss=2.6491336822509766
I0207 06:22:21.919268 140277221144320 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.6082557439804077, loss=2.676250457763672
I0207 06:22:56.898264 140277229537024 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.6256353259086609, loss=2.6620078086853027
I0207 06:23:31.881155 140277221144320 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.625420868396759, loss=2.705200672149658
I0207 06:24:06.838756 140277229537024 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.5975408554077148, loss=2.728435516357422
I0207 06:24:41.884836 140277221144320 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.5903589129447937, loss=2.623563289642334
I0207 06:25:16.917609 140277229537024 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.5983848571777344, loss=2.6848866939544678
I0207 06:25:51.925652 140277221144320 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.62019282579422, loss=2.6650352478027344
I0207 06:26:26.882967 140277229537024 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6161852478981018, loss=2.684861421585083
I0207 06:27:01.862518 140277221144320 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6100082397460938, loss=2.670172691345215
I0207 06:27:36.853210 140277229537024 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6081210374832153, loss=2.659804582595825
I0207 06:28:11.865991 140277221144320 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.6432693600654602, loss=2.705751419067383
I0207 06:28:46.912502 140277229537024 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.6077704429626465, loss=2.613009214401245
I0207 06:29:22.035431 140277221144320 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6367736458778381, loss=2.6569766998291016
I0207 06:29:37.540530 140446903760704 spec.py:321] Evaluating on the training split.
I0207 06:29:40.576306 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:32:42.557866 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 06:32:45.276112 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:35:22.593006 140446903760704 spec.py:349] Evaluating on the test split.
I0207 06:35:25.330252 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:38:00.065172 140446903760704 submission_runner.py:408] Time since start: 62524.58s, 	Step: 110446, 	{'train/accuracy': 0.6886968612670898, 'train/loss': 1.552962303161621, 'train/bleu': 34.8873148481868, 'validation/accuracy': 0.6909399628639221, 'validation/loss': 1.516938328742981, 'validation/bleu': 30.795585694647226, 'validation/num_examples': 3000, 'test/accuracy': 0.7068270444869995, 'test/loss': 1.429835557937622, 'test/bleu': 30.794322137941766, 'test/num_examples': 3003, 'score': 38680.11610341072, 'total_duration': 62524.57753229141, 'accumulated_submission_time': 38680.11610341072, 'accumulated_eval_time': 23839.350203037262, 'accumulated_logging_time': 1.5703990459442139}
I0207 06:38:00.095393 140277229537024 logging_writer.py:48] [110446] accumulated_eval_time=23839.350203, accumulated_logging_time=1.570399, accumulated_submission_time=38680.116103, global_step=110446, preemption_count=0, score=38680.116103, test/accuracy=0.706827, test/bleu=30.794322, test/loss=1.429836, test/num_examples=3003, total_duration=62524.577532, train/accuracy=0.688697, train/bleu=34.887315, train/loss=1.552962, validation/accuracy=0.690940, validation/bleu=30.795586, validation/loss=1.516938, validation/num_examples=3000
I0207 06:38:19.272066 140277221144320 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6228847503662109, loss=2.7445425987243652
I0207 06:38:54.175945 140277229537024 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6206772327423096, loss=2.6463735103607178
I0207 06:39:29.136494 140277221144320 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.6208816170692444, loss=2.7147581577301025
I0207 06:40:04.135876 140277229537024 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.6176912188529968, loss=2.6750528812408447
I0207 06:40:39.111972 140277221144320 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.629906177520752, loss=2.6975128650665283
I0207 06:41:14.139145 140277229537024 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.6206457018852234, loss=2.730285406112671
I0207 06:41:49.220333 140277221144320 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6004942059516907, loss=2.61089825630188
I0207 06:42:24.245080 140277229537024 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.6381181478500366, loss=2.586181402206421
I0207 06:42:59.263580 140277221144320 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.633577823638916, loss=2.6993396282196045
I0207 06:43:34.281682 140277229537024 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.6622295379638672, loss=2.6494083404541016
I0207 06:44:09.264837 140277221144320 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6231871843338013, loss=2.686044216156006
I0207 06:44:44.292789 140277229537024 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.6472173929214478, loss=2.638228178024292
I0207 06:45:19.320410 140277221144320 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.654218316078186, loss=2.6139719486236572
I0207 06:45:54.386789 140277229537024 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.6371312141418457, loss=2.6765737533569336
I0207 06:46:29.424507 140277221144320 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.6618991494178772, loss=2.627502679824829
I0207 06:47:04.466690 140277229537024 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6298317313194275, loss=2.6312074661254883
I0207 06:47:39.476674 140277221144320 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.6563006639480591, loss=2.6557583808898926
I0207 06:48:14.512815 140277229537024 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.6282207369804382, loss=2.6409411430358887
I0207 06:48:49.638432 140277221144320 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6502299308776855, loss=2.6776084899902344
I0207 06:49:24.689624 140277229537024 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.6226096153259277, loss=2.6585285663604736
I0207 06:49:59.695696 140277221144320 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.6363279223442078, loss=2.725489616394043
I0207 06:50:34.722215 140277229537024 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.6769916415214539, loss=2.712618827819824
I0207 06:51:09.732340 140277221144320 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.6322625279426575, loss=2.6555678844451904
I0207 06:51:44.786286 140277229537024 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.638148844242096, loss=2.595156192779541
I0207 06:52:00.298362 140446903760704 spec.py:321] Evaluating on the training split.
I0207 06:52:03.340863 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:55:04.150425 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 06:55:06.866494 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 06:57:44.128263 140446903760704 spec.py:349] Evaluating on the test split.
I0207 06:57:46.840131 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 07:00:15.698923 140446903760704 submission_runner.py:408] Time since start: 63860.21s, 	Step: 112846, 	{'train/accuracy': 0.7077298760414124, 'train/loss': 1.4527026414871216, 'train/bleu': 36.72570097184537, 'validation/accuracy': 0.6913243532180786, 'validation/loss': 1.5172661542892456, 'validation/bleu': 30.76716783706688, 'validation/num_examples': 3000, 'test/accuracy': 0.7065481543540955, 'test/loss': 1.4316315650939941, 'test/bleu': 30.935883540712304, 'test/num_examples': 3003, 'score': 39520.2310898304, 'total_duration': 63860.211265563965, 'accumulated_submission_time': 39520.2310898304, 'accumulated_eval_time': 24334.75069284439, 'accumulated_logging_time': 1.6107723712921143}
I0207 07:00:15.728543 140277221144320 logging_writer.py:48] [112846] accumulated_eval_time=24334.750693, accumulated_logging_time=1.610772, accumulated_submission_time=39520.231090, global_step=112846, preemption_count=0, score=39520.231090, test/accuracy=0.706548, test/bleu=30.935884, test/loss=1.431632, test/num_examples=3003, total_duration=63860.211266, train/accuracy=0.707730, train/bleu=36.725701, train/loss=1.452703, validation/accuracy=0.691324, validation/bleu=30.767168, validation/loss=1.517266, validation/num_examples=3000
I0207 07:00:34.921292 140277229537024 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.6630124449729919, loss=2.658155918121338
I0207 07:01:09.812729 140277221144320 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6518903374671936, loss=2.634566307067871
I0207 07:01:44.815609 140277229537024 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.698438823223114, loss=2.685042142868042
I0207 07:02:19.816493 140277221144320 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.6420057415962219, loss=2.6261322498321533
I0207 07:02:54.774268 140277229537024 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.7056564688682556, loss=2.6646358966827393
I0207 07:03:29.733379 140277221144320 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.6625401973724365, loss=2.6283979415893555
I0207 07:04:04.749302 140277229537024 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6667882800102234, loss=2.6990785598754883
I0207 07:04:39.713809 140277221144320 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6557849049568176, loss=2.6508235931396484
I0207 07:05:14.733844 140277229537024 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.6549825072288513, loss=2.6545445919036865
I0207 07:05:49.731167 140277221144320 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.6748791933059692, loss=2.6837551593780518
I0207 07:06:24.722900 140277229537024 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.6712679266929626, loss=2.7177302837371826
I0207 07:06:59.700679 140277221144320 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6657945513725281, loss=2.5738959312438965
I0207 07:07:34.697943 140277229537024 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.6803955435752869, loss=2.680908441543579
I0207 07:08:09.693483 140277221144320 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.6546648144721985, loss=2.6476874351501465
I0207 07:08:44.720818 140277229537024 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.6644901633262634, loss=2.6751363277435303
I0207 07:09:19.704099 140277221144320 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.6590635180473328, loss=2.6257002353668213
I0207 07:09:54.696549 140277229537024 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.6365669965744019, loss=2.6124260425567627
I0207 07:10:29.681481 140277221144320 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.6665772199630737, loss=2.644080638885498
I0207 07:11:04.688340 140277229537024 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.696606457233429, loss=2.65095591545105
I0207 07:11:39.700248 140277221144320 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.6789135336875916, loss=2.654139995574951
I0207 07:12:14.681239 140277229537024 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.677425742149353, loss=2.6036503314971924
I0207 07:12:49.643025 140277221144320 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.6700502634048462, loss=2.615715503692627
I0207 07:13:24.649669 140277229537024 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.6538328528404236, loss=2.607710123062134
I0207 07:13:59.657930 140277221144320 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.6998452544212341, loss=2.6022017002105713
I0207 07:14:15.826706 140446903760704 spec.py:321] Evaluating on the training split.
I0207 07:14:18.830125 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 07:17:19.720451 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 07:17:22.432201 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 07:19:51.814409 140446903760704 spec.py:349] Evaluating on the test split.
I0207 07:19:54.528772 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 07:22:19.003934 140446903760704 submission_runner.py:408] Time since start: 65183.52s, 	Step: 115248, 	{'train/accuracy': 0.69832843542099, 'train/loss': 1.5016064643859863, 'train/bleu': 35.53555504677267, 'validation/accuracy': 0.693717360496521, 'validation/loss': 1.5081539154052734, 'validation/bleu': 30.621397086007867, 'validation/num_examples': 3000, 'test/accuracy': 0.70927894115448, 'test/loss': 1.4228955507278442, 'test/bleu': 30.847111719589698, 'test/num_examples': 3003, 'score': 40360.24406290054, 'total_duration': 65183.5163064003, 'accumulated_submission_time': 40360.24406290054, 'accumulated_eval_time': 24817.927864313126, 'accumulated_logging_time': 1.6504244804382324}
I0207 07:22:19.033876 140277229537024 logging_writer.py:48] [115248] accumulated_eval_time=24817.927864, accumulated_logging_time=1.650424, accumulated_submission_time=40360.244063, global_step=115248, preemption_count=0, score=40360.244063, test/accuracy=0.709279, test/bleu=30.847112, test/loss=1.422896, test/num_examples=3003, total_duration=65183.516306, train/accuracy=0.698328, train/bleu=35.535555, train/loss=1.501606, validation/accuracy=0.693717, validation/bleu=30.621397, validation/loss=1.508154, validation/num_examples=3000
I0207 07:22:37.488059 140277221144320 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.716947078704834, loss=2.712324857711792
I0207 07:23:12.347843 140277229537024 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.6932909488677979, loss=2.6540653705596924
I0207 07:23:47.357797 140277221144320 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.687872052192688, loss=2.6486806869506836
I0207 07:24:22.351675 140277229537024 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.7134920954704285, loss=2.6494877338409424
I0207 07:24:57.349883 140277221144320 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.6755556464195251, loss=2.6268470287323
I0207 07:25:32.317415 140277229537024 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.6962352991104126, loss=2.6110594272613525
I0207 07:26:07.302361 140277221144320 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.6694435477256775, loss=2.69146990776062
I0207 07:26:42.295088 140277229537024 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.6987139582633972, loss=2.5765528678894043
I0207 07:27:17.280560 140277221144320 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.6807781457901001, loss=2.6006405353546143
I0207 07:27:52.318731 140277229537024 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.7085855007171631, loss=2.6555159091949463
I0207 07:28:27.408364 140277221144320 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.6828672885894775, loss=2.6338419914245605
I0207 07:29:02.403854 140277229537024 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.6851712465286255, loss=2.579368829727173
I0207 07:29:37.380068 140277221144320 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6760165691375732, loss=2.6483893394470215
I0207 07:30:12.382933 140277229537024 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.6978313326835632, loss=2.5570340156555176
I0207 07:30:47.408111 140277221144320 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.7213044762611389, loss=2.634671688079834
I0207 07:31:22.423898 140277229537024 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.6976600289344788, loss=2.6703460216522217
I0207 07:31:57.461047 140277221144320 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.6861702799797058, loss=2.6859750747680664
I0207 07:32:32.480069 140277229537024 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.6926684379577637, loss=2.6810247898101807
I0207 07:33:07.481444 140277221144320 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.7227393388748169, loss=2.6645619869232178
I0207 07:33:42.485851 140277229537024 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.7168865203857422, loss=2.653653860092163
I0207 07:34:17.488990 140277221144320 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.724685549736023, loss=2.592068672180176
I0207 07:34:52.520243 140277229537024 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.7117956280708313, loss=2.6990108489990234
I0207 07:35:27.527536 140277221144320 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.694299578666687, loss=2.581775426864624
I0207 07:36:02.515115 140277229537024 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.7087283134460449, loss=2.5959739685058594
I0207 07:36:19.035487 140446903760704 spec.py:321] Evaluating on the training split.
I0207 07:36:22.047307 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 07:39:37.539336 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 07:39:40.251734 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 07:42:12.020823 140446903760704 spec.py:349] Evaluating on the test split.
I0207 07:42:14.713391 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 07:44:44.694098 140446903760704 submission_runner.py:408] Time since start: 66529.21s, 	Step: 117649, 	{'train/accuracy': 0.6946881413459778, 'train/loss': 1.516922116279602, 'train/bleu': 35.48660590814062, 'validation/accuracy': 0.6925890445709229, 'validation/loss': 1.5068556070327759, 'validation/bleu': 31.07297394971343, 'validation/num_examples': 3000, 'test/accuracy': 0.7097670435905457, 'test/loss': 1.4213274717330933, 'test/bleu': 30.929849886140154, 'test/num_examples': 3003, 'score': 41200.15984940529, 'total_duration': 66529.20644688606, 'accumulated_submission_time': 41200.15984940529, 'accumulated_eval_time': 25323.58639740944, 'accumulated_logging_time': 1.6914031505584717}
I0207 07:44:44.724936 140277221144320 logging_writer.py:48] [117649] accumulated_eval_time=25323.586397, accumulated_logging_time=1.691403, accumulated_submission_time=41200.159849, global_step=117649, preemption_count=0, score=41200.159849, test/accuracy=0.709767, test/bleu=30.929850, test/loss=1.421327, test/num_examples=3003, total_duration=66529.206447, train/accuracy=0.694688, train/bleu=35.486606, train/loss=1.516922, validation/accuracy=0.692589, validation/bleu=31.072974, validation/loss=1.506856, validation/num_examples=3000
I0207 07:45:02.868330 140277229537024 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.7029101252555847, loss=2.6565370559692383
I0207 07:45:37.770754 140277221144320 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.7001098990440369, loss=2.632889986038208
I0207 07:46:12.797417 140277229537024 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.7202408313751221, loss=2.6197264194488525
I0207 07:46:47.790816 140277221144320 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.7029960751533508, loss=2.5694072246551514
I0207 07:47:22.799790 140277229537024 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.7445931434631348, loss=2.700802803039551
I0207 07:47:57.825435 140277221144320 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.7204878330230713, loss=2.6253998279571533
I0207 07:48:32.845942 140277229537024 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.7035350799560547, loss=2.651758909225464
I0207 07:49:07.880709 140277221144320 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.7305797338485718, loss=2.630866289138794
I0207 07:49:42.863250 140277229537024 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.7465980052947998, loss=2.637584686279297
I0207 07:50:17.855975 140277221144320 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.724850594997406, loss=2.6027121543884277
I0207 07:50:52.857150 140277229537024 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.7475157976150513, loss=2.688983917236328
I0207 07:51:27.865847 140277221144320 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.7247569561004639, loss=2.6245346069335938
I0207 07:52:02.912590 140277229537024 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.7189694046974182, loss=2.603588104248047
I0207 07:52:37.952453 140277221144320 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.7237539291381836, loss=2.676398754119873
I0207 07:53:13.024132 140277229537024 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.7250754237174988, loss=2.6662237644195557
I0207 07:53:48.118482 140277221144320 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.7636131048202515, loss=2.649517774581909
I0207 07:54:23.121384 140277229537024 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.7144891619682312, loss=2.6116750240325928
I0207 07:54:58.139400 140277221144320 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.7109982371330261, loss=2.5915396213531494
I0207 07:55:33.130156 140277229537024 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.7569474577903748, loss=2.5734446048736572
I0207 07:56:08.130117 140277221144320 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.9163971543312073, loss=2.6007912158966064
I0207 07:56:43.107609 140277229537024 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7134903073310852, loss=2.6087698936462402
I0207 07:57:18.115749 140277221144320 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.7377619743347168, loss=2.5767226219177246
I0207 07:57:53.128076 140277229537024 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.7405086159706116, loss=2.6405856609344482
I0207 07:58:28.144737 140277221144320 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.7397037744522095, loss=2.611583709716797
I0207 07:58:45.008536 140446903760704 spec.py:321] Evaluating on the training split.
I0207 07:58:48.020268 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:01:44.998689 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 08:01:47.704812 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:04:22.525914 140446903760704 spec.py:349] Evaluating on the test split.
I0207 08:04:25.241918 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:06:50.130000 140446903760704 submission_runner.py:408] Time since start: 67854.64s, 	Step: 120050, 	{'train/accuracy': 0.7077162861824036, 'train/loss': 1.4448779821395874, 'train/bleu': 35.699938968051285, 'validation/accuracy': 0.693221390247345, 'validation/loss': 1.5050368309020996, 'validation/bleu': 30.882217351535818, 'validation/num_examples': 3000, 'test/accuracy': 0.7109523415565491, 'test/loss': 1.4153249263763428, 'test/bleu': 31.28327925826771, 'test/num_examples': 3003, 'score': 42040.353865385056, 'total_duration': 67854.64235019684, 'accumulated_submission_time': 42040.353865385056, 'accumulated_eval_time': 25808.707792043686, 'accumulated_logging_time': 1.7335550785064697}
I0207 08:06:50.161306 140277229537024 logging_writer.py:48] [120050] accumulated_eval_time=25808.707792, accumulated_logging_time=1.733555, accumulated_submission_time=42040.353865, global_step=120050, preemption_count=0, score=42040.353865, test/accuracy=0.710952, test/bleu=31.283279, test/loss=1.415325, test/num_examples=3003, total_duration=67854.642350, train/accuracy=0.707716, train/bleu=35.699939, train/loss=1.444878, validation/accuracy=0.693221, validation/bleu=30.882217, validation/loss=1.505037, validation/num_examples=3000
I0207 08:07:07.943944 140277221144320 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.7697794437408447, loss=2.6454341411590576
I0207 08:07:42.865038 140277229537024 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.7216861248016357, loss=2.635695457458496
I0207 08:08:17.826857 140277221144320 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.7218418717384338, loss=2.6412503719329834
I0207 08:08:52.817066 140277229537024 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.7387082576751709, loss=2.5744824409484863
I0207 08:09:27.794225 140277221144320 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.7875612378120422, loss=2.6576738357543945
I0207 08:10:02.784659 140277229537024 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.7285179495811462, loss=2.662559986114502
I0207 08:10:37.836819 140277221144320 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.7186807990074158, loss=2.5794870853424072
I0207 08:11:12.864876 140277229537024 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.6911804676055908, loss=2.60996150970459
I0207 08:11:47.882525 140277221144320 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.7500013709068298, loss=2.606015682220459
I0207 08:12:22.913203 140277229537024 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7239236235618591, loss=2.6258530616760254
I0207 08:12:57.940344 140277221144320 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.7533976435661316, loss=2.6820545196533203
I0207 08:13:33.071256 140277229537024 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.719818651676178, loss=2.6458239555358887
I0207 08:14:08.141764 140277221144320 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.7121082544326782, loss=2.638103485107422
I0207 08:14:43.126845 140277229537024 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.7328974008560181, loss=2.607267379760742
I0207 08:15:18.223142 140277221144320 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.7405378818511963, loss=2.598531723022461
I0207 08:15:53.221845 140277229537024 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.7735042572021484, loss=2.6102612018585205
I0207 08:16:28.213566 140277221144320 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.7310110330581665, loss=2.605050563812256
I0207 08:17:03.264376 140277229537024 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.7717186808586121, loss=2.637007474899292
I0207 08:17:38.271293 140277221144320 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.7622969150543213, loss=2.6566319465637207
I0207 08:18:13.285051 140277229537024 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7684124112129211, loss=2.599773645401001
I0207 08:18:48.329659 140277221144320 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.7300490736961365, loss=2.546294927597046
I0207 08:19:23.474757 140277229537024 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.7509825825691223, loss=2.6326067447662354
I0207 08:19:58.500811 140277221144320 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.7422976493835449, loss=2.568131685256958
I0207 08:20:33.520460 140277229537024 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.7697368264198303, loss=2.637003183364868
I0207 08:20:50.370382 140446903760704 spec.py:321] Evaluating on the training split.
I0207 08:20:53.384247 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:23:59.818734 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 08:24:02.548877 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:26:34.165218 140446903760704 spec.py:349] Evaluating on the test split.
I0207 08:26:36.887893 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:29:00.032140 140446903760704 submission_runner.py:408] Time since start: 69184.54s, 	Step: 122450, 	{'train/accuracy': 0.7041738033294678, 'train/loss': 1.4591567516326904, 'train/bleu': 36.04297490759636, 'validation/accuracy': 0.6942257285118103, 'validation/loss': 1.5022872686386108, 'validation/bleu': 31.002648152503227, 'validation/num_examples': 3000, 'test/accuracy': 0.7108477354049683, 'test/loss': 1.4121458530426025, 'test/bleu': 31.20686190559411, 'test/num_examples': 3003, 'score': 42880.47454190254, 'total_duration': 69184.54448390007, 'accumulated_submission_time': 42880.47454190254, 'accumulated_eval_time': 26298.369471549988, 'accumulated_logging_time': 1.7748847007751465}
I0207 08:29:00.064453 140277221144320 logging_writer.py:48] [122450] accumulated_eval_time=26298.369472, accumulated_logging_time=1.774885, accumulated_submission_time=42880.474542, global_step=122450, preemption_count=0, score=42880.474542, test/accuracy=0.710848, test/bleu=31.206862, test/loss=1.412146, test/num_examples=3003, total_duration=69184.544484, train/accuracy=0.704174, train/bleu=36.042975, train/loss=1.459157, validation/accuracy=0.694226, validation/bleu=31.002648, validation/loss=1.502287, validation/num_examples=3000
I0207 08:29:17.864027 140277229537024 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.7535971999168396, loss=2.5880613327026367
I0207 08:29:52.733114 140277221144320 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.7482556700706482, loss=2.633867025375366
I0207 08:30:27.708548 140277229537024 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.7238711714744568, loss=2.5544586181640625
I0207 08:31:02.689081 140277221144320 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.7447262406349182, loss=2.651906728744507
I0207 08:31:37.676973 140277229537024 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.7634105682373047, loss=2.6162800788879395
I0207 08:32:12.677060 140277221144320 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7706765532493591, loss=2.651095151901245
I0207 08:32:47.656563 140277229537024 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.765143871307373, loss=2.56074595451355
I0207 08:33:22.656127 140277221144320 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.7754255533218384, loss=2.6568665504455566
I0207 08:33:57.692487 140277229537024 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.7582647204399109, loss=2.586643695831299
I0207 08:34:32.715684 140277221144320 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.7957410216331482, loss=2.6020803451538086
I0207 08:35:07.712140 140277229537024 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.7628153562545776, loss=2.625115394592285
I0207 08:35:42.750151 140277221144320 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.7479953765869141, loss=2.6270828247070312
I0207 08:36:17.751950 140277229537024 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.7797590494155884, loss=2.6464035511016846
I0207 08:36:52.764150 140277221144320 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.7606338858604431, loss=2.5542032718658447
I0207 08:37:27.774792 140277229537024 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.7632921934127808, loss=2.6123359203338623
I0207 08:38:02.767378 140277221144320 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.7745922207832336, loss=2.6066153049468994
I0207 08:38:37.771276 140277229537024 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.747952401638031, loss=2.6261887550354004
I0207 08:39:12.791211 140277221144320 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.7385974526405334, loss=2.5536346435546875
I0207 08:39:47.786129 140277229537024 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.7620058059692383, loss=2.6303064823150635
I0207 08:40:22.761677 140277221144320 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.7258154153823853, loss=2.6014881134033203
I0207 08:40:57.734707 140277229537024 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7221885323524475, loss=2.596444606781006
I0207 08:41:32.723993 140277221144320 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.761530339717865, loss=2.568070888519287
I0207 08:42:07.735315 140277229537024 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.763068675994873, loss=2.560257911682129
I0207 08:42:42.731714 140277221144320 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.7941944003105164, loss=2.645777940750122
I0207 08:43:00.287050 140446903760704 spec.py:321] Evaluating on the training split.
I0207 08:43:03.316338 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:46:00.816563 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 08:46:03.546628 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:48:31.328343 140446903760704 spec.py:349] Evaluating on the test split.
I0207 08:48:34.038597 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 08:51:04.992544 140446903760704 submission_runner.py:408] Time since start: 70509.50s, 	Step: 124852, 	{'train/accuracy': 0.7027409076690674, 'train/loss': 1.4704632759094238, 'train/bleu': 36.27220736720769, 'validation/accuracy': 0.6941513419151306, 'validation/loss': 1.5010319948196411, 'validation/bleu': 31.01403960420787, 'validation/num_examples': 3000, 'test/accuracy': 0.7114055156707764, 'test/loss': 1.4117215871810913, 'test/bleu': 31.294057267940786, 'test/num_examples': 3003, 'score': 43720.612193107605, 'total_duration': 70509.50488901138, 'accumulated_submission_time': 43720.612193107605, 'accumulated_eval_time': 26783.07488465309, 'accumulated_logging_time': 1.8175811767578125}
I0207 08:51:05.029417 140277229537024 logging_writer.py:48] [124852] accumulated_eval_time=26783.074885, accumulated_logging_time=1.817581, accumulated_submission_time=43720.612193, global_step=124852, preemption_count=0, score=43720.612193, test/accuracy=0.711406, test/bleu=31.294057, test/loss=1.411722, test/num_examples=3003, total_duration=70509.504889, train/accuracy=0.702741, train/bleu=36.272207, train/loss=1.470463, validation/accuracy=0.694151, validation/bleu=31.014040, validation/loss=1.501032, validation/num_examples=3000
I0207 08:51:22.111977 140277221144320 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.7642242312431335, loss=2.5980477333068848
I0207 08:51:57.026940 140277229537024 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.7658137083053589, loss=2.5935356616973877
I0207 08:52:32.011172 140277221144320 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.8003265857696533, loss=2.663059711456299
I0207 08:53:07.048234 140277229537024 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.750234067440033, loss=2.570155143737793
I0207 08:53:42.081601 140277221144320 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.7499563694000244, loss=2.5957672595977783
I0207 08:54:17.073244 140277229537024 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.8056150674819946, loss=2.6598470211029053
I0207 08:54:52.060583 140277221144320 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7464673519134521, loss=2.6281626224517822
I0207 08:55:27.053342 140277229537024 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.7584905028343201, loss=2.594797134399414
I0207 08:56:02.030941 140277221144320 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.7686744928359985, loss=2.6074283123016357
I0207 08:56:37.037586 140277229537024 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.7458227276802063, loss=2.6112608909606934
I0207 08:57:12.028219 140277221144320 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.77567058801651, loss=2.503926992416382
I0207 08:57:47.030246 140277229537024 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7783083319664001, loss=2.627054214477539
I0207 08:58:22.015690 140277221144320 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.7533735632896423, loss=2.614952325820923
I0207 08:58:57.021603 140277229537024 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.7707088589668274, loss=2.539121150970459
I0207 08:59:32.026023 140277221144320 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.7704173922538757, loss=2.644740581512451
I0207 09:00:07.067281 140277229537024 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.746394693851471, loss=2.6130852699279785
I0207 09:00:42.066262 140277221144320 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.7805970311164856, loss=2.527315378189087
I0207 09:01:17.047411 140277229537024 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.7538522481918335, loss=2.616778612136841
I0207 09:01:52.067013 140277221144320 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.7936767339706421, loss=2.5378196239471436
I0207 09:02:27.074717 140277229537024 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.7856838703155518, loss=2.5866684913635254
I0207 09:03:02.098097 140277221144320 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.7502787113189697, loss=2.6528468132019043
I0207 09:03:37.080969 140277229537024 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7482242584228516, loss=2.5625529289245605
I0207 09:04:12.075015 140277221144320 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.7674595713615417, loss=2.6123125553131104
I0207 09:04:47.050381 140277229537024 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.7504211664199829, loss=2.622537136077881
I0207 09:05:05.323271 140446903760704 spec.py:321] Evaluating on the training split.
I0207 09:05:08.340066 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:08:08.920419 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 09:08:11.628505 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:10:39.852915 140446903760704 spec.py:349] Evaluating on the test split.
I0207 09:10:42.558630 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:13:11.817096 140446903760704 submission_runner.py:408] Time since start: 71836.33s, 	Step: 127254, 	{'train/accuracy': 0.7076531052589417, 'train/loss': 1.4463974237442017, 'train/bleu': 36.59637497627496, 'validation/accuracy': 0.6947712898254395, 'validation/loss': 1.4987682104110718, 'validation/bleu': 31.142788279699538, 'validation/num_examples': 3000, 'test/accuracy': 0.7116495370864868, 'test/loss': 1.4090908765792847, 'test/bleu': 31.268713611605445, 'test/num_examples': 3003, 'score': 44560.81832766533, 'total_duration': 71836.32944989204, 'accumulated_submission_time': 44560.81832766533, 'accumulated_eval_time': 27269.56863617897, 'accumulated_logging_time': 1.865821361541748}
I0207 09:13:11.849006 140277221144320 logging_writer.py:48] [127254] accumulated_eval_time=27269.568636, accumulated_logging_time=1.865821, accumulated_submission_time=44560.818328, global_step=127254, preemption_count=0, score=44560.818328, test/accuracy=0.711650, test/bleu=31.268714, test/loss=1.409091, test/num_examples=3003, total_duration=71836.329450, train/accuracy=0.707653, train/bleu=36.596375, train/loss=1.446397, validation/accuracy=0.694771, validation/bleu=31.142788, validation/loss=1.498768, validation/num_examples=3000
I0207 09:13:28.218982 140277229537024 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.7900013327598572, loss=2.5477046966552734
I0207 09:14:03.066933 140277221144320 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.7678957581520081, loss=2.564645767211914
I0207 09:14:38.036386 140277229537024 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.7666229009628296, loss=2.6168205738067627
I0207 09:15:13.024772 140277221144320 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.7591533660888672, loss=2.643882989883423
I0207 09:15:48.001669 140277229537024 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.7547121644020081, loss=2.655067205429077
I0207 09:16:23.008782 140277221144320 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.7922479510307312, loss=2.5920493602752686
I0207 09:16:57.980649 140277229537024 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.760848343372345, loss=2.6297385692596436
I0207 09:17:33.022887 140277221144320 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.7916762232780457, loss=2.5813167095184326
I0207 09:18:08.119915 140277229537024 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.7609366774559021, loss=2.683262348175049
I0207 09:18:43.084870 140277221144320 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.7454854846000671, loss=2.6135189533233643
I0207 09:19:18.088640 140277229537024 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.7690998315811157, loss=2.6198346614837646
I0207 09:19:53.082464 140277221144320 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.775477409362793, loss=2.553978443145752
I0207 09:20:28.118455 140277229537024 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.764028787612915, loss=2.5560243129730225
I0207 09:21:03.130846 140277221144320 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.7587618827819824, loss=2.5923662185668945
I0207 09:21:38.122957 140277229537024 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.747155487537384, loss=2.5979976654052734
I0207 09:22:13.102061 140277221144320 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.7798303365707397, loss=2.6215217113494873
I0207 09:22:48.090304 140277229537024 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.7576534152030945, loss=2.604764223098755
I0207 09:23:23.083692 140277221144320 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.7629518508911133, loss=2.5884509086608887
I0207 09:23:58.103973 140277229537024 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.7636719346046448, loss=2.6497232913970947
I0207 09:24:33.098791 140277221144320 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.7757333517074585, loss=2.6222870349884033
I0207 09:25:08.106214 140277229537024 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.7256977558135986, loss=2.6587069034576416
I0207 09:25:43.099086 140277221144320 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.7873101234436035, loss=2.64457631111145
I0207 09:26:18.115934 140277229537024 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.7614380121231079, loss=2.5677456855773926
I0207 09:26:53.131065 140277221144320 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.7636315822601318, loss=2.642719268798828
I0207 09:27:12.109671 140446903760704 spec.py:321] Evaluating on the training split.
I0207 09:27:15.121569 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:30:23.490058 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 09:30:26.193909 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:32:54.576122 140446903760704 spec.py:349] Evaluating on the test split.
I0207 09:32:57.289455 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:35:25.057718 140446903760704 submission_runner.py:408] Time since start: 73169.57s, 	Step: 129656, 	{'train/accuracy': 0.7090178728103638, 'train/loss': 1.4380403757095337, 'train/bleu': 36.797734485402465, 'validation/accuracy': 0.6948084831237793, 'validation/loss': 1.4995105266571045, 'validation/bleu': 30.993740811222963, 'validation/num_examples': 3000, 'test/accuracy': 0.7116960287094116, 'test/loss': 1.4084151983261108, 'test/bleu': 31.130467907561723, 'test/num_examples': 3003, 'score': 45400.99208855629, 'total_duration': 73169.57006430626, 'accumulated_submission_time': 45400.99208855629, 'accumulated_eval_time': 27762.516602754593, 'accumulated_logging_time': 1.908890962600708}
I0207 09:35:25.090143 140277229537024 logging_writer.py:48] [129656] accumulated_eval_time=27762.516603, accumulated_logging_time=1.908891, accumulated_submission_time=45400.992089, global_step=129656, preemption_count=0, score=45400.992089, test/accuracy=0.711696, test/bleu=31.130468, test/loss=1.408415, test/num_examples=3003, total_duration=73169.570064, train/accuracy=0.709018, train/bleu=36.797734, train/loss=1.438040, validation/accuracy=0.694808, validation/bleu=30.993741, validation/loss=1.499511, validation/num_examples=3000
I0207 09:35:40.768166 140277221144320 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.7738521695137024, loss=2.6600685119628906
I0207 09:36:15.631527 140277229537024 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.7659833431243896, loss=2.589878559112549
I0207 09:36:50.585600 140277221144320 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7638570666313171, loss=2.581183910369873
I0207 09:37:25.569855 140277229537024 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.7550109624862671, loss=2.59814453125
I0207 09:38:00.589637 140277221144320 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.7645581960678101, loss=2.6317832469940186
I0207 09:38:35.582008 140277229537024 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.758000373840332, loss=2.6090471744537354
I0207 09:39:10.578235 140277221144320 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.7561458349227905, loss=2.587181329727173
I0207 09:39:45.551949 140277229537024 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.7930359244346619, loss=2.5367424488067627
I0207 09:40:20.559283 140277221144320 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.7537199854850769, loss=2.5921385288238525
I0207 09:40:55.570530 140277229537024 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.7923926711082458, loss=2.533939838409424
I0207 09:41:30.572389 140277221144320 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.7646854519844055, loss=2.6018805503845215
I0207 09:42:05.584177 140277229537024 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.7790235877037048, loss=2.634645462036133
I0207 09:42:40.567606 140277221144320 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.7592384815216064, loss=2.5191032886505127
I0207 09:43:15.559985 140277229537024 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7646775841712952, loss=2.5278356075286865
I0207 09:43:50.559715 140277221144320 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.7485796213150024, loss=2.538356065750122
I0207 09:44:25.553280 140277229537024 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.7651306986808777, loss=2.595435619354248
I0207 09:45:00.540136 140277221144320 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.7535169124603271, loss=2.599649429321289
I0207 09:45:35.560034 140277229537024 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.751073956489563, loss=2.615314245223999
I0207 09:46:10.589668 140277221144320 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.7581435441970825, loss=2.618488073348999
I0207 09:46:45.568758 140277229537024 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.7532806992530823, loss=2.57906174659729
I0207 09:47:20.553271 140277221144320 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.7603766322135925, loss=2.59619402885437
I0207 09:47:55.542536 140277229537024 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.7772365808486938, loss=2.6330947875976562
I0207 09:48:30.531122 140277221144320 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.7380714416503906, loss=2.5638694763183594
I0207 09:49:05.507732 140277229537024 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.7419346570968628, loss=2.569845676422119
I0207 09:49:25.165290 140446903760704 spec.py:321] Evaluating on the training split.
I0207 09:49:28.165189 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:52:20.964533 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 09:52:23.693790 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:54:50.846216 140446903760704 spec.py:349] Evaluating on the test split.
I0207 09:54:53.558003 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 09:57:26.740450 140446903760704 submission_runner.py:408] Time since start: 74491.25s, 	Step: 132058, 	{'train/accuracy': 0.7075369954109192, 'train/loss': 1.453471302986145, 'train/bleu': 36.62875430446796, 'validation/accuracy': 0.6945481300354004, 'validation/loss': 1.4992551803588867, 'validation/bleu': 31.076465832705026, 'validation/num_examples': 3000, 'test/accuracy': 0.7120911478996277, 'test/loss': 1.4084433317184448, 'test/bleu': 31.249781981597465, 'test/num_examples': 3003, 'score': 46240.980770111084, 'total_duration': 74491.25281620026, 'accumulated_submission_time': 46240.980770111084, 'accumulated_eval_time': 28244.09170150757, 'accumulated_logging_time': 1.952728033065796}
I0207 09:57:26.773761 140277221144320 logging_writer.py:48] [132058] accumulated_eval_time=28244.091702, accumulated_logging_time=1.952728, accumulated_submission_time=46240.980770, global_step=132058, preemption_count=0, score=46240.980770, test/accuracy=0.712091, test/bleu=31.249782, test/loss=1.408443, test/num_examples=3003, total_duration=74491.252816, train/accuracy=0.707537, train/bleu=36.628754, train/loss=1.453471, validation/accuracy=0.694548, validation/bleu=31.076466, validation/loss=1.499255, validation/num_examples=3000
I0207 09:57:41.752041 140277229537024 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.7679183483123779, loss=2.621999502182007
I0207 09:58:16.679073 140277221144320 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.7669014930725098, loss=2.6212785243988037
I0207 09:58:51.653302 140277229537024 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.7449775338172913, loss=2.607431173324585
I0207 09:59:26.679961 140277221144320 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.7656326293945312, loss=2.6392321586608887
I0207 10:00:01.686916 140277229537024 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.7403021454811096, loss=2.542288303375244
I0207 10:00:36.684700 140277221144320 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.7559689879417419, loss=2.6145076751708984
I0207 10:01:11.688839 140277229537024 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.7638991475105286, loss=2.6429390907287598
I0207 10:01:46.732377 140277221144320 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.7371856570243835, loss=2.5539534091949463
I0207 10:02:21.763332 140277229537024 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.7371659278869629, loss=2.5630502700805664
I0207 10:02:56.771966 140277221144320 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.7529944181442261, loss=2.589728832244873
I0207 10:03:31.808771 140277229537024 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.7929933667182922, loss=2.562678813934326
I0207 10:04:06.832995 140277221144320 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.7721131443977356, loss=2.525294303894043
I0207 10:04:41.817007 140277229537024 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.7707744240760803, loss=2.6668827533721924
I0207 10:04:52.741861 140446903760704 spec.py:321] Evaluating on the training split.
I0207 10:04:55.760004 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:07:53.685683 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 10:07:56.408251 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:10:23.932322 140446903760704 spec.py:349] Evaluating on the test split.
I0207 10:10:26.646759 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:12:58.230054 140446903760704 submission_runner.py:408] Time since start: 75422.74s, 	Step: 133333, 	{'train/accuracy': 0.7088993191719055, 'train/loss': 1.4459348917007446, 'train/bleu': 36.586798307686536, 'validation/accuracy': 0.6944240927696228, 'validation/loss': 1.499434232711792, 'validation/bleu': 31.059765499889604, 'validation/num_examples': 3000, 'test/accuracy': 0.7120795249938965, 'test/loss': 1.408614993095398, 'test/bleu': 31.198964185453555, 'test/num_examples': 3003, 'score': 46686.89723396301, 'total_duration': 75422.7423479557, 'accumulated_submission_time': 46686.89723396301, 'accumulated_eval_time': 28729.57975912094, 'accumulated_logging_time': 1.9961745738983154}
I0207 10:12:58.269581 140277221144320 logging_writer.py:48] [133333] accumulated_eval_time=28729.579759, accumulated_logging_time=1.996175, accumulated_submission_time=46686.897234, global_step=133333, preemption_count=0, score=46686.897234, test/accuracy=0.712080, test/bleu=31.198964, test/loss=1.408615, test/num_examples=3003, total_duration=75422.742348, train/accuracy=0.708899, train/bleu=36.586798, train/loss=1.445935, validation/accuracy=0.694424, validation/bleu=31.059765, validation/loss=1.499434, validation/num_examples=3000
I0207 10:12:58.306745 140277229537024 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46686.897234
I0207 10:12:59.814224 140446903760704 checkpoints.py:490] Saving checkpoint at step: 133333
I0207 10:13:04.794841 140446903760704 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_1/checkpoint_133333
I0207 10:13:04.799803 140446903760704 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_1/checkpoint_133333.
I0207 10:13:04.851082 140446903760704 submission_runner.py:583] Tuning trial 1/5
I0207 10:13:04.851298 140446903760704 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0207 10:13:04.857731 140446903760704 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005978592089377344, 'train/loss': 11.188164710998535, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 36.66873526573181, 'total_duration': 895.9530372619629, 'accumulated_submission_time': 36.66873526573181, 'accumulated_eval_time': 859.2842590808868, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2396, {'train/accuracy': 0.41089096665382385, 'train/loss': 4.017301082611084, 'train/bleu': 14.135664486209082, 'validation/accuracy': 0.39612650871276855, 'validation/loss': 4.122348785400391, 'validation/bleu': 9.774756744618458, 'validation/num_examples': 3000, 'test/accuracy': 0.38062867522239685, 'test/loss': 4.3253560066223145, 'test/bleu': 7.894892890419983, 'test/num_examples': 3003, 'score': 876.663633108139, 'total_duration': 2415.0562007427216, 'accumulated_submission_time': 876.663633108139, 'accumulated_eval_time': 1538.2834823131561, 'accumulated_logging_time': 0.03058004379272461, 'global_step': 2396, 'preemption_count': 0}), (4790, {'train/accuracy': 0.542180597782135, 'train/loss': 2.732574224472046, 'train/bleu': 24.602004108236525, 'validation/accuracy': 0.5455852746963501, 'validation/loss': 2.7102763652801514, 'validation/bleu': 20.36131793008979, 'validation/num_examples': 3000, 'test/accuracy': 0.5423159599304199, 'test/loss': 2.7538504600524902, 'test/bleu': 18.71880292122399, 'test/num_examples': 3003, 'score': 1716.6430037021637, 'total_duration': 3714.0869665145874, 'accumulated_submission_time': 1716.6430037021637, 'accumulated_eval_time': 1997.2269372940063, 'accumulated_logging_time': 0.06126093864440918, 'global_step': 4790, 'preemption_count': 0}), (7185, {'train/accuracy': 0.5838678479194641, 'train/loss': 2.3458170890808105, 'train/bleu': 27.234345079265907, 'validation/accuracy': 0.5861551761627197, 'validation/loss': 2.3145346641540527, 'validation/bleu': 23.179417644358757, 'validation/num_examples': 3000, 'test/accuracy': 0.5881703495979309, 'test/loss': 2.31225848197937, 'test/bleu': 21.78799174427264, 'test/num_examples': 3003, 'score': 2556.738527774811, 'total_duration': 4987.44571518898, 'accumulated_submission_time': 2556.738527774811, 'accumulated_eval_time': 2430.385052204132, 'accumulated_logging_time': 0.09026479721069336, 'global_step': 7185, 'preemption_count': 0}), (9582, {'train/accuracy': 0.5891788601875305, 'train/loss': 2.2676243782043457, 'train/bleu': 27.89985486351619, 'validation/accuracy': 0.6069980263710022, 'validation/loss': 2.1352217197418213, 'validation/bleu': 24.639766045857712, 'validation/num_examples': 3000, 'test/accuracy': 0.6104235649108887, 'test/loss': 2.1066174507141113, 'test/bleu': 23.303168552867703, 'test/num_examples': 3003, 'score': 3396.931195497513, 'total_duration': 6269.367269515991, 'accumulated_submission_time': 3396.931195497513, 'accumulated_eval_time': 2872.0077636241913, 'accumulated_logging_time': 0.11701345443725586, 'global_step': 9582, 'preemption_count': 0}), (11980, {'train/accuracy': 0.6027758717536926, 'train/loss': 2.1443228721618652, 'train/bleu': 28.891173233400416, 'validation/accuracy': 0.6202526688575745, 'validation/loss': 2.0171313285827637, 'validation/bleu': 25.276258137532324, 'validation/num_examples': 3000, 'test/accuracy': 0.6257509589195251, 'test/loss': 1.974539041519165, 'test/bleu': 24.60483532445832, 'test/num_examples': 3003, 'score': 4237.160478591919, 'total_duration': 7688.285516738892, 'accumulated_submission_time': 4237.160478591919, 'accumulated_eval_time': 3450.5933167934418, 'accumulated_logging_time': 0.14389681816101074, 'global_step': 11980, 'preemption_count': 0}), (14380, {'train/accuracy': 0.6137219071388245, 'train/loss': 2.0440056324005127, 'train/bleu': 29.763904979834205, 'validation/accuracy': 0.6301347613334656, 'validation/loss': 1.9250251054763794, 'validation/bleu': 26.04915640493588, 'validation/num_examples': 3000, 'test/accuracy': 0.637022852897644, 'test/loss': 1.8806644678115845, 'test/bleu': 25.595357216731195, 'test/num_examples': 3003, 'score': 5077.2845776081085, 'total_duration': 8969.737039804459, 'accumulated_submission_time': 5077.2845776081085, 'accumulated_eval_time': 3891.816184282303, 'accumulated_logging_time': 0.17147159576416016, 'global_step': 14380, 'preemption_count': 0}), (16776, {'train/accuracy': 0.6152881979942322, 'train/loss': 2.0378637313842773, 'train/bleu': 29.833339905367374, 'validation/accuracy': 0.6370410919189453, 'validation/loss': 1.8624820709228516, 'validation/bleu': 26.930205823477618, 'validation/num_examples': 3000, 'test/accuracy': 0.6476788520812988, 'test/loss': 1.806699275970459, 'test/bleu': 25.957416144448224, 'test/num_examples': 3003, 'score': 5917.195971488953, 'total_duration': 10249.626316785812, 'accumulated_submission_time': 5917.195971488953, 'accumulated_eval_time': 4331.687678337097, 'accumulated_logging_time': 0.1997363567352295, 'global_step': 16776, 'preemption_count': 0}), (19173, {'train/accuracy': 0.6377730965614319, 'train/loss': 1.8624444007873535, 'train/bleu': 31.384704872693035, 'validation/accuracy': 0.6448649168014526, 'validation/loss': 1.8179594278335571, 'validation/bleu': 27.341544220468872, 'validation/num_examples': 3000, 'test/accuracy': 0.6542908549308777, 'test/loss': 1.7641087770462036, 'test/bleu': 26.770263755815385, 'test/num_examples': 3003, 'score': 6757.092479228973, 'total_duration': 11546.166801214218, 'accumulated_submission_time': 6757.092479228973, 'accumulated_eval_time': 4788.225700378418, 'accumulated_logging_time': 0.22781896591186523, 'global_step': 19173, 'preemption_count': 0}), (21572, {'train/accuracy': 0.6281996965408325, 'train/loss': 1.9296544790267944, 'train/bleu': 30.43993556295968, 'validation/accuracy': 0.647431492805481, 'validation/loss': 1.7907466888427734, 'validation/bleu': 27.470066942691965, 'validation/num_examples': 3000, 'test/accuracy': 0.6562082767486572, 'test/loss': 1.7340083122253418, 'test/bleu': 26.91141067893076, 'test/num_examples': 3003, 'score': 7597.14856171608, 'total_duration': 12830.313049316406, 'accumulated_submission_time': 7597.14856171608, 'accumulated_eval_time': 5232.208755493164, 'accumulated_logging_time': 0.2553091049194336, 'global_step': 21572, 'preemption_count': 0}), (23972, {'train/accuracy': 0.6257244944572449, 'train/loss': 1.9461606740951538, 'train/bleu': 30.647498348593707, 'validation/accuracy': 0.6491302251815796, 'validation/loss': 1.7739145755767822, 'validation/bleu': 27.639039633589352, 'validation/num_examples': 3000, 'test/accuracy': 0.6590785384178162, 'test/loss': 1.7163658142089844, 'test/bleu': 26.735188246892026, 'test/num_examples': 3003, 'score': 8437.255508899689, 'total_duration': 14123.446489810944, 'accumulated_submission_time': 8437.255508899689, 'accumulated_eval_time': 5685.127123594284, 'accumulated_logging_time': 0.2881033420562744, 'global_step': 23972, 'preemption_count': 0}), (26373, {'train/accuracy': 0.6318857073783875, 'train/loss': 1.8993377685546875, 'train/bleu': 31.127981987220117, 'validation/accuracy': 0.6520439982414246, 'validation/loss': 1.7544596195220947, 'validation/bleu': 27.978040845419148, 'validation/num_examples': 3000, 'test/accuracy': 0.6620068550109863, 'test/loss': 1.7058236598968506, 'test/bleu': 27.445354021922327, 'test/num_examples': 3003, 'score': 9277.202247619629, 'total_duration': 15468.266919612885, 'accumulated_submission_time': 9277.202247619629, 'accumulated_eval_time': 6189.89656496048, 'accumulated_logging_time': 0.316986083984375, 'global_step': 26373, 'preemption_count': 0}), (28775, {'train/accuracy': 0.33275213837623596, 'train/loss': 3.951723337173462, 'train/bleu': 0.5231126249610653, 'validation/accuracy': 0.2997359037399292, 'validation/loss': 4.32647705078125, 'validation/bleu': 0.11509172939415249, 'validation/num_examples': 3000, 'test/accuracy': 0.28972169756889343, 'test/loss': 4.496841907501221, 'test/bleu': 0.07429394634716048, 'test/num_examples': 3003, 'score': 10117.357129573822, 'total_duration': 16794.98263859749, 'accumulated_submission_time': 10117.357129573822, 'accumulated_eval_time': 6676.355022907257, 'accumulated_logging_time': 0.34547972679138184, 'global_step': 28775, 'preemption_count': 0}), (31177, {'train/accuracy': 0.6317818760871887, 'train/loss': 1.9045122861862183, 'train/bleu': 30.54583083674687, 'validation/accuracy': 0.6527383327484131, 'validation/loss': 1.7534050941467285, 'validation/bleu': 27.74894091426883, 'validation/num_examples': 3000, 'test/accuracy': 0.662657618522644, 'test/loss': 1.6983014345169067, 'test/bleu': 27.054145258705955, 'test/num_examples': 3003, 'score': 10957.487800359726, 'total_duration': 18130.350570201874, 'accumulated_submission_time': 10957.487800359726, 'accumulated_eval_time': 7171.487823009491, 'accumulated_logging_time': 0.3740403652191162, 'global_step': 31177, 'preemption_count': 0}), (33579, {'train/accuracy': 0.6354771852493286, 'train/loss': 1.8794300556182861, 'train/bleu': 30.563934941582254, 'validation/accuracy': 0.6564084887504578, 'validation/loss': 1.7356117963790894, 'validation/bleu': 28.160680413584807, 'validation/num_examples': 3000, 'test/accuracy': 0.6678984761238098, 'test/loss': 1.6765241622924805, 'test/bleu': 27.420616054276252, 'test/num_examples': 3003, 'score': 11797.682716608047, 'total_duration': 19464.63546180725, 'accumulated_submission_time': 11797.682716608047, 'accumulated_eval_time': 7665.472952365875, 'accumulated_logging_time': 0.40361714363098145, 'global_step': 33579, 'preemption_count': 0}), (35981, {'train/accuracy': 0.6375195384025574, 'train/loss': 1.8644064664840698, 'train/bleu': 31.009726888604998, 'validation/accuracy': 0.6567308306694031, 'validation/loss': 1.7263554334640503, 'validation/bleu': 28.174935173674044, 'validation/num_examples': 3000, 'test/accuracy': 0.6671082377433777, 'test/loss': 1.6758179664611816, 'test/bleu': 27.54092155263876, 'test/num_examples': 3003, 'score': 12637.7005712986, 'total_duration': 20795.35339331627, 'accumulated_submission_time': 12637.7005712986, 'accumulated_eval_time': 8156.0668778419495, 'accumulated_logging_time': 0.43479251861572266, 'global_step': 35981, 'preemption_count': 0}), (38382, {'train/accuracy': 0.6450705528259277, 'train/loss': 1.804628610610962, 'train/bleu': 32.15894108712716, 'validation/accuracy': 0.6593098640441895, 'validation/loss': 1.7143281698226929, 'validation/bleu': 28.36643041451481, 'validation/num_examples': 3000, 'test/accuracy': 0.6678752303123474, 'test/loss': 1.6616512537002563, 'test/bleu': 27.693778275595207, 'test/num_examples': 3003, 'score': 13477.855075120926, 'total_duration': 22105.53423690796, 'accumulated_submission_time': 13477.855075120926, 'accumulated_eval_time': 8625.987797498703, 'accumulated_logging_time': 0.46489620208740234, 'global_step': 38382, 'preemption_count': 0}), (40783, {'train/accuracy': 0.6396812200546265, 'train/loss': 1.8572800159454346, 'train/bleu': 31.35065259778344, 'validation/accuracy': 0.6599918007850647, 'validation/loss': 1.7028095722198486, 'validation/bleu': 28.272012813109214, 'validation/num_examples': 3000, 'test/accuracy': 0.6715008020401001, 'test/loss': 1.6438584327697754, 'test/bleu': 27.98273853137321, 'test/num_examples': 3003, 'score': 14317.929590463638, 'total_duration': 23452.281606912613, 'accumulated_submission_time': 14317.929590463638, 'accumulated_eval_time': 9132.55176949501, 'accumulated_logging_time': 0.49638891220092773, 'global_step': 40783, 'preemption_count': 0}), (43185, {'train/accuracy': 0.6362661123275757, 'train/loss': 1.8771040439605713, 'train/bleu': 31.457801930764408, 'validation/accuracy': 0.6599298119544983, 'validation/loss': 1.7009010314941406, 'validation/bleu': 28.352906109577713, 'validation/num_examples': 3000, 'test/accuracy': 0.6713729500770569, 'test/loss': 1.6449847221374512, 'test/bleu': 27.856974105756258, 'test/num_examples': 3003, 'score': 15158.089678287506, 'total_duration': 24809.804277181625, 'accumulated_submission_time': 15158.089678287506, 'accumulated_eval_time': 9649.80432677269, 'accumulated_logging_time': 0.5288918018341064, 'global_step': 43185, 'preemption_count': 0}), (45589, {'train/accuracy': 0.590805172920227, 'train/loss': 2.1539804935455322, 'train/bleu': 24.934241026593256, 'validation/accuracy': 0.5927143096923828, 'validation/loss': 2.137026071548462, 'validation/bleu': 20.67777602591015, 'validation/num_examples': 3000, 'test/accuracy': 0.5968043804168701, 'test/loss': 2.1455793380737305, 'test/bleu': 19.25898748849468, 'test/num_examples': 3003, 'score': 15998.09845662117, 'total_duration': 26285.73772263527, 'accumulated_submission_time': 15998.09845662117, 'accumulated_eval_time': 10285.620803833008, 'accumulated_logging_time': 0.5597467422485352, 'global_step': 45589, 'preemption_count': 0}), (47991, {'train/accuracy': 0.6427844762802124, 'train/loss': 1.8317044973373413, 'train/bleu': 31.89963562097901, 'validation/accuracy': 0.6606737375259399, 'validation/loss': 1.6950204372406006, 'validation/bleu': 28.240349978262042, 'validation/num_examples': 3000, 'test/accuracy': 0.6726977229118347, 'test/loss': 1.6335320472717285, 'test/bleu': 27.737813681491367, 'test/num_examples': 3003, 'score': 16838.03125667572, 'total_duration': 27595.45713019371, 'accumulated_submission_time': 16838.03125667572, 'accumulated_eval_time': 10755.301027297974, 'accumulated_logging_time': 0.5908377170562744, 'global_step': 47991, 'preemption_count': 0}), (50392, {'train/accuracy': 0.6582806706428528, 'train/loss': 1.7210654020309448, 'train/bleu': 32.246968132861625, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.6892980337142944, 'validation/bleu': 28.434917155790025, 'validation/num_examples': 3000, 'test/accuracy': 0.6735227704048157, 'test/loss': 1.623759388923645, 'test/bleu': 27.932242256488045, 'test/num_examples': 3003, 'score': 17677.970749616623, 'total_duration': 28924.49044728279, 'accumulated_submission_time': 17677.970749616623, 'accumulated_eval_time': 11244.286452054977, 'accumulated_logging_time': 0.6243085861206055, 'global_step': 50392, 'preemption_count': 0}), (52793, {'train/accuracy': 0.6478515863418579, 'train/loss': 1.799898624420166, 'train/bleu': 31.518275366485874, 'validation/accuracy': 0.6649638414382935, 'validation/loss': 1.6738790273666382, 'validation/bleu': 28.84308670692472, 'validation/num_examples': 3000, 'test/accuracy': 0.6762535572052002, 'test/loss': 1.611672282218933, 'test/bleu': 28.25260805810742, 'test/num_examples': 3003, 'score': 18517.93385744095, 'total_duration': 30310.554755687714, 'accumulated_submission_time': 18517.93385744095, 'accumulated_eval_time': 11790.282139539719, 'accumulated_logging_time': 0.6552319526672363, 'global_step': 52793, 'preemption_count': 0}), (55195, {'train/accuracy': 0.6423264145851135, 'train/loss': 1.8311876058578491, 'train/bleu': 31.520352554833806, 'validation/accuracy': 0.6630296111106873, 'validation/loss': 1.6784354448318481, 'validation/bleu': 28.5659265759097, 'validation/num_examples': 3000, 'test/accuracy': 0.6748591065406799, 'test/loss': 1.615303874015808, 'test/bleu': 28.13716320799326, 'test/num_examples': 3003, 'score': 19358.101722240448, 'total_duration': 31647.902009248734, 'accumulated_submission_time': 19358.101722240448, 'accumulated_eval_time': 12287.342983961105, 'accumulated_logging_time': 0.6947894096374512, 'global_step': 55195, 'preemption_count': 0}), (57598, {'train/accuracy': 0.6510561108589172, 'train/loss': 1.7641416788101196, 'train/bleu': 31.744707806123117, 'validation/accuracy': 0.6652490496635437, 'validation/loss': 1.6621553897857666, 'validation/bleu': 28.623343659957573, 'validation/num_examples': 3000, 'test/accuracy': 0.6766951680183411, 'test/loss': 1.596364974975586, 'test/bleu': 28.10985823697703, 'test/num_examples': 3003, 'score': 20198.159299850464, 'total_duration': 32949.93321561813, 'accumulated_submission_time': 20198.159299850464, 'accumulated_eval_time': 12749.21016407013, 'accumulated_logging_time': 0.7266678810119629, 'global_step': 57598, 'preemption_count': 0}), (59999, {'train/accuracy': 0.644768476486206, 'train/loss': 1.8109740018844604, 'train/bleu': 31.910825778247982, 'validation/accuracy': 0.6663773655891418, 'validation/loss': 1.6560256481170654, 'validation/bleu': 28.56648746349024, 'validation/num_examples': 3000, 'test/accuracy': 0.6790773272514343, 'test/loss': 1.5836652517318726, 'test/bleu': 28.489317356034956, 'test/num_examples': 3003, 'score': 21038.290306568146, 'total_duration': 34302.990511894226, 'accumulated_submission_time': 21038.290306568146, 'accumulated_eval_time': 13262.024190664291, 'accumulated_logging_time': 0.7604336738586426, 'global_step': 59999, 'preemption_count': 0}), (62401, {'train/accuracy': 0.6491748094558716, 'train/loss': 1.7910131216049194, 'train/bleu': 32.01965766582803, 'validation/accuracy': 0.669477105140686, 'validation/loss': 1.6406055688858032, 'validation/bleu': 29.233560910622813, 'validation/num_examples': 3000, 'test/accuracy': 0.6824356913566589, 'test/loss': 1.5777497291564941, 'test/bleu': 28.67643381041361, 'test/num_examples': 3003, 'score': 21878.360268354416, 'total_duration': 35608.370332956314, 'accumulated_submission_time': 21878.360268354416, 'accumulated_eval_time': 13727.225650072098, 'accumulated_logging_time': 0.7934637069702148, 'global_step': 62401, 'preemption_count': 0}), (64804, {'train/accuracy': 0.6520285606384277, 'train/loss': 1.7609772682189941, 'train/bleu': 31.971719271704508, 'validation/accuracy': 0.6713989973068237, 'validation/loss': 1.6386438608169556, 'validation/bleu': 29.0899984850159, 'validation/num_examples': 3000, 'test/accuracy': 0.6805996298789978, 'test/loss': 1.5786640644073486, 'test/bleu': 28.657661996239458, 'test/num_examples': 3003, 'score': 22718.57821083069, 'total_duration': 37024.85886883736, 'accumulated_submission_time': 22718.57821083069, 'accumulated_eval_time': 14303.382984161377, 'accumulated_logging_time': 0.8290464878082275, 'global_step': 64804, 'preemption_count': 0}), (67206, {'train/accuracy': 0.6519597768783569, 'train/loss': 1.7757725715637207, 'train/bleu': 31.91697570224685, 'validation/accuracy': 0.670419454574585, 'validation/loss': 1.638580083847046, 'validation/bleu': 29.23616891361002, 'validation/num_examples': 3000, 'test/accuracy': 0.6822613477706909, 'test/loss': 1.5671225786209106, 'test/bleu': 28.72514626657771, 'test/num_examples': 3003, 'score': 23558.55596637726, 'total_duration': 38350.22704553604, 'accumulated_submission_time': 23558.55596637726, 'accumulated_eval_time': 14788.661399126053, 'accumulated_logging_time': 0.8624668121337891, 'global_step': 67206, 'preemption_count': 0}), (69609, {'train/accuracy': 0.6634093523025513, 'train/loss': 1.6923391819000244, 'train/bleu': 32.762389538704355, 'validation/accuracy': 0.6716221570968628, 'validation/loss': 1.6273186206817627, 'validation/bleu': 29.347775662744024, 'validation/num_examples': 3000, 'test/accuracy': 0.6841090321540833, 'test/loss': 1.5585025548934937, 'test/bleu': 29.071967186401448, 'test/num_examples': 3003, 'score': 24398.73073887825, 'total_duration': 39688.82049059868, 'accumulated_submission_time': 24398.73073887825, 'accumulated_eval_time': 15286.969474315643, 'accumulated_logging_time': 0.8974158763885498, 'global_step': 69609, 'preemption_count': 0}), (72012, {'train/accuracy': 0.6599642634391785, 'train/loss': 1.7183440923690796, 'train/bleu': 32.44681045615368, 'validation/accuracy': 0.6714857816696167, 'validation/loss': 1.6171070337295532, 'validation/bleu': 29.12857090191241, 'validation/num_examples': 3000, 'test/accuracy': 0.6856196522712708, 'test/loss': 1.5455368757247925, 'test/bleu': 28.909133725017643, 'test/num_examples': 3003, 'score': 25238.861780643463, 'total_duration': 41023.935676813126, 'accumulated_submission_time': 25238.861780643463, 'accumulated_eval_time': 15781.841740846634, 'accumulated_logging_time': 0.9326050281524658, 'global_step': 72012, 'preemption_count': 0}), (74415, {'train/accuracy': 0.6568201780319214, 'train/loss': 1.7489362955093384, 'train/bleu': 32.56595782076231, 'validation/accuracy': 0.6759618520736694, 'validation/loss': 1.6061208248138428, 'validation/bleu': 29.63801741455089, 'validation/num_examples': 3000, 'test/accuracy': 0.6896287202835083, 'test/loss': 1.5347216129302979, 'test/bleu': 29.456325345785103, 'test/num_examples': 3003, 'score': 26079.096412420273, 'total_duration': 42389.313524484634, 'accumulated_submission_time': 26079.096412420273, 'accumulated_eval_time': 16306.871610403061, 'accumulated_logging_time': 0.9680724143981934, 'global_step': 74415, 'preemption_count': 0}), (76817, {'train/accuracy': 0.661069929599762, 'train/loss': 1.7087233066558838, 'train/bleu': 32.60740521012439, 'validation/accuracy': 0.6762098073959351, 'validation/loss': 1.603691577911377, 'validation/bleu': 29.3602712239802, 'validation/num_examples': 3000, 'test/accuracy': 0.6898146867752075, 'test/loss': 1.5289326906204224, 'test/bleu': 29.37583258832015, 'test/num_examples': 3003, 'score': 26919.076508283615, 'total_duration': 43707.509570360184, 'accumulated_submission_time': 26919.076508283615, 'accumulated_eval_time': 16784.96683216095, 'accumulated_logging_time': 1.0114808082580566, 'global_step': 76817, 'preemption_count': 0}), (79219, {'train/accuracy': 0.6592926383018494, 'train/loss': 1.7247998714447021, 'train/bleu': 32.536791630899906, 'validation/accuracy': 0.6757262945175171, 'validation/loss': 1.6033855676651, 'validation/bleu': 29.62497735225051, 'validation/num_examples': 3000, 'test/accuracy': 0.6900470852851868, 'test/loss': 1.5301181077957153, 'test/bleu': 29.416101277503813, 'test/num_examples': 3003, 'score': 27759.00503540039, 'total_duration': 45018.331510305405, 'accumulated_submission_time': 27759.00503540039, 'accumulated_eval_time': 17255.74615764618, 'accumulated_logging_time': 1.0472896099090576, 'global_step': 79219, 'preemption_count': 0}), (81622, {'train/accuracy': 0.6859181523323059, 'train/loss': 1.5658107995986938, 'train/bleu': 34.2604911187917, 'validation/accuracy': 0.6783672571182251, 'validation/loss': 1.5894643068313599, 'validation/bleu': 29.177389345403792, 'validation/num_examples': 3000, 'test/accuracy': 0.6895938515663147, 'test/loss': 1.5182905197143555, 'test/bleu': 29.071133285384303, 'test/num_examples': 3003, 'score': 28599.207001686096, 'total_duration': 46428.168867349625, 'accumulated_submission_time': 28599.207001686096, 'accumulated_eval_time': 17825.267672777176, 'accumulated_logging_time': 1.0836036205291748, 'global_step': 81622, 'preemption_count': 0}), (84025, {'train/accuracy': 0.664563000202179, 'train/loss': 1.696059226989746, 'train/bleu': 33.119290385594596, 'validation/accuracy': 0.6789996027946472, 'validation/loss': 1.582720160484314, 'validation/bleu': 29.565914486020333, 'validation/num_examples': 3000, 'test/accuracy': 0.6921852231025696, 'test/loss': 1.5084973573684692, 'test/bleu': 29.47494867004409, 'test/num_examples': 3003, 'score': 29439.43800854683, 'total_duration': 47767.677359580994, 'accumulated_submission_time': 29439.43800854683, 'accumulated_eval_time': 18324.430958986282, 'accumulated_logging_time': 1.1216096878051758, 'global_step': 84025, 'preemption_count': 0}), (86427, {'train/accuracy': 0.6629806756973267, 'train/loss': 1.7091134786605835, 'train/bleu': 32.61050182170082, 'validation/accuracy': 0.6798179745674133, 'validation/loss': 1.5794235467910767, 'validation/bleu': 29.637436093481103, 'validation/num_examples': 3000, 'test/accuracy': 0.6930103302001953, 'test/loss': 1.5063138008117676, 'test/bleu': 29.577308672706145, 'test/num_examples': 3003, 'score': 30279.434716939926, 'total_duration': 49129.6320040226, 'accumulated_submission_time': 30279.434716939926, 'accumulated_eval_time': 18846.272495031357, 'accumulated_logging_time': 1.1595826148986816, 'global_step': 86427, 'preemption_count': 0}), (88829, {'train/accuracy': 0.6717724204063416, 'train/loss': 1.6399476528167725, 'train/bleu': 33.690016325414604, 'validation/accuracy': 0.6823846101760864, 'validation/loss': 1.570753574371338, 'validation/bleu': 30.108637798690488, 'validation/num_examples': 3000, 'test/accuracy': 0.6959851384162903, 'test/loss': 1.4950261116027832, 'test/bleu': 29.989663189851356, 'test/num_examples': 3003, 'score': 31119.40724849701, 'total_duration': 50471.155947208405, 'accumulated_submission_time': 31119.40724849701, 'accumulated_eval_time': 19347.71030664444, 'accumulated_logging_time': 1.1959412097930908, 'global_step': 88829, 'preemption_count': 0}), (91231, {'train/accuracy': 0.6652762293815613, 'train/loss': 1.6848218441009521, 'train/bleu': 33.516918895580865, 'validation/accuracy': 0.682136595249176, 'validation/loss': 1.5662659406661987, 'validation/bleu': 29.97718632659504, 'validation/num_examples': 3000, 'test/accuracy': 0.6975887417793274, 'test/loss': 1.4849611520767212, 'test/bleu': 30.002012447607637, 'test/num_examples': 3003, 'score': 31959.536118268967, 'total_duration': 51773.93786597252, 'accumulated_submission_time': 31959.536118268967, 'accumulated_eval_time': 19810.24594926834, 'accumulated_logging_time': 1.2348592281341553, 'global_step': 91231, 'preemption_count': 0}), (93633, {'train/accuracy': 0.6700052618980408, 'train/loss': 1.6654683351516724, 'train/bleu': 33.97726970739959, 'validation/accuracy': 0.6840088963508606, 'validation/loss': 1.5550308227539062, 'validation/bleu': 30.201155231440644, 'validation/num_examples': 3000, 'test/accuracy': 0.6991226673126221, 'test/loss': 1.4706521034240723, 'test/bleu': 30.245471742788, 'test/num_examples': 3003, 'score': 32799.64951753616, 'total_duration': 53139.55933403969, 'accumulated_submission_time': 32799.64951753616, 'accumulated_eval_time': 20335.630873441696, 'accumulated_logging_time': 1.280862808227539, 'global_step': 93633, 'preemption_count': 0}), (96035, {'train/accuracy': 0.6742818355560303, 'train/loss': 1.6293132305145264, 'train/bleu': 33.98130465498545, 'validation/accuracy': 0.6850255727767944, 'validation/loss': 1.5457754135131836, 'validation/bleu': 30.021522350598442, 'validation/num_examples': 3000, 'test/accuracy': 0.6990413069725037, 'test/loss': 1.4698675870895386, 'test/bleu': 30.285427126973314, 'test/num_examples': 3003, 'score': 33639.54993915558, 'total_duration': 54495.446476221085, 'accumulated_submission_time': 33639.54993915558, 'accumulated_eval_time': 20851.502648115158, 'accumulated_logging_time': 1.319817066192627, 'global_step': 96035, 'preemption_count': 0}), (98436, {'train/accuracy': 0.6713905930519104, 'train/loss': 1.6505440473556519, 'train/bleu': 33.99387977317795, 'validation/accuracy': 0.685149610042572, 'validation/loss': 1.544081687927246, 'validation/bleu': 30.28188354970151, 'validation/num_examples': 3000, 'test/accuracy': 0.7021788358688354, 'test/loss': 1.4656221866607666, 'test/bleu': 30.44932657753321, 'test/num_examples': 3003, 'score': 34479.54863166809, 'total_duration': 55827.9131128788, 'accumulated_submission_time': 34479.54863166809, 'accumulated_eval_time': 21343.85585975647, 'accumulated_logging_time': 1.3581314086914062, 'global_step': 98436, 'preemption_count': 0}), (100838, {'train/accuracy': 0.6884165406227112, 'train/loss': 1.5480619668960571, 'train/bleu': 35.2754136989264, 'validation/accuracy': 0.6868482828140259, 'validation/loss': 1.538313865661621, 'validation/bleu': 30.528489437300816, 'validation/num_examples': 3000, 'test/accuracy': 0.7017489075660706, 'test/loss': 1.4601340293884277, 'test/bleu': 30.560988154441777, 'test/num_examples': 3003, 'score': 35319.508835315704, 'total_duration': 57193.07353281975, 'accumulated_submission_time': 35319.508835315704, 'accumulated_eval_time': 21868.93807411194, 'accumulated_logging_time': 1.398036003112793, 'global_step': 100838, 'preemption_count': 0}), (103240, {'train/accuracy': 0.6779972314834595, 'train/loss': 1.604690432548523, 'train/bleu': 34.50147922318023, 'validation/accuracy': 0.6882741451263428, 'validation/loss': 1.5299122333526611, 'validation/bleu': 30.41491065908115, 'validation/num_examples': 3000, 'test/accuracy': 0.7038870453834534, 'test/loss': 1.4475866556167603, 'test/bleu': 30.51891889328535, 'test/num_examples': 3003, 'score': 36159.6839966774, 'total_duration': 58519.62125611305, 'accumulated_submission_time': 36159.6839966774, 'accumulated_eval_time': 22355.186529397964, 'accumulated_logging_time': 1.44234037399292, 'global_step': 103240, 'preemption_count': 0}), (105642, {'train/accuracy': 0.6837121248245239, 'train/loss': 1.5825554132461548, 'train/bleu': 34.452289799932295, 'validation/accuracy': 0.6886957287788391, 'validation/loss': 1.523957371711731, 'validation/bleu': 30.520724395170017, 'validation/num_examples': 3000, 'test/accuracy': 0.7051188349723816, 'test/loss': 1.4421643018722534, 'test/bleu': 30.512686812294906, 'test/num_examples': 3003, 'score': 36999.760825634, 'total_duration': 59843.622671842575, 'accumulated_submission_time': 36999.760825634, 'accumulated_eval_time': 22838.996133089066, 'accumulated_logging_time': 1.4821085929870605, 'global_step': 105642, 'preemption_count': 0}), (108044, {'train/accuracy': 0.6904653310775757, 'train/loss': 1.5444891452789307, 'train/bleu': 35.465164598367224, 'validation/accuracy': 0.6911135315895081, 'validation/loss': 1.5180892944335938, 'validation/bleu': 30.564646606616183, 'validation/num_examples': 3000, 'test/accuracy': 0.7067108154296875, 'test/loss': 1.433881402015686, 'test/bleu': 30.808667488476935, 'test/num_examples': 3003, 'score': 37839.9554643631, 'total_duration': 61181.76550936699, 'accumulated_submission_time': 37839.9554643631, 'accumulated_eval_time': 23336.82561659813, 'accumulated_logging_time': 1.5219931602478027, 'global_step': 108044, 'preemption_count': 0}), (110446, {'train/accuracy': 0.6886968612670898, 'train/loss': 1.552962303161621, 'train/bleu': 34.8873148481868, 'validation/accuracy': 0.6909399628639221, 'validation/loss': 1.516938328742981, 'validation/bleu': 30.795585694647226, 'validation/num_examples': 3000, 'test/accuracy': 0.7068270444869995, 'test/loss': 1.429835557937622, 'test/bleu': 30.794322137941766, 'test/num_examples': 3003, 'score': 38680.11610341072, 'total_duration': 62524.57753229141, 'accumulated_submission_time': 38680.11610341072, 'accumulated_eval_time': 23839.350203037262, 'accumulated_logging_time': 1.5703990459442139, 'global_step': 110446, 'preemption_count': 0}), (112846, {'train/accuracy': 0.7077298760414124, 'train/loss': 1.4527026414871216, 'train/bleu': 36.72570097184537, 'validation/accuracy': 0.6913243532180786, 'validation/loss': 1.5172661542892456, 'validation/bleu': 30.76716783706688, 'validation/num_examples': 3000, 'test/accuracy': 0.7065481543540955, 'test/loss': 1.4316315650939941, 'test/bleu': 30.935883540712304, 'test/num_examples': 3003, 'score': 39520.2310898304, 'total_duration': 63860.211265563965, 'accumulated_submission_time': 39520.2310898304, 'accumulated_eval_time': 24334.75069284439, 'accumulated_logging_time': 1.6107723712921143, 'global_step': 112846, 'preemption_count': 0}), (115248, {'train/accuracy': 0.69832843542099, 'train/loss': 1.5016064643859863, 'train/bleu': 35.53555504677267, 'validation/accuracy': 0.693717360496521, 'validation/loss': 1.5081539154052734, 'validation/bleu': 30.621397086007867, 'validation/num_examples': 3000, 'test/accuracy': 0.70927894115448, 'test/loss': 1.4228955507278442, 'test/bleu': 30.847111719589698, 'test/num_examples': 3003, 'score': 40360.24406290054, 'total_duration': 65183.5163064003, 'accumulated_submission_time': 40360.24406290054, 'accumulated_eval_time': 24817.927864313126, 'accumulated_logging_time': 1.6504244804382324, 'global_step': 115248, 'preemption_count': 0}), (117649, {'train/accuracy': 0.6946881413459778, 'train/loss': 1.516922116279602, 'train/bleu': 35.48660590814062, 'validation/accuracy': 0.6925890445709229, 'validation/loss': 1.5068556070327759, 'validation/bleu': 31.07297394971343, 'validation/num_examples': 3000, 'test/accuracy': 0.7097670435905457, 'test/loss': 1.4213274717330933, 'test/bleu': 30.929849886140154, 'test/num_examples': 3003, 'score': 41200.15984940529, 'total_duration': 66529.20644688606, 'accumulated_submission_time': 41200.15984940529, 'accumulated_eval_time': 25323.58639740944, 'accumulated_logging_time': 1.6914031505584717, 'global_step': 117649, 'preemption_count': 0}), (120050, {'train/accuracy': 0.7077162861824036, 'train/loss': 1.4448779821395874, 'train/bleu': 35.699938968051285, 'validation/accuracy': 0.693221390247345, 'validation/loss': 1.5050368309020996, 'validation/bleu': 30.882217351535818, 'validation/num_examples': 3000, 'test/accuracy': 0.7109523415565491, 'test/loss': 1.4153249263763428, 'test/bleu': 31.28327925826771, 'test/num_examples': 3003, 'score': 42040.353865385056, 'total_duration': 67854.64235019684, 'accumulated_submission_time': 42040.353865385056, 'accumulated_eval_time': 25808.707792043686, 'accumulated_logging_time': 1.7335550785064697, 'global_step': 120050, 'preemption_count': 0}), (122450, {'train/accuracy': 0.7041738033294678, 'train/loss': 1.4591567516326904, 'train/bleu': 36.04297490759636, 'validation/accuracy': 0.6942257285118103, 'validation/loss': 1.5022872686386108, 'validation/bleu': 31.002648152503227, 'validation/num_examples': 3000, 'test/accuracy': 0.7108477354049683, 'test/loss': 1.4121458530426025, 'test/bleu': 31.20686190559411, 'test/num_examples': 3003, 'score': 42880.47454190254, 'total_duration': 69184.54448390007, 'accumulated_submission_time': 42880.47454190254, 'accumulated_eval_time': 26298.369471549988, 'accumulated_logging_time': 1.7748847007751465, 'global_step': 122450, 'preemption_count': 0}), (124852, {'train/accuracy': 0.7027409076690674, 'train/loss': 1.4704632759094238, 'train/bleu': 36.27220736720769, 'validation/accuracy': 0.6941513419151306, 'validation/loss': 1.5010319948196411, 'validation/bleu': 31.01403960420787, 'validation/num_examples': 3000, 'test/accuracy': 0.7114055156707764, 'test/loss': 1.4117215871810913, 'test/bleu': 31.294057267940786, 'test/num_examples': 3003, 'score': 43720.612193107605, 'total_duration': 70509.50488901138, 'accumulated_submission_time': 43720.612193107605, 'accumulated_eval_time': 26783.07488465309, 'accumulated_logging_time': 1.8175811767578125, 'global_step': 124852, 'preemption_count': 0}), (127254, {'train/accuracy': 0.7076531052589417, 'train/loss': 1.4463974237442017, 'train/bleu': 36.59637497627496, 'validation/accuracy': 0.6947712898254395, 'validation/loss': 1.4987682104110718, 'validation/bleu': 31.142788279699538, 'validation/num_examples': 3000, 'test/accuracy': 0.7116495370864868, 'test/loss': 1.4090908765792847, 'test/bleu': 31.268713611605445, 'test/num_examples': 3003, 'score': 44560.81832766533, 'total_duration': 71836.32944989204, 'accumulated_submission_time': 44560.81832766533, 'accumulated_eval_time': 27269.56863617897, 'accumulated_logging_time': 1.865821361541748, 'global_step': 127254, 'preemption_count': 0}), (129656, {'train/accuracy': 0.7090178728103638, 'train/loss': 1.4380403757095337, 'train/bleu': 36.797734485402465, 'validation/accuracy': 0.6948084831237793, 'validation/loss': 1.4995105266571045, 'validation/bleu': 30.993740811222963, 'validation/num_examples': 3000, 'test/accuracy': 0.7116960287094116, 'test/loss': 1.4084151983261108, 'test/bleu': 31.130467907561723, 'test/num_examples': 3003, 'score': 45400.99208855629, 'total_duration': 73169.57006430626, 'accumulated_submission_time': 45400.99208855629, 'accumulated_eval_time': 27762.516602754593, 'accumulated_logging_time': 1.908890962600708, 'global_step': 129656, 'preemption_count': 0}), (132058, {'train/accuracy': 0.7075369954109192, 'train/loss': 1.453471302986145, 'train/bleu': 36.62875430446796, 'validation/accuracy': 0.6945481300354004, 'validation/loss': 1.4992551803588867, 'validation/bleu': 31.076465832705026, 'validation/num_examples': 3000, 'test/accuracy': 0.7120911478996277, 'test/loss': 1.4084433317184448, 'test/bleu': 31.249781981597465, 'test/num_examples': 3003, 'score': 46240.980770111084, 'total_duration': 74491.25281620026, 'accumulated_submission_time': 46240.980770111084, 'accumulated_eval_time': 28244.09170150757, 'accumulated_logging_time': 1.952728033065796, 'global_step': 132058, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7088993191719055, 'train/loss': 1.4459348917007446, 'train/bleu': 36.586798307686536, 'validation/accuracy': 0.6944240927696228, 'validation/loss': 1.499434232711792, 'validation/bleu': 31.059765499889604, 'validation/num_examples': 3000, 'test/accuracy': 0.7120795249938965, 'test/loss': 1.408614993095398, 'test/bleu': 31.198964185453555, 'test/num_examples': 3003, 'score': 46686.89723396301, 'total_duration': 75422.7423479557, 'accumulated_submission_time': 46686.89723396301, 'accumulated_eval_time': 28729.57975912094, 'accumulated_logging_time': 1.9961745738983154, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0207 10:13:04.858014 140446903760704 submission_runner.py:586] Timing: 46686.89723396301
I0207 10:13:04.858133 140446903760704 submission_runner.py:588] Total number of evals: 57
I0207 10:13:04.858194 140446903760704 submission_runner.py:589] ====================
I0207 10:13:04.858244 140446903760704 submission_runner.py:542] Using RNG seed 1540897543
I0207 10:13:04.860307 140446903760704 submission_runner.py:551] --- Tuning run 2/5 ---
I0207 10:13:04.860436 140446903760704 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_2.
I0207 10:13:04.860975 140446903760704 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_2/hparams.json.
I0207 10:13:04.861878 140446903760704 submission_runner.py:206] Initializing dataset.
I0207 10:13:04.864713 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 10:13:04.868450 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0207 10:13:04.912364 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 10:13:05.576543 140446903760704 submission_runner.py:213] Initializing model.
I0207 10:13:12.698155 140446903760704 submission_runner.py:255] Initializing optimizer.
I0207 10:13:13.581251 140446903760704 submission_runner.py:262] Initializing metrics bundle.
I0207 10:13:13.581509 140446903760704 submission_runner.py:280] Initializing checkpoint and logger.
I0207 10:13:13.582759 140446903760704 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/wmt_jax/trial_2 with prefix checkpoint_
I0207 10:13:13.582924 140446903760704 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_2/meta_data_0.json.
I0207 10:13:13.583240 140446903760704 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0207 10:13:13.583352 140446903760704 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0207 10:13:14.083868 140446903760704 logger_utils.py:220] Unable to record git information. Continuing without it.
I0207 10:13:14.550698 140446903760704 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_2/flags_0.json.
I0207 10:13:14.557896 140446903760704 submission_runner.py:314] Starting training loop.
I0207 10:13:43.059222 140277145609984 logging_writer.py:48] [0] global_step=0, grad_norm=4.979386329650879, loss=11.153432846069336
I0207 10:13:43.069642 140446903760704 spec.py:321] Evaluating on the training split.
I0207 10:13:45.764985 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:18:23.192391 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 10:18:25.900099 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:23:02.685755 140446903760704 spec.py:349] Evaluating on the test split.
I0207 10:23:05.397179 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:27:41.911944 140446903760704 submission_runner.py:408] Time since start: 867.35s, 	Step: 1, 	{'train/accuracy': 0.0006397879915311933, 'train/loss': 11.187285423278809, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 28.511714935302734, 'total_duration': 867.353991985321, 'accumulated_submission_time': 28.511714935302734, 'accumulated_eval_time': 838.8422293663025, 'accumulated_logging_time': 0}
I0207 10:27:41.921076 140277154002688 logging_writer.py:48] [1] accumulated_eval_time=838.842229, accumulated_logging_time=0, accumulated_submission_time=28.511715, global_step=1, preemption_count=0, score=28.511715, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.191028, test/num_examples=3003, total_duration=867.353992, train/accuracy=0.000640, train/bleu=0.000000, train/loss=11.187285, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.190282, validation/num_examples=3000
I0207 10:28:16.879546 140277145609984 logging_writer.py:48] [100] global_step=100, grad_norm=0.2830871641635895, loss=9.076077461242676
I0207 10:28:51.831475 140277154002688 logging_writer.py:48] [200] global_step=200, grad_norm=0.20622913539409637, loss=8.751753807067871
I0207 10:29:26.847995 140277145609984 logging_writer.py:48] [300] global_step=300, grad_norm=0.854816734790802, loss=8.385588645935059
I0207 10:30:01.877696 140277154002688 logging_writer.py:48] [400] global_step=400, grad_norm=0.705528736114502, loss=8.07802677154541
I0207 10:30:36.949736 140277145609984 logging_writer.py:48] [500] global_step=500, grad_norm=0.8811237812042236, loss=7.879554271697998
I0207 10:31:12.025639 140277154002688 logging_writer.py:48] [600] global_step=600, grad_norm=0.8429285883903503, loss=7.705249309539795
I0207 10:31:47.132601 140277145609984 logging_writer.py:48] [700] global_step=700, grad_norm=0.5660996437072754, loss=7.5036540031433105
I0207 10:32:22.226217 140277154002688 logging_writer.py:48] [800] global_step=800, grad_norm=0.5068932175636292, loss=7.30837869644165
I0207 10:32:57.325185 140277145609984 logging_writer.py:48] [900] global_step=900, grad_norm=0.8021007180213928, loss=7.205637454986572
I0207 10:33:32.423737 140277154002688 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5662404298782349, loss=6.953925132751465
I0207 10:34:07.514859 140277145609984 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6298307180404663, loss=6.843849182128906
I0207 10:34:42.620455 140277154002688 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5134739875793457, loss=6.692743301391602
I0207 10:35:17.708385 140277145609984 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.759030282497406, loss=6.647438049316406
I0207 10:35:52.820119 140277154002688 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6347119212150574, loss=6.5417304039001465
I0207 10:36:27.923683 140277145609984 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5585836172103882, loss=6.406623363494873
I0207 10:37:03.028561 140277154002688 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6429976224899292, loss=6.313035488128662
I0207 10:37:38.171550 140277145609984 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6418275833129883, loss=6.2541823387146
I0207 10:38:13.389186 140277154002688 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7275510430335999, loss=6.13297700881958
I0207 10:38:48.514943 140277145609984 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5948456525802612, loss=6.031875133514404
I0207 10:39:23.644405 140277154002688 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7339063286781311, loss=5.978402614593506
I0207 10:39:58.763607 140277145609984 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6789236664772034, loss=5.789960861206055
I0207 10:40:33.878546 140277154002688 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5377997159957886, loss=5.72546911239624
I0207 10:41:08.981194 140277145609984 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.6752446889877319, loss=5.663984298706055
I0207 10:41:42.052027 140446903760704 spec.py:321] Evaluating on the training split.
I0207 10:41:45.070326 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:44:48.864473 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 10:44:51.577658 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:47:50.091287 140446903760704 spec.py:349] Evaluating on the test split.
I0207 10:47:52.818354 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 10:51:23.097066 140446903760704 submission_runner.py:408] Time since start: 2288.54s, 	Step: 2396, 	{'train/accuracy': 0.42042139172554016, 'train/loss': 3.955871820449829, 'train/bleu': 14.03031101364103, 'validation/accuracy': 0.408426433801651, 'validation/loss': 4.070903778076172, 'validation/bleu': 9.933766386758254, 'validation/num_examples': 3000, 'test/accuracy': 0.3910173773765564, 'test/loss': 4.271887302398682, 'test/bleu': 8.25107154164634, 'test/num_examples': 3003, 'score': 868.5572006702423, 'total_duration': 2288.5391099452972, 'accumulated_submission_time': 868.5572006702423, 'accumulated_eval_time': 1419.8872134685516, 'accumulated_logging_time': 0.019253969192504883}
I0207 10:51:23.111949 140277154002688 logging_writer.py:48] [2396] accumulated_eval_time=1419.887213, accumulated_logging_time=0.019254, accumulated_submission_time=868.557201, global_step=2396, preemption_count=0, score=868.557201, test/accuracy=0.391017, test/bleu=8.251072, test/loss=4.271887, test/num_examples=3003, total_duration=2288.539110, train/accuracy=0.420421, train/bleu=14.030311, train/loss=3.955872, validation/accuracy=0.408426, validation/bleu=9.933766, validation/loss=4.070904, validation/num_examples=3000
I0207 10:51:24.873138 140277145609984 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5764522552490234, loss=5.581533908843994
I0207 10:51:59.823781 140277154002688 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5455609560012817, loss=5.53609037399292
I0207 10:52:34.896240 140277145609984 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6013534069061279, loss=5.440680027008057
I0207 10:53:09.999355 140277154002688 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7028096914291382, loss=5.449977874755859
I0207 10:53:45.113520 140277145609984 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5200510025024414, loss=5.254239559173584
I0207 10:54:20.208275 140277154002688 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5108324289321899, loss=5.3133320808410645
I0207 10:54:55.306388 140277145609984 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6266188025474548, loss=5.264730930328369
I0207 10:55:30.393201 140277154002688 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5019330978393555, loss=5.152255535125732
I0207 10:56:05.519024 140277145609984 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6929216384887695, loss=5.097206115722656
I0207 10:56:40.600328 140277154002688 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.49184009432792664, loss=5.0078325271606445
I0207 10:57:15.680206 140277145609984 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5439842343330383, loss=5.053866863250732
I0207 10:57:50.777040 140277154002688 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5052779912948608, loss=4.940250396728516
I0207 10:58:25.839508 140277145609984 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.44197699427604675, loss=4.952956676483154
I0207 10:59:00.895703 140277154002688 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.43984687328338623, loss=4.8659820556640625
I0207 10:59:35.975081 140277145609984 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.47163188457489014, loss=4.918844223022461
I0207 11:00:11.050855 140277154002688 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.5009783506393433, loss=4.972000598907471
I0207 11:00:46.148924 140277145609984 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.42881155014038086, loss=4.883396625518799
I0207 11:01:21.230623 140277154002688 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.44617587327957153, loss=4.841017723083496
I0207 11:01:56.325523 140277145609984 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.40296322107315063, loss=4.870784282684326
I0207 11:02:31.475282 140277154002688 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.39647388458251953, loss=4.8311262130737305
I0207 11:03:06.612089 140277145609984 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.41150352358818054, loss=4.837864875793457
I0207 11:03:41.733877 140277154002688 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.40204569697380066, loss=4.744564056396484
I0207 11:04:16.810195 140277145609984 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3845476508140564, loss=4.825039863586426
I0207 11:04:51.908247 140277154002688 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3799550533294678, loss=4.72027587890625
I0207 11:05:23.217066 140446903760704 spec.py:321] Evaluating on the training split.
I0207 11:05:26.253358 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:08:11.592430 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 11:08:14.325513 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:10:56.133031 140446903760704 spec.py:349] Evaluating on the test split.
I0207 11:10:58.860544 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:13:30.524949 140446903760704 submission_runner.py:408] Time since start: 3615.97s, 	Step: 4791, 	{'train/accuracy': 0.5435932278633118, 'train/loss': 2.867501735687256, 'train/bleu': 24.044350532229807, 'validation/accuracy': 0.5418655872344971, 'validation/loss': 2.839674472808838, 'validation/bleu': 20.273860903076134, 'validation/num_examples': 3000, 'test/accuracy': 0.5451164841651917, 'test/loss': 2.8579013347625732, 'test/bleu': 18.830678245727768, 'test/num_examples': 3003, 'score': 1708.5745656490326, 'total_duration': 3615.966960668564, 'accumulated_submission_time': 1708.5745656490326, 'accumulated_eval_time': 1907.1950154304504, 'accumulated_logging_time': 0.045763254165649414}
I0207 11:13:30.544615 140277145609984 logging_writer.py:48] [4791] accumulated_eval_time=1907.195015, accumulated_logging_time=0.045763, accumulated_submission_time=1708.574566, global_step=4791, preemption_count=0, score=1708.574566, test/accuracy=0.545116, test/bleu=18.830678, test/loss=2.857901, test/num_examples=3003, total_duration=3615.966961, train/accuracy=0.543593, train/bleu=24.044351, train/loss=2.867502, validation/accuracy=0.541866, validation/bleu=20.273861, validation/loss=2.839674, validation/num_examples=3000
I0207 11:13:34.063154 140277154002688 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.41858726739883423, loss=4.690229415893555
I0207 11:14:09.029643 140277145609984 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.37659895420074463, loss=4.710031509399414
I0207 11:14:44.056741 140277154002688 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.35276368260383606, loss=4.651819705963135
I0207 11:15:19.127697 140277145609984 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3721263110637665, loss=4.680576801300049
I0207 11:15:54.203281 140277154002688 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3522225022315979, loss=4.650272369384766
I0207 11:16:29.273945 140277145609984 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.35199064016342163, loss=4.624204635620117
I0207 11:17:04.355540 140277154002688 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.33797597885131836, loss=4.683901309967041
I0207 11:17:39.414510 140277145609984 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.36676785349845886, loss=4.62373685836792
I0207 11:18:14.496305 140277154002688 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.3854675889015198, loss=4.677262783050537
I0207 11:18:49.576517 140277145609984 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.34671106934547424, loss=4.570303916931152
I0207 11:19:24.655891 140277154002688 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.3115725517272949, loss=4.526551246643066
I0207 11:19:59.737724 140277145609984 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.3316180408000946, loss=4.622989654541016
I0207 11:20:34.796032 140277154002688 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.3238062858581543, loss=4.488970756530762
I0207 11:21:09.825938 140277145609984 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.29906103014945984, loss=4.53731632232666
I0207 11:21:44.881340 140277154002688 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.2971961796283722, loss=4.609940528869629
I0207 11:22:19.935337 140277145609984 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.29022473096847534, loss=4.53718376159668
I0207 11:22:55.023335 140277154002688 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.31707414984703064, loss=4.5622029304504395
I0207 11:23:30.072817 140277145609984 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.3060147762298584, loss=4.554978847503662
I0207 11:24:05.145468 140277154002688 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2804476022720337, loss=4.5399065017700195
I0207 11:24:40.185590 140277145609984 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.2609454393386841, loss=4.549294948577881
I0207 11:25:15.258399 140277154002688 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.31309348344802856, loss=4.483877658843994
I0207 11:25:50.308183 140277145609984 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.2501731812953949, loss=4.49404239654541
I0207 11:26:25.372380 140277154002688 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.25893789529800415, loss=4.462609767913818
I0207 11:27:00.433150 140277145609984 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.27188974618911743, loss=4.460703372955322
I0207 11:27:30.629178 140446903760704 spec.py:321] Evaluating on the training split.
I0207 11:27:33.663517 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:30:04.789535 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 11:30:07.503059 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:32:48.628902 140446903760704 spec.py:349] Evaluating on the test split.
I0207 11:32:51.336955 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:35:14.828974 140446903760704 submission_runner.py:408] Time since start: 4920.27s, 	Step: 7188, 	{'train/accuracy': 0.5837926268577576, 'train/loss': 2.4588887691497803, 'train/bleu': 26.822264155984897, 'validation/accuracy': 0.5924786925315857, 'validation/loss': 2.391460657119751, 'validation/bleu': 23.340748577255788, 'validation/num_examples': 3000, 'test/accuracy': 0.5944570302963257, 'test/loss': 2.3819665908813477, 'test/bleu': 21.934132082215903, 'test/num_examples': 3003, 'score': 2548.571653842926, 'total_duration': 4920.2710173130035, 'accumulated_submission_time': 2548.571653842926, 'accumulated_eval_time': 2371.394764184952, 'accumulated_logging_time': 0.07738113403320312}
I0207 11:35:14.844506 140277154002688 logging_writer.py:48] [7188] accumulated_eval_time=2371.394764, accumulated_logging_time=0.077381, accumulated_submission_time=2548.571654, global_step=7188, preemption_count=0, score=2548.571654, test/accuracy=0.594457, test/bleu=21.934132, test/loss=2.381967, test/num_examples=3003, total_duration=4920.271017, train/accuracy=0.583793, train/bleu=26.822264, train/loss=2.458889, validation/accuracy=0.592479, validation/bleu=23.340749, validation/loss=2.391461, validation/num_examples=3000
I0207 11:35:19.401560 140277145609984 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.2379130870103836, loss=4.446186065673828
I0207 11:35:54.302583 140277154002688 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.2352980673313141, loss=4.407566070556641
I0207 11:36:29.286876 140277145609984 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.24056777358055115, loss=4.373510360717773
I0207 11:37:04.306697 140277154002688 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.2709291875362396, loss=4.418072700500488
I0207 11:37:39.344511 140277145609984 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.23474101722240448, loss=4.400833606719971
I0207 11:38:14.394416 140277154002688 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.2475145012140274, loss=4.388287544250488
I0207 11:38:49.473510 140277145609984 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.219588965177536, loss=4.383617877960205
I0207 11:39:24.513536 140277154002688 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.24165914952754974, loss=4.359259605407715
I0207 11:39:59.546941 140277145609984 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.21648506820201874, loss=4.333077907562256
I0207 11:40:34.567100 140277154002688 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.21393781900405884, loss=4.366730213165283
I0207 11:41:09.609685 140277145609984 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.21392396092414856, loss=4.293405055999756
I0207 11:41:44.689984 140277154002688 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.21744364500045776, loss=4.418026447296143
I0207 11:42:19.753102 140277145609984 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.20725208520889282, loss=4.327675819396973
I0207 11:42:54.811807 140277154002688 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.20189860463142395, loss=4.32365608215332
I0207 11:43:29.889221 140277145609984 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.22957439720630646, loss=4.343626976013184
I0207 11:44:04.938948 140277154002688 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.21719332039356232, loss=4.3447957038879395
I0207 11:44:39.946642 140277145609984 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.22403380274772644, loss=4.289812088012695
I0207 11:45:15.012923 140277154002688 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.26129773259162903, loss=4.3422465324401855
I0207 11:45:50.072367 140277145609984 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1952262669801712, loss=4.356395244598389
I0207 11:46:25.095153 140277154002688 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.2220766544342041, loss=4.286325454711914
I0207 11:47:00.100992 140277145609984 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1867775022983551, loss=4.375924110412598
I0207 11:47:35.138609 140277154002688 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.19247910380363464, loss=4.260811805725098
I0207 11:48:10.221417 140277145609984 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.1972104161977768, loss=4.285824298858643
I0207 11:48:45.254976 140277154002688 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.201103076338768, loss=4.289107799530029
I0207 11:49:15.091584 140446903760704 spec.py:321] Evaluating on the training split.
I0207 11:49:18.103529 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:51:42.834537 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 11:51:45.549869 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:54:16.058987 140446903760704 spec.py:349] Evaluating on the test split.
I0207 11:54:18.768602 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 11:56:33.850804 140446903760704 submission_runner.py:408] Time since start: 6199.29s, 	Step: 9587, 	{'train/accuracy': 0.5973837971687317, 'train/loss': 2.325868606567383, 'train/bleu': 28.25564876003111, 'validation/accuracy': 0.6162601709365845, 'validation/loss': 2.1848175525665283, 'validation/bleu': 25.123763093156985, 'validation/num_examples': 3000, 'test/accuracy': 0.6207309365272522, 'test/loss': 2.1605913639068604, 'test/bleu': 23.869267718351175, 'test/num_examples': 3003, 'score': 3388.7338008880615, 'total_duration': 6199.2928557395935, 'accumulated_submission_time': 3388.7338008880615, 'accumulated_eval_time': 2810.153936624527, 'accumulated_logging_time': 0.10327434539794922}
I0207 11:56:33.866408 140277145609984 logging_writer.py:48] [9587] accumulated_eval_time=2810.153937, accumulated_logging_time=0.103274, accumulated_submission_time=3388.733801, global_step=9587, preemption_count=0, score=3388.733801, test/accuracy=0.620731, test/bleu=23.869268, test/loss=2.160591, test/num_examples=3003, total_duration=6199.292856, train/accuracy=0.597384, train/bleu=28.255649, train/loss=2.325869, validation/accuracy=0.616260, validation/bleu=25.123763, validation/loss=2.184818, validation/num_examples=3000
I0207 11:56:38.770581 140277154002688 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.2093537449836731, loss=4.2855000495910645
I0207 11:57:13.700706 140277145609984 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.20172694325447083, loss=4.222353935241699
I0207 11:57:48.645507 140277154002688 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.18916766345500946, loss=4.263050556182861
I0207 11:58:23.680630 140277145609984 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.178445965051651, loss=4.236443042755127
I0207 11:58:58.719603 140277154002688 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.18910425901412964, loss=4.219893455505371
I0207 11:59:33.764808 140277145609984 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.18504320085048676, loss=4.22127103805542
I0207 12:00:08.816322 140277154002688 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.21582695841789246, loss=4.246984958648682
I0207 12:00:43.890639 140277145609984 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.17452950775623322, loss=4.249225616455078
I0207 12:01:18.952640 140277154002688 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17987070977687836, loss=4.195519924163818
I0207 12:01:54.007861 140277145609984 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.21474041044712067, loss=4.298444747924805
I0207 12:02:29.060783 140277154002688 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.17413608729839325, loss=4.25010871887207
I0207 12:03:04.116337 140277145609984 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.19836167991161346, loss=4.3429179191589355
I0207 12:03:39.172501 140277154002688 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1801653355360031, loss=4.263479232788086
I0207 12:04:14.249961 140277145609984 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.17021989822387695, loss=4.208223819732666
I0207 12:04:49.335566 140277154002688 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.17915181815624237, loss=4.275029182434082
I0207 12:05:24.409911 140277145609984 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.1640574336051941, loss=4.192659854888916
I0207 12:05:59.468452 140277154002688 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.18615546822547913, loss=4.259280681610107
I0207 12:06:34.491205 140277145609984 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.17960429191589355, loss=4.185034275054932
I0207 12:07:09.540261 140277154002688 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.18444468080997467, loss=4.175841331481934
I0207 12:07:44.570709 140277145609984 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17989158630371094, loss=4.209124565124512
I0207 12:08:19.603439 140277154002688 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.18810737133026123, loss=4.198193073272705
I0207 12:08:54.623383 140277145609984 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.20731645822525024, loss=4.322722434997559
I0207 12:09:29.647022 140277154002688 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.18943344056606293, loss=4.179807662963867
I0207 12:10:04.665156 140277145609984 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.17411306500434875, loss=4.215949535369873
I0207 12:10:34.162788 140446903760704 spec.py:321] Evaluating on the training split.
I0207 12:10:37.178803 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:13:23.610164 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 12:13:26.325425 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:16:10.344955 140446903760704 spec.py:349] Evaluating on the test split.
I0207 12:16:13.068206 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:18:42.441668 140446903760704 submission_runner.py:408] Time since start: 7527.88s, 	Step: 11986, 	{'train/accuracy': 0.6148414611816406, 'train/loss': 2.1918225288391113, 'train/bleu': 29.49347492495337, 'validation/accuracy': 0.629031240940094, 'validation/loss': 2.0681145191192627, 'validation/bleu': 26.154691827326516, 'validation/num_examples': 3000, 'test/accuracy': 0.638847291469574, 'test/loss': 2.0232722759246826, 'test/bleu': 25.131617057030173, 'test/num_examples': 3003, 'score': 4228.941308021545, 'total_duration': 7527.8836851119995, 'accumulated_submission_time': 4228.941308021545, 'accumulated_eval_time': 3298.432733297348, 'accumulated_logging_time': 0.13021183013916016}
I0207 12:18:42.461103 140277154002688 logging_writer.py:48] [11986] accumulated_eval_time=3298.432733, accumulated_logging_time=0.130212, accumulated_submission_time=4228.941308, global_step=11986, preemption_count=0, score=4228.941308, test/accuracy=0.638847, test/bleu=25.131617, test/loss=2.023272, test/num_examples=3003, total_duration=7527.883685, train/accuracy=0.614841, train/bleu=29.493475, train/loss=2.191823, validation/accuracy=0.629031, validation/bleu=26.154692, validation/loss=2.068115, validation/num_examples=3000
I0207 12:18:47.719393 140277145609984 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.1807115077972412, loss=4.233078956604004
I0207 12:19:22.645861 140277154002688 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.16439500451087952, loss=4.119550704956055
I0207 12:19:57.649409 140277145609984 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.19772571325302124, loss=4.169731140136719
I0207 12:20:32.667278 140277154002688 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.17200671136379242, loss=4.205205917358398
I0207 12:21:07.697782 140277145609984 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.18225425481796265, loss=4.160146236419678
I0207 12:21:42.744159 140277154002688 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1819392591714859, loss=4.180765628814697
I0207 12:22:17.785270 140277145609984 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.17145369946956635, loss=4.169747352600098
I0207 12:22:52.810662 140277154002688 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.17013725638389587, loss=4.181148529052734
I0207 12:23:27.941583 140277145609984 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.15881142020225525, loss=4.126410961151123
I0207 12:24:03.014809 140277154002688 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.15558934211730957, loss=4.183957099914551
I0207 12:24:38.061614 140277145609984 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.1929282248020172, loss=4.220095634460449
I0207 12:25:13.121830 140277154002688 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.16955843567848206, loss=4.18330717086792
I0207 12:25:48.337643 140277145609984 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.17389529943466187, loss=4.211740970611572
I0207 12:26:23.372265 140277154002688 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.1560114622116089, loss=4.128058910369873
I0207 12:26:58.404775 140277145609984 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.16367879509925842, loss=4.175341606140137
I0207 12:27:33.428590 140277154002688 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16436834633350372, loss=4.135150909423828
I0207 12:28:08.457010 140277145609984 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.1594989150762558, loss=4.114385604858398
I0207 12:28:43.482042 140277154002688 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.17042401432991028, loss=4.147083282470703
I0207 12:29:18.525932 140277145609984 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.16728457808494568, loss=4.1664958000183105
I0207 12:29:53.573552 140277154002688 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.1843588948249817, loss=4.172586441040039
I0207 12:30:28.639420 140277145609984 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.17765946686267853, loss=4.110998630523682
I0207 12:31:03.684177 140277154002688 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.15895390510559082, loss=4.117953777313232
I0207 12:31:38.729967 140277145609984 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.18239177763462067, loss=4.144055366516113
I0207 12:32:13.765054 140277154002688 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.15793031454086304, loss=4.052745342254639
I0207 12:32:42.522907 140446903760704 spec.py:321] Evaluating on the training split.
I0207 12:32:45.521943 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:35:22.392001 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 12:35:25.108676 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:37:53.599597 140446903760704 spec.py:349] Evaluating on the test split.
I0207 12:37:56.311567 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:40:09.315366 140446903760704 submission_runner.py:408] Time since start: 8814.76s, 	Step: 14384, 	{'train/accuracy': 0.6251729726791382, 'train/loss': 2.1058154106140137, 'train/bleu': 30.452753434872445, 'validation/accuracy': 0.6403516530990601, 'validation/loss': 1.9786962270736694, 'validation/bleu': 26.710132243429697, 'validation/num_examples': 3000, 'test/accuracy': 0.6496427059173584, 'test/loss': 1.9267456531524658, 'test/bleu': 26.02713434983809, 'test/num_examples': 3003, 'score': 5068.914443492889, 'total_duration': 8814.757400751114, 'accumulated_submission_time': 5068.914443492889, 'accumulated_eval_time': 3745.2251360416412, 'accumulated_logging_time': 0.16237592697143555}
I0207 12:40:09.331974 140277145609984 logging_writer.py:48] [14384] accumulated_eval_time=3745.225136, accumulated_logging_time=0.162376, accumulated_submission_time=5068.914443, global_step=14384, preemption_count=0, score=5068.914443, test/accuracy=0.649643, test/bleu=26.027134, test/loss=1.926746, test/num_examples=3003, total_duration=8814.757401, train/accuracy=0.625173, train/bleu=30.452753, train/loss=2.105815, validation/accuracy=0.640352, validation/bleu=26.710132, validation/loss=1.978696, validation/num_examples=3000
I0207 12:40:15.272375 140277154002688 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.15919210016727448, loss=4.130621433258057
I0207 12:40:50.197181 140277145609984 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.16094280779361725, loss=4.097742080688477
I0207 12:41:25.214790 140277154002688 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.1858425736427307, loss=4.128317832946777
I0207 12:42:00.270830 140277145609984 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.15775233507156372, loss=4.084554195404053
I0207 12:42:35.332543 140277154002688 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.14964918792247772, loss=4.126914024353027
I0207 12:43:10.403392 140277145609984 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.15817266702651978, loss=4.124672889709473
I0207 12:43:45.431303 140277154002688 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2068263739347458, loss=4.054868698120117
I0207 12:44:20.512558 140277145609984 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.17034336924552917, loss=4.07009220123291
I0207 12:44:55.599999 140277154002688 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.1660727858543396, loss=4.199345588684082
I0207 12:45:30.648347 140277145609984 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.1585610806941986, loss=4.133730888366699
I0207 12:46:05.699085 140277154002688 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.17537423968315125, loss=4.031210422515869
I0207 12:46:40.771837 140277145609984 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.18909989297389984, loss=4.139252662658691
I0207 12:47:15.834458 140277154002688 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.1595514565706253, loss=4.14508056640625
I0207 12:47:50.929762 140277145609984 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1731015145778656, loss=4.0632429122924805
I0207 12:48:25.994538 140277154002688 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.1758614033460617, loss=4.115811347961426
I0207 12:49:01.034639 140277145609984 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.1668763905763626, loss=4.170217514038086
I0207 12:49:36.087471 140277154002688 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1763056218624115, loss=4.106940269470215
I0207 12:50:11.143861 140277145609984 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.15884406864643097, loss=4.1530070304870605
I0207 12:50:46.187891 140277154002688 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.17057636380195618, loss=4.043096542358398
I0207 12:51:21.234537 140277145609984 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.15728341042995453, loss=4.069940567016602
I0207 12:51:56.290083 140277154002688 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.15590307116508484, loss=4.0728983879089355
I0207 12:52:31.359868 140277145609984 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.16539794206619263, loss=4.081309795379639
I0207 12:53:06.415711 140277154002688 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.17620515823364258, loss=4.063662528991699
I0207 12:53:41.469162 140277145609984 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.21121637523174286, loss=4.038431167602539
I0207 12:54:09.580334 140446903760704 spec.py:321] Evaluating on the training split.
I0207 12:54:12.586545 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:56:40.837555 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 12:56:43.550865 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 12:59:15.869382 140446903760704 spec.py:349] Evaluating on the test split.
I0207 12:59:18.573940 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 13:01:37.987550 140446903760704 submission_runner.py:408] Time since start: 10103.43s, 	Step: 16782, 	{'train/accuracy': 0.6332809329032898, 'train/loss': 2.025113582611084, 'train/bleu': 30.834592006989155, 'validation/accuracy': 0.6474562883377075, 'validation/loss': 1.917392373085022, 'validation/bleu': 27.531752272070463, 'validation/num_examples': 3000, 'test/accuracy': 0.658288300037384, 'test/loss': 1.8623210191726685, 'test/bleu': 26.765356617369413, 'test/num_examples': 3003, 'score': 5909.078606367111, 'total_duration': 10103.429570436478, 'accumulated_submission_time': 5909.078606367111, 'accumulated_eval_time': 4193.632272481918, 'accumulated_logging_time': 0.18920087814331055}
I0207 13:01:38.007392 140277154002688 logging_writer.py:48] [16782] accumulated_eval_time=4193.632272, accumulated_logging_time=0.189201, accumulated_submission_time=5909.078606, global_step=16782, preemption_count=0, score=5909.078606, test/accuracy=0.658288, test/bleu=26.765357, test/loss=1.862321, test/num_examples=3003, total_duration=10103.429570, train/accuracy=0.633281, train/bleu=30.834592, train/loss=2.025114, validation/accuracy=0.647456, validation/bleu=27.531752, validation/loss=1.917392, validation/num_examples=3000
I0207 13:01:44.654176 140277145609984 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.23392441868782043, loss=4.121644020080566
I0207 13:02:19.560098 140277154002688 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15207165479660034, loss=4.054208278656006
I0207 13:02:54.557675 140277145609984 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2192726731300354, loss=4.126695156097412
I0207 13:03:29.592506 140277154002688 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.21484841406345367, loss=4.104519844055176
I0207 13:04:04.622326 140277145609984 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.16632896661758423, loss=4.035848140716553
I0207 13:04:39.636164 140277154002688 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.17006491124629974, loss=4.095279693603516
I0207 13:05:14.685199 140277145609984 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.18972885608673096, loss=4.040139675140381
I0207 13:05:49.710208 140277154002688 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.15697048604488373, loss=4.057409763336182
I0207 13:06:24.773673 140277145609984 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.16083620488643646, loss=4.100469589233398
I0207 13:06:59.803366 140277154002688 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.15386249125003815, loss=4.0060505867004395
I0207 13:07:34.853307 140277145609984 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.24327559769153595, loss=4.076231956481934
I0207 13:08:09.889612 140277154002688 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.17083637416362762, loss=4.057000160217285
I0207 13:08:44.975201 140277145609984 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.17659521102905273, loss=4.065222263336182
I0207 13:09:20.018538 140277154002688 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.17363570630550385, loss=4.082292079925537
I0207 13:09:55.043308 140277145609984 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.2574955224990845, loss=4.066916465759277
I0207 13:10:30.098104 140277154002688 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.21964718401432037, loss=4.0471978187561035
I0207 13:11:05.120922 140277145609984 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.1567692905664444, loss=4.038209915161133
I0207 13:11:40.158670 140277154002688 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.20041371881961823, loss=4.066579341888428
I0207 13:12:15.187288 140277145609984 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.21157902479171753, loss=4.063511848449707
I0207 13:12:50.226489 140277154002688 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.19125425815582275, loss=4.037618637084961
I0207 13:13:25.274279 140277145609984 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.23057471215724945, loss=4.066479206085205
I0207 13:14:00.311589 140277154002688 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.16639168560504913, loss=3.988213539123535
I0207 13:14:35.361344 140277145609984 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.15863287448883057, loss=4.060006141662598
I0207 13:15:10.454290 140277154002688 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.16690821945667267, loss=4.06390380859375
I0207 13:15:38.202229 140446903760704 spec.py:321] Evaluating on the training split.
I0207 13:15:41.217216 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 13:18:24.872291 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 13:18:27.580439 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 13:20:51.898983 140446903760704 spec.py:349] Evaluating on the test split.
I0207 13:20:54.620837 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 13:23:12.170984 140446903760704 submission_runner.py:408] Time since start: 11397.61s, 	Step: 19181, 	{'train/accuracy': 0.6485809087753296, 'train/loss': 1.9150187969207764, 'train/bleu': 31.71264530159373, 'validation/accuracy': 0.6535567045211792, 'validation/loss': 1.8694406747817993, 'validation/bleu': 27.791618921827165, 'validation/num_examples': 3000, 'test/accuracy': 0.6626227498054504, 'test/loss': 1.8123000860214233, 'test/bleu': 27.056916774320545, 'test/num_examples': 3003, 'score': 6749.185294866562, 'total_duration': 11397.61303448677, 'accumulated_submission_time': 6749.185294866562, 'accumulated_eval_time': 4647.600977182388, 'accumulated_logging_time': 0.22064924240112305}
I0207 13:23:12.189291 140277145609984 logging_writer.py:48] [19181] accumulated_eval_time=4647.600977, accumulated_logging_time=0.220649, accumulated_submission_time=6749.185295, global_step=19181, preemption_count=0, score=6749.185295, test/accuracy=0.662623, test/bleu=27.056917, test/loss=1.812300, test/num_examples=3003, total_duration=11397.613034, train/accuracy=0.648581, train/bleu=31.712645, train/loss=1.915019, validation/accuracy=0.653557, validation/bleu=27.791619, validation/loss=1.869441, validation/num_examples=3000
I0207 13:23:19.170607 140277154002688 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.16347071528434753, loss=4.01141357421875
I0207 13:23:54.030320 140277145609984 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.16580337285995483, loss=4.018550872802734
I0207 13:24:29.044262 140277154002688 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.20688197016716003, loss=4.0968217849731445
I0207 13:25:04.082607 140277145609984 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.17268626391887665, loss=4.092430114746094
I0207 13:25:39.110544 140277154002688 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.15765193104743958, loss=4.04184103012085
I0207 13:26:14.121769 140277145609984 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.14956052601337433, loss=4.120810508728027
I0207 13:26:49.176120 140277154002688 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17170074582099915, loss=4.029765605926514
I0207 13:27:24.314959 140277145609984 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.16811347007751465, loss=4.044556140899658
I0207 13:27:59.384202 140277154002688 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.2517397999763489, loss=4.086073398590088
I0207 13:28:34.405380 140277145609984 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.16186852753162384, loss=3.9883294105529785
I0207 13:29:09.456409 140277154002688 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.2029288411140442, loss=4.053084850311279
I0207 13:29:44.481566 140277145609984 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.16063673794269562, loss=4.060978412628174
I0207 13:30:19.536354 140277154002688 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.16035592555999756, loss=4.116659164428711
I0207 13:30:54.563019 140277145609984 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.2330254316329956, loss=4.024647235870361
I0207 13:31:29.579044 140277154002688 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.19807228446006775, loss=4.051755428314209
I0207 13:32:04.602950 140277145609984 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1818804144859314, loss=4.000985145568848
I0207 13:32:39.628978 140277154002688 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.16813236474990845, loss=4.024016857147217
I0207 13:33:14.662000 140277145609984 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.17265252768993378, loss=3.964637279510498
I0207 13:33:49.716775 140277154002688 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.16163679957389832, loss=4.024984359741211
I0207 13:34:24.768005 140277145609984 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.17666694521903992, loss=4.076769828796387
I0207 13:34:59.783057 140277154002688 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.171197310090065, loss=4.059824466705322
I0207 13:35:34.819935 140277145609984 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2897495627403259, loss=4.051050186157227
I0207 13:36:09.857843 140277154002688 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.20836485922336578, loss=4.0542168617248535
I0207 13:36:44.880931 140277145609984 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.17534208297729492, loss=4.083909511566162
I0207 13:37:12.274858 140446903760704 spec.py:321] Evaluating on the training split.
I0207 13:37:15.282599 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 13:39:58.851747 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 13:40:01.585964 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 13:42:30.945613 140446903760704 spec.py:349] Evaluating on the test split.
I0207 13:42:33.670576 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 13:44:56.699585 140446903760704 submission_runner.py:408] Time since start: 12702.14s, 	Step: 21580, 	{'train/accuracy': 0.6398290395736694, 'train/loss': 1.9732670783996582, 'train/bleu': 31.557177824770733, 'validation/accuracy': 0.6579831838607788, 'validation/loss': 1.8404289484024048, 'validation/bleu': 28.183138777133554, 'validation/num_examples': 3000, 'test/accuracy': 0.6678287386894226, 'test/loss': 1.778388500213623, 'test/bleu': 27.37082354685312, 'test/num_examples': 3003, 'score': 7589.185811042786, 'total_duration': 12702.14163517952, 'accumulated_submission_time': 7589.185811042786, 'accumulated_eval_time': 5112.02565741539, 'accumulated_logging_time': 0.2493281364440918}
I0207 13:44:56.716984 140277154002688 logging_writer.py:48] [21580] accumulated_eval_time=5112.025657, accumulated_logging_time=0.249328, accumulated_submission_time=7589.185811, global_step=21580, preemption_count=0, score=7589.185811, test/accuracy=0.667829, test/bleu=27.370824, test/loss=1.778389, test/num_examples=3003, total_duration=12702.141635, train/accuracy=0.639829, train/bleu=31.557178, train/loss=1.973267, validation/accuracy=0.657983, validation/bleu=28.183139, validation/loss=1.840429, validation/num_examples=3000
I0207 13:45:04.070828 140277145609984 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.19470418989658356, loss=3.973064422607422
I0207 13:45:38.998340 140277154002688 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.17056509852409363, loss=3.9719724655151367
I0207 13:46:13.999600 140277145609984 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.17410244047641754, loss=4.00012731552124
I0207 13:46:49.017881 140277154002688 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.18031226098537445, loss=4.071648597717285
I0207 13:47:24.063359 140277145609984 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.1788657307624817, loss=3.992326498031616
I0207 13:47:59.093904 140277154002688 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.2328452169895172, loss=4.085292339324951
I0207 13:48:34.131377 140277145609984 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.17752717435359955, loss=4.045282363891602
I0207 13:49:09.176406 140277154002688 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.15508519113063812, loss=3.9624264240264893
I0207 13:49:44.210962 140277145609984 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.2676101624965668, loss=3.985867738723755
I0207 13:50:19.239020 140277154002688 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.19787122309207916, loss=3.9882240295410156
I0207 13:50:54.284039 140277145609984 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.16953568160533905, loss=3.981412172317505
I0207 13:51:29.338135 140277154002688 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.19645658135414124, loss=4.078174591064453
I0207 13:52:04.392457 140277145609984 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.21035030484199524, loss=3.9915380477905273
I0207 13:52:39.546623 140277154002688 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.220162034034729, loss=4.005990982055664
I0207 13:53:14.608853 140277145609984 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.1756899356842041, loss=3.999372959136963
I0207 13:53:49.621990 140277154002688 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.20998568832874298, loss=4.005884170532227
I0207 13:54:24.677291 140277145609984 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.18062086403369904, loss=4.023233890533447
I0207 13:54:59.716693 140277154002688 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.18353651463985443, loss=4.033471584320068
I0207 13:55:34.789196 140277145609984 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.18512694537639618, loss=4.05850887298584
I0207 13:56:09.836539 140277154002688 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.1767108142375946, loss=3.943063735961914
I0207 13:56:44.888823 140277145609984 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3057461678981781, loss=4.06139612197876
I0207 13:57:19.917608 140277154002688 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.18499194085597992, loss=4.033816814422607
I0207 13:57:54.952289 140277145609984 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.22359782457351685, loss=3.9894165992736816
I0207 13:58:30.024032 140277154002688 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.18302004039287567, loss=3.957432270050049
I0207 13:58:56.727084 140446903760704 spec.py:321] Evaluating on the training split.
I0207 13:58:59.754728 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:01:38.761738 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 14:01:41.485370 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:04:05.891597 140446903760704 spec.py:349] Evaluating on the test split.
I0207 14:04:08.599254 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:06:26.948830 140446903760704 submission_runner.py:408] Time since start: 13992.39s, 	Step: 23978, 	{'train/accuracy': 0.6370953321456909, 'train/loss': 1.9664952754974365, 'train/bleu': 31.35145588205602, 'validation/accuracy': 0.6594586372375488, 'validation/loss': 1.8098756074905396, 'validation/bleu': 28.52456600789127, 'validation/num_examples': 3000, 'test/accuracy': 0.6701877117156982, 'test/loss': 1.7438005208969116, 'test/bleu': 27.80662204576, 'test/num_examples': 3003, 'score': 8429.109383583069, 'total_duration': 13992.390851259232, 'accumulated_submission_time': 8429.109383583069, 'accumulated_eval_time': 5562.2473402023315, 'accumulated_logging_time': 0.27814149856567383}
I0207 14:06:26.967761 140277145609984 logging_writer.py:48] [23978] accumulated_eval_time=5562.247340, accumulated_logging_time=0.278141, accumulated_submission_time=8429.109384, global_step=23978, preemption_count=0, score=8429.109384, test/accuracy=0.670188, test/bleu=27.806622, test/loss=1.743801, test/num_examples=3003, total_duration=13992.390851, train/accuracy=0.637095, train/bleu=31.351456, train/loss=1.966495, validation/accuracy=0.659459, validation/bleu=28.524566, validation/loss=1.809876, validation/num_examples=3000
I0207 14:06:35.016717 140277154002688 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.16586583852767944, loss=3.9618587493896484
I0207 14:07:09.943384 140277145609984 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.17305025458335876, loss=3.975067615509033
I0207 14:07:44.920980 140277154002688 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.2386976033449173, loss=3.997243881225586
I0207 14:08:19.911805 140277145609984 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.18972328305244446, loss=3.9811007976531982
I0207 14:08:54.928200 140277154002688 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.18987254798412323, loss=3.9672176837921143
I0207 14:09:29.958569 140277145609984 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.18025729060173035, loss=3.958937883377075
I0207 14:10:05.013113 140277154002688 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.1717846393585205, loss=3.9297897815704346
I0207 14:10:40.042001 140277145609984 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2469249963760376, loss=3.970200538635254
I0207 14:11:15.099117 140277154002688 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.18683822453022003, loss=3.961146354675293
I0207 14:11:50.256289 140277145609984 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.1859484165906906, loss=4.001652717590332
I0207 14:12:25.329107 140277154002688 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.20348265767097473, loss=3.9996683597564697
I0207 14:13:00.416898 140277145609984 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2327907234430313, loss=3.9714133739471436
I0207 14:13:35.428743 140277154002688 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2043505311012268, loss=3.933335781097412
I0207 14:14:10.480302 140277145609984 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.20308277010917664, loss=4.086254119873047
I0207 14:14:45.545300 140277154002688 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.16760961711406708, loss=3.9964253902435303
I0207 14:15:20.586944 140277145609984 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.20940962433815002, loss=3.9734866619110107
I0207 14:15:55.641497 140277154002688 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.19554224610328674, loss=3.969303607940674
I0207 14:16:30.695272 140277145609984 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.17882977426052094, loss=3.981508731842041
I0207 14:17:05.754406 140277154002688 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.25720474123954773, loss=4.0468316078186035
I0207 14:17:40.853523 140277145609984 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.22412104904651642, loss=3.9819438457489014
I0207 14:18:15.938204 140277154002688 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2691343128681183, loss=3.942248582839966
I0207 14:18:50.966618 140277145609984 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.27467432618141174, loss=3.968944549560547
I0207 14:19:26.017707 140277154002688 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.20436373353004456, loss=3.9716176986694336
I0207 14:20:01.054340 140277145609984 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.19185443222522736, loss=3.968397855758667
I0207 14:20:27.055448 140446903760704 spec.py:321] Evaluating on the training split.
I0207 14:20:30.080341 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:23:18.113603 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 14:23:20.844101 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:25:46.569347 140446903760704 spec.py:349] Evaluating on the test split.
I0207 14:25:49.292311 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:28:13.767057 140446903760704 submission_runner.py:408] Time since start: 15299.21s, 	Step: 26376, 	{'train/accuracy': 0.6522204279899597, 'train/loss': 1.8794294595718384, 'train/bleu': 31.961837474519832, 'validation/accuracy': 0.6629427671432495, 'validation/loss': 1.797146201133728, 'validation/bleu': 28.615634344157126, 'validation/num_examples': 3000, 'test/accuracy': 0.6761141419410706, 'test/loss': 1.732893466949463, 'test/bleu': 28.11392985066348, 'test/num_examples': 3003, 'score': 9269.108746528625, 'total_duration': 15299.209090471268, 'accumulated_submission_time': 9269.108746528625, 'accumulated_eval_time': 6028.958881616592, 'accumulated_logging_time': 0.308734655380249}
I0207 14:28:13.785699 140277154002688 logging_writer.py:48] [26376] accumulated_eval_time=6028.958882, accumulated_logging_time=0.308735, accumulated_submission_time=9269.108747, global_step=26376, preemption_count=0, score=9269.108747, test/accuracy=0.676114, test/bleu=28.113930, test/loss=1.732893, test/num_examples=3003, total_duration=15299.209090, train/accuracy=0.652220, train/bleu=31.961837, train/loss=1.879429, validation/accuracy=0.662943, validation/bleu=28.615634, validation/loss=1.797146, validation/num_examples=3000
I0207 14:28:22.526048 140277145609984 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.23559421300888062, loss=3.9895989894866943
I0207 14:28:57.415616 140277154002688 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.2612818777561188, loss=3.943225622177124
I0207 14:29:32.422608 140277145609984 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.22181805968284607, loss=3.9285337924957275
I0207 14:30:07.470223 140277154002688 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.16070422530174255, loss=3.9925200939178467
I0207 14:30:42.530346 140277145609984 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.2683717906475067, loss=4.009257793426514
I0207 14:31:17.591307 140277154002688 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.17778226733207703, loss=3.9686858654022217
I0207 14:31:52.664830 140277145609984 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.17506808042526245, loss=3.948695182800293
I0207 14:32:27.726501 140277154002688 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.19031287729740143, loss=3.938755512237549
I0207 14:33:02.789381 140277145609984 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.21647779643535614, loss=3.9415459632873535
I0207 14:33:37.850226 140277154002688 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.18021665513515472, loss=3.907252788543701
I0207 14:34:12.918033 140277145609984 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.19591310620307922, loss=3.958364248275757
I0207 14:34:47.984710 140277154002688 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.20488205552101135, loss=3.956439733505249
I0207 14:35:23.039203 140277145609984 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.25735270977020264, loss=3.97471284866333
I0207 14:35:58.106813 140277154002688 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.20876441895961761, loss=3.962214231491089
I0207 14:36:33.167996 140277145609984 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.20637185871601105, loss=3.980055809020996
I0207 14:37:08.238103 140277154002688 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.23719818890094757, loss=3.918625831604004
I0207 14:37:43.272488 140277145609984 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.21070921421051025, loss=4.067214488983154
I0207 14:38:18.302613 140277154002688 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.2133459895849228, loss=3.90289044380188
I0207 14:38:53.371078 140277145609984 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.21744276583194733, loss=3.9378104209899902
I0207 14:39:28.404683 140277154002688 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.22336074709892273, loss=3.9511513710021973
I0207 14:40:03.456382 140277145609984 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.23183858394622803, loss=3.9341955184936523
I0207 14:40:38.507348 140277154002688 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.17921985685825348, loss=3.9030396938323975
I0207 14:41:13.588467 140277145609984 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.26907381415367126, loss=4.037203788757324
I0207 14:41:48.646916 140277154002688 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.20520636439323425, loss=3.969440460205078
I0207 14:42:13.930648 140446903760704 spec.py:321] Evaluating on the training split.
I0207 14:42:16.944742 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:44:58.173547 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 14:45:00.881156 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:47:36.922516 140446903760704 spec.py:349] Evaluating on the test split.
I0207 14:47:39.639673 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 14:50:15.132929 140446903760704 submission_runner.py:408] Time since start: 16620.57s, 	Step: 28774, 	{'train/accuracy': 0.648067057132721, 'train/loss': 1.9048165082931519, 'train/bleu': 31.576812118177703, 'validation/accuracy': 0.6638355255126953, 'validation/loss': 1.7895036935806274, 'validation/bleu': 28.26089184313604, 'validation/num_examples': 3000, 'test/accuracy': 0.6750682592391968, 'test/loss': 1.7232335805892944, 'test/bleu': 27.884347883711857, 'test/num_examples': 3003, 'score': 10109.169246673584, 'total_duration': 16620.57497549057, 'accumulated_submission_time': 10109.169246673584, 'accumulated_eval_time': 6510.161110162735, 'accumulated_logging_time': 0.3377220630645752}
I0207 14:50:15.151684 140277145609984 logging_writer.py:48] [28774] accumulated_eval_time=6510.161110, accumulated_logging_time=0.337722, accumulated_submission_time=10109.169247, global_step=28774, preemption_count=0, score=10109.169247, test/accuracy=0.675068, test/bleu=27.884348, test/loss=1.723234, test/num_examples=3003, total_duration=16620.574975, train/accuracy=0.648067, train/bleu=31.576812, train/loss=1.904817, validation/accuracy=0.663836, validation/bleu=28.260892, validation/loss=1.789504, validation/num_examples=3000
I0207 14:50:24.577787 140277154002688 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.22090789675712585, loss=3.985217332839966
I0207 14:50:59.465890 140277145609984 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.24021382629871368, loss=4.024441719055176
I0207 14:51:34.476386 140277154002688 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.19438980519771576, loss=3.9201467037200928
I0207 14:52:09.498979 140277145609984 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.19836901128292084, loss=3.930208206176758
I0207 14:52:44.540819 140277154002688 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.23531800508499146, loss=4.021897315979004
I0207 14:53:19.589641 140277145609984 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.2154323309659958, loss=3.954000234603882
I0207 14:53:54.645704 140277154002688 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.18872098624706268, loss=4.0255231857299805
I0207 14:54:29.679667 140277145609984 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.24675032496452332, loss=3.8887197971343994
I0207 14:55:04.712942 140277154002688 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.19858887791633606, loss=3.9342105388641357
I0207 14:55:39.731109 140277145609984 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2853659391403198, loss=3.952374219894409
I0207 14:56:14.821619 140277154002688 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.21049712598323822, loss=3.960441827774048
I0207 14:56:49.900527 140277145609984 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.22478614747524261, loss=3.9303178787231445
I0207 14:57:24.942651 140277154002688 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2071208655834198, loss=3.936033248901367
I0207 14:57:59.967208 140277145609984 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.222517192363739, loss=3.9731688499450684
I0207 14:58:35.037112 140277154002688 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.2679389715194702, loss=3.9185783863067627
I0207 14:59:10.092880 140277145609984 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.20779219269752502, loss=3.9222569465637207
I0207 14:59:45.159629 140277154002688 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.23812121152877808, loss=3.990126132965088
I0207 15:00:20.188661 140277145609984 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.27539676427841187, loss=3.962757110595703
I0207 15:00:55.182182 140277154002688 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.28879690170288086, loss=3.90077805519104
I0207 15:01:30.218983 140277145609984 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.1997583657503128, loss=3.9647083282470703
I0207 15:02:05.269257 140277154002688 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.17374424636363983, loss=3.9041779041290283
I0207 15:02:40.342914 140277145609984 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.20489120483398438, loss=3.9076554775238037
I0207 15:03:15.500127 140277154002688 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.20547156035900116, loss=3.9278857707977295
I0207 15:03:50.590614 140277145609984 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.22410230338573456, loss=3.933082342147827
I0207 15:04:15.187652 140446903760704 spec.py:321] Evaluating on the training split.
I0207 15:04:18.212945 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:07:17.063429 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 15:07:19.797468 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:09:52.417974 140446903760704 spec.py:349] Evaluating on the test split.
I0207 15:09:55.152209 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:12:28.681267 140446903760704 submission_runner.py:408] Time since start: 17954.12s, 	Step: 31172, 	{'train/accuracy': 0.6439923644065857, 'train/loss': 1.927431583404541, 'train/bleu': 31.66790219778987, 'validation/accuracy': 0.6673568487167358, 'validation/loss': 1.7629570960998535, 'validation/bleu': 28.572178858129174, 'validation/num_examples': 3000, 'test/accuracy': 0.6779385209083557, 'test/loss': 1.6976670026779175, 'test/bleu': 28.39342145211166, 'test/num_examples': 3003, 'score': 10949.117016077042, 'total_duration': 17954.123304367065, 'accumulated_submission_time': 10949.117016077042, 'accumulated_eval_time': 7003.654671907425, 'accumulated_logging_time': 0.36781883239746094}
I0207 15:12:28.700212 140277154002688 logging_writer.py:48] [31172] accumulated_eval_time=7003.654672, accumulated_logging_time=0.367819, accumulated_submission_time=10949.117016, global_step=31172, preemption_count=0, score=10949.117016, test/accuracy=0.677939, test/bleu=28.393421, test/loss=1.697667, test/num_examples=3003, total_duration=17954.123304, train/accuracy=0.643992, train/bleu=31.667902, train/loss=1.927432, validation/accuracy=0.667357, validation/bleu=28.572179, validation/loss=1.762957, validation/num_examples=3000
I0207 15:12:38.813324 140277145609984 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.19926530122756958, loss=3.90403151512146
I0207 15:13:13.707676 140277154002688 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.21522638201713562, loss=3.966486692428589
I0207 15:13:48.701051 140277145609984 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2096703052520752, loss=3.9538118839263916
I0207 15:14:23.739062 140277154002688 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.1943109780550003, loss=3.9040415287017822
I0207 15:14:58.758022 140277145609984 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.21219082176685333, loss=3.90902042388916
I0207 15:15:33.807792 140277154002688 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.22913655638694763, loss=3.9782867431640625
I0207 15:16:08.828227 140277145609984 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.26333171129226685, loss=3.957369565963745
I0207 15:16:43.866829 140277154002688 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.2173234522342682, loss=3.9260008335113525
I0207 15:17:18.933321 140277145609984 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.19758766889572144, loss=3.938692331314087
I0207 15:17:53.979926 140277154002688 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.20595109462738037, loss=3.9476253986358643
I0207 15:18:29.047515 140277145609984 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.25617626309394836, loss=3.939988851547241
I0207 15:19:04.074017 140277154002688 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.2024335414171219, loss=3.8716299533843994
I0207 15:19:39.119698 140277145609984 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.22962236404418945, loss=4.0281243324279785
I0207 15:20:14.155540 140277154002688 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.2040252834558487, loss=3.909419298171997
I0207 15:20:49.178402 140277145609984 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.22564804553985596, loss=3.904728889465332
I0207 15:21:24.218569 140277154002688 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.21208123862743378, loss=3.920313596725464
I0207 15:21:59.241037 140277145609984 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.2181808352470398, loss=4.006983280181885
I0207 15:22:34.288524 140277154002688 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.18386386334896088, loss=3.925610065460205
I0207 15:23:09.327910 140277145609984 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.20219193398952484, loss=3.9458584785461426
I0207 15:23:44.352334 140277154002688 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.21071623265743256, loss=3.960711717605591
I0207 15:24:19.392052 140277145609984 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.23541656136512756, loss=4.003233432769775
I0207 15:24:54.423693 140277154002688 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.21263572573661804, loss=3.9223098754882812
I0207 15:25:29.476536 140277145609984 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.21929796040058136, loss=3.9211487770080566
I0207 15:26:04.519797 140277154002688 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.2269756942987442, loss=4.0046820640563965
I0207 15:26:28.752851 140446903760704 spec.py:321] Evaluating on the training split.
I0207 15:26:31.765080 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:29:08.369101 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 15:29:11.079967 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:31:40.741076 140446903760704 spec.py:349] Evaluating on the test split.
I0207 15:31:43.441226 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:34:14.470240 140446903760704 submission_runner.py:408] Time since start: 19259.91s, 	Step: 33571, 	{'train/accuracy': 0.6535011529922485, 'train/loss': 1.888905644416809, 'train/bleu': 31.83566635295377, 'validation/accuracy': 0.6689687371253967, 'validation/loss': 1.7727051973342896, 'validation/bleu': 29.0990490788574, 'validation/num_examples': 3000, 'test/accuracy': 0.6815641522407532, 'test/loss': 1.701885461807251, 'test/bleu': 28.767894892189727, 'test/num_examples': 3003, 'score': 11789.085807323456, 'total_duration': 19259.912284851074, 'accumulated_submission_time': 11789.085807323456, 'accumulated_eval_time': 7469.372006177902, 'accumulated_logging_time': 0.3968191146850586}
I0207 15:34:14.490333 140277145609984 logging_writer.py:48] [33571] accumulated_eval_time=7469.372006, accumulated_logging_time=0.396819, accumulated_submission_time=11789.085807, global_step=33571, preemption_count=0, score=11789.085807, test/accuracy=0.681564, test/bleu=28.767895, test/loss=1.701885, test/num_examples=3003, total_duration=19259.912285, train/accuracy=0.653501, train/bleu=31.835666, train/loss=1.888906, validation/accuracy=0.668969, validation/bleu=29.099049, validation/loss=1.772705, validation/num_examples=3000
I0207 15:34:24.974952 140277154002688 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.2599717378616333, loss=3.9182839393615723
I0207 15:34:59.869951 140277145609984 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.2134896218776703, loss=3.9031121730804443
I0207 15:35:34.882274 140277154002688 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.2101193219423294, loss=3.9157888889312744
I0207 15:36:09.906925 140277145609984 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.23957543075084686, loss=3.948511838912964
I0207 15:36:44.918689 140277154002688 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.23530064523220062, loss=3.9298906326293945
I0207 15:37:19.947302 140277145609984 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.4118566513061523, loss=4.000486850738525
I0207 15:37:54.999104 140277154002688 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.21990340948104858, loss=4.000003337860107
I0207 15:38:30.060846 140277145609984 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.23537731170654297, loss=3.9677999019622803
I0207 15:39:05.137407 140277154002688 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.24200962483882904, loss=3.8833200931549072
I0207 15:39:40.188056 140277145609984 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.25339749455451965, loss=3.964104175567627
I0207 15:40:15.217038 140277154002688 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.21048393845558167, loss=3.9126574993133545
I0207 15:40:50.269804 140277145609984 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.2127516269683838, loss=3.8964080810546875
I0207 15:41:25.319607 140277154002688 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.21840393543243408, loss=3.953608512878418
I0207 15:42:00.374003 140277145609984 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.233249232172966, loss=3.988422155380249
I0207 15:42:35.437410 140277154002688 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.23132964968681335, loss=3.906318187713623
I0207 15:43:10.461080 140277145609984 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2255311757326126, loss=3.9407660961151123
I0207 15:43:45.505685 140277154002688 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.23445935547351837, loss=3.9407570362091064
I0207 15:44:20.541599 140277145609984 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.2308843582868576, loss=3.8994851112365723
I0207 15:44:55.590439 140277154002688 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.21343475580215454, loss=3.912564277648926
I0207 15:45:30.638990 140277145609984 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.22172366082668304, loss=3.9798810482025146
I0207 15:46:05.671315 140277154002688 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.2842290997505188, loss=3.928161859512329
I0207 15:46:40.680622 140277145609984 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2531822919845581, loss=3.947889566421509
I0207 15:47:15.702912 140277154002688 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.22827254235744476, loss=3.8785815238952637
I0207 15:47:50.707266 140277145609984 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.2216833382844925, loss=3.9154086112976074
I0207 15:48:14.595187 140446903760704 spec.py:321] Evaluating on the training split.
I0207 15:48:17.600289 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:51:05.823733 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 15:51:08.534705 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:53:47.257749 140446903760704 spec.py:349] Evaluating on the test split.
I0207 15:53:49.960669 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 15:56:16.500950 140446903760704 submission_runner.py:408] Time since start: 20581.94s, 	Step: 35970, 	{'train/accuracy': 0.6479287147521973, 'train/loss': 1.900919795036316, 'train/bleu': 32.29340009964613, 'validation/accuracy': 0.6697747111320496, 'validation/loss': 1.7474881410598755, 'validation/bleu': 29.200182035206225, 'validation/num_examples': 3000, 'test/accuracy': 0.6820057034492493, 'test/loss': 1.6754212379455566, 'test/bleu': 28.820656669087295, 'test/num_examples': 3003, 'score': 12629.10321187973, 'total_duration': 20581.942991495132, 'accumulated_submission_time': 12629.10321187973, 'accumulated_eval_time': 7951.277729272842, 'accumulated_logging_time': 0.4293038845062256}
I0207 15:56:16.520452 140277154002688 logging_writer.py:48] [35970] accumulated_eval_time=7951.277729, accumulated_logging_time=0.429304, accumulated_submission_time=12629.103212, global_step=35970, preemption_count=0, score=12629.103212, test/accuracy=0.682006, test/bleu=28.820657, test/loss=1.675421, test/num_examples=3003, total_duration=20581.942991, train/accuracy=0.647929, train/bleu=32.293400, train/loss=1.900920, validation/accuracy=0.669775, validation/bleu=29.200182, validation/loss=1.747488, validation/num_examples=3000
I0207 15:56:27.372626 140277145609984 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.20959436893463135, loss=3.8734655380249023
I0207 15:57:02.311488 140277154002688 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.2563003599643707, loss=3.9652862548828125
I0207 15:57:37.347850 140277145609984 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.2857093811035156, loss=3.9062137603759766
I0207 15:58:12.420110 140277154002688 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.21929007768630981, loss=3.90322208404541
I0207 15:58:47.498925 140277145609984 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.20323216915130615, loss=3.901733636856079
I0207 15:59:22.548812 140277154002688 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.23318035900592804, loss=3.9575886726379395
I0207 15:59:57.604192 140277145609984 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.28390589356422424, loss=3.903468132019043
I0207 16:00:32.660468 140277154002688 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.244866743683815, loss=3.8931398391723633
I0207 16:01:07.721304 140277145609984 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.25837022066116333, loss=3.897808074951172
I0207 16:01:42.772632 140277154002688 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.23393772542476654, loss=3.9583449363708496
I0207 16:02:17.823955 140277145609984 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2531003952026367, loss=3.9456794261932373
I0207 16:02:52.877369 140277154002688 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.26040807366371155, loss=3.846322774887085
I0207 16:03:27.935459 140277145609984 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.27314427495002747, loss=3.92280912399292
I0207 16:04:03.012673 140277154002688 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.25507357716560364, loss=3.9091310501098633
I0207 16:04:38.070920 140277145609984 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.2530078589916229, loss=3.9321494102478027
I0207 16:05:13.137973 140277154002688 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.2265598624944687, loss=3.9518251419067383
I0207 16:05:48.195914 140277145609984 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.32608428597450256, loss=3.9038519859313965
I0207 16:06:23.248610 140277154002688 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.297794371843338, loss=3.907033681869507
I0207 16:06:58.292749 140277145609984 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.2306424230337143, loss=3.9489240646362305
I0207 16:07:33.367352 140277154002688 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2704312205314636, loss=3.9600324630737305
I0207 16:08:08.471919 140277145609984 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.20719152688980103, loss=3.922887086868286
I0207 16:08:43.546352 140277154002688 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.22996601462364197, loss=3.9224021434783936
I0207 16:09:18.610604 140277145609984 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.22404031455516815, loss=3.899515390396118
I0207 16:09:53.671651 140277154002688 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.24380867183208466, loss=3.917145252227783
I0207 16:10:16.531120 140446903760704 spec.py:321] Evaluating on the training split.
I0207 16:10:19.541412 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:12:58.044340 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 16:13:00.753849 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:15:34.116815 140446903760704 spec.py:349] Evaluating on the test split.
I0207 16:15:36.833283 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:17:53.727356 140446903760704 submission_runner.py:408] Time since start: 21879.17s, 	Step: 38367, 	{'train/accuracy': 0.66447913646698, 'train/loss': 1.7844882011413574, 'train/bleu': 33.572362901384416, 'validation/accuracy': 0.6720189452171326, 'validation/loss': 1.7210848331451416, 'validation/bleu': 29.411130481189385, 'validation/num_examples': 3000, 'test/accuracy': 0.6850618720054626, 'test/loss': 1.644993782043457, 'test/bleu': 29.158325918477285, 'test/num_examples': 3003, 'score': 13469.029118299484, 'total_duration': 21879.169389009476, 'accumulated_submission_time': 13469.029118299484, 'accumulated_eval_time': 8408.473896980286, 'accumulated_logging_time': 0.4591190814971924}
I0207 16:17:53.747291 140277145609984 logging_writer.py:48] [38367] accumulated_eval_time=8408.473897, accumulated_logging_time=0.459119, accumulated_submission_time=13469.029118, global_step=38367, preemption_count=0, score=13469.029118, test/accuracy=0.685062, test/bleu=29.158326, test/loss=1.644994, test/num_examples=3003, total_duration=21879.169389, train/accuracy=0.664479, train/bleu=33.572363, train/loss=1.784488, validation/accuracy=0.672019, validation/bleu=29.411130, validation/loss=1.721085, validation/num_examples=3000
I0207 16:18:05.636096 140277154002688 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.22390836477279663, loss=3.9025533199310303
I0207 16:18:40.587453 140277145609984 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.24771419167518616, loss=3.9029042720794678
I0207 16:19:15.592730 140277154002688 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.26174503564834595, loss=3.95920729637146
I0207 16:19:50.677141 140277145609984 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.21195542812347412, loss=3.8494977951049805
I0207 16:20:25.712933 140277154002688 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.23989234864711761, loss=3.8849737644195557
I0207 16:21:00.785154 140277145609984 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.21573688089847565, loss=3.8777377605438232
I0207 16:21:35.865911 140277154002688 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.22306853532791138, loss=3.9365947246551514
I0207 16:22:10.896231 140277145609984 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.23556913435459137, loss=3.873222827911377
I0207 16:22:45.948358 140277154002688 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.21990551054477692, loss=3.9482781887054443
I0207 16:23:20.991727 140277145609984 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.21772994101047516, loss=3.9084246158599854
I0207 16:23:56.046961 140277154002688 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.25198453664779663, loss=3.9192159175872803
I0207 16:24:31.100719 140277145609984 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.23250427842140198, loss=3.8786628246307373
I0207 16:25:06.135522 140277154002688 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.30584192276000977, loss=3.902196168899536
I0207 16:25:41.183887 140277145609984 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2635222375392914, loss=3.885679244995117
I0207 16:26:16.266631 140277154002688 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.2427954077720642, loss=3.925271987915039
I0207 16:26:51.371544 140277145609984 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.2728644013404846, loss=3.874368906021118
I0207 16:27:26.443454 140277154002688 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.27151188254356384, loss=3.938138961791992
I0207 16:28:01.509641 140277145609984 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.27877137064933777, loss=3.8696608543395996
I0207 16:28:36.609071 140277154002688 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.22414535284042358, loss=3.867462158203125
I0207 16:29:11.700077 140277145609984 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.24950151145458221, loss=3.907224655151367
I0207 16:29:46.745385 140277154002688 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.23557057976722717, loss=3.894036293029785
I0207 16:30:21.822237 140277145609984 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.24725115299224854, loss=3.8403408527374268
I0207 16:30:56.872881 140277154002688 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.2499094158411026, loss=3.92623233795166
I0207 16:31:31.916781 140277145609984 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.25658485293388367, loss=3.8943822383880615
I0207 16:31:54.063507 140446903760704 spec.py:321] Evaluating on the training split.
I0207 16:31:57.072114 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:34:53.327681 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 16:34:56.050360 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:37:24.989655 140446903760704 spec.py:349] Evaluating on the test split.
I0207 16:37:27.701538 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:39:45.400533 140446903760704 submission_runner.py:408] Time since start: 23190.84s, 	Step: 40765, 	{'train/accuracy': 0.6568623781204224, 'train/loss': 1.8464112281799316, 'train/bleu': 32.29865399133282, 'validation/accuracy': 0.6735936403274536, 'validation/loss': 1.7242670059204102, 'validation/bleu': 29.367416114376645, 'validation/num_examples': 3000, 'test/accuracy': 0.685631275177002, 'test/loss': 1.6500742435455322, 'test/bleu': 29.24589113563988, 'test/num_examples': 3003, 'score': 14309.25852894783, 'total_duration': 23190.842586278915, 'accumulated_submission_time': 14309.25852894783, 'accumulated_eval_time': 8879.810878276825, 'accumulated_logging_time': 0.489011287689209}
I0207 16:39:45.420588 140277154002688 logging_writer.py:48] [40765] accumulated_eval_time=8879.810878, accumulated_logging_time=0.489011, accumulated_submission_time=14309.258529, global_step=40765, preemption_count=0, score=14309.258529, test/accuracy=0.685631, test/bleu=29.245891, test/loss=1.650074, test/num_examples=3003, total_duration=23190.842586, train/accuracy=0.656862, train/bleu=32.298654, train/loss=1.846411, validation/accuracy=0.673594, validation/bleu=29.367416, validation/loss=1.724267, validation/num_examples=3000
I0207 16:39:58.017508 140277145609984 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.2272634357213974, loss=3.838033437728882
I0207 16:40:32.980604 140277154002688 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.21409288048744202, loss=3.906471014022827
I0207 16:41:08.051692 140277145609984 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.24562576413154602, loss=3.906700849533081
I0207 16:41:43.095781 140277154002688 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.23249126970767975, loss=3.9282116889953613
I0207 16:42:18.141903 140277145609984 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.2337803691625595, loss=3.9080429077148438
I0207 16:42:53.176277 140277154002688 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.2664613127708435, loss=3.8823838233947754
I0207 16:43:28.261582 140277145609984 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2510000467300415, loss=3.8141095638275146
I0207 16:44:03.355444 140277154002688 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.2500610053539276, loss=3.8648288249969482
I0207 16:44:38.404330 140277145609984 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2296253740787506, loss=3.9222707748413086
I0207 16:45:13.449912 140277154002688 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.28941401839256287, loss=3.9206535816192627
I0207 16:45:48.497594 140277145609984 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.255632609128952, loss=3.8657069206237793
I0207 16:46:23.539478 140277154002688 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.24352356791496277, loss=3.9045841693878174
I0207 16:46:58.582249 140277145609984 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.22699332237243652, loss=3.8823344707489014
I0207 16:47:33.662402 140277154002688 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.26771247386932373, loss=3.863086223602295
I0207 16:48:08.764773 140277145609984 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.25342094898223877, loss=3.9936461448669434
I0207 16:48:43.813562 140277154002688 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.24219423532485962, loss=3.918128728866577
I0207 16:49:18.850863 140277145609984 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2727615237236023, loss=3.8715603351593018
I0207 16:49:53.910489 140277154002688 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2574950158596039, loss=3.9198074340820312
I0207 16:50:28.957161 140277145609984 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.2571404278278351, loss=3.8759329319000244
I0207 16:51:04.042468 140277154002688 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2785797119140625, loss=3.9390969276428223
I0207 16:51:39.105784 140277145609984 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.2707647681236267, loss=3.9060075283050537
I0207 16:52:14.146152 140277154002688 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.2440786510705948, loss=3.8553032875061035
I0207 16:52:49.236270 140277145609984 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.24545049667358398, loss=3.8234753608703613
I0207 16:53:24.302823 140277154002688 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2716846466064453, loss=3.887831449508667
I0207 16:53:45.413461 140446903760704 spec.py:321] Evaluating on the training split.
I0207 16:53:48.413382 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:56:51.898163 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 16:56:54.631683 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 16:59:37.404825 140446903760704 spec.py:349] Evaluating on the test split.
I0207 16:59:40.133618 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 17:02:06.088021 140446903760704 submission_runner.py:408] Time since start: 24531.53s, 	Step: 43162, 	{'train/accuracy': 0.6540481448173523, 'train/loss': 1.8485385179519653, 'train/bleu': 31.766415795889074, 'validation/accuracy': 0.674411952495575, 'validation/loss': 1.7126529216766357, 'validation/bleu': 29.176469017738977, 'validation/num_examples': 3000, 'test/accuracy': 0.6875603199005127, 'test/loss': 1.6370383501052856, 'test/bleu': 28.950136975521982, 'test/num_examples': 3003, 'score': 15149.16226387024, 'total_duration': 24531.530041217804, 'accumulated_submission_time': 15149.16226387024, 'accumulated_eval_time': 9380.485354423523, 'accumulated_logging_time': 0.5209271907806396}
I0207 17:02:06.111476 140277145609984 logging_writer.py:48] [43162] accumulated_eval_time=9380.485354, accumulated_logging_time=0.520927, accumulated_submission_time=15149.162264, global_step=43162, preemption_count=0, score=15149.162264, test/accuracy=0.687560, test/bleu=28.950137, test/loss=1.637038, test/num_examples=3003, total_duration=24531.530041, train/accuracy=0.654048, train/bleu=31.766416, train/loss=1.848539, validation/accuracy=0.674412, validation/bleu=29.176469, validation/loss=1.712653, validation/num_examples=3000
I0207 17:02:19.722035 140277154002688 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.25774458050727844, loss=3.8716280460357666
I0207 17:02:54.654079 140277145609984 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.24632972478866577, loss=3.8485329151153564
I0207 17:03:29.705256 140277154002688 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.26142922043800354, loss=3.895383596420288
I0207 17:04:04.710349 140277145609984 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.2550608515739441, loss=3.84733510017395
I0207 17:04:39.740017 140277154002688 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.25095468759536743, loss=3.9170782566070557
I0207 17:05:14.763349 140277145609984 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.2893555164337158, loss=3.924741506576538
I0207 17:05:49.767533 140277154002688 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2535453140735626, loss=3.8410024642944336
I0207 17:06:24.800882 140277145609984 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.22415713965892792, loss=3.8613038063049316
I0207 17:06:59.831784 140277154002688 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.24849796295166016, loss=3.840970993041992
I0207 17:07:34.850642 140277145609984 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.25266537070274353, loss=3.900632858276367
I0207 17:08:09.885963 140277154002688 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.23071879148483276, loss=3.8774356842041016
I0207 17:08:44.912659 140277145609984 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2765493392944336, loss=3.8080222606658936
I0207 17:09:19.963622 140277154002688 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2660740911960602, loss=3.8406665325164795
I0207 17:09:55.018423 140277145609984 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.22931626439094543, loss=3.8983254432678223
I0207 17:10:30.059644 140277154002688 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.22937139868736267, loss=3.8903915882110596
I0207 17:11:05.086682 140277145609984 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.3969446420669556, loss=7.1588616371154785
I0207 17:11:40.059413 140277154002688 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.6324691772460938, loss=5.618645191192627
I0207 17:12:15.039305 140277145609984 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.47854849696159363, loss=5.523935317993164
I0207 17:12:49.961222 140277154002688 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.047780156135559, loss=5.498212814331055
I0207 17:13:24.928307 140277145609984 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.2801719605922699, loss=5.440833568572998
I0207 17:13:59.896666 140277154002688 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.8077594041824341, loss=5.446749210357666
I0207 17:14:34.840116 140277145609984 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5090100169181824, loss=5.415958404541016
I0207 17:15:09.782148 140277154002688 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.544170081615448, loss=5.436288833618164
I0207 17:15:44.763844 140277145609984 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2992488145828247, loss=5.403450965881348
I0207 17:16:06.149750 140446903760704 spec.py:321] Evaluating on the training split.
I0207 17:16:09.159814 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 17:19:38.278263 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 17:19:40.984843 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 17:23:03.818386 140446903760704 spec.py:349] Evaluating on the test split.
I0207 17:23:06.532455 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 17:26:15.242356 140446903760704 submission_runner.py:408] Time since start: 25980.68s, 	Step: 45563, 	{'train/accuracy': 0.3642924129962921, 'train/loss': 3.769196033477783, 'train/bleu': 1.166315163018871, 'validation/accuracy': 0.32481926679611206, 'validation/loss': 4.211953163146973, 'validation/bleu': 0.17267072757558866, 'validation/num_examples': 3000, 'test/accuracy': 0.3135901391506195, 'test/loss': 4.377139091491699, 'test/bleu': 0.15558510353138613, 'test/num_examples': 3003, 'score': 15989.111895561218, 'total_duration': 25980.68439888954, 'accumulated_submission_time': 15989.111895561218, 'accumulated_eval_time': 9989.577900886536, 'accumulated_logging_time': 0.5572404861450195}
I0207 17:26:15.263861 140277154002688 logging_writer.py:48] [45563] accumulated_eval_time=9989.577901, accumulated_logging_time=0.557240, accumulated_submission_time=15989.111896, global_step=45563, preemption_count=0, score=15989.111896, test/accuracy=0.313590, test/bleu=0.155585, test/loss=4.377139, test/num_examples=3003, total_duration=25980.684399, train/accuracy=0.364292, train/bleu=1.166315, train/loss=3.769196, validation/accuracy=0.324819, validation/bleu=0.172671, validation/loss=4.211953, validation/num_examples=3000
I0207 17:26:28.534945 140277145609984 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.2150498628616333, loss=5.404717922210693
I0207 17:27:03.426188 140277154002688 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.4860090911388397, loss=4.075345516204834
I0207 17:27:38.426537 140277145609984 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.24992166459560394, loss=3.8961172103881836
I0207 17:28:13.473916 140277154002688 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.294068306684494, loss=3.8603711128234863
I0207 17:28:48.533891 140277145609984 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.23011597990989685, loss=3.872788667678833
I0207 17:29:23.586813 140277154002688 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2269444465637207, loss=3.9152255058288574
I0207 17:29:58.627277 140277145609984 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2554786801338196, loss=3.852998733520508
I0207 17:30:33.671225 140277154002688 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.22853407263755798, loss=3.883495807647705
I0207 17:31:08.739842 140277145609984 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.30068397521972656, loss=3.946895122528076
I0207 17:31:43.815060 140277154002688 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3121603727340698, loss=3.938075542449951
I0207 17:32:18.904938 140277145609984 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.25144481658935547, loss=3.91910457611084
I0207 17:32:53.977393 140277154002688 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2151167094707489, loss=3.91599178314209
I0207 17:33:29.011291 140277145609984 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2512813210487366, loss=3.8974578380584717
I0207 17:34:04.073520 140277154002688 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.24753984808921814, loss=3.8700335025787354
I0207 17:34:39.105796 140277145609984 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.2479194849729538, loss=3.8802437782287598
I0207 17:35:14.161787 140277154002688 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.37597668170928955, loss=3.8481462001800537
I0207 17:35:49.213267 140277145609984 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.28332141041755676, loss=3.889822244644165
I0207 17:36:24.250292 140277154002688 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2345133125782013, loss=3.845332622528076
I0207 17:36:59.310579 140277145609984 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.23526225984096527, loss=3.8766539096832275
I0207 17:37:34.379049 140277154002688 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2616370618343353, loss=3.8558099269866943
I0207 17:38:09.460064 140277145609984 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2553585469722748, loss=3.8808817863464355
I0207 17:38:44.509781 140277154002688 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.2833526134490967, loss=3.910404920578003
I0207 17:39:19.597409 140277145609984 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2800193130970001, loss=3.874492645263672
I0207 17:39:54.647506 140277154002688 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.26854443550109863, loss=3.861879587173462
I0207 17:40:15.380186 140446903760704 spec.py:321] Evaluating on the training split.
I0207 17:40:18.390197 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 17:42:56.938520 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 17:42:59.654126 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 17:45:30.780425 140446903760704 spec.py:349] Evaluating on the test split.
I0207 17:45:33.490896 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 17:47:48.370027 140446903760704 submission_runner.py:408] Time since start: 27273.81s, 	Step: 47961, 	{'train/accuracy': 0.6598178744316101, 'train/loss': 1.826006293296814, 'train/bleu': 32.72687596383734, 'validation/accuracy': 0.6744863390922546, 'validation/loss': 1.7195638418197632, 'validation/bleu': 29.72216502720672, 'validation/num_examples': 3000, 'test/accuracy': 0.6885131597518921, 'test/loss': 1.6407841444015503, 'test/bleu': 29.155636425634672, 'test/num_examples': 3003, 'score': 16829.141446828842, 'total_duration': 27273.812072753906, 'accumulated_submission_time': 16829.141446828842, 'accumulated_eval_time': 10442.567687034607, 'accumulated_logging_time': 0.5890069007873535}
I0207 17:47:48.392233 140277145609984 logging_writer.py:48] [47961] accumulated_eval_time=10442.567687, accumulated_logging_time=0.589007, accumulated_submission_time=16829.141447, global_step=47961, preemption_count=0, score=16829.141447, test/accuracy=0.688513, test/bleu=29.155636, test/loss=1.640784, test/num_examples=3003, total_duration=27273.812073, train/accuracy=0.659818, train/bleu=32.726876, train/loss=1.826006, validation/accuracy=0.674486, validation/bleu=29.722165, validation/loss=1.719564, validation/num_examples=3000
I0207 17:48:02.348135 140277154002688 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.29866766929626465, loss=3.8975417613983154
I0207 17:48:37.268128 140277145609984 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3403174877166748, loss=3.87827205657959
I0207 17:49:12.258257 140277154002688 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.23817181587219238, loss=3.92155122756958
I0207 17:49:47.275758 140277145609984 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.27513688802719116, loss=3.8769338130950928
I0207 17:50:22.282683 140277154002688 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.25736701488494873, loss=3.8382604122161865
I0207 17:50:57.317742 140277145609984 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.2616272270679474, loss=3.8665688037872314
I0207 17:51:32.366765 140277154002688 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.24341543018817902, loss=3.926994562149048
I0207 17:52:07.407339 140277145609984 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.23998454213142395, loss=3.898667335510254
I0207 17:52:42.481277 140277154002688 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.2569117248058319, loss=3.8218302726745605
I0207 17:53:17.522245 140277145609984 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.25286829471588135, loss=3.825451612472534
I0207 17:53:52.555000 140277154002688 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.2715916037559509, loss=3.883152961730957
I0207 17:54:27.560857 140277145609984 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.339920312166214, loss=3.8550384044647217
I0207 17:55:02.581856 140277154002688 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2751772403717041, loss=3.8206586837768555
I0207 17:55:37.595000 140277145609984 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.24906885623931885, loss=3.819364547729492
I0207 17:56:12.589454 140277154002688 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3011128604412079, loss=3.8470561504364014
I0207 17:56:47.624431 140277145609984 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2839908003807068, loss=3.9157474040985107
I0207 17:57:22.673155 140277154002688 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.39986366033554077, loss=3.864180088043213
I0207 17:57:57.690682 140277145609984 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.2699401080608368, loss=3.9260387420654297
I0207 17:58:32.694524 140277154002688 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.2671518623828888, loss=3.8396313190460205
I0207 17:59:07.751547 140277145609984 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.24699358642101288, loss=3.895916223526001
I0207 17:59:42.830915 140277154002688 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2796536087989807, loss=3.8590312004089355
I0207 18:00:17.868752 140277145609984 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.3289603590965271, loss=3.9517290592193604
I0207 18:00:52.896126 140277154002688 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2625868618488312, loss=3.8729496002197266
I0207 18:01:27.881746 140277145609984 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2979438304901123, loss=3.812913656234741
I0207 18:01:48.644944 140446903760704 spec.py:321] Evaluating on the training split.
I0207 18:01:51.666841 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:05:43.971520 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 18:05:46.688086 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:09:34.687575 140446903760704 spec.py:349] Evaluating on the test split.
I0207 18:09:37.390839 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:12:38.149337 140446903760704 submission_runner.py:408] Time since start: 28763.59s, 	Step: 50361, 	{'train/accuracy': 0.6838412880897522, 'train/loss': 1.6663483381271362, 'train/bleu': 33.974547470472466, 'validation/accuracy': 0.6764330267906189, 'validation/loss': 1.7032217979431152, 'validation/bleu': 29.85961521752938, 'validation/num_examples': 3000, 'test/accuracy': 0.6898379325866699, 'test/loss': 1.6269718408584595, 'test/bleu': 29.56486532290584, 'test/num_examples': 3003, 'score': 17669.30721282959, 'total_duration': 28763.591370821, 'accumulated_submission_time': 17669.30721282959, 'accumulated_eval_time': 11092.072012662888, 'accumulated_logging_time': 0.622807502746582}
I0207 18:12:38.171499 140277154002688 logging_writer.py:48] [50361] accumulated_eval_time=11092.072013, accumulated_logging_time=0.622808, accumulated_submission_time=17669.307213, global_step=50361, preemption_count=0, score=17669.307213, test/accuracy=0.689838, test/bleu=29.564865, test/loss=1.626972, test/num_examples=3003, total_duration=28763.591371, train/accuracy=0.683841, train/bleu=33.974547, train/loss=1.666348, validation/accuracy=0.676433, validation/bleu=29.859615, validation/loss=1.703222, validation/num_examples=3000
I0207 18:12:52.119212 140277145609984 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.2545391619205475, loss=3.9051899909973145
I0207 18:13:27.022456 140277154002688 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.2428685575723648, loss=3.898916244506836
I0207 18:14:02.002047 140277145609984 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.2963450253009796, loss=3.8354036808013916
I0207 18:14:37.081307 140277154002688 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.25597259402275085, loss=3.8503670692443848
I0207 18:15:12.167000 140277145609984 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2704564034938812, loss=3.870697259902954
I0207 18:15:47.212397 140277154002688 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2662716805934906, loss=3.896746873855591
I0207 18:16:22.211188 140277145609984 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3265320360660553, loss=3.8793444633483887
I0207 18:16:57.239865 140277154002688 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.23174472153186798, loss=3.864243268966675
I0207 18:17:32.243582 140277145609984 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.28384891152381897, loss=3.921534538269043
I0207 18:18:07.278987 140277154002688 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.24060972034931183, loss=3.887468099594116
I0207 18:18:42.339524 140277145609984 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.27052560448646545, loss=3.851428747177124
I0207 18:19:17.368343 140277154002688 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2381889820098877, loss=3.876856565475464
I0207 18:19:52.382169 140277145609984 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.25558486580848694, loss=3.9416375160217285
I0207 18:20:27.409366 140277154002688 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2371106743812561, loss=3.8777408599853516
I0207 18:21:02.446515 140277145609984 logging_writer.py:48] [51800] global_step=51800, grad_norm=9.35876178741455, loss=4.68322229385376
I0207 18:21:37.471302 140277154002688 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2338305413722992, loss=3.847205877304077
I0207 18:22:12.499952 140277145609984 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.25387296080589294, loss=3.8474843502044678
I0207 18:22:47.509727 140277154002688 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.2653298079967499, loss=3.93416428565979
I0207 18:23:22.557561 140277145609984 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.2243989109992981, loss=3.855288505554199
I0207 18:23:57.639720 140277154002688 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.25327250361442566, loss=3.854936122894287
I0207 18:24:32.672371 140277145609984 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.26029273867607117, loss=3.901923656463623
I0207 18:25:07.693374 140277154002688 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.24356703460216522, loss=3.9281370639801025
I0207 18:25:42.728936 140277145609984 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.2620372474193573, loss=3.8914971351623535
I0207 18:26:17.768806 140277154002688 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2473139613866806, loss=3.8885738849639893
I0207 18:26:38.493031 140446903760704 spec.py:321] Evaluating on the training split.
I0207 18:26:41.503594 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:30:49.062298 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 18:30:51.786575 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:33:26.673265 140446903760704 spec.py:349] Evaluating on the test split.
I0207 18:33:29.399326 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:35:52.491787 140446903760704 submission_runner.py:408] Time since start: 30157.93s, 	Step: 52761, 	{'train/accuracy': 0.6685400009155273, 'train/loss': 1.7587262392044067, 'train/bleu': 32.919607532370215, 'validation/accuracy': 0.677734911441803, 'validation/loss': 1.6866081953048706, 'validation/bleu': 29.581894504613768, 'validation/num_examples': 3000, 'test/accuracy': 0.6924641132354736, 'test/loss': 1.6087028980255127, 'test/bleu': 29.45363869235563, 'test/num_examples': 3003, 'score': 18509.54104423523, 'total_duration': 30157.93383049965, 'accumulated_submission_time': 18509.54104423523, 'accumulated_eval_time': 11646.070712327957, 'accumulated_logging_time': 0.6562457084655762}
I0207 18:35:52.514841 140277145609984 logging_writer.py:48] [52761] accumulated_eval_time=11646.070712, accumulated_logging_time=0.656246, accumulated_submission_time=18509.541044, global_step=52761, preemption_count=0, score=18509.541044, test/accuracy=0.692464, test/bleu=29.453639, test/loss=1.608703, test/num_examples=3003, total_duration=30157.933830, train/accuracy=0.668540, train/bleu=32.919608, train/loss=1.758726, validation/accuracy=0.677735, validation/bleu=29.581895, validation/loss=1.686608, validation/num_examples=3000
I0207 18:36:06.463231 140277154002688 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.2432878017425537, loss=3.899925708770752
I0207 18:36:41.365636 140277145609984 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.287590891122818, loss=3.914647340774536
I0207 18:37:16.354754 140277154002688 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.24922043085098267, loss=3.901930093765259
I0207 18:37:51.390217 140277145609984 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.26772305369377136, loss=3.887037754058838
I0207 18:38:26.441704 140277154002688 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.23836007714271545, loss=3.8458902835845947
I0207 18:39:01.551236 140277145609984 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.29219552874565125, loss=3.82637619972229
I0207 18:39:36.587998 140277154002688 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.26271748542785645, loss=3.8813889026641846
I0207 18:40:11.611657 140277145609984 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.30250635743141174, loss=3.8439159393310547
I0207 18:40:46.631688 140277154002688 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.278170108795166, loss=3.8867719173431396
I0207 18:41:21.665817 140277145609984 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2504754364490509, loss=3.859020471572876
I0207 18:41:56.691380 140277154002688 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.3056170344352722, loss=3.900371551513672
I0207 18:42:31.740925 140277145609984 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.24660643935203552, loss=3.8206892013549805
I0207 18:43:06.786226 140277154002688 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.24795903265476227, loss=3.82692551612854
I0207 18:43:41.850862 140277145609984 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.25091487169265747, loss=3.8013455867767334
I0207 18:44:16.890453 140277154002688 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.25627264380455017, loss=3.8642630577087402
I0207 18:44:51.910583 140277145609984 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3767762780189514, loss=3.8024182319641113
I0207 18:45:26.948159 140277154002688 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.2576914131641388, loss=3.803591012954712
I0207 18:46:01.996630 140277145609984 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2822630703449249, loss=3.8513197898864746
I0207 18:46:37.047269 140277154002688 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.24773947894573212, loss=3.876734495162964
I0207 18:47:12.083877 140277145609984 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.30119311809539795, loss=3.854140281677246
I0207 18:47:47.115261 140277154002688 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.26598721742630005, loss=3.811964988708496
I0207 18:48:22.144945 140277145609984 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2753397524356842, loss=3.8770103454589844
I0207 18:48:57.199066 140277154002688 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2682115435600281, loss=3.8302881717681885
I0207 18:49:32.248774 140277145609984 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.3048989176750183, loss=3.899925947189331
I0207 18:49:52.645525 140446903760704 spec.py:321] Evaluating on the training split.
I0207 18:49:55.660498 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:52:50.871781 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 18:52:53.589517 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:55:20.793426 140446903760704 spec.py:349] Evaluating on the test split.
I0207 18:55:23.502136 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 18:57:44.601674 140446903760704 submission_runner.py:408] Time since start: 31470.04s, 	Step: 55160, 	{'train/accuracy': 0.6608024835586548, 'train/loss': 1.809146761894226, 'train/bleu': 32.987301429211975, 'validation/accuracy': 0.6783052682876587, 'validation/loss': 1.6888394355773926, 'validation/bleu': 30.181948989674293, 'validation/num_examples': 3000, 'test/accuracy': 0.6906862258911133, 'test/loss': 1.6112722158432007, 'test/bleu': 29.408274314513523, 'test/num_examples': 3003, 'score': 19349.585392713547, 'total_duration': 31470.043719291687, 'accumulated_submission_time': 19349.585392713547, 'accumulated_eval_time': 12118.026804924011, 'accumulated_logging_time': 0.6894242763519287}
I0207 18:57:44.624208 140277154002688 logging_writer.py:48] [55160] accumulated_eval_time=12118.026805, accumulated_logging_time=0.689424, accumulated_submission_time=19349.585393, global_step=55160, preemption_count=0, score=19349.585393, test/accuracy=0.690686, test/bleu=29.408274, test/loss=1.611272, test/num_examples=3003, total_duration=31470.043719, train/accuracy=0.660802, train/bleu=32.987301, train/loss=1.809147, validation/accuracy=0.678305, validation/bleu=30.181949, validation/loss=1.688839, validation/num_examples=3000
I0207 18:57:58.929278 140277145609984 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.256892591714859, loss=3.802342414855957
I0207 18:58:33.856530 140277154002688 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.26279595494270325, loss=3.8875207901000977
I0207 18:59:08.837270 140277145609984 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.2625107169151306, loss=3.858567476272583
I0207 18:59:43.852098 140277154002688 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.24477502703666687, loss=3.8308029174804688
I0207 19:00:18.885067 140277145609984 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3037453889846802, loss=3.8757073879241943
I0207 19:00:53.906679 140277154002688 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.24451135098934174, loss=3.8107118606567383
I0207 19:01:28.964569 140277145609984 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.252267450094223, loss=3.8165385723114014
I0207 19:02:04.008611 140277154002688 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.3020355701446533, loss=3.9054036140441895
I0207 19:02:39.059381 140277145609984 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2695949673652649, loss=3.8158750534057617
I0207 19:03:14.092915 140277154002688 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2644166052341461, loss=3.810223340988159
I0207 19:03:49.110861 140277145609984 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.265957236289978, loss=3.896327018737793
I0207 19:04:24.168917 140277154002688 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.3428799510002136, loss=3.8847744464874268
I0207 19:04:59.206001 140277145609984 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.24941928684711456, loss=3.827831983566284
I0207 19:05:34.226707 140277154002688 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.26958125829696655, loss=3.853886127471924
I0207 19:06:09.247494 140277145609984 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2876240313053131, loss=3.816002607345581
I0207 19:06:44.258996 140277154002688 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.30110445618629456, loss=3.8974313735961914
I0207 19:07:19.334882 140277145609984 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.26166486740112305, loss=3.9093692302703857
I0207 19:07:54.369493 140277154002688 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.37930363416671753, loss=3.9369077682495117
I0207 19:08:29.384244 140277145609984 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.25540921092033386, loss=3.8167667388916016
I0207 19:09:04.423186 140277154002688 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.26699307560920715, loss=3.8191721439361572
I0207 19:09:39.566654 140277145609984 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.25055772066116333, loss=3.8590810298919678
I0207 19:10:14.629320 140277154002688 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.24628551304340363, loss=3.8546946048736572
I0207 19:10:49.654044 140277145609984 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.2609650790691376, loss=3.8407037258148193
I0207 19:11:24.674762 140277154002688 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.25283366441726685, loss=3.9362783432006836
I0207 19:11:44.703237 140446903760704 spec.py:321] Evaluating on the training split.
I0207 19:11:47.717802 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 19:14:29.261576 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 19:14:31.983589 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 19:16:58.988282 140446903760704 spec.py:349] Evaluating on the test split.
I0207 19:17:01.729694 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 19:19:12.375615 140446903760704 submission_runner.py:408] Time since start: 32757.82s, 	Step: 57559, 	{'train/accuracy': 0.6720222234725952, 'train/loss': 1.7211450338363647, 'train/bleu': 33.596491081578876, 'validation/accuracy': 0.6793467998504639, 'validation/loss': 1.676062822341919, 'validation/bleu': 29.910391819317304, 'validation/num_examples': 3000, 'test/accuracy': 0.6937656402587891, 'test/loss': 1.5925296545028687, 'test/bleu': 29.845140575311913, 'test/num_examples': 3003, 'score': 20189.579274892807, 'total_duration': 32757.81765937805, 'accumulated_submission_time': 20189.579274892807, 'accumulated_eval_time': 12565.69912815094, 'accumulated_logging_time': 0.722074031829834}
I0207 19:19:12.398803 140277145609984 logging_writer.py:48] [57559] accumulated_eval_time=12565.699128, accumulated_logging_time=0.722074, accumulated_submission_time=20189.579275, global_step=57559, preemption_count=0, score=20189.579275, test/accuracy=0.693766, test/bleu=29.845141, test/loss=1.592530, test/num_examples=3003, total_duration=32757.817659, train/accuracy=0.672022, train/bleu=33.596491, train/loss=1.721145, validation/accuracy=0.679347, validation/bleu=29.910392, validation/loss=1.676063, validation/num_examples=3000
I0207 19:19:27.059875 140277154002688 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2822574973106384, loss=3.869016408920288
I0207 19:20:01.948759 140277145609984 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.27513155341148376, loss=3.8997585773468018
I0207 19:20:36.978842 140277154002688 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2650134265422821, loss=3.90092396736145
I0207 19:21:12.015013 140277145609984 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.24402983486652374, loss=3.7755627632141113
I0207 19:21:47.081634 140277154002688 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.4246419668197632, loss=3.820827007293701
I0207 19:22:22.190332 140277145609984 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.2668353319168091, loss=3.8381569385528564
I0207 19:22:57.345518 140277154002688 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.2593221664428711, loss=3.7808597087860107
I0207 19:23:32.398013 140277145609984 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.24167843163013458, loss=3.8238131999969482
I0207 19:24:07.442129 140277154002688 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2581717073917389, loss=3.835771083831787
I0207 19:24:42.476972 140277145609984 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.2541138231754303, loss=3.8606841564178467
I0207 19:25:17.553761 140277154002688 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2718063294887543, loss=3.8890438079833984
I0207 19:25:52.591238 140277145609984 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2665119767189026, loss=3.8402669429779053
I0207 19:26:27.609156 140277154002688 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.27589941024780273, loss=3.8326821327209473
I0207 19:27:02.661880 140277145609984 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.26413533091545105, loss=3.7754929065704346
I0207 19:27:37.698837 140277154002688 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.26890555024147034, loss=3.872335195541382
I0207 19:28:12.761242 140277145609984 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.265039324760437, loss=3.8449902534484863
I0207 19:28:47.946316 140277154002688 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.29642167687416077, loss=3.8420209884643555
I0207 19:29:22.989722 140277145609984 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.30569717288017273, loss=3.872012138366699
I0207 19:29:58.044134 140277154002688 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2656301259994507, loss=3.8324942588806152
I0207 19:30:33.081783 140277145609984 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.26659664511680603, loss=3.8319685459136963
I0207 19:31:08.115534 140277154002688 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3107726275920868, loss=3.851613759994507
I0207 19:31:43.156754 140277145609984 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.29838162660598755, loss=3.818896532058716
I0207 19:32:18.201605 140277154002688 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.28819540143013, loss=3.8153395652770996
I0207 19:32:53.240867 140277145609984 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3220458924770355, loss=3.8094191551208496
I0207 19:33:12.599075 140446903760704 spec.py:321] Evaluating on the training split.
I0207 19:33:15.608157 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 19:36:13.595985 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 19:36:16.321625 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 19:38:56.144329 140446903760704 spec.py:349] Evaluating on the test split.
I0207 19:38:58.872520 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 19:41:20.044687 140446903760704 submission_runner.py:408] Time since start: 34085.49s, 	Step: 59957, 	{'train/accuracy': 0.6659659147262573, 'train/loss': 1.7736477851867676, 'train/bleu': 33.462413155122306, 'validation/accuracy': 0.6806362867355347, 'validation/loss': 1.6755925416946411, 'validation/bleu': 30.146721231820003, 'validation/num_examples': 3000, 'test/accuracy': 0.6962756514549255, 'test/loss': 1.587439775466919, 'test/bleu': 29.90336671635429, 'test/num_examples': 3003, 'score': 21029.692541837692, 'total_duration': 34085.48671579361, 'accumulated_submission_time': 21029.692541837692, 'accumulated_eval_time': 13053.144666194916, 'accumulated_logging_time': 0.7552039623260498}
I0207 19:41:20.068487 140277154002688 logging_writer.py:48] [59957] accumulated_eval_time=13053.144666, accumulated_logging_time=0.755204, accumulated_submission_time=21029.692542, global_step=59957, preemption_count=0, score=21029.692542, test/accuracy=0.696276, test/bleu=29.903367, test/loss=1.587440, test/num_examples=3003, total_duration=34085.486716, train/accuracy=0.665966, train/bleu=33.462413, train/loss=1.773648, validation/accuracy=0.680636, validation/bleu=30.146721, validation/loss=1.675593, validation/num_examples=3000
I0207 19:41:35.429814 140277145609984 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2627388536930084, loss=3.8619794845581055
I0207 19:42:10.350384 140277154002688 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.28849145770072937, loss=3.854992389678955
I0207 19:42:45.376355 140277145609984 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.30289074778556824, loss=3.800783634185791
I0207 19:43:20.400095 140277154002688 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.47755664587020874, loss=4.058438777923584
I0207 19:43:55.431118 140277145609984 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2825271487236023, loss=3.8877291679382324
I0207 19:44:30.434164 140277154002688 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2806847095489502, loss=3.8820888996124268
I0207 19:45:05.428272 140277145609984 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.2580530047416687, loss=3.847372531890869
I0207 19:45:40.475007 140277154002688 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.2582187354564667, loss=3.843223810195923
I0207 19:46:15.515693 140277145609984 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2478208839893341, loss=3.848956346511841
I0207 19:46:50.536877 140277154002688 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.25420570373535156, loss=3.880577564239502
I0207 19:47:25.591597 140277145609984 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.276105672121048, loss=3.8417861461639404
I0207 19:48:00.673298 140277154002688 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.25980979204177856, loss=3.7839338779449463
I0207 19:48:35.699718 140277145609984 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.24519984424114227, loss=3.8281824588775635
I0207 19:49:10.742933 140277154002688 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2696526348590851, loss=3.8693795204162598
I0207 19:49:45.783716 140277145609984 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.30249276757240295, loss=3.830272674560547
I0207 19:50:20.855335 140277154002688 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2562273144721985, loss=3.831005334854126
I0207 19:50:55.930849 140277145609984 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2559185326099396, loss=3.799344301223755
I0207 19:51:30.965011 140277154002688 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.25854024291038513, loss=3.8118035793304443
I0207 19:52:06.000307 140277145609984 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3566124141216278, loss=3.8667094707489014
I0207 19:52:41.037237 140277154002688 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3207560181617737, loss=3.8262522220611572
I0207 19:53:16.077935 140277145609984 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.27172160148620605, loss=3.8657915592193604
I0207 19:53:51.106073 140277154002688 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.253989577293396, loss=3.776785373687744
I0207 19:54:26.141805 140277145609984 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.2960294187068939, loss=3.845195770263672
I0207 19:55:01.175635 140277154002688 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.294096440076828, loss=3.7670071125030518
I0207 19:55:20.144058 140446903760704 spec.py:321] Evaluating on the training split.
I0207 19:55:23.156500 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 19:58:04.218836 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 19:58:06.933253 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:00:34.144696 140446903760704 spec.py:349] Evaluating on the test split.
I0207 20:00:36.869238 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:02:48.640370 140446903760704 submission_runner.py:408] Time since start: 35374.08s, 	Step: 62356, 	{'train/accuracy': 0.6661973595619202, 'train/loss': 1.785617709159851, 'train/bleu': 33.18605866469831, 'validation/accuracy': 0.6803635358810425, 'validation/loss': 1.675857663154602, 'validation/bleu': 29.767176532351236, 'validation/num_examples': 3000, 'test/accuracy': 0.6968218088150024, 'test/loss': 1.5876115560531616, 'test/bleu': 29.91820395719109, 'test/num_examples': 3003, 'score': 21869.68085551262, 'total_duration': 35374.082418203354, 'accumulated_submission_time': 21869.68085551262, 'accumulated_eval_time': 13501.640924453735, 'accumulated_logging_time': 0.7891860008239746}
I0207 20:02:48.663445 140277145609984 logging_writer.py:48] [62356] accumulated_eval_time=13501.640924, accumulated_logging_time=0.789186, accumulated_submission_time=21869.680856, global_step=62356, preemption_count=0, score=21869.680856, test/accuracy=0.696822, test/bleu=29.918204, test/loss=1.587612, test/num_examples=3003, total_duration=35374.082418, train/accuracy=0.666197, train/bleu=33.186059, train/loss=1.785618, validation/accuracy=0.680364, validation/bleu=29.767177, validation/loss=1.675858, validation/num_examples=3000
I0207 20:03:04.357225 140277154002688 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.24702486395835876, loss=3.789368152618408
I0207 20:03:39.276012 140277145609984 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.2956363558769226, loss=3.8096742630004883
I0207 20:04:14.276916 140277154002688 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2993173897266388, loss=3.8143110275268555
I0207 20:04:49.326059 140277145609984 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3128601312637329, loss=3.789900064468384
I0207 20:05:24.372465 140277154002688 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2680116891860962, loss=3.8423030376434326
I0207 20:05:59.436809 140277145609984 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2509497106075287, loss=3.761561155319214
I0207 20:06:34.468126 140277154002688 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.28603535890579224, loss=3.877984046936035
I0207 20:07:09.518132 140277145609984 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.264178067445755, loss=3.7842588424682617
I0207 20:07:44.533527 140277154002688 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2837068736553192, loss=3.8267481327056885
I0207 20:08:19.522086 140277145609984 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.27803534269332886, loss=3.857004165649414
I0207 20:08:54.549413 140277154002688 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.33283042907714844, loss=3.9137654304504395
I0207 20:09:29.582127 140277145609984 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2670257091522217, loss=3.81660795211792
I0207 20:10:04.606482 140277154002688 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2914898693561554, loss=3.7915120124816895
I0207 20:10:39.671447 140277145609984 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.2681923508644104, loss=3.7945001125335693
I0207 20:11:14.686193 140277154002688 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3223262131214142, loss=3.91402006149292
I0207 20:11:49.711166 140277145609984 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2511545717716217, loss=3.8343489170074463
I0207 20:12:24.753873 140277154002688 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.31358203291893005, loss=3.8404650688171387
I0207 20:12:59.766542 140277145609984 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.295052707195282, loss=3.902097225189209
I0207 20:13:34.831970 140277154002688 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.28128230571746826, loss=3.870581865310669
I0207 20:14:09.853947 140277145609984 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.27591386437416077, loss=3.8082058429718018
I0207 20:14:44.871725 140277154002688 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.29679909348487854, loss=3.8551762104034424
I0207 20:15:19.887596 140277145609984 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.275497168302536, loss=3.8433570861816406
I0207 20:15:54.894033 140277154002688 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.27051472663879395, loss=3.777501344680786
I0207 20:16:29.903371 140277145609984 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3505924642086029, loss=3.7626545429229736
I0207 20:16:48.861286 140446903760704 spec.py:321] Evaluating on the training split.
I0207 20:16:51.873385 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:19:56.295710 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 20:19:59.012046 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:22:32.714917 140446903760704 spec.py:349] Evaluating on the test split.
I0207 20:22:35.429735 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:24:52.610477 140446903760704 submission_runner.py:408] Time since start: 36698.05s, 	Step: 64756, 	{'train/accuracy': 0.6719292402267456, 'train/loss': 1.71939218044281, 'train/bleu': 33.31623273945491, 'validation/accuracy': 0.6818762421607971, 'validation/loss': 1.6507298946380615, 'validation/bleu': 30.07580176805558, 'validation/num_examples': 3000, 'test/accuracy': 0.696577787399292, 'test/loss': 1.567109227180481, 'test/bleu': 30.027499266208334, 'test/num_examples': 3003, 'score': 22709.793394088745, 'total_duration': 36698.05253005028, 'accumulated_submission_time': 22709.793394088745, 'accumulated_eval_time': 13985.39007115364, 'accumulated_logging_time': 0.8220915794372559}
I0207 20:24:52.633706 140277154002688 logging_writer.py:48] [64756] accumulated_eval_time=13985.390071, accumulated_logging_time=0.822092, accumulated_submission_time=22709.793394, global_step=64756, preemption_count=0, score=22709.793394, test/accuracy=0.696578, test/bleu=30.027499, test/loss=1.567109, test/num_examples=3003, total_duration=36698.052530, train/accuracy=0.671929, train/bleu=33.316233, train/loss=1.719392, validation/accuracy=0.681876, validation/bleu=30.075802, validation/loss=1.650730, validation/num_examples=3000
I0207 20:25:08.327822 140277145609984 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.2593768239021301, loss=3.8252604007720947
I0207 20:25:43.245533 140277154002688 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.31659072637557983, loss=3.8053712844848633
I0207 20:26:18.296938 140277145609984 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.29413866996765137, loss=3.846252679824829
I0207 20:26:53.352837 140277154002688 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2642403244972229, loss=3.8109538555145264
I0207 20:27:28.396325 140277145609984 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.2772901654243469, loss=3.8533754348754883
I0207 20:28:03.424260 140277154002688 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2776492238044739, loss=3.8201043605804443
I0207 20:28:38.461708 140277145609984 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3154238164424896, loss=3.8689937591552734
I0207 20:29:13.492703 140277154002688 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2638813257217407, loss=3.7946205139160156
I0207 20:29:48.509786 140277145609984 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.31575778126716614, loss=3.8421428203582764
I0207 20:30:23.517074 140277154002688 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.280193567276001, loss=3.818613290786743
I0207 20:30:58.531338 140277145609984 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.26726678013801575, loss=3.810244560241699
I0207 20:31:33.566317 140277154002688 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2799794375896454, loss=3.8028557300567627
I0207 20:32:08.677296 140277145609984 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.26190122961997986, loss=3.818300485610962
I0207 20:32:43.715987 140277154002688 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2828529477119446, loss=3.800262928009033
I0207 20:33:18.742517 140277145609984 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3311178684234619, loss=3.797675848007202
I0207 20:33:53.761450 140277154002688 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.32621046900749207, loss=3.7479422092437744
I0207 20:34:28.790796 140277145609984 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.25599196553230286, loss=3.7709503173828125
I0207 20:35:03.815487 140277154002688 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.26745760440826416, loss=3.834040641784668
I0207 20:35:38.835669 140277145609984 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.25822868943214417, loss=3.7946364879608154
I0207 20:36:13.954778 140277154002688 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.2707229256629944, loss=3.8050296306610107
I0207 20:36:49.007292 140277145609984 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2578970193862915, loss=3.755300283432007
I0207 20:37:24.051796 140277154002688 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.28048402070999146, loss=3.8328394889831543
I0207 20:37:59.105545 140277145609984 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.262947678565979, loss=3.8477766513824463
I0207 20:38:34.126138 140277154002688 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.2793712317943573, loss=3.7598671913146973
I0207 20:38:52.765598 140446903760704 spec.py:321] Evaluating on the training split.
I0207 20:38:55.788679 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:41:35.993509 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 20:41:38.732159 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:44:04.242533 140446903760704 spec.py:349] Evaluating on the test split.
I0207 20:44:06.961399 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 20:46:20.554188 140446903760704 submission_runner.py:408] Time since start: 37986.00s, 	Step: 67155, 	{'train/accuracy': 0.6676704287528992, 'train/loss': 1.7535920143127441, 'train/bleu': 32.8873355357405, 'validation/accuracy': 0.6813802719116211, 'validation/loss': 1.6592007875442505, 'validation/bleu': 29.90753576601113, 'validation/num_examples': 3000, 'test/accuracy': 0.6964499354362488, 'test/loss': 1.5692729949951172, 'test/bleu': 29.772931959805444, 'test/num_examples': 3003, 'score': 23549.837747335434, 'total_duration': 37985.99623680115, 'accumulated_submission_time': 23549.837747335434, 'accumulated_eval_time': 14433.178615570068, 'accumulated_logging_time': 0.8567020893096924}
I0207 20:46:20.578981 140277145609984 logging_writer.py:48] [67155] accumulated_eval_time=14433.178616, accumulated_logging_time=0.856702, accumulated_submission_time=23549.837747, global_step=67155, preemption_count=0, score=23549.837747, test/accuracy=0.696450, test/bleu=29.772932, test/loss=1.569273, test/num_examples=3003, total_duration=37985.996237, train/accuracy=0.667670, train/bleu=32.887336, train/loss=1.753592, validation/accuracy=0.681380, validation/bleu=29.907536, validation/loss=1.659201, validation/num_examples=3000
I0207 20:46:36.646018 140277154002688 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.30430734157562256, loss=3.7803761959075928
I0207 20:47:11.594666 140277145609984 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.27664288878440857, loss=3.7799501419067383
I0207 20:47:46.630118 140277154002688 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.27745309472084045, loss=3.8592300415039062
I0207 20:48:21.673553 140277145609984 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.315768837928772, loss=3.790229082107544
I0207 20:48:56.714529 140277154002688 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.3032095432281494, loss=3.810415029525757
I0207 20:49:31.799105 140277145609984 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.28232014179229736, loss=3.872953414916992
I0207 20:50:06.861095 140277154002688 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.28035253286361694, loss=3.8519179821014404
I0207 20:50:41.898712 140277145609984 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2770671844482422, loss=3.7809877395629883
I0207 20:51:16.933662 140277154002688 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.27567756175994873, loss=3.750701427459717
I0207 20:51:51.969466 140277145609984 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.28600752353668213, loss=3.8392419815063477
I0207 20:52:27.005171 140277154002688 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.28760871291160583, loss=3.842384099960327
I0207 20:53:02.033499 140277145609984 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.2716231346130371, loss=3.7792625427246094
I0207 20:53:37.086946 140277154002688 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2768433392047882, loss=3.8340461254119873
I0207 20:54:12.120854 140277145609984 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2786250114440918, loss=3.7978055477142334
I0207 20:54:47.152863 140277154002688 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.295651912689209, loss=3.8130850791931152
I0207 20:55:22.196724 140277145609984 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.2965499460697174, loss=3.847517490386963
I0207 20:55:57.214894 140277154002688 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.282394140958786, loss=3.876857042312622
I0207 20:56:32.225504 140277145609984 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.26779669523239136, loss=3.7334351539611816
I0207 20:57:07.277553 140277154002688 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.28781822323799133, loss=3.824657678604126
I0207 20:57:42.322365 140277145609984 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2593596875667572, loss=3.8094608783721924
I0207 20:58:17.349372 140277154002688 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.27972549200057983, loss=3.8092970848083496
I0207 20:58:52.387010 140277145609984 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.28260675072669983, loss=3.7932305335998535
I0207 20:59:27.422570 140277154002688 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2682293653488159, loss=3.7590513229370117
I0207 21:00:02.484647 140277145609984 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.2680419981479645, loss=3.799593210220337
I0207 21:00:20.748166 140446903760704 spec.py:321] Evaluating on the training split.
I0207 21:00:23.771760 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:03:38.963574 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 21:03:41.688586 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:06:09.972910 140446903760704 spec.py:349] Evaluating on the test split.
I0207 21:06:12.679043 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:08:33.593822 140446903760704 submission_runner.py:408] Time since start: 39319.04s, 	Step: 69554, 	{'train/accuracy': 0.6829418540000916, 'train/loss': 1.6505063772201538, 'train/bleu': 34.84525919786184, 'validation/accuracy': 0.6819506287574768, 'validation/loss': 1.6456681489944458, 'validation/bleu': 30.260847580355374, 'validation/num_examples': 3000, 'test/accuracy': 0.6994596719741821, 'test/loss': 1.5576194524765015, 'test/bleu': 30.311562866280294, 'test/num_examples': 3003, 'score': 24389.92008113861, 'total_duration': 39319.03587079048, 'accumulated_submission_time': 24389.92008113861, 'accumulated_eval_time': 14926.024219036102, 'accumulated_logging_time': 0.891709566116333}
I0207 21:08:33.619159 140277154002688 logging_writer.py:48] [69554] accumulated_eval_time=14926.024219, accumulated_logging_time=0.891710, accumulated_submission_time=24389.920081, global_step=69554, preemption_count=0, score=24389.920081, test/accuracy=0.699460, test/bleu=30.311563, test/loss=1.557619, test/num_examples=3003, total_duration=39319.035871, train/accuracy=0.682942, train/bleu=34.845259, train/loss=1.650506, validation/accuracy=0.681951, validation/bleu=30.260848, validation/loss=1.645668, validation/num_examples=3000
I0207 21:08:49.999056 140277145609984 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2666105031967163, loss=3.7562484741210938
I0207 21:09:24.952554 140277154002688 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.26694047451019287, loss=3.8042304515838623
I0207 21:09:59.956291 140277145609984 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.27453795075416565, loss=3.808502674102783
I0207 21:10:34.973352 140277154002688 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.29344382882118225, loss=3.829162120819092
I0207 21:11:10.001419 140277145609984 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.28223520517349243, loss=3.768209218978882
I0207 21:11:45.062345 140277154002688 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.262098103761673, loss=3.7691235542297363
I0207 21:12:20.070966 140277145609984 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.29702749848365784, loss=3.809001922607422
I0207 21:12:55.097079 140277154002688 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3031006157398224, loss=3.7825169563293457
I0207 21:13:30.100115 140277145609984 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.26153066754341125, loss=3.7985739707946777
I0207 21:14:05.161622 140277154002688 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2919497489929199, loss=3.7612221240997314
I0207 21:14:40.182299 140277145609984 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.27406421303749084, loss=3.817927837371826
I0207 21:15:15.197257 140277154002688 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2976306676864624, loss=3.839437484741211
I0207 21:15:50.228704 140277145609984 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.2649058699607849, loss=3.787269353866577
I0207 21:16:25.258913 140277154002688 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3027570843696594, loss=3.87695574760437
I0207 21:17:00.345070 140277145609984 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.279908150434494, loss=3.81252384185791
I0207 21:17:35.360802 140277154002688 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2735692858695984, loss=3.7610607147216797
I0207 21:18:10.419442 140277145609984 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.2845376133918762, loss=3.798356294631958
I0207 21:18:45.585540 140277154002688 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.294885516166687, loss=3.84885573387146
I0207 21:19:20.662233 140277145609984 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.26227712631225586, loss=3.7459797859191895
I0207 21:19:55.671304 140277154002688 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3058739900588989, loss=3.748433828353882
I0207 21:20:30.688644 140277145609984 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.29102203249931335, loss=3.7854108810424805
I0207 21:21:05.739024 140277154002688 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2662137746810913, loss=3.80618953704834
I0207 21:21:40.798684 140277145609984 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.2726965844631195, loss=3.8686203956604004
I0207 21:22:15.840350 140277154002688 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.27935242652893066, loss=3.7583255767822266
I0207 21:22:33.772158 140446903760704 spec.py:321] Evaluating on the training split.
I0207 21:22:36.790923 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:25:43.300200 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 21:25:46.003205 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:28:10.810683 140446903760704 spec.py:349] Evaluating on the test split.
I0207 21:28:13.532498 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:30:35.775476 140446903760704 submission_runner.py:408] Time since start: 40641.22s, 	Step: 71953, 	{'train/accuracy': 0.6751496195793152, 'train/loss': 1.7136439085006714, 'train/bleu': 34.044371473527406, 'validation/accuracy': 0.683302104473114, 'validation/loss': 1.647717833518982, 'validation/bleu': 30.32466585747712, 'validation/num_examples': 3000, 'test/accuracy': 0.6987624168395996, 'test/loss': 1.561434030532837, 'test/bleu': 30.008816688644934, 'test/num_examples': 3003, 'score': 25229.987417936325, 'total_duration': 40641.21749544144, 'accumulated_submission_time': 25229.987417936325, 'accumulated_eval_time': 15408.027456998825, 'accumulated_logging_time': 0.9275662899017334}
I0207 21:30:35.805655 140277145609984 logging_writer.py:48] [71953] accumulated_eval_time=15408.027457, accumulated_logging_time=0.927566, accumulated_submission_time=25229.987418, global_step=71953, preemption_count=0, score=25229.987418, test/accuracy=0.698762, test/bleu=30.008817, test/loss=1.561434, test/num_examples=3003, total_duration=40641.217495, train/accuracy=0.675150, train/bleu=34.044371, train/loss=1.713644, validation/accuracy=0.683302, validation/bleu=30.324666, validation/loss=1.647718, validation/num_examples=3000
I0207 21:30:52.577085 140277154002688 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.27738985419273376, loss=3.818932056427002
I0207 21:31:27.497428 140277145609984 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.2775816023349762, loss=3.8147830963134766
I0207 21:32:02.496486 140277154002688 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.27316102385520935, loss=3.7748754024505615
I0207 21:32:37.537207 140277145609984 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.26534271240234375, loss=3.7732415199279785
I0207 21:33:12.561630 140277154002688 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.2912760078907013, loss=3.7761662006378174
I0207 21:33:47.602415 140277145609984 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.297149121761322, loss=3.8124942779541016
I0207 21:34:22.621053 140277154002688 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.2600925862789154, loss=3.6923723220825195
I0207 21:34:57.677158 140277145609984 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.2861655652523041, loss=3.8318917751312256
I0207 21:35:32.725866 140277154002688 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3183373510837555, loss=3.799926519393921
I0207 21:36:07.760577 140277145609984 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.30749276280403137, loss=3.7907915115356445
I0207 21:36:42.791815 140277154002688 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.30857154726982117, loss=3.80190110206604
I0207 21:37:17.833151 140277145609984 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.31257545948028564, loss=3.762909173965454
I0207 21:37:52.838429 140277154002688 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.2816299796104431, loss=3.7969000339508057
I0207 21:38:27.865310 140277145609984 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.2837584316730499, loss=3.7462053298950195
I0207 21:39:02.878968 140277154002688 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.27384278178215027, loss=3.8083972930908203
I0207 21:39:37.925932 140277145609984 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.27210259437561035, loss=3.7627692222595215
I0207 21:40:13.028881 140277154002688 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.2980506420135498, loss=3.7693042755126953
I0207 21:40:48.063775 140277145609984 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.30880171060562134, loss=3.787442922592163
I0207 21:41:23.115326 140277154002688 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.26561951637268066, loss=3.7714450359344482
I0207 21:41:58.155060 140277145609984 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.28570061922073364, loss=3.7963409423828125
I0207 21:42:33.181846 140277154002688 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.29636189341545105, loss=3.7787387371063232
I0207 21:43:08.233440 140277145609984 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.2780660092830658, loss=3.781477928161621
I0207 21:43:43.254227 140277154002688 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.29239219427108765, loss=3.8084022998809814
I0207 21:44:18.287892 140277145609984 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.2912762761116028, loss=3.755878448486328
I0207 21:44:35.865978 140446903760704 spec.py:321] Evaluating on the training split.
I0207 21:44:38.888138 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:47:34.985865 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 21:47:37.699861 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:50:10.182545 140446903760704 spec.py:349] Evaluating on the test split.
I0207 21:50:12.903675 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 21:52:37.066325 140446903760704 submission_runner.py:408] Time since start: 41962.51s, 	Step: 74352, 	{'train/accuracy': 0.6767979860305786, 'train/loss': 1.709175705909729, 'train/bleu': 33.86340278617494, 'validation/accuracy': 0.6831409335136414, 'validation/loss': 1.6428673267364502, 'validation/bleu': 30.189353259735633, 'validation/num_examples': 3000, 'test/accuracy': 0.6999825835227966, 'test/loss': 1.5527477264404297, 'test/bleu': 30.17969249679205, 'test/num_examples': 3003, 'score': 26069.960821390152, 'total_duration': 41962.508370399475, 'accumulated_submission_time': 26069.960821390152, 'accumulated_eval_time': 15889.227751493454, 'accumulated_logging_time': 0.9696736335754395}
I0207 21:52:37.091417 140277154002688 logging_writer.py:48] [74352] accumulated_eval_time=15889.227751, accumulated_logging_time=0.969674, accumulated_submission_time=26069.960821, global_step=74352, preemption_count=0, score=26069.960821, test/accuracy=0.699983, test/bleu=30.179692, test/loss=1.552748, test/num_examples=3003, total_duration=41962.508370, train/accuracy=0.676798, train/bleu=33.863403, train/loss=1.709176, validation/accuracy=0.683141, validation/bleu=30.189353, validation/loss=1.642867, validation/num_examples=3000
I0207 21:52:54.206374 140277145609984 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.2707054316997528, loss=3.849048376083374
I0207 21:53:29.136606 140277154002688 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.26219597458839417, loss=3.79764723777771
I0207 21:54:04.204860 140277145609984 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2668423652648926, loss=3.7978200912475586
I0207 21:54:39.241224 140277154002688 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.2884729504585266, loss=3.8079612255096436
I0207 21:55:14.290249 140277145609984 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.27960988879203796, loss=3.761690616607666
I0207 21:55:49.342394 140277154002688 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.28157466650009155, loss=3.7117385864257812
I0207 21:56:24.380536 140277145609984 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.28304097056388855, loss=3.772965431213379
I0207 21:56:59.417450 140277154002688 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2885919213294983, loss=3.7642335891723633
I0207 21:57:34.470871 140277145609984 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.28834983706474304, loss=3.8101346492767334
I0207 21:58:09.539259 140277154002688 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.2868788540363312, loss=3.7390124797821045
I0207 21:58:44.650079 140277145609984 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.27826744318008423, loss=3.6954009532928467
I0207 21:59:19.744985 140277154002688 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.27332189679145813, loss=3.723904609680176
I0207 21:59:54.823348 140277145609984 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.29190605878829956, loss=3.813953399658203
I0207 22:00:29.863568 140277154002688 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.2623993456363678, loss=3.73852801322937
I0207 22:01:04.916430 140277145609984 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.29636287689208984, loss=3.7755656242370605
I0207 22:01:39.965896 140277154002688 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.359599769115448, loss=3.808232307434082
I0207 22:02:15.013522 140277145609984 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.28467562794685364, loss=3.7884175777435303
I0207 22:02:50.035704 140277154002688 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.3633281886577606, loss=3.798334836959839
I0207 22:03:25.092145 140277145609984 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.28107184171676636, loss=3.7980706691741943
I0207 22:04:00.225961 140277154002688 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.2980800271034241, loss=3.836721420288086
I0207 22:04:35.308052 140277145609984 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3005853593349457, loss=3.8299877643585205
I0207 22:05:10.359569 140277154002688 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.3056482970714569, loss=3.7948031425476074
I0207 22:05:45.427578 140277145609984 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3337821364402771, loss=3.775695562362671
I0207 22:06:20.476020 140277154002688 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2732785940170288, loss=3.6799798011779785
I0207 22:06:37.350725 140446903760704 spec.py:321] Evaluating on the training split.
I0207 22:06:40.366602 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:09:31.193565 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 22:09:33.903347 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:12:09.580846 140446903760704 spec.py:349] Evaluating on the test split.
I0207 22:12:12.296767 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:14:36.620738 140446903760704 submission_runner.py:408] Time since start: 43282.06s, 	Step: 76750, 	{'train/accuracy': 0.6845331788063049, 'train/loss': 1.6683518886566162, 'train/bleu': 34.869740284048476, 'validation/accuracy': 0.683984100818634, 'validation/loss': 1.6397377252578735, 'validation/bleu': 30.00869039508164, 'validation/num_examples': 3000, 'test/accuracy': 0.6988554000854492, 'test/loss': 1.5575097799301147, 'test/bleu': 30.020957346602792, 'test/num_examples': 3003, 'score': 26910.131754636765, 'total_duration': 43282.06278991699, 'accumulated_submission_time': 26910.131754636765, 'accumulated_eval_time': 16368.497725009918, 'accumulated_logging_time': 1.0062716007232666}
I0207 22:14:36.646281 140277145609984 logging_writer.py:48] [76750] accumulated_eval_time=16368.497725, accumulated_logging_time=1.006272, accumulated_submission_time=26910.131755, global_step=76750, preemption_count=0, score=26910.131755, test/accuracy=0.698855, test/bleu=30.020957, test/loss=1.557510, test/num_examples=3003, total_duration=43282.062790, train/accuracy=0.684533, train/bleu=34.869740, train/loss=1.668352, validation/accuracy=0.683984, validation/bleu=30.008690, validation/loss=1.639738, validation/num_examples=3000
I0207 22:14:54.447154 140277154002688 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.28421932458877563, loss=3.8019113540649414
I0207 22:15:29.403159 140277145609984 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.27760401368141174, loss=3.8074779510498047
I0207 22:16:04.431194 140277154002688 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.29987239837646484, loss=3.8210599422454834
I0207 22:16:39.460153 140277145609984 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.30683135986328125, loss=3.7827651500701904
I0207 22:17:14.483849 140277154002688 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.28516724705696106, loss=3.797602415084839
I0207 22:17:49.532615 140277145609984 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.30435365438461304, loss=3.8238725662231445
I0207 22:18:24.604804 140277154002688 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.3088972866535187, loss=3.799093008041382
I0207 22:18:59.663072 140277145609984 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2797092795372009, loss=3.767720937728882
I0207 22:19:34.728856 140277154002688 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.30709800124168396, loss=3.737100124359131
I0207 22:20:09.777459 140277145609984 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.29055631160736084, loss=3.7498884201049805
I0207 22:20:44.778499 140277154002688 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.291843056678772, loss=3.7340247631073
I0207 22:21:19.822655 140277145609984 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.29782164096832275, loss=3.834336996078491
I0207 22:21:54.899882 140277154002688 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.28704115748405457, loss=3.784531593322754
I0207 22:22:29.953881 140277145609984 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2955845296382904, loss=3.7568774223327637
I0207 22:23:04.988350 140277154002688 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.27903375029563904, loss=3.7712836265563965
I0207 22:23:40.008270 140277145609984 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.28372061252593994, loss=3.7684342861175537
I0207 22:24:15.032666 140277154002688 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.2845998704433441, loss=3.724921464920044
I0207 22:24:50.074700 140277145609984 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2797113358974457, loss=3.7475674152374268
I0207 22:25:25.109913 140277154002688 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2964152991771698, loss=3.8361167907714844
I0207 22:26:00.138948 140277145609984 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.3160799443721771, loss=3.8010406494140625
I0207 22:26:35.188964 140277154002688 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.28876450657844543, loss=3.7780590057373047
I0207 22:27:10.229696 140277145609984 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.27502700686454773, loss=3.773439407348633
I0207 22:27:45.277592 140277154002688 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.2916370630264282, loss=3.804717779159546
I0207 22:28:20.317828 140277145609984 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.28463149070739746, loss=3.7685387134552
I0207 22:28:36.865111 140446903760704 spec.py:321] Evaluating on the training split.
I0207 22:28:39.880069 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:31:16.776105 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 22:31:19.493837 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:33:53.451618 140446903760704 spec.py:349] Evaluating on the test split.
I0207 22:33:56.202552 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:36:24.201230 140446903760704 submission_runner.py:408] Time since start: 44589.64s, 	Step: 79149, 	{'train/accuracy': 0.6779137253761292, 'train/loss': 1.6993038654327393, 'train/bleu': 33.970007069879514, 'validation/accuracy': 0.684653639793396, 'validation/loss': 1.6361262798309326, 'validation/bleu': 30.250622985921563, 'validation/num_examples': 3000, 'test/accuracy': 0.7008192539215088, 'test/loss': 1.5451098680496216, 'test/bleu': 30.329277062653787, 'test/num_examples': 3003, 'score': 27750.265002012253, 'total_duration': 44589.64325428009, 'accumulated_submission_time': 27750.265002012253, 'accumulated_eval_time': 16835.83377790451, 'accumulated_logging_time': 1.0420207977294922}
I0207 22:36:24.232641 140277154002688 logging_writer.py:48] [79149] accumulated_eval_time=16835.833778, accumulated_logging_time=1.042021, accumulated_submission_time=27750.265002, global_step=79149, preemption_count=0, score=27750.265002, test/accuracy=0.700819, test/bleu=30.329277, test/loss=1.545110, test/num_examples=3003, total_duration=44589.643254, train/accuracy=0.677914, train/bleu=33.970007, train/loss=1.699304, validation/accuracy=0.684654, validation/bleu=30.250623, validation/loss=1.636126, validation/num_examples=3000
I0207 22:36:42.383217 140277145609984 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.27868950366973877, loss=3.757995367050171
I0207 22:37:17.310266 140277154002688 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.27397990226745605, loss=3.772643566131592
I0207 22:37:52.298014 140277145609984 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3057973086833954, loss=3.7597529888153076
I0207 22:38:27.319024 140277154002688 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.29402971267700195, loss=3.7242684364318848
I0207 22:39:02.359407 140277145609984 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.29173770546913147, loss=3.7449238300323486
I0207 22:39:37.412354 140277154002688 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2771216630935669, loss=3.81119704246521
I0207 22:40:12.436215 140277145609984 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.28586336970329285, loss=3.751798391342163
I0207 22:40:47.482355 140277154002688 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.28052929043769836, loss=3.7863266468048096
I0207 22:41:22.521982 140277145609984 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.312931090593338, loss=3.7239115238189697
I0207 22:41:57.589647 140277154002688 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3312279284000397, loss=3.795151472091675
I0207 22:42:32.617361 140277145609984 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.28957146406173706, loss=3.771294593811035
I0207 22:43:07.666334 140277154002688 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2872805595397949, loss=3.791334867477417
I0207 22:43:42.685141 140277145609984 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.2859266400337219, loss=3.7657878398895264
I0207 22:44:17.713579 140277154002688 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.2847634553909302, loss=3.769038438796997
I0207 22:44:52.746404 140277145609984 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.29941776394844055, loss=3.783783197402954
I0207 22:45:27.788685 140277154002688 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.2887909710407257, loss=3.7891533374786377
I0207 22:46:02.829895 140277145609984 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.29423150420188904, loss=3.738220453262329
I0207 22:46:37.863564 140277154002688 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.29296764731407166, loss=3.8119418621063232
I0207 22:47:12.866518 140277145609984 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.28486377000808716, loss=3.7848317623138428
I0207 22:47:47.883226 140277154002688 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.28310126066207886, loss=3.727806329727173
I0207 22:48:22.900916 140277145609984 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.275265634059906, loss=3.726003646850586
I0207 22:48:57.963261 140277154002688 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.28618550300598145, loss=3.7002522945404053
I0207 22:49:32.982996 140277145609984 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.30590957403182983, loss=3.8638761043548584
I0207 22:50:07.969088 140277154002688 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2788078486919403, loss=3.7379939556121826
I0207 22:50:24.476720 140446903760704 spec.py:321] Evaluating on the training split.
I0207 22:50:27.489549 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:53:07.821878 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 22:53:10.531991 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:55:41.942435 140446903760704 spec.py:349] Evaluating on the test split.
I0207 22:55:44.654722 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 22:58:01.496447 140446903760704 submission_runner.py:408] Time since start: 45886.94s, 	Step: 81549, 	{'train/accuracy': 0.7079644799232483, 'train/loss': 1.5214544534683228, 'train/bleu': 36.10748172508141, 'validation/accuracy': 0.6850627660751343, 'validation/loss': 1.627657413482666, 'validation/bleu': 30.422390302489923, 'validation/num_examples': 3000, 'test/accuracy': 0.701830267906189, 'test/loss': 1.5316359996795654, 'test/bleu': 30.135994300813106, 'test/num_examples': 3003, 'score': 28590.42280459404, 'total_duration': 45886.93843173981, 'accumulated_submission_time': 28590.42280459404, 'accumulated_eval_time': 17292.853385925293, 'accumulated_logging_time': 1.0856754779815674}
I0207 22:58:01.536761 140277145609984 logging_writer.py:48] [81549] accumulated_eval_time=17292.853386, accumulated_logging_time=1.085675, accumulated_submission_time=28590.422805, global_step=81549, preemption_count=0, score=28590.422805, test/accuracy=0.701830, test/bleu=30.135994, test/loss=1.531636, test/num_examples=3003, total_duration=45886.938432, train/accuracy=0.707964, train/bleu=36.107482, train/loss=1.521454, validation/accuracy=0.685063, validation/bleu=30.422390, validation/loss=1.627657, validation/num_examples=3000
I0207 22:58:19.692985 140277154002688 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.31221190094947815, loss=3.789076089859009
I0207 22:58:54.610854 140277145609984 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.30363866686820984, loss=3.7361221313476562
I0207 22:59:29.651246 140277154002688 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.30842307209968567, loss=3.7921879291534424
I0207 23:00:04.677120 140277145609984 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.27699485421180725, loss=3.755279779434204
I0207 23:00:39.694450 140277154002688 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.31385257840156555, loss=3.727534532546997
I0207 23:01:14.742254 140277145609984 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2907635569572449, loss=3.7452197074890137
I0207 23:01:49.800030 140277154002688 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.2991306781768799, loss=3.715428113937378
I0207 23:02:24.856519 140277145609984 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.31839218735694885, loss=3.8106770515441895
I0207 23:02:59.894675 140277154002688 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.30635830760002136, loss=3.7468388080596924
I0207 23:03:34.965360 140277145609984 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2921791672706604, loss=3.7264559268951416
I0207 23:04:09.996096 140277154002688 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.2992476224899292, loss=3.739008665084839
I0207 23:04:45.037971 140277145609984 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.2839207649230957, loss=3.75683856010437
I0207 23:05:20.101129 140277154002688 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.3249445855617523, loss=3.7225897312164307
I0207 23:05:55.207909 140277145609984 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.29826873540878296, loss=3.7741849422454834
I0207 23:06:30.252366 140277154002688 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.30535006523132324, loss=3.7172300815582275
I0207 23:07:05.324209 140277145609984 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.274842768907547, loss=3.7300026416778564
I0207 23:07:40.344853 140277154002688 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.317632794380188, loss=3.750072479248047
I0207 23:08:15.385194 140277145609984 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2893993556499481, loss=3.7299280166625977
I0207 23:08:50.406077 140277154002688 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.31085991859436035, loss=3.7408642768859863
I0207 23:09:25.451492 140277145609984 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.31861743330955505, loss=3.7389769554138184
I0207 23:10:00.498994 140277154002688 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.29938915371894836, loss=3.7653567790985107
I0207 23:10:35.568111 140277145609984 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.29044321179389954, loss=3.8031165599823
I0207 23:11:10.627491 140277154002688 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.3083612322807312, loss=3.802354097366333
I0207 23:11:45.711443 140277145609984 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.2940106689929962, loss=3.7825732231140137
I0207 23:12:01.564867 140446903760704 spec.py:321] Evaluating on the training split.
I0207 23:12:04.585063 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 23:14:48.742230 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 23:14:51.453356 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 23:17:16.167704 140446903760704 spec.py:349] Evaluating on the test split.
I0207 23:17:18.888863 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 23:19:36.426020 140446903760704 submission_runner.py:408] Time since start: 47181.87s, 	Step: 83947, 	{'train/accuracy': 0.6867976188659668, 'train/loss': 1.6339534521102905, 'train/bleu': 34.510879718808624, 'validation/accuracy': 0.6870218515396118, 'validation/loss': 1.6197893619537354, 'validation/bleu': 30.322945821213274, 'validation/num_examples': 3000, 'test/accuracy': 0.7033990025520325, 'test/loss': 1.5269720554351807, 'test/bleu': 30.452418231049375, 'test/num_examples': 3003, 'score': 29430.36559510231, 'total_duration': 47181.868070364, 'accumulated_submission_time': 29430.36559510231, 'accumulated_eval_time': 17747.714488744736, 'accumulated_logging_time': 1.1382238864898682}
I0207 23:19:36.453137 140277154002688 logging_writer.py:48] [83947] accumulated_eval_time=17747.714489, accumulated_logging_time=1.138224, accumulated_submission_time=29430.365595, global_step=83947, preemption_count=0, score=29430.365595, test/accuracy=0.703399, test/bleu=30.452418, test/loss=1.526972, test/num_examples=3003, total_duration=47181.868070, train/accuracy=0.686798, train/bleu=34.510880, train/loss=1.633953, validation/accuracy=0.687022, validation/bleu=30.322946, validation/loss=1.619789, validation/num_examples=3000
I0207 23:19:55.286905 140277145609984 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.32918596267700195, loss=3.7588119506835938
I0207 23:20:30.225540 140277154002688 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.31402575969696045, loss=3.762329578399658
I0207 23:21:05.232228 140277145609984 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3267190754413605, loss=3.7676076889038086
I0207 23:21:40.267765 140277154002688 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3235376179218292, loss=3.7966699600219727
I0207 23:22:15.310626 140277145609984 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.31809675693511963, loss=3.775207996368408
I0207 23:22:50.342891 140277154002688 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.32608574628829956, loss=3.7977166175842285
I0207 23:23:25.356499 140277145609984 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.30728697776794434, loss=3.7566096782684326
I0207 23:24:00.392898 140277154002688 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.3020560145378113, loss=3.7052037715911865
I0207 23:24:35.414002 140277145609984 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.31287387013435364, loss=3.7402989864349365
I0207 23:25:10.446332 140277154002688 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.3059542179107666, loss=3.74066424369812
I0207 23:25:45.508949 140277145609984 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.31537720561027527, loss=3.7605066299438477
I0207 23:26:20.558400 140277154002688 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.29707053303718567, loss=3.744046211242676
I0207 23:26:55.599319 140277145609984 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3074829876422882, loss=3.7523245811462402
I0207 23:27:30.630263 140277154002688 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.2903653383255005, loss=3.8120546340942383
I0207 23:28:05.658547 140277145609984 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3049072325229645, loss=3.7648255825042725
I0207 23:28:40.697505 140277154002688 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.31001922488212585, loss=3.8107221126556396
I0207 23:29:15.724057 140277145609984 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.31140294671058655, loss=3.7662055492401123
I0207 23:29:50.747112 140277154002688 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.3314725160598755, loss=3.765115261077881
I0207 23:30:25.824391 140277145609984 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.3263392448425293, loss=3.7492306232452393
I0207 23:31:00.890456 140277154002688 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.28340545296669006, loss=3.7656188011169434
I0207 23:31:35.958210 140277145609984 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.3566765785217285, loss=3.794965982437134
I0207 23:32:11.000141 140277154002688 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.32189908623695374, loss=3.7980895042419434
I0207 23:32:46.040182 140277145609984 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.3278934955596924, loss=3.7401227951049805
I0207 23:33:21.107783 140277154002688 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.28570976853370667, loss=3.75860333442688
I0207 23:33:36.583580 140446903760704 spec.py:321] Evaluating on the training split.
I0207 23:33:39.604777 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 23:36:58.094219 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 23:37:00.808332 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 23:39:34.968627 140446903760704 spec.py:349] Evaluating on the test split.
I0207 23:39:37.689852 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 23:41:54.180165 140446903760704 submission_runner.py:408] Time since start: 48519.62s, 	Step: 86346, 	{'train/accuracy': 0.6832473278045654, 'train/loss': 1.6666216850280762, 'train/bleu': 34.83272741944827, 'validation/accuracy': 0.6869474649429321, 'validation/loss': 1.6207863092422485, 'validation/bleu': 30.546641891474536, 'validation/num_examples': 3000, 'test/accuracy': 0.7028993368148804, 'test/loss': 1.5305290222167969, 'test/bleu': 30.481362155277292, 'test/num_examples': 3003, 'score': 30270.410665035248, 'total_duration': 48519.62218332291, 'accumulated_submission_time': 30270.410665035248, 'accumulated_eval_time': 18245.310992002487, 'accumulated_logging_time': 1.1753811836242676}
I0207 23:41:54.211824 140277145609984 logging_writer.py:48] [86346] accumulated_eval_time=18245.310992, accumulated_logging_time=1.175381, accumulated_submission_time=30270.410665, global_step=86346, preemption_count=0, score=30270.410665, test/accuracy=0.702899, test/bleu=30.481362, test/loss=1.530529, test/num_examples=3003, total_duration=48519.622183, train/accuracy=0.683247, train/bleu=34.832727, train/loss=1.666622, validation/accuracy=0.686947, validation/bleu=30.546642, validation/loss=1.620786, validation/num_examples=3000
I0207 23:42:13.411194 140277154002688 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.361581951379776, loss=3.7670419216156006
I0207 23:42:48.291005 140277145609984 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3321985602378845, loss=3.7647194862365723
I0207 23:43:23.275273 140277154002688 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.31314361095428467, loss=3.7641334533691406
I0207 23:43:58.298264 140277145609984 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.34723222255706787, loss=3.747851610183716
I0207 23:44:33.308889 140277154002688 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.2990039885044098, loss=3.7025535106658936
I0207 23:45:08.323645 140277145609984 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.3233327865600586, loss=3.7329301834106445
I0207 23:45:43.364409 140277154002688 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2932640016078949, loss=3.712401866912842
I0207 23:46:18.434532 140277145609984 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.31030723452568054, loss=3.7544727325439453
I0207 23:46:53.483785 140277154002688 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.28897401690483093, loss=3.722092390060425
I0207 23:47:28.553971 140277145609984 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.3026260733604431, loss=3.706584930419922
I0207 23:48:03.605314 140277154002688 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3080679774284363, loss=3.7227578163146973
I0207 23:48:38.659716 140277145609984 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3033420741558075, loss=3.683894634246826
I0207 23:49:13.694046 140277154002688 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.32315781712532043, loss=3.75667667388916
I0207 23:49:48.721929 140277145609984 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.3148348033428192, loss=3.720150947570801
I0207 23:50:23.730996 140277154002688 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.2982507050037384, loss=3.689038038253784
I0207 23:50:58.761988 140277145609984 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3019323945045471, loss=3.745391845703125
I0207 23:51:33.789675 140277154002688 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.30828699469566345, loss=3.736419916152954
I0207 23:52:08.827983 140277145609984 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3061882257461548, loss=3.750843048095703
I0207 23:52:43.841641 140277154002688 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3142210841178894, loss=3.7610766887664795
I0207 23:53:18.881491 140277145609984 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.3348787724971771, loss=3.7404189109802246
I0207 23:53:53.891714 140277154002688 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.30958256125450134, loss=3.71884822845459
I0207 23:54:28.931676 140277145609984 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.30920907855033875, loss=3.721022129058838
I0207 23:55:03.946193 140277154002688 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3207319378852844, loss=3.7690110206604004
I0207 23:55:38.948774 140277145609984 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.36247095465660095, loss=3.7735772132873535
I0207 23:55:54.422654 140446903760704 spec.py:321] Evaluating on the training split.
I0207 23:55:57.440984 140446903760704 workload.py:181] Translating evaluation dataset.
I0207 23:58:43.095543 140446903760704 spec.py:333] Evaluating on the validation split.
I0207 23:58:45.810182 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:01:15.633888 140446903760704 spec.py:349] Evaluating on the test split.
I0208 00:01:18.358210 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:03:42.930559 140446903760704 submission_runner.py:408] Time since start: 49828.37s, 	Step: 88746, 	{'train/accuracy': 0.7003049254417419, 'train/loss': 1.5541876554489136, 'train/bleu': 35.16525340135614, 'validation/accuracy': 0.6866498589515686, 'validation/loss': 1.6179324388504028, 'validation/bleu': 30.46771344521242, 'validation/num_examples': 3000, 'test/accuracy': 0.7047004699707031, 'test/loss': 1.5223288536071777, 'test/bleu': 30.563865944400106, 'test/num_examples': 3003, 'score': 31110.536297559738, 'total_duration': 49828.37257575989, 'accumulated_submission_time': 31110.536297559738, 'accumulated_eval_time': 18713.81881380081, 'accumulated_logging_time': 1.2181808948516846}
I0208 00:03:42.964476 140277154002688 logging_writer.py:48] [88746] accumulated_eval_time=18713.818814, accumulated_logging_time=1.218181, accumulated_submission_time=31110.536298, global_step=88746, preemption_count=0, score=31110.536298, test/accuracy=0.704700, test/bleu=30.563866, test/loss=1.522329, test/num_examples=3003, total_duration=49828.372576, train/accuracy=0.700305, train/bleu=35.165253, train/loss=1.554188, validation/accuracy=0.686650, validation/bleu=30.467713, validation/loss=1.617932, validation/num_examples=3000
I0208 00:04:02.145710 140277145609984 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3055914342403412, loss=3.738600730895996
I0208 00:04:37.052190 140277154002688 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3101192116737366, loss=3.7258379459381104
I0208 00:05:12.053847 140277145609984 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3070608377456665, loss=3.7013800144195557
I0208 00:05:47.068986 140277154002688 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.31565847992897034, loss=3.7190961837768555
I0208 00:06:22.086134 140277145609984 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3111298084259033, loss=3.7601916790008545
I0208 00:06:57.118896 140277154002688 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.29568931460380554, loss=3.6696603298187256
I0208 00:07:32.144233 140277145609984 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.28615739941596985, loss=3.6693296432495117
I0208 00:08:07.167438 140277154002688 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.327070415019989, loss=3.7748093605041504
I0208 00:08:42.188003 140277145609984 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.36249154806137085, loss=3.7512106895446777
I0208 00:09:17.229483 140277154002688 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3054776191711426, loss=3.7635750770568848
I0208 00:09:52.242706 140277145609984 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.31327560544013977, loss=3.720047950744629
I0208 00:10:27.267776 140277154002688 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.318338543176651, loss=3.770832061767578
I0208 00:11:02.283636 140277145609984 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.2979808747768402, loss=3.756782054901123
I0208 00:11:37.324497 140277154002688 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.31005337834358215, loss=3.77221941947937
I0208 00:12:12.363334 140277145609984 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.30793073773384094, loss=3.732741594314575
I0208 00:12:47.386608 140277154002688 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.34299561381340027, loss=3.7802820205688477
I0208 00:13:22.423837 140277145609984 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3459666669368744, loss=3.7555320262908936
I0208 00:13:57.568974 140277154002688 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.31943339109420776, loss=3.7096502780914307
I0208 00:14:32.596640 140277145609984 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.33687543869018555, loss=3.7702600955963135
I0208 00:15:07.631243 140277154002688 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.3056127429008484, loss=3.7276227474212646
I0208 00:15:42.661166 140277145609984 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.30818629264831543, loss=3.7851450443267822
I0208 00:16:17.694410 140277154002688 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.3181205093860626, loss=3.716217517852783
I0208 00:16:52.720526 140277145609984 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.31078433990478516, loss=3.7267205715179443
I0208 00:17:27.787512 140277154002688 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.3170495629310608, loss=3.6661324501037598
I0208 00:17:42.948267 140446903760704 spec.py:321] Evaluating on the training split.
I0208 00:17:45.974723 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:20:32.627459 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 00:20:35.336652 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:22:59.540930 140446903760704 spec.py:349] Evaluating on the test split.
I0208 00:23:02.282648 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:25:18.408684 140446903760704 submission_runner.py:408] Time since start: 51123.85s, 	Step: 91145, 	{'train/accuracy': 0.6862742900848389, 'train/loss': 1.6273705959320068, 'train/bleu': 34.99582119196632, 'validation/accuracy': 0.6887701153755188, 'validation/loss': 1.612417221069336, 'validation/bleu': 30.406371078037647, 'validation/num_examples': 3000, 'test/accuracy': 0.7067689299583435, 'test/loss': 1.5179078578948975, 'test/bleu': 30.68588993681621, 'test/num_examples': 3003, 'score': 31950.433703422546, 'total_duration': 51123.85071182251, 'accumulated_submission_time': 31950.433703422546, 'accumulated_eval_time': 19169.279168844223, 'accumulated_logging_time': 1.2637646198272705}
I0208 00:25:18.437172 140277145609984 logging_writer.py:48] [91145] accumulated_eval_time=19169.279169, accumulated_logging_time=1.263765, accumulated_submission_time=31950.433703, global_step=91145, preemption_count=0, score=31950.433703, test/accuracy=0.706769, test/bleu=30.685890, test/loss=1.517908, test/num_examples=3003, total_duration=51123.850712, train/accuracy=0.686274, train/bleu=34.995821, train/loss=1.627371, validation/accuracy=0.688770, validation/bleu=30.406371, validation/loss=1.612417, validation/num_examples=3000
I0208 00:25:37.980637 140277154002688 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3184019923210144, loss=3.737760305404663
I0208 00:26:12.930785 140277145609984 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.3181757926940918, loss=3.690206527709961
I0208 00:26:47.955953 140277154002688 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.33485496044158936, loss=3.7050719261169434
I0208 00:27:22.989157 140277145609984 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.3241402804851532, loss=3.704047918319702
I0208 00:27:58.030860 140277154002688 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.32117408514022827, loss=3.7716453075408936
I0208 00:28:33.047951 140277145609984 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.3125961124897003, loss=3.7185211181640625
I0208 00:29:08.074089 140277154002688 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3007846474647522, loss=3.7083113193511963
I0208 00:29:43.077440 140277145609984 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.3668995499610901, loss=3.6809725761413574
I0208 00:30:18.115247 140277154002688 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.35085824131965637, loss=3.7555036544799805
I0208 00:30:53.139390 140277145609984 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3087272346019745, loss=3.7526509761810303
I0208 00:31:28.170450 140277154002688 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3364589810371399, loss=3.7226502895355225
I0208 00:32:03.202798 140277145609984 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.30997970700263977, loss=3.6933648586273193
I0208 00:32:38.254018 140277154002688 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.33641424775123596, loss=3.777454376220703
I0208 00:33:13.399831 140277145609984 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.32671982049942017, loss=3.7123353481292725
I0208 00:33:48.400547 140277154002688 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.3293212056159973, loss=3.7111968994140625
I0208 00:34:23.423896 140277145609984 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.3075014054775238, loss=3.6939542293548584
I0208 00:34:58.450505 140277154002688 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.3218294084072113, loss=3.711555242538452
I0208 00:35:33.506419 140277145609984 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3003716766834259, loss=3.6922590732574463
I0208 00:36:08.549903 140277154002688 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.31302687525749207, loss=3.7602179050445557
I0208 00:36:43.597616 140277145609984 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.32718828320503235, loss=3.773266315460205
I0208 00:37:18.647556 140277154002688 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.32215020060539246, loss=3.73878812789917
I0208 00:37:53.688392 140277145609984 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3298954367637634, loss=3.7291250228881836
I0208 00:38:28.705338 140277154002688 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3255978226661682, loss=3.7175915241241455
I0208 00:39:03.751126 140277145609984 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3357017934322357, loss=3.6817688941955566
I0208 00:39:18.518839 140446903760704 spec.py:321] Evaluating on the training split.
I0208 00:39:21.531716 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:41:58.403866 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 00:42:01.123487 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:44:24.300268 140446903760704 spec.py:349] Evaluating on the test split.
I0208 00:44:27.010311 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 00:46:39.591163 140446903760704 submission_runner.py:408] Time since start: 52405.03s, 	Step: 93544, 	{'train/accuracy': 0.6899000406265259, 'train/loss': 1.6128188371658325, 'train/bleu': 34.90819228231661, 'validation/accuracy': 0.6882493495941162, 'validation/loss': 1.6095622777938843, 'validation/bleu': 30.37207243465497, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.51614511013031, 'test/bleu': 30.48390357612902, 'test/num_examples': 3003, 'score': 32790.42817759514, 'total_duration': 52405.03318023682, 'accumulated_submission_time': 32790.42817759514, 'accumulated_eval_time': 19610.351407289505, 'accumulated_logging_time': 1.3041329383850098}
I0208 00:46:39.619008 140277154002688 logging_writer.py:48] [93544] accumulated_eval_time=19610.351407, accumulated_logging_time=1.304133, accumulated_submission_time=32790.428178, global_step=93544, preemption_count=0, score=32790.428178, test/accuracy=0.705816, test/bleu=30.483904, test/loss=1.516145, test/num_examples=3003, total_duration=52405.033180, train/accuracy=0.689900, train/bleu=34.908192, train/loss=1.612819, validation/accuracy=0.688249, validation/bleu=30.372072, validation/loss=1.609562, validation/num_examples=3000
I0208 00:46:59.518155 140277145609984 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.33847421407699585, loss=3.697141647338867
I0208 00:47:34.435673 140277154002688 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.3021368682384491, loss=3.6792490482330322
I0208 00:48:09.414572 140277145609984 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.31738927960395813, loss=3.6578493118286133
I0208 00:48:44.435889 140277154002688 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.331964910030365, loss=3.7294185161590576
I0208 00:49:19.449522 140277145609984 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3275200426578522, loss=3.6798465251922607
I0208 00:49:54.469300 140277154002688 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.33169567584991455, loss=3.6880834102630615
I0208 00:50:29.519017 140277145609984 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.32307904958724976, loss=3.7189807891845703
I0208 00:51:04.564279 140277154002688 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.33275488018989563, loss=3.7295982837677
I0208 00:51:39.625770 140277145609984 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3187994360923767, loss=3.7192575931549072
I0208 00:52:14.743150 140277154002688 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.32811999320983887, loss=3.733046293258667
I0208 00:52:49.763288 140277145609984 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.31815284490585327, loss=3.744511365890503
I0208 00:53:24.862536 140277154002688 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.31768423318862915, loss=3.7005114555358887
I0208 00:53:59.917358 140277145609984 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.31159526109695435, loss=3.687460422515869
I0208 00:54:34.961153 140277154002688 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.31957903504371643, loss=3.702637195587158
I0208 00:55:09.967189 140277145609984 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3257969915866852, loss=3.683589458465576
I0208 00:55:44.983356 140277154002688 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.33679163455963135, loss=3.7318403720855713
I0208 00:56:19.991941 140277145609984 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.31084105372428894, loss=3.7473716735839844
I0208 00:56:54.996994 140277154002688 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3395622670650482, loss=3.729017734527588
I0208 00:57:30.019794 140277145609984 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3312865197658539, loss=3.6516456604003906
I0208 00:58:05.080331 140277154002688 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.32657578587532043, loss=3.6567740440368652
I0208 00:58:40.104046 140277145609984 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.31633251905441284, loss=3.6924009323120117
I0208 00:59:15.148994 140277154002688 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3427165746688843, loss=3.767367124557495
I0208 00:59:50.182387 140277145609984 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.33858829736709595, loss=3.7442405223846436
I0208 01:00:25.220610 140277154002688 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3483300805091858, loss=3.766901969909668
I0208 01:00:39.663867 140446903760704 spec.py:321] Evaluating on the training split.
I0208 01:00:42.676998 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:03:40.606209 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 01:03:43.313386 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:06:05.629901 140446903760704 spec.py:349] Evaluating on the test split.
I0208 01:06:08.343093 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:08:23.272282 140446903760704 submission_runner.py:408] Time since start: 53708.71s, 	Step: 95943, 	{'train/accuracy': 0.697588324546814, 'train/loss': 1.5702698230743408, 'train/bleu': 35.739230605844355, 'validation/accuracy': 0.6895760893821716, 'validation/loss': 1.610058069229126, 'validation/bleu': 30.57486585549891, 'validation/num_examples': 3000, 'test/accuracy': 0.7060717344284058, 'test/loss': 1.5157976150512695, 'test/bleu': 30.849107356658006, 'test/num_examples': 3003, 'score': 33630.38622021675, 'total_duration': 53708.71432638168, 'accumulated_submission_time': 33630.38622021675, 'accumulated_eval_time': 20073.95977115631, 'accumulated_logging_time': 1.3421132564544678}
I0208 01:08:23.300881 140277145609984 logging_writer.py:48] [95943] accumulated_eval_time=20073.959771, accumulated_logging_time=1.342113, accumulated_submission_time=33630.386220, global_step=95943, preemption_count=0, score=33630.386220, test/accuracy=0.706072, test/bleu=30.849107, test/loss=1.515798, test/num_examples=3003, total_duration=53708.714326, train/accuracy=0.697588, train/bleu=35.739231, train/loss=1.570270, validation/accuracy=0.689576, validation/bleu=30.574866, validation/loss=1.610058, validation/num_examples=3000
I0208 01:08:43.491726 140277154002688 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3337962329387665, loss=3.6916050910949707
I0208 01:09:18.439706 140277145609984 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3133705258369446, loss=3.6856744289398193
I0208 01:09:53.446927 140277154002688 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.337715208530426, loss=3.6727490425109863
I0208 01:10:28.493969 140277145609984 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.3201468586921692, loss=3.7376761436462402
I0208 01:11:03.514150 140277154002688 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.36087092757225037, loss=3.747941493988037
I0208 01:11:38.543648 140277145609984 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3204616606235504, loss=3.6874969005584717
I0208 01:12:13.553501 140277154002688 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.394304096698761, loss=3.750204086303711
I0208 01:12:48.572568 140277145609984 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3314111530780792, loss=3.677415609359741
I0208 01:13:23.608298 140277154002688 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3472309708595276, loss=3.7567026615142822
I0208 01:13:58.651853 140277145609984 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.33216923475265503, loss=3.706700563430786
I0208 01:14:33.669597 140277154002688 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3415168225765228, loss=3.711203098297119
I0208 01:15:08.695900 140277145609984 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.31405729055404663, loss=3.670062780380249
I0208 01:15:43.715084 140277154002688 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3404604494571686, loss=3.668642520904541
I0208 01:16:18.737398 140277145609984 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.33806779980659485, loss=3.711519956588745
I0208 01:16:53.756556 140277154002688 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.34332072734832764, loss=3.7137346267700195
I0208 01:17:28.782402 140277145609984 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.33779510855674744, loss=3.716855764389038
I0208 01:18:03.830325 140277154002688 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.32798701524734497, loss=3.7032065391540527
I0208 01:18:38.842435 140277145609984 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.3375007212162018, loss=3.6883373260498047
I0208 01:19:13.888153 140277154002688 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.356099396944046, loss=3.7315571308135986
I0208 01:19:48.925775 140277145609984 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3451451063156128, loss=3.682969808578491
I0208 01:20:23.963033 140277154002688 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.3465103805065155, loss=3.7596170902252197
I0208 01:20:58.979205 140277145609984 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.34039801359176636, loss=3.6952943801879883
I0208 01:21:34.027298 140277154002688 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.3252097964286804, loss=3.7227399349212646
I0208 01:22:09.098993 140277145609984 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.34469643235206604, loss=3.7481839656829834
I0208 01:22:23.523552 140446903760704 spec.py:321] Evaluating on the training split.
I0208 01:22:26.564611 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:25:17.489594 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 01:25:20.214816 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:27:48.441747 140446903760704 spec.py:349] Evaluating on the test split.
I0208 01:27:51.169706 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:30:13.469584 140446903760704 submission_runner.py:408] Time since start: 55018.91s, 	Step: 98343, 	{'train/accuracy': 0.6986111998558044, 'train/loss': 1.5686570405960083, 'train/bleu': 35.27395572303037, 'validation/accuracy': 0.6885965466499329, 'validation/loss': 1.6070998907089233, 'validation/bleu': 30.290472276833565, 'validation/num_examples': 3000, 'test/accuracy': 0.7058973908424377, 'test/loss': 1.5149260759353638, 'test/bleu': 30.596273861079318, 'test/num_examples': 3003, 'score': 34470.52443647385, 'total_duration': 55018.911603450775, 'accumulated_submission_time': 34470.52443647385, 'accumulated_eval_time': 20543.905728816986, 'accumulated_logging_time': 1.3811841011047363}
I0208 01:30:13.498764 140277154002688 logging_writer.py:48] [98343] accumulated_eval_time=20543.905729, accumulated_logging_time=1.381184, accumulated_submission_time=34470.524436, global_step=98343, preemption_count=0, score=34470.524436, test/accuracy=0.705897, test/bleu=30.596274, test/loss=1.514926, test/num_examples=3003, total_duration=55018.911603, train/accuracy=0.698611, train/bleu=35.273956, train/loss=1.568657, validation/accuracy=0.688597, validation/bleu=30.290472, validation/loss=1.607100, validation/num_examples=3000
I0208 01:30:33.729529 140277145609984 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3339981734752655, loss=3.642625093460083
I0208 01:31:08.697134 140277154002688 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3219514489173889, loss=3.702880859375
I0208 01:31:43.724499 140277145609984 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3364192545413971, loss=3.692305564880371
I0208 01:32:18.759207 140277154002688 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.3335925042629242, loss=3.666045904159546
I0208 01:32:53.798488 140277145609984 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3575790226459503, loss=3.7602643966674805
I0208 01:33:28.850328 140277154002688 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.3392601013183594, loss=3.7421936988830566
I0208 01:34:03.861786 140277145609984 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.34195929765701294, loss=3.696061849594116
I0208 01:34:38.915390 140277154002688 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.3366934657096863, loss=3.6835672855377197
I0208 01:35:13.973917 140277145609984 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.33820927143096924, loss=3.664240837097168
I0208 01:35:48.983042 140277154002688 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.32146528363227844, loss=3.6884961128234863
I0208 01:36:24.050225 140277145609984 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.33667612075805664, loss=3.67268705368042
I0208 01:36:59.096526 140277154002688 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.33409905433654785, loss=3.6842591762542725
I0208 01:37:34.165466 140277145609984 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.33290383219718933, loss=3.6888139247894287
I0208 01:38:09.221003 140277154002688 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.3292384743690491, loss=3.6394221782684326
I0208 01:38:44.263991 140277145609984 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.34835514426231384, loss=3.710737943649292
I0208 01:39:19.340534 140277154002688 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.34446030855178833, loss=3.6399004459381104
I0208 01:39:54.420135 140277145609984 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3407907783985138, loss=3.651881217956543
I0208 01:40:29.455033 140277154002688 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.34925538301467896, loss=3.7243971824645996
I0208 01:41:04.470153 140277145609984 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.33073002099990845, loss=3.6853647232055664
I0208 01:41:39.490217 140277154002688 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.3511546552181244, loss=3.6551384925842285
I0208 01:42:14.503648 140277145609984 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.34651297330856323, loss=3.697530746459961
I0208 01:42:49.554645 140277154002688 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3495241403579712, loss=3.6906189918518066
I0208 01:43:24.598389 140277145609984 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3719250559806824, loss=3.6745948791503906
I0208 01:43:59.623009 140277154002688 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.34616243839263916, loss=3.6666579246520996
I0208 01:44:13.715084 140446903760704 spec.py:321] Evaluating on the training split.
I0208 01:44:16.720999 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:47:03.554737 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 01:47:06.262426 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:49:33.362941 140446903760704 spec.py:349] Evaluating on the test split.
I0208 01:49:36.071807 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 01:51:53.122970 140446903760704 submission_runner.py:408] Time since start: 56318.57s, 	Step: 100742, 	{'train/accuracy': 0.710821807384491, 'train/loss': 1.5044238567352295, 'train/bleu': 37.11803812727856, 'validation/accuracy': 0.6893652677536011, 'validation/loss': 1.6094584465026855, 'validation/bleu': 30.65314764916486, 'validation/num_examples': 3000, 'test/accuracy': 0.7069548964500427, 'test/loss': 1.5113070011138916, 'test/bleu': 30.70333007489312, 'test/num_examples': 3003, 'score': 35310.65348362923, 'total_duration': 56318.565016269684, 'accumulated_submission_time': 35310.65348362923, 'accumulated_eval_time': 21003.313571691513, 'accumulated_logging_time': 1.4205613136291504}
I0208 01:51:53.151875 140277145609984 logging_writer.py:48] [100742] accumulated_eval_time=21003.313572, accumulated_logging_time=1.420561, accumulated_submission_time=35310.653484, global_step=100742, preemption_count=0, score=35310.653484, test/accuracy=0.706955, test/bleu=30.703330, test/loss=1.511307, test/num_examples=3003, total_duration=56318.565016, train/accuracy=0.710822, train/bleu=37.118038, train/loss=1.504424, validation/accuracy=0.689365, validation/bleu=30.653148, validation/loss=1.609458, validation/num_examples=3000
I0208 01:52:13.756447 140277154002688 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.3576802909374237, loss=3.6689867973327637
I0208 01:52:48.655393 140277145609984 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.32488271594047546, loss=3.695225715637207
I0208 01:53:23.636048 140277154002688 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3672543168067932, loss=3.7179574966430664
I0208 01:53:58.641462 140277145609984 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3351233899593353, loss=3.697117567062378
I0208 01:54:33.700107 140277154002688 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.37979090213775635, loss=3.710369348526001
I0208 01:55:08.730227 140277145609984 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.3601958453655243, loss=3.7414283752441406
I0208 01:55:43.760221 140277154002688 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3500737249851227, loss=3.7118585109710693
I0208 01:56:18.820801 140277145609984 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3284643590450287, loss=3.695253849029541
I0208 01:56:53.866771 140277154002688 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.34738510847091675, loss=3.6760823726654053
I0208 01:57:28.921333 140277145609984 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.3610653579235077, loss=3.676316022872925
I0208 01:58:03.953819 140277154002688 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.35603222250938416, loss=3.7050414085388184
I0208 01:58:38.998732 140277145609984 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3403671979904175, loss=3.746960401535034
I0208 01:59:14.020218 140277154002688 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3412444293498993, loss=3.705976963043213
I0208 01:59:49.045255 140277145609984 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3323899209499359, loss=3.6762516498565674
I0208 02:00:24.065228 140277154002688 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3587130308151245, loss=3.7108654975891113
I0208 02:00:59.096710 140277145609984 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3555402457714081, loss=3.658651828765869
I0208 02:01:34.124999 140277154002688 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.33947813510894775, loss=3.6865458488464355
I0208 02:02:09.162647 140277145609984 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3400781452655792, loss=3.6751532554626465
I0208 02:02:44.202352 140277154002688 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.35285159945487976, loss=3.67490816116333
I0208 02:03:19.250193 140277145609984 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.37639209628105164, loss=3.764248847961426
I0208 02:03:54.290583 140277154002688 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.35223427414894104, loss=3.7034823894500732
I0208 02:04:29.296341 140277145609984 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3566442131996155, loss=3.695185661315918
I0208 02:05:04.323811 140277154002688 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3344976007938385, loss=3.682671070098877
I0208 02:05:39.344083 140277145609984 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.35794544219970703, loss=3.6660244464874268
I0208 02:05:53.441351 140446903760704 spec.py:321] Evaluating on the training split.
I0208 02:05:56.461348 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:08:46.896849 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 02:08:49.608796 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:11:17.642914 140446903760704 spec.py:349] Evaluating on the test split.
I0208 02:11:20.355369 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:13:35.830264 140446903760704 submission_runner.py:408] Time since start: 57621.27s, 	Step: 103142, 	{'train/accuracy': 0.7058877348899841, 'train/loss': 1.5339775085449219, 'train/bleu': 36.642378290460236, 'validation/accuracy': 0.6901340484619141, 'validation/loss': 1.6044197082519531, 'validation/bleu': 30.616822115538966, 'validation/num_examples': 3000, 'test/accuracy': 0.7081401348114014, 'test/loss': 1.5082166194915771, 'test/bleu': 30.886960740934175, 'test/num_examples': 3003, 'score': 36150.857697963715, 'total_duration': 57621.27230811119, 'accumulated_submission_time': 36150.857697963715, 'accumulated_eval_time': 21465.7024269104, 'accumulated_logging_time': 1.4596493244171143}
I0208 02:13:35.859867 140277154002688 logging_writer.py:48] [103142] accumulated_eval_time=21465.702427, accumulated_logging_time=1.459649, accumulated_submission_time=36150.857698, global_step=103142, preemption_count=0, score=36150.857698, test/accuracy=0.708140, test/bleu=30.886961, test/loss=1.508217, test/num_examples=3003, total_duration=57621.272308, train/accuracy=0.705888, train/bleu=36.642378, train/loss=1.533978, validation/accuracy=0.690134, validation/bleu=30.616822, validation/loss=1.604420, validation/num_examples=3000
I0208 02:13:56.441395 140277145609984 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.3516984283924103, loss=3.730595350265503
I0208 02:14:31.361708 140277154002688 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.3448769152164459, loss=3.7175252437591553
I0208 02:15:06.379691 140277145609984 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.35904794931411743, loss=3.7024104595184326
I0208 02:15:41.394280 140277154002688 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.32583725452423096, loss=3.65680193901062
I0208 02:16:16.416306 140277145609984 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3470057249069214, loss=3.6618003845214844
I0208 02:16:51.452166 140277154002688 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3367351293563843, loss=3.6342172622680664
I0208 02:17:26.464576 140277145609984 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.35458752512931824, loss=3.7192375659942627
I0208 02:18:01.501252 140277154002688 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.33686208724975586, loss=3.683453321456909
I0208 02:18:36.531054 140277145609984 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3560367822647095, loss=3.69681715965271
I0208 02:19:11.546484 140277154002688 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.3443255126476288, loss=3.6295976638793945
I0208 02:19:46.582230 140277145609984 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.36243489384651184, loss=3.6851260662078857
I0208 02:20:21.602555 140277154002688 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.34240633249282837, loss=3.665719985961914
I0208 02:20:56.646572 140277145609984 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.3607490360736847, loss=3.721827507019043
I0208 02:21:31.670345 140277154002688 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3643171787261963, loss=3.685243606567383
I0208 02:22:06.720098 140277145609984 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.355523943901062, loss=3.6440610885620117
I0208 02:22:41.744872 140277154002688 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.3372834622859955, loss=3.665144443511963
I0208 02:23:16.787256 140277145609984 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3727760314941406, loss=3.6630682945251465
I0208 02:23:51.817296 140277154002688 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.35261785984039307, loss=3.66572642326355
I0208 02:24:26.875284 140277145609984 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3533480167388916, loss=3.6304099559783936
I0208 02:25:01.916891 140277154002688 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.35361525416374207, loss=3.684124231338501
I0208 02:25:36.945948 140277145609984 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.34762921929359436, loss=3.6962666511535645
I0208 02:26:11.975463 140277154002688 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.34422793984413147, loss=3.685901641845703
I0208 02:26:46.984228 140277145609984 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3485763370990753, loss=3.6463229656219482
I0208 02:27:22.004239 140277154002688 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.3897012174129486, loss=3.6680357456207275
I0208 02:27:36.368792 140446903760704 spec.py:321] Evaluating on the training split.
I0208 02:27:39.123439 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:30:23.797475 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 02:30:26.513841 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:32:54.193818 140446903760704 spec.py:349] Evaluating on the test split.
I0208 02:32:56.913178 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:35:15.765025 140446903760704 submission_runner.py:408] Time since start: 58921.21s, 	Step: 105542, 	{'train/accuracy': 0.7012184858322144, 'train/loss': 1.5534294843673706, 'train/bleu': 36.010873962839284, 'validation/accuracy': 0.6888568997383118, 'validation/loss': 1.6038175821304321, 'validation/bleu': 30.25613399959102, 'validation/num_examples': 3000, 'test/accuracy': 0.7084190249443054, 'test/loss': 1.5058610439300537, 'test/bleu': 30.523381697580785, 'test/num_examples': 3003, 'score': 36991.28224873543, 'total_duration': 58921.207073926926, 'accumulated_submission_time': 36991.28224873543, 'accumulated_eval_time': 21925.09859728813, 'accumulated_logging_time': 1.5006978511810303}
I0208 02:35:15.795264 140277145609984 logging_writer.py:48] [105542] accumulated_eval_time=21925.098597, accumulated_logging_time=1.500698, accumulated_submission_time=36991.282249, global_step=105542, preemption_count=0, score=36991.282249, test/accuracy=0.708419, test/bleu=30.523382, test/loss=1.505861, test/num_examples=3003, total_duration=58921.207074, train/accuracy=0.701218, train/bleu=36.010874, train/loss=1.553429, validation/accuracy=0.688857, validation/bleu=30.256134, validation/loss=1.603818, validation/num_examples=3000
I0208 02:35:36.374362 140277154002688 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3607424199581146, loss=3.6532821655273438
I0208 02:36:11.376172 140277145609984 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.36227181553840637, loss=3.625072956085205
I0208 02:36:46.414370 140277154002688 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.39707890152931213, loss=3.674469470977783
I0208 02:37:21.438820 140277145609984 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.3744898736476898, loss=3.7131447792053223
I0208 02:37:56.458976 140277154002688 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.35968682169914246, loss=3.680913209915161
I0208 02:38:31.506828 140277145609984 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.34300604462623596, loss=3.6463470458984375
I0208 02:39:06.549286 140277154002688 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3882209062576294, loss=3.658982276916504
I0208 02:39:41.635360 140277145609984 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.3691701889038086, loss=3.67818021774292
I0208 02:40:16.681856 140277154002688 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3504517078399658, loss=3.6478285789489746
I0208 02:40:51.708109 140277145609984 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.35924088954925537, loss=3.6538541316986084
I0208 02:41:26.752603 140277154002688 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.34289461374282837, loss=3.621332883834839
I0208 02:42:01.798937 140277145609984 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3449254035949707, loss=3.6399197578430176
I0208 02:42:36.858432 140277154002688 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.35672301054000854, loss=3.661837577819824
I0208 02:43:11.918240 140277145609984 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3607490062713623, loss=3.6305932998657227
I0208 02:43:46.970947 140277154002688 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.37622514367103577, loss=3.743108034133911
I0208 02:44:22.025856 140277145609984 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.3679782450199127, loss=3.6486496925354004
I0208 02:44:57.061647 140277154002688 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3603019416332245, loss=3.6604509353637695
I0208 02:45:32.088774 140277145609984 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3821045160293579, loss=3.6856143474578857
I0208 02:46:07.129267 140277154002688 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.372807115316391, loss=3.701687812805176
I0208 02:46:42.168043 140277145609984 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3572518825531006, loss=3.5988495349884033
I0208 02:47:17.219972 140277154002688 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.375400185585022, loss=3.6539952754974365
I0208 02:47:52.279580 140277145609984 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.3542250096797943, loss=3.6162335872650146
I0208 02:48:27.319517 140277154002688 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.34986260533332825, loss=3.6804802417755127
I0208 02:49:02.359151 140277145609984 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3558836877346039, loss=3.618741750717163
I0208 02:49:16.079591 140446903760704 spec.py:321] Evaluating on the training split.
I0208 02:49:19.091375 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:52:03.959584 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 02:52:06.674040 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:54:28.632425 140446903760704 spec.py:349] Evaluating on the test split.
I0208 02:54:31.354302 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 02:56:41.444268 140446903760704 submission_runner.py:408] Time since start: 60206.89s, 	Step: 107941, 	{'train/accuracy': 0.7100006937980652, 'train/loss': 1.5051331520080566, 'train/bleu': 36.99780640058015, 'validation/accuracy': 0.689811646938324, 'validation/loss': 1.60414719581604, 'validation/bleu': 30.53258891829982, 'validation/num_examples': 3000, 'test/accuracy': 0.7079658508300781, 'test/loss': 1.504045844078064, 'test/bleu': 30.685208275329504, 'test/num_examples': 3003, 'score': 37831.47907757759, 'total_duration': 60206.88630771637, 'accumulated_submission_time': 37831.47907757759, 'accumulated_eval_time': 22370.46321105957, 'accumulated_logging_time': 1.5430285930633545}
I0208 02:56:41.474741 140277154002688 logging_writer.py:48] [107941] accumulated_eval_time=22370.463211, accumulated_logging_time=1.543029, accumulated_submission_time=37831.479078, global_step=107941, preemption_count=0, score=37831.479078, test/accuracy=0.707966, test/bleu=30.685208, test/loss=1.504046, test/num_examples=3003, total_duration=60206.886308, train/accuracy=0.710001, train/bleu=36.997806, train/loss=1.505133, validation/accuracy=0.689812, validation/bleu=30.532589, validation/loss=1.604147, validation/num_examples=3000
I0208 02:57:02.371746 140277145609984 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.380799800157547, loss=3.666186571121216
I0208 02:57:37.332690 140277154002688 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.35654956102371216, loss=3.6528103351593018
I0208 02:58:12.330535 140277145609984 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.36335229873657227, loss=3.671673059463501
I0208 02:58:47.369936 140277154002688 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.35705605149269104, loss=3.683161973953247
I0208 02:59:22.408061 140277145609984 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3986245095729828, loss=3.6591079235076904
I0208 02:59:57.431356 140277154002688 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3791135549545288, loss=3.602963924407959
I0208 03:00:32.461623 140277145609984 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.35978105664253235, loss=3.669703483581543
I0208 03:01:07.533347 140277154002688 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3634743094444275, loss=3.623786687850952
I0208 03:01:42.543362 140277145609984 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.365314245223999, loss=3.5848872661590576
I0208 03:02:17.586755 140277154002688 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.3592926859855652, loss=3.642788887023926
I0208 03:02:52.700720 140277145609984 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.36861851811408997, loss=3.6447765827178955
I0208 03:03:27.749932 140277154002688 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.35629767179489136, loss=3.62632155418396
I0208 03:04:02.766706 140277145609984 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.38443896174430847, loss=3.646669387817383
I0208 03:04:37.810021 140277154002688 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.368992418050766, loss=3.635664224624634
I0208 03:05:12.856448 140277145609984 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.38173142075538635, loss=3.671370267868042
I0208 03:05:47.906388 140277154002688 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.38252905011177063, loss=3.691063404083252
I0208 03:06:22.960618 140277145609984 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.3706841766834259, loss=3.6082143783569336
I0208 03:06:57.989995 140277154002688 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.35750648379325867, loss=3.6576151847839355
I0208 03:07:33.044652 140277145609984 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.37207379937171936, loss=3.640753984451294
I0208 03:08:08.083240 140277154002688 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.3734252154827118, loss=3.658640146255493
I0208 03:08:43.104657 140277145609984 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.3715471029281616, loss=3.643078327178955
I0208 03:09:18.142010 140277154002688 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.3771238923072815, loss=3.641489028930664
I0208 03:09:53.184550 140277145609984 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.36125096678733826, loss=3.6689348220825195
I0208 03:10:28.209982 140277154002688 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.36909592151641846, loss=3.5941989421844482
I0208 03:10:41.589498 140446903760704 spec.py:321] Evaluating on the training split.
I0208 03:10:44.606945 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:13:27.324618 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 03:13:30.040472 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:15:54.504862 140446903760704 spec.py:349] Evaluating on the test split.
I0208 03:15:57.224918 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:18:18.492838 140446903760704 submission_runner.py:408] Time since start: 61503.93s, 	Step: 110340, 	{'train/accuracy': 0.7088562846183777, 'train/loss': 1.5129475593566895, 'train/bleu': 35.960513350985146, 'validation/accuracy': 0.6904191970825195, 'validation/loss': 1.6018781661987305, 'validation/bleu': 30.43087989291936, 'validation/num_examples': 3000, 'test/accuracy': 0.7091511487960815, 'test/loss': 1.5065263509750366, 'test/bleu': 30.692955515135495, 'test/num_examples': 3003, 'score': 38671.50674414635, 'total_duration': 61503.93488526344, 'accumulated_submission_time': 38671.50674414635, 'accumulated_eval_time': 22827.366498708725, 'accumulated_logging_time': 1.5851430892944336}
I0208 03:18:18.522792 140277145609984 logging_writer.py:48] [110340] accumulated_eval_time=22827.366499, accumulated_logging_time=1.585143, accumulated_submission_time=38671.506744, global_step=110340, preemption_count=0, score=38671.506744, test/accuracy=0.709151, test/bleu=30.692956, test/loss=1.506526, test/num_examples=3003, total_duration=61503.934885, train/accuracy=0.708856, train/bleu=35.960513, train/loss=1.512948, validation/accuracy=0.690419, validation/bleu=30.430880, validation/loss=1.601878, validation/num_examples=3000
I0208 03:18:39.787497 140277154002688 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3853359818458557, loss=3.6379640102386475
I0208 03:19:14.735606 140277145609984 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.39044561982154846, loss=3.708955764770508
I0208 03:19:49.714790 140277154002688 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.3857575058937073, loss=3.625469923019409
I0208 03:20:24.733992 140277145609984 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.37378183007240295, loss=3.6851048469543457
I0208 03:20:59.746643 140277154002688 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.36273932456970215, loss=3.653078079223633
I0208 03:21:34.794181 140277145609984 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3836570382118225, loss=3.667713165283203
I0208 03:22:09.813155 140277154002688 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.369614839553833, loss=3.6983907222747803
I0208 03:22:44.838348 140277145609984 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.36319249868392944, loss=3.6039602756500244
I0208 03:23:19.878648 140277154002688 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3567315638065338, loss=3.5811421871185303
I0208 03:23:54.910700 140277145609984 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.369533896446228, loss=3.671088695526123
I0208 03:24:29.952862 140277154002688 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.4008832275867462, loss=3.633373737335205
I0208 03:25:04.987710 140277145609984 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.37943077087402344, loss=3.662367820739746
I0208 03:25:40.007498 140277154002688 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.377495676279068, loss=3.616987705230713
I0208 03:26:15.010553 140277145609984 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3809969425201416, loss=3.599719524383545
I0208 03:26:50.011223 140277154002688 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.36522042751312256, loss=3.6511032581329346
I0208 03:27:25.095446 140277145609984 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.37490347027778625, loss=3.6103358268737793
I0208 03:28:00.129983 140277154002688 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3816981613636017, loss=3.6159276962280273
I0208 03:28:35.144311 140277145609984 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.38445377349853516, loss=3.6350772380828857
I0208 03:29:10.184336 140277154002688 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.37852615118026733, loss=3.6295971870422363
I0208 03:29:45.221215 140277145609984 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.3562658727169037, loss=3.6492433547973633
I0208 03:30:20.262994 140277154002688 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3778565227985382, loss=3.6331756114959717
I0208 03:30:55.311724 140277145609984 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.4000145196914673, loss=3.691758871078491
I0208 03:31:30.363345 140277154002688 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.39587336778640747, loss=3.6799564361572266
I0208 03:32:05.421006 140277145609984 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.36797550320625305, loss=3.6298420429229736
I0208 03:32:18.805672 140446903760704 spec.py:321] Evaluating on the training split.
I0208 03:32:21.825809 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:35:11.314901 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 03:35:14.031215 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:37:38.476747 140446903760704 spec.py:349] Evaluating on the test split.
I0208 03:37:41.200161 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:39:57.237966 140446903760704 submission_runner.py:408] Time since start: 62802.68s, 	Step: 112740, 	{'train/accuracy': 0.7243646383285522, 'train/loss': 1.4386584758758545, 'train/bleu': 37.83614617089411, 'validation/accuracy': 0.6897496581077576, 'validation/loss': 1.5978442430496216, 'validation/bleu': 30.643926207512962, 'validation/num_examples': 3000, 'test/accuracy': 0.7088257670402527, 'test/loss': 1.500476598739624, 'test/bleu': 30.616537234812025, 'test/num_examples': 3003, 'score': 39511.70456314087, 'total_duration': 62802.67998576164, 'accumulated_submission_time': 39511.70456314087, 'accumulated_eval_time': 23285.7987074852, 'accumulated_logging_time': 1.6249699592590332}
I0208 03:39:57.275616 140277154002688 logging_writer.py:48] [112740] accumulated_eval_time=23285.798707, accumulated_logging_time=1.624970, accumulated_submission_time=39511.704563, global_step=112740, preemption_count=0, score=39511.704563, test/accuracy=0.708826, test/bleu=30.616537, test/loss=1.500477, test/num_examples=3003, total_duration=62802.679986, train/accuracy=0.724365, train/bleu=37.836146, train/loss=1.438658, validation/accuracy=0.689750, validation/bleu=30.643926, validation/loss=1.597844, validation/num_examples=3000
I0208 03:40:18.564095 140277145609984 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.379719614982605, loss=3.5895001888275146
I0208 03:40:53.477332 140277154002688 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.37540143728256226, loss=3.6384551525115967
I0208 03:41:28.484072 140277145609984 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.36849451065063477, loss=3.6205861568450928
I0208 03:42:03.533517 140277154002688 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.3824472725391388, loss=3.6639511585235596
I0208 03:42:38.561774 140277145609984 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.373796284198761, loss=3.6097099781036377
I0208 03:43:13.563939 140277154002688 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.3987826704978943, loss=3.637091636657715
I0208 03:43:48.586149 140277145609984 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3746592700481415, loss=3.6114113330841064
I0208 03:44:23.615914 140277154002688 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3698267340660095, loss=3.6767845153808594
I0208 03:44:58.642032 140277145609984 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.377960741519928, loss=3.6394431591033936
I0208 03:45:33.738470 140277154002688 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.37548691034317017, loss=3.6316463947296143
I0208 03:46:08.823781 140277145609984 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.38573116064071655, loss=3.661248207092285
I0208 03:46:43.889933 140277154002688 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.39551666378974915, loss=3.687246322631836
I0208 03:47:18.931114 140277145609984 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.36686238646507263, loss=3.5778396129608154
I0208 03:47:53.970036 140277154002688 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3734672963619232, loss=3.6638083457946777
I0208 03:48:28.991330 140277145609984 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3809846043586731, loss=3.632362127304077
I0208 03:49:04.052412 140277154002688 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3842664659023285, loss=3.6544933319091797
I0208 03:49:39.081017 140277145609984 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3705527186393738, loss=3.612356662750244
I0208 03:50:14.096451 140277154002688 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.36305904388427734, loss=3.6084630489349365
I0208 03:50:49.137801 140277145609984 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.39999791979789734, loss=3.6227529048919678
I0208 03:51:24.171825 140277154002688 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.39677733182907104, loss=3.629499673843384
I0208 03:51:59.220217 140277145609984 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3768457770347595, loss=3.638529062271118
I0208 03:52:34.233618 140277154002688 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.37856167554855347, loss=3.5986499786376953
I0208 03:53:09.253915 140277145609984 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3801362216472626, loss=3.606969118118286
I0208 03:53:44.256028 140277154002688 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.3896994888782501, loss=3.6058218479156494
I0208 03:53:57.292885 140446903760704 spec.py:321] Evaluating on the training split.
I0208 03:54:00.304649 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:56:47.951323 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 03:56:50.669796 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 03:59:14.463934 140446903760704 spec.py:349] Evaluating on the test split.
I0208 03:59:17.190154 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 04:01:31.250716 140446903760704 submission_runner.py:408] Time since start: 64096.69s, 	Step: 115139, 	{'train/accuracy': 0.7164209485054016, 'train/loss': 1.4740687608718872, 'train/bleu': 37.267175904620444, 'validation/accuracy': 0.6905679702758789, 'validation/loss': 1.598555564880371, 'validation/bleu': 30.91084348148604, 'validation/num_examples': 3000, 'test/accuracy': 0.7087792754173279, 'test/loss': 1.4993302822113037, 'test/bleu': 30.776656235656343, 'test/num_examples': 3003, 'score': 40351.63377261162, 'total_duration': 64096.69275712967, 'accumulated_submission_time': 40351.63377261162, 'accumulated_eval_time': 23739.756477594376, 'accumulated_logging_time': 1.674572467803955}
I0208 04:01:31.282342 140277145609984 logging_writer.py:48] [115139] accumulated_eval_time=23739.756478, accumulated_logging_time=1.674572, accumulated_submission_time=40351.633773, global_step=115139, preemption_count=0, score=40351.633773, test/accuracy=0.708779, test/bleu=30.776656, test/loss=1.499330, test/num_examples=3003, total_duration=64096.692757, train/accuracy=0.716421, train/bleu=37.267176, train/loss=1.474069, validation/accuracy=0.690568, validation/bleu=30.910843, validation/loss=1.598556, validation/num_examples=3000
I0208 04:01:52.927618 140277154002688 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.3999623656272888, loss=3.5941433906555176
I0208 04:02:27.943587 140277145609984 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.4123440682888031, loss=3.6829843521118164
I0208 04:03:02.984960 140277154002688 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.37978386878967285, loss=3.6376843452453613
I0208 04:03:38.028685 140277145609984 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.40439897775650024, loss=3.6331589221954346
I0208 04:04:13.072268 140277154002688 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3705381751060486, loss=3.6340842247009277
I0208 04:04:48.089920 140277145609984 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.38350147008895874, loss=3.622771739959717
I0208 04:05:23.126364 140277154002688 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3893640637397766, loss=3.609992027282715
I0208 04:05:58.141274 140277145609984 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.38014599680900574, loss=3.6776044368743896
I0208 04:06:33.155812 140277154002688 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.38075968623161316, loss=3.5708062648773193
I0208 04:07:08.170418 140277145609984 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.37798938155174255, loss=3.5946240425109863
I0208 04:07:43.199011 140277154002688 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.39129817485809326, loss=3.6415328979492188
I0208 04:08:18.270487 140277145609984 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.39006462693214417, loss=3.6196506023406982
I0208 04:08:53.313366 140277154002688 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.3737867474555969, loss=3.5827810764312744
I0208 04:09:28.331030 140277145609984 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3853858411312103, loss=3.6298441886901855
I0208 04:10:03.350739 140277154002688 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.388526976108551, loss=3.5606689453125
I0208 04:10:38.401863 140277145609984 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.38943007588386536, loss=3.6263136863708496
I0208 04:11:13.468804 140277154002688 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.39138662815093994, loss=3.6536924839019775
I0208 04:11:48.500338 140277145609984 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.3949604332447052, loss=3.667292594909668
I0208 04:12:23.561746 140277154002688 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3849586844444275, loss=3.669888496398926
I0208 04:12:58.597672 140277145609984 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.39272770285606384, loss=3.6484832763671875
I0208 04:13:33.619338 140277154002688 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.38877546787261963, loss=3.6474485397338867
I0208 04:14:08.634364 140277145609984 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.39561736583709717, loss=3.592320442199707
I0208 04:14:43.639568 140277154002688 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.4139282703399658, loss=3.6720592975616455
I0208 04:15:18.668189 140277145609984 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3608876168727875, loss=3.585176944732666
I0208 04:15:31.332376 140446903760704 spec.py:321] Evaluating on the training split.
I0208 04:15:34.342075 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 04:18:21.732589 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 04:18:24.478262 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 04:20:46.054570 140446903760704 spec.py:349] Evaluating on the test split.
I0208 04:20:48.792039 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 04:23:00.059654 140446903760704 submission_runner.py:408] Time since start: 65385.50s, 	Step: 117538, 	{'train/accuracy': 0.7153463959693909, 'train/loss': 1.4766026735305786, 'train/bleu': 36.84388016626823, 'validation/accuracy': 0.6916590929031372, 'validation/loss': 1.6009763479232788, 'validation/bleu': 30.74155176517884, 'validation/num_examples': 3000, 'test/accuracy': 0.7088141441345215, 'test/loss': 1.501481056213379, 'test/bleu': 30.903441013207743, 'test/num_examples': 3003, 'score': 41191.59611129761, 'total_duration': 65385.50169849396, 'accumulated_submission_time': 41191.59611129761, 'accumulated_eval_time': 24188.48369383812, 'accumulated_logging_time': 1.718095302581787}
I0208 04:23:00.091127 140277154002688 logging_writer.py:48] [117538] accumulated_eval_time=24188.483694, accumulated_logging_time=1.718095, accumulated_submission_time=41191.596111, global_step=117538, preemption_count=0, score=41191.596111, test/accuracy=0.708814, test/bleu=30.903441, test/loss=1.501481, test/num_examples=3003, total_duration=65385.501698, train/accuracy=0.715346, train/bleu=36.843880, train/loss=1.476603, validation/accuracy=0.691659, validation/bleu=30.741552, validation/loss=1.600976, validation/num_examples=3000
I0208 04:23:22.066903 140277145609984 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.37311261892318726, loss=3.595022678375244
I0208 04:23:56.993256 140277154002688 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3863244652748108, loss=3.6412200927734375
I0208 04:24:32.030267 140277145609984 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3919340968132019, loss=3.626288414001465
I0208 04:25:07.025200 140277154002688 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.40099936723709106, loss=3.6110727787017822
I0208 04:25:42.040306 140277145609984 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.37903106212615967, loss=3.568873643875122
I0208 04:26:17.076847 140277154002688 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.4046006202697754, loss=3.6759541034698486
I0208 04:26:52.120482 140277145609984 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.4016810953617096, loss=3.615419387817383
I0208 04:27:27.140739 140277154002688 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3919600248336792, loss=3.6363940238952637
I0208 04:28:02.160013 140277145609984 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3750936686992645, loss=3.6220715045928955
I0208 04:28:37.240380 140277154002688 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3835742771625519, loss=3.6268932819366455
I0208 04:29:12.302510 140277145609984 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.39151158928871155, loss=3.6054251194000244
I0208 04:29:47.474087 140277154002688 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.40163254737854004, loss=3.6664621829986572
I0208 04:30:22.530311 140277145609984 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.40295419096946716, loss=3.616269826889038
I0208 04:30:57.628367 140277154002688 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.38449394702911377, loss=3.599778175354004
I0208 04:31:32.658901 140277145609984 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.39543989300727844, loss=3.661480188369751
I0208 04:32:07.700964 140277154002688 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.4013065695762634, loss=3.6502785682678223
I0208 04:32:42.724902 140277145609984 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.4021127223968506, loss=3.6437177658081055
I0208 04:33:17.731250 140277154002688 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.3885958790779114, loss=3.612926721572876
I0208 04:33:52.741821 140277145609984 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.38866356015205383, loss=3.589284896850586
I0208 04:34:27.783488 140277154002688 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.3985121548175812, loss=3.574357271194458
I0208 04:35:02.811197 140277145609984 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.4106258451938629, loss=3.602966785430908
I0208 04:35:37.832091 140277154002688 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.39188432693481445, loss=3.608577251434326
I0208 04:36:12.882094 140277145609984 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.40479227900505066, loss=3.5818281173706055
I0208 04:36:47.968304 140277154002688 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.3902159035205841, loss=3.6341640949249268
I0208 04:37:00.297425 140446903760704 spec.py:321] Evaluating on the training split.
I0208 04:37:03.330156 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 04:39:55.303296 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 04:39:58.012601 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 04:42:25.027046 140446903760704 spec.py:349] Evaluating on the test split.
I0208 04:42:27.750461 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 04:44:48.505096 140446903760704 submission_runner.py:408] Time since start: 66693.95s, 	Step: 119937, 	{'train/accuracy': 0.7232077717781067, 'train/loss': 1.438293695449829, 'train/bleu': 37.65100512157615, 'validation/accuracy': 0.6920310854911804, 'validation/loss': 1.6015228033065796, 'validation/bleu': 30.751410962903055, 'validation/num_examples': 3000, 'test/accuracy': 0.7088955044746399, 'test/loss': 1.4994897842407227, 'test/bleu': 30.883720879674414, 'test/num_examples': 3003, 'score': 42031.71455454826, 'total_duration': 66693.94711279869, 'accumulated_submission_time': 42031.71455454826, 'accumulated_eval_time': 24656.691277503967, 'accumulated_logging_time': 1.7594947814941406}
I0208 04:44:48.545370 140277145609984 logging_writer.py:48] [119937] accumulated_eval_time=24656.691278, accumulated_logging_time=1.759495, accumulated_submission_time=42031.714555, global_step=119937, preemption_count=0, score=42031.714555, test/accuracy=0.708896, test/bleu=30.883721, test/loss=1.499490, test/num_examples=3003, total_duration=66693.947113, train/accuracy=0.723208, train/bleu=37.651005, train/loss=1.438294, validation/accuracy=0.692031, validation/bleu=30.751411, validation/loss=1.601523, validation/num_examples=3000
I0208 04:45:10.979319 140277154002688 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.4018959105014801, loss=3.610452890396118
I0208 04:45:45.945441 140277145609984 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.4141632616519928, loss=3.6383068561553955
I0208 04:46:20.954035 140277154002688 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.39481261372566223, loss=3.6298718452453613
I0208 04:46:55.974495 140277145609984 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3851851224899292, loss=3.6346287727355957
I0208 04:47:30.984469 140277154002688 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.39772674441337585, loss=3.587082624435425
I0208 04:48:05.989284 140277145609984 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.40424472093582153, loss=3.648158311843872
I0208 04:48:41.004551 140277154002688 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3869653046131134, loss=3.6549432277679443
I0208 04:49:16.033704 140277145609984 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.3964739143848419, loss=3.585411787033081
I0208 04:49:51.076948 140277154002688 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.381255567073822, loss=3.618501663208008
I0208 04:50:26.137229 140277145609984 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.39570027589797974, loss=3.607485771179199
I0208 04:51:01.205404 140277154002688 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.4017762839794159, loss=3.6256020069122314
I0208 04:51:36.248683 140277145609984 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3959622085094452, loss=3.672980785369873
I0208 04:52:11.298780 140277154002688 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.38207221031188965, loss=3.6425211429595947
I0208 04:52:46.321644 140277145609984 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.39243072271347046, loss=3.635305643081665
I0208 04:53:21.324018 140277154002688 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3854294717311859, loss=3.6040570735931396
I0208 04:53:56.356280 140277145609984 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.39446425437927246, loss=3.598729133605957
I0208 04:54:31.417439 140277154002688 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.4029111862182617, loss=3.6108238697052
I0208 04:55:06.468325 140277145609984 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.38437148928642273, loss=3.60797119140625
I0208 04:55:41.497536 140277154002688 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.40268856287002563, loss=3.631791591644287
I0208 04:56:16.531602 140277145609984 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.40946948528289795, loss=3.6538994312286377
I0208 04:56:51.527899 140277154002688 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.40991759300231934, loss=3.6016933917999268
I0208 04:57:26.572699 140277145609984 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.38881129026412964, loss=3.5629138946533203
I0208 04:58:01.649672 140277154002688 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.4062342941761017, loss=3.6345887184143066
I0208 04:58:36.728266 140277145609984 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3974301517009735, loss=3.5772814750671387
I0208 04:58:48.726467 140446903760704 spec.py:321] Evaluating on the training split.
I0208 04:58:51.745732 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:01:47.930259 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 05:01:50.675212 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:04:15.555222 140446903760704 spec.py:349] Evaluating on the test split.
I0208 05:04:18.288841 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:06:40.805876 140446903760704 submission_runner.py:408] Time since start: 68006.25s, 	Step: 122336, 	{'train/accuracy': 0.7199275493621826, 'train/loss': 1.4584884643554688, 'train/bleu': 37.4064410559461, 'validation/accuracy': 0.6925642490386963, 'validation/loss': 1.5980294942855835, 'validation/bleu': 30.635947764975892, 'validation/num_examples': 3000, 'test/accuracy': 0.70961594581604, 'test/loss': 1.4969851970672607, 'test/bleu': 30.907040212287704, 'test/num_examples': 3003, 'score': 42871.72847676277, 'total_duration': 68006.24787807465, 'accumulated_submission_time': 42871.72847676277, 'accumulated_eval_time': 25128.770598888397, 'accumulated_logging_time': 1.8893790245056152}
I0208 05:06:40.848127 140277154002688 logging_writer.py:48] [122336] accumulated_eval_time=25128.770599, accumulated_logging_time=1.889379, accumulated_submission_time=42871.728477, global_step=122336, preemption_count=0, score=42871.728477, test/accuracy=0.709616, test/bleu=30.907040, test/loss=1.496985, test/num_examples=3003, total_duration=68006.247878, train/accuracy=0.719928, train/bleu=37.406441, train/loss=1.458488, validation/accuracy=0.692564, validation/bleu=30.635948, validation/loss=1.598029, validation/num_examples=3000
I0208 05:07:03.539564 140277145609984 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.40110790729522705, loss=3.638406276702881
I0208 05:07:38.519593 140277154002688 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.40190601348876953, loss=3.5907185077667236
I0208 05:08:13.554417 140277145609984 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.3936414122581482, loss=3.6360180377960205
I0208 05:08:48.615908 140277154002688 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.3728627860546112, loss=3.5713815689086914
I0208 05:09:23.619997 140277145609984 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3888159692287445, loss=3.643765449523926
I0208 05:09:58.634646 140277154002688 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.3867824673652649, loss=3.6242892742156982
I0208 05:10:33.651738 140277145609984 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.40679365396499634, loss=3.6474030017852783
I0208 05:11:08.703929 140277154002688 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.39893418550491333, loss=3.579050302505493
I0208 05:11:43.756228 140277145609984 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.39755210280418396, loss=3.651165008544922
I0208 05:12:18.736742 140277154002688 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.38762882351875305, loss=3.5902841091156006
I0208 05:12:53.757443 140277145609984 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.39396679401397705, loss=3.60540771484375
I0208 05:13:28.782378 140277154002688 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.40556034445762634, loss=3.6233766078948975
I0208 05:14:03.823549 140277145609984 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.39533790946006775, loss=3.62528133392334
I0208 05:14:38.971322 140277154002688 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.39604049921035767, loss=3.6361217498779297
I0208 05:15:14.033970 140277145609984 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3878425061702728, loss=3.56740403175354
I0208 05:15:49.054563 140277154002688 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.4002809226512909, loss=3.617412567138672
I0208 05:16:24.086093 140277145609984 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.3990941047668457, loss=3.613752603530884
I0208 05:16:59.129549 140277154002688 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.39125531911849976, loss=3.6293511390686035
I0208 05:17:34.169318 140277145609984 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.38798627257347107, loss=3.5668694972991943
I0208 05:18:09.222106 140277154002688 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.39684945344924927, loss=3.635213851928711
I0208 05:18:44.321027 140277145609984 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.3898068070411682, loss=3.605607271194458
I0208 05:19:19.368084 140277154002688 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.3933957517147064, loss=3.6190898418426514
I0208 05:19:54.399077 140277145609984 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.3992443382740021, loss=3.5763165950775146
I0208 05:20:29.550463 140277154002688 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.39035090804100037, loss=3.578230142593384
I0208 05:20:40.839666 140446903760704 spec.py:321] Evaluating on the training split.
I0208 05:20:43.874005 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:23:29.482057 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 05:23:32.200293 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:26:02.445798 140446903760704 spec.py:349] Evaluating on the test split.
I0208 05:26:05.165836 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:28:18.576827 140446903760704 submission_runner.py:408] Time since start: 69304.02s, 	Step: 124734, 	{'train/accuracy': 0.7197436094284058, 'train/loss': 1.4523401260375977, 'train/bleu': 37.86308040490159, 'validation/accuracy': 0.6918451189994812, 'validation/loss': 1.6004658937454224, 'validation/bleu': 30.577418563951106, 'validation/num_examples': 3000, 'test/accuracy': 0.7094067931175232, 'test/loss': 1.498136281967163, 'test/bleu': 30.645121997646555, 'test/num_examples': 3003, 'score': 43711.629980802536, 'total_duration': 69304.01886892319, 'accumulated_submission_time': 43711.629980802536, 'accumulated_eval_time': 25586.507713079453, 'accumulated_logging_time': 1.9442377090454102}
I0208 05:28:18.611077 140277145609984 logging_writer.py:48] [124734] accumulated_eval_time=25586.507713, accumulated_logging_time=1.944238, accumulated_submission_time=43711.629981, global_step=124734, preemption_count=0, score=43711.629981, test/accuracy=0.709407, test/bleu=30.645122, test/loss=1.498136, test/num_examples=3003, total_duration=69304.018869, train/accuracy=0.719744, train/bleu=37.863080, train/loss=1.452340, validation/accuracy=0.691845, validation/bleu=30.577419, validation/loss=1.600466, validation/num_examples=3000
I0208 05:28:42.015858 140277154002688 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.4122578501701355, loss=3.6346609592437744
I0208 05:29:16.938170 140277145609984 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.3881835639476776, loss=3.6029365062713623
I0208 05:29:51.974811 140277154002688 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.38752302527427673, loss=3.5980889797210693
I0208 05:30:26.981803 140277145609984 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.39942431449890137, loss=3.660062074661255
I0208 05:31:01.989082 140277154002688 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.39842966198921204, loss=3.582636594772339
I0208 05:31:37.011670 140277145609984 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.38290658593177795, loss=3.597120761871338
I0208 05:32:12.053007 140277154002688 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.41078710556030273, loss=3.6552329063415527
I0208 05:32:47.054042 140277145609984 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.4063149392604828, loss=3.63554048538208
I0208 05:33:22.082987 140277154002688 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.38792529702186584, loss=3.6000423431396484
I0208 05:33:57.138886 140277145609984 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.4012530446052551, loss=3.6142842769622803
I0208 05:34:32.208540 140277154002688 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.392657607793808, loss=3.616171360015869
I0208 05:35:07.222825 140277145609984 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.37968909740448, loss=3.5218586921691895
I0208 05:35:42.268112 140277154002688 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.41258129477500916, loss=3.6352791786193848
I0208 05:36:17.296066 140277145609984 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.38278716802597046, loss=3.6264946460723877
I0208 05:36:52.300442 140277154002688 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.4046483337879181, loss=3.5621604919433594
I0208 05:37:27.322014 140277145609984 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.4149596393108368, loss=3.647876739501953
I0208 05:38:02.342694 140277154002688 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.38643574714660645, loss=3.6277592182159424
I0208 05:38:37.367719 140277145609984 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.39011380076408386, loss=3.5531575679779053
I0208 05:39:12.429535 140277154002688 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.39720192551612854, loss=3.628528356552124
I0208 05:39:47.521101 140277145609984 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.40878039598464966, loss=3.560194969177246
I0208 05:40:22.660782 140277154002688 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.39570555090904236, loss=3.6033802032470703
I0208 05:40:57.684812 140277145609984 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.3929280936717987, loss=3.658228874206543
I0208 05:41:32.700279 140277154002688 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.376730352640152, loss=3.579970598220825
I0208 05:42:07.737906 140277145609984 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.40367254614830017, loss=3.6246654987335205
I0208 05:42:18.667169 140446903760704 spec.py:321] Evaluating on the training split.
I0208 05:42:21.694262 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:45:17.289606 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 05:45:20.020309 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:47:50.300031 140446903760704 spec.py:349] Evaluating on the test split.
I0208 05:47:53.025924 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 05:50:09.100127 140446903760704 submission_runner.py:408] Time since start: 70614.54s, 	Step: 127133, 	{'train/accuracy': 0.723660409450531, 'train/loss': 1.441123127937317, 'train/bleu': 37.82017963906209, 'validation/accuracy': 0.6915971040725708, 'validation/loss': 1.5994668006896973, 'validation/bleu': 30.58672769907038, 'validation/num_examples': 3000, 'test/accuracy': 0.70961594581604, 'test/loss': 1.4976170063018799, 'test/bleu': 30.935873794008984, 'test/num_examples': 3003, 'score': 44551.60100340843, 'total_duration': 70614.54214811325, 'accumulated_submission_time': 44551.60100340843, 'accumulated_eval_time': 26056.940609931946, 'accumulated_logging_time': 1.98885178565979}
I0208 05:50:09.137159 140277154002688 logging_writer.py:48] [127133] accumulated_eval_time=26056.940610, accumulated_logging_time=1.988852, accumulated_submission_time=44551.601003, global_step=127133, preemption_count=0, score=44551.601003, test/accuracy=0.709616, test/bleu=30.935874, test/loss=1.497617, test/num_examples=3003, total_duration=70614.542148, train/accuracy=0.723660, train/bleu=37.820180, train/loss=1.441123, validation/accuracy=0.691597, validation/bleu=30.586728, validation/loss=1.599467, validation/num_examples=3000
I0208 05:50:32.876703 140277145609984 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.4045376777648926, loss=3.633018732070923
I0208 05:51:07.801300 140277154002688 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.3880589008331299, loss=3.569075107574463
I0208 05:51:42.817194 140277145609984 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.4158541262149811, loss=3.5830464363098145
I0208 05:52:17.826797 140277154002688 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.4111557602882385, loss=3.628460168838501
I0208 05:52:52.849214 140277145609984 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.4123639166355133, loss=3.6440956592559814
I0208 05:53:27.872291 140277154002688 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.39057236909866333, loss=3.6602120399475098
I0208 05:54:02.911757 140277145609984 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.3853698670864105, loss=3.602034330368042
I0208 05:54:37.971103 140277154002688 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.40540552139282227, loss=3.6434366703033447
I0208 05:55:13.006356 140277145609984 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.39995893836021423, loss=3.591461658477783
I0208 05:55:48.050246 140277154002688 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.40050017833709717, loss=3.68365478515625
I0208 05:56:23.134421 140277145609984 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.3904176950454712, loss=3.624196767807007
I0208 05:56:58.196934 140277154002688 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.40749382972717285, loss=3.6329846382141113
I0208 05:57:33.239165 140277145609984 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.39840325713157654, loss=3.5779590606689453
I0208 05:58:08.267355 140277154002688 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.389283686876297, loss=3.5723512172698975
I0208 05:58:43.307684 140277145609984 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.39295434951782227, loss=3.607801914215088
I0208 05:59:18.333221 140277154002688 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.39749646186828613, loss=3.6132309436798096
I0208 05:59:53.376863 140277145609984 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.39670243859291077, loss=3.624032735824585
I0208 06:00:28.405281 140277154002688 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.39318519830703735, loss=3.6125099658966064
I0208 06:01:03.431980 140277145609984 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.39835479855537415, loss=3.6020970344543457
I0208 06:01:38.453713 140277154002688 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.39015963673591614, loss=3.6646056175231934
I0208 06:02:13.493957 140277145609984 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.408648818731308, loss=3.6353983879089355
I0208 06:02:48.529768 140277154002688 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.3954245448112488, loss=3.667043924331665
I0208 06:03:23.561706 140277145609984 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.4118967354297638, loss=3.651935577392578
I0208 06:03:58.601190 140277154002688 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.3949737846851349, loss=3.587907552719116
I0208 06:04:09.181481 140446903760704 spec.py:321] Evaluating on the training split.
I0208 06:04:12.201536 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:07:02.819108 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 06:07:05.541323 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:09:31.650820 140446903760704 spec.py:349] Evaluating on the test split.
I0208 06:09:34.370132 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:11:55.386424 140446903760704 submission_runner.py:408] Time since start: 71920.83s, 	Step: 129532, 	{'train/accuracy': 0.7209928035736084, 'train/loss': 1.453560709953308, 'train/bleu': 37.57740041402509, 'validation/accuracy': 0.6917583346366882, 'validation/loss': 1.5988129377365112, 'validation/bleu': 30.576942252558766, 'validation/num_examples': 3000, 'test/accuracy': 0.709732174873352, 'test/loss': 1.4976327419281006, 'test/bleu': 30.904820565100355, 'test/num_examples': 3003, 'score': 45391.557307481766, 'total_duration': 71920.82846522331, 'accumulated_submission_time': 45391.557307481766, 'accumulated_eval_time': 26523.145493984222, 'accumulated_logging_time': 2.03778076171875}
I0208 06:11:55.421146 140277145609984 logging_writer.py:48] [129532] accumulated_eval_time=26523.145494, accumulated_logging_time=2.037781, accumulated_submission_time=45391.557307, global_step=129532, preemption_count=0, score=45391.557307, test/accuracy=0.709732, test/bleu=30.904821, test/loss=1.497633, test/num_examples=3003, total_duration=71920.828465, train/accuracy=0.720993, train/bleu=37.577400, train/loss=1.453561, validation/accuracy=0.691758, validation/bleu=30.576942, validation/loss=1.598813, validation/num_examples=3000
I0208 06:12:19.486371 140277154002688 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3997918367385864, loss=3.6498594284057617
I0208 06:12:54.456003 140277145609984 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.4108096659183502, loss=3.662705898284912
I0208 06:13:29.617627 140277154002688 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.3849536180496216, loss=3.6048104763031006
I0208 06:14:04.685973 140277145609984 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.39313942193984985, loss=3.5989437103271484
I0208 06:14:39.692968 140277154002688 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.3799858093261719, loss=3.618215322494507
I0208 06:15:14.725479 140277145609984 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.398164302110672, loss=3.6454107761383057
I0208 06:15:49.762980 140277154002688 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.38277676701545715, loss=3.623147964477539
I0208 06:16:24.803802 140277145609984 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.39479997754096985, loss=3.6049273014068604
I0208 06:16:59.818876 140277154002688 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.4086160957813263, loss=3.560307025909424
I0208 06:17:34.837406 140277145609984 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.39392468333244324, loss=3.6038711071014404
I0208 06:18:09.887563 140277154002688 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.3891654312610626, loss=3.561702013015747
I0208 06:18:44.924888 140277145609984 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.40126246213912964, loss=3.614833354949951
I0208 06:19:20.005503 140277154002688 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.4058518409729004, loss=3.644368886947632
I0208 06:19:55.031101 140277145609984 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.3784358501434326, loss=3.549254894256592
I0208 06:20:30.039140 140277154002688 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.3949582576751709, loss=3.5567948818206787
I0208 06:21:05.059324 140277145609984 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.3831772208213806, loss=3.56331205368042
I0208 06:21:40.101786 140277154002688 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.3939869999885559, loss=3.6070847511291504
I0208 06:22:15.157720 140277145609984 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.3984978497028351, loss=3.613417863845825
I0208 06:22:50.172679 140277154002688 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.39165839552879333, loss=3.630005121231079
I0208 06:23:25.181088 140277145609984 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.40593042969703674, loss=3.6288135051727295
I0208 06:24:00.209455 140277154002688 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.38148704171180725, loss=3.5924930572509766
I0208 06:24:35.224956 140277145609984 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.39692091941833496, loss=3.6126763820648193
I0208 06:25:10.248934 140277154002688 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.39456313848495483, loss=3.641684055328369
I0208 06:25:45.300954 140277145609984 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.3812686800956726, loss=3.5817182064056396
I0208 06:25:55.519774 140446903760704 spec.py:321] Evaluating on the training split.
I0208 06:25:58.538174 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:28:45.015774 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 06:28:47.740181 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:31:13.907448 140446903760704 spec.py:349] Evaluating on the test split.
I0208 06:31:16.630934 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:33:36.364011 140446903760704 submission_runner.py:408] Time since start: 73221.81s, 	Step: 131931, 	{'train/accuracy': 0.7224260568618774, 'train/loss': 1.4456599950790405, 'train/bleu': 37.93193093486411, 'validation/accuracy': 0.6920434832572937, 'validation/loss': 1.5994871854782104, 'validation/bleu': 30.504024003063833, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.4979794025421143, 'test/bleu': 30.837799961632033, 'test/num_examples': 3003, 'score': 46231.56835961342, 'total_duration': 73221.8060324192, 'accumulated_submission_time': 46231.56835961342, 'accumulated_eval_time': 26983.98964881897, 'accumulated_logging_time': 2.0844168663024902}
I0208 06:33:36.403247 140277154002688 logging_writer.py:48] [131931] accumulated_eval_time=26983.989649, accumulated_logging_time=2.084417, accumulated_submission_time=46231.568360, global_step=131931, preemption_count=0, score=46231.568360, test/accuracy=0.709976, test/bleu=30.837800, test/loss=1.497979, test/num_examples=3003, total_duration=73221.806032, train/accuracy=0.722426, train/bleu=37.931931, train/loss=1.445660, validation/accuracy=0.692043, validation/bleu=30.504024, validation/loss=1.599487, validation/num_examples=3000
I0208 06:34:00.804509 140277145609984 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.3974365293979645, loss=3.5885252952575684
I0208 06:34:35.731178 140277154002688 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.4048828184604645, loss=3.631298065185547
I0208 06:35:10.717872 140277145609984 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.40164893865585327, loss=3.6296193599700928
I0208 06:35:45.745302 140277154002688 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.40155166387557983, loss=3.621342420578003
I0208 06:36:20.762010 140277145609984 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.4054074287414551, loss=3.651843309402466
I0208 06:36:55.801225 140277154002688 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.39001035690307617, loss=3.5753026008605957
I0208 06:37:30.835953 140277145609984 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.41180291771888733, loss=3.635432004928589
I0208 06:38:05.904031 140277154002688 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.41392791271209717, loss=3.657885789871216
I0208 06:38:40.931083 140277145609984 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.3938353359699249, loss=3.5810487270355225
I0208 06:39:15.970422 140277154002688 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.40364953875541687, loss=3.584774971008301
I0208 06:39:50.985943 140277145609984 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.390121728181839, loss=3.609966516494751
I0208 06:40:26.019797 140277154002688 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.39592188596725464, loss=3.588019609451294
I0208 06:41:01.052908 140277145609984 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.39316025376319885, loss=3.5535309314727783
I0208 06:41:36.128519 140277154002688 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.3890593647956848, loss=3.670537233352661
I0208 06:41:47.063198 140446903760704 spec.py:321] Evaluating on the training split.
I0208 06:41:50.094289 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:44:41.110893 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 06:44:43.827000 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:47:08.677561 140446903760704 spec.py:349] Evaluating on the test split.
I0208 06:47:11.399174 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:49:30.892333 140446903760704 submission_runner.py:408] Time since start: 74176.33s, 	Step: 133333, 	{'train/accuracy': 0.7234098315238953, 'train/loss': 1.439003586769104, 'train/bleu': 37.50861974519182, 'validation/accuracy': 0.6921550631523132, 'validation/loss': 1.5993489027023315, 'validation/bleu': 30.494101371783113, 'validation/num_examples': 3000, 'test/accuracy': 0.7098832130432129, 'test/loss': 1.4978277683258057, 'test/bleu': 30.838390296957208, 'test/num_examples': 3003, 'score': 46722.17216897011, 'total_duration': 74176.3343679905, 'accumulated_submission_time': 46722.17216897011, 'accumulated_eval_time': 27447.8187623024, 'accumulated_logging_time': 2.1351213455200195}
I0208 06:49:30.932723 140277145609984 logging_writer.py:48] [133333] accumulated_eval_time=27447.818762, accumulated_logging_time=2.135121, accumulated_submission_time=46722.172169, global_step=133333, preemption_count=0, score=46722.172169, test/accuracy=0.709883, test/bleu=30.838390, test/loss=1.497828, test/num_examples=3003, total_duration=74176.334368, train/accuracy=0.723410, train/bleu=37.508620, train/loss=1.439004, validation/accuracy=0.692155, validation/bleu=30.494101, validation/loss=1.599349, validation/num_examples=3000
I0208 06:49:30.971735 140277154002688 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46722.172169
I0208 06:49:32.222753 140446903760704 checkpoints.py:490] Saving checkpoint at step: 133333
I0208 06:49:36.295859 140446903760704 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_2/checkpoint_133333
I0208 06:49:36.300789 140446903760704 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_2/checkpoint_133333.
I0208 06:49:36.348123 140446903760704 submission_runner.py:583] Tuning trial 2/5
I0208 06:49:36.348297 140446903760704 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0208 06:49:36.355710 140446903760704 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006397879915311933, 'train/loss': 11.187285423278809, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 28.511714935302734, 'total_duration': 867.353991985321, 'accumulated_submission_time': 28.511714935302734, 'accumulated_eval_time': 838.8422293663025, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2396, {'train/accuracy': 0.42042139172554016, 'train/loss': 3.955871820449829, 'train/bleu': 14.03031101364103, 'validation/accuracy': 0.408426433801651, 'validation/loss': 4.070903778076172, 'validation/bleu': 9.933766386758254, 'validation/num_examples': 3000, 'test/accuracy': 0.3910173773765564, 'test/loss': 4.271887302398682, 'test/bleu': 8.25107154164634, 'test/num_examples': 3003, 'score': 868.5572006702423, 'total_duration': 2288.5391099452972, 'accumulated_submission_time': 868.5572006702423, 'accumulated_eval_time': 1419.8872134685516, 'accumulated_logging_time': 0.019253969192504883, 'global_step': 2396, 'preemption_count': 0}), (4791, {'train/accuracy': 0.5435932278633118, 'train/loss': 2.867501735687256, 'train/bleu': 24.044350532229807, 'validation/accuracy': 0.5418655872344971, 'validation/loss': 2.839674472808838, 'validation/bleu': 20.273860903076134, 'validation/num_examples': 3000, 'test/accuracy': 0.5451164841651917, 'test/loss': 2.8579013347625732, 'test/bleu': 18.830678245727768, 'test/num_examples': 3003, 'score': 1708.5745656490326, 'total_duration': 3615.966960668564, 'accumulated_submission_time': 1708.5745656490326, 'accumulated_eval_time': 1907.1950154304504, 'accumulated_logging_time': 0.045763254165649414, 'global_step': 4791, 'preemption_count': 0}), (7188, {'train/accuracy': 0.5837926268577576, 'train/loss': 2.4588887691497803, 'train/bleu': 26.822264155984897, 'validation/accuracy': 0.5924786925315857, 'validation/loss': 2.391460657119751, 'validation/bleu': 23.340748577255788, 'validation/num_examples': 3000, 'test/accuracy': 0.5944570302963257, 'test/loss': 2.3819665908813477, 'test/bleu': 21.934132082215903, 'test/num_examples': 3003, 'score': 2548.571653842926, 'total_duration': 4920.2710173130035, 'accumulated_submission_time': 2548.571653842926, 'accumulated_eval_time': 2371.394764184952, 'accumulated_logging_time': 0.07738113403320312, 'global_step': 7188, 'preemption_count': 0}), (9587, {'train/accuracy': 0.5973837971687317, 'train/loss': 2.325868606567383, 'train/bleu': 28.25564876003111, 'validation/accuracy': 0.6162601709365845, 'validation/loss': 2.1848175525665283, 'validation/bleu': 25.123763093156985, 'validation/num_examples': 3000, 'test/accuracy': 0.6207309365272522, 'test/loss': 2.1605913639068604, 'test/bleu': 23.869267718351175, 'test/num_examples': 3003, 'score': 3388.7338008880615, 'total_duration': 6199.2928557395935, 'accumulated_submission_time': 3388.7338008880615, 'accumulated_eval_time': 2810.153936624527, 'accumulated_logging_time': 0.10327434539794922, 'global_step': 9587, 'preemption_count': 0}), (11986, {'train/accuracy': 0.6148414611816406, 'train/loss': 2.1918225288391113, 'train/bleu': 29.49347492495337, 'validation/accuracy': 0.629031240940094, 'validation/loss': 2.0681145191192627, 'validation/bleu': 26.154691827326516, 'validation/num_examples': 3000, 'test/accuracy': 0.638847291469574, 'test/loss': 2.0232722759246826, 'test/bleu': 25.131617057030173, 'test/num_examples': 3003, 'score': 4228.941308021545, 'total_duration': 7527.8836851119995, 'accumulated_submission_time': 4228.941308021545, 'accumulated_eval_time': 3298.432733297348, 'accumulated_logging_time': 0.13021183013916016, 'global_step': 11986, 'preemption_count': 0}), (14384, {'train/accuracy': 0.6251729726791382, 'train/loss': 2.1058154106140137, 'train/bleu': 30.452753434872445, 'validation/accuracy': 0.6403516530990601, 'validation/loss': 1.9786962270736694, 'validation/bleu': 26.710132243429697, 'validation/num_examples': 3000, 'test/accuracy': 0.6496427059173584, 'test/loss': 1.9267456531524658, 'test/bleu': 26.02713434983809, 'test/num_examples': 3003, 'score': 5068.914443492889, 'total_duration': 8814.757400751114, 'accumulated_submission_time': 5068.914443492889, 'accumulated_eval_time': 3745.2251360416412, 'accumulated_logging_time': 0.16237592697143555, 'global_step': 14384, 'preemption_count': 0}), (16782, {'train/accuracy': 0.6332809329032898, 'train/loss': 2.025113582611084, 'train/bleu': 30.834592006989155, 'validation/accuracy': 0.6474562883377075, 'validation/loss': 1.917392373085022, 'validation/bleu': 27.531752272070463, 'validation/num_examples': 3000, 'test/accuracy': 0.658288300037384, 'test/loss': 1.8623210191726685, 'test/bleu': 26.765356617369413, 'test/num_examples': 3003, 'score': 5909.078606367111, 'total_duration': 10103.429570436478, 'accumulated_submission_time': 5909.078606367111, 'accumulated_eval_time': 4193.632272481918, 'accumulated_logging_time': 0.18920087814331055, 'global_step': 16782, 'preemption_count': 0}), (19181, {'train/accuracy': 0.6485809087753296, 'train/loss': 1.9150187969207764, 'train/bleu': 31.71264530159373, 'validation/accuracy': 0.6535567045211792, 'validation/loss': 1.8694406747817993, 'validation/bleu': 27.791618921827165, 'validation/num_examples': 3000, 'test/accuracy': 0.6626227498054504, 'test/loss': 1.8123000860214233, 'test/bleu': 27.056916774320545, 'test/num_examples': 3003, 'score': 6749.185294866562, 'total_duration': 11397.61303448677, 'accumulated_submission_time': 6749.185294866562, 'accumulated_eval_time': 4647.600977182388, 'accumulated_logging_time': 0.22064924240112305, 'global_step': 19181, 'preemption_count': 0}), (21580, {'train/accuracy': 0.6398290395736694, 'train/loss': 1.9732670783996582, 'train/bleu': 31.557177824770733, 'validation/accuracy': 0.6579831838607788, 'validation/loss': 1.8404289484024048, 'validation/bleu': 28.183138777133554, 'validation/num_examples': 3000, 'test/accuracy': 0.6678287386894226, 'test/loss': 1.778388500213623, 'test/bleu': 27.37082354685312, 'test/num_examples': 3003, 'score': 7589.185811042786, 'total_duration': 12702.14163517952, 'accumulated_submission_time': 7589.185811042786, 'accumulated_eval_time': 5112.02565741539, 'accumulated_logging_time': 0.2493281364440918, 'global_step': 21580, 'preemption_count': 0}), (23978, {'train/accuracy': 0.6370953321456909, 'train/loss': 1.9664952754974365, 'train/bleu': 31.35145588205602, 'validation/accuracy': 0.6594586372375488, 'validation/loss': 1.8098756074905396, 'validation/bleu': 28.52456600789127, 'validation/num_examples': 3000, 'test/accuracy': 0.6701877117156982, 'test/loss': 1.7438005208969116, 'test/bleu': 27.80662204576, 'test/num_examples': 3003, 'score': 8429.109383583069, 'total_duration': 13992.390851259232, 'accumulated_submission_time': 8429.109383583069, 'accumulated_eval_time': 5562.2473402023315, 'accumulated_logging_time': 0.27814149856567383, 'global_step': 23978, 'preemption_count': 0}), (26376, {'train/accuracy': 0.6522204279899597, 'train/loss': 1.8794294595718384, 'train/bleu': 31.961837474519832, 'validation/accuracy': 0.6629427671432495, 'validation/loss': 1.797146201133728, 'validation/bleu': 28.615634344157126, 'validation/num_examples': 3000, 'test/accuracy': 0.6761141419410706, 'test/loss': 1.732893466949463, 'test/bleu': 28.11392985066348, 'test/num_examples': 3003, 'score': 9269.108746528625, 'total_duration': 15299.209090471268, 'accumulated_submission_time': 9269.108746528625, 'accumulated_eval_time': 6028.958881616592, 'accumulated_logging_time': 0.308734655380249, 'global_step': 26376, 'preemption_count': 0}), (28774, {'train/accuracy': 0.648067057132721, 'train/loss': 1.9048165082931519, 'train/bleu': 31.576812118177703, 'validation/accuracy': 0.6638355255126953, 'validation/loss': 1.7895036935806274, 'validation/bleu': 28.26089184313604, 'validation/num_examples': 3000, 'test/accuracy': 0.6750682592391968, 'test/loss': 1.7232335805892944, 'test/bleu': 27.884347883711857, 'test/num_examples': 3003, 'score': 10109.169246673584, 'total_duration': 16620.57497549057, 'accumulated_submission_time': 10109.169246673584, 'accumulated_eval_time': 6510.161110162735, 'accumulated_logging_time': 0.3377220630645752, 'global_step': 28774, 'preemption_count': 0}), (31172, {'train/accuracy': 0.6439923644065857, 'train/loss': 1.927431583404541, 'train/bleu': 31.66790219778987, 'validation/accuracy': 0.6673568487167358, 'validation/loss': 1.7629570960998535, 'validation/bleu': 28.572178858129174, 'validation/num_examples': 3000, 'test/accuracy': 0.6779385209083557, 'test/loss': 1.6976670026779175, 'test/bleu': 28.39342145211166, 'test/num_examples': 3003, 'score': 10949.117016077042, 'total_duration': 17954.123304367065, 'accumulated_submission_time': 10949.117016077042, 'accumulated_eval_time': 7003.654671907425, 'accumulated_logging_time': 0.36781883239746094, 'global_step': 31172, 'preemption_count': 0}), (33571, {'train/accuracy': 0.6535011529922485, 'train/loss': 1.888905644416809, 'train/bleu': 31.83566635295377, 'validation/accuracy': 0.6689687371253967, 'validation/loss': 1.7727051973342896, 'validation/bleu': 29.0990490788574, 'validation/num_examples': 3000, 'test/accuracy': 0.6815641522407532, 'test/loss': 1.701885461807251, 'test/bleu': 28.767894892189727, 'test/num_examples': 3003, 'score': 11789.085807323456, 'total_duration': 19259.912284851074, 'accumulated_submission_time': 11789.085807323456, 'accumulated_eval_time': 7469.372006177902, 'accumulated_logging_time': 0.3968191146850586, 'global_step': 33571, 'preemption_count': 0}), (35970, {'train/accuracy': 0.6479287147521973, 'train/loss': 1.900919795036316, 'train/bleu': 32.29340009964613, 'validation/accuracy': 0.6697747111320496, 'validation/loss': 1.7474881410598755, 'validation/bleu': 29.200182035206225, 'validation/num_examples': 3000, 'test/accuracy': 0.6820057034492493, 'test/loss': 1.6754212379455566, 'test/bleu': 28.820656669087295, 'test/num_examples': 3003, 'score': 12629.10321187973, 'total_duration': 20581.942991495132, 'accumulated_submission_time': 12629.10321187973, 'accumulated_eval_time': 7951.277729272842, 'accumulated_logging_time': 0.4293038845062256, 'global_step': 35970, 'preemption_count': 0}), (38367, {'train/accuracy': 0.66447913646698, 'train/loss': 1.7844882011413574, 'train/bleu': 33.572362901384416, 'validation/accuracy': 0.6720189452171326, 'validation/loss': 1.7210848331451416, 'validation/bleu': 29.411130481189385, 'validation/num_examples': 3000, 'test/accuracy': 0.6850618720054626, 'test/loss': 1.644993782043457, 'test/bleu': 29.158325918477285, 'test/num_examples': 3003, 'score': 13469.029118299484, 'total_duration': 21879.169389009476, 'accumulated_submission_time': 13469.029118299484, 'accumulated_eval_time': 8408.473896980286, 'accumulated_logging_time': 0.4591190814971924, 'global_step': 38367, 'preemption_count': 0}), (40765, {'train/accuracy': 0.6568623781204224, 'train/loss': 1.8464112281799316, 'train/bleu': 32.29865399133282, 'validation/accuracy': 0.6735936403274536, 'validation/loss': 1.7242670059204102, 'validation/bleu': 29.367416114376645, 'validation/num_examples': 3000, 'test/accuracy': 0.685631275177002, 'test/loss': 1.6500742435455322, 'test/bleu': 29.24589113563988, 'test/num_examples': 3003, 'score': 14309.25852894783, 'total_duration': 23190.842586278915, 'accumulated_submission_time': 14309.25852894783, 'accumulated_eval_time': 8879.810878276825, 'accumulated_logging_time': 0.489011287689209, 'global_step': 40765, 'preemption_count': 0}), (43162, {'train/accuracy': 0.6540481448173523, 'train/loss': 1.8485385179519653, 'train/bleu': 31.766415795889074, 'validation/accuracy': 0.674411952495575, 'validation/loss': 1.7126529216766357, 'validation/bleu': 29.176469017738977, 'validation/num_examples': 3000, 'test/accuracy': 0.6875603199005127, 'test/loss': 1.6370383501052856, 'test/bleu': 28.950136975521982, 'test/num_examples': 3003, 'score': 15149.16226387024, 'total_duration': 24531.530041217804, 'accumulated_submission_time': 15149.16226387024, 'accumulated_eval_time': 9380.485354423523, 'accumulated_logging_time': 0.5209271907806396, 'global_step': 43162, 'preemption_count': 0}), (45563, {'train/accuracy': 0.3642924129962921, 'train/loss': 3.769196033477783, 'train/bleu': 1.166315163018871, 'validation/accuracy': 0.32481926679611206, 'validation/loss': 4.211953163146973, 'validation/bleu': 0.17267072757558866, 'validation/num_examples': 3000, 'test/accuracy': 0.3135901391506195, 'test/loss': 4.377139091491699, 'test/bleu': 0.15558510353138613, 'test/num_examples': 3003, 'score': 15989.111895561218, 'total_duration': 25980.68439888954, 'accumulated_submission_time': 15989.111895561218, 'accumulated_eval_time': 9989.577900886536, 'accumulated_logging_time': 0.5572404861450195, 'global_step': 45563, 'preemption_count': 0}), (47961, {'train/accuracy': 0.6598178744316101, 'train/loss': 1.826006293296814, 'train/bleu': 32.72687596383734, 'validation/accuracy': 0.6744863390922546, 'validation/loss': 1.7195638418197632, 'validation/bleu': 29.72216502720672, 'validation/num_examples': 3000, 'test/accuracy': 0.6885131597518921, 'test/loss': 1.6407841444015503, 'test/bleu': 29.155636425634672, 'test/num_examples': 3003, 'score': 16829.141446828842, 'total_duration': 27273.812072753906, 'accumulated_submission_time': 16829.141446828842, 'accumulated_eval_time': 10442.567687034607, 'accumulated_logging_time': 0.5890069007873535, 'global_step': 47961, 'preemption_count': 0}), (50361, {'train/accuracy': 0.6838412880897522, 'train/loss': 1.6663483381271362, 'train/bleu': 33.974547470472466, 'validation/accuracy': 0.6764330267906189, 'validation/loss': 1.7032217979431152, 'validation/bleu': 29.85961521752938, 'validation/num_examples': 3000, 'test/accuracy': 0.6898379325866699, 'test/loss': 1.6269718408584595, 'test/bleu': 29.56486532290584, 'test/num_examples': 3003, 'score': 17669.30721282959, 'total_duration': 28763.591370821, 'accumulated_submission_time': 17669.30721282959, 'accumulated_eval_time': 11092.072012662888, 'accumulated_logging_time': 0.622807502746582, 'global_step': 50361, 'preemption_count': 0}), (52761, {'train/accuracy': 0.6685400009155273, 'train/loss': 1.7587262392044067, 'train/bleu': 32.919607532370215, 'validation/accuracy': 0.677734911441803, 'validation/loss': 1.6866081953048706, 'validation/bleu': 29.581894504613768, 'validation/num_examples': 3000, 'test/accuracy': 0.6924641132354736, 'test/loss': 1.6087028980255127, 'test/bleu': 29.45363869235563, 'test/num_examples': 3003, 'score': 18509.54104423523, 'total_duration': 30157.93383049965, 'accumulated_submission_time': 18509.54104423523, 'accumulated_eval_time': 11646.070712327957, 'accumulated_logging_time': 0.6562457084655762, 'global_step': 52761, 'preemption_count': 0}), (55160, {'train/accuracy': 0.6608024835586548, 'train/loss': 1.809146761894226, 'train/bleu': 32.987301429211975, 'validation/accuracy': 0.6783052682876587, 'validation/loss': 1.6888394355773926, 'validation/bleu': 30.181948989674293, 'validation/num_examples': 3000, 'test/accuracy': 0.6906862258911133, 'test/loss': 1.6112722158432007, 'test/bleu': 29.408274314513523, 'test/num_examples': 3003, 'score': 19349.585392713547, 'total_duration': 31470.043719291687, 'accumulated_submission_time': 19349.585392713547, 'accumulated_eval_time': 12118.026804924011, 'accumulated_logging_time': 0.6894242763519287, 'global_step': 55160, 'preemption_count': 0}), (57559, {'train/accuracy': 0.6720222234725952, 'train/loss': 1.7211450338363647, 'train/bleu': 33.596491081578876, 'validation/accuracy': 0.6793467998504639, 'validation/loss': 1.676062822341919, 'validation/bleu': 29.910391819317304, 'validation/num_examples': 3000, 'test/accuracy': 0.6937656402587891, 'test/loss': 1.5925296545028687, 'test/bleu': 29.845140575311913, 'test/num_examples': 3003, 'score': 20189.579274892807, 'total_duration': 32757.81765937805, 'accumulated_submission_time': 20189.579274892807, 'accumulated_eval_time': 12565.69912815094, 'accumulated_logging_time': 0.722074031829834, 'global_step': 57559, 'preemption_count': 0}), (59957, {'train/accuracy': 0.6659659147262573, 'train/loss': 1.7736477851867676, 'train/bleu': 33.462413155122306, 'validation/accuracy': 0.6806362867355347, 'validation/loss': 1.6755925416946411, 'validation/bleu': 30.146721231820003, 'validation/num_examples': 3000, 'test/accuracy': 0.6962756514549255, 'test/loss': 1.587439775466919, 'test/bleu': 29.90336671635429, 'test/num_examples': 3003, 'score': 21029.692541837692, 'total_duration': 34085.48671579361, 'accumulated_submission_time': 21029.692541837692, 'accumulated_eval_time': 13053.144666194916, 'accumulated_logging_time': 0.7552039623260498, 'global_step': 59957, 'preemption_count': 0}), (62356, {'train/accuracy': 0.6661973595619202, 'train/loss': 1.785617709159851, 'train/bleu': 33.18605866469831, 'validation/accuracy': 0.6803635358810425, 'validation/loss': 1.675857663154602, 'validation/bleu': 29.767176532351236, 'validation/num_examples': 3000, 'test/accuracy': 0.6968218088150024, 'test/loss': 1.5876115560531616, 'test/bleu': 29.91820395719109, 'test/num_examples': 3003, 'score': 21869.68085551262, 'total_duration': 35374.082418203354, 'accumulated_submission_time': 21869.68085551262, 'accumulated_eval_time': 13501.640924453735, 'accumulated_logging_time': 0.7891860008239746, 'global_step': 62356, 'preemption_count': 0}), (64756, {'train/accuracy': 0.6719292402267456, 'train/loss': 1.71939218044281, 'train/bleu': 33.31623273945491, 'validation/accuracy': 0.6818762421607971, 'validation/loss': 1.6507298946380615, 'validation/bleu': 30.07580176805558, 'validation/num_examples': 3000, 'test/accuracy': 0.696577787399292, 'test/loss': 1.567109227180481, 'test/bleu': 30.027499266208334, 'test/num_examples': 3003, 'score': 22709.793394088745, 'total_duration': 36698.05253005028, 'accumulated_submission_time': 22709.793394088745, 'accumulated_eval_time': 13985.39007115364, 'accumulated_logging_time': 0.8220915794372559, 'global_step': 64756, 'preemption_count': 0}), (67155, {'train/accuracy': 0.6676704287528992, 'train/loss': 1.7535920143127441, 'train/bleu': 32.8873355357405, 'validation/accuracy': 0.6813802719116211, 'validation/loss': 1.6592007875442505, 'validation/bleu': 29.90753576601113, 'validation/num_examples': 3000, 'test/accuracy': 0.6964499354362488, 'test/loss': 1.5692729949951172, 'test/bleu': 29.772931959805444, 'test/num_examples': 3003, 'score': 23549.837747335434, 'total_duration': 37985.99623680115, 'accumulated_submission_time': 23549.837747335434, 'accumulated_eval_time': 14433.178615570068, 'accumulated_logging_time': 0.8567020893096924, 'global_step': 67155, 'preemption_count': 0}), (69554, {'train/accuracy': 0.6829418540000916, 'train/loss': 1.6505063772201538, 'train/bleu': 34.84525919786184, 'validation/accuracy': 0.6819506287574768, 'validation/loss': 1.6456681489944458, 'validation/bleu': 30.260847580355374, 'validation/num_examples': 3000, 'test/accuracy': 0.6994596719741821, 'test/loss': 1.5576194524765015, 'test/bleu': 30.311562866280294, 'test/num_examples': 3003, 'score': 24389.92008113861, 'total_duration': 39319.03587079048, 'accumulated_submission_time': 24389.92008113861, 'accumulated_eval_time': 14926.024219036102, 'accumulated_logging_time': 0.891709566116333, 'global_step': 69554, 'preemption_count': 0}), (71953, {'train/accuracy': 0.6751496195793152, 'train/loss': 1.7136439085006714, 'train/bleu': 34.044371473527406, 'validation/accuracy': 0.683302104473114, 'validation/loss': 1.647717833518982, 'validation/bleu': 30.32466585747712, 'validation/num_examples': 3000, 'test/accuracy': 0.6987624168395996, 'test/loss': 1.561434030532837, 'test/bleu': 30.008816688644934, 'test/num_examples': 3003, 'score': 25229.987417936325, 'total_duration': 40641.21749544144, 'accumulated_submission_time': 25229.987417936325, 'accumulated_eval_time': 15408.027456998825, 'accumulated_logging_time': 0.9275662899017334, 'global_step': 71953, 'preemption_count': 0}), (74352, {'train/accuracy': 0.6767979860305786, 'train/loss': 1.709175705909729, 'train/bleu': 33.86340278617494, 'validation/accuracy': 0.6831409335136414, 'validation/loss': 1.6428673267364502, 'validation/bleu': 30.189353259735633, 'validation/num_examples': 3000, 'test/accuracy': 0.6999825835227966, 'test/loss': 1.5527477264404297, 'test/bleu': 30.17969249679205, 'test/num_examples': 3003, 'score': 26069.960821390152, 'total_duration': 41962.508370399475, 'accumulated_submission_time': 26069.960821390152, 'accumulated_eval_time': 15889.227751493454, 'accumulated_logging_time': 0.9696736335754395, 'global_step': 74352, 'preemption_count': 0}), (76750, {'train/accuracy': 0.6845331788063049, 'train/loss': 1.6683518886566162, 'train/bleu': 34.869740284048476, 'validation/accuracy': 0.683984100818634, 'validation/loss': 1.6397377252578735, 'validation/bleu': 30.00869039508164, 'validation/num_examples': 3000, 'test/accuracy': 0.6988554000854492, 'test/loss': 1.5575097799301147, 'test/bleu': 30.020957346602792, 'test/num_examples': 3003, 'score': 26910.131754636765, 'total_duration': 43282.06278991699, 'accumulated_submission_time': 26910.131754636765, 'accumulated_eval_time': 16368.497725009918, 'accumulated_logging_time': 1.0062716007232666, 'global_step': 76750, 'preemption_count': 0}), (79149, {'train/accuracy': 0.6779137253761292, 'train/loss': 1.6993038654327393, 'train/bleu': 33.970007069879514, 'validation/accuracy': 0.684653639793396, 'validation/loss': 1.6361262798309326, 'validation/bleu': 30.250622985921563, 'validation/num_examples': 3000, 'test/accuracy': 0.7008192539215088, 'test/loss': 1.5451098680496216, 'test/bleu': 30.329277062653787, 'test/num_examples': 3003, 'score': 27750.265002012253, 'total_duration': 44589.64325428009, 'accumulated_submission_time': 27750.265002012253, 'accumulated_eval_time': 16835.83377790451, 'accumulated_logging_time': 1.0420207977294922, 'global_step': 79149, 'preemption_count': 0}), (81549, {'train/accuracy': 0.7079644799232483, 'train/loss': 1.5214544534683228, 'train/bleu': 36.10748172508141, 'validation/accuracy': 0.6850627660751343, 'validation/loss': 1.627657413482666, 'validation/bleu': 30.422390302489923, 'validation/num_examples': 3000, 'test/accuracy': 0.701830267906189, 'test/loss': 1.5316359996795654, 'test/bleu': 30.135994300813106, 'test/num_examples': 3003, 'score': 28590.42280459404, 'total_duration': 45886.93843173981, 'accumulated_submission_time': 28590.42280459404, 'accumulated_eval_time': 17292.853385925293, 'accumulated_logging_time': 1.0856754779815674, 'global_step': 81549, 'preemption_count': 0}), (83947, {'train/accuracy': 0.6867976188659668, 'train/loss': 1.6339534521102905, 'train/bleu': 34.510879718808624, 'validation/accuracy': 0.6870218515396118, 'validation/loss': 1.6197893619537354, 'validation/bleu': 30.322945821213274, 'validation/num_examples': 3000, 'test/accuracy': 0.7033990025520325, 'test/loss': 1.5269720554351807, 'test/bleu': 30.452418231049375, 'test/num_examples': 3003, 'score': 29430.36559510231, 'total_duration': 47181.868070364, 'accumulated_submission_time': 29430.36559510231, 'accumulated_eval_time': 17747.714488744736, 'accumulated_logging_time': 1.1382238864898682, 'global_step': 83947, 'preemption_count': 0}), (86346, {'train/accuracy': 0.6832473278045654, 'train/loss': 1.6666216850280762, 'train/bleu': 34.83272741944827, 'validation/accuracy': 0.6869474649429321, 'validation/loss': 1.6207863092422485, 'validation/bleu': 30.546641891474536, 'validation/num_examples': 3000, 'test/accuracy': 0.7028993368148804, 'test/loss': 1.5305290222167969, 'test/bleu': 30.481362155277292, 'test/num_examples': 3003, 'score': 30270.410665035248, 'total_duration': 48519.62218332291, 'accumulated_submission_time': 30270.410665035248, 'accumulated_eval_time': 18245.310992002487, 'accumulated_logging_time': 1.1753811836242676, 'global_step': 86346, 'preemption_count': 0}), (88746, {'train/accuracy': 0.7003049254417419, 'train/loss': 1.5541876554489136, 'train/bleu': 35.16525340135614, 'validation/accuracy': 0.6866498589515686, 'validation/loss': 1.6179324388504028, 'validation/bleu': 30.46771344521242, 'validation/num_examples': 3000, 'test/accuracy': 0.7047004699707031, 'test/loss': 1.5223288536071777, 'test/bleu': 30.563865944400106, 'test/num_examples': 3003, 'score': 31110.536297559738, 'total_duration': 49828.37257575989, 'accumulated_submission_time': 31110.536297559738, 'accumulated_eval_time': 18713.81881380081, 'accumulated_logging_time': 1.2181808948516846, 'global_step': 88746, 'preemption_count': 0}), (91145, {'train/accuracy': 0.6862742900848389, 'train/loss': 1.6273705959320068, 'train/bleu': 34.99582119196632, 'validation/accuracy': 0.6887701153755188, 'validation/loss': 1.612417221069336, 'validation/bleu': 30.406371078037647, 'validation/num_examples': 3000, 'test/accuracy': 0.7067689299583435, 'test/loss': 1.5179078578948975, 'test/bleu': 30.68588993681621, 'test/num_examples': 3003, 'score': 31950.433703422546, 'total_duration': 51123.85071182251, 'accumulated_submission_time': 31950.433703422546, 'accumulated_eval_time': 19169.279168844223, 'accumulated_logging_time': 1.2637646198272705, 'global_step': 91145, 'preemption_count': 0}), (93544, {'train/accuracy': 0.6899000406265259, 'train/loss': 1.6128188371658325, 'train/bleu': 34.90819228231661, 'validation/accuracy': 0.6882493495941162, 'validation/loss': 1.6095622777938843, 'validation/bleu': 30.37207243465497, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.51614511013031, 'test/bleu': 30.48390357612902, 'test/num_examples': 3003, 'score': 32790.42817759514, 'total_duration': 52405.03318023682, 'accumulated_submission_time': 32790.42817759514, 'accumulated_eval_time': 19610.351407289505, 'accumulated_logging_time': 1.3041329383850098, 'global_step': 93544, 'preemption_count': 0}), (95943, {'train/accuracy': 0.697588324546814, 'train/loss': 1.5702698230743408, 'train/bleu': 35.739230605844355, 'validation/accuracy': 0.6895760893821716, 'validation/loss': 1.610058069229126, 'validation/bleu': 30.57486585549891, 'validation/num_examples': 3000, 'test/accuracy': 0.7060717344284058, 'test/loss': 1.5157976150512695, 'test/bleu': 30.849107356658006, 'test/num_examples': 3003, 'score': 33630.38622021675, 'total_duration': 53708.71432638168, 'accumulated_submission_time': 33630.38622021675, 'accumulated_eval_time': 20073.95977115631, 'accumulated_logging_time': 1.3421132564544678, 'global_step': 95943, 'preemption_count': 0}), (98343, {'train/accuracy': 0.6986111998558044, 'train/loss': 1.5686570405960083, 'train/bleu': 35.27395572303037, 'validation/accuracy': 0.6885965466499329, 'validation/loss': 1.6070998907089233, 'validation/bleu': 30.290472276833565, 'validation/num_examples': 3000, 'test/accuracy': 0.7058973908424377, 'test/loss': 1.5149260759353638, 'test/bleu': 30.596273861079318, 'test/num_examples': 3003, 'score': 34470.52443647385, 'total_duration': 55018.911603450775, 'accumulated_submission_time': 34470.52443647385, 'accumulated_eval_time': 20543.905728816986, 'accumulated_logging_time': 1.3811841011047363, 'global_step': 98343, 'preemption_count': 0}), (100742, {'train/accuracy': 0.710821807384491, 'train/loss': 1.5044238567352295, 'train/bleu': 37.11803812727856, 'validation/accuracy': 0.6893652677536011, 'validation/loss': 1.6094584465026855, 'validation/bleu': 30.65314764916486, 'validation/num_examples': 3000, 'test/accuracy': 0.7069548964500427, 'test/loss': 1.5113070011138916, 'test/bleu': 30.70333007489312, 'test/num_examples': 3003, 'score': 35310.65348362923, 'total_duration': 56318.565016269684, 'accumulated_submission_time': 35310.65348362923, 'accumulated_eval_time': 21003.313571691513, 'accumulated_logging_time': 1.4205613136291504, 'global_step': 100742, 'preemption_count': 0}), (103142, {'train/accuracy': 0.7058877348899841, 'train/loss': 1.5339775085449219, 'train/bleu': 36.642378290460236, 'validation/accuracy': 0.6901340484619141, 'validation/loss': 1.6044197082519531, 'validation/bleu': 30.616822115538966, 'validation/num_examples': 3000, 'test/accuracy': 0.7081401348114014, 'test/loss': 1.5082166194915771, 'test/bleu': 30.886960740934175, 'test/num_examples': 3003, 'score': 36150.857697963715, 'total_duration': 57621.27230811119, 'accumulated_submission_time': 36150.857697963715, 'accumulated_eval_time': 21465.7024269104, 'accumulated_logging_time': 1.4596493244171143, 'global_step': 103142, 'preemption_count': 0}), (105542, {'train/accuracy': 0.7012184858322144, 'train/loss': 1.5534294843673706, 'train/bleu': 36.010873962839284, 'validation/accuracy': 0.6888568997383118, 'validation/loss': 1.6038175821304321, 'validation/bleu': 30.25613399959102, 'validation/num_examples': 3000, 'test/accuracy': 0.7084190249443054, 'test/loss': 1.5058610439300537, 'test/bleu': 30.523381697580785, 'test/num_examples': 3003, 'score': 36991.28224873543, 'total_duration': 58921.207073926926, 'accumulated_submission_time': 36991.28224873543, 'accumulated_eval_time': 21925.09859728813, 'accumulated_logging_time': 1.5006978511810303, 'global_step': 105542, 'preemption_count': 0}), (107941, {'train/accuracy': 0.7100006937980652, 'train/loss': 1.5051331520080566, 'train/bleu': 36.99780640058015, 'validation/accuracy': 0.689811646938324, 'validation/loss': 1.60414719581604, 'validation/bleu': 30.53258891829982, 'validation/num_examples': 3000, 'test/accuracy': 0.7079658508300781, 'test/loss': 1.504045844078064, 'test/bleu': 30.685208275329504, 'test/num_examples': 3003, 'score': 37831.47907757759, 'total_duration': 60206.88630771637, 'accumulated_submission_time': 37831.47907757759, 'accumulated_eval_time': 22370.46321105957, 'accumulated_logging_time': 1.5430285930633545, 'global_step': 107941, 'preemption_count': 0}), (110340, {'train/accuracy': 0.7088562846183777, 'train/loss': 1.5129475593566895, 'train/bleu': 35.960513350985146, 'validation/accuracy': 0.6904191970825195, 'validation/loss': 1.6018781661987305, 'validation/bleu': 30.43087989291936, 'validation/num_examples': 3000, 'test/accuracy': 0.7091511487960815, 'test/loss': 1.5065263509750366, 'test/bleu': 30.692955515135495, 'test/num_examples': 3003, 'score': 38671.50674414635, 'total_duration': 61503.93488526344, 'accumulated_submission_time': 38671.50674414635, 'accumulated_eval_time': 22827.366498708725, 'accumulated_logging_time': 1.5851430892944336, 'global_step': 110340, 'preemption_count': 0}), (112740, {'train/accuracy': 0.7243646383285522, 'train/loss': 1.4386584758758545, 'train/bleu': 37.83614617089411, 'validation/accuracy': 0.6897496581077576, 'validation/loss': 1.5978442430496216, 'validation/bleu': 30.643926207512962, 'validation/num_examples': 3000, 'test/accuracy': 0.7088257670402527, 'test/loss': 1.500476598739624, 'test/bleu': 30.616537234812025, 'test/num_examples': 3003, 'score': 39511.70456314087, 'total_duration': 62802.67998576164, 'accumulated_submission_time': 39511.70456314087, 'accumulated_eval_time': 23285.7987074852, 'accumulated_logging_time': 1.6249699592590332, 'global_step': 112740, 'preemption_count': 0}), (115139, {'train/accuracy': 0.7164209485054016, 'train/loss': 1.4740687608718872, 'train/bleu': 37.267175904620444, 'validation/accuracy': 0.6905679702758789, 'validation/loss': 1.598555564880371, 'validation/bleu': 30.91084348148604, 'validation/num_examples': 3000, 'test/accuracy': 0.7087792754173279, 'test/loss': 1.4993302822113037, 'test/bleu': 30.776656235656343, 'test/num_examples': 3003, 'score': 40351.63377261162, 'total_duration': 64096.69275712967, 'accumulated_submission_time': 40351.63377261162, 'accumulated_eval_time': 23739.756477594376, 'accumulated_logging_time': 1.674572467803955, 'global_step': 115139, 'preemption_count': 0}), (117538, {'train/accuracy': 0.7153463959693909, 'train/loss': 1.4766026735305786, 'train/bleu': 36.84388016626823, 'validation/accuracy': 0.6916590929031372, 'validation/loss': 1.6009763479232788, 'validation/bleu': 30.74155176517884, 'validation/num_examples': 3000, 'test/accuracy': 0.7088141441345215, 'test/loss': 1.501481056213379, 'test/bleu': 30.903441013207743, 'test/num_examples': 3003, 'score': 41191.59611129761, 'total_duration': 65385.50169849396, 'accumulated_submission_time': 41191.59611129761, 'accumulated_eval_time': 24188.48369383812, 'accumulated_logging_time': 1.718095302581787, 'global_step': 117538, 'preemption_count': 0}), (119937, {'train/accuracy': 0.7232077717781067, 'train/loss': 1.438293695449829, 'train/bleu': 37.65100512157615, 'validation/accuracy': 0.6920310854911804, 'validation/loss': 1.6015228033065796, 'validation/bleu': 30.751410962903055, 'validation/num_examples': 3000, 'test/accuracy': 0.7088955044746399, 'test/loss': 1.4994897842407227, 'test/bleu': 30.883720879674414, 'test/num_examples': 3003, 'score': 42031.71455454826, 'total_duration': 66693.94711279869, 'accumulated_submission_time': 42031.71455454826, 'accumulated_eval_time': 24656.691277503967, 'accumulated_logging_time': 1.7594947814941406, 'global_step': 119937, 'preemption_count': 0}), (122336, {'train/accuracy': 0.7199275493621826, 'train/loss': 1.4584884643554688, 'train/bleu': 37.4064410559461, 'validation/accuracy': 0.6925642490386963, 'validation/loss': 1.5980294942855835, 'validation/bleu': 30.635947764975892, 'validation/num_examples': 3000, 'test/accuracy': 0.70961594581604, 'test/loss': 1.4969851970672607, 'test/bleu': 30.907040212287704, 'test/num_examples': 3003, 'score': 42871.72847676277, 'total_duration': 68006.24787807465, 'accumulated_submission_time': 42871.72847676277, 'accumulated_eval_time': 25128.770598888397, 'accumulated_logging_time': 1.8893790245056152, 'global_step': 122336, 'preemption_count': 0}), (124734, {'train/accuracy': 0.7197436094284058, 'train/loss': 1.4523401260375977, 'train/bleu': 37.86308040490159, 'validation/accuracy': 0.6918451189994812, 'validation/loss': 1.6004658937454224, 'validation/bleu': 30.577418563951106, 'validation/num_examples': 3000, 'test/accuracy': 0.7094067931175232, 'test/loss': 1.498136281967163, 'test/bleu': 30.645121997646555, 'test/num_examples': 3003, 'score': 43711.629980802536, 'total_duration': 69304.01886892319, 'accumulated_submission_time': 43711.629980802536, 'accumulated_eval_time': 25586.507713079453, 'accumulated_logging_time': 1.9442377090454102, 'global_step': 124734, 'preemption_count': 0}), (127133, {'train/accuracy': 0.723660409450531, 'train/loss': 1.441123127937317, 'train/bleu': 37.82017963906209, 'validation/accuracy': 0.6915971040725708, 'validation/loss': 1.5994668006896973, 'validation/bleu': 30.58672769907038, 'validation/num_examples': 3000, 'test/accuracy': 0.70961594581604, 'test/loss': 1.4976170063018799, 'test/bleu': 30.935873794008984, 'test/num_examples': 3003, 'score': 44551.60100340843, 'total_duration': 70614.54214811325, 'accumulated_submission_time': 44551.60100340843, 'accumulated_eval_time': 26056.940609931946, 'accumulated_logging_time': 1.98885178565979, 'global_step': 127133, 'preemption_count': 0}), (129532, {'train/accuracy': 0.7209928035736084, 'train/loss': 1.453560709953308, 'train/bleu': 37.57740041402509, 'validation/accuracy': 0.6917583346366882, 'validation/loss': 1.5988129377365112, 'validation/bleu': 30.576942252558766, 'validation/num_examples': 3000, 'test/accuracy': 0.709732174873352, 'test/loss': 1.4976327419281006, 'test/bleu': 30.904820565100355, 'test/num_examples': 3003, 'score': 45391.557307481766, 'total_duration': 71920.82846522331, 'accumulated_submission_time': 45391.557307481766, 'accumulated_eval_time': 26523.145493984222, 'accumulated_logging_time': 2.03778076171875, 'global_step': 129532, 'preemption_count': 0}), (131931, {'train/accuracy': 0.7224260568618774, 'train/loss': 1.4456599950790405, 'train/bleu': 37.93193093486411, 'validation/accuracy': 0.6920434832572937, 'validation/loss': 1.5994871854782104, 'validation/bleu': 30.504024003063833, 'validation/num_examples': 3000, 'test/accuracy': 0.7099761962890625, 'test/loss': 1.4979794025421143, 'test/bleu': 30.837799961632033, 'test/num_examples': 3003, 'score': 46231.56835961342, 'total_duration': 73221.8060324192, 'accumulated_submission_time': 46231.56835961342, 'accumulated_eval_time': 26983.98964881897, 'accumulated_logging_time': 2.0844168663024902, 'global_step': 131931, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7234098315238953, 'train/loss': 1.439003586769104, 'train/bleu': 37.50861974519182, 'validation/accuracy': 0.6921550631523132, 'validation/loss': 1.5993489027023315, 'validation/bleu': 30.494101371783113, 'validation/num_examples': 3000, 'test/accuracy': 0.7098832130432129, 'test/loss': 1.4978277683258057, 'test/bleu': 30.838390296957208, 'test/num_examples': 3003, 'score': 46722.17216897011, 'total_duration': 74176.3343679905, 'accumulated_submission_time': 46722.17216897011, 'accumulated_eval_time': 27447.8187623024, 'accumulated_logging_time': 2.1351213455200195, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0208 06:49:36.355943 140446903760704 submission_runner.py:586] Timing: 46722.17216897011
I0208 06:49:36.356004 140446903760704 submission_runner.py:588] Total number of evals: 57
I0208 06:49:36.356055 140446903760704 submission_runner.py:589] ====================
I0208 06:49:36.356107 140446903760704 submission_runner.py:542] Using RNG seed 1540897543
I0208 06:49:36.357828 140446903760704 submission_runner.py:551] --- Tuning run 3/5 ---
I0208 06:49:36.357932 140446903760704 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_3.
I0208 06:49:36.358299 140446903760704 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_3/hparams.json.
I0208 06:49:36.359075 140446903760704 submission_runner.py:206] Initializing dataset.
I0208 06:49:36.361908 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 06:49:36.364845 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0208 06:49:36.404091 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 06:49:37.057280 140446903760704 submission_runner.py:213] Initializing model.
I0208 06:49:43.880134 140446903760704 submission_runner.py:255] Initializing optimizer.
I0208 06:49:44.686655 140446903760704 submission_runner.py:262] Initializing metrics bundle.
I0208 06:49:44.686835 140446903760704 submission_runner.py:280] Initializing checkpoint and logger.
I0208 06:49:44.687760 140446903760704 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/wmt_jax/trial_3 with prefix checkpoint_
I0208 06:49:44.687890 140446903760704 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_3/meta_data_0.json.
I0208 06:49:44.688097 140446903760704 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0208 06:49:44.688159 140446903760704 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0208 06:49:45.242315 140446903760704 logger_utils.py:220] Unable to record git information. Continuing without it.
I0208 06:49:45.763873 140446903760704 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_3/flags_0.json.
I0208 06:49:45.767399 140446903760704 submission_runner.py:314] Starting training loop.
I0208 06:50:13.194238 140277094741760 logging_writer.py:48] [0] global_step=0, grad_norm=5.882989406585693, loss=11.163689613342285
I0208 06:50:13.206830 140446903760704 spec.py:321] Evaluating on the training split.
I0208 06:50:15.912191 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:54:54.524272 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 06:54:57.253265 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 06:59:33.639977 140446903760704 spec.py:349] Evaluating on the test split.
I0208 06:59:36.356715 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 07:04:13.156508 140446903760704 submission_runner.py:408] Time since start: 867.39s, 	Step: 1, 	{'train/accuracy': 0.000624552892986685, 'train/loss': 11.190462112426758, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.4393572807312, 'total_duration': 867.3890187740326, 'accumulated_submission_time': 27.4393572807312, 'accumulated_eval_time': 839.9496130943298, 'accumulated_logging_time': 0}
I0208 07:04:13.165590 140277103134464 logging_writer.py:48] [1] accumulated_eval_time=839.949613, accumulated_logging_time=0, accumulated_submission_time=27.439357, global_step=1, preemption_count=0, score=27.439357, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.191028, test/num_examples=3003, total_duration=867.389019, train/accuracy=0.000625, train/bleu=0.000000, train/loss=11.190462, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.190282, validation/num_examples=3000
I0208 07:04:48.122544 140277094741760 logging_writer.py:48] [100] global_step=100, grad_norm=0.4706319272518158, loss=8.721490859985352
I0208 07:05:23.077830 140277103134464 logging_writer.py:48] [200] global_step=200, grad_norm=0.1890704184770584, loss=8.367546081542969
I0208 07:05:58.081087 140277094741760 logging_writer.py:48] [300] global_step=300, grad_norm=0.23125241696834564, loss=8.113974571228027
I0208 07:06:33.126697 140277103134464 logging_writer.py:48] [400] global_step=400, grad_norm=0.2942652702331543, loss=7.651282787322998
I0208 07:07:08.173199 140277094741760 logging_writer.py:48] [500] global_step=500, grad_norm=0.4144660234451294, loss=7.279980182647705
I0208 07:07:43.337810 140277103134464 logging_writer.py:48] [600] global_step=600, grad_norm=0.581066906452179, loss=7.035874843597412
I0208 07:08:18.442798 140277094741760 logging_writer.py:48] [700] global_step=700, grad_norm=0.6190925240516663, loss=6.781818389892578
I0208 07:08:53.535039 140277103134464 logging_writer.py:48] [800] global_step=800, grad_norm=0.6298009157180786, loss=6.515779495239258
I0208 07:09:28.645259 140277094741760 logging_writer.py:48] [900] global_step=900, grad_norm=0.5615819096565247, loss=6.326858997344971
I0208 07:10:03.712398 140277103134464 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6084052324295044, loss=5.989687919616699
I0208 07:10:38.772284 140277094741760 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7408734560012817, loss=5.806454181671143
I0208 07:11:13.871686 140277103134464 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7336597442626953, loss=5.566746711730957
I0208 07:11:49.057916 140277094741760 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0276644229888916, loss=5.455422401428223
I0208 07:12:24.205203 140277103134464 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5867651700973511, loss=5.289326190948486
I0208 07:12:59.293242 140277094741760 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.2533528804779053, loss=5.117802619934082
I0208 07:13:34.381972 140277103134464 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6876034736633301, loss=4.944925308227539
I0208 07:14:09.465158 140277094741760 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8147586584091187, loss=4.864109992980957
I0208 07:14:44.549583 140277103134464 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8682399392127991, loss=4.649642467498779
I0208 07:15:19.634540 140277094741760 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.3522754907608032, loss=4.59893274307251
I0208 07:15:54.693725 140277103134464 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1559585332870483, loss=4.496230125427246
I0208 07:16:29.765763 140277094741760 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9250082969665527, loss=4.231875896453857
I0208 07:17:04.880423 140277103134464 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8850589394569397, loss=4.191498279571533
I0208 07:17:39.973912 140277094741760 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9125306010246277, loss=4.086455345153809
I0208 07:18:13.352220 140446903760704 spec.py:321] Evaluating on the training split.
I0208 07:18:16.374491 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 07:22:06.016296 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 07:22:08.758136 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 07:26:00.830961 140446903760704 spec.py:349] Evaluating on the test split.
I0208 07:26:03.553083 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 07:30:19.982356 140446903760704 submission_runner.py:408] Time since start: 2434.21s, 	Step: 2397, 	{'train/accuracy': 0.41179487109184265, 'train/loss': 3.941603660583496, 'train/bleu': 14.546588761941905, 'validation/accuracy': 0.3955065608024597, 'validation/loss': 4.074873924255371, 'validation/bleu': 9.821189947778166, 'validation/num_examples': 3000, 'test/accuracy': 0.38170936703681946, 'test/loss': 4.273231029510498, 'test/bleu': 7.8068550044099725, 'test/num_examples': 3003, 'score': 867.5393142700195, 'total_duration': 2434.214878797531, 'accumulated_submission_time': 867.5393142700195, 'accumulated_eval_time': 1566.579701423645, 'accumulated_logging_time': 0.0190737247467041}
I0208 07:30:19.997143 140277103134464 logging_writer.py:48] [2397] accumulated_eval_time=1566.579701, accumulated_logging_time=0.019074, accumulated_submission_time=867.539314, global_step=2397, preemption_count=0, score=867.539314, test/accuracy=0.381709, test/bleu=7.806855, test/loss=4.273231, test/num_examples=3003, total_duration=2434.214879, train/accuracy=0.411795, train/bleu=14.546589, train/loss=3.941604, validation/accuracy=0.395507, validation/bleu=9.821190, validation/loss=4.074874, validation/num_examples=3000
I0208 07:30:21.412282 140277094741760 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7759048342704773, loss=3.958223581314087
I0208 07:30:56.340199 140277103134464 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.080675482749939, loss=3.8930845260620117
I0208 07:31:31.364354 140277094741760 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8309037685394287, loss=3.756315231323242
I0208 07:32:06.442256 140277103134464 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8783284425735474, loss=3.72739839553833
I0208 07:32:41.529025 140277094741760 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9864908456802368, loss=3.513230562210083
I0208 07:33:16.613774 140277103134464 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.737920343875885, loss=3.5755090713500977
I0208 07:33:51.684308 140277094741760 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8712968826293945, loss=3.4831361770629883
I0208 07:34:26.791850 140277103134464 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9866884350776672, loss=3.3339922428131104
I0208 07:35:01.881351 140277094741760 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.3148540258407593, loss=3.2094151973724365
I0208 07:35:36.980588 140277103134464 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9223987460136414, loss=3.149305582046509
I0208 07:36:12.062799 140277094741760 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7077582478523254, loss=3.172111749649048
I0208 07:36:47.146922 140277103134464 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.084343671798706, loss=3.024444818496704
I0208 07:37:22.236190 140277094741760 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.0661600828170776, loss=3.0479888916015625
I0208 07:37:57.420837 140277103134464 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7045097351074219, loss=2.8967485427856445
I0208 07:38:32.496261 140277094741760 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8597467541694641, loss=2.97233247756958
I0208 07:39:07.599395 140277103134464 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8841952085494995, loss=3.0301175117492676
I0208 07:39:42.661511 140277094741760 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6709666848182678, loss=2.9151391983032227
I0208 07:40:17.769495 140277103134464 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9795652031898499, loss=2.868878126144409
I0208 07:40:52.889149 140277094741760 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6731809377670288, loss=2.9061596393585205
I0208 07:41:27.985505 140277103134464 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7403491139411926, loss=2.8368747234344482
I0208 07:42:03.114256 140277094741760 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6746262907981873, loss=2.8435275554656982
I0208 07:42:38.218833 140277103134464 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.642228364944458, loss=2.7183873653411865
I0208 07:43:13.299620 140277094741760 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6696427464485168, loss=2.8080379962921143
I0208 07:43:48.396708 140277103134464 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7133246660232544, loss=2.676612377166748
I0208 07:44:20.028451 140446903760704 spec.py:321] Evaluating on the training split.
I0208 07:44:23.046353 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 07:46:59.638179 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 07:47:02.375779 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 07:49:34.612327 140446903760704 spec.py:349] Evaluating on the test split.
I0208 07:49:37.339571 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 07:51:59.491989 140446903760704 submission_runner.py:408] Time since start: 3733.72s, 	Step: 4792, 	{'train/accuracy': 0.5426854491233826, 'train/loss': 2.662497043609619, 'train/bleu': 24.877218045562664, 'validation/accuracy': 0.5426343083381653, 'validation/loss': 2.6319589614868164, 'validation/bleu': 20.313745733230586, 'validation/num_examples': 3000, 'test/accuracy': 0.5415606498718262, 'test/loss': 2.6677498817443848, 'test/bleu': 18.69656506174673, 'test/num_examples': 3003, 'score': 1707.4862928390503, 'total_duration': 3733.7244775295258, 'accumulated_submission_time': 1707.4862928390503, 'accumulated_eval_time': 2026.0431470870972, 'accumulated_logging_time': 0.044114112854003906}
I0208 07:51:59.510316 140277094741760 logging_writer.py:48] [4792] accumulated_eval_time=2026.043147, accumulated_logging_time=0.044114, accumulated_submission_time=1707.486293, global_step=4792, preemption_count=0, score=1707.486293, test/accuracy=0.541561, test/bleu=18.696565, test/loss=2.667750, test/num_examples=3003, total_duration=3733.724478, train/accuracy=0.542685, train/bleu=24.877218, train/loss=2.662497, validation/accuracy=0.542634, validation/bleu=20.313746, validation/loss=2.631959, validation/num_examples=3000
I0208 07:52:02.689730 140277103134464 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6265666484832764, loss=2.614335536956787
I0208 07:52:37.623797 140277094741760 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6761629581451416, loss=2.6573545932769775
I0208 07:53:12.633412 140277103134464 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6193910241127014, loss=2.585414409637451
I0208 07:53:47.699438 140277094741760 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6364688277244568, loss=2.6099629402160645
I0208 07:54:22.766792 140277103134464 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6485515832901001, loss=2.5664517879486084
I0208 07:54:57.820486 140277094741760 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7147272229194641, loss=2.5413167476654053
I0208 07:55:32.885092 140277103134464 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6169868111610413, loss=2.6086602210998535
I0208 07:56:07.953323 140277094741760 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5952879786491394, loss=2.5276925563812256
I0208 07:56:43.009750 140277103134464 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7092970609664917, loss=2.600309133529663
I0208 07:57:18.073268 140277094741760 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7493918538093567, loss=2.47646427154541
I0208 07:57:53.129412 140277103134464 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5773379802703857, loss=2.4172916412353516
I0208 07:58:28.191871 140277094741760 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6859408020973206, loss=2.5479860305786133
I0208 07:59:03.261538 140277103134464 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5423730611801147, loss=2.351825475692749
I0208 07:59:38.360551 140277094741760 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5181106328964233, loss=2.425419330596924
I0208 08:00:13.446781 140277103134464 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.587325930595398, loss=2.51456618309021
I0208 08:00:48.520154 140277094741760 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5220293402671814, loss=2.419110059738159
I0208 08:01:23.579909 140277103134464 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6282638907432556, loss=2.451148271560669
I0208 08:01:58.691260 140277094741760 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6407616138458252, loss=2.4505763053894043
I0208 08:02:33.755896 140277103134464 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.49658361077308655, loss=2.4272568225860596
I0208 08:03:08.833662 140277094741760 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4879801571369171, loss=2.440429210662842
I0208 08:03:43.898293 140277103134464 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5409682989120483, loss=2.352224588394165
I0208 08:04:18.954982 140277094741760 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5055803060531616, loss=2.386930465698242
I0208 08:04:54.037613 140277103134464 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.45982953906059265, loss=2.345890998840332
I0208 08:05:29.107421 140277094741760 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.47231006622314453, loss=2.3299994468688965
I0208 08:05:59.689732 140446903760704 spec.py:321] Evaluating on the training split.
I0208 08:06:02.725630 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:08:46.899645 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 08:08:49.611649 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:11:29.243423 140446903760704 spec.py:349] Evaluating on the test split.
I0208 08:11:31.958707 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:14:00.430900 140446903760704 submission_runner.py:408] Time since start: 5054.66s, 	Step: 7189, 	{'train/accuracy': 0.5803403854370117, 'train/loss': 2.2744669914245605, 'train/bleu': 27.177722377753945, 'validation/accuracy': 0.585894763469696, 'validation/loss': 2.2277345657348633, 'validation/bleu': 23.692412799424854, 'validation/num_examples': 3000, 'test/accuracy': 0.5870431661605835, 'test/loss': 2.226987838745117, 'test/bleu': 21.947922210608603, 'test/num_examples': 3003, 'score': 2547.5802307128906, 'total_duration': 5054.663430452347, 'accumulated_submission_time': 2547.5802307128906, 'accumulated_eval_time': 2506.784266471863, 'accumulated_logging_time': 0.07352590560913086}
I0208 08:14:00.445954 140277103134464 logging_writer.py:48] [7189] accumulated_eval_time=2506.784266, accumulated_logging_time=0.073526, accumulated_submission_time=2547.580231, global_step=7189, preemption_count=0, score=2547.580231, test/accuracy=0.587043, test/bleu=21.947922, test/loss=2.226988, test/num_examples=3003, total_duration=5054.663430, train/accuracy=0.580340, train/bleu=27.177722, train/loss=2.274467, validation/accuracy=0.585895, validation/bleu=23.692413, validation/loss=2.227735, validation/num_examples=3000
I0208 08:14:04.652126 140277094741760 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4940781891345978, loss=2.3289546966552734
I0208 08:14:39.573028 140277103134464 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4198094606399536, loss=2.2880568504333496
I0208 08:15:14.588604 140277094741760 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5877674221992493, loss=2.239927053451538
I0208 08:15:49.667247 140277103134464 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5194919109344482, loss=2.305410146713257
I0208 08:16:24.738360 140277094741760 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8033495545387268, loss=2.288898468017578
I0208 08:16:59.832293 140277103134464 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.44942450523376465, loss=2.2649776935577393
I0208 08:17:34.983742 140277094741760 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4238924980163574, loss=2.2674953937530518
I0208 08:18:10.073781 140277103134464 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4608036279678345, loss=2.231086254119873
I0208 08:18:45.113141 140277094741760 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4031878411769867, loss=2.198554277420044
I0208 08:19:20.145547 140277103134464 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.41042444109916687, loss=2.2460837364196777
I0208 08:19:55.184501 140277094741760 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4044198989868164, loss=2.1460442543029785
I0208 08:20:30.234453 140277103134464 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.40898382663726807, loss=2.3071184158325195
I0208 08:21:05.273987 140277094741760 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.38648706674575806, loss=2.185678005218506
I0208 08:21:40.333475 140277103134464 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4792834520339966, loss=2.196812868118286
I0208 08:22:15.388091 140277094741760 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4769607484340668, loss=2.227112293243408
I0208 08:22:50.433147 140277103134464 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.41491132974624634, loss=2.22857928276062
I0208 08:23:25.517975 140277094741760 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.4366208016872406, loss=2.161709785461426
I0208 08:24:00.550446 140277103134464 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.47305262088775635, loss=2.227513313293457
I0208 08:24:35.581228 140277094741760 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.4369116723537445, loss=2.2386257648468018
I0208 08:25:10.628409 140277103134464 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.434876412153244, loss=2.1579089164733887
I0208 08:25:45.685588 140277094741760 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.36964622139930725, loss=2.2773995399475098
I0208 08:26:20.744094 140277103134464 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3493838608264923, loss=2.1198079586029053
I0208 08:26:55.823065 140277094741760 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3958986699581146, loss=2.1692402362823486
I0208 08:27:30.912737 140277103134464 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3768567442893982, loss=2.170903205871582
I0208 08:28:00.770512 140446903760704 spec.py:321] Evaluating on the training split.
I0208 08:28:03.804369 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:30:39.118772 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 08:30:41.819216 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:33:07.677869 140446903760704 spec.py:349] Evaluating on the test split.
I0208 08:33:10.377097 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:35:21.890335 140446903760704 submission_runner.py:408] Time since start: 6336.12s, 	Step: 9587, 	{'train/accuracy': 0.5935688018798828, 'train/loss': 2.1319386959075928, 'train/bleu': 28.43446243478617, 'validation/accuracy': 0.604071855545044, 'validation/loss': 2.039424180984497, 'validation/bleu': 24.43123456805673, 'validation/num_examples': 3000, 'test/accuracy': 0.6092615127563477, 'test/loss': 2.0072202682495117, 'test/bleu': 23.120191808823755, 'test/num_examples': 3003, 'score': 3387.8171026706696, 'total_duration': 6336.122858285904, 'accumulated_submission_time': 3387.8171026706696, 'accumulated_eval_time': 2947.9040400981903, 'accumulated_logging_time': 0.0987389087677002}
I0208 08:35:21.907898 140277094741760 logging_writer.py:48] [9587] accumulated_eval_time=2947.904040, accumulated_logging_time=0.098739, accumulated_submission_time=3387.817103, global_step=9587, preemption_count=0, score=3387.817103, test/accuracy=0.609262, test/bleu=23.120192, test/loss=2.007220, test/num_examples=3003, total_duration=6336.122858, train/accuracy=0.593569, train/bleu=28.434462, train/loss=2.131939, validation/accuracy=0.604072, validation/bleu=24.431235, validation/loss=2.039424, validation/num_examples=3000
I0208 08:35:26.811832 140277103134464 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3659065067768097, loss=2.1642353534698486
I0208 08:36:01.736230 140277094741760 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3347874879837036, loss=2.085092067718506
I0208 08:36:36.738415 140277103134464 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.31936928629875183, loss=2.125610828399658
I0208 08:37:11.796402 140277094741760 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.31999847292900085, loss=2.1027448177337646
I0208 08:37:46.838760 140277103134464 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.34064626693725586, loss=2.075296401977539
I0208 08:38:21.907641 140277094741760 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.338734894990921, loss=2.0751335620880127
I0208 08:38:56.980896 140277103134464 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.34284430742263794, loss=2.106285810470581
I0208 08:39:32.072767 140277094741760 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.3344913423061371, loss=2.121608257293701
I0208 08:40:07.140232 140277103134464 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3135940432548523, loss=2.0438079833984375
I0208 08:40:42.200505 140277094741760 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.33151930570602417, loss=2.1797666549682617
I0208 08:41:17.248110 140277103134464 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.340884804725647, loss=2.1275951862335205
I0208 08:41:52.319792 140277094741760 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3002038896083832, loss=2.2277188301086426
I0208 08:42:27.387536 140277103134464 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.35868823528289795, loss=2.1440744400024414
I0208 08:43:02.446959 140277094741760 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.330016553401947, loss=2.069025754928589
I0208 08:43:37.500359 140277103134464 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.2802196741104126, loss=2.1555635929107666
I0208 08:44:12.568902 140277094741760 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2969815731048584, loss=2.0460407733917236
I0208 08:44:47.619295 140277103134464 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.34712663292884827, loss=2.1342649459838867
I0208 08:45:22.660996 140277094741760 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2806435823440552, loss=2.0417215824127197
I0208 08:45:57.701411 140277103134464 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.32105082273483276, loss=2.0296292304992676
I0208 08:46:32.750826 140277094741760 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.30710431933403015, loss=2.068537712097168
I0208 08:47:07.784835 140277103134464 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.30024591088294983, loss=2.0636520385742188
I0208 08:47:42.823863 140277094741760 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.28546810150146484, loss=2.222446918487549
I0208 08:48:17.905652 140277103134464 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.3232685327529907, loss=2.030670404434204
I0208 08:48:53.263776 140277094741760 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.3234771192073822, loss=2.094402551651001
I0208 08:49:22.055660 140446903760704 spec.py:321] Evaluating on the training split.
I0208 08:49:25.068353 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:53:41.146950 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 08:53:43.861939 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 08:58:01.459086 140446903760704 spec.py:349] Evaluating on the test split.
I0208 08:58:04.187174 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 09:01:36.018782 140446903760704 submission_runner.py:408] Time since start: 7910.25s, 	Step: 11984, 	{'train/accuracy': 0.6023198962211609, 'train/loss': 2.0523767471313477, 'train/bleu': 28.641659970283314, 'validation/accuracy': 0.6189383864402771, 'validation/loss': 1.9206935167312622, 'validation/bleu': 25.24411501245022, 'validation/num_examples': 3000, 'test/accuracy': 0.6242752075195312, 'test/loss': 1.8770338296890259, 'test/bleu': 24.5813471409335, 'test/num_examples': 3003, 'score': 4227.878267049789, 'total_duration': 7910.251308679581, 'accumulated_submission_time': 4227.878267049789, 'accumulated_eval_time': 3681.8671090602875, 'accumulated_logging_time': 0.1276702880859375}
I0208 09:01:36.035808 140277103134464 logging_writer.py:48] [11984] accumulated_eval_time=3681.867109, accumulated_logging_time=0.127670, accumulated_submission_time=4227.878267, global_step=11984, preemption_count=0, score=4227.878267, test/accuracy=0.624275, test/bleu=24.581347, test/loss=1.877034, test/num_examples=3003, total_duration=7910.251309, train/accuracy=0.602320, train/bleu=28.641660, train/loss=2.052377, validation/accuracy=0.618938, validation/bleu=25.244115, validation/loss=1.920694, validation/num_examples=3000
I0208 09:01:41.966423 140277094741760 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2828308343887329, loss=2.115218162536621
I0208 09:02:16.847560 140277103134464 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3127157986164093, loss=1.9594107866287231
I0208 09:02:51.867648 140277094741760 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2696321904659271, loss=2.0279667377471924
I0208 09:03:26.867928 140277103134464 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2775196135044098, loss=2.0615055561065674
I0208 09:04:01.896606 140277094741760 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.2876615524291992, loss=2.010132074356079
I0208 09:04:36.940854 140277103134464 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2801649570465088, loss=2.0472307205200195
I0208 09:05:11.968757 140277094741760 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.2951778471469879, loss=2.0304152965545654
I0208 09:05:46.976972 140277103134464 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.26985353231430054, loss=2.037917137145996
I0208 09:06:22.010856 140277094741760 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.271275132894516, loss=1.9695180654525757
I0208 09:06:57.048978 140277103134464 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.2800338864326477, loss=2.0529160499572754
I0208 09:07:32.099729 140277094741760 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.31568819284439087, loss=2.090914726257324
I0208 09:08:07.175401 140277103134464 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2679443061351776, loss=2.060715913772583
I0208 09:08:42.233705 140277094741760 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3299311399459839, loss=2.0934009552001953
I0208 09:09:17.266129 140277103134464 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2956576645374298, loss=1.9886467456817627
I0208 09:09:52.298647 140277094741760 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.27844518423080444, loss=2.038989782333374
I0208 09:10:27.343548 140277103134464 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3264033794403076, loss=2.003462791442871
I0208 09:11:02.365159 140277094741760 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.28713521361351013, loss=1.9730169773101807
I0208 09:11:37.410123 140277103134464 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.33737313747406006, loss=2.0053510665893555
I0208 09:12:12.484632 140277094741760 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.29096415638923645, loss=2.0386366844177246
I0208 09:12:47.618386 140277103134464 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.28857699036598206, loss=2.0429763793945312
I0208 09:13:22.669047 140277094741760 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.32216164469718933, loss=1.955725908279419
I0208 09:13:57.699367 140277103134464 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.29603442549705505, loss=1.9654687643051147
I0208 09:14:32.750600 140277094741760 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.34364792704582214, loss=1.998552680015564
I0208 09:15:07.786886 140277103134464 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2664110064506531, loss=1.8822740316390991
I0208 09:15:36.211092 140446903760704 spec.py:321] Evaluating on the training split.
I0208 09:15:39.226781 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 09:18:07.482156 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 09:18:10.198007 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 09:20:29.155951 140446903760704 spec.py:349] Evaluating on the test split.
I0208 09:20:31.864511 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 09:22:36.157709 140446903760704 submission_runner.py:408] Time since start: 9170.39s, 	Step: 14383, 	{'train/accuracy': 0.6155896186828613, 'train/loss': 1.9335036277770996, 'train/bleu': 29.66881520325619, 'validation/accuracy': 0.629217267036438, 'validation/loss': 1.829842209815979, 'validation/bleu': 26.167779487244733, 'validation/num_examples': 3000, 'test/accuracy': 0.6378014087677002, 'test/loss': 1.7801294326782227, 'test/bleu': 25.51934415460533, 'test/num_examples': 3003, 'score': 5067.965177059174, 'total_duration': 9170.390238761902, 'accumulated_submission_time': 5067.965177059174, 'accumulated_eval_time': 4101.813674926758, 'accumulated_logging_time': 0.15631937980651855}
I0208 09:22:36.174813 140277094741760 logging_writer.py:48] [14383] accumulated_eval_time=4101.813675, accumulated_logging_time=0.156319, accumulated_submission_time=5067.965177, global_step=14383, preemption_count=0, score=5067.965177, test/accuracy=0.637801, test/bleu=25.519344, test/loss=1.780129, test/num_examples=3003, total_duration=9170.390239, train/accuracy=0.615590, train/bleu=29.668815, train/loss=1.933504, validation/accuracy=0.629217, validation/bleu=26.167779, validation/loss=1.829842, validation/num_examples=3000
I0208 09:22:42.469242 140277103134464 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.3077770173549652, loss=1.9788713455200195
I0208 09:23:17.360010 140277094741760 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.2943536639213562, loss=1.9434523582458496
I0208 09:23:52.391049 140277103134464 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3306138217449188, loss=1.9819222688674927
I0208 09:24:27.454119 140277094741760 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2850705683231354, loss=1.9165855646133423
I0208 09:25:02.503954 140277103134464 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.27670466899871826, loss=1.970301628112793
I0208 09:25:37.575854 140277094741760 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3354099690914154, loss=1.9749330282211304
I0208 09:26:12.602053 140277103134464 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3649299144744873, loss=1.880194902420044
I0208 09:26:47.653051 140277094741760 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.27917835116386414, loss=1.903617262840271
I0208 09:27:22.725849 140277103134464 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.29955846071243286, loss=2.0745303630828857
I0208 09:27:57.785386 140277094741760 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3179536461830139, loss=1.9831552505493164
I0208 09:28:32.834600 140277103134464 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3510373830795288, loss=1.8547908067703247
I0208 09:29:07.895763 140277094741760 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.32101234793663025, loss=1.9911953210830688
I0208 09:29:42.933973 140277103134464 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2930888235569, loss=1.990986704826355
I0208 09:30:18.001013 140277094741760 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.36924225091934204, loss=1.891566514968872
I0208 09:30:53.068218 140277103134464 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.32197266817092896, loss=1.9635852575302124
I0208 09:31:28.136541 140277094741760 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.27215540409088135, loss=2.0161561965942383
I0208 09:32:03.206135 140277103134464 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.34141072630882263, loss=1.9538280963897705
I0208 09:32:38.243855 140277094741760 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2979971766471863, loss=2.0133047103881836
I0208 09:33:13.312667 140277103134464 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.30853208899497986, loss=1.870392084121704
I0208 09:33:48.342436 140277094741760 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.28812193870544434, loss=1.8975391387939453
I0208 09:34:23.429615 140277103134464 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.3446184992790222, loss=1.9081709384918213
I0208 09:34:58.509731 140277094741760 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.36085042357444763, loss=1.9330902099609375
I0208 09:35:33.607006 140277103134464 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.29630157351493835, loss=1.8969135284423828
I0208 09:36:08.722964 140277094741760 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.32188794016838074, loss=1.8573393821716309
I0208 09:36:36.494410 140446903760704 spec.py:321] Evaluating on the training split.
I0208 09:36:39.506047 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 09:39:19.601150 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 09:39:22.311549 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 09:41:52.074350 140446903760704 spec.py:349] Evaluating on the test split.
I0208 09:41:54.787303 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 09:44:17.229766 140446903760704 submission_runner.py:408] Time since start: 10471.46s, 	Step: 16781, 	{'train/accuracy': 0.6175857782363892, 'train/loss': 1.9224374294281006, 'train/bleu': 30.239096733865786, 'validation/accuracy': 0.6388513445854187, 'validation/loss': 1.7686958312988281, 'validation/bleu': 26.848252135974953, 'validation/num_examples': 3000, 'test/accuracy': 0.6479228734970093, 'test/loss': 1.7082493305206299, 'test/bleu': 26.094720521677864, 'test/num_examples': 3003, 'score': 5908.19517326355, 'total_duration': 10471.462261676788, 'accumulated_submission_time': 5908.19517326355, 'accumulated_eval_time': 4562.548948049545, 'accumulated_logging_time': 0.185197114944458}
I0208 09:44:17.248858 140277103134464 logging_writer.py:48] [16781] accumulated_eval_time=4562.548948, accumulated_logging_time=0.185197, accumulated_submission_time=5908.195173, global_step=16781, preemption_count=0, score=5908.195173, test/accuracy=0.647923, test/bleu=26.094721, test/loss=1.708249, test/num_examples=3003, total_duration=10471.462262, train/accuracy=0.617586, train/bleu=30.239097, train/loss=1.922437, validation/accuracy=0.638851, validation/bleu=26.848252, validation/loss=1.768696, validation/num_examples=3000
I0208 09:44:24.237801 140277094741760 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3514723777770996, loss=1.9555131196975708
I0208 09:44:59.128643 140277103134464 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.32163408398628235, loss=1.8826825618743896
I0208 09:45:34.101775 140277094741760 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.44069811701774597, loss=1.9861348867416382
I0208 09:46:09.108309 140277103134464 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.32321614027023315, loss=1.945421814918518
I0208 09:46:44.093692 140277094741760 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.33534878492355347, loss=1.8525886535644531
I0208 09:47:19.139305 140277103134464 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.36233943700790405, loss=1.942207932472229
I0208 09:47:54.163196 140277094741760 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.4127389192581177, loss=1.8610596656799316
I0208 09:48:29.168204 140277103134464 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.356705904006958, loss=1.8909960985183716
I0208 09:49:04.201410 140277094741760 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.32270246744155884, loss=1.9494290351867676
I0208 09:49:39.214185 140277103134464 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3521306812763214, loss=1.8283332586288452
I0208 09:50:14.237860 140277094741760 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.46880432963371277, loss=1.9192808866500854
I0208 09:50:49.265005 140277103134464 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.34062275290489197, loss=1.8944623470306396
I0208 09:51:24.288356 140277094741760 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3320593535900116, loss=1.9019380807876587
I0208 09:51:59.306119 140277103134464 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3292328119277954, loss=1.9185007810592651
I0208 09:52:34.333942 140277094741760 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5317129492759705, loss=1.8995782136917114
I0208 09:53:09.357531 140277103134464 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.37374675273895264, loss=1.8764135837554932
I0208 09:53:44.394884 140277094741760 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.30747589468955994, loss=1.8675307035446167
I0208 09:54:19.451691 140277103134464 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.39384713768959045, loss=1.9041507244110107
I0208 09:54:54.464217 140277094741760 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3720667064189911, loss=1.898275375366211
I0208 09:55:29.494020 140277103134464 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.4529639184474945, loss=1.8741642236709595
I0208 09:56:04.534642 140277094741760 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.42545363306999207, loss=1.904771327972412
I0208 09:56:39.558740 140277103134464 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.42368394136428833, loss=1.8140296936035156
I0208 09:57:14.595692 140277094741760 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3591935336589813, loss=1.9015834331512451
I0208 09:57:49.636325 140277103134464 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.40282708406448364, loss=1.8961845636367798
I0208 09:58:17.366289 140446903760704 spec.py:321] Evaluating on the training split.
I0208 09:58:20.373950 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:02:24.185261 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 10:02:26.888175 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:05:28.195637 140446903760704 spec.py:349] Evaluating on the test split.
I0208 10:05:30.910382 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:08:12.788391 140446903760704 submission_runner.py:408] Time since start: 11907.02s, 	Step: 19181, 	{'train/accuracy': 0.6363543272018433, 'train/loss': 1.7792028188705444, 'train/bleu': 31.287817954743222, 'validation/accuracy': 0.6457824110984802, 'validation/loss': 1.7178845405578613, 'validation/bleu': 27.271873368897552, 'validation/num_examples': 3000, 'test/accuracy': 0.6533612608909607, 'test/loss': 1.6571162939071655, 'test/bleu': 26.776162031607065, 'test/num_examples': 3003, 'score': 6748.225657224655, 'total_duration': 11907.020893573761, 'accumulated_submission_time': 6748.225657224655, 'accumulated_eval_time': 5157.970972537994, 'accumulated_logging_time': 0.2151799201965332}
I0208 10:08:12.809517 140277094741760 logging_writer.py:48] [19181] accumulated_eval_time=5157.970973, accumulated_logging_time=0.215180, accumulated_submission_time=6748.225657, global_step=19181, preemption_count=0, score=6748.225657, test/accuracy=0.653361, test/bleu=26.776162, test/loss=1.657116, test/num_examples=3003, total_duration=11907.020894, train/accuracy=0.636354, train/bleu=31.287818, train/loss=1.779203, validation/accuracy=0.645782, validation/bleu=27.271873, validation/loss=1.717885, validation/num_examples=3000
I0208 10:08:19.804189 140277103134464 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3547488749027252, loss=1.8322594165802002
I0208 10:08:54.659356 140277094741760 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.34263110160827637, loss=1.8457082509994507
I0208 10:09:29.618758 140277103134464 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3386116027832031, loss=1.9409306049346924
I0208 10:10:04.614030 140277094741760 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.40727129578590393, loss=1.9415088891983032
I0208 10:10:39.617877 140277103134464 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3352693021297455, loss=1.872372031211853
I0208 10:11:14.624056 140277094741760 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.35767191648483276, loss=1.980665683746338
I0208 10:11:49.647331 140277103134464 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3653908371925354, loss=1.8528072834014893
I0208 10:12:24.654345 140277094741760 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.35927629470825195, loss=1.8838834762573242
I0208 10:12:59.659278 140277103134464 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.47031956911087036, loss=1.9349310398101807
I0208 10:13:34.662163 140277094741760 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.404548704624176, loss=1.8082820177078247
I0208 10:14:09.690356 140277103134464 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.40750133991241455, loss=1.8904480934143066
I0208 10:14:44.707113 140277094741760 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.3735358417034149, loss=1.9069246053695679
I0208 10:15:19.726790 140277103134464 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.4225713014602661, loss=1.983614444732666
I0208 10:15:54.737217 140277094741760 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.40322643518447876, loss=1.8540178537368774
I0208 10:16:29.775563 140277103134464 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.38841670751571655, loss=1.8934431076049805
I0208 10:17:04.808404 140277094741760 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3455450236797333, loss=1.8319787979125977
I0208 10:17:39.874990 140277103134464 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.3894469439983368, loss=1.8672837018966675
I0208 10:18:14.910237 140277094741760 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3877573311328888, loss=1.7817283868789673
I0208 10:18:49.916301 140277103134464 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.4201163947582245, loss=1.863787293434143
I0208 10:19:24.952342 140277094741760 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.46448466181755066, loss=1.935920000076294
I0208 10:19:59.972755 140277103134464 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4029790461063385, loss=1.9185079336166382
I0208 10:20:35.009210 140277094741760 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.4491082429885864, loss=1.8935409784317017
I0208 10:21:10.009434 140277103134464 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.36525774002075195, loss=1.9128400087356567
I0208 10:21:45.019408 140277094741760 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.4918031394481659, loss=1.9442975521087646
I0208 10:22:13.123394 140446903760704 spec.py:321] Evaluating on the training split.
I0208 10:22:16.143083 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:26:48.378264 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 10:26:51.107964 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:29:34.927371 140446903760704 spec.py:349] Evaluating on the test split.
I0208 10:29:37.632355 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:31:57.508155 140446903760704 submission_runner.py:408] Time since start: 13331.74s, 	Step: 21582, 	{'train/accuracy': 0.6294018626213074, 'train/loss': 1.8236947059631348, 'train/bleu': 30.38335666819069, 'validation/accuracy': 0.6469727754592896, 'validation/loss': 1.6930538415908813, 'validation/bleu': 27.204733443076528, 'validation/num_examples': 3000, 'test/accuracy': 0.6578351259231567, 'test/loss': 1.6361119747161865, 'test/bleu': 26.86156974459988, 'test/num_examples': 3003, 'score': 7588.451881408691, 'total_duration': 13331.740639448166, 'accumulated_submission_time': 7588.451881408691, 'accumulated_eval_time': 5742.355647802353, 'accumulated_logging_time': 0.24896502494812012}
I0208 10:31:57.526478 140277103134464 logging_writer.py:48] [21582] accumulated_eval_time=5742.355648, accumulated_logging_time=0.248965, accumulated_submission_time=7588.451881, global_step=21582, preemption_count=0, score=7588.451881, test/accuracy=0.657835, test/bleu=26.861570, test/loss=1.636112, test/num_examples=3003, total_duration=13331.740639, train/accuracy=0.629402, train/bleu=30.383357, train/loss=1.823695, validation/accuracy=0.646973, validation/bleu=27.204733, validation/loss=1.693054, validation/num_examples=3000
I0208 10:32:04.159716 140277094741760 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.45929470658302307, loss=1.7925257682800293
I0208 10:32:39.030940 140277103134464 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.4149165153503418, loss=1.7875174283981323
I0208 10:33:13.994752 140277094741760 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.36266905069351196, loss=1.8297648429870605
I0208 10:33:49.049435 140277103134464 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5147303342819214, loss=1.932273507118225
I0208 10:34:24.083335 140277094741760 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4728383421897888, loss=1.8379310369491577
I0208 10:34:59.215611 140277103134464 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.3831668496131897, loss=1.9496252536773682
I0208 10:35:34.276243 140277094741760 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.44614315032958984, loss=1.9032347202301025
I0208 10:36:09.287716 140277103134464 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3968738615512848, loss=1.7938740253448486
I0208 10:36:44.296666 140277094741760 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.5714487433433533, loss=1.8134502172470093
I0208 10:37:19.327830 140277103134464 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.42085281014442444, loss=1.8261404037475586
I0208 10:37:54.328361 140277094741760 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.34356585144996643, loss=1.806061863899231
I0208 10:38:29.343017 140277103134464 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.5494740605354309, loss=1.942908525466919
I0208 10:39:04.368184 140277094741760 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5243982076644897, loss=1.8326694965362549
I0208 10:39:39.352477 140277103134464 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.4054275453090668, loss=1.8450313806533813
I0208 10:40:14.370806 140277094741760 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.3830743134021759, loss=1.8378583192825317
I0208 10:40:49.485754 140277103134464 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.4079427421092987, loss=1.8579365015029907
I0208 10:41:24.507338 140277094741760 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3891468346118927, loss=1.866256833076477
I0208 10:41:59.512901 140277103134464 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.4915563762187958, loss=1.8932853937149048
I0208 10:42:34.531602 140277094741760 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3670823574066162, loss=1.9134453535079956
I0208 10:43:09.531975 140277103134464 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.38158419728279114, loss=1.7668946981430054
I0208 10:43:44.524546 140277094741760 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5564275979995728, loss=1.9331445693969727
I0208 10:44:19.529731 140277103134464 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.45132237672805786, loss=1.8908779621124268
I0208 10:44:54.551238 140277094741760 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.3837626278400421, loss=1.8249897956848145
I0208 10:45:29.560987 140277103134464 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.404028981924057, loss=1.7885347604751587
I0208 10:45:57.617797 140446903760704 spec.py:321] Evaluating on the training split.
I0208 10:46:00.628550 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:49:17.396232 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 10:49:20.102758 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:51:43.673909 140446903760704 spec.py:349] Evaluating on the test split.
I0208 10:51:46.381595 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 10:54:08.524819 140446903760704 submission_runner.py:408] Time since start: 14662.76s, 	Step: 23982, 	{'train/accuracy': 0.6340451240539551, 'train/loss': 1.8066679239273071, 'train/bleu': 30.86761748790029, 'validation/accuracy': 0.6483242511749268, 'validation/loss': 1.6817162036895752, 'validation/bleu': 27.5362496664734, 'validation/num_examples': 3000, 'test/accuracy': 0.6590552926063538, 'test/loss': 1.6155636310577393, 'test/bleu': 26.74998812857728, 'test/num_examples': 3003, 'score': 8428.454867362976, 'total_duration': 14662.757348537445, 'accumulated_submission_time': 8428.454867362976, 'accumulated_eval_time': 6233.2626214027405, 'accumulated_logging_time': 0.27758240699768066}
I0208 10:54:08.543240 140277094741760 logging_writer.py:48] [23982] accumulated_eval_time=6233.262621, accumulated_logging_time=0.277582, accumulated_submission_time=8428.454867, global_step=23982, preemption_count=0, score=8428.454867, test/accuracy=0.659055, test/bleu=26.749988, test/loss=1.615564, test/num_examples=3003, total_duration=14662.757349, train/accuracy=0.634045, train/bleu=30.867617, train/loss=1.806668, validation/accuracy=0.648324, validation/bleu=27.536250, validation/loss=1.681716, validation/num_examples=3000
I0208 10:54:15.177663 140277103134464 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.38018476963043213, loss=1.791958212852478
I0208 10:54:50.096025 140277094741760 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.4158104956150055, loss=1.815709114074707
I0208 10:55:25.106160 140277103134464 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.4261610507965088, loss=1.8464882373809814
I0208 10:56:00.106176 140277094741760 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.4507732689380646, loss=1.8112906217575073
I0208 10:56:35.120514 140277103134464 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.36408767104148865, loss=1.8015103340148926
I0208 10:57:10.163806 140277094741760 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3962685167789459, loss=1.7851698398590088
I0208 10:57:45.209532 140277103134464 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.3521806299686432, loss=1.749943733215332
I0208 10:58:20.238019 140277094741760 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.43517133593559265, loss=1.806723952293396
I0208 10:58:55.220243 140277103134464 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.38984572887420654, loss=1.7984132766723633
I0208 10:59:30.219049 140277094741760 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3789422810077667, loss=1.8521146774291992
I0208 11:00:05.232493 140277103134464 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.40058383345603943, loss=1.8446769714355469
I0208 11:00:40.241895 140277094741760 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3792024850845337, loss=1.8138059377670288
I0208 11:01:15.250799 140277103134464 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.37424489855766296, loss=1.7617714405059814
I0208 11:01:50.272454 140277094741760 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.420504093170166, loss=1.9653595685958862
I0208 11:02:25.322188 140277103134464 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.4387933313846588, loss=1.847446084022522
I0208 11:03:00.359860 140277094741760 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.44756603240966797, loss=1.8352601528167725
I0208 11:03:35.372161 140277103134464 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.38883644342422485, loss=1.8233531713485718
I0208 11:04:10.370245 140277094741760 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.41077545285224915, loss=1.8274548053741455
I0208 11:04:45.407088 140277103134464 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4806644916534424, loss=1.9096825122833252
I0208 11:05:20.432863 140277094741760 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.45376816391944885, loss=1.8295235633850098
I0208 11:05:55.422126 140277103134464 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5246254801750183, loss=1.7801340818405151
I0208 11:06:30.416503 140277094741760 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.37897005677223206, loss=1.8147218227386475
I0208 11:07:05.405547 140277103134464 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.4504839777946472, loss=1.8250744342803955
I0208 11:07:40.416738 140277094741760 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.40789180994033813, loss=1.8136208057403564
I0208 11:08:08.867885 140446903760704 spec.py:321] Evaluating on the training split.
I0208 11:08:11.873632 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:12:12.257632 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 11:12:14.973696 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:14:48.868146 140446903760704 spec.py:349] Evaluating on the test split.
I0208 11:14:51.581835 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:16:59.801221 140446903760704 submission_runner.py:408] Time since start: 16034.03s, 	Step: 26383, 	{'train/accuracy': 0.6357397437095642, 'train/loss': 1.7810266017913818, 'train/bleu': 30.912190908003147, 'validation/accuracy': 0.6519448161125183, 'validation/loss': 1.6675649881362915, 'validation/bleu': 27.51485381411525, 'validation/num_examples': 3000, 'test/accuracy': 0.6617047190666199, 'test/loss': 1.606836199760437, 'test/bleu': 26.932696320278875, 'test/num_examples': 3003, 'score': 9268.692746162415, 'total_duration': 16034.033752679825, 'accumulated_submission_time': 9268.692746162415, 'accumulated_eval_time': 6764.1959137916565, 'accumulated_logging_time': 0.3061375617980957}
I0208 11:16:59.819920 140277103134464 logging_writer.py:48] [26383] accumulated_eval_time=6764.195914, accumulated_logging_time=0.306138, accumulated_submission_time=9268.692746, global_step=26383, preemption_count=0, score=9268.692746, test/accuracy=0.661705, test/bleu=26.932696, test/loss=1.606836, test/num_examples=3003, total_duration=16034.033753, train/accuracy=0.635740, train/bleu=30.912191, train/loss=1.781027, validation/accuracy=0.651945, validation/bleu=27.514854, validation/loss=1.667565, validation/num_examples=3000
I0208 11:17:06.103382 140277094741760 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.3993000090122223, loss=1.835130214691162
I0208 11:17:40.972404 140277103134464 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.4066561758518219, loss=1.7735213041305542
I0208 11:18:15.942641 140277094741760 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5991916656494141, loss=1.7615547180175781
I0208 11:18:50.926298 140277103134464 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.42989382147789, loss=1.8498926162719727
I0208 11:19:25.908452 140277094741760 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.45788103342056274, loss=1.8751683235168457
I0208 11:20:00.897333 140277103134464 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.45104408264160156, loss=1.8219047784805298
I0208 11:20:35.896389 140277094741760 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.37145328521728516, loss=1.7855738401412964
I0208 11:21:10.965214 140277103134464 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.42426183819770813, loss=1.774746060371399
I0208 11:21:45.997266 140277094741760 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.4330004155635834, loss=1.78509521484375
I0208 11:22:21.006134 140277103134464 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.42534157633781433, loss=1.7366260290145874
I0208 11:22:56.060795 140277094741760 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.41260412335395813, loss=1.803130030632019
I0208 11:23:31.095600 140277103134464 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.40771591663360596, loss=1.799682855606079
I0208 11:24:06.101100 140277094741760 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.5934062004089355, loss=1.8275647163391113
I0208 11:24:41.102772 140277103134464 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4115290641784668, loss=1.810314655303955
I0208 11:25:16.148632 140277094741760 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.7724260687828064, loss=1.8510726690292358
I0208 11:25:51.153901 140277103134464 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.8521705269813538, loss=1.7573821544647217
I0208 11:26:26.166912 140277094741760 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4407232999801636, loss=1.9540077447891235
I0208 11:27:01.159690 140277103134464 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.43611863255500793, loss=1.7318633794784546
I0208 11:27:36.320970 140277094741760 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.43785059452056885, loss=1.7799084186553955
I0208 11:28:11.344804 140277103134464 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.4577000141143799, loss=1.7969856262207031
I0208 11:28:46.338178 140277094741760 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.4265846908092499, loss=1.7775652408599854
I0208 11:29:21.345040 140277103134464 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.4888443350791931, loss=1.7404497861862183
I0208 11:29:56.376596 140277094741760 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.4387557804584503, loss=1.9135658740997314
I0208 11:30:31.404732 140277103134464 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.4015442132949829, loss=1.8106437921524048
I0208 11:30:59.819503 140446903760704 spec.py:321] Evaluating on the training split.
I0208 11:31:02.858758 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:34:07.297174 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 11:34:10.017954 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:36:42.702409 140446903760704 spec.py:349] Evaluating on the test split.
I0208 11:36:45.429817 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:39:12.919346 140446903760704 submission_runner.py:408] Time since start: 17367.15s, 	Step: 28783, 	{'train/accuracy': 0.6305766105651855, 'train/loss': 1.8134877681732178, 'train/bleu': 30.657916208514465, 'validation/accuracy': 0.6531599164009094, 'validation/loss': 1.6515889167785645, 'validation/bleu': 27.818056014519808, 'validation/num_examples': 3000, 'test/accuracy': 0.6621811985969543, 'test/loss': 1.5930308103561401, 'test/bleu': 26.89210431803698, 'test/num_examples': 3003, 'score': 10108.606403827667, 'total_duration': 17367.151869773865, 'accumulated_submission_time': 10108.606403827667, 'accumulated_eval_time': 7257.295704364777, 'accumulated_logging_time': 0.3352222442626953}
I0208 11:39:12.938372 140277094741760 logging_writer.py:48] [28783] accumulated_eval_time=7257.295704, accumulated_logging_time=0.335222, accumulated_submission_time=10108.606404, global_step=28783, preemption_count=0, score=10108.606404, test/accuracy=0.662181, test/bleu=26.892104, test/loss=1.593031, test/num_examples=3003, total_duration=17367.151870, train/accuracy=0.630577, train/bleu=30.657916, train/loss=1.813488, validation/accuracy=0.653160, validation/bleu=27.818056, validation/loss=1.651589, validation/num_examples=3000
I0208 11:39:19.215445 140277103134464 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.351057767868042, loss=1.8376270532608032
I0208 11:39:54.104901 140277094741760 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.3936862051486969, loss=1.9050776958465576
I0208 11:40:29.086236 140277103134464 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.37501105666160583, loss=1.7616405487060547
I0208 11:41:04.069994 140277094741760 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.46548131108283997, loss=1.778369426727295
I0208 11:41:39.104628 140277103134464 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4347168505191803, loss=1.8923698663711548
I0208 11:42:14.098296 140277094741760 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.4309288263320923, loss=1.8090596199035645
I0208 11:42:49.097606 140277103134464 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.4190395474433899, loss=1.9050649404525757
I0208 11:43:24.137143 140277094741760 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.45033103227615356, loss=1.7173367738723755
I0208 11:43:59.143396 140277103134464 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.38982081413269043, loss=1.7866102457046509
I0208 11:44:34.186876 140277094741760 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.40896880626678467, loss=1.8069857358932495
I0208 11:45:09.181282 140277103134464 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.46792036294937134, loss=1.809875249862671
I0208 11:45:44.185384 140277094741760 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.46511220932006836, loss=1.7780817747116089
I0208 11:46:19.229667 140277103134464 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.411933571100235, loss=1.8003674745559692
I0208 11:46:54.227281 140277094741760 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3999035060405731, loss=1.830694317817688
I0208 11:47:29.338630 140277103134464 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.41947275400161743, loss=1.768670678138733
I0208 11:48:04.347392 140277094741760 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.39847880601882935, loss=1.767174482345581
I0208 11:48:39.336702 140277103134464 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.4591011703014374, loss=1.850531816482544
I0208 11:49:14.323682 140277094741760 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.4051612615585327, loss=1.8237485885620117
I0208 11:49:49.310297 140277103134464 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.3687819838523865, loss=1.7366262674331665
I0208 11:50:24.307052 140277094741760 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.45335838198661804, loss=1.8261715173721313
I0208 11:50:59.296039 140277103134464 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.3851243853569031, loss=1.748468279838562
I0208 11:51:34.268868 140277094741760 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.5850111246109009, loss=1.7579190731048584
I0208 11:52:09.276068 140277103134464 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.39281028509140015, loss=1.7747366428375244
I0208 11:52:44.232366 140277094741760 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.373380571603775, loss=1.7828768491744995
I0208 11:53:12.989888 140446903760704 spec.py:321] Evaluating on the training split.
I0208 11:53:16.004865 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:56:18.358323 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 11:56:21.085029 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 11:58:51.655708 140446903760704 spec.py:349] Evaluating on the test split.
I0208 11:58:54.397179 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 12:01:20.205326 140446903760704 submission_runner.py:408] Time since start: 18694.44s, 	Step: 31184, 	{'train/accuracy': 0.6314585208892822, 'train/loss': 1.8155176639556885, 'train/bleu': 30.780932147538294, 'validation/accuracy': 0.6558877229690552, 'validation/loss': 1.641958475112915, 'validation/bleu': 27.954856722763047, 'validation/num_examples': 3000, 'test/accuracy': 0.6656208634376526, 'test/loss': 1.5788705348968506, 'test/bleu': 27.360726859055152, 'test/num_examples': 3003, 'score': 10948.571624994278, 'total_duration': 18694.437842607498, 'accumulated_submission_time': 10948.571624994278, 'accumulated_eval_time': 7744.511089324951, 'accumulated_logging_time': 0.3644375801086426}
I0208 12:01:20.225013 140277103134464 logging_writer.py:48] [31184] accumulated_eval_time=7744.511089, accumulated_logging_time=0.364438, accumulated_submission_time=10948.571625, global_step=31184, preemption_count=0, score=10948.571625, test/accuracy=0.665621, test/bleu=27.360727, test/loss=1.578871, test/num_examples=3003, total_duration=18694.437843, train/accuracy=0.631459, train/bleu=30.780932, train/loss=1.815518, validation/accuracy=0.655888, validation/bleu=27.954857, validation/loss=1.641958, validation/num_examples=3000
I0208 12:01:26.196508 140277094741760 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.3891676366329193, loss=1.7462451457977295
I0208 12:02:01.118259 140277103134464 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3907392621040344, loss=1.8297076225280762
I0208 12:02:36.097607 140277094741760 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.41937440633773804, loss=1.8078162670135498
I0208 12:03:11.133405 140277103134464 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.41126736998558044, loss=1.7471867799758911
I0208 12:03:46.138464 140277094741760 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.43035101890563965, loss=1.7556675672531128
I0208 12:04:21.222841 140277103134464 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4313948154449463, loss=1.8457695245742798
I0208 12:04:56.290933 140277094741760 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.42056217789649963, loss=1.8108676671981812
I0208 12:05:31.331989 140277103134464 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.38473421335220337, loss=1.7865155935287476
I0208 12:06:06.426205 140277094741760 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.47050952911376953, loss=1.7957688570022583
I0208 12:06:41.492587 140277103134464 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.39854690432548523, loss=1.806636929512024
I0208 12:07:16.525924 140277094741760 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.36379843950271606, loss=1.7897440195083618
I0208 12:07:51.540850 140277103134464 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3879844546318054, loss=1.710597038269043
I0208 12:08:26.573720 140277094741760 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.4710853695869446, loss=1.9177219867706299
I0208 12:09:01.615193 140277103134464 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3896239697933197, loss=1.7588849067687988
I0208 12:09:36.648385 140277094741760 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.4362088441848755, loss=1.747696876525879
I0208 12:10:11.716900 140277103134464 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.42784637212753296, loss=1.7797034978866577
I0208 12:10:46.750965 140277094741760 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.40397292375564575, loss=1.8818306922912598
I0208 12:11:21.784775 140277103134464 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.451337993144989, loss=1.7931519746780396
I0208 12:11:56.827248 140277094741760 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.38211411237716675, loss=1.8114290237426758
I0208 12:12:31.856403 140277103134464 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.4037506580352783, loss=1.8329267501831055
I0208 12:13:06.925807 140277094741760 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.43087127804756165, loss=1.8783687353134155
I0208 12:13:41.920787 140277103134464 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.3843037486076355, loss=1.7743443250656128
I0208 12:14:16.984677 140277094741760 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4288807809352875, loss=1.7835017442703247
I0208 12:14:51.983051 140277103134464 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.4802246391773224, loss=1.8800687789916992
I0208 12:15:20.408987 140446903760704 spec.py:321] Evaluating on the training split.
I0208 12:15:23.423399 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 12:19:01.864891 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 12:19:04.571561 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 12:21:30.639446 140446903760704 spec.py:349] Evaluating on the test split.
I0208 12:21:33.370459 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 12:24:13.953204 140446903760704 submission_runner.py:408] Time since start: 20068.19s, 	Step: 33583, 	{'train/accuracy': 0.6337587833404541, 'train/loss': 1.78791081905365, 'train/bleu': 31.308028349740688, 'validation/accuracy': 0.6552677750587463, 'validation/loss': 1.638217806816101, 'validation/bleu': 28.209447885022723, 'validation/num_examples': 3000, 'test/accuracy': 0.6670966148376465, 'test/loss': 1.5727591514587402, 'test/bleu': 27.57975050809596, 'test/num_examples': 3003, 'score': 11788.665621519089, 'total_duration': 20068.18571949005, 'accumulated_submission_time': 11788.665621519089, 'accumulated_eval_time': 8278.055241584778, 'accumulated_logging_time': 0.39438962936401367}
I0208 12:24:13.972654 140277094741760 logging_writer.py:48] [33583] accumulated_eval_time=8278.055242, accumulated_logging_time=0.394390, accumulated_submission_time=11788.665622, global_step=33583, preemption_count=0, score=11788.665622, test/accuracy=0.667097, test/bleu=27.579751, test/loss=1.572759, test/num_examples=3003, total_duration=20068.185719, train/accuracy=0.633759, train/bleu=31.308028, train/loss=1.787911, validation/accuracy=0.655268, validation/bleu=28.209448, validation/loss=1.638218, validation/num_examples=3000
I0208 12:24:20.252088 140277103134464 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.5567780137062073, loss=1.779134750366211
I0208 12:24:55.103682 140277094741760 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.3584526479244232, loss=1.7513824701309204
I0208 12:25:30.105033 140277103134464 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.3749855160713196, loss=1.7755415439605713
I0208 12:26:05.126409 140277094741760 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.42581960558891296, loss=1.8225955963134766
I0208 12:26:40.093073 140277103134464 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.43539708852767944, loss=1.7912966012954712
I0208 12:27:15.084519 140277094741760 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.4020473062992096, loss=1.7609975337982178
I0208 12:27:50.072782 140277103134464 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.43347397446632385, loss=1.8742188215255737
I0208 12:28:25.091975 140277094741760 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.4079758822917938, loss=1.8441098928451538
I0208 12:29:00.143211 140277103134464 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.4534938633441925, loss=1.7290892601013184
I0208 12:29:35.168607 140277094741760 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4010218679904938, loss=1.832318663597107
I0208 12:30:10.265237 140277103134464 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.41069427132606506, loss=1.7604269981384277
I0208 12:30:45.291967 140277094741760 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.4262726306915283, loss=1.747536540031433
I0208 12:31:20.296646 140277103134464 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.4512435793876648, loss=1.829512357711792
I0208 12:31:55.297500 140277094741760 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.4596398174762726, loss=1.8757340908050537
I0208 12:32:30.340028 140277103134464 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3808321952819824, loss=1.7673218250274658
I0208 12:33:05.380886 140277094741760 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.4095131754875183, loss=1.8145309686660767
I0208 12:33:40.434941 140277103134464 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3941056728363037, loss=1.8084796667099
I0208 12:34:15.470405 140277094741760 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.41183221340179443, loss=1.7616894245147705
I0208 12:34:50.455946 140277103134464 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.42756134271621704, loss=1.7709860801696777
I0208 12:35:25.467433 140277094741760 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3852360248565674, loss=1.8566449880599976
I0208 12:36:00.460806 140277103134464 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.47802457213401794, loss=1.7912533283233643
I0208 12:36:35.483864 140277094741760 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.41861769556999207, loss=1.8178036212921143
I0208 12:37:10.489173 140277103134464 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.4294697344303131, loss=1.7187552452087402
I0208 12:37:45.500559 140277094741760 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.41093945503234863, loss=1.7852376699447632
I0208 12:38:14.294025 140446903760704 spec.py:321] Evaluating on the training split.
I0208 12:38:17.325695 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 12:40:56.401384 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 12:40:59.136553 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 12:43:27.346741 140446903760704 spec.py:349] Evaluating on the test split.
I0208 12:43:30.053501 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 12:45:50.726326 140446903760704 submission_runner.py:408] Time since start: 21364.96s, 	Step: 35984, 	{'train/accuracy': 0.6375605463981628, 'train/loss': 1.7774264812469482, 'train/bleu': 30.86603327927146, 'validation/accuracy': 0.6572268009185791, 'validation/loss': 1.6223275661468506, 'validation/bleu': 28.12284049602379, 'validation/num_examples': 3000, 'test/accuracy': 0.6696415543556213, 'test/loss': 1.5568478107452393, 'test/bleu': 27.97125458605869, 'test/num_examples': 3003, 'score': 12628.895520687103, 'total_duration': 21364.958856105804, 'accumulated_submission_time': 12628.895520687103, 'accumulated_eval_time': 8734.487503767014, 'accumulated_logging_time': 0.42507266998291016}
I0208 12:45:50.746522 140277103134464 logging_writer.py:48] [35984] accumulated_eval_time=8734.487504, accumulated_logging_time=0.425073, accumulated_submission_time=12628.895521, global_step=35984, preemption_count=0, score=12628.895521, test/accuracy=0.669642, test/bleu=27.971255, test/loss=1.556848, test/num_examples=3003, total_duration=21364.958856, train/accuracy=0.637561, train/bleu=30.866033, train/loss=1.777426, validation/accuracy=0.657227, validation/bleu=28.122840, validation/loss=1.622328, validation/num_examples=3000
I0208 12:45:56.679854 140277094741760 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.38972780108451843, loss=1.7301299571990967
I0208 12:46:31.527573 140277103134464 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.48318225145339966, loss=1.8377372026443481
I0208 12:47:06.491308 140277094741760 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.46942877769470215, loss=1.764522671699524
I0208 12:47:41.456722 140277103134464 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.40950462222099304, loss=1.7587932348251343
I0208 12:48:16.463397 140277094741760 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3834376931190491, loss=1.7602359056472778
I0208 12:48:51.468054 140277103134464 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.4386613368988037, loss=1.8354133367538452
I0208 12:49:26.477597 140277094741760 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.526387095451355, loss=1.7650245428085327
I0208 12:50:01.537355 140277103134464 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.5218617916107178, loss=1.7441060543060303
I0208 12:50:36.613842 140277094741760 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.46531838178634644, loss=1.7527722120285034
I0208 12:51:11.647902 140277103134464 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.4561503827571869, loss=1.846011996269226
I0208 12:51:46.703221 140277094741760 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.4363235533237457, loss=1.8152052164077759
I0208 12:52:21.715452 140277103134464 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.39545485377311707, loss=1.6806328296661377
I0208 12:52:56.705177 140277094741760 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.4325823187828064, loss=1.7959837913513184
I0208 12:53:31.695036 140277103134464 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4379563629627228, loss=1.770701289176941
I0208 12:54:06.701050 140277094741760 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3812671899795532, loss=1.804127812385559
I0208 12:54:41.673275 140277103134464 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.42002540826797485, loss=1.830305814743042
I0208 12:55:16.680636 140277094741760 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.40211471915245056, loss=1.767377257347107
I0208 12:55:51.674404 140277103134464 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.4035189151763916, loss=1.7724486589431763
I0208 12:56:26.684194 140277094741760 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.41774874925613403, loss=1.8116647005081177
I0208 12:57:01.725010 140277103134464 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.4128471612930298, loss=1.841206431388855
I0208 12:57:36.773735 140277094741760 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3594256341457367, loss=1.790449619293213
I0208 12:58:11.786909 140277103134464 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.44375452399253845, loss=1.7972702980041504
I0208 12:58:46.813499 140277094741760 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.42030903697013855, loss=1.7570701837539673
I0208 12:59:21.851844 140277103134464 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.4063330888748169, loss=1.7924386262893677
I0208 12:59:50.966928 140446903760704 spec.py:321] Evaluating on the training split.
I0208 12:59:53.979813 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:02:58.895193 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 13:03:01.621535 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:05:39.602331 140446903760704 spec.py:349] Evaluating on the test split.
I0208 13:05:42.305428 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:08:19.364403 140446903760704 submission_runner.py:408] Time since start: 22713.60s, 	Step: 38385, 	{'train/accuracy': 0.6480917930603027, 'train/loss': 1.692330002784729, 'train/bleu': 31.930142130136353, 'validation/accuracy': 0.6595950126647949, 'validation/loss': 1.6177055835723877, 'validation/bleu': 28.111084985560286, 'validation/num_examples': 3000, 'test/accuracy': 0.669780969619751, 'test/loss': 1.5530155897140503, 'test/bleu': 27.636952264917564, 'test/num_examples': 3003, 'score': 13469.028195142746, 'total_duration': 22713.596930027008, 'accumulated_submission_time': 13469.028195142746, 'accumulated_eval_time': 9242.88492488861, 'accumulated_logging_time': 0.45506882667541504}
I0208 13:08:19.384843 140277094741760 logging_writer.py:48] [38385] accumulated_eval_time=9242.884925, accumulated_logging_time=0.455069, accumulated_submission_time=13469.028195, global_step=38385, preemption_count=0, score=13469.028195, test/accuracy=0.669781, test/bleu=27.636952, test/loss=1.553016, test/num_examples=3003, total_duration=22713.596930, train/accuracy=0.648092, train/bleu=31.930142, train/loss=1.692330, validation/accuracy=0.659595, validation/bleu=28.111085, validation/loss=1.617706, validation/num_examples=3000
I0208 13:08:24.977480 140277103134464 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.42402929067611694, loss=1.7599563598632812
I0208 13:08:59.817456 140277094741760 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.4255254864692688, loss=1.7769591808319092
I0208 13:09:34.798404 140277103134464 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.44622859358787537, loss=1.837105631828308
I0208 13:10:09.816297 140277094741760 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.4412076771259308, loss=1.6994868516921997
I0208 13:10:44.831750 140277103134464 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.41739049553871155, loss=1.7473065853118896
I0208 13:11:19.849740 140277094741760 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.38370275497436523, loss=1.7312819957733154
I0208 13:11:54.839517 140277103134464 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3846421539783478, loss=1.8135578632354736
I0208 13:12:29.828326 140277094741760 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.44406938552856445, loss=1.7298136949539185
I0208 13:13:04.846874 140277103134464 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.39225050806999207, loss=1.8334256410598755
I0208 13:13:39.798702 140277094741760 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.4306943714618683, loss=1.7707887887954712
I0208 13:14:14.791407 140277103134464 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4127458930015564, loss=1.791593074798584
I0208 13:14:49.792000 140277094741760 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.42041030526161194, loss=1.7297002077102661
I0208 13:15:24.784580 140277103134464 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3958306908607483, loss=1.7713096141815186
I0208 13:15:59.775535 140277094741760 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.3632641136646271, loss=1.7397669553756714
I0208 13:16:34.777477 140277103134464 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.434560626745224, loss=1.7913347482681274
I0208 13:17:09.760923 140277094741760 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.44519683718681335, loss=1.745315432548523
I0208 13:17:44.741848 140277103134464 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.39806926250457764, loss=1.8133639097213745
I0208 13:18:19.740952 140277094741760 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.37991416454315186, loss=1.7171756029129028
I0208 13:18:54.723814 140277103134464 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3868043124675751, loss=1.7181591987609863
I0208 13:19:29.694417 140277094741760 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.4059862792491913, loss=1.764725923538208
I0208 13:20:04.676485 140277103134464 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.41409844160079956, loss=1.7586238384246826
I0208 13:20:39.642099 140277094741760 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.4741879403591156, loss=1.68827486038208
I0208 13:21:14.637796 140277103134464 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3911149203777313, loss=1.8032833337783813
I0208 13:21:49.669807 140277094741760 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.38712167739868164, loss=1.7525309324264526
I0208 13:22:19.475910 140446903760704 spec.py:321] Evaluating on the training split.
I0208 13:22:22.492641 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:25:30.695981 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 13:25:33.394607 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:28:29.423943 140446903760704 spec.py:349] Evaluating on the test split.
I0208 13:28:32.129756 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:31:04.951073 140446903760704 submission_runner.py:408] Time since start: 24079.18s, 	Step: 40787, 	{'train/accuracy': 0.6398926973342896, 'train/loss': 1.7486668825149536, 'train/bleu': 31.34876197331421, 'validation/accuracy': 0.6608969569206238, 'validation/loss': 1.6113759279251099, 'validation/bleu': 28.497071212994896, 'validation/num_examples': 3000, 'test/accuracy': 0.6694207191467285, 'test/loss': 1.5531401634216309, 'test/bleu': 27.761626027709138, 'test/num_examples': 3003, 'score': 14309.030643939972, 'total_duration': 24079.183569192886, 'accumulated_submission_time': 14309.030643939972, 'accumulated_eval_time': 9768.36000418663, 'accumulated_logging_time': 0.48691678047180176}
I0208 13:31:04.975715 140277103134464 logging_writer.py:48] [40787] accumulated_eval_time=9768.360004, accumulated_logging_time=0.486917, accumulated_submission_time=14309.030644, global_step=40787, preemption_count=0, score=14309.030644, test/accuracy=0.669421, test/bleu=27.761626, test/loss=1.553140, test/num_examples=3003, total_duration=24079.183569, train/accuracy=0.639893, train/bleu=31.348762, train/loss=1.748667, validation/accuracy=0.660897, validation/bleu=28.497071, validation/loss=1.611376, validation/num_examples=3000
I0208 13:31:09.864617 140277094741760 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.5016986727714539, loss=1.6966116428375244
I0208 13:31:44.693978 140277103134464 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3855031728744507, loss=1.7807140350341797
I0208 13:32:19.634704 140277094741760 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.40793055295944214, loss=1.7690895795822144
I0208 13:32:54.588795 140277103134464 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.4285729229450226, loss=1.7973366975784302
I0208 13:33:29.571350 140277094741760 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.39693284034729004, loss=1.7718262672424316
I0208 13:34:04.573377 140277103134464 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.40904757380485535, loss=1.7320090532302856
I0208 13:34:39.553554 140277094741760 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.44343605637550354, loss=1.6488670110702515
I0208 13:35:14.558339 140277103134464 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3731735348701477, loss=1.72127366065979
I0208 13:35:49.536183 140277094741760 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.4482885003089905, loss=1.7992990016937256
I0208 13:36:24.535065 140277103134464 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.45951059460639954, loss=1.78374183177948
I0208 13:36:59.515306 140277094741760 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.4213561713695526, loss=1.7266478538513184
I0208 13:37:34.482224 140277103134464 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3967211842536926, loss=1.775688886642456
I0208 13:38:09.476048 140277094741760 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.3932979106903076, loss=1.7470496892929077
I0208 13:38:44.477748 140277103134464 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.4010162353515625, loss=1.7338467836380005
I0208 13:39:19.479434 140277094741760 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4120781719684601, loss=1.8863722085952759
I0208 13:39:54.462509 140277103134464 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.4306944012641907, loss=1.7996071577072144
I0208 13:40:29.446729 140277094741760 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.43675801157951355, loss=1.724876880645752
I0208 13:41:04.425485 140277103134464 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.4059610962867737, loss=1.7840038537979126
I0208 13:41:39.434245 140277094741760 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.411310613155365, loss=1.733489990234375
I0208 13:42:14.455191 140277103134464 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.37176012992858887, loss=1.819146990776062
I0208 13:42:49.457738 140277094741760 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.39268171787261963, loss=1.7720460891723633
I0208 13:43:24.478977 140277103134464 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3747539818286896, loss=1.7195342779159546
I0208 13:43:59.461965 140277094741760 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3570774495601654, loss=1.66268789768219
I0208 13:44:34.444028 140277103134464 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.5097108483314514, loss=1.7600208520889282
I0208 13:45:04.956019 140446903760704 spec.py:321] Evaluating on the training split.
I0208 13:45:07.965784 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:48:26.305381 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 13:48:29.003919 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:51:00.429964 140446903760704 spec.py:349] Evaluating on the test split.
I0208 13:51:03.159565 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 13:53:25.961102 140446903760704 submission_runner.py:408] Time since start: 25420.19s, 	Step: 43189, 	{'train/accuracy': 0.6379035115242004, 'train/loss': 1.7647839784622192, 'train/bleu': 31.63035083470584, 'validation/accuracy': 0.6619136929512024, 'validation/loss': 1.5984268188476562, 'validation/bleu': 28.621136606657313, 'validation/num_examples': 3000, 'test/accuracy': 0.6715589165687561, 'test/loss': 1.5377026796340942, 'test/bleu': 28.04822035841773, 'test/num_examples': 3003, 'score': 15148.924597978592, 'total_duration': 25420.193618297577, 'accumulated_submission_time': 15148.924597978592, 'accumulated_eval_time': 10269.365025520325, 'accumulated_logging_time': 0.5226850509643555}
I0208 13:53:25.981936 140277094741760 logging_writer.py:48] [43189] accumulated_eval_time=10269.365026, accumulated_logging_time=0.522685, accumulated_submission_time=15148.924598, global_step=43189, preemption_count=0, score=15148.924598, test/accuracy=0.671559, test/bleu=28.048220, test/loss=1.537703, test/num_examples=3003, total_duration=25420.193618, train/accuracy=0.637904, train/bleu=31.630351, train/loss=1.764784, validation/accuracy=0.661914, validation/bleu=28.621137, validation/loss=1.598427, validation/num_examples=3000
I0208 13:53:30.181912 140277103134464 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.4322657883167267, loss=1.7226793766021729
I0208 13:54:05.020870 140277094741760 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.4208241403102875, loss=1.709460973739624
I0208 13:54:39.981467 140277103134464 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.44771313667297363, loss=1.7558553218841553
I0208 13:55:14.976473 140277094741760 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3862040936946869, loss=1.6946061849594116
I0208 13:55:49.959589 140277103134464 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.37153375148773193, loss=1.7846636772155762
I0208 13:56:24.959779 140277094741760 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.4156947731971741, loss=1.8021211624145508
I0208 13:56:59.948687 140277103134464 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.46963319182395935, loss=1.699280858039856
I0208 13:57:34.926691 140277094741760 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.5154318809509277, loss=1.7509560585021973
I0208 13:58:09.919714 140277103134464 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.58974289894104, loss=1.6954847574234009
I0208 13:58:44.909806 140277094741760 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.42169830203056335, loss=1.7768917083740234
I0208 13:59:19.894814 140277103134464 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.4356970489025116, loss=1.755771279335022
I0208 13:59:54.879291 140277094741760 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.47854000329971313, loss=1.648174524307251
I0208 14:00:29.882975 140277103134464 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.45814579725265503, loss=1.6966277360916138
I0208 14:01:04.887387 140277094741760 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.45724451541900635, loss=1.7608699798583984
I0208 14:01:39.892507 140277103134464 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.41987156867980957, loss=1.7563589811325073
I0208 14:02:14.894189 140277094741760 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.42054522037506104, loss=1.7226413488388062
I0208 14:02:49.960273 140277103134464 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.420156329870224, loss=1.794978141784668
I0208 14:03:24.952902 140277094741760 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.40263718366622925, loss=1.6860809326171875
I0208 14:03:59.951606 140277103134464 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3862566351890564, loss=1.6610904932022095
I0208 14:04:34.964679 140277094741760 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.4339800179004669, loss=1.695021629333496
I0208 14:05:09.949513 140277103134464 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.375501424074173, loss=1.7411712408065796
I0208 14:05:44.928950 140277094741760 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.35499078035354614, loss=1.7469432353973389
I0208 14:06:19.945554 140277103134464 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3656425178050995, loss=1.7243156433105469
I0208 14:06:54.920305 140277094741760 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.3990190625190735, loss=1.8225922584533691
I0208 14:07:26.154740 140446903760704 spec.py:321] Evaluating on the training split.
I0208 14:07:29.164245 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 14:11:09.229872 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 14:11:11.949543 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 14:13:45.242760 140446903760704 spec.py:349] Evaluating on the test split.
I0208 14:13:47.954251 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 14:16:23.152205 140446903760704 submission_runner.py:408] Time since start: 26797.38s, 	Step: 45591, 	{'train/accuracy': 0.645022988319397, 'train/loss': 1.7124336957931519, 'train/bleu': 31.1812755361264, 'validation/accuracy': 0.6615293025970459, 'validation/loss': 1.5967493057250977, 'validation/bleu': 28.51240267229416, 'validation/num_examples': 3000, 'test/accuracy': 0.6716750860214233, 'test/loss': 1.53374445438385, 'test/bleu': 27.956589459535934, 'test/num_examples': 3003, 'score': 15989.009542703629, 'total_duration': 26797.38473558426, 'accumulated_submission_time': 15989.009542703629, 'accumulated_eval_time': 10806.36244225502, 'accumulated_logging_time': 0.5551190376281738}
I0208 14:16:23.172899 140277103134464 logging_writer.py:48] [45591] accumulated_eval_time=10806.362442, accumulated_logging_time=0.555119, accumulated_submission_time=15989.009543, global_step=45591, preemption_count=0, score=15989.009543, test/accuracy=0.671675, test/bleu=27.956589, test/loss=1.533744, test/num_examples=3003, total_duration=26797.384736, train/accuracy=0.645023, train/bleu=31.181276, train/loss=1.712434, validation/accuracy=0.661529, validation/bleu=28.512403, validation/loss=1.596749, validation/num_examples=3000
I0208 14:16:26.663952 140277094741760 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3837442100048065, loss=1.7244840860366821
I0208 14:17:01.505133 140277103134464 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.4402235448360443, loss=1.8297721147537231
I0208 14:17:36.426471 140277094741760 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.40496766567230225, loss=1.699320673942566
I0208 14:18:11.419140 140277103134464 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3968234658241272, loss=1.688162088394165
I0208 14:18:46.422722 140277094741760 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.41302239894866943, loss=1.7175887823104858
I0208 14:19:21.412516 140277103134464 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.4286736249923706, loss=1.7887519598007202
I0208 14:19:56.375403 140277094741760 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.3641042113304138, loss=1.6977990865707397
I0208 14:20:31.366941 140277103134464 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.3867661952972412, loss=1.7430726289749146
I0208 14:21:06.355938 140277094741760 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.4330882430076599, loss=1.830329179763794
I0208 14:21:41.366687 140277103134464 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3790820837020874, loss=1.8228601217269897
I0208 14:22:16.399955 140277094741760 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.4953173100948334, loss=1.8138686418533325
I0208 14:22:51.384918 140277103134464 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.4389638602733612, loss=1.8056074380874634
I0208 14:23:26.385611 140277094741760 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.40207383036613464, loss=1.7610313892364502
I0208 14:24:01.380297 140277103134464 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.34493136405944824, loss=1.7302170991897583
I0208 14:24:36.413277 140277094741760 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.37234213948249817, loss=1.7457846403121948
I0208 14:25:11.387402 140277103134464 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.43287310004234314, loss=1.6983888149261475
I0208 14:25:46.382309 140277094741760 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.44603231549263, loss=1.759190320968628
I0208 14:26:21.403218 140277103134464 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.44277551770210266, loss=1.7032976150512695
I0208 14:26:56.426985 140277094741760 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.38229456543922424, loss=1.745412826538086
I0208 14:27:31.448412 140277103134464 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.4545850455760956, loss=1.7080243825912476
I0208 14:28:06.426715 140277094741760 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.434807151556015, loss=1.7464104890823364
I0208 14:28:41.434576 140277103134464 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.5894697904586792, loss=1.7928072214126587
I0208 14:29:16.440481 140277094741760 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.4289831817150116, loss=1.744927167892456
I0208 14:29:51.419174 140277103134464 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3914331793785095, loss=1.7176804542541504
I0208 14:30:23.340921 140446903760704 spec.py:321] Evaluating on the training split.
I0208 14:30:26.344013 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 14:33:48.117806 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 14:33:50.841196 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 14:36:22.550361 140446903760704 spec.py:349] Evaluating on the test split.
I0208 14:36:25.262707 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 14:38:56.664790 140446903760704 submission_runner.py:408] Time since start: 28150.90s, 	Step: 47993, 	{'train/accuracy': 0.6419897079467773, 'train/loss': 1.7327611446380615, 'train/bleu': 31.192088849007714, 'validation/accuracy': 0.6634387373924255, 'validation/loss': 1.5877983570098877, 'validation/bleu': 28.441895240859274, 'validation/num_examples': 3000, 'test/accuracy': 0.6764046549797058, 'test/loss': 1.518065333366394, 'test/bleu': 27.780123571158285, 'test/num_examples': 3003, 'score': 16829.091136455536, 'total_duration': 28150.897320508957, 'accumulated_submission_time': 16829.091136455536, 'accumulated_eval_time': 11319.686262845993, 'accumulated_logging_time': 0.5872867107391357}
I0208 14:38:56.685842 140277094741760 logging_writer.py:48] [47993] accumulated_eval_time=11319.686263, accumulated_logging_time=0.587287, accumulated_submission_time=16829.091136, global_step=47993, preemption_count=0, score=16829.091136, test/accuracy=0.676405, test/bleu=27.780124, test/loss=1.518065, test/num_examples=3003, total_duration=28150.897321, train/accuracy=0.641990, train/bleu=31.192089, train/loss=1.732761, validation/accuracy=0.663439, validation/bleu=28.441895, validation/loss=1.587798, validation/num_examples=3000
I0208 14:38:59.492595 140277103134464 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.48565834760665894, loss=1.7749199867248535
I0208 14:39:34.343842 140277094741760 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.48232465982437134, loss=1.7443944215774536
I0208 14:40:09.304602 140277103134464 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.4040636718273163, loss=1.8052321672439575
I0208 14:40:44.315064 140277094741760 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.4251285493373871, loss=1.7465388774871826
I0208 14:41:19.307781 140277103134464 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.4033832848072052, loss=1.6885849237442017
I0208 14:41:54.288072 140277094741760 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.4755384624004364, loss=1.7117751836776733
I0208 14:42:29.258758 140277103134464 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.38510972261428833, loss=1.8109159469604492
I0208 14:43:04.251337 140277094741760 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.4447086751461029, loss=1.7751882076263428
I0208 14:43:39.223919 140277103134464 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.39536505937576294, loss=1.6799341440200806
I0208 14:44:14.189091 140277094741760 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3840164542198181, loss=1.6786625385284424
I0208 14:44:49.156328 140277103134464 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.41580694913864136, loss=1.7540419101715088
I0208 14:45:24.148319 140277094741760 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.41859185695648193, loss=1.708799123764038
I0208 14:45:59.181112 140277103134464 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.4579065442085266, loss=1.6631114482879639
I0208 14:46:34.215111 140277094741760 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3710917532444, loss=1.6666648387908936
I0208 14:47:09.237718 140277103134464 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.41527819633483887, loss=1.7048699855804443
I0208 14:47:44.242263 140277094741760 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3797681927680969, loss=1.7881218194961548
I0208 14:48:19.205468 140277103134464 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.45036259293556213, loss=1.7167234420776367
I0208 14:48:54.181307 140277094741760 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.40297457575798035, loss=1.8113672733306885
I0208 14:49:29.175962 140277103134464 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.40348345041275024, loss=1.6913396120071411
I0208 14:50:04.161128 140277094741760 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3924006521701813, loss=1.7648311853408813
I0208 14:50:39.175592 140277103134464 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.4385288953781128, loss=1.7156362533569336
I0208 14:51:14.216125 140277094741760 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.4630798101425171, loss=1.8468114137649536
I0208 14:51:49.241213 140277103134464 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.37583205103874207, loss=1.7371224164962769
I0208 14:52:24.279075 140277094741760 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.448668897151947, loss=1.676509976387024
I0208 14:52:56.910951 140446903760704 spec.py:321] Evaluating on the training split.
I0208 14:52:59.924717 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 14:56:59.880388 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 14:57:02.607204 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:00:54.300810 140446903760704 spec.py:349] Evaluating on the test split.
I0208 15:00:57.014167 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:04:15.591476 140446903760704 submission_runner.py:408] Time since start: 29669.82s, 	Step: 50395, 	{'train/accuracy': 0.6616469025611877, 'train/loss': 1.6047558784484863, 'train/bleu': 32.69124701519562, 'validation/accuracy': 0.6644430756568909, 'validation/loss': 1.5778286457061768, 'validation/bleu': 28.24018160352546, 'validation/num_examples': 3000, 'test/accuracy': 0.6767880916595459, 'test/loss': 1.508074164390564, 'test/bleu': 28.356528811797137, 'test/num_examples': 3003, 'score': 17669.225746631622, 'total_duration': 29669.82399916649, 'accumulated_submission_time': 17669.225746631622, 'accumulated_eval_time': 11998.36673951149, 'accumulated_logging_time': 0.6197023391723633}
I0208 15:04:15.614558 140277103134464 logging_writer.py:48] [50395] accumulated_eval_time=11998.366740, accumulated_logging_time=0.619702, accumulated_submission_time=17669.225747, global_step=50395, preemption_count=0, score=17669.225747, test/accuracy=0.676788, test/bleu=28.356529, test/loss=1.508074, test/num_examples=3003, total_duration=29669.823999, train/accuracy=0.661647, train/bleu=32.691247, train/loss=1.604756, validation/accuracy=0.664443, validation/bleu=28.240182, validation/loss=1.577829, validation/num_examples=3000
I0208 15:04:17.712361 140277094741760 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.4294201731681824, loss=1.7831248044967651
I0208 15:04:52.490791 140277103134464 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.37226659059524536, loss=1.7702972888946533
I0208 15:05:27.421362 140277094741760 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.4211142659187317, loss=1.705093502998352
I0208 15:06:02.403314 140277103134464 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.42474302649497986, loss=1.725618839263916
I0208 15:06:37.389666 140277094741760 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.4510691165924072, loss=1.7479772567749023
I0208 15:07:12.336753 140277103134464 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.41264575719833374, loss=1.7691104412078857
I0208 15:07:47.315195 140277094741760 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.407753050327301, loss=1.7188485860824585
I0208 15:08:22.310370 140277103134464 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3568236827850342, loss=1.6979014873504639
I0208 15:08:57.294388 140277094741760 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3921224772930145, loss=1.7759209871292114
I0208 15:09:32.287103 140277103134464 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.3916085660457611, loss=1.7437365055084229
I0208 15:10:07.280244 140277094741760 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.41881853342056274, loss=1.691266655921936
I0208 15:10:42.255176 140277103134464 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3776257336139679, loss=1.7197952270507812
I0208 15:11:17.231405 140277094741760 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3989642560482025, loss=1.8110772371292114
I0208 15:11:52.223808 140277103134464 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.3917733430862427, loss=1.717956304550171
I0208 15:12:27.219155 140277094741760 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.4205552041530609, loss=1.8701939582824707
I0208 15:13:02.244215 140277103134464 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.36371609568595886, loss=1.6619477272033691
I0208 15:13:37.306632 140277094741760 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.4742402136325836, loss=1.693049430847168
I0208 15:14:12.282103 140277103134464 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.39052364230155945, loss=1.8087215423583984
I0208 15:14:47.255308 140277094741760 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.37940713763237, loss=1.7194921970367432
I0208 15:15:22.246281 140277103134464 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.39626291394233704, loss=1.7183892726898193
I0208 15:15:57.221944 140277094741760 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.41215774416923523, loss=1.7807426452636719
I0208 15:16:32.205893 140277103134464 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3762985169887543, loss=1.8042290210723877
I0208 15:17:07.201206 140277094741760 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.4041294753551483, loss=1.7719358205795288
I0208 15:17:42.194949 140277103134464 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.37902066111564636, loss=1.7711353302001953
I0208 15:18:15.830030 140446903760704 spec.py:321] Evaluating on the training split.
I0208 15:18:18.854450 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:21:34.035939 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 15:21:36.767579 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:24:16.274115 140446903760704 spec.py:349] Evaluating on the test split.
I0208 15:24:18.985912 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:26:58.368253 140446903760704 submission_runner.py:408] Time since start: 31032.60s, 	Step: 52798, 	{'train/accuracy': 0.6459488868713379, 'train/loss': 1.7108194828033447, 'train/bleu': 31.285477518950703, 'validation/accuracy': 0.6637115478515625, 'validation/loss': 1.577656865119934, 'validation/bleu': 28.79268462181212, 'validation/num_examples': 3000, 'test/accuracy': 0.6748591065406799, 'test/loss': 1.510031819343567, 'test/bleu': 28.116153582756937, 'test/num_examples': 3003, 'score': 18509.355145454407, 'total_duration': 31032.600776195526, 'accumulated_submission_time': 18509.355145454407, 'accumulated_eval_time': 12520.904906749725, 'accumulated_logging_time': 0.6526608467102051}
I0208 15:26:58.390832 140277094741760 logging_writer.py:48] [52798] accumulated_eval_time=12520.904907, accumulated_logging_time=0.652661, accumulated_submission_time=18509.355145, global_step=52798, preemption_count=0, score=18509.355145, test/accuracy=0.674859, test/bleu=28.116154, test/loss=1.510032, test/num_examples=3003, total_duration=31032.600776, train/accuracy=0.645949, train/bleu=31.285478, train/loss=1.710819, validation/accuracy=0.663712, validation/bleu=28.792685, validation/loss=1.577657, validation/num_examples=3000
I0208 15:26:59.458986 140277103134464 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.4252080023288727, loss=1.782219648361206
I0208 15:27:34.267948 140277094741760 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.4188286364078522, loss=1.804030179977417
I0208 15:28:09.199855 140277103134464 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.4284482002258301, loss=1.7869977951049805
I0208 15:28:44.194766 140277094741760 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.40042445063591003, loss=1.7587555646896362
I0208 15:29:19.147409 140277103134464 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.4077576994895935, loss=1.7140530347824097
I0208 15:29:54.131804 140277094741760 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.4410229027271271, loss=1.689512848854065
I0208 15:30:29.118188 140277103134464 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.39509347081184387, loss=1.7500869035720825
I0208 15:31:04.123121 140277094741760 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3757363557815552, loss=1.7033326625823975
I0208 15:31:39.123627 140277103134464 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.39638030529022217, loss=1.7616099119186401
I0208 15:32:14.109863 140277094741760 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.37922990322113037, loss=1.733481526374817
I0208 15:32:49.104055 140277103134464 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.39553824067115784, loss=1.7637242078781128
I0208 15:33:24.157311 140277094741760 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.43943309783935547, loss=1.6704344749450684
I0208 15:33:59.184490 140277103134464 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.40970689058303833, loss=1.6729973554611206
I0208 15:34:34.183605 140277094741760 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.37867292761802673, loss=1.6473748683929443
I0208 15:35:09.193860 140277103134464 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.3897107243537903, loss=1.7396454811096191
I0208 15:35:44.243757 140277094741760 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.41864243149757385, loss=1.6569957733154297
I0208 15:36:19.310543 140277103134464 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.370723694562912, loss=1.658021330833435
I0208 15:36:54.308856 140277094741760 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.43574610352516174, loss=1.7161842584609985
I0208 15:37:29.305639 140277103134464 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.4088687002658844, loss=1.7480055093765259
I0208 15:38:04.306813 140277094741760 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.4688292443752289, loss=1.7132841348648071
I0208 15:38:39.278783 140277103134464 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3924081027507782, loss=1.6650097370147705
I0208 15:39:14.262765 140277094741760 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.4053097665309906, loss=1.7401202917099
I0208 15:39:49.244374 140277103134464 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.37894681096076965, loss=1.686426043510437
I0208 15:40:24.273938 140277094741760 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.42663705348968506, loss=1.782467007637024
I0208 15:40:58.665600 140446903760704 spec.py:321] Evaluating on the training split.
I0208 15:41:01.683285 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:45:03.403878 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 15:45:06.112046 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:48:05.924789 140446903760704 spec.py:349] Evaluating on the test split.
I0208 15:48:08.628219 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 15:50:34.453704 140446903760704 submission_runner.py:408] Time since start: 32448.69s, 	Step: 55200, 	{'train/accuracy': 0.6462436318397522, 'train/loss': 1.7085494995117188, 'train/bleu': 31.661851946131762, 'validation/accuracy': 0.6645422577857971, 'validation/loss': 1.571349024772644, 'validation/bleu': 28.521983085309316, 'validation/num_examples': 3000, 'test/accuracy': 0.6769275665283203, 'test/loss': 1.498882532119751, 'test/bleu': 28.070425518899892, 'test/num_examples': 3003, 'score': 19349.541662931442, 'total_duration': 32448.686230182648, 'accumulated_submission_time': 19349.541662931442, 'accumulated_eval_time': 13096.69297504425, 'accumulated_logging_time': 0.6853039264678955}
I0208 15:50:34.475680 140277103134464 logging_writer.py:48] [55200] accumulated_eval_time=13096.692975, accumulated_logging_time=0.685304, accumulated_submission_time=19349.541663, global_step=55200, preemption_count=0, score=19349.541663, test/accuracy=0.676928, test/bleu=28.070426, test/loss=1.498883, test/num_examples=3003, total_duration=32448.686230, train/accuracy=0.646244, train/bleu=31.661852, train/loss=1.708549, validation/accuracy=0.664542, validation/bleu=28.521983, validation/loss=1.571349, validation/num_examples=3000
I0208 15:50:34.852278 140277094741760 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.37282997369766235, loss=1.650443434715271
I0208 15:51:09.671422 140277103134464 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.39950019121170044, loss=1.7630192041397095
I0208 15:51:44.615247 140277094741760 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3920239210128784, loss=1.723766803741455
I0208 15:52:19.592060 140277103134464 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.3817179799079895, loss=1.6862454414367676
I0208 15:52:54.554575 140277094741760 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.4228178560733795, loss=1.7490731477737427
I0208 15:53:29.552550 140277103134464 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.4255704879760742, loss=1.6554888486862183
I0208 15:54:04.534998 140277094741760 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.7092851400375366, loss=1.7397233247756958
I0208 15:54:39.513496 140277103134464 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.5846450924873352, loss=1.8086029291152954
I0208 15:55:14.494079 140277094741760 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.4019547998905182, loss=1.6749684810638428
I0208 15:55:49.488252 140277103134464 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.43219903111457825, loss=1.6715317964553833
I0208 15:56:24.504069 140277094741760 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.4225185811519623, loss=1.7778874635696411
I0208 15:56:59.503186 140277103134464 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.4329901933670044, loss=1.767637848854065
I0208 15:57:34.513719 140277094741760 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3931888937950134, loss=1.6816747188568115
I0208 15:58:09.497025 140277103134464 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3747636079788208, loss=1.736305594444275
I0208 15:58:44.496905 140277094741760 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.397585928440094, loss=1.6732925176620483
I0208 15:59:19.493948 140277103134464 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.46174490451812744, loss=1.785636305809021
I0208 15:59:54.511673 140277094741760 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3748343288898468, loss=1.7983158826828003
I0208 16:00:29.494076 140277103134464 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.42820805311203003, loss=1.8254255056381226
I0208 16:01:04.470312 140277094741760 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3561621904373169, loss=1.6742833852767944
I0208 16:01:39.439712 140277103134464 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.3609524965286255, loss=1.66364586353302
I0208 16:02:14.457305 140277094741760 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.39799967408180237, loss=1.7185890674591064
I0208 16:02:49.446105 140277103134464 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.397683709859848, loss=1.712777853012085
I0208 16:03:24.433816 140277094741760 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.44658660888671875, loss=1.6954054832458496
I0208 16:03:59.427872 140277103134464 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.4083479046821594, loss=1.8216596841812134
I0208 16:04:34.422348 140277094741760 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.37619099020957947, loss=1.7287179231643677
I0208 16:04:34.496360 140446903760704 spec.py:321] Evaluating on the training split.
I0208 16:04:37.509051 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:08:27.647921 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 16:08:30.359474 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:11:13.472371 140446903760704 spec.py:349] Evaluating on the test split.
I0208 16:11:16.190680 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:13:52.981893 140446903760704 submission_runner.py:408] Time since start: 33847.21s, 	Step: 57602, 	{'train/accuracy': 0.6559958457946777, 'train/loss': 1.6449164152145386, 'train/bleu': 31.921049540477092, 'validation/accuracy': 0.6649266481399536, 'validation/loss': 1.5654634237289429, 'validation/bleu': 28.650881275581277, 'validation/num_examples': 3000, 'test/accuracy': 0.6784266233444214, 'test/loss': 1.4948620796203613, 'test/bleu': 28.403415924123994, 'test/num_examples': 3003, 'score': 20189.47763967514, 'total_duration': 33847.21442198753, 'accumulated_submission_time': 20189.47763967514, 'accumulated_eval_time': 13655.178454637527, 'accumulated_logging_time': 0.7172021865844727}
I0208 16:13:53.005151 140277103134464 logging_writer.py:48] [57602] accumulated_eval_time=13655.178455, accumulated_logging_time=0.717202, accumulated_submission_time=20189.477640, global_step=57602, preemption_count=0, score=20189.477640, test/accuracy=0.678427, test/bleu=28.403416, test/loss=1.494862, test/num_examples=3003, total_duration=33847.214422, train/accuracy=0.655996, train/bleu=31.921050, train/loss=1.644916, validation/accuracy=0.664927, validation/bleu=28.650881, validation/loss=1.565463, validation/num_examples=3000
I0208 16:14:27.449087 140277094741760 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.39609652757644653, loss=1.7691160440444946
I0208 16:15:02.379323 140277103134464 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.40718144178390503, loss=1.780396819114685
I0208 16:15:37.354014 140277094741760 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.4220562279224396, loss=1.602414608001709
I0208 16:16:12.345063 140277103134464 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.4294264614582062, loss=1.6646348237991333
I0208 16:16:47.308557 140277094741760 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.378871887922287, loss=1.678335189819336
I0208 16:17:22.272079 140277103134464 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.39483046531677246, loss=1.6194000244140625
I0208 16:17:57.239834 140277094741760 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.39186516404151917, loss=1.6748744249343872
I0208 16:18:32.255733 140277103134464 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.4037194848060608, loss=1.6875208616256714
I0208 16:19:07.221831 140277094741760 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.35927388072013855, loss=1.7279003858566284
I0208 16:19:42.228369 140277103134464 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3767678737640381, loss=1.7628093957901
I0208 16:20:17.208317 140277094741760 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.4330248236656189, loss=1.705513834953308
I0208 16:20:52.179404 140277103134464 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.39566776156425476, loss=1.6879278421401978
I0208 16:21:27.159584 140277094741760 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.4132630228996277, loss=1.6241072416305542
I0208 16:22:02.146623 140277103134464 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.39466288685798645, loss=1.7428897619247437
I0208 16:22:37.157637 140277094741760 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.4066716134548187, loss=1.7093169689178467
I0208 16:23:12.124076 140277103134464 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.4258089065551758, loss=1.7038058042526245
I0208 16:23:47.110774 140277094741760 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.41724467277526855, loss=1.7534711360931396
I0208 16:24:22.108096 140277103134464 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3954746425151825, loss=1.6958221197128296
I0208 16:24:57.125874 140277094741760 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.42951518297195435, loss=1.714532732963562
I0208 16:25:32.127920 140277103134464 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.401496559381485, loss=1.720892310142517
I0208 16:26:07.112154 140277094741760 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3942970931529999, loss=1.6680247783660889
I0208 16:26:42.178727 140277103134464 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.4276815950870514, loss=1.65839421749115
I0208 16:27:17.212009 140277094741760 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.4250488579273224, loss=1.6681212186813354
I0208 16:27:52.201341 140277103134464 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.43278810381889343, loss=1.7345179319381714
I0208 16:27:53.316939 140446903760704 spec.py:321] Evaluating on the training split.
I0208 16:27:56.329499 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:31:01.331110 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 16:31:04.063470 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:33:46.153659 140446903760704 spec.py:349] Evaluating on the test split.
I0208 16:33:48.864734 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:36:23.069628 140446903760704 submission_runner.py:408] Time since start: 35197.30s, 	Step: 60005, 	{'train/accuracy': 0.6456266641616821, 'train/loss': 1.7141486406326294, 'train/bleu': 32.09485250326785, 'validation/accuracy': 0.6683239936828613, 'validation/loss': 1.5589754581451416, 'validation/bleu': 28.846310976430967, 'validation/num_examples': 3000, 'test/accuracy': 0.6812503933906555, 'test/loss': 1.4764920473098755, 'test/bleu': 28.37237056623999, 'test/num_examples': 3003, 'score': 21029.704471111298, 'total_duration': 35197.30215525627, 'accumulated_submission_time': 21029.704471111298, 'accumulated_eval_time': 14164.931086301804, 'accumulated_logging_time': 0.7506864070892334}
I0208 16:36:23.092521 140277094741760 logging_writer.py:48] [60005] accumulated_eval_time=14164.931086, accumulated_logging_time=0.750686, accumulated_submission_time=21029.704471, global_step=60005, preemption_count=0, score=21029.704471, test/accuracy=0.681250, test/bleu=28.372371, test/loss=1.476492, test/num_examples=3003, total_duration=35197.302155, train/accuracy=0.645627, train/bleu=32.094853, train/loss=1.714149, validation/accuracy=0.668324, validation/bleu=28.846311, validation/loss=1.558975, validation/num_examples=3000
I0208 16:36:56.506281 140277103134464 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.4279722273349762, loss=1.7198460102081299
I0208 16:37:31.444040 140277094741760 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.42120790481567383, loss=1.6545474529266357
I0208 16:38:06.418193 140277103134464 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.35812991857528687, loss=1.7135686874389648
I0208 16:38:41.397642 140277094741760 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.41688570380210876, loss=1.737475872039795
I0208 16:39:16.381412 140277103134464 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.4130156636238098, loss=1.761966586112976
I0208 16:39:51.351869 140277094741760 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.4080743193626404, loss=1.7166916131973267
I0208 16:40:26.326646 140277103134464 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.41158783435821533, loss=1.717918038368225
I0208 16:41:01.300404 140277094741760 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.38319340348243713, loss=1.7140742540359497
I0208 16:41:36.305412 140277103134464 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3955880105495453, loss=1.7643558979034424
I0208 16:42:11.330069 140277094741760 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.41835904121398926, loss=1.7137993574142456
I0208 16:42:46.391304 140277103134464 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.3966793119907379, loss=1.6399364471435547
I0208 16:43:21.364837 140277094741760 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.43701764941215515, loss=1.6988801956176758
I0208 16:43:56.350126 140277103134464 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.39717671275138855, loss=1.7459222078323364
I0208 16:44:31.332465 140277094741760 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.39915168285369873, loss=1.69800865650177
I0208 16:45:06.313012 140277103134464 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.39090171456336975, loss=1.6969940662384033
I0208 16:45:41.329175 140277094741760 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.3859030306339264, loss=1.656832218170166
I0208 16:46:16.316792 140277103134464 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3965141177177429, loss=1.6760368347167969
I0208 16:46:51.357851 140277094741760 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.38237300515174866, loss=1.7520406246185303
I0208 16:47:26.413029 140277103134464 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.39206841588020325, loss=1.6926919221878052
I0208 16:48:01.383432 140277094741760 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.4394409954547882, loss=1.7608764171600342
I0208 16:48:36.365088 140277103134464 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.419956773519516, loss=1.6315395832061768
I0208 16:49:11.362750 140277094741760 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.4201711416244507, loss=1.712950348854065
I0208 16:49:46.395112 140277103134464 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.4109967350959778, loss=1.6172947883605957
I0208 16:50:21.420128 140277094741760 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.39649492502212524, loss=1.6427794694900513
I0208 16:50:23.241692 140446903760704 spec.py:321] Evaluating on the training split.
I0208 16:50:26.248606 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:53:29.620229 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 16:53:32.338399 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:56:09.364446 140446903760704 spec.py:349] Evaluating on the test split.
I0208 16:56:12.085848 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 16:59:02.305866 140446903760704 submission_runner.py:408] Time since start: 36556.54s, 	Step: 62407, 	{'train/accuracy': 0.6473643183708191, 'train/loss': 1.701988935470581, 'train/bleu': 31.688075634827747, 'validation/accuracy': 0.6715973615646362, 'validation/loss': 1.5442315340042114, 'validation/bleu': 28.907307741563486, 'validation/num_examples': 3000, 'test/accuracy': 0.6835861206054688, 'test/loss': 1.4696898460388184, 'test/bleu': 28.859389761738015, 'test/num_examples': 3003, 'score': 21869.767882585526, 'total_duration': 36556.538370132446, 'accumulated_submission_time': 21869.767882585526, 'accumulated_eval_time': 14683.995180606842, 'accumulated_logging_time': 0.7836964130401611}
I0208 16:59:02.329375 140277103134464 logging_writer.py:48] [62407] accumulated_eval_time=14683.995181, accumulated_logging_time=0.783696, accumulated_submission_time=21869.767883, global_step=62407, preemption_count=0, score=21869.767883, test/accuracy=0.683586, test/bleu=28.859390, test/loss=1.469690, test/num_examples=3003, total_duration=36556.538370, train/accuracy=0.647364, train/bleu=31.688076, train/loss=1.701989, validation/accuracy=0.671597, validation/bleu=28.907308, validation/loss=1.544232, validation/num_examples=3000
I0208 16:59:35.050850 140277094741760 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.44262129068374634, loss=1.6733314990997314
I0208 17:00:10.024820 140277103134464 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3746212422847748, loss=1.67867910861969
I0208 17:00:45.005852 140277094741760 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.44778791069984436, loss=1.6395587921142578
I0208 17:01:19.977838 140277103134464 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.41570696234703064, loss=1.703359842300415
I0208 17:01:54.994564 140277094741760 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.4111706614494324, loss=1.615776777267456
I0208 17:02:29.988773 140277103134464 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.45908302068710327, loss=1.7499110698699951
I0208 17:03:04.946911 140277094741760 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.40612488985061646, loss=1.6478914022445679
I0208 17:03:39.906547 140277103134464 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.4018065631389618, loss=1.6818995475769043
I0208 17:04:14.908620 140277094741760 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.4039948284626007, loss=1.7357003688812256
I0208 17:04:49.891119 140277103134464 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.425663560628891, loss=1.8028719425201416
I0208 17:05:24.869754 140277094741760 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.4230300486087799, loss=1.6639779806137085
I0208 17:05:59.833949 140277103134464 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.38276615738868713, loss=1.6342707872390747
I0208 17:06:34.807066 140277094741760 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.40215232968330383, loss=1.6329516172409058
I0208 17:07:09.781435 140277103134464 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.41526922583580017, loss=1.8000866174697876
I0208 17:07:44.769133 140277094741760 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3974109888076782, loss=1.693365216255188
I0208 17:08:19.784126 140277103134464 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.43008264899253845, loss=1.7039566040039062
I0208 17:08:54.776373 140277094741760 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.4390000104904175, loss=1.7880898714065552
I0208 17:09:29.776839 140277103134464 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.4124504327774048, loss=1.7479521036148071
I0208 17:10:04.775386 140277094741760 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.39267587661743164, loss=1.647568941116333
I0208 17:10:39.816268 140277103134464 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.4166339337825775, loss=1.7133153676986694
I0208 17:11:14.798617 140277094741760 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.4163571000099182, loss=1.7105783224105835
I0208 17:11:49.797405 140277103134464 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.4204792380332947, loss=1.6223376989364624
I0208 17:12:24.772548 140277094741760 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.8653832077980042, loss=1.606905221939087
I0208 17:12:59.776825 140277103134464 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.4231623113155365, loss=1.6803289651870728
I0208 17:13:02.651201 140446903760704 spec.py:321] Evaluating on the training split.
I0208 17:13:05.658241 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 17:16:42.854941 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 17:16:45.552490 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 17:19:20.927482 140446903760704 spec.py:349] Evaluating on the test split.
I0208 17:19:23.633621 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 17:22:39.084099 140446903760704 submission_runner.py:408] Time since start: 37973.32s, 	Step: 64810, 	{'train/accuracy': 0.6546890735626221, 'train/loss': 1.657198429107666, 'train/bleu': 31.84385511875302, 'validation/accuracy': 0.6724157333374023, 'validation/loss': 1.5407817363739014, 'validation/bleu': 29.134471317994127, 'validation/num_examples': 3000, 'test/accuracy': 0.682877242565155, 'test/loss': 1.4688782691955566, 'test/bleu': 28.835831003764778, 'test/num_examples': 3003, 'score': 22710.003426790237, 'total_duration': 37973.3166179657, 'accumulated_submission_time': 22710.003426790237, 'accumulated_eval_time': 15260.428012609482, 'accumulated_logging_time': 0.8176324367523193}
I0208 17:22:39.107782 140277094741760 logging_writer.py:48] [64810] accumulated_eval_time=15260.428013, accumulated_logging_time=0.817632, accumulated_submission_time=22710.003427, global_step=64810, preemption_count=0, score=22710.003427, test/accuracy=0.682877, test/bleu=28.835831, test/loss=1.468878, test/num_examples=3003, total_duration=37973.316618, train/accuracy=0.654689, train/bleu=31.843855, train/loss=1.657198, validation/accuracy=0.672416, validation/bleu=29.134471, validation/loss=1.540782, validation/num_examples=3000
I0208 17:23:10.874929 140277103134464 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.43335577845573425, loss=1.6580631732940674
I0208 17:23:45.799309 140277094741760 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3963925540447235, loss=1.715537428855896
I0208 17:24:20.820903 140277103134464 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.42906495928764343, loss=1.6806273460388184
I0208 17:24:55.824331 140277094741760 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.39680016040802, loss=1.7274924516677856
I0208 17:25:30.817986 140277103134464 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.41438111662864685, loss=1.6869560480117798
I0208 17:26:05.772561 140277094741760 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.4314151704311371, loss=1.7477751970291138
I0208 17:26:40.742839 140277103134464 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.4001455307006836, loss=1.6484928131103516
I0208 17:27:15.759390 140277094741760 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.4462098479270935, loss=1.7281663417816162
I0208 17:27:50.722278 140277103134464 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.3906579315662384, loss=1.6765775680541992
I0208 17:28:25.698295 140277094741760 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.41078755259513855, loss=1.6762092113494873
I0208 17:29:00.662736 140277103134464 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3826446831226349, loss=1.6602964401245117
I0208 17:29:35.630323 140277094741760 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.4088113009929657, loss=1.6719106435775757
I0208 17:30:10.632475 140277103134464 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.42236822843551636, loss=1.6593884229660034
I0208 17:30:45.599854 140277094741760 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.40484943985939026, loss=1.652734637260437
I0208 17:31:20.614471 140277103134464 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.4413743317127228, loss=1.5884687900543213
I0208 17:31:55.606327 140277094741760 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.41284701228141785, loss=1.6166719198226929
I0208 17:32:30.593037 140277103134464 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.43379348516464233, loss=1.705687403678894
I0208 17:33:05.594112 140277094741760 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.41361770033836365, loss=1.6320164203643799
I0208 17:33:40.558171 140277103134464 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.39031243324279785, loss=1.6530002355575562
I0208 17:34:15.563253 140277094741760 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.41530123353004456, loss=1.5874295234680176
I0208 17:34:50.581932 140277103134464 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.4274565875530243, loss=1.696799874305725
I0208 17:35:25.618942 140277094741760 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.4136040210723877, loss=1.7142646312713623
I0208 17:36:00.620651 140277103134464 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3893944323062897, loss=1.5922632217407227
I0208 17:36:35.613304 140277094741760 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.42054036259651184, loss=1.6314178705215454
I0208 17:36:39.185220 140446903760704 spec.py:321] Evaluating on the training split.
I0208 17:36:42.193918 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 17:40:11.657100 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 17:40:14.374687 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 17:42:54.251587 140446903760704 spec.py:349] Evaluating on the test split.
I0208 17:42:56.956962 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 17:45:26.176499 140446903760704 submission_runner.py:408] Time since start: 39340.41s, 	Step: 67212, 	{'train/accuracy': 0.6506806015968323, 'train/loss': 1.6883158683776855, 'train/bleu': 31.947221106489987, 'validation/accuracy': 0.6726140975952148, 'validation/loss': 1.5377442836761475, 'validation/bleu': 29.383137318665202, 'validation/num_examples': 3000, 'test/accuracy': 0.6839579343795776, 'test/loss': 1.4663946628570557, 'test/bleu': 28.923713488856702, 'test/num_examples': 3003, 'score': 23549.912605524063, 'total_duration': 39340.40902590752, 'accumulated_submission_time': 23549.912605524063, 'accumulated_eval_time': 15787.419231653214, 'accumulated_logging_time': 0.9314663410186768}
I0208 17:45:26.201586 140277103134464 logging_writer.py:48] [67212] accumulated_eval_time=15787.419232, accumulated_logging_time=0.931466, accumulated_submission_time=23549.912606, global_step=67212, preemption_count=0, score=23549.912606, test/accuracy=0.683958, test/bleu=28.923713, test/loss=1.466395, test/num_examples=3003, total_duration=39340.409026, train/accuracy=0.650681, train/bleu=31.947221, train/loss=1.688316, validation/accuracy=0.672614, validation/bleu=29.383137, validation/loss=1.537744, validation/num_examples=3000
I0208 17:45:57.229632 140277094741760 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.41978582739830017, loss=1.644024133682251
I0208 17:46:32.119318 140277103134464 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.40006116032600403, loss=1.7342230081558228
I0208 17:47:07.082017 140277094741760 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.5753886103630066, loss=1.6558136940002441
I0208 17:47:42.044202 140277103134464 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.39665281772613525, loss=1.6869075298309326
I0208 17:48:17.015152 140277094741760 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.1150803565979004, loss=1.7565224170684814
I0208 17:48:52.014861 140277103134464 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.4197492003440857, loss=1.7270172834396362
I0208 17:49:27.056607 140277094741760 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.4190343916416168, loss=1.636160135269165
I0208 17:50:02.174160 140277103134464 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.38784027099609375, loss=1.5939600467681885
I0208 17:50:37.208350 140277094741760 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.43935421109199524, loss=1.7249943017959595
I0208 17:51:12.170929 140277103134464 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.42700493335723877, loss=1.7211925983428955
I0208 17:51:47.145207 140277094741760 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.40181148052215576, loss=1.6346758604049683
I0208 17:52:22.101033 140277103134464 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.43550077080726624, loss=1.7098013162612915
I0208 17:52:57.068799 140277094741760 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.40922245383262634, loss=1.6585396528244019
I0208 17:53:32.064891 140277103134464 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.4073401391506195, loss=1.6726938486099243
I0208 17:54:07.049355 140277094741760 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.41796615719795227, loss=1.724924921989441
I0208 17:54:42.039661 140277103134464 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.42782825231552124, loss=1.752946138381958
I0208 17:55:17.028179 140277094741760 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.39184510707855225, loss=1.5721509456634521
I0208 17:55:52.047331 140277103134464 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.45499905943870544, loss=1.7220531702041626
I0208 17:56:27.041624 140277094741760 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.42137253284454346, loss=1.6791497468948364
I0208 17:57:02.068919 140277103134464 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.4000571370124817, loss=1.669638752937317
I0208 17:57:37.100739 140277094741760 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3957357108592987, loss=1.6526609659194946
I0208 17:58:12.133439 140277103134464 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3980080187320709, loss=1.6050890684127808
I0208 17:58:47.098053 140277094741760 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.37987276911735535, loss=1.653373122215271
I0208 17:59:22.089992 140277103134464 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3995245397090912, loss=1.604097843170166
I0208 17:59:26.362238 140446903760704 spec.py:321] Evaluating on the training split.
I0208 17:59:29.369267 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:02:48.791871 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 18:02:51.507392 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:05:27.612013 140446903760704 spec.py:349] Evaluating on the test split.
I0208 18:05:30.351892 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:08:19.670923 140446903760704 submission_runner.py:408] Time since start: 40713.90s, 	Step: 69614, 	{'train/accuracy': 0.6651747822761536, 'train/loss': 1.5777531862258911, 'train/bleu': 32.645817061232485, 'validation/accuracy': 0.6728744506835938, 'validation/loss': 1.5295296907424927, 'validation/bleu': 29.578541313522177, 'validation/num_examples': 3000, 'test/accuracy': 0.6869211792945862, 'test/loss': 1.4553210735321045, 'test/bleu': 29.16384456800778, 'test/num_examples': 3003, 'score': 24389.983780145645, 'total_duration': 40713.90342450142, 'accumulated_submission_time': 24389.983780145645, 'accumulated_eval_time': 16320.727837085724, 'accumulated_logging_time': 0.9683539867401123}
I0208 18:08:19.702642 140277094741760 logging_writer.py:48] [69614] accumulated_eval_time=16320.727837, accumulated_logging_time=0.968354, accumulated_submission_time=24389.983780, global_step=69614, preemption_count=0, score=24389.983780, test/accuracy=0.686921, test/bleu=29.163845, test/loss=1.455321, test/num_examples=3003, total_duration=40713.903425, train/accuracy=0.665175, train/bleu=32.645817, train/loss=1.577753, validation/accuracy=0.672874, validation/bleu=29.578541, validation/loss=1.529530, validation/num_examples=3000
I0208 18:08:50.024726 140277103134464 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.43708574771881104, loss=1.6567606925964355
I0208 18:09:24.938697 140277094741760 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.45587652921676636, loss=1.677807092666626
I0208 18:09:59.891697 140277103134464 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.4185777008533478, loss=1.7018495798110962
I0208 18:10:35.004193 140277094741760 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.41980063915252686, loss=1.6094197034835815
I0208 18:11:10.018790 140277103134464 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.41466405987739563, loss=1.6216342449188232
I0208 18:11:45.014864 140277094741760 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.4336758852005005, loss=1.668586015701294
I0208 18:12:19.970741 140277103134464 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.48940134048461914, loss=1.6353157758712769
I0208 18:12:54.955643 140277094741760 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.39972972869873047, loss=1.6528342962265015
I0208 18:13:29.950767 140277103134464 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.43602967262268066, loss=1.6030144691467285
I0208 18:14:04.911926 140277094741760 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.4020172655582428, loss=1.680637001991272
I0208 18:14:39.878707 140277103134464 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.41402503848075867, loss=1.7070118188858032
I0208 18:15:14.880269 140277094741760 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.4359050989151001, loss=1.6366324424743652
I0208 18:15:49.853617 140277103134464 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.4254628121852875, loss=1.7627384662628174
I0208 18:16:24.857001 140277094741760 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3891442120075226, loss=1.6758787631988525
I0208 18:16:59.820647 140277103134464 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.4134795069694519, loss=1.6049847602844238
I0208 18:17:34.811985 140277094741760 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.42339783906936646, loss=1.657468318939209
I0208 18:18:09.791531 140277103134464 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.466824471950531, loss=1.7276756763458252
I0208 18:18:44.765562 140277094741760 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.39848873019218445, loss=1.5886818170547485
I0208 18:19:19.735038 140277103134464 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.4468057155609131, loss=1.597482681274414
I0208 18:19:54.732536 140277094741760 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.4187479317188263, loss=1.6516412496566772
I0208 18:20:29.727692 140277103134464 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.4388523995876312, loss=1.675404667854309
I0208 18:21:04.734946 140277094741760 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.38380277156829834, loss=1.7468100786209106
I0208 18:21:39.724228 140277103134464 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.39997076988220215, loss=1.6016771793365479
I0208 18:22:14.749983 140277094741760 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.40399259328842163, loss=1.6768803596496582
I0208 18:22:19.724506 140446903760704 spec.py:321] Evaluating on the training split.
I0208 18:22:22.732336 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:25:56.335750 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 18:25:59.053323 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:28:35.383090 140446903760704 spec.py:349] Evaluating on the test split.
I0208 18:28:38.090648 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:31:20.235918 140446903760704 submission_runner.py:408] Time since start: 42094.47s, 	Step: 72016, 	{'train/accuracy': 0.6537835001945496, 'train/loss': 1.6537264585494995, 'train/bleu': 32.53735942164496, 'validation/accuracy': 0.673072874546051, 'validation/loss': 1.5194854736328125, 'validation/bleu': 29.342807344486634, 'validation/num_examples': 3000, 'test/accuracy': 0.6847946047782898, 'test/loss': 1.4506778717041016, 'test/bleu': 28.99919040780581, 'test/num_examples': 3003, 'score': 25229.91768527031, 'total_duration': 42094.46844172478, 'accumulated_submission_time': 25229.91768527031, 'accumulated_eval_time': 16861.239188194275, 'accumulated_logging_time': 1.0111916065216064}
I0208 18:31:20.261471 140277103134464 logging_writer.py:48] [72016] accumulated_eval_time=16861.239188, accumulated_logging_time=1.011192, accumulated_submission_time=25229.917685, global_step=72016, preemption_count=0, score=25229.917685, test/accuracy=0.684795, test/bleu=28.999190, test/loss=1.450678, test/num_examples=3003, total_duration=42094.468442, train/accuracy=0.653784, train/bleu=32.537359, train/loss=1.653726, validation/accuracy=0.673073, validation/bleu=29.342807, validation/loss=1.519485, validation/num_examples=3000
I0208 18:31:49.859216 140277094741760 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.45766350626945496, loss=1.6849462985992432
I0208 18:32:24.774355 140277103134464 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.42176172137260437, loss=1.622195839881897
I0208 18:32:59.731941 140277094741760 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.4031585156917572, loss=1.623822808265686
I0208 18:33:34.689552 140277103134464 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.403702050447464, loss=1.630861520767212
I0208 18:34:09.680762 140277094741760 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.4008830189704895, loss=1.6795958280563354
I0208 18:34:44.631347 140277103134464 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.40335193276405334, loss=1.5143592357635498
I0208 18:35:19.626147 140277094741760 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.3900231719017029, loss=1.7005183696746826
I0208 18:35:54.580893 140277103134464 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.41303664445877075, loss=1.6393036842346191
I0208 18:36:29.533970 140277094741760 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.40729817748069763, loss=1.6555331945419312
I0208 18:37:04.497984 140277103134464 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.40270015597343445, loss=1.6556934118270874
I0208 18:37:39.495906 140277094741760 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3873405158519745, loss=1.6097851991653442
I0208 18:38:14.467228 140277103134464 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.4475739002227783, loss=1.6564526557922363
I0208 18:38:49.434123 140277094741760 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.47364670038223267, loss=1.5836960077285767
I0208 18:39:24.435615 140277103134464 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.42811277508735657, loss=1.6706846952438354
I0208 18:39:59.400719 140277094741760 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.40697285532951355, loss=1.6120176315307617
I0208 18:40:34.376749 140277103134464 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.39847707748413086, loss=1.6194669008255005
I0208 18:41:09.346627 140277094741760 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.9588772058486938, loss=1.6479719877243042
I0208 18:41:44.325344 140277103134464 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.4044812023639679, loss=1.6315313577651978
I0208 18:42:19.336683 140277094741760 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.4333189129829407, loss=1.656089186668396
I0208 18:42:54.323527 140277103134464 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.41311967372894287, loss=1.6313446760177612
I0208 18:43:29.437806 140277094741760 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.426017165184021, loss=1.6370066404342651
I0208 18:44:04.490136 140277103134464 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.4172692894935608, loss=1.6717313528060913
I0208 18:44:39.517175 140277094741760 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.4579794406890869, loss=1.6031556129455566
I0208 18:45:14.553943 140277103134464 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.4050733745098114, loss=1.7254403829574585
I0208 18:45:20.238424 140446903760704 spec.py:321] Evaluating on the training split.
I0208 18:45:23.267393 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:49:05.874709 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 18:49:08.573738 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:51:43.729258 140446903760704 spec.py:349] Evaluating on the test split.
I0208 18:51:46.440897 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 18:54:09.422216 140446903760704 submission_runner.py:408] Time since start: 43463.65s, 	Step: 74418, 	{'train/accuracy': 0.6587724685668945, 'train/loss': 1.636032223701477, 'train/bleu': 32.378273362747734, 'validation/accuracy': 0.6745235323905945, 'validation/loss': 1.510649561882019, 'validation/bleu': 29.499814008006418, 'validation/num_examples': 3000, 'test/accuracy': 0.6886991262435913, 'test/loss': 1.439121961593628, 'test/bleu': 29.56834137146062, 'test/num_examples': 3003, 'score': 26069.80594444275, 'total_duration': 43463.65474176407, 'accumulated_submission_time': 26069.80594444275, 'accumulated_eval_time': 17390.422934532166, 'accumulated_logging_time': 1.0483558177947998}
I0208 18:54:09.448913 140277094741760 logging_writer.py:48] [74418] accumulated_eval_time=17390.422935, accumulated_logging_time=1.048356, accumulated_submission_time=26069.805944, global_step=74418, preemption_count=0, score=26069.805944, test/accuracy=0.688699, test/bleu=29.568341, test/loss=1.439122, test/num_examples=3003, total_duration=43463.654742, train/accuracy=0.658772, train/bleu=32.378273, train/loss=1.636032, validation/accuracy=0.674524, validation/bleu=29.499814, validation/loss=1.510650, validation/num_examples=3000
I0208 18:54:38.343884 140277103134464 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.4179641604423523, loss=1.671647548675537
I0208 18:55:13.240755 140277094741760 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.41539764404296875, loss=1.6664237976074219
I0208 18:55:48.256466 140277103134464 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.41193678975105286, loss=1.6763865947723389
I0208 18:56:23.239468 140277094741760 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.40423986315727234, loss=1.61197829246521
I0208 18:56:58.236791 140277103134464 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.4240912199020386, loss=1.5493899583816528
I0208 18:57:33.206614 140277094741760 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.42817509174346924, loss=1.6280388832092285
I0208 18:58:08.231641 140277103134464 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.41640037298202515, loss=1.6169848442077637
I0208 18:58:43.205121 140277094741760 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.4052013158798218, loss=1.6825731992721558
I0208 18:59:18.211237 140277103134464 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.3819273114204407, loss=1.5817512273788452
I0208 18:59:53.185920 140277094741760 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.42527735233306885, loss=1.5188820362091064
I0208 19:00:28.175961 140277103134464 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.4271852672100067, loss=1.5753830671310425
I0208 19:01:03.160556 140277094741760 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.42204123735427856, loss=1.6876425743103027
I0208 19:01:38.222456 140277103134464 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.4577309191226959, loss=1.5883231163024902
I0208 19:02:13.199351 140277094741760 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.43728846311569214, loss=1.626077651977539
I0208 19:02:48.171380 140277103134464 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.4352143406867981, loss=1.6665245294570923
I0208 19:03:23.137773 140277094741760 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.43004316091537476, loss=1.650002360343933
I0208 19:03:58.176146 140277103134464 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.4859934449195862, loss=1.6622707843780518
I0208 19:04:33.286601 140277094741760 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.4416939616203308, loss=1.6679316759109497
I0208 19:05:08.255736 140277103134464 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.4324515163898468, loss=1.7132668495178223
I0208 19:05:43.221411 140277094741760 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.4602131247520447, loss=1.7080786228179932
I0208 19:06:18.183879 140277103134464 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.4218558967113495, loss=1.6370259523391724
I0208 19:06:53.162588 140277094741760 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.4529629647731781, loss=1.6272990703582764
I0208 19:07:28.169085 140277103134464 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.41580095887184143, loss=1.4914275407791138
I0208 19:08:03.136036 140277094741760 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.4129447638988495, loss=1.6584266424179077
I0208 19:08:09.516317 140446903760704 spec.py:321] Evaluating on the training split.
I0208 19:08:12.525151 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:11:40.523292 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 19:11:43.235010 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:14:15.590628 140446903760704 spec.py:349] Evaluating on the test split.
I0208 19:14:18.288793 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:17:10.421264 140446903760704 submission_runner.py:408] Time since start: 44844.65s, 	Step: 76820, 	{'train/accuracy': 0.6653993725776672, 'train/loss': 1.5844321250915527, 'train/bleu': 32.9791006066284, 'validation/accuracy': 0.6778588891029358, 'validation/loss': 1.5037970542907715, 'validation/bleu': 29.33848707278825, 'validation/num_examples': 3000, 'test/accuracy': 0.6883272528648376, 'test/loss': 1.435570240020752, 'test/bleu': 29.255923845563185, 'test/num_examples': 3003, 'score': 26909.785906791687, 'total_duration': 44844.65379357338, 'accumulated_submission_time': 26909.785906791687, 'accumulated_eval_time': 17931.327827215195, 'accumulated_logging_time': 1.0869407653808594}
I0208 19:17:10.447751 140277103134464 logging_writer.py:48] [76820] accumulated_eval_time=17931.327827, accumulated_logging_time=1.086941, accumulated_submission_time=26909.785907, global_step=76820, preemption_count=0, score=26909.785907, test/accuracy=0.688327, test/bleu=29.255924, test/loss=1.435570, test/num_examples=3003, total_duration=44844.653794, train/accuracy=0.665399, train/bleu=32.979101, train/loss=1.584432, validation/accuracy=0.677859, validation/bleu=29.338487, validation/loss=1.503797, validation/num_examples=3000
I0208 19:17:38.627483 140277094741760 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.40195757150650024, loss=1.661582589149475
I0208 19:18:13.568434 140277103134464 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.4657571315765381, loss=1.69706392288208
I0208 19:18:48.547026 140277094741760 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.4272078275680542, loss=1.6380259990692139
I0208 19:19:23.562652 140277103134464 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.42885518074035645, loss=1.6612966060638428
I0208 19:19:58.584375 140277094741760 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.43949395418167114, loss=1.680245280265808
I0208 19:20:33.543900 140277103134464 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.4319349229335785, loss=1.6675992012023926
I0208 19:21:08.515163 140277094741760 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.42997416853904724, loss=1.6141263246536255
I0208 19:21:43.494925 140277103134464 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.4467580318450928, loss=1.5850499868392944
I0208 19:22:18.497450 140277094741760 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.40903493762016296, loss=1.6004834175109863
I0208 19:22:53.484261 140277103134464 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.4275355637073517, loss=1.570874810218811
I0208 19:23:28.473164 140277094741760 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.4462008774280548, loss=1.7022819519042969
I0208 19:24:03.449990 140277103134464 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.43137338757514954, loss=1.650473952293396
I0208 19:24:38.425649 140277094741760 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.45005419850349426, loss=1.6065865755081177
I0208 19:25:13.397131 140277103134464 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.42843204736709595, loss=1.6360024213790894
I0208 19:25:48.414482 140277094741760 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.40703508257865906, loss=1.6313550472259521
I0208 19:26:23.444604 140277103134464 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.4176926910877228, loss=1.5749821662902832
I0208 19:26:58.438807 140277094741760 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.4194120764732361, loss=1.598931908607483
I0208 19:27:33.418828 140277103134464 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.4300537705421448, loss=1.7124907970428467
I0208 19:28:08.408918 140277094741760 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4400484263896942, loss=1.6778172254562378
I0208 19:28:43.386390 140277103134464 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.4494999349117279, loss=1.6361374855041504
I0208 19:29:18.407825 140277094741760 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.45149683952331543, loss=1.6395360231399536
I0208 19:29:53.415687 140277103134464 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.4254264831542969, loss=1.6830520629882812
I0208 19:30:28.382410 140277094741760 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.4167487323284149, loss=1.614309310913086
I0208 19:31:03.396186 140277103134464 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.41372987627983093, loss=1.6055405139923096
I0208 19:31:10.491781 140446903760704 spec.py:321] Evaluating on the training split.
I0208 19:31:13.524709 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:34:21.738145 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 19:34:24.448500 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:37:11.733480 140446903760704 spec.py:349] Evaluating on the test split.
I0208 19:37:14.427583 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:39:54.152405 140446903760704 submission_runner.py:408] Time since start: 46208.38s, 	Step: 79222, 	{'train/accuracy': 0.6607912182807922, 'train/loss': 1.6076757907867432, 'train/bleu': 33.057591815314304, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.5020564794540405, 'validation/bleu': 29.761661695762974, 'validation/num_examples': 3000, 'test/accuracy': 0.6905118823051453, 'test/loss': 1.4266692399978638, 'test/bleu': 29.21426525986103, 'test/num_examples': 3003, 'score': 27749.74374437332, 'total_duration': 46208.38493394852, 'accumulated_submission_time': 27749.74374437332, 'accumulated_eval_time': 18454.988405942917, 'accumulated_logging_time': 1.1236071586608887}
I0208 19:39:54.180371 140277094741760 logging_writer.py:48] [79222] accumulated_eval_time=18454.988406, accumulated_logging_time=1.123607, accumulated_submission_time=27749.743744, global_step=79222, preemption_count=0, score=27749.743744, test/accuracy=0.690512, test/bleu=29.214265, test/loss=1.426669, test/num_examples=3003, total_duration=46208.384934, train/accuracy=0.660791, train/bleu=33.057592, train/loss=1.607676, validation/accuracy=0.676334, validation/bleu=29.761662, validation/loss=1.502056, validation/num_examples=3000
I0208 19:40:21.698399 140277103134464 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.4070521295070648, loss=1.626254677772522
I0208 19:40:56.603345 140277094741760 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.44678831100463867, loss=1.6085083484649658
I0208 19:41:31.602764 140277103134464 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.4144973158836365, loss=1.5674296617507935
I0208 19:42:06.602409 140277094741760 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.4097055196762085, loss=1.600754737854004
I0208 19:42:41.563688 140277103134464 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.4469304084777832, loss=1.6746764183044434
I0208 19:43:16.524827 140277094741760 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.45541054010391235, loss=1.6026802062988281
I0208 19:43:51.506114 140277103134464 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.47386571764945984, loss=1.6479398012161255
I0208 19:44:26.554687 140277094741760 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.4770730435848236, loss=1.5670686960220337
I0208 19:45:01.520614 140277103134464 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.44258782267570496, loss=1.6609667539596558
I0208 19:45:36.485760 140277094741760 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.4289645254611969, loss=1.6211857795715332
I0208 19:46:11.458390 140277103134464 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.4540254771709442, loss=1.6496576070785522
I0208 19:46:46.444895 140277094741760 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.4298478066921234, loss=1.6275838613510132
I0208 19:47:21.424499 140277103134464 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.43153634667396545, loss=1.6167320013046265
I0208 19:47:56.417935 140277094741760 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.4211031198501587, loss=1.6416476964950562
I0208 19:48:31.391422 140277103134464 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.4312901198863983, loss=1.6534196138381958
I0208 19:49:06.356246 140277094741760 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.44535163044929504, loss=1.584412932395935
I0208 19:49:41.330117 140277103134464 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.41883009672164917, loss=1.6985862255096436
I0208 19:50:16.324287 140277094741760 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.42818939685821533, loss=1.6488879919052124
I0208 19:50:51.313671 140277103134464 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.4231182932853699, loss=1.5672985315322876
I0208 19:51:26.346165 140277094741760 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.4403782784938812, loss=1.5683544874191284
I0208 19:52:01.455981 140277103134464 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.4291856586933136, loss=1.5269283056259155
I0208 19:52:36.473988 140277094741760 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.4418061077594757, loss=1.7513923645019531
I0208 19:53:11.476706 140277103134464 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.4430694282054901, loss=1.576850175857544
I0208 19:53:46.485984 140277094741760 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.4441777169704437, loss=1.6516181230545044
I0208 19:53:54.246869 140446903760704 spec.py:321] Evaluating on the training split.
I0208 19:53:57.261086 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:56:56.159185 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 19:56:58.887063 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 19:59:38.764039 140446903760704 spec.py:349] Evaluating on the test split.
I0208 19:59:41.481116 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 20:02:32.261664 140446903760704 submission_runner.py:408] Time since start: 47566.49s, 	Step: 81624, 	{'train/accuracy': 0.6834427714347839, 'train/loss': 1.4710636138916016, 'train/bleu': 34.51069994560347, 'validation/accuracy': 0.6774373650550842, 'validation/loss': 1.4913984537124634, 'validation/bleu': 29.57352650655724, 'validation/num_examples': 3000, 'test/accuracy': 0.6915345191955566, 'test/loss': 1.4193542003631592, 'test/bleu': 29.30637132482035, 'test/num_examples': 3003, 'score': 28589.722110033035, 'total_duration': 47566.49417424202, 'accumulated_submission_time': 28589.722110033035, 'accumulated_eval_time': 18973.0031478405, 'accumulated_logging_time': 1.1630005836486816}
I0208 20:02:32.288434 140277103134464 logging_writer.py:48] [81624] accumulated_eval_time=18973.003148, accumulated_logging_time=1.163001, accumulated_submission_time=28589.722110, global_step=81624, preemption_count=0, score=28589.722110, test/accuracy=0.691535, test/bleu=29.306371, test/loss=1.419354, test/num_examples=3003, total_duration=47566.494174, train/accuracy=0.683443, train/bleu=34.510700, train/loss=1.471064, validation/accuracy=0.677437, validation/bleu=29.573527, validation/loss=1.491398, validation/num_examples=3000
I0208 20:02:59.114185 140277094741760 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.42933255434036255, loss=1.5870097875595093
I0208 20:03:34.035163 140277103134464 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.4423476457595825, loss=1.6590663194656372
I0208 20:04:08.978770 140277094741760 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.4537093937397003, loss=1.6061440706253052
I0208 20:04:43.961508 140277103134464 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4751797318458557, loss=1.5657819509506226
I0208 20:05:18.942372 140277094741760 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.43914249539375305, loss=1.594610333442688
I0208 20:05:53.949571 140277103134464 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.45685404539108276, loss=1.5514737367630005
I0208 20:06:28.957848 140277094741760 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.4436778426170349, loss=1.6723873615264893
I0208 20:07:03.946425 140277103134464 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4462709128856659, loss=1.5914497375488281
I0208 20:07:38.920116 140277094741760 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.4649011790752411, loss=1.5716195106506348
I0208 20:08:13.929078 140277103134464 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.41724568605422974, loss=1.5780543088912964
I0208 20:08:48.916026 140277094741760 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.4097972810268402, loss=1.5993655920028687
I0208 20:09:23.913109 140277103134464 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.4369674324989319, loss=1.5588246583938599
I0208 20:09:58.885599 140277094741760 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.4352363646030426, loss=1.6271284818649292
I0208 20:10:33.843729 140277103134464 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.4277024567127228, loss=1.5479342937469482
I0208 20:11:08.845188 140277094741760 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.429324746131897, loss=1.567176103591919
I0208 20:11:43.837456 140277103134464 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.4574207663536072, loss=1.5922654867172241
I0208 20:12:18.848449 140277094741760 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.45160144567489624, loss=1.56448233127594
I0208 20:12:53.806936 140277103134464 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.474067360162735, loss=1.589392066001892
I0208 20:13:28.803632 140277094741760 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.4815664291381836, loss=1.5822844505310059
I0208 20:14:03.801512 140277103134464 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.47362133860588074, loss=1.6210908889770508
I0208 20:14:38.775612 140277094741760 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.49904197454452515, loss=1.6593918800354004
I0208 20:15:13.773989 140277103134464 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.4473736882209778, loss=1.6785941123962402
I0208 20:15:48.799529 140277094741760 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.4403332471847534, loss=1.6454167366027832
I0208 20:16:23.778417 140277103134464 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.4748629033565521, loss=1.60140860080719
I0208 20:16:32.266228 140446903760704 spec.py:321] Evaluating on the training split.
I0208 20:16:35.299907 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 20:20:42.725794 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 20:20:45.435572 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 20:23:30.286082 140446903760704 spec.py:349] Evaluating on the test split.
I0208 20:23:32.982745 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 20:26:12.989122 140446903760704 submission_runner.py:408] Time since start: 48987.22s, 	Step: 84026, 	{'train/accuracy': 0.6655948162078857, 'train/loss': 1.5831010341644287, 'train/bleu': 32.66122393028471, 'validation/accuracy': 0.675899863243103, 'validation/loss': 1.4887354373931885, 'validation/bleu': 29.643160583379153, 'validation/num_examples': 3000, 'test/accuracy': 0.6908488869667053, 'test/loss': 1.4123516082763672, 'test/bleu': 29.32195293579914, 'test/num_examples': 3003, 'score': 29429.614921092987, 'total_duration': 48987.2216424942, 'accumulated_submission_time': 29429.614921092987, 'accumulated_eval_time': 19553.72599005699, 'accumulated_logging_time': 1.1998803615570068}
I0208 20:26:13.016706 140277094741760 logging_writer.py:48] [84026] accumulated_eval_time=19553.725990, accumulated_logging_time=1.199880, accumulated_submission_time=29429.614921, global_step=84026, preemption_count=0, score=29429.614921, test/accuracy=0.690849, test/bleu=29.321953, test/loss=1.412352, test/num_examples=3003, total_duration=48987.221642, train/accuracy=0.665595, train/bleu=32.661224, train/loss=1.583101, validation/accuracy=0.675900, validation/bleu=29.643161, validation/loss=1.488735, validation/num_examples=3000
I0208 20:26:39.129161 140277103134464 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.4894108176231384, loss=1.6055620908737183
I0208 20:27:14.027964 140277094741760 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.4724426865577698, loss=1.6142051219940186
I0208 20:27:49.025341 140277103134464 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.8916174173355103, loss=1.6664279699325562
I0208 20:28:24.026116 140277094741760 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.47861048579216003, loss=1.631513237953186
I0208 20:28:58.988876 140277103134464 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.45819351077079773, loss=1.6684070825576782
I0208 20:29:33.972098 140277094741760 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.45777562260627747, loss=1.6042910814285278
I0208 20:30:08.959398 140277103134464 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.42738375067710876, loss=1.541542887687683
I0208 20:30:43.927742 140277094741760 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.47004222869873047, loss=1.5925439596176147
I0208 20:31:18.913038 140277103134464 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4352757930755615, loss=1.587717056274414
I0208 20:31:53.889304 140277094741760 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.46764105558395386, loss=1.6240663528442383
I0208 20:32:28.873322 140277103134464 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4444609582424164, loss=1.5912290811538696
I0208 20:33:03.865619 140277094741760 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.46558064222335815, loss=1.606838345527649
I0208 20:33:38.874349 140277103134464 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.46392175555229187, loss=1.6744391918182373
I0208 20:34:13.837271 140277094741760 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.4698532819747925, loss=1.615742802619934
I0208 20:34:48.810546 140277103134464 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.441686749458313, loss=1.679585337638855
I0208 20:35:23.787057 140277094741760 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.4924183189868927, loss=1.6251795291900635
I0208 20:35:58.771251 140277103134464 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.46085160970687866, loss=1.6181648969650269
I0208 20:36:33.793762 140277094741760 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.47090232372283936, loss=1.6003021001815796
I0208 20:37:08.781956 140277103134464 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.4491727650165558, loss=1.6275254487991333
I0208 20:37:43.749131 140277094741760 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.49077221751213074, loss=1.6593972444534302
I0208 20:38:18.729918 140277103134464 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.46409761905670166, loss=1.6663486957550049
I0208 20:38:53.708155 140277094741760 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.46627578139305115, loss=1.5781774520874023
I0208 20:39:28.757401 140277103134464 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.439985990524292, loss=1.6184399127960205
I0208 20:40:03.922909 140277094741760 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4790390431880951, loss=1.6200107336044312
I0208 20:40:13.090711 140446903760704 spec.py:321] Evaluating on the training split.
I0208 20:40:16.102836 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 20:43:46.046592 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 20:43:48.761628 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 20:46:20.038912 140446903760704 spec.py:349] Evaluating on the test split.
I0208 20:46:22.757194 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 20:48:51.189101 140446903760704 submission_runner.py:408] Time since start: 50345.42s, 	Step: 86428, 	{'train/accuracy': 0.667698323726654, 'train/loss': 1.5811502933502197, 'train/bleu': 33.187502389890206, 'validation/accuracy': 0.6793344020843506, 'validation/loss': 1.481445074081421, 'validation/bleu': 29.7653434375351, 'validation/num_examples': 3000, 'test/accuracy': 0.6907791495323181, 'test/loss': 1.4073405265808105, 'test/bleu': 29.4747702020092, 'test/num_examples': 3003, 'score': 30269.603388786316, 'total_duration': 50345.42161464691, 'accumulated_submission_time': 30269.603388786316, 'accumulated_eval_time': 20071.824310064316, 'accumulated_logging_time': 1.2375590801239014}
I0208 20:48:51.217235 140277103134464 logging_writer.py:48] [86428] accumulated_eval_time=20071.824310, accumulated_logging_time=1.237559, accumulated_submission_time=30269.603389, global_step=86428, preemption_count=0, score=30269.603389, test/accuracy=0.690779, test/bleu=29.474770, test/loss=1.407341, test/num_examples=3003, total_duration=50345.421615, train/accuracy=0.667698, train/bleu=33.187502, train/loss=1.581150, validation/accuracy=0.679334, validation/bleu=29.765343, validation/loss=1.481445, validation/num_examples=3000
I0208 20:49:16.654209 140277094741760 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.4760400652885437, loss=1.6280266046524048
I0208 20:49:51.553135 140277103134464 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4639860689640045, loss=1.6260292530059814
I0208 20:50:26.552887 140277094741760 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.46694135665893555, loss=1.6031197309494019
I0208 20:51:01.551952 140277103134464 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.46756792068481445, loss=1.5409685373306274
I0208 20:51:36.629193 140277094741760 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4772348403930664, loss=1.5734589099884033
I0208 20:52:11.679106 140277103134464 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.4634086489677429, loss=1.5590031147003174
I0208 20:52:46.707720 140277094741760 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.44814175367355347, loss=1.613443374633789
I0208 20:53:21.713101 140277103134464 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.45902004837989807, loss=1.5614972114562988
I0208 20:53:56.754611 140277094741760 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4480046033859253, loss=1.5485903024673462
I0208 20:54:31.894719 140277103134464 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.4584135413169861, loss=1.5669068098068237
I0208 20:55:06.920224 140277094741760 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.45747554302215576, loss=1.5148892402648926
I0208 20:55:41.923599 140277103134464 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.4569004476070404, loss=1.6043686866760254
I0208 20:56:16.909591 140277094741760 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.4329858422279358, loss=1.559389352798462
I0208 20:56:51.919843 140277103134464 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.45871561765670776, loss=1.5216947793960571
I0208 20:57:26.936791 140277094741760 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.46839639544487, loss=1.6008446216583252
I0208 20:58:01.993066 140277103134464 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.468336284160614, loss=1.5865906476974487
I0208 20:58:37.055256 140277094741760 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.46422237157821655, loss=1.6061885356903076
I0208 20:59:12.089553 140277103134464 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.4828687310218811, loss=1.6141166687011719
I0208 20:59:47.107289 140277094741760 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.41901177167892456, loss=1.5851117372512817
I0208 21:00:22.144397 140277103134464 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4507234990596771, loss=1.559915542602539
I0208 21:00:57.193784 140277094741760 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.5042842030525208, loss=1.554667592048645
I0208 21:01:32.217942 140277103134464 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.46447813510894775, loss=1.6232876777648926
I0208 21:02:07.217226 140277094741760 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.515138566493988, loss=1.6343358755111694
I0208 21:02:42.237774 140277103134464 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.459317684173584, loss=1.587744116783142
I0208 21:02:51.421323 140446903760704 spec.py:321] Evaluating on the training split.
I0208 21:02:54.448775 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:06:34.787992 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 21:06:37.502499 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:09:33.843440 140446903760704 spec.py:349] Evaluating on the test split.
I0208 21:09:36.565706 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:12:20.749774 140446903760704 submission_runner.py:408] Time since start: 51754.98s, 	Step: 88828, 	{'train/accuracy': 0.6753185987472534, 'train/loss': 1.5222102403640747, 'train/bleu': 33.65455010911434, 'validation/accuracy': 0.6817150115966797, 'validation/loss': 1.4678661823272705, 'validation/bleu': 30.07307836815615, 'validation/num_examples': 3000, 'test/accuracy': 0.6958921551704407, 'test/loss': 1.3935723304748535, 'test/bleu': 29.70958983317945, 'test/num_examples': 3003, 'score': 31109.718526124954, 'total_duration': 51754.98230290413, 'accumulated_submission_time': 31109.718526124954, 'accumulated_eval_time': 20641.15272283554, 'accumulated_logging_time': 1.2758679389953613}
I0208 21:12:20.777933 140277094741760 logging_writer.py:48] [88828] accumulated_eval_time=20641.152723, accumulated_logging_time=1.275868, accumulated_submission_time=31109.718526, global_step=88828, preemption_count=0, score=31109.718526, test/accuracy=0.695892, test/bleu=29.709590, test/loss=1.393572, test/num_examples=3003, total_duration=51754.982303, train/accuracy=0.675319, train/bleu=33.654550, train/loss=1.522210, validation/accuracy=0.681715, validation/bleu=30.073078, validation/loss=1.467866, validation/num_examples=3000
I0208 21:12:46.226047 140277103134464 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4941912591457367, loss=1.569207787513733
I0208 21:13:21.188151 140277094741760 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.45819634199142456, loss=1.5337212085723877
I0208 21:13:56.202883 140277103134464 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.5003814697265625, loss=1.5619052648544312
I0208 21:14:31.237266 140277094741760 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.44499751925468445, loss=1.6143791675567627
I0208 21:15:06.282555 140277103134464 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.4432564377784729, loss=1.4861407279968262
I0208 21:15:41.260353 140277094741760 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.4786068797111511, loss=1.489508867263794
I0208 21:16:16.254719 140277103134464 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4803353548049927, loss=1.6393544673919678
I0208 21:16:51.263882 140277094741760 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.483354777097702, loss=1.6055110692977905
I0208 21:17:26.285717 140277103134464 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4641445577144623, loss=1.615445613861084
I0208 21:18:01.277661 140277094741760 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.48864296078681946, loss=1.550370454788208
I0208 21:18:36.269750 140277103134464 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.49989569187164307, loss=1.6381380558013916
I0208 21:19:11.262062 140277094741760 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.4773963987827301, loss=1.6078273057937622
I0208 21:19:46.283650 140277103134464 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.4704186022281647, loss=1.625237226486206
I0208 21:20:21.295746 140277094741760 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.5202692747116089, loss=1.6041637659072876
I0208 21:20:56.291171 140277103134464 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.4915556013584137, loss=1.641136646270752
I0208 21:21:31.294785 140277094741760 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.5217693448066711, loss=1.612180471420288
I0208 21:22:06.302536 140277103134464 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4888724088668823, loss=1.5479707717895508
I0208 21:22:41.286883 140277094741760 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.48361149430274963, loss=1.6222683191299438
I0208 21:23:16.281884 140277103134464 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.4676693081855774, loss=1.5816413164138794
I0208 21:23:51.279437 140277094741760 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4778262674808502, loss=1.6470705270767212
I0208 21:24:26.314462 140277103134464 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.47253385186195374, loss=1.5638326406478882
I0208 21:25:01.307568 140277094741760 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.47235792875289917, loss=1.575263500213623
I0208 21:25:36.293883 140277103134464 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.5092912912368774, loss=1.4926989078521729
I0208 21:26:11.280522 140277094741760 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.4470209777355194, loss=1.588286280632019
I0208 21:26:20.811049 140446903760704 spec.py:321] Evaluating on the training split.
I0208 21:26:23.846322 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:29:30.419921 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 21:29:33.130394 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:32:15.440688 140446903760704 spec.py:349] Evaluating on the test split.
I0208 21:32:18.147806 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:34:44.490490 140446903760704 submission_runner.py:408] Time since start: 53098.72s, 	Step: 91229, 	{'train/accuracy': 0.6722777485847473, 'train/loss': 1.5488667488098145, 'train/bleu': 33.57607223090198, 'validation/accuracy': 0.6819506287574768, 'validation/loss': 1.4655590057373047, 'validation/bleu': 30.27933128209323, 'validation/num_examples': 3000, 'test/accuracy': 0.697681725025177, 'test/loss': 1.3859319686889648, 'test/bleu': 30.023480420024, 'test/num_examples': 3003, 'score': 31949.66242647171, 'total_duration': 53098.72299027443, 'accumulated_submission_time': 31949.66242647171, 'accumulated_eval_time': 21144.83209013939, 'accumulated_logging_time': 1.315791130065918}
I0208 21:34:44.518368 140277103134464 logging_writer.py:48] [91229] accumulated_eval_time=21144.832090, accumulated_logging_time=1.315791, accumulated_submission_time=31949.662426, global_step=91229, preemption_count=0, score=31949.662426, test/accuracy=0.697682, test/bleu=30.023480, test/loss=1.385932, test/num_examples=3003, total_duration=53098.722990, train/accuracy=0.672278, train/bleu=33.576072, train/loss=1.548867, validation/accuracy=0.681951, validation/bleu=30.279331, validation/loss=1.465559, validation/num_examples=3000
I0208 21:35:09.587696 140277094741760 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.46858078241348267, loss=1.5243533849716187
I0208 21:35:44.481405 140277103134464 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4728536307811737, loss=1.5424039363861084
I0208 21:36:19.463036 140277094741760 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.4794885218143463, loss=1.544147253036499
I0208 21:36:54.441869 140277103134464 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.48657462000846863, loss=1.6210598945617676
I0208 21:37:29.422642 140277094741760 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.5121561288833618, loss=1.5614464282989502
I0208 21:38:04.403845 140277103134464 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4517247974872589, loss=1.5403468608856201
I0208 21:38:39.384307 140277094741760 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.5000340342521667, loss=1.5045766830444336
I0208 21:39:14.371677 140277103134464 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.7134815454483032, loss=1.6134687662124634
I0208 21:39:49.366952 140277094741760 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.4746759831905365, loss=1.5966581106185913
I0208 21:40:24.360271 140277103134464 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.5264050364494324, loss=1.5692239999771118
I0208 21:40:59.366867 140277094741760 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4939337372779846, loss=1.5280396938323975
I0208 21:41:34.366969 140277103134464 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.5003411769866943, loss=1.6360284090042114
I0208 21:42:09.487064 140277094741760 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.5087975859642029, loss=1.547752857208252
I0208 21:42:44.477358 140277103134464 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.4779728353023529, loss=1.542638897895813
I0208 21:43:19.478179 140277094741760 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.48515093326568604, loss=1.5282087326049805
I0208 21:43:54.453666 140277103134464 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.48726198077201843, loss=1.5477581024169922
I0208 21:44:29.455261 140277094741760 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.5128632187843323, loss=1.5186208486557007
I0208 21:45:04.452458 140277103134464 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.5238050222396851, loss=1.6145185232162476
I0208 21:45:39.433554 140277094741760 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.521086573600769, loss=1.6387875080108643
I0208 21:46:14.446251 140277103134464 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.4659101963043213, loss=1.5865432024002075
I0208 21:46:49.470338 140277094741760 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4887472093105316, loss=1.574231743812561
I0208 21:47:24.431415 140277103134464 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4896126985549927, loss=1.5668959617614746
I0208 21:47:59.456529 140277094741760 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.4869012236595154, loss=1.5166503190994263
I0208 21:48:34.437718 140277103134464 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.49534228444099426, loss=1.5304646492004395
I0208 21:48:44.643326 140446903760704 spec.py:321] Evaluating on the training split.
I0208 21:48:47.648830 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:52:27.768192 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 21:52:30.484426 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:54:55.137664 140446903760704 spec.py:349] Evaluating on the test split.
I0208 21:54:57.838798 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 21:57:27.857130 140446903760704 submission_runner.py:408] Time since start: 54462.09s, 	Step: 93631, 	{'train/accuracy': 0.6707874536514282, 'train/loss': 1.561752438545227, 'train/bleu': 33.5174534912928, 'validation/accuracy': 0.6827317476272583, 'validation/loss': 1.4605400562286377, 'validation/bleu': 30.069448126420095, 'validation/num_examples': 3000, 'test/accuracy': 0.7006217241287231, 'test/loss': 1.3737126588821411, 'test/bleu': 30.232836229992873, 'test/num_examples': 3003, 'score': 32789.70082950592, 'total_duration': 54462.08965873718, 'accumulated_submission_time': 32789.70082950592, 'accumulated_eval_time': 21668.045841693878, 'accumulated_logging_time': 1.3538849353790283}
I0208 21:57:27.885853 140277094741760 logging_writer.py:48] [93631] accumulated_eval_time=21668.045842, accumulated_logging_time=1.353885, accumulated_submission_time=32789.700830, global_step=93631, preemption_count=0, score=32789.700830, test/accuracy=0.700622, test/bleu=30.232836, test/loss=1.373713, test/num_examples=3003, total_duration=54462.089659, train/accuracy=0.670787, train/bleu=33.517453, train/loss=1.561752, validation/accuracy=0.682732, validation/bleu=30.069448, validation/loss=1.460540, validation/num_examples=3000
I0208 21:57:52.269321 140277103134464 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.483768105506897, loss=1.5086355209350586
I0208 21:58:27.159255 140277094741760 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.49462875723838806, loss=1.4821436405181885
I0208 21:59:02.139726 140277103134464 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.5233625769615173, loss=1.587215542793274
I0208 21:59:37.127575 140277094741760 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.4887191653251648, loss=1.5105026960372925
I0208 22:00:12.140420 140277103134464 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.4931721091270447, loss=1.5227234363555908
I0208 22:00:47.159326 140277094741760 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.5148041248321533, loss=1.5657552480697632
I0208 22:01:22.165480 140277103134464 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.5008044838905334, loss=1.579221248626709
I0208 22:01:57.161360 140277094741760 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.47661033272743225, loss=1.5658116340637207
I0208 22:02:32.153707 140277103134464 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.5158280730247498, loss=1.5828098058700562
I0208 22:03:07.148035 140277094741760 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.4933460056781769, loss=1.604196548461914
I0208 22:03:42.138859 140277103134464 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.4948694109916687, loss=1.5450044870376587
I0208 22:04:17.138234 140277094741760 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.504848301410675, loss=1.5221364498138428
I0208 22:04:52.113508 140277103134464 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.5101093649864197, loss=1.5335664749145508
I0208 22:05:27.071338 140277094741760 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.5102763175964355, loss=1.5069832801818848
I0208 22:06:02.068684 140277103134464 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.5342491269111633, loss=1.5818469524383545
I0208 22:06:37.072302 140277094741760 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.49315449595451355, loss=1.6000285148620605
I0208 22:07:12.097139 140277103134464 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.5275527834892273, loss=1.5737639665603638
I0208 22:07:47.097835 140277094741760 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.5200287699699402, loss=1.472583293914795
I0208 22:08:22.079800 140277103134464 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.5262969732284546, loss=1.4820469617843628
I0208 22:08:57.070379 140277094741760 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.5107043981552124, loss=1.5109622478485107
I0208 22:09:32.039371 140277103134464 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.5304380655288696, loss=1.6294997930526733
I0208 22:10:07.053696 140277094741760 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.5034475326538086, loss=1.5980775356292725
I0208 22:10:42.014425 140277103134464 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.5535343885421753, loss=1.6355650424957275
I0208 22:11:17.005099 140277094741760 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5245822072029114, loss=1.5257281064987183
I0208 22:11:27.927646 140446903760704 spec.py:321] Evaluating on the training split.
I0208 22:11:30.938451 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 22:15:16.589670 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 22:15:19.289338 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 22:17:47.689881 140446903760704 spec.py:349] Evaluating on the test split.
I0208 22:17:50.398817 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 22:20:25.725440 140446903760704 submission_runner.py:408] Time since start: 55839.96s, 	Step: 96033, 	{'train/accuracy': 0.6781882047653198, 'train/loss': 1.5128364562988281, 'train/bleu': 34.07090496618493, 'validation/accuracy': 0.6862531304359436, 'validation/loss': 1.4463741779327393, 'validation/bleu': 30.29679901504323, 'validation/num_examples': 3000, 'test/accuracy': 0.6993434429168701, 'test/loss': 1.370069980621338, 'test/bleu': 30.150542856499595, 'test/num_examples': 3003, 'score': 33629.65852046013, 'total_duration': 55839.95797109604, 'accumulated_submission_time': 33629.65852046013, 'accumulated_eval_time': 22205.843585014343, 'accumulated_logging_time': 1.392345666885376}
I0208 22:20:25.754021 140277103134464 logging_writer.py:48] [96033] accumulated_eval_time=22205.843585, accumulated_logging_time=1.392346, accumulated_submission_time=33629.658520, global_step=96033, preemption_count=0, score=33629.658520, test/accuracy=0.699343, test/bleu=30.150543, test/loss=1.370070, test/num_examples=3003, total_duration=55839.957971, train/accuracy=0.678188, train/bleu=34.070905, train/loss=1.512836, validation/accuracy=0.686253, validation/bleu=30.296799, validation/loss=1.446374, validation/num_examples=3000
I0208 22:20:49.415234 140277094741760 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.5177626609802246, loss=1.5153849124908447
I0208 22:21:24.308281 140277103134464 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.5232622027397156, loss=1.5011982917785645
I0208 22:21:59.289879 140277094741760 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.5220308303833008, loss=1.5916334390640259
I0208 22:22:34.284031 140277103134464 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5462627410888672, loss=1.5969891548156738
I0208 22:23:09.277318 140277094741760 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.5219160318374634, loss=1.5064113140106201
I0208 22:23:44.251976 140277103134464 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.548805832862854, loss=1.599156379699707
I0208 22:24:19.218475 140277094741760 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.5316649675369263, loss=1.507017970085144
I0208 22:24:54.216445 140277103134464 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5568400025367737, loss=1.6093759536743164
I0208 22:25:29.214448 140277094741760 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.5242481827735901, loss=1.553173542022705
I0208 22:26:04.205270 140277103134464 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5259813070297241, loss=1.5475289821624756
I0208 22:26:39.188769 140277094741760 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.5173699855804443, loss=1.4897195100784302
I0208 22:27:14.211070 140277103134464 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.5446057915687561, loss=1.4915329217910767
I0208 22:27:49.206796 140277094741760 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5388617515563965, loss=1.5507965087890625
I0208 22:28:24.187478 140277103134464 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5285695791244507, loss=1.5420992374420166
I0208 22:28:59.205270 140277094741760 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.5117732882499695, loss=1.5497543811798096
I0208 22:29:34.195963 140277103134464 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.5207184553146362, loss=1.5445947647094727
I0208 22:30:09.184354 140277094741760 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.5385547876358032, loss=1.524940848350525
I0208 22:30:44.177568 140277103134464 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.5437282919883728, loss=1.5810974836349487
I0208 22:31:19.183788 140277094741760 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.5170660614967346, loss=1.510237455368042
I0208 22:31:54.228986 140277103134464 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.541314959526062, loss=1.6088387966156006
I0208 22:32:29.243250 140277094741760 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5388451218605042, loss=1.528929352760315
I0208 22:33:04.249150 140277103134464 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.5275395512580872, loss=1.5648554563522339
I0208 22:33:39.262125 140277094741760 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5635348558425903, loss=1.595198631286621
I0208 22:34:14.237737 140277103134464 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.5193629264831543, loss=1.457290768623352
I0208 22:34:25.860729 140446903760704 spec.py:321] Evaluating on the training split.
I0208 22:34:28.887470 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 22:38:13.381689 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 22:38:16.091777 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 22:40:41.107219 140446903760704 spec.py:349] Evaluating on the test split.
I0208 22:40:43.822917 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 22:43:19.865795 140446903760704 submission_runner.py:408] Time since start: 57214.10s, 	Step: 98435, 	{'train/accuracy': 0.6749489307403564, 'train/loss': 1.5274507999420166, 'train/bleu': 33.67612239774069, 'validation/accuracy': 0.6862407326698303, 'validation/loss': 1.4482619762420654, 'validation/bleu': 30.31411470974103, 'validation/num_examples': 3000, 'test/accuracy': 0.7017953991889954, 'test/loss': 1.3655441999435425, 'test/bleu': 30.476222430731852, 'test/num_examples': 3003, 'score': 34469.67932343483, 'total_duration': 57214.098304986954, 'accumulated_submission_time': 34469.67932343483, 'accumulated_eval_time': 22739.84860920906, 'accumulated_logging_time': 1.4309487342834473}
I0208 22:43:19.894281 140277094741760 logging_writer.py:48] [98435] accumulated_eval_time=22739.848609, accumulated_logging_time=1.430949, accumulated_submission_time=34469.679323, global_step=98435, preemption_count=0, score=34469.679323, test/accuracy=0.701795, test/bleu=30.476222, test/loss=1.365544, test/num_examples=3003, total_duration=57214.098305, train/accuracy=0.674949, train/bleu=33.676122, train/loss=1.527451, validation/accuracy=0.686241, validation/bleu=30.314115, validation/loss=1.448262, validation/num_examples=3000
I0208 22:43:42.896854 140277103134464 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5513843297958374, loss=1.543779969215393
I0208 22:44:17.822354 140277094741760 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5269468426704407, loss=1.5189080238342285
I0208 22:44:52.784092 140277103134464 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5526949167251587, loss=1.493000864982605
I0208 22:45:27.768200 140277094741760 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.549500584602356, loss=1.6120636463165283
I0208 22:46:02.785064 140277103134464 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5373942852020264, loss=1.585892915725708
I0208 22:46:37.778769 140277094741760 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5208958387374878, loss=1.5304405689239502
I0208 22:47:12.766900 140277103134464 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5278962254524231, loss=1.5128084421157837
I0208 22:47:47.778707 140277094741760 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5509051084518433, loss=1.4867966175079346
I0208 22:48:22.784025 140277103134464 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5368672013282776, loss=1.5195270776748657
I0208 22:48:57.755484 140277094741760 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5601645708084106, loss=1.4947059154510498
I0208 22:49:32.766432 140277103134464 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.551811695098877, loss=1.5191391706466675
I0208 22:50:07.773163 140277094741760 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5294421911239624, loss=1.5200061798095703
I0208 22:50:42.840634 140277103134464 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.563737690448761, loss=1.4492582082748413
I0208 22:51:17.882294 140277094741760 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5683477520942688, loss=1.5475636720657349
I0208 22:51:52.909336 140277103134464 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5682353377342224, loss=1.4556835889816284
I0208 22:52:27.896564 140277094741760 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5451533198356628, loss=1.473278284072876
I0208 22:53:02.925598 140277103134464 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5668576955795288, loss=1.5679600238800049
I0208 22:53:37.916465 140277094741760 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.536118745803833, loss=1.5089771747589111
I0208 22:54:12.936356 140277103134464 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5541114807128906, loss=1.472839117050171
I0208 22:54:47.896511 140277094741760 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5928985476493835, loss=1.5476083755493164
I0208 22:55:22.918802 140277103134464 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.6066078543663025, loss=1.5228619575500488
I0208 22:55:58.058839 140277094741760 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5451213121414185, loss=1.4983781576156616
I0208 22:56:33.055310 140277103134464 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5763707160949707, loss=1.4947468042373657
I0208 22:57:08.008016 140277094741760 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5690751671791077, loss=1.4891374111175537
I0208 22:57:19.958967 140446903760704 spec.py:321] Evaluating on the training split.
I0208 22:57:22.966895 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:00:32.703797 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 23:00:35.410037 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:03:02.989673 140446903760704 spec.py:349] Evaluating on the test split.
I0208 23:03:05.717533 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:05:32.573467 140446903760704 submission_runner.py:408] Time since start: 58546.81s, 	Step: 100836, 	{'train/accuracy': 0.6923024654388428, 'train/loss': 1.423957109451294, 'train/bleu': 35.541552632794804, 'validation/accuracy': 0.6876789927482605, 'validation/loss': 1.4401698112487793, 'validation/bleu': 30.672736533118787, 'validation/num_examples': 3000, 'test/accuracy': 0.7013770341873169, 'test/loss': 1.360811710357666, 'test/bleu': 30.386893649507694, 'test/num_examples': 3003, 'score': 35309.65514802933, 'total_duration': 58546.80599832535, 'accumulated_submission_time': 35309.65514802933, 'accumulated_eval_time': 23232.463057994843, 'accumulated_logging_time': 1.4708812236785889}
I0208 23:05:32.602476 140277103134464 logging_writer.py:48] [100836] accumulated_eval_time=23232.463058, accumulated_logging_time=1.470881, accumulated_submission_time=35309.655148, global_step=100836, preemption_count=0, score=35309.655148, test/accuracy=0.701377, test/bleu=30.386894, test/loss=1.360812, test/num_examples=3003, total_duration=58546.805998, train/accuracy=0.692302, train/bleu=35.541553, train/loss=1.423957, validation/accuracy=0.687679, validation/bleu=30.672737, validation/loss=1.440170, validation/num_examples=3000
I0208 23:05:55.247548 140277094741760 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5694278478622437, loss=1.528630018234253
I0208 23:06:30.110515 140277103134464 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5855680108070374, loss=1.5462573766708374
I0208 23:07:05.105906 140277094741760 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5453171730041504, loss=1.5263943672180176
I0208 23:07:40.128365 140277103134464 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.604102373123169, loss=1.544656753540039
I0208 23:08:15.111785 140277094741760 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.579744279384613, loss=1.581418752670288
I0208 23:08:50.092214 140277103134464 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5627269148826599, loss=1.5386337041854858
I0208 23:09:25.069810 140277094741760 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5514959692955017, loss=1.5270360708236694
I0208 23:10:00.048045 140277103134464 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5853066444396973, loss=1.4946627616882324
I0208 23:10:35.074777 140277094741760 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5911270380020142, loss=1.4899547100067139
I0208 23:11:10.086059 140277103134464 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5574225187301636, loss=1.5365291833877563
I0208 23:11:45.085519 140277094741760 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5759463310241699, loss=1.59799325466156
I0208 23:12:20.069682 140277103134464 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5650366544723511, loss=1.530766248703003
I0208 23:12:55.062994 140277094741760 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5912840366363525, loss=1.5029709339141846
I0208 23:13:30.044106 140277103134464 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5959970355033875, loss=1.5426809787750244
I0208 23:14:05.045113 140277094741760 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5801517963409424, loss=1.476635456085205
I0208 23:14:40.042209 140277103134464 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5924121737480164, loss=1.5087909698486328
I0208 23:15:15.052698 140277094741760 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5734434127807617, loss=1.498812198638916
I0208 23:15:50.049727 140277103134464 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5784280300140381, loss=1.5096447467803955
I0208 23:16:25.107191 140277094741760 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.6050437092781067, loss=1.6215932369232178
I0208 23:17:00.132002 140277103134464 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5901435017585754, loss=1.5378905534744263
I0208 23:17:35.146405 140277094741760 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.604099690914154, loss=1.528521180152893
I0208 23:18:10.141388 140277103134464 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5804471373558044, loss=1.5138362646102905
I0208 23:18:45.153960 140277094741760 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.6046651601791382, loss=1.4900680780410767
I0208 23:19:20.154380 140277103134464 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.583228349685669, loss=1.5694738626480103
I0208 23:19:32.813818 140446903760704 spec.py:321] Evaluating on the training split.
I0208 23:19:35.828661 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:23:14.868305 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 23:23:17.588122 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:25:51.814994 140446903760704 spec.py:349] Evaluating on the test split.
I0208 23:25:54.537648 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:28:39.493166 140446903760704 submission_runner.py:408] Time since start: 59933.73s, 	Step: 103238, 	{'train/accuracy': 0.6829273700714111, 'train/loss': 1.4789576530456543, 'train/bleu': 34.769351800976146, 'validation/accuracy': 0.6884477734565735, 'validation/loss': 1.4318045377731323, 'validation/bleu': 30.403672649418127, 'validation/num_examples': 3000, 'test/accuracy': 0.7033408880233765, 'test/loss': 1.354027271270752, 'test/bleu': 30.52896680762646, 'test/num_examples': 3003, 'score': 36149.77913951874, 'total_duration': 59933.7256834507, 'accumulated_submission_time': 36149.77913951874, 'accumulated_eval_time': 23779.142338991165, 'accumulated_logging_time': 1.5111699104309082}
I0208 23:28:39.522373 140277094741760 logging_writer.py:48] [103238] accumulated_eval_time=23779.142339, accumulated_logging_time=1.511170, accumulated_submission_time=36149.779140, global_step=103238, preemption_count=0, score=36149.779140, test/accuracy=0.703341, test/bleu=30.528967, test/loss=1.354027, test/num_examples=3003, total_duration=59933.725683, train/accuracy=0.682927, train/bleu=34.769352, train/loss=1.478958, validation/accuracy=0.688448, validation/bleu=30.403673, validation/loss=1.431805, validation/num_examples=3000
I0208 23:29:01.454962 140277103134464 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5974767208099365, loss=1.5661598443984985
I0208 23:29:36.372205 140277094741760 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5888834595680237, loss=1.536657691001892
I0208 23:30:11.366590 140277103134464 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5999317169189453, loss=1.4693464040756226
I0208 23:30:46.341193 140277094741760 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5884165167808533, loss=1.4843037128448486
I0208 23:31:21.419571 140277103134464 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5902553200721741, loss=1.4428977966308594
I0208 23:31:56.401101 140277094741760 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5874253511428833, loss=1.551974892616272
I0208 23:32:31.386850 140277103134464 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.6058536171913147, loss=1.5026684999465942
I0208 23:33:06.383662 140277094741760 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5824837684631348, loss=1.5248104333877563
I0208 23:33:41.360973 140277103134464 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.6048496961593628, loss=1.4330718517303467
I0208 23:34:16.381413 140277094741760 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.6186171770095825, loss=1.513832688331604
I0208 23:34:51.384936 140277103134464 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.6175196766853333, loss=1.4808166027069092
I0208 23:35:26.381516 140277094741760 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.6055684685707092, loss=1.5545231103897095
I0208 23:36:01.352328 140277103134464 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.6033859252929688, loss=1.5054047107696533
I0208 23:36:36.308368 140277094741760 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5708840489387512, loss=1.4507254362106323
I0208 23:37:11.331920 140277103134464 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.5991630554199219, loss=1.4740710258483887
I0208 23:37:46.311740 140277094741760 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.6300146579742432, loss=1.4806382656097412
I0208 23:38:21.307259 140277103134464 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.6230242848396301, loss=1.4789395332336426
I0208 23:38:56.331291 140277094741760 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.6140162944793701, loss=1.4414249658584595
I0208 23:39:31.336517 140277103134464 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.6172968745231628, loss=1.510085105895996
I0208 23:40:06.337338 140277094741760 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.6121927499771118, loss=1.5205795764923096
I0208 23:40:41.306846 140277103134464 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.6125925183296204, loss=1.5093098878860474
I0208 23:41:16.277696 140277094741760 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.6143298745155334, loss=1.4613169431686401
I0208 23:41:51.271500 140277103134464 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.6449404358863831, loss=1.492610216140747
I0208 23:42:26.317512 140277094741760 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.6163681149482727, loss=1.4593757390975952
I0208 23:42:39.719415 140446903760704 spec.py:321] Evaluating on the training split.
I0208 23:42:42.746186 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:45:48.080377 140446903760704 spec.py:333] Evaluating on the validation split.
I0208 23:45:50.796158 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:48:20.961580 140446903760704 spec.py:349] Evaluating on the test split.
I0208 23:48:23.682655 140446903760704 workload.py:181] Translating evaluation dataset.
I0208 23:50:58.178083 140446903760704 submission_runner.py:408] Time since start: 61272.41s, 	Step: 105640, 	{'train/accuracy': 0.6826414465904236, 'train/loss': 1.486212134361267, 'train/bleu': 34.5595366890984, 'validation/accuracy': 0.689315676689148, 'validation/loss': 1.428796410560608, 'validation/bleu': 30.46267469124044, 'validation/num_examples': 3000, 'test/accuracy': 0.7058508992195129, 'test/loss': 1.3480515480041504, 'test/bleu': 30.580010769399582, 'test/num_examples': 3003, 'score': 36989.887207746506, 'total_duration': 61272.41058278084, 'accumulated_submission_time': 36989.887207746506, 'accumulated_eval_time': 24277.60093665123, 'accumulated_logging_time': 1.552889108657837}
I0208 23:50:58.215718 140277103134464 logging_writer.py:48] [105640] accumulated_eval_time=24277.600937, accumulated_logging_time=1.552889, accumulated_submission_time=36989.887208, global_step=105640, preemption_count=0, score=36989.887208, test/accuracy=0.705851, test/bleu=30.580011, test/loss=1.348052, test/num_examples=3003, total_duration=61272.410583, train/accuracy=0.682641, train/bleu=34.559537, train/loss=1.486212, validation/accuracy=0.689316, validation/bleu=30.462675, validation/loss=1.428796, validation/num_examples=3000
I0208 23:51:19.462917 140277094741760 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.6460086703300476, loss=1.437667965888977
I0208 23:51:54.339043 140277103134464 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.6242428421974182, loss=1.4939321279525757
I0208 23:52:29.303084 140277094741760 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.6218510866165161, loss=1.5547688007354736
I0208 23:53:04.256712 140277103134464 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.6355409622192383, loss=1.5094190835952759
I0208 23:53:39.230850 140277094741760 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.5913273096084595, loss=1.4679431915283203
I0208 23:54:14.217174 140277103134464 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.6419844031333923, loss=1.4738792181015015
I0208 23:54:49.158397 140277094741760 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.6343085765838623, loss=1.499485969543457
I0208 23:55:24.203723 140277103134464 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.6288139224052429, loss=1.4624691009521484
I0208 23:55:59.194477 140277094741760 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.656502902507782, loss=1.4775652885437012
I0208 23:56:34.165353 140277103134464 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.6058529019355774, loss=1.4255263805389404
I0208 23:57:09.185266 140277094741760 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.6146882772445679, loss=1.4511284828186035
I0208 23:57:44.174576 140277103134464 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.620205819606781, loss=1.4758260250091553
I0208 23:58:19.128541 140277094741760 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.6675903797149658, loss=1.4394441843032837
I0208 23:58:54.110358 140277103134464 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.6527464389801025, loss=1.5794609785079956
I0208 23:59:29.115767 140277094741760 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.6704295873641968, loss=1.4710302352905273
I0209 00:00:04.114135 140277103134464 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.630375325679779, loss=1.4787359237670898
I0209 00:00:39.084713 140277094741760 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.6586799025535583, loss=1.5008331537246704
I0209 00:01:14.051315 140277103134464 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.6415772438049316, loss=1.5267170667648315
I0209 00:01:49.058198 140277094741760 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.645514726638794, loss=1.3903162479400635
I0209 00:02:24.055045 140277103134464 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.6360936164855957, loss=1.4614633321762085
I0209 00:02:59.024881 140277094741760 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.6209397315979004, loss=1.401614785194397
I0209 00:03:34.020394 140277103134464 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.626803994178772, loss=1.493999719619751
I0209 00:04:09.010527 140277094741760 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.6563623547554016, loss=1.4185439348220825
I0209 00:04:43.976673 140277103134464 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.6545577645301819, loss=1.482347846031189
I0209 00:04:58.378813 140446903760704 spec.py:321] Evaluating on the training split.
I0209 00:05:01.385577 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:08:21.334544 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 00:08:24.033115 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:10:52.271199 140446903760704 spec.py:349] Evaluating on the test split.
I0209 00:10:55.012275 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:13:14.299496 140446903760704 submission_runner.py:408] Time since start: 62608.53s, 	Step: 108043, 	{'train/accuracy': 0.6920853853225708, 'train/loss': 1.4408038854599, 'train/bleu': 35.49696534352309, 'validation/accuracy': 0.6909027695655823, 'validation/loss': 1.4207086563110352, 'validation/bleu': 30.654077905274068, 'validation/num_examples': 3000, 'test/accuracy': 0.7048399448394775, 'test/loss': 1.3408323526382446, 'test/bleu': 30.626020680832532, 'test/num_examples': 3003, 'score': 37829.96246767044, 'total_duration': 62608.53202152252, 'accumulated_submission_time': 37829.96246767044, 'accumulated_eval_time': 24773.52156305313, 'accumulated_logging_time': 1.6027710437774658}
I0209 00:13:14.330712 140277094741760 logging_writer.py:48] [108043] accumulated_eval_time=24773.521563, accumulated_logging_time=1.602771, accumulated_submission_time=37829.962468, global_step=108043, preemption_count=0, score=37829.962468, test/accuracy=0.704840, test/bleu=30.626021, test/loss=1.340832, test/num_examples=3003, total_duration=62608.532022, train/accuracy=0.692085, train/bleu=35.496965, train/loss=1.440804, validation/accuracy=0.690903, validation/bleu=30.654078, validation/loss=1.420709, validation/num_examples=3000
I0209 00:13:34.519376 140277103134464 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.6605352163314819, loss=1.450263261795044
I0209 00:14:09.399693 140277094741760 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.669048011302948, loss=1.483414649963379
I0209 00:14:44.335180 140277103134464 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.6429360508918762, loss=1.497275710105896
I0209 00:15:19.306058 140277094741760 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.6470190286636353, loss=1.471016526222229
I0209 00:15:54.295796 140277103134464 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.665596067905426, loss=1.3915510177612305
I0209 00:16:29.247061 140277094741760 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.649429976940155, loss=1.4791231155395508
I0209 00:17:04.221567 140277103134464 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.6631518602371216, loss=1.4255574941635132
I0209 00:17:39.220334 140277094741760 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.6536819338798523, loss=1.3806630373001099
I0209 00:18:14.245372 140277103134464 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6793937087059021, loss=1.4436997175216675
I0209 00:18:49.227339 140277094741760 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6704161763191223, loss=1.4530543088912964
I0209 00:19:24.225279 140277103134464 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.6664243340492249, loss=1.423600435256958
I0209 00:19:59.188758 140277094741760 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.6768409013748169, loss=1.4517656564712524
I0209 00:20:34.178451 140277103134464 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.6885409355163574, loss=1.4384626150131226
I0209 00:21:09.176754 140277094741760 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.68642657995224, loss=1.4940130710601807
I0209 00:21:44.165403 140277103134464 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.6844320893287659, loss=1.5116015672683716
I0209 00:22:19.135495 140277094741760 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.6448590159416199, loss=1.4017882347106934
I0209 00:22:54.163332 140277103134464 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.6285076141357422, loss=1.467244029045105
I0209 00:23:29.174436 140277094741760 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.6775137782096863, loss=1.4408352375030518
I0209 00:24:04.181292 140277103134464 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6542152762413025, loss=1.4670944213867188
I0209 00:24:39.181378 140277094741760 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6868050694465637, loss=1.450402855873108
I0209 00:25:14.153478 140277103134464 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6917072534561157, loss=1.4411303997039795
I0209 00:25:49.134092 140277094741760 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.6941816806793213, loss=1.4893312454223633
I0209 00:26:24.101630 140277103134464 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.7081184983253479, loss=1.386491298675537
I0209 00:26:59.088100 140277094741760 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6892614364624023, loss=1.43601393699646
I0209 00:27:14.547616 140446903760704 spec.py:321] Evaluating on the training split.
I0209 00:27:17.564242 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:30:40.836763 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 00:30:43.560438 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:33:13.389498 140446903760704 spec.py:349] Evaluating on the test split.
I0209 00:33:16.111152 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:35:46.613291 140446903760704 submission_runner.py:408] Time since start: 63960.85s, 	Step: 110446, 	{'train/accuracy': 0.6905348300933838, 'train/loss': 1.441590428352356, 'train/bleu': 34.91691907999978, 'validation/accuracy': 0.6907168030738831, 'validation/loss': 1.420137882232666, 'validation/bleu': 30.70651994536399, 'validation/num_examples': 3000, 'test/accuracy': 0.7077334523200989, 'test/loss': 1.335753321647644, 'test/bleu': 30.601082920116674, 'test/num_examples': 3003, 'score': 38670.09367990494, 'total_duration': 63960.845781326294, 'accumulated_submission_time': 38670.09367990494, 'accumulated_eval_time': 25285.587152004242, 'accumulated_logging_time': 1.6442315578460693}
I0209 00:35:46.651491 140277103134464 logging_writer.py:48] [110446] accumulated_eval_time=25285.587152, accumulated_logging_time=1.644232, accumulated_submission_time=38670.093680, global_step=110446, preemption_count=0, score=38670.093680, test/accuracy=0.707733, test/bleu=30.601083, test/loss=1.335753, test/num_examples=3003, total_duration=63960.845781, train/accuracy=0.690535, train/bleu=34.916919, train/loss=1.441590, validation/accuracy=0.690717, validation/bleu=30.706520, validation/loss=1.420138, validation/num_examples=3000
I0209 00:36:05.785536 140277094741760 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6878336071968079, loss=1.5353630781173706
I0209 00:36:40.683939 140277103134464 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6946024298667908, loss=1.4207051992416382
I0209 00:37:15.651772 140277094741760 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.7173557281494141, loss=1.5011284351348877
I0209 00:37:50.609435 140277103134464 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.7164285182952881, loss=1.454996109008789
I0209 00:38:25.580912 140277094741760 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.6985065937042236, loss=1.480910301208496
I0209 00:39:00.615340 140277103134464 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.7068184018135071, loss=1.520263910293579
I0209 00:39:35.594362 140277094741760 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6558882594108582, loss=1.3854917287826538
I0209 00:40:10.566065 140277103134464 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.6839519739151001, loss=1.3546857833862305
I0209 00:40:45.566329 140277094741760 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.688642144203186, loss=1.4757330417633057
I0209 00:41:20.575833 140277103134464 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.7221096754074097, loss=1.4338288307189941
I0209 00:41:55.545007 140277094741760 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6993352174758911, loss=1.4702965021133423
I0209 00:42:30.530557 140277103134464 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.7140002846717834, loss=1.4151043891906738
I0209 00:43:05.531354 140277094741760 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.7099490165710449, loss=1.3854869604110718
I0209 00:43:40.488538 140277103134464 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.6854851245880127, loss=1.4527952671051025
I0209 00:44:15.470297 140277094741760 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.728606104850769, loss=1.401935338973999
I0209 00:44:50.430465 140277103134464 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6808499693870544, loss=1.4075915813446045
I0209 00:45:25.456080 140277094741760 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.7002830505371094, loss=1.4278894662857056
I0209 00:46:00.441748 140277103134464 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.6934598684310913, loss=1.4166990518569946
I0209 00:46:35.451846 140277094741760 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6872325539588928, loss=1.4590777158737183
I0209 00:47:10.432600 140277103134464 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.6949765682220459, loss=1.4397515058517456
I0209 00:47:45.422421 140277094741760 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.7453213334083557, loss=1.5136005878448486
I0209 00:48:20.422767 140277103134464 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.7480453848838806, loss=1.496829867362976
I0209 00:48:55.443666 140277094741760 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.7038567066192627, loss=1.4320372343063354
I0209 00:49:30.453402 140277103134464 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.7045450806617737, loss=1.3712042570114136
I0209 00:49:46.624964 140446903760704 spec.py:321] Evaluating on the training split.
I0209 00:49:49.636782 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:53:13.362681 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 00:53:16.081355 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:55:47.638168 140446903760704 spec.py:349] Evaluating on the test split.
I0209 00:55:50.379818 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 00:58:39.757254 140446903760704 submission_runner.py:408] Time since start: 65333.99s, 	Step: 112848, 	{'train/accuracy': 0.7087449431419373, 'train/loss': 1.3407204151153564, 'train/bleu': 36.47203853513563, 'validation/accuracy': 0.691969096660614, 'validation/loss': 1.416434407234192, 'validation/bleu': 31.080921156801807, 'validation/num_examples': 3000, 'test/accuracy': 0.7069665193557739, 'test/loss': 1.3334004878997803, 'test/bleu': 30.83949578884752, 'test/num_examples': 3003, 'score': 39509.97726178169, 'total_duration': 65333.98978304863, 'accumulated_submission_time': 39509.97726178169, 'accumulated_eval_time': 25818.719385623932, 'accumulated_logging_time': 1.6956148147583008}
I0209 00:58:39.787024 140277094741760 logging_writer.py:48] [112848] accumulated_eval_time=25818.719386, accumulated_logging_time=1.695615, accumulated_submission_time=39509.977262, global_step=112848, preemption_count=0, score=39509.977262, test/accuracy=0.706967, test/bleu=30.839496, test/loss=1.333400, test/num_examples=3003, total_duration=65333.989783, train/accuracy=0.708745, train/bleu=36.472039, train/loss=1.340720, validation/accuracy=0.691969, validation/bleu=31.080921, validation/loss=1.416434, validation/num_examples=3000
I0209 00:58:58.223664 140277103134464 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.7473715543746948, loss=1.4380886554718018
I0209 00:59:33.080184 140277094741760 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.724953293800354, loss=1.4161479473114014
I0209 01:00:08.049034 140277103134464 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.7526047825813293, loss=1.4738032817840576
I0209 01:00:43.012319 140277094741760 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.7190989255905151, loss=1.3996013402938843
I0209 01:01:17.992840 140277103134464 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.7374809980392456, loss=1.4402210712432861
I0209 01:01:52.996639 140277094741760 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.7242710590362549, loss=1.3989028930664062
I0209 01:02:27.996287 140277103134464 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.7330232858657837, loss=1.4840965270996094
I0209 01:03:02.988144 140277094741760 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.750794529914856, loss=1.4293824434280396
I0209 01:03:37.984176 140277103134464 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.7400085926055908, loss=1.4266966581344604
I0209 01:04:12.969355 140277094741760 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.7718093991279602, loss=1.4679362773895264
I0209 01:04:47.967617 140277103134464 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.7269265651702881, loss=1.50424325466156
I0209 01:05:22.944906 140277094741760 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.7410079836845398, loss=1.346658706665039
I0209 01:05:57.922149 140277103134464 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.7431245446205139, loss=1.463852047920227
I0209 01:06:32.930293 140277094741760 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.7343167662620544, loss=1.4224474430084229
I0209 01:07:07.944624 140277103134464 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.7392116785049438, loss=1.4541125297546387
I0209 01:07:42.918184 140277094741760 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.7219464778900146, loss=1.3985702991485596
I0209 01:08:17.910497 140277103134464 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.7030131220817566, loss=1.3821358680725098
I0209 01:08:52.942920 140277094741760 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.7367058396339417, loss=1.418260931968689
I0209 01:09:27.959678 140277103134464 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.7419987916946411, loss=1.4303616285324097
I0209 01:10:02.977647 140277094741760 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.7401939630508423, loss=1.4274623394012451
I0209 01:10:37.964167 140277103134464 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.7175605893135071, loss=1.3797305822372437
I0209 01:11:12.969141 140277094741760 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.762840747833252, loss=1.4006505012512207
I0209 01:11:47.983073 140277103134464 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.737423300743103, loss=1.3801096677780151
I0209 01:12:22.975977 140277094741760 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.7641409039497375, loss=1.3660318851470947
I0209 01:12:39.835073 140446903760704 spec.py:321] Evaluating on the training split.
I0209 01:12:42.841914 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 01:16:17.920943 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 01:16:20.643164 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 01:18:48.017045 140446903760704 spec.py:349] Evaluating on the test split.
I0209 01:18:50.737922 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 01:21:27.017599 140446903760704 submission_runner.py:408] Time since start: 66701.25s, 	Step: 115250, 	{'train/accuracy': 0.6991199254989624, 'train/loss': 1.3918790817260742, 'train/bleu': 36.12728996133859, 'validation/accuracy': 0.693630576133728, 'validation/loss': 1.4105188846588135, 'validation/bleu': 31.00381458821544, 'validation/num_examples': 3000, 'test/accuracy': 0.7082214951515198, 'test/loss': 1.327876091003418, 'test/bleu': 30.625093720903884, 'test/num_examples': 3003, 'score': 40349.939930677414, 'total_duration': 66701.25009989738, 'accumulated_submission_time': 40349.939930677414, 'accumulated_eval_time': 26345.901841640472, 'accumulated_logging_time': 1.7358145713806152}
I0209 01:21:27.053789 140277103134464 logging_writer.py:48] [115250] accumulated_eval_time=26345.901842, accumulated_logging_time=1.735815, accumulated_submission_time=40349.939931, global_step=115250, preemption_count=0, score=40349.939931, test/accuracy=0.708221, test/bleu=30.625094, test/loss=1.327876, test/num_examples=3003, total_duration=66701.250100, train/accuracy=0.699120, train/bleu=36.127290, train/loss=1.391879, validation/accuracy=0.693631, validation/bleu=31.003815, validation/loss=1.410519, validation/num_examples=3000
I0209 01:21:44.820691 140277094741760 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.7474223375320435, loss=1.4975838661193848
I0209 01:22:19.734858 140277103134464 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.7701842188835144, loss=1.431077241897583
I0209 01:22:54.715947 140277094741760 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.770029604434967, loss=1.4236773252487183
I0209 01:23:29.730207 140277103134464 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.7379929423332214, loss=1.4249615669250488
I0209 01:24:04.726664 140277094741760 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.7437402009963989, loss=1.4055981636047363
I0209 01:24:39.735208 140277103134464 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.7682732939720154, loss=1.3768900632858276
I0209 01:25:14.753083 140277094741760 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.7236362099647522, loss=1.472609519958496
I0209 01:25:49.853517 140277103134464 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.8283889293670654, loss=1.342386245727539
I0209 01:26:24.827524 140277094741760 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.7794309258460999, loss=1.3713198900222778
I0209 01:26:59.798039 140277103134464 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.7801257371902466, loss=1.4301681518554688
I0209 01:27:34.872024 140277094741760 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.7463945746421814, loss=1.4041571617126465
I0209 01:28:09.907702 140277103134464 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.7416126728057861, loss=1.3467483520507812
I0209 01:28:44.945063 140277094741760 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.7801262736320496, loss=1.4227783679962158
I0209 01:29:19.984466 140277103134464 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.7629382610321045, loss=1.3216674327850342
I0209 01:29:54.954316 140277094741760 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.7458425164222717, loss=1.4112900495529175
I0209 01:30:29.973006 140277103134464 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.7695754170417786, loss=1.449237585067749
I0209 01:31:04.959374 140277094741760 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.7799071669578552, loss=1.4645922183990479
I0209 01:31:39.989206 140277103134464 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.7778511047363281, loss=1.4622702598571777
I0209 01:32:14.978174 140277094741760 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.7621357440948486, loss=1.4396204948425293
I0209 01:32:49.979058 140277103134464 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.7820056080818176, loss=1.4311152696609497
I0209 01:33:24.935602 140277094741760 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.7733876705169678, loss=1.3608959913253784
I0209 01:33:59.929891 140277103134464 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.0575487613677979, loss=1.4811335802078247
I0209 01:34:34.885157 140277094741760 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.7543387413024902, loss=1.3541854619979858
I0209 01:35:09.869684 140277103134464 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.7829238176345825, loss=1.364343285560608
I0209 01:35:27.093840 140446903760704 spec.py:321] Evaluating on the training split.
I0209 01:35:30.114757 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 01:39:03.010279 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 01:39:05.715313 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 01:41:35.083682 140446903760704 spec.py:349] Evaluating on the test split.
I0209 01:41:37.810095 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 01:44:04.408669 140446903760704 submission_runner.py:408] Time since start: 68058.64s, 	Step: 117651, 	{'train/accuracy': 0.6992983222007751, 'train/loss': 1.3957855701446533, 'train/bleu': 35.97326922225457, 'validation/accuracy': 0.6929610371589661, 'validation/loss': 1.409582495689392, 'validation/bleu': 31.041739597257934, 'validation/num_examples': 3000, 'test/accuracy': 0.7084422707557678, 'test/loss': 1.3249000310897827, 'test/bleu': 30.823761511515347, 'test/num_examples': 3003, 'score': 41189.89118170738, 'total_duration': 68058.64119935036, 'accumulated_submission_time': 41189.89118170738, 'accumulated_eval_time': 26863.21662259102, 'accumulated_logging_time': 1.7834343910217285}
I0209 01:44:04.438848 140277094741760 logging_writer.py:48] [117651] accumulated_eval_time=26863.216623, accumulated_logging_time=1.783434, accumulated_submission_time=41189.891182, global_step=117651, preemption_count=0, score=41189.891182, test/accuracy=0.708442, test/bleu=30.823762, test/loss=1.324900, test/num_examples=3003, total_duration=68058.641199, train/accuracy=0.699298, train/bleu=35.973269, train/loss=1.395786, validation/accuracy=0.692961, validation/bleu=31.041740, validation/loss=1.409582, validation/num_examples=3000
I0209 01:44:21.837265 140277103134464 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.7810760736465454, loss=1.430910348892212
I0209 01:44:56.705091 140277094741760 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.7541144490242004, loss=1.4110724925994873
I0209 01:45:31.647145 140277103134464 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.8101718425750732, loss=1.3935755491256714
I0209 01:46:06.612507 140277094741760 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.774041473865509, loss=1.3325823545455933
I0209 01:46:41.593028 140277103134464 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.7936314940452576, loss=1.474596619606018
I0209 01:47:16.676251 140277094741760 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.8258926868438721, loss=1.3979748487472534
I0209 01:47:51.685074 140277103134464 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.7738131284713745, loss=1.4323397874832153
I0209 01:48:26.666754 140277094741760 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.7969367504119873, loss=1.407387375831604
I0209 01:49:01.631752 140277103134464 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.8060478568077087, loss=1.413747787475586
I0209 01:49:36.620564 140277094741760 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.788013756275177, loss=1.379642128944397
I0209 01:50:11.608401 140277103134464 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.8027234673500061, loss=1.4695104360580444
I0209 01:50:46.636027 140277094741760 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.7994474768638611, loss=1.401493787765503
I0209 01:51:21.622045 140277103134464 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.7866531014442444, loss=1.375924825668335
I0209 01:51:56.745224 140277094741760 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.8322571516036987, loss=1.4606693983078003
I0209 01:52:31.731660 140277103134464 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.827634871006012, loss=1.4434813261032104
I0209 01:53:06.730804 140277094741760 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.8317175507545471, loss=1.4297828674316406
I0209 01:53:41.752128 140277103134464 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.8031882047653198, loss=1.387357473373413
I0209 01:54:16.780839 140277094741760 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.7868778705596924, loss=1.3615682125091553
I0209 01:54:51.801436 140277103134464 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.8185843229293823, loss=1.3443957567214966
I0209 01:55:26.815343 140277094741760 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.7972445487976074, loss=1.3727538585662842
I0209 01:56:01.803011 140277103134464 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7967516183853149, loss=1.3825517892837524
I0209 01:56:36.795735 140277094741760 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.7805870771408081, loss=1.3413379192352295
I0209 01:57:11.783023 140277103134464 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.795276403427124, loss=1.4157012701034546
I0209 01:57:46.849584 140277094741760 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.8171039819717407, loss=1.3850390911102295
I0209 01:58:04.438162 140446903760704 spec.py:321] Evaluating on the training split.
I0209 01:58:07.457596 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:01:27.662596 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 02:01:30.384937 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:04:10.314494 140446903760704 spec.py:349] Evaluating on the test split.
I0209 02:04:13.032454 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:06:53.034121 140446903760704 submission_runner.py:408] Time since start: 69427.27s, 	Step: 120052, 	{'train/accuracy': 0.7118774056434631, 'train/loss': 1.3286157846450806, 'train/bleu': 36.789703266642434, 'validation/accuracy': 0.6940025687217712, 'validation/loss': 1.4108715057373047, 'validation/bleu': 30.938154978696495, 'validation/num_examples': 3000, 'test/accuracy': 0.7096391916275024, 'test/loss': 1.3247169256210327, 'test/bleu': 30.95885245135478, 'test/num_examples': 3003, 'score': 42029.80330038071, 'total_duration': 69427.26662182808, 'accumulated_submission_time': 42029.80330038071, 'accumulated_eval_time': 27391.812509059906, 'accumulated_logging_time': 1.8251049518585205}
I0209 02:06:53.073482 140277103134464 logging_writer.py:48] [120052] accumulated_eval_time=27391.812509, accumulated_logging_time=1.825105, accumulated_submission_time=42029.803300, global_step=120052, preemption_count=0, score=42029.803300, test/accuracy=0.709639, test/bleu=30.958852, test/loss=1.324717, test/num_examples=3003, total_duration=69427.266622, train/accuracy=0.711877, train/bleu=36.789703, train/loss=1.328616, validation/accuracy=0.694003, validation/bleu=30.938155, validation/loss=1.410872, validation/num_examples=3000
I0209 02:07:10.172274 140277094741760 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.8030999302864075, loss=1.42180597782135
I0209 02:07:45.081271 140277103134464 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.8110753297805786, loss=1.4134804010391235
I0209 02:08:20.045247 140277094741760 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.8429393768310547, loss=1.4168076515197754
I0209 02:08:55.032670 140277103134464 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.8247470855712891, loss=1.344393014907837
I0209 02:09:29.982111 140277094741760 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.8520919680595398, loss=1.4332246780395508
I0209 02:10:04.970199 140277103134464 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.8049010038375854, loss=1.4370273351669312
I0209 02:10:39.944866 140277094741760 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.8058286905288696, loss=1.3439897298812866
I0209 02:11:14.929096 140277103134464 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.8104022145271301, loss=1.3869328498840332
I0209 02:11:49.921792 140277094741760 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.806617021560669, loss=1.3812141418457031
I0209 02:12:24.907411 140277103134464 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.8021993637084961, loss=1.3983230590820312
I0209 02:12:59.926823 140277094741760 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.83381587266922, loss=1.4603047370910645
I0209 02:13:34.905809 140277103134464 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.833628237247467, loss=1.4274145364761353
I0209 02:14:09.910183 140277094741760 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.782864511013031, loss=1.413334608078003
I0209 02:14:44.881890 140277103134464 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.8036507368087769, loss=1.3804031610488892
I0209 02:15:19.864259 140277094741760 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.8263809084892273, loss=1.3705928325653076
I0209 02:15:54.889694 140277103134464 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.8233463764190674, loss=1.3811789751052856
I0209 02:16:29.890779 140277094741760 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.8057396411895752, loss=1.3734182119369507
I0209 02:17:04.900875 140277103134464 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.8598940372467041, loss=1.4100120067596436
I0209 02:17:39.890643 140277094741760 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.8255038261413574, loss=1.438225507736206
I0209 02:18:14.908342 140277103134464 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.8658782839775085, loss=1.3682684898376465
I0209 02:18:50.003694 140277094741760 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.8308385610580444, loss=1.3204107284545898
I0209 02:19:25.019766 140277103134464 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.8171738982200623, loss=1.412179708480835
I0209 02:20:00.014608 140277094741760 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.8292643427848816, loss=1.3344663381576538
I0209 02:20:35.000711 140277103134464 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.8385155200958252, loss=1.4133257865905762
I0209 02:20:53.261243 140446903760704 spec.py:321] Evaluating on the training split.
I0209 02:20:56.263792 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:24:36.381666 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 02:24:39.100549 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:27:09.707445 140446903760704 spec.py:349] Evaluating on the test split.
I0209 02:27:12.416501 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:29:41.946570 140446903760704 submission_runner.py:408] Time since start: 70796.18s, 	Step: 122454, 	{'train/accuracy': 0.707240104675293, 'train/loss': 1.3504246473312378, 'train/bleu': 36.60618759390239, 'validation/accuracy': 0.6941017508506775, 'validation/loss': 1.4063982963562012, 'validation/bleu': 31.11960561212031, 'validation/num_examples': 3000, 'test/accuracy': 0.7113706469535828, 'test/loss': 1.3193117380142212, 'test/bleu': 30.76013239179441, 'test/num_examples': 3003, 'score': 42869.90353536606, 'total_duration': 70796.17909526825, 'accumulated_submission_time': 42869.90353536606, 'accumulated_eval_time': 27920.49778151512, 'accumulated_logging_time': 1.8757200241088867}
I0209 02:29:41.978792 140277094741760 logging_writer.py:48] [122454] accumulated_eval_time=27920.497782, accumulated_logging_time=1.875720, accumulated_submission_time=42869.903535, global_step=122454, preemption_count=0, score=42869.903535, test/accuracy=0.711371, test/bleu=30.760132, test/loss=1.319312, test/num_examples=3003, total_duration=70796.179095, train/accuracy=0.707240, train/bleu=36.606188, train/loss=1.350425, validation/accuracy=0.694102, validation/bleu=31.119606, validation/loss=1.406398, validation/num_examples=3000
I0209 02:29:58.357541 140277103134464 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.8339932560920715, loss=1.3565641641616821
I0209 02:30:33.250428 140277094741760 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.8193206191062927, loss=1.4090338945388794
I0209 02:31:08.206761 140277103134464 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.8010302186012268, loss=1.3293043375015259
I0209 02:31:43.219676 140277094741760 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.815105140209198, loss=1.4293190240859985
I0209 02:32:18.208611 140277103134464 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.8236286044120789, loss=1.393752932548523
I0209 02:32:53.213205 140277094741760 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.8561180233955383, loss=1.4309407472610474
I0209 02:33:28.246399 140277103134464 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.8023892641067505, loss=1.3276238441467285
I0209 02:34:03.221781 140277094741760 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.8346542716026306, loss=1.4341392517089844
I0209 02:34:38.218410 140277103134464 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.8481416702270508, loss=1.3505220413208008
I0209 02:35:13.215398 140277094741760 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.853667140007019, loss=1.370007038116455
I0209 02:35:48.205213 140277103134464 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.8322560787200928, loss=1.396502137184143
I0209 02:36:23.192537 140277094741760 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.8286694884300232, loss=1.4008660316467285
I0209 02:36:58.149336 140277103134464 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.8678802847862244, loss=1.4190598726272583
I0209 02:37:33.151252 140277094741760 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.8222572207450867, loss=1.3204245567321777
I0209 02:38:08.153346 140277103134464 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.8626611232757568, loss=1.3787118196487427
I0209 02:38:43.120039 140277094741760 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.829993724822998, loss=1.381556749343872
I0209 02:39:18.132809 140277103134464 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.8458642959594727, loss=1.4025194644927979
I0209 02:39:53.182563 140277094741760 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.824087381362915, loss=1.3150553703308105
I0209 02:40:28.192915 140277103134464 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.8068515658378601, loss=1.408022165298462
I0209 02:41:03.177331 140277094741760 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.8251546621322632, loss=1.3713349103927612
I0209 02:41:38.175569 140277103134464 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.8298580050468445, loss=1.369567632675171
I0209 02:42:13.194087 140277094741760 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.8039200305938721, loss=1.3319798707962036
I0209 02:42:48.194545 140277103134464 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.8547355532646179, loss=1.3244067430496216
I0209 02:43:23.196059 140277094741760 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.8591814041137695, loss=1.4222564697265625
I0209 02:43:42.185907 140446903760704 spec.py:321] Evaluating on the training split.
I0209 02:43:45.220052 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:47:26.640796 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 02:47:29.378720 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:49:52.362713 140446903760704 spec.py:349] Evaluating on the test split.
I0209 02:49:55.097976 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 02:52:20.938323 140446903760704 submission_runner.py:408] Time since start: 72155.17s, 	Step: 124856, 	{'train/accuracy': 0.7027835845947266, 'train/loss': 1.3786816596984863, 'train/bleu': 36.60843246209514, 'validation/accuracy': 0.6942257285118103, 'validation/loss': 1.4070228338241577, 'validation/bleu': 31.052944646685532, 'validation/num_examples': 3000, 'test/accuracy': 0.711661159992218, 'test/loss': 1.3174381256103516, 'test/bleu': 30.829254280160164, 'test/num_examples': 3003, 'score': 43710.02218127251, 'total_duration': 72155.17084789276, 'accumulated_submission_time': 43710.02218127251, 'accumulated_eval_time': 28439.25016117096, 'accumulated_logging_time': 1.9196293354034424}
I0209 02:52:20.971260 140277103134464 logging_writer.py:48] [124856] accumulated_eval_time=28439.250161, accumulated_logging_time=1.919629, accumulated_submission_time=43710.022181, global_step=124856, preemption_count=0, score=43710.022181, test/accuracy=0.711661, test/bleu=30.829254, test/loss=1.317438, test/num_examples=3003, total_duration=72155.170848, train/accuracy=0.702784, train/bleu=36.608432, train/loss=1.378682, validation/accuracy=0.694226, validation/bleu=31.052945, validation/loss=1.407023, validation/num_examples=3000
I0209 02:52:36.649720 140277094741760 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.8306934833526611, loss=1.3740283250808716
I0209 02:53:11.529481 140277103134464 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.8367332220077515, loss=1.3595508337020874
I0209 02:53:46.504904 140277094741760 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.8556557297706604, loss=1.4377137422561646
I0209 02:54:21.506529 140277103134464 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.8401275277137756, loss=1.3360530138015747
I0209 02:54:56.512562 140277094741760 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.8107103705406189, loss=1.3659569025039673
I0209 02:55:31.496485 140277103134464 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.8549738526344299, loss=1.434725284576416
I0209 02:56:06.476284 140277094741760 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.8223339319229126, loss=1.4063359498977661
I0209 02:56:41.509050 140277103134464 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.82636958360672, loss=1.3607497215270996
I0209 02:57:16.573838 140277094741760 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.8369457721710205, loss=1.3834800720214844
I0209 02:57:51.550106 140277103134464 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.8071233630180359, loss=1.3865476846694946
I0209 02:58:26.549763 140277094741760 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.8632481694221497, loss=1.2602906227111816
I0209 02:59:01.553083 140277103134464 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.8731717467308044, loss=1.402902364730835
I0209 02:59:36.541515 140277094741760 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.8371493816375732, loss=1.3921281099319458
I0209 03:00:11.525202 140277103134464 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.855579674243927, loss=1.3027182817459106
I0209 03:00:46.507603 140277094741760 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.8558546900749207, loss=1.4244463443756104
I0209 03:01:21.528007 140277103134464 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.8541231155395508, loss=1.3795853853225708
I0209 03:01:56.520259 140277094741760 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.8278933167457581, loss=1.2898428440093994
I0209 03:02:31.518262 140277103134464 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.8256707787513733, loss=1.3930109739303589
I0209 03:03:06.532168 140277094741760 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.8749023675918579, loss=1.304240107536316
I0209 03:03:41.528143 140277103134464 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.8559103012084961, loss=1.35603666305542
I0209 03:04:16.535479 140277094741760 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.8325039744377136, loss=1.4301273822784424
I0209 03:04:51.533631 140277103134464 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.8554487228393555, loss=1.3227996826171875
I0209 03:05:26.544501 140277094741760 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.8498808145523071, loss=1.379496455192566
I0209 03:06:01.707908 140277103134464 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.8538480401039124, loss=1.3974809646606445
I0209 03:06:21.011161 140446903760704 spec.py:321] Evaluating on the training split.
I0209 03:06:24.027187 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:09:41.317556 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 03:09:44.026551 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:12:13.779761 140446903760704 spec.py:349] Evaluating on the test split.
I0209 03:12:16.492266 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:14:50.756398 140446903760704 submission_runner.py:408] Time since start: 73504.99s, 	Step: 127257, 	{'train/accuracy': 0.7099313735961914, 'train/loss': 1.3409501314163208, 'train/bleu': 36.71479864667537, 'validation/accuracy': 0.6950068473815918, 'validation/loss': 1.405134677886963, 'validation/bleu': 31.12469228435073, 'validation/num_examples': 3000, 'test/accuracy': 0.7116146683692932, 'test/loss': 1.3157283067703247, 'test/bleu': 30.96810588775437, 'test/num_examples': 3003, 'score': 44549.97412252426, 'total_duration': 73504.98891401291, 'accumulated_submission_time': 44549.97412252426, 'accumulated_eval_time': 28948.995346546173, 'accumulated_logging_time': 1.9638566970825195}
I0209 03:14:50.789702 140277094741760 logging_writer.py:48] [127257] accumulated_eval_time=28948.995347, accumulated_logging_time=1.963857, accumulated_submission_time=44549.974123, global_step=127257, preemption_count=0, score=44549.974123, test/accuracy=0.711615, test/bleu=30.968106, test/loss=1.315728, test/num_examples=3003, total_duration=73504.988914, train/accuracy=0.709931, train/bleu=36.714799, train/loss=1.340950, validation/accuracy=0.695007, validation/bleu=31.124692, validation/loss=1.405135, validation/num_examples=3000
I0209 03:15:06.102431 140277103134464 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.838087797164917, loss=1.3092437982559204
I0209 03:15:40.954964 140277094741760 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.8313776254653931, loss=1.331841230392456
I0209 03:16:15.955869 140277103134464 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.8524735569953918, loss=1.3910189867019653
I0209 03:16:50.959827 140277094741760 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.87661212682724, loss=1.4180376529693604
I0209 03:17:25.948894 140277103134464 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.8402238488197327, loss=1.4362531900405884
I0209 03:18:00.960869 140277094741760 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.8434527516365051, loss=1.3527066707611084
I0209 03:18:35.949633 140277103134464 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.8373087644577026, loss=1.3997735977172852
I0209 03:19:10.940541 140277094741760 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.8604829907417297, loss=1.3475308418273926
I0209 03:19:45.934967 140277103134464 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.8304709196090698, loss=1.4600353240966797
I0209 03:20:21.033026 140277094741760 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.8214741945266724, loss=1.3899734020233154
I0209 03:20:56.056050 140277103134464 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.8723387122154236, loss=1.3994884490966797
I0209 03:21:31.070160 140277094741760 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.8560107350349426, loss=1.3300907611846924
I0209 03:22:06.075742 140277103134464 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.8452105522155762, loss=1.3162702322006226
I0209 03:22:41.075979 140277094741760 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.820605456829071, loss=1.3597182035446167
I0209 03:23:16.056483 140277103134464 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.8366990089416504, loss=1.3763850927352905
I0209 03:23:51.030663 140277094741760 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.8430789709091187, loss=1.395581841468811
I0209 03:24:26.056905 140277103134464 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.8333145380020142, loss=1.3743869066238403
I0209 03:25:01.049153 140277094741760 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.8436862826347351, loss=1.3525711297988892
I0209 03:25:36.052194 140277103134464 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.8340945839881897, loss=1.4298332929611206
I0209 03:26:11.058367 140277094741760 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.8807163834571838, loss=1.3987046480178833
I0209 03:26:46.058092 140277103134464 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.8391364216804504, loss=1.4391467571258545
I0209 03:27:21.072581 140277094741760 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.8457449078559875, loss=1.4169763326644897
I0209 03:27:56.051478 140277103134464 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.8676962852478027, loss=1.3330681324005127
I0209 03:28:31.044304 140277094741760 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.8435983061790466, loss=1.4188534021377563
I0209 03:28:51.071295 140446903760704 spec.py:321] Evaluating on the training split.
I0209 03:28:54.090930 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:32:13.966953 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 03:32:16.688577 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:34:46.523793 140446903760704 spec.py:349] Evaluating on the test split.
I0209 03:34:49.240076 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:37:19.914908 140446903760704 submission_runner.py:408] Time since start: 74854.15s, 	Step: 129659, 	{'train/accuracy': 0.7101288437843323, 'train/loss': 1.3376935720443726, 'train/bleu': 36.566009419018926, 'validation/accuracy': 0.6951928734779358, 'validation/loss': 1.4060457944869995, 'validation/bleu': 31.224968534748918, 'validation/num_examples': 3000, 'test/accuracy': 0.7120562791824341, 'test/loss': 1.3163964748382568, 'test/bleu': 30.971645040855503, 'test/num_examples': 3003, 'score': 45390.17083954811, 'total_duration': 74854.14743614197, 'accumulated_submission_time': 45390.17083954811, 'accumulated_eval_time': 29457.83890938759, 'accumulated_logging_time': 2.007380247116089}
I0209 03:37:19.947899 140277103134464 logging_writer.py:48] [129659] accumulated_eval_time=29457.838909, accumulated_logging_time=2.007380, accumulated_submission_time=45390.170840, global_step=129659, preemption_count=0, score=45390.170840, test/accuracy=0.712056, test/bleu=30.971645, test/loss=1.316396, test/num_examples=3003, total_duration=74854.147436, train/accuracy=0.710129, train/bleu=36.566009, train/loss=1.337694, validation/accuracy=0.695193, validation/bleu=31.224969, validation/loss=1.406046, validation/num_examples=3000
I0209 03:37:34.548439 140277094741760 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.8479190468788147, loss=1.434080958366394
I0209 03:38:09.410861 140277103134464 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.8331805467605591, loss=1.3614747524261475
I0209 03:38:44.411870 140277094741760 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.8524016737937927, loss=1.3538693189620972
I0209 03:39:19.415881 140277103134464 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.8045576214790344, loss=1.3674159049987793
I0209 03:39:54.423769 140277094741760 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.8528425693511963, loss=1.4076964855194092
I0209 03:40:29.433206 140277103134464 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.8219044208526611, loss=1.3784114122390747
I0209 03:41:04.472904 140277094741760 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.8371984362602234, loss=1.351574420928955
I0209 03:41:39.469990 140277103134464 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.8422266840934753, loss=1.296401858329773
I0209 03:42:14.460344 140277094741760 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.8369663953781128, loss=1.3596850633621216
I0209 03:42:49.474051 140277103134464 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.8288282155990601, loss=1.295469880104065
I0209 03:43:24.520200 140277094741760 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.8391929864883423, loss=1.3748029470443726
I0209 03:43:59.552948 140277103134464 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.8489634394645691, loss=1.4043068885803223
I0209 03:44:34.571930 140277094741760 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.8517763614654541, loss=1.279088020324707
I0209 03:45:09.598313 140277103134464 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.8442849516868591, loss=1.2826310396194458
I0209 03:45:44.602884 140277094741760 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.8113350868225098, loss=1.3086402416229248
I0209 03:46:19.606751 140277103134464 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.8173577189445496, loss=1.3633458614349365
I0209 03:46:54.584537 140277094741760 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.8397461175918579, loss=1.3638888597488403
I0209 03:47:29.617410 140277103134464 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.8202610611915588, loss=1.3974275588989258
I0209 03:48:04.663064 140277094741760 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.848497748374939, loss=1.3873481750488281
I0209 03:48:39.657681 140277103134464 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.8324078321456909, loss=1.347478985786438
I0209 03:49:14.717191 140277094741760 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.827629029750824, loss=1.3629206418991089
I0209 03:49:49.741314 140277103134464 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.8513403534889221, loss=1.4083884954452515
I0209 03:50:24.778514 140277094741760 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.8233072757720947, loss=1.3311667442321777
I0209 03:50:59.838829 140277103134464 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.8431301116943359, loss=1.3350250720977783
I0209 03:51:20.218930 140446903760704 spec.py:321] Evaluating on the training split.
I0209 03:51:23.230650 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:54:51.152325 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 03:54:53.892331 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:57:17.869868 140446903760704 spec.py:349] Evaluating on the test split.
I0209 03:57:20.595506 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 03:59:39.744547 140446903760704 submission_runner.py:408] Time since start: 76193.98s, 	Step: 132060, 	{'train/accuracy': 0.7102059721946716, 'train/loss': 1.3387534618377686, 'train/bleu': 36.377493267921025, 'validation/accuracy': 0.6951432824134827, 'validation/loss': 1.4058610200881958, 'validation/bleu': 31.325841393904454, 'validation/num_examples': 3000, 'test/accuracy': 0.7124397158622742, 'test/loss': 1.315784215927124, 'test/bleu': 30.992799598798722, 'test/num_examples': 3003, 'score': 46230.35232543945, 'total_duration': 76193.97706127167, 'accumulated_submission_time': 46230.35232543945, 'accumulated_eval_time': 29957.364458322525, 'accumulated_logging_time': 2.0503439903259277}
I0209 03:59:39.780760 140277094741760 logging_writer.py:48] [132060] accumulated_eval_time=29957.364458, accumulated_logging_time=2.050344, accumulated_submission_time=46230.352325, global_step=132060, preemption_count=0, score=46230.352325, test/accuracy=0.712440, test/bleu=30.992800, test/loss=1.315784, test/num_examples=3003, total_duration=76193.977061, train/accuracy=0.710206, train/bleu=36.377493, train/loss=1.338753, validation/accuracy=0.695143, validation/bleu=31.325841, validation/loss=1.405861, validation/num_examples=3000
I0209 03:59:54.062937 140277103134464 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.8635228872299194, loss=1.391247034072876
I0209 04:00:28.928109 140277094741760 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.8259952068328857, loss=1.3883196115493774
I0209 04:01:03.902771 140277103134464 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.8429242372512817, loss=1.3805749416351318
I0209 04:01:38.901176 140277094741760 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.8806419372558594, loss=1.4208524227142334
I0209 04:02:13.891891 140277103134464 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.823356568813324, loss=1.312697172164917
I0209 04:02:48.907851 140277094741760 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.8413911461830139, loss=1.3959558010101318
I0209 04:03:23.916359 140277103134464 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.8464823961257935, loss=1.4133678674697876
I0209 04:03:58.952532 140277094741760 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.8117151260375977, loss=1.3211055994033813
I0209 04:04:33.978128 140277103134464 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.8056220412254333, loss=1.324225664138794
I0209 04:05:08.974448 140277094741760 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.8188757300376892, loss=1.3607438802719116
I0209 04:05:43.977484 140277103134464 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.8438699841499329, loss=1.3290177583694458
I0209 04:06:18.969497 140277094741760 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.8243985772132874, loss=1.2882496118545532
I0209 04:06:53.935941 140277103134464 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.8721327185630798, loss=1.4440202713012695
I0209 04:07:04.857271 140446903760704 spec.py:321] Evaluating on the training split.
I0209 04:07:07.869061 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:10:40.042723 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 04:10:42.756191 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:13:06.844706 140446903760704 spec.py:349] Evaluating on the test split.
I0209 04:13:09.568918 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:15:31.950550 140446903760704 submission_runner.py:408] Time since start: 77146.18s, 	Step: 133333, 	{'train/accuracy': 0.7138813138008118, 'train/loss': 1.3141133785247803, 'train/bleu': 36.535556096203386, 'validation/accuracy': 0.6952672600746155, 'validation/loss': 1.4057084321975708, 'validation/bleu': 31.22940684635679, 'validation/num_examples': 3000, 'test/accuracy': 0.7124048471450806, 'test/loss': 1.3156583309173584, 'test/bleu': 30.969127426676877, 'test/num_examples': 3003, 'score': 46675.375801324844, 'total_duration': 77146.18306612968, 'accumulated_submission_time': 46675.375801324844, 'accumulated_eval_time': 30464.457669734955, 'accumulated_logging_time': 2.0986831188201904}
I0209 04:15:31.984409 140277094741760 logging_writer.py:48] [133333] accumulated_eval_time=30464.457670, accumulated_logging_time=2.098683, accumulated_submission_time=46675.375801, global_step=133333, preemption_count=0, score=46675.375801, test/accuracy=0.712405, test/bleu=30.969127, test/loss=1.315658, test/num_examples=3003, total_duration=77146.183066, train/accuracy=0.713881, train/bleu=36.535556, train/loss=1.314113, validation/accuracy=0.695267, validation/bleu=31.229407, validation/loss=1.405708, validation/num_examples=3000
I0209 04:15:32.019077 140277103134464 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46675.375801
I0209 04:15:33.246022 140446903760704 checkpoints.py:490] Saving checkpoint at step: 133333
I0209 04:15:37.178549 140446903760704 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_3/checkpoint_133333
I0209 04:15:37.183207 140446903760704 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_3/checkpoint_133333.
I0209 04:15:37.234816 140446903760704 submission_runner.py:583] Tuning trial 3/5
I0209 04:15:37.234979 140446903760704 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0209 04:15:37.244906 140446903760704 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000624552892986685, 'train/loss': 11.190462112426758, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.4393572807312, 'total_duration': 867.3890187740326, 'accumulated_submission_time': 27.4393572807312, 'accumulated_eval_time': 839.9496130943298, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2397, {'train/accuracy': 0.41179487109184265, 'train/loss': 3.941603660583496, 'train/bleu': 14.546588761941905, 'validation/accuracy': 0.3955065608024597, 'validation/loss': 4.074873924255371, 'validation/bleu': 9.821189947778166, 'validation/num_examples': 3000, 'test/accuracy': 0.38170936703681946, 'test/loss': 4.273231029510498, 'test/bleu': 7.8068550044099725, 'test/num_examples': 3003, 'score': 867.5393142700195, 'total_duration': 2434.214878797531, 'accumulated_submission_time': 867.5393142700195, 'accumulated_eval_time': 1566.579701423645, 'accumulated_logging_time': 0.0190737247467041, 'global_step': 2397, 'preemption_count': 0}), (4792, {'train/accuracy': 0.5426854491233826, 'train/loss': 2.662497043609619, 'train/bleu': 24.877218045562664, 'validation/accuracy': 0.5426343083381653, 'validation/loss': 2.6319589614868164, 'validation/bleu': 20.313745733230586, 'validation/num_examples': 3000, 'test/accuracy': 0.5415606498718262, 'test/loss': 2.6677498817443848, 'test/bleu': 18.69656506174673, 'test/num_examples': 3003, 'score': 1707.4862928390503, 'total_duration': 3733.7244775295258, 'accumulated_submission_time': 1707.4862928390503, 'accumulated_eval_time': 2026.0431470870972, 'accumulated_logging_time': 0.044114112854003906, 'global_step': 4792, 'preemption_count': 0}), (7189, {'train/accuracy': 0.5803403854370117, 'train/loss': 2.2744669914245605, 'train/bleu': 27.177722377753945, 'validation/accuracy': 0.585894763469696, 'validation/loss': 2.2277345657348633, 'validation/bleu': 23.692412799424854, 'validation/num_examples': 3000, 'test/accuracy': 0.5870431661605835, 'test/loss': 2.226987838745117, 'test/bleu': 21.947922210608603, 'test/num_examples': 3003, 'score': 2547.5802307128906, 'total_duration': 5054.663430452347, 'accumulated_submission_time': 2547.5802307128906, 'accumulated_eval_time': 2506.784266471863, 'accumulated_logging_time': 0.07352590560913086, 'global_step': 7189, 'preemption_count': 0}), (9587, {'train/accuracy': 0.5935688018798828, 'train/loss': 2.1319386959075928, 'train/bleu': 28.43446243478617, 'validation/accuracy': 0.604071855545044, 'validation/loss': 2.039424180984497, 'validation/bleu': 24.43123456805673, 'validation/num_examples': 3000, 'test/accuracy': 0.6092615127563477, 'test/loss': 2.0072202682495117, 'test/bleu': 23.120191808823755, 'test/num_examples': 3003, 'score': 3387.8171026706696, 'total_duration': 6336.122858285904, 'accumulated_submission_time': 3387.8171026706696, 'accumulated_eval_time': 2947.9040400981903, 'accumulated_logging_time': 0.0987389087677002, 'global_step': 9587, 'preemption_count': 0}), (11984, {'train/accuracy': 0.6023198962211609, 'train/loss': 2.0523767471313477, 'train/bleu': 28.641659970283314, 'validation/accuracy': 0.6189383864402771, 'validation/loss': 1.9206935167312622, 'validation/bleu': 25.24411501245022, 'validation/num_examples': 3000, 'test/accuracy': 0.6242752075195312, 'test/loss': 1.8770338296890259, 'test/bleu': 24.5813471409335, 'test/num_examples': 3003, 'score': 4227.878267049789, 'total_duration': 7910.251308679581, 'accumulated_submission_time': 4227.878267049789, 'accumulated_eval_time': 3681.8671090602875, 'accumulated_logging_time': 0.1276702880859375, 'global_step': 11984, 'preemption_count': 0}), (14383, {'train/accuracy': 0.6155896186828613, 'train/loss': 1.9335036277770996, 'train/bleu': 29.66881520325619, 'validation/accuracy': 0.629217267036438, 'validation/loss': 1.829842209815979, 'validation/bleu': 26.167779487244733, 'validation/num_examples': 3000, 'test/accuracy': 0.6378014087677002, 'test/loss': 1.7801294326782227, 'test/bleu': 25.51934415460533, 'test/num_examples': 3003, 'score': 5067.965177059174, 'total_duration': 9170.390238761902, 'accumulated_submission_time': 5067.965177059174, 'accumulated_eval_time': 4101.813674926758, 'accumulated_logging_time': 0.15631937980651855, 'global_step': 14383, 'preemption_count': 0}), (16781, {'train/accuracy': 0.6175857782363892, 'train/loss': 1.9224374294281006, 'train/bleu': 30.239096733865786, 'validation/accuracy': 0.6388513445854187, 'validation/loss': 1.7686958312988281, 'validation/bleu': 26.848252135974953, 'validation/num_examples': 3000, 'test/accuracy': 0.6479228734970093, 'test/loss': 1.7082493305206299, 'test/bleu': 26.094720521677864, 'test/num_examples': 3003, 'score': 5908.19517326355, 'total_duration': 10471.462261676788, 'accumulated_submission_time': 5908.19517326355, 'accumulated_eval_time': 4562.548948049545, 'accumulated_logging_time': 0.185197114944458, 'global_step': 16781, 'preemption_count': 0}), (19181, {'train/accuracy': 0.6363543272018433, 'train/loss': 1.7792028188705444, 'train/bleu': 31.287817954743222, 'validation/accuracy': 0.6457824110984802, 'validation/loss': 1.7178845405578613, 'validation/bleu': 27.271873368897552, 'validation/num_examples': 3000, 'test/accuracy': 0.6533612608909607, 'test/loss': 1.6571162939071655, 'test/bleu': 26.776162031607065, 'test/num_examples': 3003, 'score': 6748.225657224655, 'total_duration': 11907.020893573761, 'accumulated_submission_time': 6748.225657224655, 'accumulated_eval_time': 5157.970972537994, 'accumulated_logging_time': 0.2151799201965332, 'global_step': 19181, 'preemption_count': 0}), (21582, {'train/accuracy': 0.6294018626213074, 'train/loss': 1.8236947059631348, 'train/bleu': 30.38335666819069, 'validation/accuracy': 0.6469727754592896, 'validation/loss': 1.6930538415908813, 'validation/bleu': 27.204733443076528, 'validation/num_examples': 3000, 'test/accuracy': 0.6578351259231567, 'test/loss': 1.6361119747161865, 'test/bleu': 26.86156974459988, 'test/num_examples': 3003, 'score': 7588.451881408691, 'total_duration': 13331.740639448166, 'accumulated_submission_time': 7588.451881408691, 'accumulated_eval_time': 5742.355647802353, 'accumulated_logging_time': 0.24896502494812012, 'global_step': 21582, 'preemption_count': 0}), (23982, {'train/accuracy': 0.6340451240539551, 'train/loss': 1.8066679239273071, 'train/bleu': 30.86761748790029, 'validation/accuracy': 0.6483242511749268, 'validation/loss': 1.6817162036895752, 'validation/bleu': 27.5362496664734, 'validation/num_examples': 3000, 'test/accuracy': 0.6590552926063538, 'test/loss': 1.6155636310577393, 'test/bleu': 26.74998812857728, 'test/num_examples': 3003, 'score': 8428.454867362976, 'total_duration': 14662.757348537445, 'accumulated_submission_time': 8428.454867362976, 'accumulated_eval_time': 6233.2626214027405, 'accumulated_logging_time': 0.27758240699768066, 'global_step': 23982, 'preemption_count': 0}), (26383, {'train/accuracy': 0.6357397437095642, 'train/loss': 1.7810266017913818, 'train/bleu': 30.912190908003147, 'validation/accuracy': 0.6519448161125183, 'validation/loss': 1.6675649881362915, 'validation/bleu': 27.51485381411525, 'validation/num_examples': 3000, 'test/accuracy': 0.6617047190666199, 'test/loss': 1.606836199760437, 'test/bleu': 26.932696320278875, 'test/num_examples': 3003, 'score': 9268.692746162415, 'total_duration': 16034.033752679825, 'accumulated_submission_time': 9268.692746162415, 'accumulated_eval_time': 6764.1959137916565, 'accumulated_logging_time': 0.3061375617980957, 'global_step': 26383, 'preemption_count': 0}), (28783, {'train/accuracy': 0.6305766105651855, 'train/loss': 1.8134877681732178, 'train/bleu': 30.657916208514465, 'validation/accuracy': 0.6531599164009094, 'validation/loss': 1.6515889167785645, 'validation/bleu': 27.818056014519808, 'validation/num_examples': 3000, 'test/accuracy': 0.6621811985969543, 'test/loss': 1.5930308103561401, 'test/bleu': 26.89210431803698, 'test/num_examples': 3003, 'score': 10108.606403827667, 'total_duration': 17367.151869773865, 'accumulated_submission_time': 10108.606403827667, 'accumulated_eval_time': 7257.295704364777, 'accumulated_logging_time': 0.3352222442626953, 'global_step': 28783, 'preemption_count': 0}), (31184, {'train/accuracy': 0.6314585208892822, 'train/loss': 1.8155176639556885, 'train/bleu': 30.780932147538294, 'validation/accuracy': 0.6558877229690552, 'validation/loss': 1.641958475112915, 'validation/bleu': 27.954856722763047, 'validation/num_examples': 3000, 'test/accuracy': 0.6656208634376526, 'test/loss': 1.5788705348968506, 'test/bleu': 27.360726859055152, 'test/num_examples': 3003, 'score': 10948.571624994278, 'total_duration': 18694.437842607498, 'accumulated_submission_time': 10948.571624994278, 'accumulated_eval_time': 7744.511089324951, 'accumulated_logging_time': 0.3644375801086426, 'global_step': 31184, 'preemption_count': 0}), (33583, {'train/accuracy': 0.6337587833404541, 'train/loss': 1.78791081905365, 'train/bleu': 31.308028349740688, 'validation/accuracy': 0.6552677750587463, 'validation/loss': 1.638217806816101, 'validation/bleu': 28.209447885022723, 'validation/num_examples': 3000, 'test/accuracy': 0.6670966148376465, 'test/loss': 1.5727591514587402, 'test/bleu': 27.57975050809596, 'test/num_examples': 3003, 'score': 11788.665621519089, 'total_duration': 20068.18571949005, 'accumulated_submission_time': 11788.665621519089, 'accumulated_eval_time': 8278.055241584778, 'accumulated_logging_time': 0.39438962936401367, 'global_step': 33583, 'preemption_count': 0}), (35984, {'train/accuracy': 0.6375605463981628, 'train/loss': 1.7774264812469482, 'train/bleu': 30.86603327927146, 'validation/accuracy': 0.6572268009185791, 'validation/loss': 1.6223275661468506, 'validation/bleu': 28.12284049602379, 'validation/num_examples': 3000, 'test/accuracy': 0.6696415543556213, 'test/loss': 1.5568478107452393, 'test/bleu': 27.97125458605869, 'test/num_examples': 3003, 'score': 12628.895520687103, 'total_duration': 21364.958856105804, 'accumulated_submission_time': 12628.895520687103, 'accumulated_eval_time': 8734.487503767014, 'accumulated_logging_time': 0.42507266998291016, 'global_step': 35984, 'preemption_count': 0}), (38385, {'train/accuracy': 0.6480917930603027, 'train/loss': 1.692330002784729, 'train/bleu': 31.930142130136353, 'validation/accuracy': 0.6595950126647949, 'validation/loss': 1.6177055835723877, 'validation/bleu': 28.111084985560286, 'validation/num_examples': 3000, 'test/accuracy': 0.669780969619751, 'test/loss': 1.5530155897140503, 'test/bleu': 27.636952264917564, 'test/num_examples': 3003, 'score': 13469.028195142746, 'total_duration': 22713.596930027008, 'accumulated_submission_time': 13469.028195142746, 'accumulated_eval_time': 9242.88492488861, 'accumulated_logging_time': 0.45506882667541504, 'global_step': 38385, 'preemption_count': 0}), (40787, {'train/accuracy': 0.6398926973342896, 'train/loss': 1.7486668825149536, 'train/bleu': 31.34876197331421, 'validation/accuracy': 0.6608969569206238, 'validation/loss': 1.6113759279251099, 'validation/bleu': 28.497071212994896, 'validation/num_examples': 3000, 'test/accuracy': 0.6694207191467285, 'test/loss': 1.5531401634216309, 'test/bleu': 27.761626027709138, 'test/num_examples': 3003, 'score': 14309.030643939972, 'total_duration': 24079.183569192886, 'accumulated_submission_time': 14309.030643939972, 'accumulated_eval_time': 9768.36000418663, 'accumulated_logging_time': 0.48691678047180176, 'global_step': 40787, 'preemption_count': 0}), (43189, {'train/accuracy': 0.6379035115242004, 'train/loss': 1.7647839784622192, 'train/bleu': 31.63035083470584, 'validation/accuracy': 0.6619136929512024, 'validation/loss': 1.5984268188476562, 'validation/bleu': 28.621136606657313, 'validation/num_examples': 3000, 'test/accuracy': 0.6715589165687561, 'test/loss': 1.5377026796340942, 'test/bleu': 28.04822035841773, 'test/num_examples': 3003, 'score': 15148.924597978592, 'total_duration': 25420.193618297577, 'accumulated_submission_time': 15148.924597978592, 'accumulated_eval_time': 10269.365025520325, 'accumulated_logging_time': 0.5226850509643555, 'global_step': 43189, 'preemption_count': 0}), (45591, {'train/accuracy': 0.645022988319397, 'train/loss': 1.7124336957931519, 'train/bleu': 31.1812755361264, 'validation/accuracy': 0.6615293025970459, 'validation/loss': 1.5967493057250977, 'validation/bleu': 28.51240267229416, 'validation/num_examples': 3000, 'test/accuracy': 0.6716750860214233, 'test/loss': 1.53374445438385, 'test/bleu': 27.956589459535934, 'test/num_examples': 3003, 'score': 15989.009542703629, 'total_duration': 26797.38473558426, 'accumulated_submission_time': 15989.009542703629, 'accumulated_eval_time': 10806.36244225502, 'accumulated_logging_time': 0.5551190376281738, 'global_step': 45591, 'preemption_count': 0}), (47993, {'train/accuracy': 0.6419897079467773, 'train/loss': 1.7327611446380615, 'train/bleu': 31.192088849007714, 'validation/accuracy': 0.6634387373924255, 'validation/loss': 1.5877983570098877, 'validation/bleu': 28.441895240859274, 'validation/num_examples': 3000, 'test/accuracy': 0.6764046549797058, 'test/loss': 1.518065333366394, 'test/bleu': 27.780123571158285, 'test/num_examples': 3003, 'score': 16829.091136455536, 'total_duration': 28150.897320508957, 'accumulated_submission_time': 16829.091136455536, 'accumulated_eval_time': 11319.686262845993, 'accumulated_logging_time': 0.5872867107391357, 'global_step': 47993, 'preemption_count': 0}), (50395, {'train/accuracy': 0.6616469025611877, 'train/loss': 1.6047558784484863, 'train/bleu': 32.69124701519562, 'validation/accuracy': 0.6644430756568909, 'validation/loss': 1.5778286457061768, 'validation/bleu': 28.24018160352546, 'validation/num_examples': 3000, 'test/accuracy': 0.6767880916595459, 'test/loss': 1.508074164390564, 'test/bleu': 28.356528811797137, 'test/num_examples': 3003, 'score': 17669.225746631622, 'total_duration': 29669.82399916649, 'accumulated_submission_time': 17669.225746631622, 'accumulated_eval_time': 11998.36673951149, 'accumulated_logging_time': 0.6197023391723633, 'global_step': 50395, 'preemption_count': 0}), (52798, {'train/accuracy': 0.6459488868713379, 'train/loss': 1.7108194828033447, 'train/bleu': 31.285477518950703, 'validation/accuracy': 0.6637115478515625, 'validation/loss': 1.577656865119934, 'validation/bleu': 28.79268462181212, 'validation/num_examples': 3000, 'test/accuracy': 0.6748591065406799, 'test/loss': 1.510031819343567, 'test/bleu': 28.116153582756937, 'test/num_examples': 3003, 'score': 18509.355145454407, 'total_duration': 31032.600776195526, 'accumulated_submission_time': 18509.355145454407, 'accumulated_eval_time': 12520.904906749725, 'accumulated_logging_time': 0.6526608467102051, 'global_step': 52798, 'preemption_count': 0}), (55200, {'train/accuracy': 0.6462436318397522, 'train/loss': 1.7085494995117188, 'train/bleu': 31.661851946131762, 'validation/accuracy': 0.6645422577857971, 'validation/loss': 1.571349024772644, 'validation/bleu': 28.521983085309316, 'validation/num_examples': 3000, 'test/accuracy': 0.6769275665283203, 'test/loss': 1.498882532119751, 'test/bleu': 28.070425518899892, 'test/num_examples': 3003, 'score': 19349.541662931442, 'total_duration': 32448.686230182648, 'accumulated_submission_time': 19349.541662931442, 'accumulated_eval_time': 13096.69297504425, 'accumulated_logging_time': 0.6853039264678955, 'global_step': 55200, 'preemption_count': 0}), (57602, {'train/accuracy': 0.6559958457946777, 'train/loss': 1.6449164152145386, 'train/bleu': 31.921049540477092, 'validation/accuracy': 0.6649266481399536, 'validation/loss': 1.5654634237289429, 'validation/bleu': 28.650881275581277, 'validation/num_examples': 3000, 'test/accuracy': 0.6784266233444214, 'test/loss': 1.4948620796203613, 'test/bleu': 28.403415924123994, 'test/num_examples': 3003, 'score': 20189.47763967514, 'total_duration': 33847.21442198753, 'accumulated_submission_time': 20189.47763967514, 'accumulated_eval_time': 13655.178454637527, 'accumulated_logging_time': 0.7172021865844727, 'global_step': 57602, 'preemption_count': 0}), (60005, {'train/accuracy': 0.6456266641616821, 'train/loss': 1.7141486406326294, 'train/bleu': 32.09485250326785, 'validation/accuracy': 0.6683239936828613, 'validation/loss': 1.5589754581451416, 'validation/bleu': 28.846310976430967, 'validation/num_examples': 3000, 'test/accuracy': 0.6812503933906555, 'test/loss': 1.4764920473098755, 'test/bleu': 28.37237056623999, 'test/num_examples': 3003, 'score': 21029.704471111298, 'total_duration': 35197.30215525627, 'accumulated_submission_time': 21029.704471111298, 'accumulated_eval_time': 14164.931086301804, 'accumulated_logging_time': 0.7506864070892334, 'global_step': 60005, 'preemption_count': 0}), (62407, {'train/accuracy': 0.6473643183708191, 'train/loss': 1.701988935470581, 'train/bleu': 31.688075634827747, 'validation/accuracy': 0.6715973615646362, 'validation/loss': 1.5442315340042114, 'validation/bleu': 28.907307741563486, 'validation/num_examples': 3000, 'test/accuracy': 0.6835861206054688, 'test/loss': 1.4696898460388184, 'test/bleu': 28.859389761738015, 'test/num_examples': 3003, 'score': 21869.767882585526, 'total_duration': 36556.538370132446, 'accumulated_submission_time': 21869.767882585526, 'accumulated_eval_time': 14683.995180606842, 'accumulated_logging_time': 0.7836964130401611, 'global_step': 62407, 'preemption_count': 0}), (64810, {'train/accuracy': 0.6546890735626221, 'train/loss': 1.657198429107666, 'train/bleu': 31.84385511875302, 'validation/accuracy': 0.6724157333374023, 'validation/loss': 1.5407817363739014, 'validation/bleu': 29.134471317994127, 'validation/num_examples': 3000, 'test/accuracy': 0.682877242565155, 'test/loss': 1.4688782691955566, 'test/bleu': 28.835831003764778, 'test/num_examples': 3003, 'score': 22710.003426790237, 'total_duration': 37973.3166179657, 'accumulated_submission_time': 22710.003426790237, 'accumulated_eval_time': 15260.428012609482, 'accumulated_logging_time': 0.8176324367523193, 'global_step': 64810, 'preemption_count': 0}), (67212, {'train/accuracy': 0.6506806015968323, 'train/loss': 1.6883158683776855, 'train/bleu': 31.947221106489987, 'validation/accuracy': 0.6726140975952148, 'validation/loss': 1.5377442836761475, 'validation/bleu': 29.383137318665202, 'validation/num_examples': 3000, 'test/accuracy': 0.6839579343795776, 'test/loss': 1.4663946628570557, 'test/bleu': 28.923713488856702, 'test/num_examples': 3003, 'score': 23549.912605524063, 'total_duration': 39340.40902590752, 'accumulated_submission_time': 23549.912605524063, 'accumulated_eval_time': 15787.419231653214, 'accumulated_logging_time': 0.9314663410186768, 'global_step': 67212, 'preemption_count': 0}), (69614, {'train/accuracy': 0.6651747822761536, 'train/loss': 1.5777531862258911, 'train/bleu': 32.645817061232485, 'validation/accuracy': 0.6728744506835938, 'validation/loss': 1.5295296907424927, 'validation/bleu': 29.578541313522177, 'validation/num_examples': 3000, 'test/accuracy': 0.6869211792945862, 'test/loss': 1.4553210735321045, 'test/bleu': 29.16384456800778, 'test/num_examples': 3003, 'score': 24389.983780145645, 'total_duration': 40713.90342450142, 'accumulated_submission_time': 24389.983780145645, 'accumulated_eval_time': 16320.727837085724, 'accumulated_logging_time': 0.9683539867401123, 'global_step': 69614, 'preemption_count': 0}), (72016, {'train/accuracy': 0.6537835001945496, 'train/loss': 1.6537264585494995, 'train/bleu': 32.53735942164496, 'validation/accuracy': 0.673072874546051, 'validation/loss': 1.5194854736328125, 'validation/bleu': 29.342807344486634, 'validation/num_examples': 3000, 'test/accuracy': 0.6847946047782898, 'test/loss': 1.4506778717041016, 'test/bleu': 28.99919040780581, 'test/num_examples': 3003, 'score': 25229.91768527031, 'total_duration': 42094.46844172478, 'accumulated_submission_time': 25229.91768527031, 'accumulated_eval_time': 16861.239188194275, 'accumulated_logging_time': 1.0111916065216064, 'global_step': 72016, 'preemption_count': 0}), (74418, {'train/accuracy': 0.6587724685668945, 'train/loss': 1.636032223701477, 'train/bleu': 32.378273362747734, 'validation/accuracy': 0.6745235323905945, 'validation/loss': 1.510649561882019, 'validation/bleu': 29.499814008006418, 'validation/num_examples': 3000, 'test/accuracy': 0.6886991262435913, 'test/loss': 1.439121961593628, 'test/bleu': 29.56834137146062, 'test/num_examples': 3003, 'score': 26069.80594444275, 'total_duration': 43463.65474176407, 'accumulated_submission_time': 26069.80594444275, 'accumulated_eval_time': 17390.422934532166, 'accumulated_logging_time': 1.0483558177947998, 'global_step': 74418, 'preemption_count': 0}), (76820, {'train/accuracy': 0.6653993725776672, 'train/loss': 1.5844321250915527, 'train/bleu': 32.9791006066284, 'validation/accuracy': 0.6778588891029358, 'validation/loss': 1.5037970542907715, 'validation/bleu': 29.33848707278825, 'validation/num_examples': 3000, 'test/accuracy': 0.6883272528648376, 'test/loss': 1.435570240020752, 'test/bleu': 29.255923845563185, 'test/num_examples': 3003, 'score': 26909.785906791687, 'total_duration': 44844.65379357338, 'accumulated_submission_time': 26909.785906791687, 'accumulated_eval_time': 17931.327827215195, 'accumulated_logging_time': 1.0869407653808594, 'global_step': 76820, 'preemption_count': 0}), (79222, {'train/accuracy': 0.6607912182807922, 'train/loss': 1.6076757907867432, 'train/bleu': 33.057591815314304, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.5020564794540405, 'validation/bleu': 29.761661695762974, 'validation/num_examples': 3000, 'test/accuracy': 0.6905118823051453, 'test/loss': 1.4266692399978638, 'test/bleu': 29.21426525986103, 'test/num_examples': 3003, 'score': 27749.74374437332, 'total_duration': 46208.38493394852, 'accumulated_submission_time': 27749.74374437332, 'accumulated_eval_time': 18454.988405942917, 'accumulated_logging_time': 1.1236071586608887, 'global_step': 79222, 'preemption_count': 0}), (81624, {'train/accuracy': 0.6834427714347839, 'train/loss': 1.4710636138916016, 'train/bleu': 34.51069994560347, 'validation/accuracy': 0.6774373650550842, 'validation/loss': 1.4913984537124634, 'validation/bleu': 29.57352650655724, 'validation/num_examples': 3000, 'test/accuracy': 0.6915345191955566, 'test/loss': 1.4193542003631592, 'test/bleu': 29.30637132482035, 'test/num_examples': 3003, 'score': 28589.722110033035, 'total_duration': 47566.49417424202, 'accumulated_submission_time': 28589.722110033035, 'accumulated_eval_time': 18973.0031478405, 'accumulated_logging_time': 1.1630005836486816, 'global_step': 81624, 'preemption_count': 0}), (84026, {'train/accuracy': 0.6655948162078857, 'train/loss': 1.5831010341644287, 'train/bleu': 32.66122393028471, 'validation/accuracy': 0.675899863243103, 'validation/loss': 1.4887354373931885, 'validation/bleu': 29.643160583379153, 'validation/num_examples': 3000, 'test/accuracy': 0.6908488869667053, 'test/loss': 1.4123516082763672, 'test/bleu': 29.32195293579914, 'test/num_examples': 3003, 'score': 29429.614921092987, 'total_duration': 48987.2216424942, 'accumulated_submission_time': 29429.614921092987, 'accumulated_eval_time': 19553.72599005699, 'accumulated_logging_time': 1.1998803615570068, 'global_step': 84026, 'preemption_count': 0}), (86428, {'train/accuracy': 0.667698323726654, 'train/loss': 1.5811502933502197, 'train/bleu': 33.187502389890206, 'validation/accuracy': 0.6793344020843506, 'validation/loss': 1.481445074081421, 'validation/bleu': 29.7653434375351, 'validation/num_examples': 3000, 'test/accuracy': 0.6907791495323181, 'test/loss': 1.4073405265808105, 'test/bleu': 29.4747702020092, 'test/num_examples': 3003, 'score': 30269.603388786316, 'total_duration': 50345.42161464691, 'accumulated_submission_time': 30269.603388786316, 'accumulated_eval_time': 20071.824310064316, 'accumulated_logging_time': 1.2375590801239014, 'global_step': 86428, 'preemption_count': 0}), (88828, {'train/accuracy': 0.6753185987472534, 'train/loss': 1.5222102403640747, 'train/bleu': 33.65455010911434, 'validation/accuracy': 0.6817150115966797, 'validation/loss': 1.4678661823272705, 'validation/bleu': 30.07307836815615, 'validation/num_examples': 3000, 'test/accuracy': 0.6958921551704407, 'test/loss': 1.3935723304748535, 'test/bleu': 29.70958983317945, 'test/num_examples': 3003, 'score': 31109.718526124954, 'total_duration': 51754.98230290413, 'accumulated_submission_time': 31109.718526124954, 'accumulated_eval_time': 20641.15272283554, 'accumulated_logging_time': 1.2758679389953613, 'global_step': 88828, 'preemption_count': 0}), (91229, {'train/accuracy': 0.6722777485847473, 'train/loss': 1.5488667488098145, 'train/bleu': 33.57607223090198, 'validation/accuracy': 0.6819506287574768, 'validation/loss': 1.4655590057373047, 'validation/bleu': 30.27933128209323, 'validation/num_examples': 3000, 'test/accuracy': 0.697681725025177, 'test/loss': 1.3859319686889648, 'test/bleu': 30.023480420024, 'test/num_examples': 3003, 'score': 31949.66242647171, 'total_duration': 53098.72299027443, 'accumulated_submission_time': 31949.66242647171, 'accumulated_eval_time': 21144.83209013939, 'accumulated_logging_time': 1.315791130065918, 'global_step': 91229, 'preemption_count': 0}), (93631, {'train/accuracy': 0.6707874536514282, 'train/loss': 1.561752438545227, 'train/bleu': 33.5174534912928, 'validation/accuracy': 0.6827317476272583, 'validation/loss': 1.4605400562286377, 'validation/bleu': 30.069448126420095, 'validation/num_examples': 3000, 'test/accuracy': 0.7006217241287231, 'test/loss': 1.3737126588821411, 'test/bleu': 30.232836229992873, 'test/num_examples': 3003, 'score': 32789.70082950592, 'total_duration': 54462.08965873718, 'accumulated_submission_time': 32789.70082950592, 'accumulated_eval_time': 21668.045841693878, 'accumulated_logging_time': 1.3538849353790283, 'global_step': 93631, 'preemption_count': 0}), (96033, {'train/accuracy': 0.6781882047653198, 'train/loss': 1.5128364562988281, 'train/bleu': 34.07090496618493, 'validation/accuracy': 0.6862531304359436, 'validation/loss': 1.4463741779327393, 'validation/bleu': 30.29679901504323, 'validation/num_examples': 3000, 'test/accuracy': 0.6993434429168701, 'test/loss': 1.370069980621338, 'test/bleu': 30.150542856499595, 'test/num_examples': 3003, 'score': 33629.65852046013, 'total_duration': 55839.95797109604, 'accumulated_submission_time': 33629.65852046013, 'accumulated_eval_time': 22205.843585014343, 'accumulated_logging_time': 1.392345666885376, 'global_step': 96033, 'preemption_count': 0}), (98435, {'train/accuracy': 0.6749489307403564, 'train/loss': 1.5274507999420166, 'train/bleu': 33.67612239774069, 'validation/accuracy': 0.6862407326698303, 'validation/loss': 1.4482619762420654, 'validation/bleu': 30.31411470974103, 'validation/num_examples': 3000, 'test/accuracy': 0.7017953991889954, 'test/loss': 1.3655441999435425, 'test/bleu': 30.476222430731852, 'test/num_examples': 3003, 'score': 34469.67932343483, 'total_duration': 57214.098304986954, 'accumulated_submission_time': 34469.67932343483, 'accumulated_eval_time': 22739.84860920906, 'accumulated_logging_time': 1.4309487342834473, 'global_step': 98435, 'preemption_count': 0}), (100836, {'train/accuracy': 0.6923024654388428, 'train/loss': 1.423957109451294, 'train/bleu': 35.541552632794804, 'validation/accuracy': 0.6876789927482605, 'validation/loss': 1.4401698112487793, 'validation/bleu': 30.672736533118787, 'validation/num_examples': 3000, 'test/accuracy': 0.7013770341873169, 'test/loss': 1.360811710357666, 'test/bleu': 30.386893649507694, 'test/num_examples': 3003, 'score': 35309.65514802933, 'total_duration': 58546.80599832535, 'accumulated_submission_time': 35309.65514802933, 'accumulated_eval_time': 23232.463057994843, 'accumulated_logging_time': 1.4708812236785889, 'global_step': 100836, 'preemption_count': 0}), (103238, {'train/accuracy': 0.6829273700714111, 'train/loss': 1.4789576530456543, 'train/bleu': 34.769351800976146, 'validation/accuracy': 0.6884477734565735, 'validation/loss': 1.4318045377731323, 'validation/bleu': 30.403672649418127, 'validation/num_examples': 3000, 'test/accuracy': 0.7033408880233765, 'test/loss': 1.354027271270752, 'test/bleu': 30.52896680762646, 'test/num_examples': 3003, 'score': 36149.77913951874, 'total_duration': 59933.7256834507, 'accumulated_submission_time': 36149.77913951874, 'accumulated_eval_time': 23779.142338991165, 'accumulated_logging_time': 1.5111699104309082, 'global_step': 103238, 'preemption_count': 0}), (105640, {'train/accuracy': 0.6826414465904236, 'train/loss': 1.486212134361267, 'train/bleu': 34.5595366890984, 'validation/accuracy': 0.689315676689148, 'validation/loss': 1.428796410560608, 'validation/bleu': 30.46267469124044, 'validation/num_examples': 3000, 'test/accuracy': 0.7058508992195129, 'test/loss': 1.3480515480041504, 'test/bleu': 30.580010769399582, 'test/num_examples': 3003, 'score': 36989.887207746506, 'total_duration': 61272.41058278084, 'accumulated_submission_time': 36989.887207746506, 'accumulated_eval_time': 24277.60093665123, 'accumulated_logging_time': 1.552889108657837, 'global_step': 105640, 'preemption_count': 0}), (108043, {'train/accuracy': 0.6920853853225708, 'train/loss': 1.4408038854599, 'train/bleu': 35.49696534352309, 'validation/accuracy': 0.6909027695655823, 'validation/loss': 1.4207086563110352, 'validation/bleu': 30.654077905274068, 'validation/num_examples': 3000, 'test/accuracy': 0.7048399448394775, 'test/loss': 1.3408323526382446, 'test/bleu': 30.626020680832532, 'test/num_examples': 3003, 'score': 37829.96246767044, 'total_duration': 62608.53202152252, 'accumulated_submission_time': 37829.96246767044, 'accumulated_eval_time': 24773.52156305313, 'accumulated_logging_time': 1.6027710437774658, 'global_step': 108043, 'preemption_count': 0}), (110446, {'train/accuracy': 0.6905348300933838, 'train/loss': 1.441590428352356, 'train/bleu': 34.91691907999978, 'validation/accuracy': 0.6907168030738831, 'validation/loss': 1.420137882232666, 'validation/bleu': 30.70651994536399, 'validation/num_examples': 3000, 'test/accuracy': 0.7077334523200989, 'test/loss': 1.335753321647644, 'test/bleu': 30.601082920116674, 'test/num_examples': 3003, 'score': 38670.09367990494, 'total_duration': 63960.845781326294, 'accumulated_submission_time': 38670.09367990494, 'accumulated_eval_time': 25285.587152004242, 'accumulated_logging_time': 1.6442315578460693, 'global_step': 110446, 'preemption_count': 0}), (112848, {'train/accuracy': 0.7087449431419373, 'train/loss': 1.3407204151153564, 'train/bleu': 36.47203853513563, 'validation/accuracy': 0.691969096660614, 'validation/loss': 1.416434407234192, 'validation/bleu': 31.080921156801807, 'validation/num_examples': 3000, 'test/accuracy': 0.7069665193557739, 'test/loss': 1.3334004878997803, 'test/bleu': 30.83949578884752, 'test/num_examples': 3003, 'score': 39509.97726178169, 'total_duration': 65333.98978304863, 'accumulated_submission_time': 39509.97726178169, 'accumulated_eval_time': 25818.719385623932, 'accumulated_logging_time': 1.6956148147583008, 'global_step': 112848, 'preemption_count': 0}), (115250, {'train/accuracy': 0.6991199254989624, 'train/loss': 1.3918790817260742, 'train/bleu': 36.12728996133859, 'validation/accuracy': 0.693630576133728, 'validation/loss': 1.4105188846588135, 'validation/bleu': 31.00381458821544, 'validation/num_examples': 3000, 'test/accuracy': 0.7082214951515198, 'test/loss': 1.327876091003418, 'test/bleu': 30.625093720903884, 'test/num_examples': 3003, 'score': 40349.939930677414, 'total_duration': 66701.25009989738, 'accumulated_submission_time': 40349.939930677414, 'accumulated_eval_time': 26345.901841640472, 'accumulated_logging_time': 1.7358145713806152, 'global_step': 115250, 'preemption_count': 0}), (117651, {'train/accuracy': 0.6992983222007751, 'train/loss': 1.3957855701446533, 'train/bleu': 35.97326922225457, 'validation/accuracy': 0.6929610371589661, 'validation/loss': 1.409582495689392, 'validation/bleu': 31.041739597257934, 'validation/num_examples': 3000, 'test/accuracy': 0.7084422707557678, 'test/loss': 1.3249000310897827, 'test/bleu': 30.823761511515347, 'test/num_examples': 3003, 'score': 41189.89118170738, 'total_duration': 68058.64119935036, 'accumulated_submission_time': 41189.89118170738, 'accumulated_eval_time': 26863.21662259102, 'accumulated_logging_time': 1.7834343910217285, 'global_step': 117651, 'preemption_count': 0}), (120052, {'train/accuracy': 0.7118774056434631, 'train/loss': 1.3286157846450806, 'train/bleu': 36.789703266642434, 'validation/accuracy': 0.6940025687217712, 'validation/loss': 1.4108715057373047, 'validation/bleu': 30.938154978696495, 'validation/num_examples': 3000, 'test/accuracy': 0.7096391916275024, 'test/loss': 1.3247169256210327, 'test/bleu': 30.95885245135478, 'test/num_examples': 3003, 'score': 42029.80330038071, 'total_duration': 69427.26662182808, 'accumulated_submission_time': 42029.80330038071, 'accumulated_eval_time': 27391.812509059906, 'accumulated_logging_time': 1.8251049518585205, 'global_step': 120052, 'preemption_count': 0}), (122454, {'train/accuracy': 0.707240104675293, 'train/loss': 1.3504246473312378, 'train/bleu': 36.60618759390239, 'validation/accuracy': 0.6941017508506775, 'validation/loss': 1.4063982963562012, 'validation/bleu': 31.11960561212031, 'validation/num_examples': 3000, 'test/accuracy': 0.7113706469535828, 'test/loss': 1.3193117380142212, 'test/bleu': 30.76013239179441, 'test/num_examples': 3003, 'score': 42869.90353536606, 'total_duration': 70796.17909526825, 'accumulated_submission_time': 42869.90353536606, 'accumulated_eval_time': 27920.49778151512, 'accumulated_logging_time': 1.8757200241088867, 'global_step': 122454, 'preemption_count': 0}), (124856, {'train/accuracy': 0.7027835845947266, 'train/loss': 1.3786816596984863, 'train/bleu': 36.60843246209514, 'validation/accuracy': 0.6942257285118103, 'validation/loss': 1.4070228338241577, 'validation/bleu': 31.052944646685532, 'validation/num_examples': 3000, 'test/accuracy': 0.711661159992218, 'test/loss': 1.3174381256103516, 'test/bleu': 30.829254280160164, 'test/num_examples': 3003, 'score': 43710.02218127251, 'total_duration': 72155.17084789276, 'accumulated_submission_time': 43710.02218127251, 'accumulated_eval_time': 28439.25016117096, 'accumulated_logging_time': 1.9196293354034424, 'global_step': 124856, 'preemption_count': 0}), (127257, {'train/accuracy': 0.7099313735961914, 'train/loss': 1.3409501314163208, 'train/bleu': 36.71479864667537, 'validation/accuracy': 0.6950068473815918, 'validation/loss': 1.405134677886963, 'validation/bleu': 31.12469228435073, 'validation/num_examples': 3000, 'test/accuracy': 0.7116146683692932, 'test/loss': 1.3157283067703247, 'test/bleu': 30.96810588775437, 'test/num_examples': 3003, 'score': 44549.97412252426, 'total_duration': 73504.98891401291, 'accumulated_submission_time': 44549.97412252426, 'accumulated_eval_time': 28948.995346546173, 'accumulated_logging_time': 1.9638566970825195, 'global_step': 127257, 'preemption_count': 0}), (129659, {'train/accuracy': 0.7101288437843323, 'train/loss': 1.3376935720443726, 'train/bleu': 36.566009419018926, 'validation/accuracy': 0.6951928734779358, 'validation/loss': 1.4060457944869995, 'validation/bleu': 31.224968534748918, 'validation/num_examples': 3000, 'test/accuracy': 0.7120562791824341, 'test/loss': 1.3163964748382568, 'test/bleu': 30.971645040855503, 'test/num_examples': 3003, 'score': 45390.17083954811, 'total_duration': 74854.14743614197, 'accumulated_submission_time': 45390.17083954811, 'accumulated_eval_time': 29457.83890938759, 'accumulated_logging_time': 2.007380247116089, 'global_step': 129659, 'preemption_count': 0}), (132060, {'train/accuracy': 0.7102059721946716, 'train/loss': 1.3387534618377686, 'train/bleu': 36.377493267921025, 'validation/accuracy': 0.6951432824134827, 'validation/loss': 1.4058610200881958, 'validation/bleu': 31.325841393904454, 'validation/num_examples': 3000, 'test/accuracy': 0.7124397158622742, 'test/loss': 1.315784215927124, 'test/bleu': 30.992799598798722, 'test/num_examples': 3003, 'score': 46230.35232543945, 'total_duration': 76193.97706127167, 'accumulated_submission_time': 46230.35232543945, 'accumulated_eval_time': 29957.364458322525, 'accumulated_logging_time': 2.0503439903259277, 'global_step': 132060, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7138813138008118, 'train/loss': 1.3141133785247803, 'train/bleu': 36.535556096203386, 'validation/accuracy': 0.6952672600746155, 'validation/loss': 1.4057084321975708, 'validation/bleu': 31.22940684635679, 'validation/num_examples': 3000, 'test/accuracy': 0.7124048471450806, 'test/loss': 1.3156583309173584, 'test/bleu': 30.969127426676877, 'test/num_examples': 3003, 'score': 46675.375801324844, 'total_duration': 77146.18306612968, 'accumulated_submission_time': 46675.375801324844, 'accumulated_eval_time': 30464.457669734955, 'accumulated_logging_time': 2.0986831188201904, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0209 04:15:37.245139 140446903760704 submission_runner.py:586] Timing: 46675.375801324844
I0209 04:15:37.245193 140446903760704 submission_runner.py:588] Total number of evals: 57
I0209 04:15:37.245236 140446903760704 submission_runner.py:589] ====================
I0209 04:15:37.245280 140446903760704 submission_runner.py:542] Using RNG seed 1540897543
I0209 04:15:37.246878 140446903760704 submission_runner.py:551] --- Tuning run 4/5 ---
I0209 04:15:37.246997 140446903760704 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_4.
I0209 04:15:37.247395 140446903760704 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_4/hparams.json.
I0209 04:15:37.248240 140446903760704 submission_runner.py:206] Initializing dataset.
I0209 04:15:37.250799 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 04:15:37.253782 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0209 04:15:37.291849 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 04:15:37.945723 140446903760704 submission_runner.py:213] Initializing model.
I0209 04:15:44.419360 140446903760704 submission_runner.py:255] Initializing optimizer.
I0209 04:15:45.226307 140446903760704 submission_runner.py:262] Initializing metrics bundle.
I0209 04:15:45.226472 140446903760704 submission_runner.py:280] Initializing checkpoint and logger.
I0209 04:15:45.227488 140446903760704 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/wmt_jax/trial_4 with prefix checkpoint_
I0209 04:15:45.227614 140446903760704 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_4/meta_data_0.json.
I0209 04:15:45.227812 140446903760704 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0209 04:15:45.227869 140446903760704 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0209 04:15:45.772294 140446903760704 logger_utils.py:220] Unable to record git information. Continuing without it.
I0209 04:15:46.297082 140446903760704 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_4/flags_0.json.
I0209 04:15:46.300659 140446903760704 submission_runner.py:314] Starting training loop.
I0209 04:16:13.929419 140277054387968 logging_writer.py:48] [0] global_step=0, grad_norm=5.882989406585693, loss=11.163689613342285
I0209 04:16:13.940633 140446903760704 spec.py:321] Evaluating on the training split.
I0209 04:16:16.643750 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:20:53.621506 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 04:20:56.331048 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:25:33.086443 140446903760704 spec.py:349] Evaluating on the test split.
I0209 04:25:35.796210 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:30:12.606997 140446903760704 submission_runner.py:408] Time since start: 866.31s, 	Step: 1, 	{'train/accuracy': 0.0004915465833619237, 'train/loss': 11.190376281738281, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.639936685562134, 'total_duration': 866.306262254715, 'accumulated_submission_time': 27.639936685562134, 'accumulated_eval_time': 838.6662838459015, 'accumulated_logging_time': 0}
I0209 04:30:12.616026 140277062780672 logging_writer.py:48] [1] accumulated_eval_time=838.666284, accumulated_logging_time=0, accumulated_submission_time=27.639937, global_step=1, preemption_count=0, score=27.639937, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.191028, test/num_examples=3003, total_duration=866.306262, train/accuracy=0.000492, train/bleu=0.000000, train/loss=11.190376, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.190282, validation/num_examples=3000
I0209 04:30:47.510890 140277054387968 logging_writer.py:48] [100] global_step=100, grad_norm=0.7193015217781067, loss=7.609030246734619
I0209 04:31:22.482818 140277062780672 logging_writer.py:48] [200] global_step=200, grad_norm=0.6340203881263733, loss=6.695077896118164
I0209 04:31:57.530736 140277054387968 logging_writer.py:48] [300] global_step=300, grad_norm=0.7479122281074524, loss=5.965047359466553
I0209 04:32:32.553177 140277062780672 logging_writer.py:48] [400] global_step=400, grad_norm=0.653260350227356, loss=5.435127258300781
I0209 04:33:07.601436 140277054387968 logging_writer.py:48] [500] global_step=500, grad_norm=0.6800159811973572, loss=5.116532802581787
I0209 04:33:42.615021 140277062780672 logging_writer.py:48] [600] global_step=600, grad_norm=0.6123815774917603, loss=4.8370442390441895
I0209 04:34:17.613239 140277054387968 logging_writer.py:48] [700] global_step=700, grad_norm=0.5395627021789551, loss=4.473989009857178
I0209 04:34:52.648950 140277062780672 logging_writer.py:48] [800] global_step=800, grad_norm=0.6651414632797241, loss=4.245877265930176
I0209 04:35:27.679017 140277054387968 logging_writer.py:48] [900] global_step=900, grad_norm=0.418497771024704, loss=3.996910810470581
I0209 04:36:02.720924 140277062780672 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5168086290359497, loss=3.8008720874786377
I0209 04:36:37.766463 140277054387968 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.36535215377807617, loss=3.6310720443725586
I0209 04:37:12.814846 140277062780672 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.41841188073158264, loss=3.518446445465088
I0209 04:37:47.857886 140277054387968 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.37761399149894714, loss=3.444235324859619
I0209 04:38:22.896200 140277062780672 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.34311792254447937, loss=3.3288791179656982
I0209 04:38:57.928454 140277054387968 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.3182164132595062, loss=3.1610898971557617
I0209 04:39:32.927363 140277062780672 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.3174764811992645, loss=3.10408878326416
I0209 04:40:07.943945 140277054387968 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.20649076998233795, loss=2.981342077255249
I0209 04:40:42.960150 140277062780672 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.2673986852169037, loss=2.866089344024658
I0209 04:41:17.985471 140277054387968 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.25400835275650024, loss=2.874441146850586
I0209 04:41:53.025058 140277062780672 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.1907823234796524, loss=2.815728187561035
I0209 04:42:28.058283 140277054387968 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.19430041313171387, loss=2.696056842803955
I0209 04:43:03.079158 140277062780672 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.2288677841424942, loss=2.683352470397949
I0209 04:43:38.095634 140277054387968 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.2264019399881363, loss=2.6434316635131836
I0209 04:44:13.100346 140277062780672 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2462327629327774, loss=2.6225645542144775
I0209 04:44:13.106940 140446903760704 spec.py:321] Evaluating on the training split.
I0209 04:44:15.834343 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:46:47.952062 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 04:46:50.658202 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:49:28.666150 140446903760704 spec.py:349] Evaluating on the test split.
I0209 04:49:31.377474 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 04:51:50.091639 140446903760704 submission_runner.py:408] Time since start: 2163.79s, 	Step: 2401, 	{'train/accuracy': 0.5356904864311218, 'train/loss': 2.585400342941284, 'train/bleu': 23.428443626878675, 'validation/accuracy': 0.5370299220085144, 'validation/loss': 2.548142910003662, 'validation/bleu': 19.524479217978104, 'validation/num_examples': 3000, 'test/accuracy': 0.536575436592102, 'test/loss': 2.5754520893096924, 'test/bleu': 18.380749662236845, 'test/num_examples': 3003, 'score': 868.045458316803, 'total_duration': 2163.7909049987793, 'accumulated_submission_time': 868.045458316803, 'accumulated_eval_time': 1295.6509058475494, 'accumulated_logging_time': 0.018857955932617188}
I0209 04:51:50.106640 140277054387968 logging_writer.py:48] [2401] accumulated_eval_time=1295.650906, accumulated_logging_time=0.018858, accumulated_submission_time=868.045458, global_step=2401, preemption_count=0, score=868.045458, test/accuracy=0.536575, test/bleu=18.380750, test/loss=2.575452, test/num_examples=3003, total_duration=2163.790905, train/accuracy=0.535690, train/bleu=23.428444, train/loss=2.585400, validation/accuracy=0.537030, validation/bleu=19.524479, validation/loss=2.548143, validation/num_examples=3000
I0209 04:52:24.940985 140277062780672 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.271375834941864, loss=2.6439871788024902
I0209 04:52:59.921211 140277054387968 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2382274866104126, loss=2.54416561126709
I0209 04:53:34.909102 140277062780672 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.19543549418449402, loss=2.5420756340026855
I0209 04:54:09.890629 140277054387968 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.16879533231258392, loss=2.3519744873046875
I0209 04:54:44.863064 140277062780672 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.20660148561000824, loss=2.5108375549316406
I0209 04:55:19.852998 140277054387968 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.21344821155071259, loss=2.4606971740722656
I0209 04:55:54.846755 140277062780672 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.25600433349609375, loss=2.379866361618042
I0209 04:56:29.843341 140277054387968 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.31468746066093445, loss=2.296306848526001
I0209 04:57:04.858415 140277062780672 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.48200666904449463, loss=2.2792294025421143
I0209 04:57:39.838570 140277054387968 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.31246501207351685, loss=2.326338052749634
I0209 04:58:14.835686 140277062780672 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2883245348930359, loss=2.2333788871765137
I0209 04:58:49.792267 140277054387968 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.24294988811016083, loss=2.280181884765625
I0209 04:59:24.773971 140277062780672 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.46167057752609253, loss=2.1865463256835938
I0209 04:59:59.757310 140277054387968 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.2744574248790741, loss=2.236372709274292
I0209 05:00:34.786642 140277062780672 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.20571579039096832, loss=2.3234477043151855
I0209 05:01:09.815035 140277054387968 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.44365277886390686, loss=2.281809091567993
I0209 05:01:44.820084 140277062780672 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.4248448312282562, loss=2.2425694465637207
I0209 05:02:19.854435 140277054387968 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.2560131251811981, loss=2.2847254276275635
I0209 05:02:54.856279 140277062780672 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.35286200046539307, loss=2.2678515911102295
I0209 05:03:29.881633 140277054387968 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.3927038013935089, loss=2.296006441116333
I0209 05:04:04.861355 140277062780672 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3008626699447632, loss=2.19296932220459
I0209 05:04:39.888720 140277054387968 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5900669097900391, loss=2.311056137084961
I0209 05:05:14.877866 140277062780672 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.41310563683509827, loss=2.2029526233673096
I0209 05:05:49.878944 140277054387968 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.583561360836029, loss=2.168912649154663
I0209 05:05:50.300368 140446903760704 spec.py:321] Evaluating on the training split.
I0209 05:05:53.302703 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:08:50.210268 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 05:08:52.910192 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:11:24.083909 140446903760704 spec.py:349] Evaluating on the test split.
I0209 05:11:26.791198 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:13:49.380997 140446903760704 submission_runner.py:408] Time since start: 3483.08s, 	Step: 4803, 	{'train/accuracy': 0.5805889964103699, 'train/loss': 2.216299295425415, 'train/bleu': 27.33088719134872, 'validation/accuracy': 0.5971283316612244, 'validation/loss': 2.0716443061828613, 'validation/bleu': 23.725006938623974, 'validation/num_examples': 3000, 'test/accuracy': 0.6001161932945251, 'test/loss': 2.041588306427002, 'test/bleu': 22.17443894805829, 'test/num_examples': 3003, 'score': 1708.1538841724396, 'total_duration': 3483.080261707306, 'accumulated_submission_time': 1708.1538841724396, 'accumulated_eval_time': 1774.7314743995667, 'accumulated_logging_time': 0.04454636573791504}
I0209 05:13:49.396858 140277062780672 logging_writer.py:48] [4803] accumulated_eval_time=1774.731474, accumulated_logging_time=0.044546, accumulated_submission_time=1708.153884, global_step=4803, preemption_count=0, score=1708.153884, test/accuracy=0.600116, test/bleu=22.174439, test/loss=2.041588, test/num_examples=3003, total_duration=3483.080262, train/accuracy=0.580589, train/bleu=27.330887, train/loss=2.216299, validation/accuracy=0.597128, validation/bleu=23.725007, validation/loss=2.071644, validation/num_examples=3000
I0209 05:14:23.535825 140277054387968 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.37206438183784485, loss=2.2121710777282715
I0209 05:14:58.466439 140277062780672 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.20901310443878174, loss=2.1679186820983887
I0209 05:15:33.439016 140277054387968 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.4222024977207184, loss=2.1906816959381104
I0209 05:16:08.455429 140277062780672 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.31346753239631653, loss=2.169679641723633
I0209 05:16:43.423057 140277054387968 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.2756263017654419, loss=2.157661199569702
I0209 05:17:18.426546 140277062780672 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.3254135251045227, loss=2.2422404289245605
I0209 05:17:53.397774 140277054387968 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.3121010363101959, loss=2.159095287322998
I0209 05:18:28.393359 140277062780672 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.4192216992378235, loss=2.2248873710632324
I0209 05:19:03.356998 140277054387968 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6479533314704895, loss=2.1374504566192627
I0209 05:19:38.346550 140277062780672 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.3483336269855499, loss=2.1088762283325195
I0209 05:20:13.325669 140277054387968 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.28659263253211975, loss=2.2247314453125
I0209 05:20:48.360659 140277062780672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.3494292199611664, loss=2.0657365322113037
I0209 05:21:23.372044 140277054387968 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5779939889907837, loss=2.1383204460144043
I0209 05:21:58.358836 140277062780672 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5044882893562317, loss=2.245478868484497
I0209 05:22:33.355498 140277054387968 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5439109802246094, loss=2.166771173477173
I0209 05:23:08.345616 140277062780672 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.3493748903274536, loss=2.1950807571411133
I0209 05:23:43.342705 140277054387968 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.34776899218559265, loss=2.2092225551605225
I0209 05:24:18.385198 140277062780672 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.29470163583755493, loss=2.197507381439209
I0209 05:24:53.408687 140277054387968 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.47619712352752686, loss=2.2299022674560547
I0209 05:25:28.399413 140277062780672 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5590004920959473, loss=2.139577865600586
I0209 05:26:03.394433 140277054387968 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.2890026569366455, loss=2.192603588104248
I0209 05:26:38.413444 140277062780672 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6812379956245422, loss=2.1714935302734375
I0209 05:27:13.433886 140277054387968 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.3387340307235718, loss=2.1550047397613525
I0209 05:27:48.435169 140277062780672 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.40421006083488464, loss=2.154787540435791
I0209 05:27:49.557643 140446903760704 spec.py:321] Evaluating on the training split.
I0209 05:27:52.557208 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:30:41.553469 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 05:30:44.257019 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:33:25.104114 140446903760704 spec.py:349] Evaluating on the test split.
I0209 05:33:27.832378 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:36:05.249554 140446903760704 submission_runner.py:408] Time since start: 4818.95s, 	Step: 7205, 	{'train/accuracy': 0.5939295291900635, 'train/loss': 2.099783182144165, 'train/bleu': 27.582765593774532, 'validation/accuracy': 0.6013316512107849, 'validation/loss': 2.019782781600952, 'validation/bleu': 24.02121224259647, 'validation/num_examples': 3000, 'test/accuracy': 0.6096101403236389, 'test/loss': 1.9771912097930908, 'test/bleu': 23.194267112786342, 'test/num_examples': 3003, 'score': 2548.2274305820465, 'total_duration': 4818.948817253113, 'accumulated_submission_time': 2548.2274305820465, 'accumulated_eval_time': 2270.4233243465424, 'accumulated_logging_time': 0.07170796394348145}
I0209 05:36:05.265124 140277054387968 logging_writer.py:48] [7205] accumulated_eval_time=2270.423324, accumulated_logging_time=0.071708, accumulated_submission_time=2548.227431, global_step=7205, preemption_count=0, score=2548.227431, test/accuracy=0.609610, test/bleu=23.194267, test/loss=1.977191, test/num_examples=3003, total_duration=4818.948817, train/accuracy=0.593930, train/bleu=27.582766, train/loss=2.099783, validation/accuracy=0.601332, validation/bleu=24.021212, validation/loss=2.019783, validation/num_examples=3000
I0209 05:36:38.741612 140277062780672 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.40429213643074036, loss=2.119563102722168
I0209 05:37:13.699369 140277054387968 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.30317023396492004, loss=2.0850393772125244
I0209 05:37:48.675123 140277062780672 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.383211225271225, loss=2.165691375732422
I0209 05:38:23.674041 140277054387968 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.3813323676586151, loss=2.1292777061462402
I0209 05:38:58.652576 140277062780672 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.40345093607902527, loss=2.1180105209350586
I0209 05:39:33.655050 140277054387968 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.3049622178077698, loss=2.136927604675293
I0209 05:40:08.649978 140277062780672 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4353988766670227, loss=2.10246205329895
I0209 05:40:43.634133 140277054387968 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3997564911842346, loss=2.0696847438812256
I0209 05:41:18.615772 140277062780672 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5320925712585449, loss=2.1369028091430664
I0209 05:41:53.618068 140277054387968 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7054833173751831, loss=2.050543785095215
I0209 05:42:28.621376 140277062780672 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6180346012115479, loss=2.2171342372894287
I0209 05:43:03.626558 140277054387968 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.25242602825164795, loss=2.0867388248443604
I0209 05:43:38.642970 140277062780672 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.40306946635246277, loss=2.099147319793701
I0209 05:44:13.672978 140277054387968 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4424813985824585, loss=2.1310038566589355
I0209 05:44:48.658943 140277062780672 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.40108129382133484, loss=2.1385867595672607
I0209 05:45:23.645251 140277054387968 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3104196786880493, loss=2.0765810012817383
I0209 05:45:58.650241 140277062780672 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6332429647445679, loss=2.139232873916626
I0209 05:46:33.668430 140277054387968 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.35411766171455383, loss=2.182796001434326
I0209 05:47:08.660053 140277062780672 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.342721551656723, loss=2.0898351669311523
I0209 05:47:43.705857 140277054387968 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.4014799892902374, loss=2.2146472930908203
I0209 05:48:18.719052 140277062780672 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.2541077435016632, loss=2.0749564170837402
I0209 05:48:53.731386 140277054387968 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.34959667921066284, loss=2.1217589378356934
I0209 05:49:28.746985 140277062780672 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7388337254524231, loss=2.123960256576538
I0209 05:50:03.737370 140277054387968 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.2815381586551666, loss=2.1089699268341064
I0209 05:50:05.559489 140446903760704 spec.py:321] Evaluating on the training split.
I0209 05:50:08.555688 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:53:05.497820 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 05:53:08.202149 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:55:57.220889 140446903760704 spec.py:349] Evaluating on the test split.
I0209 05:55:59.936628 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 05:58:19.668981 140446903760704 submission_runner.py:408] Time since start: 6153.37s, 	Step: 9607, 	{'train/accuracy': 0.5930240750312805, 'train/loss': 2.122828960418701, 'train/bleu': 28.102238158911632, 'validation/accuracy': 0.610048234462738, 'validation/loss': 1.9808986186981201, 'validation/bleu': 24.52583438503706, 'validation/num_examples': 3000, 'test/accuracy': 0.6173610091209412, 'test/loss': 1.931437611579895, 'test/bleu': 23.49182647289432, 'test/num_examples': 3003, 'score': 3388.435542821884, 'total_duration': 6153.368245840073, 'accumulated_submission_time': 3388.435542821884, 'accumulated_eval_time': 2764.5327610969543, 'accumulated_logging_time': 0.09765505790710449}
I0209 05:58:19.685859 140277062780672 logging_writer.py:48] [9607] accumulated_eval_time=2764.532761, accumulated_logging_time=0.097655, accumulated_submission_time=3388.435543, global_step=9607, preemption_count=0, score=3388.435543, test/accuracy=0.617361, test/bleu=23.491826, test/loss=1.931438, test/num_examples=3003, total_duration=6153.368246, train/accuracy=0.593024, train/bleu=28.102238, train/loss=2.122829, validation/accuracy=0.610048, validation/bleu=24.525834, validation/loss=1.980899, validation/num_examples=3000
I0209 05:58:52.439980 140277054387968 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.47755128145217896, loss=2.054673194885254
I0209 05:59:27.383761 140277062780672 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.47117558121681213, loss=2.1015844345092773
I0209 06:00:02.576149 140277054387968 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.25133514404296875, loss=2.0609781742095947
I0209 06:00:37.568289 140277062780672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.2746443748474121, loss=2.0427165031433105
I0209 06:01:12.566287 140277054387968 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.4662673771381378, loss=2.051321029663086
I0209 06:01:47.574682 140277062780672 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.40677154064178467, loss=2.1017725467681885
I0209 06:02:22.562706 140277054387968 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.32185569405555725, loss=2.1067981719970703
I0209 06:02:57.588438 140277062780672 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3357540965080261, loss=2.0371627807617188
I0209 06:03:32.596782 140277054387968 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2881717085838318, loss=2.1713032722473145
I0209 06:04:07.593261 140277062780672 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.513918936252594, loss=2.121171474456787
I0209 06:04:42.565478 140277054387968 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.6138820648193359, loss=2.232450246810913
I0209 06:05:17.571484 140277062780672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4534585177898407, loss=2.1395463943481445
I0209 06:05:52.537376 140277054387968 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.2496211975812912, loss=2.0616393089294434
I0209 06:06:27.555759 140277062780672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.25756722688674927, loss=2.152998208999634
I0209 06:07:02.544582 140277054387968 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3614339828491211, loss=2.063795328140259
I0209 06:07:37.523191 140277062780672 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.42921799421310425, loss=2.1464972496032715
I0209 06:08:12.543852 140277054387968 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6890079975128174, loss=2.0728931427001953
I0209 06:08:47.541457 140277062780672 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7501260638237, loss=2.052964687347412
I0209 06:09:22.519253 140277054387968 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2522420287132263, loss=2.0832600593566895
I0209 06:09:57.513439 140277062780672 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5732191205024719, loss=2.09781813621521
I0209 06:10:32.499385 140277054387968 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.33199992775917053, loss=2.247349739074707
I0209 06:11:07.489390 140277062780672 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2869732081890106, loss=2.059303045272827
I0209 06:11:42.486505 140277054387968 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.48096007108688354, loss=2.1249024868011475
I0209 06:12:17.476713 140277062780672 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.291753888130188, loss=2.156076192855835
I0209 06:12:20.001298 140446903760704 spec.py:321] Evaluating on the training split.
I0209 06:12:23.006116 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 06:16:15.591936 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 06:16:18.289490 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 06:19:26.143657 140446903760704 spec.py:349] Evaluating on the test split.
I0209 06:19:28.850162 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 06:22:24.074506 140446903760704 submission_runner.py:408] Time since start: 7597.77s, 	Step: 12009, 	{'train/accuracy': 0.5922099947929382, 'train/loss': 2.1225879192352295, 'train/bleu': 28.130188839500367, 'validation/accuracy': 0.6139291524887085, 'validation/loss': 1.9412788152694702, 'validation/bleu': 25.084190063223115, 'validation/num_examples': 3000, 'test/accuracy': 0.6177793145179749, 'test/loss': 1.9158536195755005, 'test/bleu': 23.448882076309648, 'test/num_examples': 3003, 'score': 4228.665568351746, 'total_duration': 7597.773768186569, 'accumulated_submission_time': 4228.665568351746, 'accumulated_eval_time': 3368.605906009674, 'accumulated_logging_time': 0.12425565719604492}
I0209 06:22:24.091857 140277054387968 logging_writer.py:48] [12009] accumulated_eval_time=3368.605906, accumulated_logging_time=0.124256, accumulated_submission_time=4228.665568, global_step=12009, preemption_count=0, score=4228.665568, test/accuracy=0.617779, test/bleu=23.448882, test/loss=1.915854, test/num_examples=3003, total_duration=7597.773768, train/accuracy=0.592210, train/bleu=28.130189, train/loss=2.122588, validation/accuracy=0.613929, validation/bleu=25.084190, validation/loss=1.941279, validation/num_examples=3000
I0209 06:22:56.136356 140277062780672 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6018613576889038, loss=2.0153067111968994
I0209 06:23:31.059531 140277054387968 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6012895703315735, loss=2.090904712677002
I0209 06:24:06.035556 140277062780672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5381771922111511, loss=2.118443012237549
I0209 06:24:41.013989 140277054387968 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.4603271484375, loss=2.06648325920105
I0209 06:25:16.016216 140277062780672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.4197392463684082, loss=2.1006343364715576
I0209 06:25:50.992423 140277054387968 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.43591445684432983, loss=2.0796890258789062
I0209 06:26:26.012188 140277062780672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.33807238936424255, loss=2.096079111099243
I0209 06:27:00.995388 140277054387968 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4980221390724182, loss=2.0282440185546875
I0209 06:27:35.989101 140277062780672 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.29054558277130127, loss=2.110898733139038
I0209 06:28:10.979587 140277054387968 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.4176312983036041, loss=2.157677173614502
I0209 06:28:45.995766 140277062780672 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4627377986907959, loss=2.124803304672241
I0209 06:29:21.028294 140277054387968 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2657370865345001, loss=2.1611390113830566
I0209 06:29:56.040624 140277062780672 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2927229702472687, loss=2.0453295707702637
I0209 06:30:31.026539 140277054387968 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.883935272693634, loss=2.1356334686279297
I0209 06:31:06.023868 140277062780672 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3805318772792816, loss=2.0702896118164062
I0209 06:31:41.030776 140277054387968 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8647682666778564, loss=2.0479724407196045
I0209 06:32:16.035872 140277062780672 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.38458067178726196, loss=2.0920190811157227
I0209 06:32:51.049795 140277054387968 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7158600091934204, loss=2.118842363357544
I0209 06:33:26.071167 140277062780672 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.32172444462776184, loss=2.127776861190796
I0209 06:34:01.066099 140277054387968 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2920702397823334, loss=2.042020082473755
I0209 06:34:36.097729 140277062780672 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5345838069915771, loss=2.055339813232422
I0209 06:35:11.127977 140277054387968 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.45643454790115356, loss=2.097912549972534
I0209 06:35:46.126234 140277062780672 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.3891449570655823, loss=1.9800622463226318
I0209 06:36:21.155887 140277054387968 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2898615002632141, loss=2.0687942504882812
I0209 06:36:24.382263 140446903760704 spec.py:321] Evaluating on the training split.
I0209 06:36:27.376922 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 06:40:54.973010 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 06:40:57.683641 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 06:45:29.995683 140446903760704 spec.py:349] Evaluating on the test split.
I0209 06:45:32.696255 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 06:50:07.756547 140446903760704 submission_runner.py:408] Time since start: 9261.46s, 	Step: 14411, 	{'train/accuracy': 0.5989102721214294, 'train/loss': 2.0769054889678955, 'train/bleu': 27.914914125015642, 'validation/accuracy': 0.61870276927948, 'validation/loss': 1.9237356185913086, 'validation/bleu': 25.035330958622882, 'validation/num_examples': 3000, 'test/accuracy': 0.6252977848052979, 'test/loss': 1.8740822076797485, 'test/bleu': 23.781386907356108, 'test/num_examples': 3003, 'score': 5068.8694977760315, 'total_duration': 9261.455813646317, 'accumulated_submission_time': 5068.8694977760315, 'accumulated_eval_time': 4191.980131864548, 'accumulated_logging_time': 0.1532905101776123}
I0209 06:50:07.774009 140277062780672 logging_writer.py:48] [14411] accumulated_eval_time=4191.980132, accumulated_logging_time=0.153291, accumulated_submission_time=5068.869498, global_step=14411, preemption_count=0, score=5068.869498, test/accuracy=0.625298, test/bleu=23.781387, test/loss=1.874082, test/num_examples=3003, total_duration=9261.455814, train/accuracy=0.598910, train/bleu=27.914914, train/loss=2.076905, validation/accuracy=0.618703, validation/bleu=25.035331, validation/loss=1.923736, validation/num_examples=3000
I0209 06:50:39.130159 140277054387968 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.4883349537849426, loss=2.046915292739868
I0209 06:51:14.067074 140277062780672 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5855282545089722, loss=2.083010673522949
I0209 06:51:49.109010 140277054387968 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.38415709137916565, loss=2.033409595489502
I0209 06:52:24.133193 140277062780672 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.29006218910217285, loss=2.081942558288574
I0209 06:52:59.139390 140277054387968 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.36698877811431885, loss=2.104853868484497
I0209 06:53:34.155541 140277062780672 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.41211163997650146, loss=1.9908372163772583
I0209 06:54:09.228286 140277054387968 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.4350433051586151, loss=2.018310070037842
I0209 06:54:44.248853 140277062780672 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.516180157661438, loss=2.1928799152374268
I0209 06:55:19.306716 140277054387968 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.4942757487297058, loss=2.116746187210083
I0209 06:55:54.386256 140277062780672 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.4374482035636902, loss=1.9699561595916748
I0209 06:56:29.403393 140277054387968 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6225112080574036, loss=2.124127149581909
I0209 06:57:04.430110 140277062780672 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.34596359729766846, loss=2.1168439388275146
I0209 06:57:39.448825 140277054387968 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.45325160026550293, loss=2.0277953147888184
I0209 06:58:14.466416 140277062780672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6006383299827576, loss=2.0984482765197754
I0209 06:58:49.479366 140277054387968 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.2598039507865906, loss=2.155885934829712
I0209 06:59:24.497260 140277062780672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2545001208782196, loss=2.0898261070251465
I0209 06:59:59.489983 140277054387968 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5263257622718811, loss=2.1591594219207764
I0209 07:00:34.485569 140277062780672 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.4189480245113373, loss=2.0078585147857666
I0209 07:01:09.485790 140277054387968 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.4563997685909271, loss=2.046649932861328
I0209 07:01:44.484510 140277062780672 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.29343318939208984, loss=2.062143087387085
I0209 07:02:19.517939 140277054387968 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6752282977104187, loss=2.0658798217773438
I0209 07:02:54.521546 140277062780672 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.29015886783599854, loss=2.0367419719696045
I0209 07:03:29.559980 140277054387968 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3144502639770508, loss=1.9929383993148804
I0209 07:04:04.654507 140277062780672 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.34188714623451233, loss=2.1149983406066895
I0209 07:04:07.879204 140446903760704 spec.py:321] Evaluating on the training split.
I0209 07:04:10.882183 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:07:20.143415 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 07:07:22.847295 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:10:32.371990 140446903760704 spec.py:349] Evaluating on the test split.
I0209 07:10:35.075481 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:13:15.204568 140446903760704 submission_runner.py:408] Time since start: 10648.90s, 	Step: 16811, 	{'train/accuracy': 0.5971294641494751, 'train/loss': 2.0828609466552734, 'train/bleu': 28.010472281108505, 'validation/accuracy': 0.6194343566894531, 'validation/loss': 1.906266689300537, 'validation/bleu': 25.16562009613152, 'validation/num_examples': 3000, 'test/accuracy': 0.6220208406448364, 'test/loss': 1.8677488565444946, 'test/bleu': 23.640765822774625, 'test/num_examples': 3003, 'score': 5908.888803958893, 'total_duration': 10648.903832674026, 'accumulated_submission_time': 5908.888803958893, 'accumulated_eval_time': 4739.305437326431, 'accumulated_logging_time': 0.18077826499938965}
I0209 07:13:15.222525 140277054387968 logging_writer.py:48] [16811] accumulated_eval_time=4739.305437, accumulated_logging_time=0.180778, accumulated_submission_time=5908.888804, global_step=16811, preemption_count=0, score=5908.888804, test/accuracy=0.622021, test/bleu=23.640766, test/loss=1.867749, test/num_examples=3003, total_duration=10648.903833, train/accuracy=0.597129, train/bleu=28.010472, train/loss=2.082861, validation/accuracy=0.619434, validation/bleu=25.165620, validation/loss=1.906267, validation/num_examples=3000
I0209 07:13:46.559782 140277062780672 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8019441366195679, loss=2.040208101272583
I0209 07:14:21.518851 140277054387968 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3403901755809784, loss=2.1276068687438965
I0209 07:14:56.520026 140277062780672 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5724656581878662, loss=2.1064164638519287
I0209 07:15:31.547417 140277054387968 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.295322448015213, loss=2.009169578552246
I0209 07:16:06.542874 140277062780672 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3380371630191803, loss=2.101094961166382
I0209 07:16:41.532448 140277054387968 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6527064442634583, loss=2.0200388431549072
I0209 07:17:16.539708 140277062780672 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7869983911514282, loss=2.0557546615600586
I0209 07:17:51.530497 140277054387968 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.43108585476875305, loss=2.112696886062622
I0209 07:18:26.517964 140277062780672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.26862189173698425, loss=1.9864299297332764
I0209 07:19:01.505798 140277054387968 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2937418222427368, loss=2.073781728744507
I0209 07:19:36.488400 140277062780672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.37158727645874023, loss=2.066819190979004
I0209 07:20:11.506351 140277054387968 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.35252368450164795, loss=2.0722999572753906
I0209 07:20:46.514155 140277062780672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.32177549600601196, loss=2.096616506576538
I0209 07:21:21.501160 140277054387968 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.28175613284111023, loss=2.0577173233032227
I0209 07:21:56.544916 140277062780672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5175959467887878, loss=2.038649320602417
I0209 07:22:31.577344 140277054387968 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3411341905593872, loss=2.03159499168396
I0209 07:23:06.564338 140277062780672 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4826723635196686, loss=2.086890697479248
I0209 07:23:41.578154 140277054387968 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8833571076393127, loss=2.084397315979004
I0209 07:24:16.602398 140277062780672 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3428137004375458, loss=2.042813301086426
I0209 07:24:51.624995 140277054387968 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.6194316744804382, loss=2.0669422149658203
I0209 07:25:26.657830 140277062780672 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.5882629752159119, loss=1.9800729751586914
I0209 07:26:01.674502 140277054387968 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.515255331993103, loss=2.0752270221710205
I0209 07:26:36.680838 140277062780672 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8431568145751953, loss=2.0831258296966553
I0209 07:27:11.683954 140277054387968 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.2554893493652344, loss=2.0073390007019043
I0209 07:27:15.256095 140446903760704 spec.py:321] Evaluating on the training split.
I0209 07:27:18.261610 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:30:23.420956 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 07:30:26.117483 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:33:08.018708 140446903760704 spec.py:349] Evaluating on the test split.
I0209 07:33:10.722118 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:35:57.685194 140446903760704 submission_runner.py:408] Time since start: 12011.38s, 	Step: 19212, 	{'train/accuracy': 0.6081152558326721, 'train/loss': 1.9887399673461914, 'train/bleu': 29.55355088171276, 'validation/accuracy': 0.6222489476203918, 'validation/loss': 1.901801586151123, 'validation/bleu': 25.599329361585855, 'validation/num_examples': 3000, 'test/accuracy': 0.6279588937759399, 'test/loss': 1.8503223657608032, 'test/bleu': 24.208979117206603, 'test/num_examples': 3003, 'score': 6748.833927154541, 'total_duration': 12011.384460687637, 'accumulated_submission_time': 6748.833927154541, 'accumulated_eval_time': 5261.734477043152, 'accumulated_logging_time': 0.21037697792053223}
I0209 07:35:57.703211 140277062780672 logging_writer.py:48] [19212] accumulated_eval_time=5261.734477, accumulated_logging_time=0.210377, accumulated_submission_time=6748.833927, global_step=19212, preemption_count=0, score=6748.833927, test/accuracy=0.627959, test/bleu=24.208979, test/loss=1.850322, test/num_examples=3003, total_duration=12011.384461, train/accuracy=0.608115, train/bleu=29.553551, train/loss=1.988740, validation/accuracy=0.622249, validation/bleu=25.599329, validation/loss=1.901802, validation/num_examples=3000
I0209 07:36:28.719363 140277054387968 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.325708270072937, loss=2.022308111190796
I0209 07:37:03.661515 140277062780672 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3852347135543823, loss=2.125295639038086
I0209 07:37:38.658690 140277054387968 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5276444554328918, loss=2.1265532970428467
I0209 07:38:13.684811 140277062780672 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.24666643142700195, loss=2.0574679374694824
I0209 07:38:48.680909 140277054387968 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3312782645225525, loss=2.1719295978546143
I0209 07:39:23.678989 140277062780672 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.39718157052993774, loss=2.039320945739746
I0209 07:39:58.669726 140277054387968 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.38887307047843933, loss=2.0709993839263916
I0209 07:40:33.668243 140277062780672 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.30471935868263245, loss=2.131377935409546
I0209 07:41:08.657705 140277054387968 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.2771502435207367, loss=1.9929180145263672
I0209 07:41:43.660955 140277062780672 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.34542837738990784, loss=2.0798473358154297
I0209 07:42:18.671926 140277054387968 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.46787217259407043, loss=2.0968217849731445
I0209 07:42:53.699261 140277062780672 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.3003503084182739, loss=2.16518235206604
I0209 07:43:28.740699 140277054387968 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.34828656911849976, loss=2.040153980255127
I0209 07:44:03.776978 140277062780672 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3475819230079651, loss=2.0792298316955566
I0209 07:44:38.909421 140277054387968 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.48691418766975403, loss=2.0148558616638184
I0209 07:45:13.931891 140277062780672 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6048625707626343, loss=2.0563251972198486
I0209 07:45:48.924727 140277054387968 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.2731221616268158, loss=1.9718637466430664
I0209 07:46:23.920684 140277062780672 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.4742632210254669, loss=2.0445468425750732
I0209 07:46:58.937845 140277054387968 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.48150181770324707, loss=2.1326210498809814
I0209 07:47:33.921846 140277062780672 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.740979015827179, loss=2.135739803314209
I0209 07:48:08.911058 140277054387968 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.45288723707199097, loss=2.0929007530212402
I0209 07:48:43.903022 140277062780672 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.2830943763256073, loss=2.106170415878296
I0209 07:49:18.904284 140277054387968 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2626512050628662, loss=2.1243507862091064
I0209 07:49:53.882892 140277062780672 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.42975205183029175, loss=1.9886308908462524
I0209 07:49:57.805237 140446903760704 spec.py:321] Evaluating on the training split.
I0209 07:50:00.799615 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:52:52.698985 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 07:52:55.419646 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:55:43.816211 140446903760704 spec.py:349] Evaluating on the test split.
I0209 07:55:46.520315 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 07:58:20.978078 140446903760704 submission_runner.py:408] Time since start: 13354.68s, 	Step: 21613, 	{'train/accuracy': 0.6023226380348206, 'train/loss': 2.051201105117798, 'train/bleu': 28.267704243990025, 'validation/accuracy': 0.6232532858848572, 'validation/loss': 1.8913344144821167, 'validation/bleu': 25.35422343622899, 'validation/num_examples': 3000, 'test/accuracy': 0.6319214701652527, 'test/loss': 1.8304063081741333, 'test/bleu': 25.061395584676017, 'test/num_examples': 3003, 'score': 7588.848192691803, 'total_duration': 13354.67732167244, 'accumulated_submission_time': 7588.848192691803, 'accumulated_eval_time': 5764.907237768173, 'accumulated_logging_time': 0.24018168449401855}
I0209 07:58:20.996595 140277054387968 logging_writer.py:48] [21613] accumulated_eval_time=5764.907238, accumulated_logging_time=0.240182, accumulated_submission_time=7588.848193, global_step=21613, preemption_count=0, score=7588.848193, test/accuracy=0.631921, test/bleu=25.061396, test/loss=1.830406, test/num_examples=3003, total_duration=13354.677322, train/accuracy=0.602323, train/bleu=28.267704, train/loss=2.051201, validation/accuracy=0.623253, validation/bleu=25.354223, validation/loss=1.891334, validation/num_examples=3000
I0209 07:58:51.655544 140277062780672 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.28783997893333435, loss=1.983057975769043
I0209 07:59:26.703354 140277054387968 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.6233367323875427, loss=2.0310404300689697
I0209 08:00:01.745669 140277062780672 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.6114979386329651, loss=2.128864288330078
I0209 08:00:36.715986 140277054387968 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5274382829666138, loss=2.0305228233337402
I0209 08:01:11.713866 140277062780672 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.3052808940410614, loss=2.1611931324005127
I0209 08:01:46.739510 140277054387968 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.2984682321548462, loss=2.0978124141693115
I0209 08:02:21.723849 140277062780672 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.44251784682273865, loss=1.9924874305725098
I0209 08:02:56.717889 140277054387968 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7771456241607666, loss=2.020681142807007
I0209 08:03:31.729253 140277062780672 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7088217735290527, loss=2.0273947715759277
I0209 08:04:06.760503 140277054387968 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.4603586494922638, loss=2.0236740112304688
I0209 08:04:41.747924 140277062780672 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.39214831590652466, loss=2.1510818004608154
I0209 08:05:16.772432 140277054387968 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.6429327130317688, loss=2.0221855640411377
I0209 08:05:51.814359 140277062780672 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.33439233899116516, loss=2.0459465980529785
I0209 08:06:26.844776 140277054387968 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5410546660423279, loss=2.0315310955047607
I0209 08:07:01.845002 140277062780672 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5411618947982788, loss=2.05446195602417
I0209 08:07:36.832706 140277054387968 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.4823404550552368, loss=2.0622119903564453
I0209 08:08:11.829411 140277062780672 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.31128421425819397, loss=2.092560052871704
I0209 08:08:46.841268 140277054387968 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.47732698917388916, loss=2.1367504596710205
I0209 08:09:21.866072 140277062780672 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4753482937812805, loss=1.9815362691879272
I0209 08:09:56.886421 140277054387968 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.37704014778137207, loss=2.1356513500213623
I0209 08:10:31.910324 140277062780672 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3270021378993988, loss=2.0900962352752686
I0209 08:11:06.923749 140277054387968 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.30238860845565796, loss=2.0306107997894287
I0209 08:11:41.920288 140277062780672 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3411729037761688, loss=1.9901057481765747
I0209 08:12:16.905278 140277054387968 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.42131704092025757, loss=1.9965558052062988
I0209 08:12:21.182582 140446903760704 spec.py:321] Evaluating on the training split.
I0209 08:12:24.173435 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 08:15:14.763688 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 08:15:17.471845 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 08:17:53.637575 140446903760704 spec.py:349] Evaluating on the test split.
I0209 08:17:56.348845 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 08:20:17.058324 140446903760704 submission_runner.py:408] Time since start: 14670.76s, 	Step: 24014, 	{'train/accuracy': 0.601838231086731, 'train/loss': 2.06695556640625, 'train/bleu': 28.35724378331637, 'validation/accuracy': 0.622211754322052, 'validation/loss': 1.8881958723068237, 'validation/bleu': 25.447011922234104, 'validation/num_examples': 3000, 'test/accuracy': 0.6272500157356262, 'test/loss': 1.8352961540222168, 'test/bleu': 24.594230099449156, 'test/num_examples': 3003, 'score': 8428.94619846344, 'total_duration': 14670.757593154907, 'accumulated_submission_time': 8428.94619846344, 'accumulated_eval_time': 6240.782923460007, 'accumulated_logging_time': 0.2702040672302246}
I0209 08:20:17.076624 140277062780672 logging_writer.py:48] [24014] accumulated_eval_time=6240.782923, accumulated_logging_time=0.270204, accumulated_submission_time=8428.946198, global_step=24014, preemption_count=0, score=8428.946198, test/accuracy=0.627250, test/bleu=24.594230, test/loss=1.835296, test/num_examples=3003, total_duration=14670.757593, train/accuracy=0.601838, train/bleu=28.357244, train/loss=2.066956, validation/accuracy=0.622212, validation/bleu=25.447012, validation/loss=1.888196, validation/num_examples=3000
I0209 08:20:47.394886 140277054387968 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.4905695617198944, loss=2.02504563331604
I0209 08:21:22.373839 140277062780672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.333015114068985, loss=2.065100908279419
I0209 08:21:57.398697 140277054387968 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.39755478501319885, loss=2.035310745239258
I0209 08:22:32.443824 140277062780672 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.24745649099349976, loss=1.9906973838806152
I0209 08:23:07.487474 140277054387968 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4360429048538208, loss=1.9889817237854004
I0209 08:23:42.458713 140277062780672 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.348752498626709, loss=1.9620610475540161
I0209 08:24:17.458483 140277054387968 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.4604407548904419, loss=2.001373529434204
I0209 08:24:52.463942 140277062780672 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.44758450984954834, loss=2.0174126625061035
I0209 08:25:27.446892 140277054387968 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3287356495857239, loss=2.0667972564697266
I0209 08:26:02.475629 140277062780672 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5525304079055786, loss=2.059924364089966
I0209 08:26:37.456243 140277054387968 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3700335919857025, loss=2.0189738273620605
I0209 08:27:12.476737 140277062780672 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4374871850013733, loss=1.9768612384796143
I0209 08:27:47.492380 140277054387968 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.27558633685112, loss=2.166700839996338
I0209 08:28:22.494738 140277062780672 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6430098414421082, loss=2.0631814002990723
I0209 08:28:57.565900 140277054387968 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4007563889026642, loss=2.0377049446105957
I0209 08:29:32.562149 140277062780672 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.640320897102356, loss=2.0321741104125977
I0209 08:30:07.578147 140277054387968 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.4485330581665039, loss=2.0383999347686768
I0209 08:30:42.583287 140277062780672 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.45629051327705383, loss=2.1178171634674072
I0209 08:31:17.589135 140277054387968 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.45313024520874023, loss=2.046560287475586
I0209 08:31:52.628398 140277062780672 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5694518089294434, loss=1.98387610912323
I0209 08:32:27.610945 140277054387968 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.4566657245159149, loss=2.0360028743743896
I0209 08:33:02.628018 140277062780672 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.39236992597579956, loss=2.042651653289795
I0209 08:33:37.612591 140277054387968 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4449160099029541, loss=2.028888463973999
I0209 08:34:12.639820 140277062780672 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.35308748483657837, loss=2.048940896987915
I0209 08:34:17.261700 140446903760704 spec.py:321] Evaluating on the training split.
I0209 08:34:20.275857 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 08:37:09.021096 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 08:37:11.742485 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 08:39:51.943337 140446903760704 spec.py:349] Evaluating on the test split.
I0209 08:39:54.654217 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 08:42:25.255063 140446903760704 submission_runner.py:408] Time since start: 15998.95s, 	Step: 26415, 	{'train/accuracy': 0.6068945527076721, 'train/loss': 2.0167477130889893, 'train/bleu': 29.215509208704106, 'validation/accuracy': 0.6261794567108154, 'validation/loss': 1.86761474609375, 'validation/bleu': 26.067529651612723, 'validation/num_examples': 3000, 'test/accuracy': 0.6306548118591309, 'test/loss': 1.823778748512268, 'test/bleu': 24.450720016161004, 'test/num_examples': 3003, 'score': 9269.041829109192, 'total_duration': 15998.954328775406, 'accumulated_submission_time': 9269.041829109192, 'accumulated_eval_time': 6728.776242017746, 'accumulated_logging_time': 0.2997102737426758}
I0209 08:42:25.275719 140277054387968 logging_writer.py:48] [26415] accumulated_eval_time=6728.776242, accumulated_logging_time=0.299710, accumulated_submission_time=9269.041829, global_step=26415, preemption_count=0, score=9269.041829, test/accuracy=0.630655, test/bleu=24.450720, test/loss=1.823779, test/num_examples=3003, total_duration=15998.954329, train/accuracy=0.606895, train/bleu=29.215509, train/loss=2.016748, validation/accuracy=0.626179, validation/bleu=26.067530, validation/loss=1.867615, validation/num_examples=3000
I0209 08:42:55.270153 140277062780672 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.42918893694877625, loss=1.9838013648986816
I0209 08:43:30.238586 140277054387968 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.36928102374076843, loss=1.9776611328125
I0209 08:44:05.269665 140277062780672 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3583437204360962, loss=2.0605430603027344
I0209 08:44:40.289108 140277054387968 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.25749146938323975, loss=2.079336643218994
I0209 08:45:15.307887 140277062780672 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.41334351897239685, loss=2.033693313598633
I0209 08:45:50.321741 140277054387968 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.30221104621887207, loss=2.0133450031280518
I0209 08:46:25.340884 140277062780672 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4508216381072998, loss=2.002615213394165
I0209 08:47:00.373485 140277054387968 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3916912078857422, loss=1.9790018796920776
I0209 08:47:35.375377 140277062780672 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.41060760617256165, loss=1.9400006532669067
I0209 08:48:10.382087 140277054387968 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2944429814815521, loss=2.0008785724639893
I0209 08:48:45.373921 140277062780672 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2586188018321991, loss=2.016364097595215
I0209 08:49:20.357869 140277054387968 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.28598707914352417, loss=2.0612571239471436
I0209 08:49:55.347579 140277062780672 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4271010458469391, loss=2.0108609199523926
I0209 08:50:30.348645 140277054387968 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.3698245584964752, loss=2.0556893348693848
I0209 08:51:05.355772 140277062780672 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.3001120984554291, loss=1.9729722738265991
I0209 08:51:40.334801 140277054387968 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6476174592971802, loss=2.186457872390747
I0209 08:52:15.349831 140277062780672 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.47271728515625, loss=1.944369912147522
I0209 08:52:50.341676 140277054387968 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.5539298057556152, loss=1.999645471572876
I0209 08:53:25.372256 140277062780672 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.4228600859642029, loss=2.0200576782226562
I0209 08:54:00.375246 140277054387968 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.2747986912727356, loss=1.9812535047531128
I0209 08:54:35.358959 140277062780672 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.4083004891872406, loss=1.9580217599868774
I0209 08:55:10.399205 140277054387968 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.360045850276947, loss=2.129509449005127
I0209 08:55:45.389080 140277062780672 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6646687984466553, loss=2.0340144634246826
I0209 08:56:20.419612 140277054387968 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3079570233821869, loss=2.063427448272705
I0209 08:56:25.394578 140446903760704 spec.py:321] Evaluating on the training split.
I0209 08:56:28.399538 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:00:12.149926 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 09:00:14.852656 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:03:17.455095 140446903760704 spec.py:349] Evaluating on the test split.
I0209 09:03:20.157660 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:06:58.345798 140446903760704 submission_runner.py:408] Time since start: 17472.05s, 	Step: 28816, 	{'train/accuracy': 0.6076061129570007, 'train/loss': 2.0170085430145264, 'train/bleu': 28.619133118767294, 'validation/accuracy': 0.6236252188682556, 'validation/loss': 1.8711217641830444, 'validation/bleu': 25.540494610045837, 'validation/num_examples': 3000, 'test/accuracy': 0.6310731768608093, 'test/loss': 1.814279556274414, 'test/bleu': 24.502543676121828, 'test/num_examples': 3003, 'score': 10109.073967218399, 'total_duration': 17472.045060634613, 'accumulated_submission_time': 10109.073967218399, 'accumulated_eval_time': 7361.727405786514, 'accumulated_logging_time': 0.3307063579559326}
I0209 09:06:58.365657 140277062780672 logging_writer.py:48] [28816] accumulated_eval_time=7361.727406, accumulated_logging_time=0.330706, accumulated_submission_time=10109.073967, global_step=28816, preemption_count=0, score=10109.073967, test/accuracy=0.631073, test/bleu=24.502544, test/loss=1.814280, test/num_examples=3003, total_duration=17472.045061, train/accuracy=0.607606, train/bleu=28.619133, train/loss=2.017009, validation/accuracy=0.623625, validation/bleu=25.540495, validation/loss=1.871122, validation/num_examples=3000
I0209 09:07:27.980150 140277054387968 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.48891860246658325, loss=2.1180360317230225
I0209 09:08:03.030920 140277062780672 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.26564645767211914, loss=1.9661002159118652
I0209 09:08:37.983380 140277054387968 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.32726719975471497, loss=1.991389274597168
I0209 09:09:12.984760 140277062780672 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.2739698588848114, loss=2.0999979972839355
I0209 09:09:47.966523 140277054387968 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6610601544380188, loss=2.036437749862671
I0209 09:10:22.968210 140277062780672 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.32190102338790894, loss=2.114558696746826
I0209 09:10:58.028678 140277054387968 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2962845265865326, loss=1.9278472661972046
I0209 09:11:33.048562 140277062780672 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5659221410751343, loss=1.9941736459732056
I0209 09:12:08.037426 140277054387968 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6796315908432007, loss=2.026456356048584
I0209 09:12:43.027029 140277062780672 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.2390350103378296, loss=2.0247669219970703
I0209 09:13:18.061527 140277054387968 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.2705560326576233, loss=1.986040711402893
I0209 09:13:53.104448 140277062780672 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6640028953552246, loss=2.0284299850463867
I0209 09:14:28.142359 140277054387968 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.2668306827545166, loss=2.049901247024536
I0209 09:15:03.123390 140277062780672 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.704215407371521, loss=1.9778867959976196
I0209 09:15:38.150710 140277054387968 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.2571689486503601, loss=1.9816958904266357
I0209 09:16:13.117387 140277062780672 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.2225283533334732, loss=2.0573699474334717
I0209 09:16:48.101464 140277054387968 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.44940683245658875, loss=2.0434563159942627
I0209 09:17:23.102753 140277062780672 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.373306542634964, loss=1.9500969648361206
I0209 09:17:58.092398 140277054387968 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3213241398334503, loss=2.073280096054077
I0209 09:18:33.061619 140277062780672 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.667608380317688, loss=1.9642432928085327
I0209 09:19:08.020685 140277054387968 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3979460597038269, loss=1.9687072038650513
I0209 09:19:43.027875 140277062780672 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.4421417713165283, loss=1.9990566968917847
I0209 09:20:18.004639 140277054387968 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.2614758312702179, loss=2.000368356704712
I0209 09:20:53.010993 140277062780672 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6096358299255371, loss=1.9592058658599854
I0209 09:20:58.683798 140446903760704 spec.py:321] Evaluating on the training split.
I0209 09:21:01.698359 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:24:09.461726 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 09:24:12.164756 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:26:55.138591 140446903760704 spec.py:349] Evaluating on the test split.
I0209 09:26:57.853785 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:29:46.036158 140446903760704 submission_runner.py:408] Time since start: 18839.74s, 	Step: 31218, 	{'train/accuracy': 0.6038236021995544, 'train/loss': 2.032376766204834, 'train/bleu': 28.428667172127135, 'validation/accuracy': 0.6241955757141113, 'validation/loss': 1.8636945486068726, 'validation/bleu': 25.288898614166587, 'validation/num_examples': 3000, 'test/accuracy': 0.630387544631958, 'test/loss': 1.82143235206604, 'test/bleu': 24.071335933790778, 'test/num_examples': 3003, 'score': 10949.304612398148, 'total_duration': 18839.73539876938, 'accumulated_submission_time': 10949.304612398148, 'accumulated_eval_time': 7889.079682350159, 'accumulated_logging_time': 0.3619084358215332}
I0209 09:29:46.056063 140277054387968 logging_writer.py:48] [31218] accumulated_eval_time=7889.079682, accumulated_logging_time=0.361908, accumulated_submission_time=10949.304612, global_step=31218, preemption_count=0, score=10949.304612, test/accuracy=0.630388, test/bleu=24.071336, test/loss=1.821432, test/num_examples=3003, total_duration=18839.735399, train/accuracy=0.603824, train/bleu=28.428667, train/loss=2.032377, validation/accuracy=0.624196, validation/bleu=25.288899, validation/loss=1.863695, validation/num_examples=3000
I0209 09:30:14.992585 140277062780672 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3728792369365692, loss=2.0532002449035645
I0209 09:30:49.905761 140277054387968 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.3245415687561035, loss=2.0354363918304443
I0209 09:31:24.909097 140277062780672 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.42887499928474426, loss=1.9631792306900024
I0209 09:31:59.925234 140277054387968 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.25842538475990295, loss=1.9624197483062744
I0209 09:32:34.885708 140277062780672 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.32814574241638184, loss=2.07069993019104
I0209 09:33:09.912740 140277054387968 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3027278184890747, loss=2.0428614616394043
I0209 09:33:44.904319 140277062780672 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.42122694849967957, loss=2.00293231010437
I0209 09:34:19.904035 140277054387968 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3596898019313812, loss=2.0096452236175537
I0209 09:34:54.905333 140277062780672 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6573591828346252, loss=2.0259995460510254
I0209 09:35:29.901942 140277054387968 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.4887073040008545, loss=2.017773151397705
I0209 09:36:04.909131 140277062780672 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3790971040725708, loss=1.9241076707839966
I0209 09:36:39.897574 140277054387968 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2968748211860657, loss=2.140697956085205
I0209 09:37:14.874554 140277062780672 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7667806148529053, loss=1.9865351915359497
I0209 09:37:49.865963 140277054387968 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3366832435131073, loss=1.9620529413223267
I0209 09:38:24.886084 140277062780672 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3750091791152954, loss=1.9851096868515015
I0209 09:38:59.875825 140277054387968 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.2732755243778229, loss=2.1129560470581055
I0209 09:39:34.883946 140277062780672 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.33622387051582336, loss=2.003429889678955
I0209 09:40:09.898247 140277054387968 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3937276303768158, loss=2.022411346435547
I0209 09:40:44.898561 140277062780672 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.3303782343864441, loss=2.0423436164855957
I0209 09:41:19.898875 140277054387968 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.23074957728385925, loss=2.105363130569458
I0209 09:41:54.906903 140277062780672 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.2818068563938141, loss=1.9935601949691772
I0209 09:42:29.882648 140277054387968 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6717877984046936, loss=1.9958608150482178
I0209 09:43:04.906771 140277062780672 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.31165650486946106, loss=2.108851671218872
I0209 09:43:39.899838 140277054387968 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.35255318880081177, loss=2.0003082752227783
I0209 09:43:46.284201 140446903760704 spec.py:321] Evaluating on the training split.
I0209 09:43:49.276796 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:46:38.010500 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 09:46:40.705198 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:49:18.146235 140446903760704 spec.py:349] Evaluating on the test split.
I0209 09:49:20.843623 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 09:52:04.432835 140446903760704 submission_runner.py:408] Time since start: 20178.13s, 	Step: 33620, 	{'train/accuracy': 0.609279990196228, 'train/loss': 1.99933660030365, 'train/bleu': 28.38434953125899, 'validation/accuracy': 0.6294032335281372, 'validation/loss': 1.85016667842865, 'validation/bleu': 25.796464877571452, 'validation/num_examples': 3000, 'test/accuracy': 0.6363604664802551, 'test/loss': 1.7914565801620483, 'test/bleu': 24.920774993304907, 'test/num_examples': 3003, 'score': 11789.44777727127, 'total_duration': 20178.132102012634, 'accumulated_submission_time': 11789.44777727127, 'accumulated_eval_time': 8387.22826910019, 'accumulated_logging_time': 0.3919835090637207}
I0209 09:52:04.452215 140277062780672 logging_writer.py:48] [33620] accumulated_eval_time=8387.228269, accumulated_logging_time=0.391984, accumulated_submission_time=11789.447777, global_step=33620, preemption_count=0, score=11789.447777, test/accuracy=0.636360, test/bleu=24.920775, test/loss=1.791457, test/num_examples=3003, total_duration=20178.132102, train/accuracy=0.609280, train/bleu=28.384350, train/loss=1.999337, validation/accuracy=0.629403, validation/bleu=25.796465, validation/loss=1.850167, validation/num_examples=3000
I0209 09:52:32.693359 140277054387968 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.2885884642601013, loss=1.9684789180755615
I0209 09:53:07.593608 140277062780672 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6609572768211365, loss=2.0033700466156006
I0209 09:53:42.576155 140277054387968 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5590699315071106, loss=2.0522491931915283
I0209 09:54:17.541626 140277062780672 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2598607838153839, loss=2.0090291500091553
I0209 09:54:52.518171 140277054387968 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.3199350833892822, loss=1.9847973585128784
I0209 09:55:27.512522 140277062780672 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.46316343545913696, loss=2.09954571723938
I0209 09:56:02.496364 140277054387968 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.490767240524292, loss=2.0744986534118652
I0209 09:56:37.519981 140277062780672 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.4670000970363617, loss=1.943589687347412
I0209 09:57:12.540749 140277054387968 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.26305192708969116, loss=2.0557079315185547
I0209 09:57:47.556556 140277062780672 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.3746132254600525, loss=1.979668378829956
I0209 09:58:22.531732 140277054387968 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3964216113090515, loss=1.9665645360946655
I0209 09:58:57.518968 140277062780672 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.3096064627170563, loss=2.038191318511963
I0209 09:59:32.527697 140277054387968 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.2818738520145416, loss=2.0891454219818115
I0209 10:00:07.537513 140277062780672 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.498262494802475, loss=2.0089211463928223
I0209 10:00:42.524294 140277054387968 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.3050517439842224, loss=2.0468411445617676
I0209 10:01:17.528209 140277062780672 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6155271530151367, loss=2.0357422828674316
I0209 10:01:52.534481 140277054387968 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.3404276967048645, loss=1.9742677211761475
I0209 10:02:27.550896 140277062780672 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.28507503867149353, loss=1.9849591255187988
I0209 10:03:02.555249 140277054387968 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.412964791059494, loss=2.080540180206299
I0209 10:03:37.558129 140277062780672 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.552169144153595, loss=2.006622076034546
I0209 10:04:12.599944 140277054387968 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2781737148761749, loss=2.0384361743927
I0209 10:04:47.600540 140277062780672 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.3875456154346466, loss=1.9241223335266113
I0209 10:05:22.594337 140277054387968 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.4554695785045624, loss=1.9937101602554321
I0209 10:05:57.607619 140277062780672 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.33601778745651245, loss=1.9423959255218506
I0209 10:06:04.687719 140446903760704 spec.py:321] Evaluating on the training split.
I0209 10:06:07.688730 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:10:06.273005 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 10:10:08.959616 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:12:57.070835 140446903760704 spec.py:349] Evaluating on the test split.
I0209 10:12:59.772885 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:15:36.700689 140446903760704 submission_runner.py:408] Time since start: 21590.40s, 	Step: 36022, 	{'train/accuracy': 0.6051180958747864, 'train/loss': 2.022434711456299, 'train/bleu': 28.503332269728375, 'validation/accuracy': 0.6277913451194763, 'validation/loss': 1.8456335067749023, 'validation/bleu': 25.854004220279077, 'validation/num_examples': 3000, 'test/accuracy': 0.6338969469070435, 'test/loss': 1.800976037979126, 'test/bleu': 24.793330930167617, 'test/num_examples': 3003, 'score': 12629.596199512482, 'total_duration': 21590.399958848953, 'accumulated_submission_time': 12629.596199512482, 'accumulated_eval_time': 8959.241186380386, 'accumulated_logging_time': 0.42243266105651855}
I0209 10:15:36.720665 140277054387968 logging_writer.py:48] [36022] accumulated_eval_time=8959.241186, accumulated_logging_time=0.422433, accumulated_submission_time=12629.596200, global_step=36022, preemption_count=0, score=12629.596200, test/accuracy=0.633897, test/bleu=24.793331, test/loss=1.800976, test/num_examples=3003, total_duration=21590.399959, train/accuracy=0.605118, train/bleu=28.503332, train/loss=2.022435, validation/accuracy=0.627791, validation/bleu=25.854004, validation/loss=1.845634, validation/num_examples=3000
I0209 10:16:04.279579 140277062780672 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.30284759402275085, loss=2.0684590339660645
I0209 10:16:39.222929 140277054387968 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.3089058995246887, loss=1.9955849647521973
I0209 10:17:14.211431 140277062780672 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.378695547580719, loss=1.982545018196106
I0209 10:17:49.184847 140277054387968 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3301818072795868, loss=1.9849755764007568
I0209 10:18:24.168273 140277062780672 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.4347783327102661, loss=2.0491554737091064
I0209 10:18:59.176743 140277054387968 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.2760225534439087, loss=1.9662694931030273
I0209 10:19:34.212573 140277062780672 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.6579313278198242, loss=1.9922915697097778
I0209 10:20:09.202345 140277054387968 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6723487377166748, loss=1.9905569553375244
I0209 10:20:44.211832 140277062780672 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8028349876403809, loss=2.0546517372131348
I0209 10:21:19.202263 140277054387968 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2879386246204376, loss=2.0388987064361572
I0209 10:21:54.204205 140277062780672 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3550196886062622, loss=1.8979511260986328
I0209 10:22:29.203126 140277054387968 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.34805870056152344, loss=2.000298261642456
I0209 10:23:04.192913 140277062780672 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.33095481991767883, loss=1.9979190826416016
I0209 10:23:39.184844 140277054387968 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.26004818081855774, loss=2.0272789001464844
I0209 10:24:14.193605 140277062780672 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.38978663086891174, loss=2.0512449741363525
I0209 10:24:49.247019 140277054387968 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5474738478660583, loss=1.991909384727478
I0209 10:25:24.325417 140277062780672 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.4236391484737396, loss=1.9846633672714233
I0209 10:25:59.338584 140277054387968 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3169531524181366, loss=2.0357472896575928
I0209 10:26:34.342625 140277062780672 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8199223279953003, loss=2.072045087814331
I0209 10:27:09.333587 140277054387968 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.41124728322029114, loss=2.024784803390503
I0209 10:27:44.362353 140277062780672 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.30352818965911865, loss=2.012387275695801
I0209 10:28:19.365656 140277054387968 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3535993695259094, loss=1.97567617893219
I0209 10:28:54.396003 140277062780672 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5743887424468994, loss=2.0276033878326416
I0209 10:29:29.430667 140277054387968 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.4600256681442261, loss=1.9735068082809448
I0209 10:29:36.838144 140446903760704 spec.py:321] Evaluating on the training split.
I0209 10:29:39.840293 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:32:42.178223 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 10:32:44.890234 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:35:19.122913 140446903760704 spec.py:349] Evaluating on the test split.
I0209 10:35:21.822376 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:37:52.353913 140446903760704 submission_runner.py:408] Time since start: 22926.05s, 	Step: 38423, 	{'train/accuracy': 0.6163857579231262, 'train/loss': 1.9339035749435425, 'train/bleu': 29.14323735552401, 'validation/accuracy': 0.6316102743148804, 'validation/loss': 1.8279942274093628, 'validation/bleu': 26.215031467429903, 'validation/num_examples': 3000, 'test/accuracy': 0.6375108957290649, 'test/loss': 1.7810250520706177, 'test/bleu': 25.07801187053838, 'test/num_examples': 3003, 'score': 13469.625989198685, 'total_duration': 22926.05318045616, 'accumulated_submission_time': 13469.625989198685, 'accumulated_eval_time': 9454.756899356842, 'accumulated_logging_time': 0.45250535011291504}
I0209 10:37:52.374322 140277062780672 logging_writer.py:48] [38423] accumulated_eval_time=9454.756899, accumulated_logging_time=0.452505, accumulated_submission_time=13469.625989, global_step=38423, preemption_count=0, score=13469.625989, test/accuracy=0.637511, test/bleu=25.078012, test/loss=1.781025, test/num_examples=3003, total_duration=22926.053180, train/accuracy=0.616386, train/bleu=29.143237, train/loss=1.933904, validation/accuracy=0.631610, validation/bleu=26.215031, validation/loss=1.827994, validation/num_examples=3000
I0209 10:38:19.548179 140277054387968 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.4793638586997986, loss=1.986562967300415
I0209 10:38:54.423549 140277062780672 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.44714969396591187, loss=2.0660669803619385
I0209 10:39:29.431114 140277054387968 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3261101245880127, loss=1.913733720779419
I0209 10:40:04.410432 140277062780672 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.5404707193374634, loss=1.9716843366622925
I0209 10:40:39.414752 140277054387968 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.3954352140426636, loss=1.957458734512329
I0209 10:41:14.427071 140277062780672 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.25822949409484863, loss=2.0299201011657715
I0209 10:41:49.447528 140277054387968 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.28904712200164795, loss=1.9484484195709229
I0209 10:42:24.440820 140277062780672 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.2845193147659302, loss=2.078942060470581
I0209 10:42:59.442996 140277054387968 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.4993961453437805, loss=1.996352195739746
I0209 10:43:34.433251 140277062780672 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.35728371143341064, loss=2.0334091186523438
I0209 10:44:09.440010 140277054387968 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.4127521216869354, loss=1.9685457944869995
I0209 10:44:44.447355 140277062780672 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6281918287277222, loss=1.981816053390503
I0209 10:45:19.449169 140277054387968 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.37664592266082764, loss=1.9766770601272583
I0209 10:45:54.456862 140277062780672 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5316004157066345, loss=2.029818296432495
I0209 10:46:29.442888 140277054387968 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.5469353795051575, loss=1.9736777544021606
I0209 10:47:04.463970 140277062780672 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.41118913888931274, loss=2.0371456146240234
I0209 10:47:39.461632 140277054387968 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.33356770873069763, loss=1.940632939338684
I0209 10:48:14.460968 140277062780672 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.6459388732910156, loss=1.9452418088912964
I0209 10:48:49.450196 140277054387968 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.5404738783836365, loss=1.9941736459732056
I0209 10:49:24.596951 140277062780672 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.2715602517127991, loss=1.9763628244400024
I0209 10:49:59.603226 140277054387968 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6136922240257263, loss=1.9233145713806152
I0209 10:50:34.630813 140277062780672 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.4944760501384735, loss=2.0149636268615723
I0209 10:51:09.643936 140277054387968 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.851288378238678, loss=1.9845271110534668
I0209 10:51:44.650449 140277062780672 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.358011394739151, loss=1.8958626985549927
I0209 10:51:52.419278 140446903760704 spec.py:321] Evaluating on the training split.
I0209 10:51:55.425233 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:55:04.536911 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 10:55:07.242070 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 10:57:51.063657 140446903760704 spec.py:349] Evaluating on the test split.
I0209 10:57:53.773710 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 11:00:36.630575 140446903760704 submission_runner.py:408] Time since start: 24290.33s, 	Step: 40824, 	{'train/accuracy': 0.6103520393371582, 'train/loss': 1.9843041896820068, 'train/bleu': 29.142409587723943, 'validation/accuracy': 0.6277045607566833, 'validation/loss': 1.8335964679718018, 'validation/bleu': 25.61308660868012, 'validation/num_examples': 3000, 'test/accuracy': 0.6380454301834106, 'test/loss': 1.7811000347137451, 'test/bleu': 24.74312727864808, 'test/num_examples': 3003, 'score': 14309.583218336105, 'total_duration': 24290.329845905304, 'accumulated_submission_time': 14309.583218336105, 'accumulated_eval_time': 9978.96815109253, 'accumulated_logging_time': 0.4831573963165283}
I0209 11:00:36.650696 140277054387968 logging_writer.py:48] [40824] accumulated_eval_time=9978.968151, accumulated_logging_time=0.483157, accumulated_submission_time=14309.583218, global_step=40824, preemption_count=0, score=14309.583218, test/accuracy=0.638045, test/bleu=24.743127, test/loss=1.781100, test/num_examples=3003, total_duration=24290.329846, train/accuracy=0.610352, train/bleu=29.142410, train/loss=1.984304, validation/accuracy=0.627705, validation/bleu=25.613087, validation/loss=1.833596, validation/num_examples=3000
I0209 11:01:03.483538 140277062780672 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.5694055557250977, loss=1.9993526935577393
I0209 11:01:38.400231 140277054387968 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2507871687412262, loss=1.9809874296188354
I0209 11:02:13.420413 140277062780672 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.764183759689331, loss=2.0086159706115723
I0209 11:02:48.449840 140277054387968 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.4584977924823761, loss=1.9971531629562378
I0209 11:03:23.446541 140277062780672 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6211523413658142, loss=1.9474211931228638
I0209 11:03:58.490681 140277054387968 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.4093835651874542, loss=1.8670638799667358
I0209 11:04:33.454328 140277062780672 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3121328353881836, loss=1.9463845491409302
I0209 11:05:08.454016 140277054387968 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.4581558406352997, loss=2.0256073474884033
I0209 11:05:43.417218 140277062780672 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.42215976119041443, loss=2.0042097568511963
I0209 11:06:18.424288 140277054387968 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.309190571308136, loss=1.944848656654358
I0209 11:06:53.395722 140277062780672 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.4667673707008362, loss=1.9947185516357422
I0209 11:07:28.370597 140277054387968 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.3257405459880829, loss=1.9666529893875122
I0209 11:08:03.347785 140277062780672 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.28740644454956055, loss=1.9468278884887695
I0209 11:08:38.405984 140277054387968 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.6255049109458923, loss=2.129878520965576
I0209 11:09:13.457936 140277062780672 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3806845247745514, loss=2.0176682472229004
I0209 11:09:48.452990 140277054387968 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3508833646774292, loss=1.9642953872680664
I0209 11:10:23.455782 140277062780672 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3591781556606293, loss=1.9991947412490845
I0209 11:10:58.442559 140277054387968 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7708545327186584, loss=1.9464136362075806
I0209 11:11:33.455919 140277062780672 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.8151124715805054, loss=2.0398359298706055
I0209 11:12:08.477590 140277054387968 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6682209968566895, loss=2.012565851211548
I0209 11:12:43.473438 140277062780672 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3055541515350342, loss=1.937963604927063
I0209 11:13:18.457268 140277054387968 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3073876202106476, loss=1.8816733360290527
I0209 11:13:53.471972 140277062780672 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.27820971608161926, loss=1.9865398406982422
I0209 11:14:28.473829 140277054387968 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.5052829384803772, loss=1.9447801113128662
I0209 11:14:36.938430 140446903760704 spec.py:321] Evaluating on the training split.
I0209 11:14:39.938920 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 11:19:15.569621 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 11:19:18.279217 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 11:23:53.146842 140446903760704 spec.py:349] Evaluating on the test split.
I0209 11:23:55.868488 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 11:28:31.373232 140446903760704 submission_runner.py:408] Time since start: 25965.07s, 	Step: 43226, 	{'train/accuracy': 0.6160485148429871, 'train/loss': 1.9517220258712769, 'train/bleu': 29.20263873077558, 'validation/accuracy': 0.6300479769706726, 'validation/loss': 1.8143119812011719, 'validation/bleu': 22.437207824955046, 'validation/num_examples': 3000, 'test/accuracy': 0.6379989981651306, 'test/loss': 1.769517183303833, 'test/bleu': 24.556187006987688, 'test/num_examples': 3003, 'score': 15149.78208065033, 'total_duration': 25965.072471618652, 'accumulated_submission_time': 15149.78208065033, 'accumulated_eval_time': 10813.402871608734, 'accumulated_logging_time': 0.5145649909973145}
I0209 11:28:31.398147 140277062780672 logging_writer.py:48] [43226] accumulated_eval_time=10813.402872, accumulated_logging_time=0.514565, accumulated_submission_time=15149.782081, global_step=43226, preemption_count=0, score=15149.782081, test/accuracy=0.637999, test/bleu=24.556187, test/loss=1.769517, test/num_examples=3003, total_duration=25965.072472, train/accuracy=0.616049, train/bleu=29.202639, train/loss=1.951722, validation/accuracy=0.630048, validation/bleu=22.437208, validation/loss=1.814312, validation/num_examples=3000
I0209 11:28:57.519380 140277054387968 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.41226863861083984, loss=1.9368599653244019
I0209 11:29:32.439500 140277062780672 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3622819781303406, loss=1.9724112749099731
I0209 11:30:07.430228 140277054387968 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5592891573905945, loss=1.9035029411315918
I0209 11:30:42.396633 140277062780672 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.3176593780517578, loss=2.0001213550567627
I0209 11:31:17.386198 140277054387968 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.5206714868545532, loss=2.0294229984283447
I0209 11:31:52.401546 140277062780672 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6485301852226257, loss=1.9388242959976196
I0209 11:32:27.421620 140277054387968 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.40286996960639954, loss=1.9443036317825317
I0209 11:33:02.407228 140277062780672 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.4136764407157898, loss=1.904319405555725
I0209 11:33:37.428791 140277054387968 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.46610215306282043, loss=1.9931433200836182
I0209 11:34:12.409859 140277062780672 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2949538230895996, loss=1.9569834470748901
I0209 11:34:47.383518 140277054387968 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.388659805059433, loss=1.8606106042861938
I0209 11:35:22.396734 140277062780672 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7320321798324585, loss=1.9057610034942627
I0209 11:35:57.408383 140277054387968 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.3945188820362091, loss=1.9915573596954346
I0209 11:36:32.415947 140277062780672 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.389876127243042, loss=1.970934510231018
I0209 11:37:07.442844 140277054387968 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.3538588285446167, loss=1.9479517936706543
I0209 11:37:42.509882 140277062780672 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.27920711040496826, loss=2.028250217437744
I0209 11:38:17.504335 140277054387968 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.3626687228679657, loss=1.9122244119644165
I0209 11:38:52.457875 140277062780672 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3253026306629181, loss=1.8794398307800293
I0209 11:39:27.461378 140277054387968 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.5166969895362854, loss=1.913956642150879
I0209 11:40:02.484245 140277062780672 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.310302197933197, loss=1.9558995962142944
I0209 11:40:37.471348 140277054387968 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.4904586970806122, loss=1.9732558727264404
I0209 11:41:12.456362 140277062780672 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.4550282657146454, loss=1.9574753046035767
I0209 11:41:47.470402 140277054387968 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.5384886860847473, loss=2.0428123474121094
I0209 11:42:22.484371 140277062780672 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.2962484061717987, loss=1.9424599409103394
I0209 11:42:31.681011 140446903760704 spec.py:321] Evaluating on the training split.
I0209 11:42:34.709996 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 11:46:41.886200 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 11:46:44.588129 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 11:50:07.164483 140446903760704 spec.py:349] Evaluating on the test split.
I0209 11:50:09.860090 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 11:53:08.092778 140446903760704 submission_runner.py:408] Time since start: 27441.79s, 	Step: 45628, 	{'train/accuracy': 0.6102014183998108, 'train/loss': 1.9810327291488647, 'train/bleu': 29.17744358049914, 'validation/accuracy': 0.6297379732131958, 'validation/loss': 1.8217298984527588, 'validation/bleu': 25.96525554826419, 'validation/num_examples': 3000, 'test/accuracy': 0.6361513137817383, 'test/loss': 1.7746611833572388, 'test/bleu': 24.85202517879502, 'test/num_examples': 3003, 'score': 15989.976817846298, 'total_duration': 27441.79203939438, 'accumulated_submission_time': 15989.976817846298, 'accumulated_eval_time': 11449.814586162567, 'accumulated_logging_time': 0.5513138771057129}
I0209 11:53:08.114490 140277054387968 logging_writer.py:48] [45628] accumulated_eval_time=11449.814586, accumulated_logging_time=0.551314, accumulated_submission_time=15989.976818, global_step=45628, preemption_count=0, score=15989.976818, test/accuracy=0.636151, test/bleu=24.852025, test/loss=1.774661, test/num_examples=3003, total_duration=27441.792039, train/accuracy=0.610201, train/bleu=29.177444, train/loss=1.981033, validation/accuracy=0.629738, validation/bleu=25.965256, validation/loss=1.821730, validation/num_examples=3000
I0209 11:53:33.521585 140277062780672 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.4002033770084381, loss=2.064365863800049
I0209 11:54:08.418468 140277054387968 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.4466966390609741, loss=1.9560120105743408
I0209 11:54:43.473049 140277062780672 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.47588130831718445, loss=1.9176102876663208
I0209 11:55:18.471196 140277054387968 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.33092358708381653, loss=1.929719090461731
I0209 11:55:53.472388 140277062780672 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.48618125915527344, loss=2.011357307434082
I0209 11:56:28.457895 140277054387968 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.45490169525146484, loss=1.9248461723327637
I0209 11:57:03.503893 140277062780672 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.4913162291049957, loss=1.963728666305542
I0209 11:57:38.504551 140277054387968 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.43392691016197205, loss=2.0518507957458496
I0209 11:58:13.512324 140277062780672 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.29214876890182495, loss=2.0407321453094482
I0209 11:58:48.497879 140277054387968 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.38785165548324585, loss=2.034855842590332
I0209 11:59:23.486609 140277062780672 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2786044478416443, loss=2.0138590335845947
I0209 11:59:58.462601 140277054387968 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.3102152645587921, loss=1.9833873510360718
I0209 12:00:33.440647 140277062780672 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.34793007373809814, loss=1.9560017585754395
I0209 12:01:08.436170 140277054387968 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.47051331400871277, loss=1.9428372383117676
I0209 12:01:43.463566 140277062780672 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.609024703502655, loss=1.9201312065124512
I0209 12:02:18.476886 140277054387968 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.5544915795326233, loss=1.9887456893920898
I0209 12:02:53.476467 140277062780672 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.3589937388896942, loss=1.9158880710601807
I0209 12:03:28.502673 140277054387968 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3539121747016907, loss=1.9806514978408813
I0209 12:04:03.519780 140277062780672 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.4739704430103302, loss=1.92643141746521
I0209 12:04:38.520362 140277054387968 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2421015352010727, loss=1.9559783935546875
I0209 12:05:13.507986 140277062780672 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.32479655742645264, loss=2.018489122390747
I0209 12:05:48.483336 140277054387968 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2739505469799042, loss=1.9500116109848022
I0209 12:06:23.491971 140277062780672 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6249906420707703, loss=1.9248874187469482
I0209 12:06:58.491192 140277054387968 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.3744511902332306, loss=1.996835470199585
I0209 12:07:08.381491 140446903760704 spec.py:321] Evaluating on the training split.
I0209 12:07:11.410030 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:10:40.353945 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 12:10:43.085988 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:13:46.455370 140446903760704 spec.py:349] Evaluating on the test split.
I0209 12:13:49.175039 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:17:05.822691 140446903760704 submission_runner.py:408] Time since start: 28879.52s, 	Step: 48030, 	{'train/accuracy': 0.6131874322891235, 'train/loss': 1.9688488245010376, 'train/bleu': 29.100882515421855, 'validation/accuracy': 0.6333833336830139, 'validation/loss': 1.80632483959198, 'validation/bleu': 26.431980916570602, 'validation/num_examples': 3000, 'test/accuracy': 0.6418104767799377, 'test/loss': 1.7534323930740356, 'test/bleu': 25.610204331052604, 'test/num_examples': 3003, 'score': 16830.157977104187, 'total_duration': 28879.521926641464, 'accumulated_submission_time': 16830.157977104187, 'accumulated_eval_time': 12047.255705833435, 'accumulated_logging_time': 0.5844166278839111}
I0209 12:17:05.848613 140277062780672 logging_writer.py:48] [48030] accumulated_eval_time=12047.255706, accumulated_logging_time=0.584417, accumulated_submission_time=16830.157977, global_step=48030, preemption_count=0, score=16830.157977, test/accuracy=0.641810, test/bleu=25.610204, test/loss=1.753432, test/num_examples=3003, total_duration=28879.521927, train/accuracy=0.613187, train/bleu=29.100883, train/loss=1.968849, validation/accuracy=0.633383, validation/bleu=26.431981, validation/loss=1.806325, validation/num_examples=3000
I0209 12:17:30.627706 140277054387968 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.4959189295768738, loss=1.9625544548034668
I0209 12:18:05.553729 140277062780672 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.28419095277786255, loss=2.022925853729248
I0209 12:18:40.595616 140277054387968 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.36123067140579224, loss=1.962816596031189
I0209 12:19:15.632777 140277062780672 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7829320430755615, loss=1.909978985786438
I0209 12:19:50.681781 140277054387968 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.32030683755874634, loss=1.9228694438934326
I0209 12:20:25.690228 140277062780672 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.30103519558906555, loss=2.046100378036499
I0209 12:21:00.682973 140277054387968 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.26597827672958374, loss=1.9853334426879883
I0209 12:21:35.699520 140277062780672 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.38449543714523315, loss=1.8857879638671875
I0209 12:22:10.718989 140277054387968 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3961467742919922, loss=1.8944091796875
I0209 12:22:45.731422 140277062780672 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3545913100242615, loss=1.98125422000885
I0209 12:23:20.700566 140277054387968 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.24885500967502594, loss=1.9280840158462524
I0209 12:23:55.693902 140277062780672 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3545156419277191, loss=1.8789364099502563
I0209 12:24:30.702384 140277054387968 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.2576311230659485, loss=1.878461241722107
I0209 12:25:05.690702 140277062780672 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.31061020493507385, loss=1.9286479949951172
I0209 12:25:40.663663 140277054387968 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.31769663095474243, loss=1.9986321926116943
I0209 12:26:15.673852 140277062780672 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.49675166606903076, loss=1.9411401748657227
I0209 12:26:50.683389 140277054387968 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3825715482234955, loss=2.0238590240478516
I0209 12:27:25.683708 140277062780672 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.4692496359348297, loss=1.8951785564422607
I0209 12:28:00.687439 140277054387968 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.5429772138595581, loss=2.0204668045043945
I0209 12:28:35.657970 140277062780672 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3925599455833435, loss=1.9455783367156982
I0209 12:29:10.649825 140277054387968 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.5127730965614319, loss=2.057126522064209
I0209 12:29:45.678809 140277062780672 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3978618085384369, loss=1.96206533908844
I0209 12:30:20.666501 140277054387968 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.3105311989784241, loss=1.8808937072753906
I0209 12:30:55.664304 140277062780672 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.32448312640190125, loss=1.9939136505126953
I0209 12:31:05.885875 140446903760704 spec.py:321] Evaluating on the training split.
I0209 12:31:08.887306 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:33:52.140277 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 12:33:54.841111 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:36:40.629846 140446903760704 spec.py:349] Evaluating on the test split.
I0209 12:36:43.324165 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:39:20.070164 140446903760704 submission_runner.py:408] Time since start: 30213.77s, 	Step: 50431, 	{'train/accuracy': 0.6287590861320496, 'train/loss': 1.834293246269226, 'train/bleu': 30.29873589696989, 'validation/accuracy': 0.6369046568870544, 'validation/loss': 1.7885897159576416, 'validation/bleu': 26.45663032630946, 'validation/num_examples': 3000, 'test/accuracy': 0.6455406546592712, 'test/loss': 1.7254810333251953, 'test/bleu': 25.790316454042895, 'test/num_examples': 3003, 'score': 17670.106913089752, 'total_duration': 30213.769419670105, 'accumulated_submission_time': 17670.106913089752, 'accumulated_eval_time': 12541.439943313599, 'accumulated_logging_time': 0.6211183071136475}
I0209 12:39:20.092219 140277054387968 logging_writer.py:48] [50431] accumulated_eval_time=12541.439943, accumulated_logging_time=0.621118, accumulated_submission_time=17670.106913, global_step=50431, preemption_count=0, score=17670.106913, test/accuracy=0.645541, test/bleu=25.790316, test/loss=1.725481, test/num_examples=3003, total_duration=30213.769420, train/accuracy=0.628759, train/bleu=30.298736, train/loss=1.834293, validation/accuracy=0.636905, validation/bleu=26.456630, validation/loss=1.788590, validation/num_examples=3000
I0209 12:39:44.477156 140277062780672 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.26980507373809814, loss=1.9857856035232544
I0209 12:40:19.376100 140277054387968 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3071565330028534, loss=1.907971978187561
I0209 12:40:54.371734 140277062780672 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.3090968430042267, loss=1.9278086423873901
I0209 12:41:29.355595 140277054387968 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.35537952184677124, loss=1.9500385522842407
I0209 12:42:04.369813 140277062780672 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.28057458996772766, loss=1.9845021963119507
I0209 12:42:39.413368 140277054387968 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.41699016094207764, loss=1.9243817329406738
I0209 12:43:14.408614 140277062780672 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.26375412940979004, loss=1.9236434698104858
I0209 12:43:49.370340 140277054387968 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3741621673107147, loss=1.9995290040969849
I0209 12:44:24.382142 140277062780672 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.4839077889919281, loss=1.9665764570236206
I0209 12:44:59.386609 140277054387968 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.26850616931915283, loss=1.8907756805419922
I0209 12:45:34.366954 140277062780672 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2901572585105896, loss=1.9313256740570068
I0209 12:46:09.375526 140277054387968 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.4545222222805023, loss=2.0379979610443115
I0209 12:46:44.390077 140277062780672 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.33671805262565613, loss=1.9253931045532227
I0209 12:47:19.384450 140277054387968 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2771397829055786, loss=2.0918891429901123
I0209 12:47:54.424964 140277062780672 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3486250042915344, loss=1.8717540502548218
I0209 12:48:29.421779 140277054387968 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.33328694105148315, loss=1.9044679403305054
I0209 12:49:04.402479 140277062780672 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.37759459018707275, loss=2.03164005279541
I0209 12:49:39.428641 140277054387968 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3015338182449341, loss=1.9312238693237305
I0209 12:50:14.434346 140277062780672 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.3960283100605011, loss=1.9204835891723633
I0209 12:50:49.430974 140277054387968 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.2815239727497101, loss=1.9959136247634888
I0209 12:51:24.477792 140277062780672 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.30259329080581665, loss=2.016902208328247
I0209 12:51:59.498412 140277054387968 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.32139527797698975, loss=1.9800031185150146
I0209 12:52:34.533397 140277062780672 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3732954263687134, loss=1.9911365509033203
I0209 12:53:09.513746 140277054387968 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.45549914240837097, loss=2.002645492553711
I0209 12:53:20.081904 140446903760704 spec.py:321] Evaluating on the training split.
I0209 12:53:23.085318 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:56:33.960985 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 12:56:36.655729 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 12:59:16.807782 140446903760704 spec.py:349] Evaluating on the test split.
I0209 12:59:19.504229 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 13:02:09.145643 140446903760704 submission_runner.py:408] Time since start: 31582.84s, 	Step: 52832, 	{'train/accuracy': 0.6180623173713684, 'train/loss': 1.9361435174942017, 'train/bleu': 29.478945279105627, 'validation/accuracy': 0.6362723112106323, 'validation/loss': 1.7838091850280762, 'validation/bleu': 26.286487484829664, 'validation/num_examples': 3000, 'test/accuracy': 0.6466097235679626, 'test/loss': 1.7135682106018066, 'test/bleu': 25.517370648362107, 'test/num_examples': 3003, 'score': 18510.01056957245, 'total_duration': 31582.84491109848, 'accumulated_submission_time': 18510.01056957245, 'accumulated_eval_time': 13070.503627538681, 'accumulated_logging_time': 0.6545243263244629}
I0209 13:02:09.168962 140277062780672 logging_writer.py:48] [52832] accumulated_eval_time=13070.503628, accumulated_logging_time=0.654524, accumulated_submission_time=18510.010570, global_step=52832, preemption_count=0, score=18510.010570, test/accuracy=0.646610, test/bleu=25.517371, test/loss=1.713568, test/num_examples=3003, total_duration=31582.844911, train/accuracy=0.618062, train/bleu=29.478945, train/loss=1.936144, validation/accuracy=0.636272, validation/bleu=26.286487, validation/loss=1.783809, validation/num_examples=3000
I0209 13:02:33.211837 140277054387968 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.7034000754356384, loss=2.0311503410339355
I0209 13:03:08.109542 140277062780672 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.5650995969772339, loss=2.001086950302124
I0209 13:03:43.077952 140277054387968 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.29576975107192993, loss=1.9741649627685547
I0209 13:04:18.080255 140277062780672 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.4091866612434387, loss=1.9198048114776611
I0209 13:04:53.111342 140277054387968 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.42886075377464294, loss=1.884842038154602
I0209 13:05:28.090435 140277062780672 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.24498139321804047, loss=1.9530158042907715
I0209 13:06:03.078726 140277054387968 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.504709780216217, loss=1.9202888011932373
I0209 13:06:38.069742 140277062780672 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.4231843948364258, loss=1.9684579372406006
I0209 13:07:13.079197 140277054387968 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3736450970172882, loss=1.9571030139923096
I0209 13:07:48.082698 140277062780672 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.3979952931404114, loss=1.9912166595458984
I0209 13:08:23.073787 140277054387968 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.29696124792099, loss=1.8613605499267578
I0209 13:08:58.097982 140277062780672 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.5149697661399841, loss=1.8952704668045044
I0209 13:09:33.108550 140277054387968 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2813304364681244, loss=1.841233491897583
I0209 13:10:08.102549 140277062780672 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.31453782320022583, loss=1.9473201036453247
I0209 13:10:43.127048 140277054387968 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3962651789188385, loss=1.8647011518478394
I0209 13:11:18.165884 140277062780672 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.30111080408096313, loss=1.8683816194534302
I0209 13:11:53.192945 140277054387968 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.33513370156288147, loss=1.9285252094268799
I0209 13:12:28.160715 140277062780672 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.40240705013275146, loss=1.9765805006027222
I0209 13:13:03.151741 140277054387968 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.4735485017299652, loss=1.929526686668396
I0209 13:13:38.248609 140277062780672 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.27883750200271606, loss=1.8618788719177246
I0209 13:14:13.232522 140277054387968 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.40311163663864136, loss=1.9461334943771362
I0209 13:14:48.200221 140277062780672 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3775257170200348, loss=1.88890540599823
I0209 13:15:23.223243 140277054387968 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.4198434054851532, loss=1.9955058097839355
I0209 13:15:58.224696 140277062780672 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.3537585437297821, loss=1.860587477684021
I0209 13:16:09.161050 140446903760704 spec.py:321] Evaluating on the training split.
I0209 13:16:12.164249 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 13:19:20.767182 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 13:19:23.489985 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 13:22:04.123569 140446903760704 spec.py:349] Evaluating on the test split.
I0209 13:22:06.834105 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 13:24:49.315381 140446903760704 submission_runner.py:408] Time since start: 32943.01s, 	Step: 55233, 	{'train/accuracy': 0.6131559014320374, 'train/loss': 1.9650341272354126, 'train/bleu': 29.613710414488942, 'validation/accuracy': 0.6389381289482117, 'validation/loss': 1.767750859260559, 'validation/bleu': 26.565091468583024, 'validation/num_examples': 3000, 'test/accuracy': 0.6476556062698364, 'test/loss': 1.7083313465118408, 'test/bleu': 25.627177441123937, 'test/num_examples': 3003, 'score': 19349.917988538742, 'total_duration': 32943.01463651657, 'accumulated_submission_time': 19349.917988538742, 'accumulated_eval_time': 13590.657889842987, 'accumulated_logging_time': 0.6880850791931152}
I0209 13:24:49.338011 140277054387968 logging_writer.py:48] [55233] accumulated_eval_time=13590.657890, accumulated_logging_time=0.688085, accumulated_submission_time=19349.917989, global_step=55233, preemption_count=0, score=19349.917989, test/accuracy=0.647656, test/bleu=25.627177, test/loss=1.708331, test/num_examples=3003, total_duration=32943.014637, train/accuracy=0.613156, train/bleu=29.613710, train/loss=1.965034, validation/accuracy=0.638938, validation/bleu=26.565091, validation/loss=1.767751, validation/num_examples=3000
I0209 13:25:13.014296 140277062780672 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.2677527964115143, loss=1.9826655387878418
I0209 13:25:47.907828 140277054387968 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3487889766693115, loss=1.9330071210861206
I0209 13:26:22.878870 140277062780672 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.28049221634864807, loss=1.8969441652297974
I0209 13:26:57.866856 140277054387968 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.41802528500556946, loss=1.95704984664917
I0209 13:27:32.889273 140277062780672 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.29261237382888794, loss=1.8684473037719727
I0209 13:28:07.873732 140277054387968 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.4505271315574646, loss=1.8855845928192139
I0209 13:28:42.891309 140277062780672 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.4931439757347107, loss=2.0104520320892334
I0209 13:29:17.901885 140277054387968 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.30812835693359375, loss=1.8765653371810913
I0209 13:29:52.897747 140277062780672 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.7257059812545776, loss=1.8788611888885498
I0209 13:30:27.900916 140277054387968 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.26658740639686584, loss=1.9875764846801758
I0209 13:31:02.876672 140277062780672 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.36144450306892395, loss=1.993786334991455
I0209 13:31:37.946984 140277054387968 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3898032307624817, loss=1.8849563598632812
I0209 13:32:12.995833 140277062780672 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3075534999370575, loss=1.9485671520233154
I0209 13:32:48.038680 140277054387968 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.32554107904434204, loss=1.8693960905075073
I0209 13:33:23.102829 140277062780672 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.3023577630519867, loss=2.001354932785034
I0209 13:33:58.192922 140277054387968 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.25376880168914795, loss=2.0049962997436523
I0209 13:34:33.218869 140277062780672 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.3241424560546875, loss=2.0450968742370605
I0209 13:35:08.225543 140277054387968 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.434489905834198, loss=1.878624439239502
I0209 13:35:43.236062 140277062780672 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.42567694187164307, loss=1.864931583404541
I0209 13:36:18.234503 140277054387968 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.5197836756706238, loss=1.916393518447876
I0209 13:36:53.208023 140277062780672 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.27990585565567017, loss=1.9137970209121704
I0209 13:37:28.197314 140277054387968 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.56623774766922, loss=1.9095102548599243
I0209 13:38:03.179894 140277062780672 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3519881069660187, loss=2.0348379611968994
I0209 13:38:38.174585 140277054387968 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3798494338989258, loss=1.9414678812026978
I0209 13:38:49.439149 140446903760704 spec.py:321] Evaluating on the training split.
I0209 13:38:52.436333 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 13:42:42.528175 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 13:42:45.247754 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 13:45:44.835049 140446903760704 spec.py:349] Evaluating on the test split.
I0209 13:45:47.536183 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 13:48:31.523390 140446903760704 submission_runner.py:408] Time since start: 34365.22s, 	Step: 57634, 	{'train/accuracy': 0.623281717300415, 'train/loss': 1.8893014192581177, 'train/bleu': 30.039212527666372, 'validation/accuracy': 0.639272928237915, 'validation/loss': 1.763260841369629, 'validation/bleu': 26.7444395678232, 'validation/num_examples': 3000, 'test/accuracy': 0.6502469778060913, 'test/loss': 1.700022578239441, 'test/bleu': 26.166706111756557, 'test/num_examples': 3003, 'score': 20189.931703090668, 'total_duration': 34365.222650527954, 'accumulated_submission_time': 20189.931703090668, 'accumulated_eval_time': 14172.742085933685, 'accumulated_logging_time': 0.7207436561584473}
I0209 13:48:31.546745 140277062780672 logging_writer.py:48] [57634] accumulated_eval_time=14172.742086, accumulated_logging_time=0.720744, accumulated_submission_time=20189.931703, global_step=57634, preemption_count=0, score=20189.931703, test/accuracy=0.650247, test/bleu=26.166706, test/loss=1.700023, test/num_examples=3003, total_duration=34365.222651, train/accuracy=0.623282, train/bleu=30.039213, train/loss=1.889301, validation/accuracy=0.639273, validation/bleu=26.744440, validation/loss=1.763261, validation/num_examples=3000
I0209 13:48:54.890248 140277054387968 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2733544111251831, loss=1.980073094367981
I0209 13:49:29.806184 140277062780672 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.36055296659469604, loss=1.9919250011444092
I0209 13:50:04.752627 140277054387968 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.4290551245212555, loss=1.8021085262298584
I0209 13:50:39.731873 140277062780672 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2984030246734619, loss=1.8708751201629639
I0209 13:51:14.716435 140277054387968 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.467891663312912, loss=1.8876707553863525
I0209 13:51:49.708900 140277062780672 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.4616657793521881, loss=1.8123760223388672
I0209 13:52:24.706921 140277054387968 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3316745162010193, loss=1.8768519163131714
I0209 13:52:59.664933 140277062780672 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.448282927274704, loss=1.894574761390686
I0209 13:53:34.681311 140277054387968 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.442023903131485, loss=1.9499574899673462
I0209 13:54:09.693847 140277062780672 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3942646384239197, loss=1.9688572883605957
I0209 13:54:44.674843 140277054387968 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.39005234837532043, loss=1.9264965057373047
I0209 13:55:19.667473 140277062780672 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2682928740978241, loss=1.8848655223846436
I0209 13:55:54.700900 140277054387968 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.6204144954681396, loss=1.8118666410446167
I0209 13:56:29.680655 140277062780672 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.41248753666877747, loss=1.9606201648712158
I0209 13:57:04.688307 140277054387968 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.5155077576637268, loss=1.9193781614303589
I0209 13:57:39.684216 140277062780672 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.29433390498161316, loss=1.9103937149047852
I0209 13:58:14.664715 140277054387968 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.34688782691955566, loss=1.9726988077163696
I0209 13:58:49.651349 140277062780672 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.5896494388580322, loss=1.9090603590011597
I0209 13:59:24.678286 140277054387968 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.27801603078842163, loss=1.9083532094955444
I0209 13:59:59.667911 140277062780672 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.38217243552207947, loss=1.9290435314178467
I0209 14:00:34.723745 140277054387968 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3112083673477173, loss=1.8855129480361938
I0209 14:01:09.748274 140277062780672 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.30384665727615356, loss=1.860487937927246
I0209 14:01:44.759788 140277054387968 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3134443163871765, loss=1.8587703704833984
I0209 14:02:19.796451 140277062780672 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.27923354506492615, loss=1.9395508766174316
I0209 14:02:31.791358 140446903760704 spec.py:321] Evaluating on the training split.
I0209 14:02:34.822149 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:05:10.459795 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 14:05:13.170116 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:07:41.456577 140446903760704 spec.py:349] Evaluating on the test split.
I0209 14:07:44.152764 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:10:10.512653 140446903760704 submission_runner.py:408] Time since start: 35664.21s, 	Step: 60036, 	{'train/accuracy': 0.6204534769058228, 'train/loss': 1.9200528860092163, 'train/bleu': 30.193340853845427, 'validation/accuracy': 0.6428934335708618, 'validation/loss': 1.7423086166381836, 'validation/bleu': 26.78540750934711, 'validation/num_examples': 3000, 'test/accuracy': 0.6512114405632019, 'test/loss': 1.682421326637268, 'test/bleu': 25.748610033015733, 'test/num_examples': 3003, 'score': 21030.09056377411, 'total_duration': 35664.21192359924, 'accumulated_submission_time': 21030.09056377411, 'accumulated_eval_time': 14631.463346242905, 'accumulated_logging_time': 0.7540163993835449}
I0209 14:10:10.537189 140277054387968 logging_writer.py:48] [60036] accumulated_eval_time=14631.463346, accumulated_logging_time=0.754016, accumulated_submission_time=21030.090564, global_step=60036, preemption_count=0, score=21030.090564, test/accuracy=0.651211, test/bleu=25.748610, test/loss=1.682421, test/num_examples=3003, total_duration=35664.211924, train/accuracy=0.620453, train/bleu=30.193341, train/loss=1.920053, validation/accuracy=0.642893, validation/bleu=26.785408, validation/loss=1.742309, validation/num_examples=3000
I0209 14:10:33.179592 140277062780672 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.31841936707496643, loss=1.9289591312408447
I0209 14:11:08.039759 140277054387968 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.33938348293304443, loss=1.8619500398635864
I0209 14:11:43.011730 140277062780672 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.31561174988746643, loss=1.9187052249908447
I0209 14:12:17.978837 140277054387968 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.300332635641098, loss=1.9403777122497559
I0209 14:12:52.996408 140277062780672 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.28589123487472534, loss=1.954227328300476
I0209 14:13:28.010975 140277054387968 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.3419076204299927, loss=1.924497127532959
I0209 14:14:02.996531 140277062780672 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.4337601661682129, loss=1.9232139587402344
I0209 14:14:38.010547 140277054387968 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.30856993794441223, loss=1.924471378326416
I0209 14:15:12.999853 140277062780672 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.35625430941581726, loss=1.9687117338180542
I0209 14:15:47.999186 140277054387968 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3047938942909241, loss=1.9440566301345825
I0209 14:16:23.021996 140277062780672 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.26987433433532715, loss=1.8461297750473022
I0209 14:16:58.010306 140277054387968 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3497660756111145, loss=1.905316710472107
I0209 14:17:32.978245 140277062780672 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2782685160636902, loss=1.9641705751419067
I0209 14:18:07.967649 140277054387968 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.2563456594944, loss=1.898840308189392
I0209 14:18:42.962983 140277062780672 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.6532284021377563, loss=1.9032199382781982
I0209 14:19:17.954186 140277054387968 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.3110562860965729, loss=1.863716721534729
I0209 14:19:52.940409 140277062780672 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.38779163360595703, loss=1.8805853128433228
I0209 14:20:27.960319 140277054387968 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3618823289871216, loss=1.9619468450546265
I0209 14:21:02.938134 140277062780672 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2795049846172333, loss=1.8994853496551514
I0209 14:21:37.950246 140277054387968 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3420470058917999, loss=1.9632855653762817
I0209 14:22:12.979982 140277062780672 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.2577655017375946, loss=1.8235089778900146
I0209 14:22:47.968720 140277054387968 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.26336219906806946, loss=1.9171183109283447
I0209 14:23:22.985863 140277062780672 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.2535054087638855, loss=1.8176871538162231
I0209 14:23:58.031729 140277054387968 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.2697928547859192, loss=1.8365970849990845
I0209 14:24:10.719116 140446903760704 spec.py:321] Evaluating on the training split.
I0209 14:24:13.743238 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:27:03.195375 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 14:27:05.910197 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:29:40.546361 140446903760704 spec.py:349] Evaluating on the test split.
I0209 14:29:43.252140 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:32:33.455533 140446903760704 submission_runner.py:408] Time since start: 37007.15s, 	Step: 62438, 	{'train/accuracy': 0.6210818886756897, 'train/loss': 1.9125397205352783, 'train/bleu': 29.44747130858823, 'validation/accuracy': 0.6429926156997681, 'validation/loss': 1.7286237478256226, 'validation/bleu': 27.028646288240786, 'validation/num_examples': 3000, 'test/accuracy': 0.6535122990608215, 'test/loss': 1.6790618896484375, 'test/bleu': 26.31763856807123, 'test/num_examples': 3003, 'score': 21870.185875177383, 'total_duration': 37007.154800891876, 'accumulated_submission_time': 21870.185875177383, 'accumulated_eval_time': 15134.199719667435, 'accumulated_logging_time': 0.7899153232574463}
I0209 14:32:33.479763 140277062780672 logging_writer.py:48] [62438] accumulated_eval_time=15134.199720, accumulated_logging_time=0.789915, accumulated_submission_time=21870.185875, global_step=62438, preemption_count=0, score=21870.185875, test/accuracy=0.653512, test/bleu=26.317639, test/loss=1.679062, test/num_examples=3003, total_duration=37007.154801, train/accuracy=0.621082, train/bleu=29.447471, train/loss=1.912540, validation/accuracy=0.642993, validation/bleu=27.028646, validation/loss=1.728624, validation/num_examples=3000
I0209 14:32:55.459434 140277054387968 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.25597384572029114, loss=1.8711901903152466
I0209 14:33:30.368266 140277062780672 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.25604677200317383, loss=1.879234790802002
I0209 14:34:05.387960 140277054387968 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.27526435256004333, loss=1.8464021682739258
I0209 14:34:40.420343 140277062780672 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.31048980355262756, loss=1.907830834388733
I0209 14:35:15.415443 140277054387968 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3180263042449951, loss=1.803411602973938
I0209 14:35:50.397430 140277062780672 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.5348868370056152, loss=1.9632461071014404
I0209 14:36:25.402315 140277054387968 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.41349801421165466, loss=1.8602190017700195
I0209 14:37:00.377048 140277062780672 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.26958632469177246, loss=1.8959978818893433
I0209 14:37:35.446631 140277054387968 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.35657116770744324, loss=1.9426523447036743
I0209 14:38:10.441133 140277062780672 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.32593756914138794, loss=2.0039498805999756
I0209 14:38:45.448843 140277054387968 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.34867605566978455, loss=1.8524528741836548
I0209 14:39:20.472648 140277062780672 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3390776813030243, loss=1.8437039852142334
I0209 14:39:55.525883 140277054387968 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.3690437972545624, loss=1.8314404487609863
I0209 14:40:30.540279 140277062780672 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3136175870895386, loss=2.006700277328491
I0209 14:41:05.550076 140277054387968 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.33929648995399475, loss=1.9005014896392822
I0209 14:41:40.592544 140277062780672 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.31920477747917175, loss=1.9089539051055908
I0209 14:42:15.711951 140277054387968 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3247230052947998, loss=1.9969786405563354
I0209 14:42:50.783753 140277062780672 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.3631204664707184, loss=1.9388532638549805
I0209 14:43:25.819532 140277054387968 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.29292282462120056, loss=1.8345249891281128
I0209 14:44:00.825957 140277062780672 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3059118688106537, loss=1.9102996587753296
I0209 14:44:35.843411 140277054387968 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2683548033237457, loss=1.904919981956482
I0209 14:45:10.836874 140277062780672 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3298352062702179, loss=1.8180785179138184
I0209 14:45:45.802439 140277054387968 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.27654099464416504, loss=1.7887588739395142
I0209 14:46:20.808661 140277062780672 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.30505654215812683, loss=1.880013108253479
I0209 14:46:33.468713 140446903760704 spec.py:321] Evaluating on the training split.
I0209 14:46:36.471378 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:49:28.434222 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 14:49:31.139212 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:52:07.665503 140446903760704 spec.py:349] Evaluating on the test split.
I0209 14:52:10.374248 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 14:54:42.161928 140446903760704 submission_runner.py:408] Time since start: 38335.86s, 	Step: 64838, 	{'train/accuracy': 0.6214345097541809, 'train/loss': 1.899621844291687, 'train/bleu': 29.87296898313337, 'validation/accuracy': 0.6450632810592651, 'validation/loss': 1.725380539894104, 'validation/bleu': 26.96320628106604, 'validation/num_examples': 3000, 'test/accuracy': 0.653872549533844, 'test/loss': 1.662144660949707, 'test/bleu': 26.29416092890208, 'test/num_examples': 3003, 'score': 22710.087017774582, 'total_duration': 38335.861196517944, 'accumulated_submission_time': 22710.087017774582, 'accumulated_eval_time': 15622.89288187027, 'accumulated_logging_time': 0.8244888782501221}
I0209 14:54:42.186905 140277054387968 logging_writer.py:48] [64838] accumulated_eval_time=15622.892882, accumulated_logging_time=0.824489, accumulated_submission_time=22710.087018, global_step=64838, preemption_count=0, score=22710.087018, test/accuracy=0.653873, test/bleu=26.294161, test/loss=1.662145, test/num_examples=3003, total_duration=38335.861197, train/accuracy=0.621435, train/bleu=29.872969, train/loss=1.899622, validation/accuracy=0.645063, validation/bleu=26.963206, validation/loss=1.725381, validation/num_examples=3000
I0209 14:55:04.137091 140277062780672 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2599082887172699, loss=1.8526127338409424
I0209 14:55:39.029379 140277054387968 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.33405816555023193, loss=1.9206236600875854
I0209 14:56:14.006991 140277062780672 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3079487979412079, loss=1.8766752481460571
I0209 14:56:49.001408 140277054387968 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3255115747451782, loss=1.9306706190109253
I0209 14:57:24.020751 140277062780672 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.3366778790950775, loss=1.8824201822280884
I0209 14:57:59.012777 140277054387968 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3312643766403198, loss=1.9539560079574585
I0209 14:58:34.009470 140277062780672 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3587005138397217, loss=1.8364496231079102
I0209 14:59:08.986696 140277054387968 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3725169599056244, loss=1.9111623764038086
I0209 14:59:44.004089 140277062780672 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.2877422273159027, loss=1.8848512172698975
I0209 15:00:19.025738 140277054387968 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.30108603835105896, loss=1.875511884689331
I0209 15:00:54.049400 140277062780672 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.37278059124946594, loss=1.8586562871932983
I0209 15:01:29.074167 140277054387968 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.37527212500572205, loss=1.878221869468689
I0209 15:02:04.094645 140277062780672 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.28525397181510925, loss=1.8578784465789795
I0209 15:02:39.130332 140277054387968 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.33043372631073, loss=1.8406440019607544
I0209 15:03:14.133293 140277062780672 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.26304930448532104, loss=1.7766848802566528
I0209 15:03:49.137482 140277054387968 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.31212785840034485, loss=1.8105493783950806
I0209 15:04:24.148750 140277062780672 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.300367534160614, loss=1.9020849466323853
I0209 15:04:59.198470 140277054387968 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.26284903287887573, loss=1.8394944667816162
I0209 15:05:34.224698 140277062780672 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3796159029006958, loss=1.854273796081543
I0209 15:06:09.248598 140277054387968 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3241581320762634, loss=1.788670539855957
I0209 15:06:44.251701 140277062780672 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.33311501145362854, loss=1.9006824493408203
I0209 15:07:19.240924 140277054387968 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.27224382758140564, loss=1.9111627340316772
I0209 15:07:54.232835 140277062780672 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3359932005405426, loss=1.7911447286605835
I0209 15:08:29.235514 140277054387968 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.3337043523788452, loss=1.8353314399719238
I0209 15:08:42.258547 140446903760704 spec.py:321] Evaluating on the training split.
I0209 15:08:45.262210 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 15:12:41.688389 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 15:12:44.385914 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 15:15:48.990039 140446903760704 spec.py:349] Evaluating on the test split.
I0209 15:15:51.703790 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 15:18:36.214577 140446903760704 submission_runner.py:408] Time since start: 39769.91s, 	Step: 67239, 	{'train/accuracy': 0.6219140887260437, 'train/loss': 1.8977916240692139, 'train/bleu': 29.898942772496444, 'validation/accuracy': 0.6444805264472961, 'validation/loss': 1.7192747592926025, 'validation/bleu': 27.21427000478681, 'validation/num_examples': 3000, 'test/accuracy': 0.6557550430297852, 'test/loss': 1.6552984714508057, 'test/bleu': 26.504229713711247, 'test/num_examples': 3003, 'score': 23550.073257923126, 'total_duration': 39769.913810014725, 'accumulated_submission_time': 23550.073257923126, 'accumulated_eval_time': 16216.848822593689, 'accumulated_logging_time': 0.8594467639923096}
I0209 15:18:36.246973 140277062780672 logging_writer.py:48] [67239] accumulated_eval_time=16216.848823, accumulated_logging_time=0.859447, accumulated_submission_time=23550.073258, global_step=67239, preemption_count=0, score=23550.073258, test/accuracy=0.655755, test/bleu=26.504230, test/loss=1.655298, test/num_examples=3003, total_duration=39769.913810, train/accuracy=0.621914, train/bleu=29.898943, train/loss=1.897792, validation/accuracy=0.644481, validation/bleu=27.214270, validation/loss=1.719275, validation/num_examples=3000
I0209 15:18:57.883296 140277054387968 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.2767796516418457, loss=1.8324981927871704
I0209 15:19:32.764172 140277062780672 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3631429076194763, loss=1.9345581531524658
I0209 15:20:07.763055 140277054387968 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3539174795150757, loss=1.8392565250396729
I0209 15:20:42.816227 140277062780672 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.29294201731681824, loss=1.870744228363037
I0209 15:21:17.803280 140277054387968 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3234916627407074, loss=1.9497321844100952
I0209 15:21:52.830908 140277062780672 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2976635694503784, loss=1.9247678518295288
I0209 15:22:27.815034 140277054387968 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.3005223572254181, loss=1.8418844938278198
I0209 15:23:02.810323 140277062780672 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.314738929271698, loss=1.7850092649459839
I0209 15:23:37.791326 140277054387968 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.29912132024765015, loss=1.924194097518921
I0209 15:24:12.795834 140277062780672 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.30649954080581665, loss=1.9290179014205933
I0209 15:24:47.791309 140277054387968 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.33850908279418945, loss=1.8321664333343506
I0209 15:25:22.793966 140277062780672 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.26715153455734253, loss=1.9064669609069824
I0209 15:25:57.800904 140277054387968 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2539694309234619, loss=1.8473725318908691
I0209 15:26:32.799340 140277062780672 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.31525254249572754, loss=1.8663967847824097
I0209 15:27:07.776305 140277054387968 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.28549817204475403, loss=1.9150973558425903
I0209 15:27:42.810223 140277062780672 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2800169885158539, loss=1.9730181694030762
I0209 15:28:17.821984 140277054387968 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.35063067078590393, loss=1.755067229270935
I0209 15:28:52.825033 140277062780672 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2814722955226898, loss=1.8776057958602905
I0209 15:29:27.830156 140277054387968 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2835032343864441, loss=1.860961675643921
I0209 15:30:02.838442 140277062780672 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2692866027355194, loss=1.8714959621429443
I0209 15:30:37.842110 140277054387968 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.30455902218818665, loss=1.8528144359588623
I0209 15:31:12.859501 140277062780672 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3289366066455841, loss=1.799803376197815
I0209 15:31:47.890873 140277054387968 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.276924192905426, loss=1.8550199270248413
I0209 15:32:22.913891 140277062780672 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.28547167778015137, loss=1.779699683189392
I0209 15:32:36.283458 140446903760704 spec.py:321] Evaluating on the training split.
I0209 15:32:39.289789 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 15:36:04.275520 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 15:36:07.006968 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 15:38:38.976118 140446903760704 spec.py:349] Evaluating on the test split.
I0209 15:38:41.684274 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 15:41:18.472177 140446903760704 submission_runner.py:408] Time since start: 41132.17s, 	Step: 69640, 	{'train/accuracy': 0.6379268765449524, 'train/loss': 1.7811611890792847, 'train/bleu': 30.41896362964424, 'validation/accuracy': 0.649774968624115, 'validation/loss': 1.6948308944702148, 'validation/bleu': 27.45221541734196, 'validation/num_examples': 3000, 'test/accuracy': 0.6588344573974609, 'test/loss': 1.639184832572937, 'test/bleu': 26.867683681614956, 'test/num_examples': 3003, 'score': 24390.021370887756, 'total_duration': 41132.171432971954, 'accumulated_submission_time': 24390.021370887756, 'accumulated_eval_time': 16739.037479639053, 'accumulated_logging_time': 0.9037342071533203}
I0209 15:41:18.498102 140277054387968 logging_writer.py:48] [69640] accumulated_eval_time=16739.037480, accumulated_logging_time=0.903734, accumulated_submission_time=24390.021371, global_step=69640, preemption_count=0, score=24390.021371, test/accuracy=0.658834, test/bleu=26.867684, test/loss=1.639185, test/num_examples=3003, total_duration=41132.171433, train/accuracy=0.637927, train/bleu=30.418964, train/loss=1.781161, validation/accuracy=0.649775, validation/bleu=27.452215, validation/loss=1.694831, validation/num_examples=3000
I0209 15:41:39.766935 140277062780672 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.4522233009338379, loss=1.8515956401824951
I0209 15:42:14.626465 140277054387968 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2717694044113159, loss=1.867031455039978
I0209 15:42:49.556907 140277062780672 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2772858142852783, loss=1.903226613998413
I0209 15:43:24.530613 140277054387968 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3210478127002716, loss=1.793642282485962
I0209 15:43:59.492227 140277062780672 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3430735468864441, loss=1.7995173931121826
I0209 15:44:34.511688 140277054387968 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.323274165391922, loss=1.8617595434188843
I0209 15:45:09.521379 140277062780672 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.398940771818161, loss=1.8195451498031616
I0209 15:45:44.548236 140277054387968 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.28499364852905273, loss=1.8413902521133423
I0209 15:46:19.544675 140277062780672 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2544843256473541, loss=1.7981611490249634
I0209 15:46:54.558153 140277054387968 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.35183703899383545, loss=1.8749998807907104
I0209 15:47:29.574115 140277062780672 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.30065006017684937, loss=1.8899272680282593
I0209 15:48:04.582053 140277054387968 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.28319963812828064, loss=1.8269957304000854
I0209 15:48:39.603893 140277062780672 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3402538001537323, loss=1.967260718345642
I0209 15:49:14.606022 140277054387968 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.30883240699768066, loss=1.8753688335418701
I0209 15:49:49.593316 140277062780672 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.31393197178840637, loss=1.7941749095916748
I0209 15:50:24.550895 140277054387968 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.27999481558799744, loss=1.8554414510726929
I0209 15:50:59.548156 140277062780672 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.40009719133377075, loss=1.918746829032898
I0209 15:51:34.571383 140277054387968 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3178725838661194, loss=1.7701042890548706
I0209 15:52:09.557425 140277062780672 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3070121705532074, loss=1.7937349081039429
I0209 15:52:44.540272 140277054387968 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.34591516852378845, loss=1.8397539854049683
I0209 15:53:19.523303 140277062780672 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.39966535568237305, loss=1.8726792335510254
I0209 15:53:54.497798 140277054387968 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.26213690638542175, loss=1.944061040878296
I0209 15:54:29.512482 140277062780672 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.311287522315979, loss=1.7950637340545654
I0209 15:55:04.499962 140277054387968 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2848188877105713, loss=1.867518424987793
I0209 15:55:18.552135 140446903760704 spec.py:321] Evaluating on the training split.
I0209 15:55:21.555062 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 15:58:47.791476 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 15:58:50.492271 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:01:50.218489 140446903760704 spec.py:349] Evaluating on the test split.
I0209 16:01:52.926702 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:04:44.733420 140446903760704 submission_runner.py:408] Time since start: 42538.43s, 	Step: 72042, 	{'train/accuracy': 0.6333906650543213, 'train/loss': 1.8417229652404785, 'train/bleu': 30.65259166358259, 'validation/accuracy': 0.6506924629211426, 'validation/loss': 1.6889138221740723, 'validation/bleu': 27.472684786958762, 'validation/num_examples': 3000, 'test/accuracy': 0.6625297665596008, 'test/loss': 1.6213717460632324, 'test/bleu': 27.263224574075057, 'test/num_examples': 3003, 'score': 25229.988456487656, 'total_duration': 42538.432683467865, 'accumulated_submission_time': 25229.988456487656, 'accumulated_eval_time': 17305.21870613098, 'accumulated_logging_time': 0.9400687217712402}
I0209 16:04:44.760365 140277062780672 logging_writer.py:48] [72042] accumulated_eval_time=17305.218706, accumulated_logging_time=0.940069, accumulated_submission_time=25229.988456, global_step=72042, preemption_count=0, score=25229.988456, test/accuracy=0.662530, test/bleu=27.263225, test/loss=1.621372, test/num_examples=3003, total_duration=42538.432683, train/accuracy=0.633391, train/bleu=30.652592, train/loss=1.841723, validation/accuracy=0.650692, validation/bleu=27.472685, validation/loss=1.688914, validation/num_examples=3000
I0209 16:05:05.339246 140277054387968 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.43366384506225586, loss=1.8788838386535645
I0209 16:05:40.228038 140277062780672 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2882062792778015, loss=1.8048009872436523
I0209 16:06:15.178290 140277054387968 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.2978587746620178, loss=1.8104853630065918
I0209 16:06:50.210130 140277062780672 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.35068875551223755, loss=1.8146476745605469
I0209 16:07:25.220544 140277054387968 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.30887317657470703, loss=1.8602524995803833
I0209 16:08:00.205401 140277062780672 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3662457764148712, loss=1.6956632137298584
I0209 16:08:35.202188 140277054387968 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.26996371150016785, loss=1.888257622718811
I0209 16:09:10.221032 140277062780672 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2900485098361969, loss=1.8389555215835571
I0209 16:09:45.219825 140277054387968 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.27948620915412903, loss=1.8451876640319824
I0209 16:10:20.250549 140277062780672 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.29969078302383423, loss=1.8452041149139404
I0209 16:10:55.254181 140277054387968 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.25519314408302307, loss=1.7882837057113647
I0209 16:11:30.270785 140277062780672 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3736702501773834, loss=1.8489699363708496
I0209 16:12:05.277822 140277054387968 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.2909211218357086, loss=1.7719038724899292
I0209 16:12:40.258891 140277062780672 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3034587800502777, loss=1.8683758974075317
I0209 16:13:15.288085 140277054387968 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.29281866550445557, loss=1.801416277885437
I0209 16:13:50.281560 140277062780672 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.24376529455184937, loss=1.7866266965866089
I0209 16:14:25.289002 140277054387968 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3503885269165039, loss=1.8390166759490967
I0209 16:15:00.286859 140277062780672 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.31454452872276306, loss=1.8165740966796875
I0209 16:15:35.300787 140277054387968 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.31106051802635193, loss=1.8304355144500732
I0209 16:16:10.350873 140277062780672 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.31356319785118103, loss=1.8404673337936401
I0209 16:16:45.348483 140277054387968 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.284817099571228, loss=1.8279186487197876
I0209 16:17:20.345966 140277062780672 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.27120211720466614, loss=1.8620151281356812
I0209 16:17:55.338928 140277054387968 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.32389646768569946, loss=1.7929556369781494
I0209 16:18:30.334915 140277062780672 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.29662787914276123, loss=1.9003949165344238
I0209 16:18:44.753326 140446903760704 spec.py:321] Evaluating on the training split.
I0209 16:18:47.757966 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:22:09.746535 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 16:22:12.450678 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:24:51.402884 140446903760704 spec.py:349] Evaluating on the test split.
I0209 16:24:54.117068 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:27:54.102630 140446903760704 submission_runner.py:408] Time since start: 43927.80s, 	Step: 74443, 	{'train/accuracy': 0.6327632665634155, 'train/loss': 1.840661883354187, 'train/bleu': 30.128673062601425, 'validation/accuracy': 0.6526143550872803, 'validation/loss': 1.6699992418289185, 'validation/bleu': 27.475459978186038, 'validation/num_examples': 3000, 'test/accuracy': 0.662494957447052, 'test/loss': 1.6078910827636719, 'test/bleu': 26.571996580797872, 'test/num_examples': 3003, 'score': 26069.895349264145, 'total_duration': 43927.80186915398, 'accumulated_submission_time': 26069.895349264145, 'accumulated_eval_time': 17854.567930936813, 'accumulated_logging_time': 0.9772353172302246}
I0209 16:27:54.136615 140277054387968 logging_writer.py:48] [74443] accumulated_eval_time=17854.567931, accumulated_logging_time=0.977235, accumulated_submission_time=26069.895349, global_step=74443, preemption_count=0, score=26069.895349, test/accuracy=0.662495, test/bleu=26.571997, test/loss=1.607891, test/num_examples=3003, total_duration=43927.801869, train/accuracy=0.632763, train/bleu=30.128673, train/loss=1.840662, validation/accuracy=0.652614, validation/bleu=27.475460, validation/loss=1.669999, validation/num_examples=3000
I0209 16:28:14.351781 140277062780672 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3098331689834595, loss=1.8426975011825562
I0209 16:28:49.198287 140277054387968 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.32561346888542175, loss=1.846636414527893
I0209 16:29:24.211013 140277062780672 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.2751831114292145, loss=1.8684377670288086
I0209 16:29:59.206784 140277054387968 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.3016150891780853, loss=1.7996532917022705
I0209 16:30:34.200962 140277062780672 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.32991018891334534, loss=1.722877025604248
I0209 16:31:09.209668 140277054387968 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3438270688056946, loss=1.8191308975219727
I0209 16:31:44.236750 140277062780672 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.37260550260543823, loss=1.7984874248504639
I0209 16:32:19.272981 140277054387968 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.30245766043663025, loss=1.8638442754745483
I0209 16:32:54.288602 140277062780672 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.29143476486206055, loss=1.7581477165222168
I0209 16:33:29.293718 140277054387968 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.2695426940917969, loss=1.6985859870910645
I0209 16:34:04.293570 140277062780672 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.2895018756389618, loss=1.7514946460723877
I0209 16:34:39.293531 140277054387968 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.29647719860076904, loss=1.8708268404006958
I0209 16:35:14.307615 140277062780672 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.2628748416900635, loss=1.769049048423767
I0209 16:35:49.329930 140277054387968 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.29550543427467346, loss=1.810331106185913
I0209 16:36:24.327734 140277062780672 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.287081778049469, loss=1.8481652736663818
I0209 16:36:59.329547 140277054387968 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3038173019886017, loss=1.8316065073013306
I0209 16:37:34.358189 140277062780672 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.2851944863796234, loss=1.8402429819107056
I0209 16:38:09.374877 140277054387968 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.37561285495758057, loss=1.833963394165039
I0209 16:38:44.378162 140277062780672 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.30178824067115784, loss=1.9004669189453125
I0209 16:39:19.400716 140277054387968 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2990564703941345, loss=1.8964539766311646
I0209 16:39:54.427826 140277062780672 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.32758355140686035, loss=1.8193176984786987
I0209 16:40:29.460841 140277054387968 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2813444435596466, loss=1.8106306791305542
I0209 16:41:04.471633 140277062780672 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2488667517900467, loss=1.6623080968856812
I0209 16:41:39.478131 140277054387968 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3932707905769348, loss=1.839647650718689
I0209 16:41:54.252915 140446903760704 spec.py:321] Evaluating on the training split.
I0209 16:41:57.276934 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:45:26.646641 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 16:45:29.364793 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:48:25.570613 140446903760704 spec.py:349] Evaluating on the test split.
I0209 16:48:28.255690 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 16:51:21.741468 140446903760704 submission_runner.py:408] Time since start: 45335.44s, 	Step: 76844, 	{'train/accuracy': 0.6360828280448914, 'train/loss': 1.7997148036956787, 'train/bleu': 30.947704964285027, 'validation/accuracy': 0.6537302732467651, 'validation/loss': 1.655122995376587, 'validation/bleu': 27.672505117892413, 'validation/num_examples': 3000, 'test/accuracy': 0.6651095151901245, 'test/loss': 1.5921093225479126, 'test/bleu': 27.0778855815688, 'test/num_examples': 3003, 'score': 26909.92446255684, 'total_duration': 45335.44073653221, 'accumulated_submission_time': 26909.92446255684, 'accumulated_eval_time': 18422.05643105507, 'accumulated_logging_time': 1.0228424072265625}
I0209 16:51:21.767593 140277062780672 logging_writer.py:48] [76844] accumulated_eval_time=18422.056431, accumulated_logging_time=1.022842, accumulated_submission_time=26909.924463, global_step=76844, preemption_count=0, score=26909.924463, test/accuracy=0.665110, test/bleu=27.077886, test/loss=1.592109, test/num_examples=3003, total_duration=45335.440737, train/accuracy=0.636083, train/bleu=30.947705, train/loss=1.799715, validation/accuracy=0.653730, validation/bleu=27.672505, validation/loss=1.655123, validation/num_examples=3000
I0209 16:51:41.618303 140277054387968 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.32412317395210266, loss=1.8481557369232178
I0209 16:52:16.530043 140277062780672 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.3850118815898895, loss=1.9053434133529663
I0209 16:52:51.506237 140277054387968 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.3730112612247467, loss=1.8235374689102173
I0209 16:53:26.484441 140277062780672 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.28803515434265137, loss=1.8385188579559326
I0209 16:54:01.476834 140277054387968 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.2985020577907562, loss=1.8815301656723022
I0209 16:54:36.440292 140277062780672 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.31943371891975403, loss=1.8431912660598755
I0209 16:55:11.453319 140277054387968 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2863037586212158, loss=1.7962087392807007
I0209 16:55:46.467397 140277062780672 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.3035484552383423, loss=1.77878999710083
I0209 16:56:21.465155 140277054387968 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.2761571705341339, loss=1.7747000455856323
I0209 16:56:56.445416 140277062780672 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3481666147708893, loss=1.7494478225708008
I0209 16:57:31.454236 140277054387968 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.29009678959846497, loss=1.9019566774368286
I0209 16:58:06.449990 140277062780672 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3208782374858856, loss=1.8344480991363525
I0209 16:58:41.440427 140277054387968 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3020710051059723, loss=1.7812464237213135
I0209 16:59:16.421503 140277062780672 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.29449257254600525, loss=1.8129454851150513
I0209 16:59:51.403447 140277054387968 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2755643129348755, loss=1.8068830966949463
I0209 17:00:26.446446 140277062780672 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.33390727639198303, loss=1.752123236656189
I0209 17:01:01.523563 140277054387968 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.32677552103996277, loss=1.7720897197723389
I0209 17:01:36.645771 140277062780672 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2710542678833008, loss=1.9031578302383423
I0209 17:02:11.641761 140277054387968 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2875976264476776, loss=1.8618066310882568
I0209 17:02:46.650200 140277062780672 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3378545045852661, loss=1.8251218795776367
I0209 17:03:21.650917 140277054387968 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3031837046146393, loss=1.8099956512451172
I0209 17:03:56.641255 140277062780672 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3061954379081726, loss=1.8608930110931396
I0209 17:04:31.614656 140277054387968 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.27270981669425964, loss=1.7865545749664307
I0209 17:05:06.654546 140277062780672 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.26541680097579956, loss=1.7810896635055542
I0209 17:05:21.796458 140446903760704 spec.py:321] Evaluating on the training split.
I0209 17:05:24.814479 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:08:56.326140 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 17:08:59.029986 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:11:36.892109 140446903760704 spec.py:349] Evaluating on the test split.
I0209 17:11:39.598451 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:14:13.511815 140446903760704 submission_runner.py:408] Time since start: 46707.21s, 	Step: 79245, 	{'train/accuracy': 0.6354596018791199, 'train/loss': 1.8094682693481445, 'train/bleu': 30.61698412674778, 'validation/accuracy': 0.6570656299591064, 'validation/loss': 1.6474754810333252, 'validation/bleu': 27.837613650228427, 'validation/num_examples': 3000, 'test/accuracy': 0.666852593421936, 'test/loss': 1.5803241729736328, 'test/bleu': 27.10222512757891, 'test/num_examples': 3003, 'score': 27749.865286827087, 'total_duration': 46707.21108341217, 'accumulated_submission_time': 27749.865286827087, 'accumulated_eval_time': 18953.771744966507, 'accumulated_logging_time': 1.0598394870758057}
I0209 17:14:13.538142 140277054387968 logging_writer.py:48] [79245] accumulated_eval_time=18953.771745, accumulated_logging_time=1.059839, accumulated_submission_time=27749.865287, global_step=79245, preemption_count=0, score=27749.865287, test/accuracy=0.666853, test/bleu=27.102225, test/loss=1.580324, test/num_examples=3003, total_duration=46707.211083, train/accuracy=0.635460, train/bleu=30.616984, train/loss=1.809468, validation/accuracy=0.657066, validation/bleu=27.837614, validation/loss=1.647475, validation/num_examples=3000
I0209 17:14:33.025827 140277062780672 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2507871985435486, loss=1.8040192127227783
I0209 17:15:07.902328 140277054387968 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3414417803287506, loss=1.7798203229904175
I0209 17:15:42.878339 140277062780672 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.29116323590278625, loss=1.7366007566452026
I0209 17:16:17.887850 140277054387968 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3135403096675873, loss=1.7762823104858398
I0209 17:16:52.830886 140277062780672 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3130127191543579, loss=1.8626161813735962
I0209 17:17:27.834575 140277054387968 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3535177707672119, loss=1.7794687747955322
I0209 17:18:02.801708 140277062780672 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2891453206539154, loss=1.8205064535140991
I0209 17:18:37.833311 140277054387968 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.307113379240036, loss=1.7420406341552734
I0209 17:19:12.850287 140277062780672 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2987264096736908, loss=1.8418940305709839
I0209 17:19:47.860243 140277054387968 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.29255369305610657, loss=1.794206142425537
I0209 17:20:22.866011 140277062780672 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.28068917989730835, loss=1.8230959177017212
I0209 17:20:57.865962 140277054387968 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.27095335721969604, loss=1.8049921989440918
I0209 17:21:32.927732 140277062780672 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.29723095893859863, loss=1.8048889636993408
I0209 17:22:07.992252 140277054387968 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.36919334530830383, loss=1.8128012418746948
I0209 17:22:43.160727 140277062780672 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.271412193775177, loss=1.829148292541504
I0209 17:23:18.173509 140277054387968 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.3007383644580841, loss=1.7639387845993042
I0209 17:23:53.199444 140277062780672 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3216942548751831, loss=1.8861587047576904
I0209 17:24:28.277087 140277054387968 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3142703175544739, loss=1.8342056274414062
I0209 17:25:03.322991 140277062780672 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3175651431083679, loss=1.7439544200897217
I0209 17:25:38.354297 140277054387968 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.30167585611343384, loss=1.7380821704864502
I0209 17:26:13.407626 140277062780672 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.29491594433784485, loss=1.7069801092147827
I0209 17:26:48.432500 140277054387968 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3101068139076233, loss=1.9451926946640015
I0209 17:27:23.444413 140277062780672 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2848709523677826, loss=1.74547278881073
I0209 17:27:58.446679 140277054387968 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.29599717259407043, loss=1.8303812742233276
I0209 17:28:13.599923 140446903760704 spec.py:321] Evaluating on the training split.
I0209 17:28:16.601216 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:31:30.912812 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 17:31:33.628204 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:34:11.255990 140446903760704 spec.py:349] Evaluating on the test split.
I0209 17:34:13.955393 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:37:03.142960 140446903760704 submission_runner.py:408] Time since start: 48076.84s, 	Step: 81645, 	{'train/accuracy': 0.6552977561950684, 'train/loss': 1.6481138467788696, 'train/bleu': 32.10947252151286, 'validation/accuracy': 0.6575492024421692, 'validation/loss': 1.6359890699386597, 'validation/bleu': 27.994461884509338, 'validation/num_examples': 3000, 'test/accuracy': 0.6707803606987, 'test/loss': 1.570162057876587, 'test/bleu': 27.857890901754264, 'test/num_examples': 3003, 'score': 28589.836373090744, 'total_duration': 48076.842170238495, 'accumulated_submission_time': 28589.836373090744, 'accumulated_eval_time': 19483.31466794014, 'accumulated_logging_time': 1.0974700450897217}
I0209 17:37:03.175366 140277062780672 logging_writer.py:48] [81645] accumulated_eval_time=19483.314668, accumulated_logging_time=1.097470, accumulated_submission_time=28589.836373, global_step=81645, preemption_count=0, score=28589.836373, test/accuracy=0.670780, test/bleu=27.857891, test/loss=1.570162, test/num_examples=3003, total_duration=48076.842170, train/accuracy=0.655298, train/bleu=32.109473, train/loss=1.648114, validation/accuracy=0.657549, validation/bleu=27.994462, validation/loss=1.635989, validation/num_examples=3000
I0209 17:37:22.712609 140277054387968 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2975122928619385, loss=1.7626639604568481
I0209 17:37:57.609976 140277062780672 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.27820461988449097, loss=1.8283369541168213
I0209 17:38:32.575437 140277054387968 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.30889517068862915, loss=1.781202793121338
I0209 17:39:07.533505 140277062780672 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3261193037033081, loss=1.7402336597442627
I0209 17:39:42.526793 140277054387968 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.30415067076683044, loss=1.7784667015075684
I0209 17:40:17.516479 140277062780672 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.27850571274757385, loss=1.7361929416656494
I0209 17:40:52.523082 140277054387968 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.28433364629745483, loss=1.8552029132843018
I0209 17:41:27.541849 140277062780672 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.2912335991859436, loss=1.7668607234954834
I0209 17:42:02.553928 140277054387968 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2875809669494629, loss=1.7495346069335938
I0209 17:42:37.549357 140277062780672 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.28611883521080017, loss=1.7556418180465698
I0209 17:43:12.534165 140277054387968 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3121519684791565, loss=1.7637027502059937
I0209 17:43:47.493104 140277062780672 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.3147869408130646, loss=1.7348699569702148
I0209 17:44:22.472851 140277054387968 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.30278000235557556, loss=1.8025896549224854
I0209 17:44:57.484200 140277062780672 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.37534213066101074, loss=1.7220960855484009
I0209 17:45:32.454425 140277054387968 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.32107865810394287, loss=1.7324645519256592
I0209 17:46:07.421608 140277062780672 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.28252550959587097, loss=1.7687150239944458
I0209 17:46:42.403562 140277054387968 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2929297685623169, loss=1.7367428541183472
I0209 17:47:17.397494 140277062780672 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3107943534851074, loss=1.764223575592041
I0209 17:47:52.349889 140277054387968 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3694128692150116, loss=1.7638790607452393
I0209 17:48:27.332847 140277062780672 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.28226613998413086, loss=1.799033761024475
I0209 17:49:02.324722 140277054387968 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3109576106071472, loss=1.8466001749038696
I0209 17:49:37.308228 140277062780672 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.3065887689590454, loss=1.8567981719970703
I0209 17:50:12.279194 140277054387968 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.30365830659866333, loss=1.808724045753479
I0209 17:50:47.253080 140277062780672 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.3120609521865845, loss=1.7589970827102661
I0209 17:51:03.422111 140446903760704 spec.py:321] Evaluating on the training split.
I0209 17:51:06.430086 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:55:11.470048 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 17:55:14.176734 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 17:57:58.100770 140446903760704 spec.py:349] Evaluating on the test split.
I0209 17:58:00.828507 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 18:00:49.339607 140446903760704 submission_runner.py:408] Time since start: 49503.04s, 	Step: 84048, 	{'train/accuracy': 0.6372441649436951, 'train/loss': 1.792824149131775, 'train/bleu': 31.231938001811475, 'validation/accuracy': 0.6590867042541504, 'validation/loss': 1.6225743293762207, 'validation/bleu': 28.227901930406095, 'validation/num_examples': 3000, 'test/accuracy': 0.6720934510231018, 'test/loss': 1.5535181760787964, 'test/bleu': 27.58812304327058, 'test/num_examples': 3003, 'score': 29429.99524831772, 'total_duration': 49503.03884482384, 'accumulated_submission_time': 29429.99524831772, 'accumulated_eval_time': 20069.23208117485, 'accumulated_logging_time': 1.141160249710083}
I0209 18:00:49.372982 140277054387968 logging_writer.py:48] [84048] accumulated_eval_time=20069.232081, accumulated_logging_time=1.141160, accumulated_submission_time=29429.995248, global_step=84048, preemption_count=0, score=29429.995248, test/accuracy=0.672093, test/bleu=27.588123, test/loss=1.553518, test/num_examples=3003, total_duration=49503.038845, train/accuracy=0.637244, train/bleu=31.231938, train/loss=1.792824, validation/accuracy=0.659087, validation/bleu=28.227902, validation/loss=1.622574, validation/num_examples=3000
I0209 18:01:07.860110 140277062780672 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.32435065507888794, loss=1.7803568840026855
I0209 18:01:42.735990 140277054387968 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3303510844707489, loss=1.7930608987808228
I0209 18:02:17.730378 140277062780672 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.27764829993247986, loss=1.843366265296936
I0209 18:02:52.737378 140277054387968 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.30054402351379395, loss=1.8151029348373413
I0209 18:03:27.708308 140277062780672 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3054218590259552, loss=1.8432509899139404
I0209 18:04:02.716679 140277054387968 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.29441583156585693, loss=1.7850251197814941
I0209 18:04:37.739570 140277062780672 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.28866592049598694, loss=1.710718035697937
I0209 18:05:12.735997 140277054387968 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.3314196467399597, loss=1.763953685760498
I0209 18:05:47.718052 140277062780672 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.284568190574646, loss=1.7605116367340088
I0209 18:06:22.718273 140277054387968 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.291323721408844, loss=1.804970622062683
I0209 18:06:57.727536 140277062780672 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.3051254153251648, loss=1.7601383924484253
I0209 18:07:32.761829 140277054387968 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.25993648171424866, loss=1.778434157371521
I0209 18:08:07.774981 140277062780672 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.38991913199424744, loss=1.850969910621643
I0209 18:08:42.761927 140277054387968 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.31630846858024597, loss=1.7854951620101929
I0209 18:09:17.765373 140277062780672 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.316590279340744, loss=1.8554104566574097
I0209 18:09:52.760638 140277054387968 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.31662413477897644, loss=1.783338189125061
I0209 18:10:27.750645 140277062780672 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.2877833843231201, loss=1.7772949934005737
I0209 18:11:02.770887 140277054387968 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2893564999103546, loss=1.7744591236114502
I0209 18:11:37.777403 140277062780672 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.2943675220012665, loss=1.797532081604004
I0209 18:12:12.810359 140277054387968 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.358460396528244, loss=1.837727427482605
I0209 18:12:47.826833 140277062780672 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3353736400604248, loss=1.8419989347457886
I0209 18:13:22.857065 140277054387968 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.2982114553451538, loss=1.758492112159729
I0209 18:13:57.863871 140277062780672 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.3212466537952423, loss=1.7939586639404297
I0209 18:14:32.890601 140277054387968 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.32698357105255127, loss=1.8007287979125977
I0209 18:14:49.404123 140446903760704 spec.py:321] Evaluating on the training split.
I0209 18:14:52.417126 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 18:19:07.035432 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 18:19:09.732605 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 18:22:41.936304 140446903760704 spec.py:349] Evaluating on the test split.
I0209 18:22:44.632642 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 18:26:19.683650 140446903760704 submission_runner.py:408] Time since start: 51033.38s, 	Step: 86449, 	{'train/accuracy': 0.6410945057868958, 'train/loss': 1.7650405168533325, 'train/bleu': 31.419086215696446, 'validation/accuracy': 0.6631659865379333, 'validation/loss': 1.604163408279419, 'validation/bleu': 28.648366919736237, 'validation/num_examples': 3000, 'test/accuracy': 0.6744524240493774, 'test/loss': 1.5341390371322632, 'test/bleu': 27.97908133401334, 'test/num_examples': 3003, 'score': 30269.94028711319, 'total_duration': 51033.38291668892, 'accumulated_submission_time': 30269.94028711319, 'accumulated_eval_time': 20759.51155924797, 'accumulated_logging_time': 1.185424566268921}
I0209 18:26:19.710664 140277062780672 logging_writer.py:48] [86449] accumulated_eval_time=20759.511559, accumulated_logging_time=1.185425, accumulated_submission_time=30269.940287, global_step=86449, preemption_count=0, score=30269.940287, test/accuracy=0.674452, test/bleu=27.979081, test/loss=1.534139, test/num_examples=3003, total_duration=51033.382917, train/accuracy=0.641095, train/bleu=31.419086, train/loss=1.765041, validation/accuracy=0.663166, validation/bleu=28.648367, validation/loss=1.604163, validation/num_examples=3000
I0209 18:26:37.794474 140277054387968 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.30369383096694946, loss=1.8072623014450073
I0209 18:27:12.674576 140277062780672 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4440109431743622, loss=1.8050153255462646
I0209 18:27:47.652217 140277054387968 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.28794530034065247, loss=1.7734861373901367
I0209 18:28:22.650412 140277062780672 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3126198351383209, loss=1.6999844312667847
I0209 18:28:57.759592 140277054387968 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.3017897605895996, loss=1.7504100799560547
I0209 18:29:32.734173 140277062780672 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.30444106459617615, loss=1.7194782495498657
I0209 18:30:07.710019 140277054387968 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.3116338551044464, loss=1.7771133184432983
I0209 18:30:42.698538 140277062780672 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2936193346977234, loss=1.7175917625427246
I0209 18:31:17.706582 140277054387968 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.29494479298591614, loss=1.705522060394287
I0209 18:31:52.697325 140277062780672 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3426949977874756, loss=1.7364698648452759
I0209 18:32:27.662377 140277054387968 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3014456331729889, loss=1.681078553199768
I0209 18:33:02.644038 140277062780672 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.31631922721862793, loss=1.7759701013565063
I0209 18:33:37.657402 140277054387968 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.3034600019454956, loss=1.7232710123062134
I0209 18:34:12.633388 140277062780672 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3408491909503937, loss=1.6862804889678955
I0209 18:34:47.631659 140277054387968 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.35994312167167664, loss=1.7634178400039673
I0209 18:35:22.612684 140277062780672 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3080465495586395, loss=1.7424975633621216
I0209 18:35:57.580393 140277054387968 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3099343180656433, loss=1.7769978046417236
I0209 18:36:32.550737 140277062780672 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.307081937789917, loss=1.786811113357544
I0209 18:37:07.558943 140277054387968 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.2976216971874237, loss=1.7564834356307983
I0209 18:37:42.528189 140277062780672 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.291759729385376, loss=1.718693733215332
I0209 18:38:17.509987 140277054387968 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.30155470967292786, loss=1.7116044759750366
I0209 18:38:52.519507 140277062780672 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.29864683747291565, loss=1.7880133390426636
I0209 18:39:27.497566 140277054387968 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.3017106056213379, loss=1.804953694343567
I0209 18:40:02.467698 140277062780672 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.2925199866294861, loss=1.7513511180877686
I0209 18:40:19.693344 140446903760704 spec.py:321] Evaluating on the training split.
I0209 18:40:22.707976 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 18:43:49.325640 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 18:43:52.022935 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 18:46:33.230153 140446903760704 spec.py:349] Evaluating on the test split.
I0209 18:46:35.936161 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 18:49:25.084435 140446903760704 submission_runner.py:408] Time since start: 52418.78s, 	Step: 88851, 	{'train/accuracy': 0.6504623293876648, 'train/loss': 1.689774990081787, 'train/bleu': 31.84581123567893, 'validation/accuracy': 0.6630296111106873, 'validation/loss': 1.5953129529953003, 'validation/bleu': 28.532335408202147, 'validation/num_examples': 3000, 'test/accuracy': 0.6743943095207214, 'test/loss': 1.5237705707550049, 'test/bleu': 27.792506126010068, 'test/num_examples': 3003, 'score': 31109.83723974228, 'total_duration': 52418.78368616104, 'accumulated_submission_time': 31109.83723974228, 'accumulated_eval_time': 21304.902579307556, 'accumulated_logging_time': 1.2225456237792969}
I0209 18:49:25.113049 140277054387968 logging_writer.py:48] [88851] accumulated_eval_time=21304.902579, accumulated_logging_time=1.222546, accumulated_submission_time=31109.837240, global_step=88851, preemption_count=0, score=31109.837240, test/accuracy=0.674394, test/bleu=27.792506, test/loss=1.523771, test/num_examples=3003, total_duration=52418.783686, train/accuracy=0.650462, train/bleu=31.845811, train/loss=1.689775, validation/accuracy=0.663030, validation/bleu=28.532335, validation/loss=1.595313, validation/num_examples=3000
I0209 18:49:42.495420 140277062780672 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3184344172477722, loss=1.7357901334762573
I0209 18:50:17.314614 140277054387968 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.39207157492637634, loss=1.6966460943222046
I0209 18:50:52.254452 140277062780672 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.3449145257472992, loss=1.7235734462738037
I0209 18:51:27.241781 140277054387968 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.4346439838409424, loss=1.7788748741149902
I0209 18:52:02.237960 140277062780672 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.27248090505599976, loss=1.6502397060394287
I0209 18:52:37.216990 140277054387968 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.27272605895996094, loss=1.6423723697662354
I0209 18:53:12.207892 140277062780672 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.3368174135684967, loss=1.80819571018219
I0209 18:53:47.165826 140277054387968 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.30889034271240234, loss=1.7746320962905884
I0209 18:54:22.145839 140277062780672 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.32849541306495667, loss=1.7867037057876587
I0209 18:54:57.127977 140277054387968 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.2969903349876404, loss=1.7178822755813599
I0209 18:55:32.110135 140277062780672 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.31587809324264526, loss=1.8118717670440674
I0209 18:56:07.119476 140277054387968 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.30212610960006714, loss=1.7767544984817505
I0209 18:56:42.093015 140277062780672 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3312799334526062, loss=1.7963472604751587
I0209 18:57:17.084082 140277054387968 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.5081197023391724, loss=1.7603404521942139
I0209 18:57:52.042378 140277062780672 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3936241865158081, loss=1.7940326929092407
I0209 18:58:27.013945 140277054387968 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.2973909378051758, loss=1.7739185094833374
I0209 18:59:01.984067 140277062780672 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3256298005580902, loss=1.7022336721420288
I0209 18:59:36.974645 140277054387968 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.2987578213214874, loss=1.7841299772262573
I0209 19:00:11.945224 140277062780672 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.31025606393814087, loss=1.7402138710021973
I0209 19:00:46.943389 140277054387968 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.31282925605773926, loss=1.8075921535491943
I0209 19:01:21.955346 140277062780672 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.3071930706501007, loss=1.7208337783813477
I0209 19:01:57.067362 140277054387968 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3249034881591797, loss=1.7266504764556885
I0209 19:02:32.035570 140277062780672 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.30396872758865356, loss=1.654477596282959
I0209 19:03:07.004324 140277054387968 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.27179744839668274, loss=1.7469613552093506
I0209 19:03:25.247972 140446903760704 spec.py:321] Evaluating on the training split.
I0209 19:03:28.252153 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:06:53.938879 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 19:06:56.658237 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:09:42.966979 140446903760704 spec.py:349] Evaluating on the test split.
I0209 19:09:45.681920 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:12:26.562955 140446903760704 submission_runner.py:408] Time since start: 53800.26s, 	Step: 91254, 	{'train/accuracy': 0.6479614973068237, 'train/loss': 1.720964789390564, 'train/bleu': 31.763669002056673, 'validation/accuracy': 0.665571391582489, 'validation/loss': 1.5849394798278809, 'validation/bleu': 28.862445004881646, 'validation/num_examples': 3000, 'test/accuracy': 0.6779152750968933, 'test/loss': 1.5046041011810303, 'test/bleu': 28.25653450368869, 'test/num_examples': 3003, 'score': 31949.886343479156, 'total_duration': 53800.26221823692, 'accumulated_submission_time': 31949.886343479156, 'accumulated_eval_time': 21846.217509269714, 'accumulated_logging_time': 1.2612571716308594}
I0209 19:12:26.591832 140277062780672 logging_writer.py:48] [91254] accumulated_eval_time=21846.217509, accumulated_logging_time=1.261257, accumulated_submission_time=31949.886343, global_step=91254, preemption_count=0, score=31949.886343, test/accuracy=0.677915, test/bleu=28.256535, test/loss=1.504604, test/num_examples=3003, total_duration=53800.262218, train/accuracy=0.647961, train/bleu=31.763669, train/loss=1.720965, validation/accuracy=0.665571, validation/bleu=28.862445, validation/loss=1.584939, validation/num_examples=3000
I0209 19:12:42.968710 140277054387968 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.2814716696739197, loss=1.6823137998580933
I0209 19:13:17.828827 140277062780672 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.3221062421798706, loss=1.6943544149398804
I0209 19:13:52.796400 140277054387968 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.28090834617614746, loss=1.7071598768234253
I0209 19:14:27.820393 140277062780672 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.34775251150131226, loss=1.784613013267517
I0209 19:15:02.858218 140277054387968 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.30108100175857544, loss=1.7190816402435303
I0209 19:15:37.827917 140277062780672 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.292828768491745, loss=1.6961880922317505
I0209 19:16:12.816896 140277054387968 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.3171112835407257, loss=1.666301965713501
I0209 19:16:47.873060 140277062780672 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3373230993747711, loss=1.7768889665603638
I0209 19:17:22.859037 140277054387968 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.28354713320732117, loss=1.7589364051818848
I0209 19:17:57.863436 140277062780672 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3454023003578186, loss=1.7318110466003418
I0209 19:18:32.850177 140277054387968 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3212260603904724, loss=1.6778606176376343
I0209 19:19:07.907575 140277062780672 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3147665858268738, loss=1.7901380062103271
I0209 19:19:42.886168 140277054387968 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.34815704822540283, loss=1.7121039628982544
I0209 19:20:17.867160 140277062780672 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.28708478808403015, loss=1.7080332040786743
I0209 19:20:52.847858 140277054387968 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.29379403591156006, loss=1.6818017959594727
I0209 19:21:27.834147 140277062780672 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.28741297125816345, loss=1.7021021842956543
I0209 19:22:02.831178 140277054387968 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2977886497974396, loss=1.6848257780075073
I0209 19:22:37.828688 140277062780672 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3668138384819031, loss=1.7769474983215332
I0209 19:23:12.804472 140277054387968 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.30934444069862366, loss=1.7969452142715454
I0209 19:23:47.790641 140277062780672 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.28815996646881104, loss=1.7480336427688599
I0209 19:24:22.779238 140277054387968 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.29700329899787903, loss=1.7259349822998047
I0209 19:24:57.739582 140277062780672 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.33073168992996216, loss=1.7296611070632935
I0209 19:25:32.736982 140277054387968 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.30026566982269287, loss=1.671817660331726
I0209 19:26:07.722929 140277062780672 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3375028371810913, loss=1.7080953121185303
I0209 19:26:26.663534 140446903760704 spec.py:321] Evaluating on the training split.
I0209 19:26:29.680344 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:30:13.781008 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 19:30:16.498407 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:33:07.958232 140446903760704 spec.py:349] Evaluating on the test split.
I0209 19:33:10.664386 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:35:44.144317 140446903760704 submission_runner.py:408] Time since start: 55197.84s, 	Step: 93656, 	{'train/accuracy': 0.6468232274055481, 'train/loss': 1.723307490348816, 'train/bleu': 31.683186844159014, 'validation/accuracy': 0.6688323616981506, 'validation/loss': 1.5702892541885376, 'validation/bleu': 29.080144950437283, 'validation/num_examples': 3000, 'test/accuracy': 0.6798908114433289, 'test/loss': 1.4960421323776245, 'test/bleu': 28.425006038283964, 'test/num_examples': 3003, 'score': 32789.87203192711, 'total_duration': 55197.843589782715, 'accumulated_submission_time': 32789.87203192711, 'accumulated_eval_time': 22403.698242902756, 'accumulated_logging_time': 1.300079584121704}
I0209 19:35:44.172062 140277054387968 logging_writer.py:48] [93656] accumulated_eval_time=22403.698243, accumulated_logging_time=1.300080, accumulated_submission_time=32789.872032, global_step=93656, preemption_count=0, score=32789.872032, test/accuracy=0.679891, test/bleu=28.425006, test/loss=1.496042, test/num_examples=3003, total_duration=55197.843590, train/accuracy=0.646823, train/bleu=31.683187, train/loss=1.723307, validation/accuracy=0.668832, validation/bleu=29.080145, validation/loss=1.570289, validation/num_examples=3000
I0209 19:35:59.841313 140277062780672 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.2859036922454834, loss=1.6596494913101196
I0209 19:36:34.719146 140277054387968 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.28384140133857727, loss=1.6285290718078613
I0209 19:37:09.723175 140277062780672 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.2947620749473572, loss=1.7576324939727783
I0209 19:37:44.701594 140277054387968 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.31665176153182983, loss=1.6579407453536987
I0209 19:38:19.684947 140277062780672 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.32095861434936523, loss=1.6742150783538818
I0209 19:38:54.696588 140277054387968 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3159236013889313, loss=1.7194514274597168
I0209 19:39:29.662872 140277062780672 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.2923011779785156, loss=1.7422726154327393
I0209 19:40:04.647021 140277054387968 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.2982470691204071, loss=1.7151800394058228
I0209 19:40:39.604725 140277062780672 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3119352459907532, loss=1.7248915433883667
I0209 19:41:14.616012 140277054387968 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.33216509222984314, loss=1.7599296569824219
I0209 19:41:49.596342 140277062780672 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.31369686126708984, loss=1.6947165727615356
I0209 19:42:24.610289 140277054387968 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.29460975527763367, loss=1.6758711338043213
I0209 19:42:59.645601 140277062780672 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.2918809950351715, loss=1.6913490295410156
I0209 19:43:34.656327 140277054387968 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.2949426472187042, loss=1.659393310546875
I0209 19:44:09.653150 140277062780672 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3203760087490082, loss=1.7270234823226929
I0209 19:44:44.657252 140277054387968 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.29780399799346924, loss=1.7618439197540283
I0209 19:45:19.695664 140277062780672 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3101058602333069, loss=1.739292025566101
I0209 19:45:54.723860 140277054387968 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2940763235092163, loss=1.6170408725738525
I0209 19:46:29.724367 140277062780672 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3016299307346344, loss=1.6308965682983398
I0209 19:47:04.732582 140277054387968 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3160300850868225, loss=1.662621259689331
I0209 19:47:39.733333 140277062780672 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.33385390043258667, loss=1.7790887355804443
I0209 19:48:14.713582 140277054387968 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.2872607707977295, loss=1.754935383796692
I0209 19:48:49.697173 140277062780672 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.35069015622138977, loss=1.781758427619934
I0209 19:49:24.682861 140277054387968 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.33413493633270264, loss=1.6806145906448364
I0209 19:49:44.339277 140446903760704 spec.py:321] Evaluating on the training split.
I0209 19:49:47.350365 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:53:05.925808 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 19:53:08.652986 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:55:56.319501 140446903760704 spec.py:349] Evaluating on the test split.
I0209 19:55:59.054270 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 19:58:49.840256 140446903760704 submission_runner.py:408] Time since start: 56583.54s, 	Step: 96058, 	{'train/accuracy': 0.6511693596839905, 'train/loss': 1.6873743534088135, 'train/bleu': 31.714046536608137, 'validation/accuracy': 0.6717833280563354, 'validation/loss': 1.547297716140747, 'validation/bleu': 29.245886051656544, 'validation/num_examples': 3000, 'test/accuracy': 0.6813665628433228, 'test/loss': 1.4757575988769531, 'test/bleu': 28.803274428417076, 'test/num_examples': 3003, 'score': 33629.953258514404, 'total_duration': 56583.53949093819, 'accumulated_submission_time': 33629.953258514404, 'accumulated_eval_time': 22949.19913959503, 'accumulated_logging_time': 1.3382477760314941}
I0209 19:58:49.876702 140277062780672 logging_writer.py:48] [96058] accumulated_eval_time=22949.199140, accumulated_logging_time=1.338248, accumulated_submission_time=33629.953259, global_step=96058, preemption_count=0, score=33629.953259, test/accuracy=0.681367, test/bleu=28.803274, test/loss=1.475758, test/num_examples=3003, total_duration=56583.539491, train/accuracy=0.651169, train/bleu=31.714047, train/loss=1.687374, validation/accuracy=0.671783, validation/bleu=29.245886, validation/loss=1.547298, validation/num_examples=3000
I0209 19:59:04.862761 140277054387968 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.2956741750240326, loss=1.6616485118865967
I0209 19:59:39.704810 140277062780672 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.30360668897628784, loss=1.6518495082855225
I0209 20:00:14.640126 140277054387968 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.29803186655044556, loss=1.7522941827774048
I0209 20:00:49.607559 140277062780672 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3274109363555908, loss=1.7542624473571777
I0209 20:01:24.590252 140277054387968 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.30578774213790894, loss=1.6668338775634766
I0209 20:01:59.602747 140277062780672 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.3720879852771759, loss=1.7573484182357788
I0209 20:02:34.580368 140277054387968 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3047277629375458, loss=1.6637550592422485
I0209 20:03:09.567744 140277062780672 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.2901915907859802, loss=1.7541179656982422
I0209 20:03:44.544530 140277054387968 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.30051305890083313, loss=1.7095122337341309
I0209 20:04:19.528794 140277062780672 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.2897854745388031, loss=1.6987600326538086
I0209 20:04:54.509810 140277054387968 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.30811282992362976, loss=1.6344866752624512
I0209 20:05:29.480813 140277062780672 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.2978489100933075, loss=1.629522681236267
I0209 20:06:04.461895 140277054387968 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.30682796239852905, loss=1.6969506740570068
I0209 20:06:39.468930 140277062780672 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3237852156162262, loss=1.6815950870513916
I0209 20:07:14.486399 140277054387968 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3499467074871063, loss=1.6944575309753418
I0209 20:07:49.473051 140277062780672 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.31949707865715027, loss=1.6968718767166138
I0209 20:08:24.476114 140277054387968 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.28756511211395264, loss=1.67037832736969
I0209 20:08:59.438242 140277062780672 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3080418109893799, loss=1.7253763675689697
I0209 20:09:34.424445 140277054387968 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.2944684624671936, loss=1.6490036249160767
I0209 20:10:09.407739 140277062780672 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.32819727063179016, loss=1.7571765184402466
I0209 20:10:44.396919 140277054387968 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.2966196537017822, loss=1.6684908866882324
I0209 20:11:19.399260 140277062780672 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.32685112953186035, loss=1.7149245738983154
I0209 20:11:54.375239 140277054387968 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.31789425015449524, loss=1.7573214769363403
I0209 20:12:29.335947 140277062780672 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3041857182979584, loss=1.6028631925582886
I0209 20:12:50.021099 140446903760704 spec.py:321] Evaluating on the training split.
I0209 20:12:53.024006 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 20:16:09.488647 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 20:16:12.182227 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 20:18:54.968455 140446903760704 spec.py:349] Evaluating on the test split.
I0209 20:18:57.680078 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 20:21:23.530969 140446903760704 submission_runner.py:408] Time since start: 57937.23s, 	Step: 98461, 	{'train/accuracy': 0.6557013392448425, 'train/loss': 1.67685866355896, 'train/bleu': 32.65956777295396, 'validation/accuracy': 0.6736308336257935, 'validation/loss': 1.5366019010543823, 'validation/bleu': 29.419734290930183, 'validation/num_examples': 3000, 'test/accuracy': 0.6865260601043701, 'test/loss': 1.456973910331726, 'test/bleu': 29.064059597154397, 'test/num_examples': 3003, 'score': 34470.011729478836, 'total_duration': 57937.23023700714, 'accumulated_submission_time': 34470.011729478836, 'accumulated_eval_time': 23462.708975553513, 'accumulated_logging_time': 1.385880470275879}
I0209 20:21:23.560289 140277054387968 logging_writer.py:48] [98461] accumulated_eval_time=23462.708976, accumulated_logging_time=1.385880, accumulated_submission_time=34470.011729, global_step=98461, preemption_count=0, score=34470.011729, test/accuracy=0.686526, test/bleu=29.064060, test/loss=1.456974, test/num_examples=3003, total_duration=57937.230237, train/accuracy=0.655701, train/bleu=32.659568, train/loss=1.676859, validation/accuracy=0.673631, validation/bleu=29.419734, validation/loss=1.536602, validation/num_examples=3000
I0209 20:21:37.523356 140277062780672 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3192460834980011, loss=1.6899794340133667
I0209 20:22:12.402797 140277054387968 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.32420799136161804, loss=1.6596605777740479
I0209 20:22:47.350511 140277062780672 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.28887683153152466, loss=1.63084077835083
I0209 20:23:22.321426 140277054387968 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.32230478525161743, loss=1.7673890590667725
I0209 20:23:57.306174 140277062780672 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.30344557762145996, loss=1.734336256980896
I0209 20:24:32.311433 140277054387968 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2892513573169708, loss=1.6809160709381104
I0209 20:25:07.306193 140277062780672 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.29513272643089294, loss=1.6594536304473877
I0209 20:25:42.274567 140277054387968 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.2947884500026703, loss=1.6330254077911377
I0209 20:26:17.263413 140277062780672 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3198740482330322, loss=1.6711689233779907
I0209 20:26:52.245210 140277054387968 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.30440792441368103, loss=1.634346604347229
I0209 20:27:27.233142 140277062780672 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.2851913571357727, loss=1.6594992876052856
I0209 20:28:02.203553 140277054387968 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.29408779740333557, loss=1.6533594131469727
I0209 20:28:37.190755 140277062780672 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.30236539244651794, loss=1.584686517715454
I0209 20:29:12.176710 140277054387968 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.3333609104156494, loss=1.694168210029602
I0209 20:29:47.146373 140277062780672 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.28331485390663147, loss=1.6040596961975098
I0209 20:30:22.140310 140277054387968 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.31472545862197876, loss=1.6075254678726196
I0209 20:30:57.116611 140277062780672 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.31110990047454834, loss=1.721453309059143
I0209 20:31:32.091102 140277054387968 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3181445598602295, loss=1.6580177545547485
I0209 20:32:07.102474 140277062780672 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.33640623092651367, loss=1.6177164316177368
I0209 20:32:42.125480 140277054387968 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.33754491806030273, loss=1.6843174695968628
I0209 20:33:17.150440 140277062780672 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.301667183637619, loss=1.668882966041565
I0209 20:33:52.192212 140277054387968 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.2886350154876709, loss=1.636950969696045
I0209 20:34:27.198547 140277062780672 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3320271372795105, loss=1.6308673620224
I0209 20:35:02.162763 140277054387968 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.31483790278434753, loss=1.6323421001434326
I0209 20:35:23.563466 140446903760704 spec.py:321] Evaluating on the training split.
I0209 20:35:26.566973 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 20:39:06.577686 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 20:39:09.297400 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 20:42:11.287961 140446903760704 spec.py:349] Evaluating on the test split.
I0209 20:42:14.012244 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 20:44:53.671506 140446903760704 submission_runner.py:408] Time since start: 59347.37s, 	Step: 100863, 	{'train/accuracy': 0.6670910716056824, 'train/loss': 1.576935052871704, 'train/bleu': 33.74426266233717, 'validation/accuracy': 0.6752303242683411, 'validation/loss': 1.5253691673278809, 'validation/bleu': 29.383349787408342, 'validation/num_examples': 3000, 'test/accuracy': 0.6878159642219543, 'test/loss': 1.442979335784912, 'test/bleu': 29.206599892438135, 'test/num_examples': 3003, 'score': 35309.92882537842, 'total_duration': 59347.370725631714, 'accumulated_submission_time': 35309.92882537842, 'accumulated_eval_time': 24032.816915750504, 'accumulated_logging_time': 1.4265773296356201}
I0209 20:44:53.707583 140277062780672 logging_writer.py:48] [100863] accumulated_eval_time=24032.816916, accumulated_logging_time=1.426577, accumulated_submission_time=35309.928825, global_step=100863, preemption_count=0, score=35309.928825, test/accuracy=0.687816, test/bleu=29.206600, test/loss=1.442979, test/num_examples=3003, total_duration=59347.370726, train/accuracy=0.667091, train/bleu=33.744263, train/loss=1.576935, validation/accuracy=0.675230, validation/bleu=29.383350, validation/loss=1.525369, validation/num_examples=3000
I0209 20:45:06.952269 140277054387968 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.30098459124565125, loss=1.6726343631744385
I0209 20:45:41.798558 140277062780672 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3085289001464844, loss=1.6853440999984741
I0209 20:46:16.755760 140277054387968 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3216056525707245, loss=1.6641435623168945
I0209 20:46:51.727208 140277062780672 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.32381966710090637, loss=1.6877938508987427
I0209 20:47:26.745459 140277054387968 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.320125937461853, loss=1.7198063135147095
I0209 20:48:01.764884 140277062780672 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.30869001150131226, loss=1.673245906829834
I0209 20:48:36.778612 140277054387968 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3058324456214905, loss=1.6675646305084229
I0209 20:49:11.736430 140277062780672 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.30947020649909973, loss=1.6356171369552612
I0209 20:49:46.725823 140277054387968 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.32404160499572754, loss=1.634804606437683
I0209 20:50:21.725069 140277062780672 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.28724873065948486, loss=1.6689196825027466
I0209 20:50:56.730751 140277054387968 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3010856807231903, loss=1.7274188995361328
I0209 20:51:31.749240 140277062780672 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3126107454299927, loss=1.6687074899673462
I0209 20:52:06.746926 140277054387968 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.32242268323898315, loss=1.6436372995376587
I0209 20:52:41.763468 140277062780672 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3242979645729065, loss=1.6907479763031006
I0209 20:53:16.763270 140277054387968 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3169287145137787, loss=1.6087087392807007
I0209 20:53:51.778593 140277062780672 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.3404754400253296, loss=1.6510926485061646
I0209 20:54:26.761313 140277054387968 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3361242711544037, loss=1.6386909484863281
I0209 20:55:01.773470 140277062780672 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.36578369140625, loss=1.645787239074707
I0209 20:55:36.754533 140277054387968 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.32161128520965576, loss=1.7678821086883545
I0209 20:56:11.740990 140277062780672 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3252369165420532, loss=1.680851936340332
I0209 20:56:46.716168 140277054387968 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.33574923872947693, loss=1.6658284664154053
I0209 20:57:21.693037 140277062780672 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3322626054286957, loss=1.6424590349197388
I0209 20:57:56.683719 140277054387968 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.3152085244655609, loss=1.6225718259811401
I0209 20:58:31.644665 140277062780672 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.3105093836784363, loss=1.713547706604004
I0209 20:58:53.747628 140446903760704 spec.py:321] Evaluating on the training split.
I0209 20:58:56.752619 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:02:56.867622 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 21:02:59.568558 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:05:38.346251 140446903760704 spec.py:349] Evaluating on the test split.
I0209 21:05:41.042622 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:08:33.548687 140446903760704 submission_runner.py:408] Time since start: 60767.25s, 	Step: 103265, 	{'train/accuracy': 0.6593321561813354, 'train/loss': 1.6377383470535278, 'train/bleu': 32.99818121622395, 'validation/accuracy': 0.676978588104248, 'validation/loss': 1.5108222961425781, 'validation/bleu': 29.280341283493588, 'validation/num_examples': 3000, 'test/accuracy': 0.6917320489883423, 'test/loss': 1.4219063520431519, 'test/bleu': 29.722389977868755, 'test/num_examples': 3003, 'score': 36149.880427360535, 'total_duration': 60767.247957229614, 'accumulated_submission_time': 36149.880427360535, 'accumulated_eval_time': 24612.617929458618, 'accumulated_logging_time': 1.474708080291748}
I0209 21:08:33.577815 140277054387968 logging_writer.py:48] [103265] accumulated_eval_time=24612.617929, accumulated_logging_time=1.474708, accumulated_submission_time=36149.880427, global_step=103265, preemption_count=0, score=36149.880427, test/accuracy=0.691732, test/bleu=29.722390, test/loss=1.421906, test/num_examples=3003, total_duration=60767.247957, train/accuracy=0.659332, train/bleu=32.998181, train/loss=1.637738, validation/accuracy=0.676979, validation/bleu=29.280341, validation/loss=1.510822, validation/num_examples=3000
I0209 21:08:46.118984 140277062780672 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.2978924512863159, loss=1.699617862701416
I0209 21:09:20.969107 140277054387968 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.32027432322502136, loss=1.6707921028137207
I0209 21:09:55.913318 140277062780672 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.31225770711898804, loss=1.6079552173614502
I0209 21:10:30.884073 140277054387968 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.335212379693985, loss=1.6222971677780151
I0209 21:11:05.885488 140277062780672 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3040007948875427, loss=1.5768920183181763
I0209 21:11:40.873438 140277054387968 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3238406181335449, loss=1.6892918348312378
I0209 21:12:15.836967 140277062780672 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.30260375142097473, loss=1.6295642852783203
I0209 21:12:50.877425 140277054387968 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3206372559070587, loss=1.6714167594909668
I0209 21:13:25.883926 140277062780672 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.30995309352874756, loss=1.559217095375061
I0209 21:14:00.847557 140277054387968 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3151800334453583, loss=1.647098422050476
I0209 21:14:35.825292 140277062780672 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.30827319622039795, loss=1.6181038618087769
I0209 21:15:10.814204 140277054387968 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.32593581080436707, loss=1.701660394668579
I0209 21:15:45.813912 140277062780672 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3150680959224701, loss=1.6421620845794678
I0209 21:16:20.797514 140277054387968 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3027857542037964, loss=1.5800347328186035
I0209 21:16:55.800078 140277062780672 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.30358612537384033, loss=1.6013202667236328
I0209 21:17:30.780975 140277054387968 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3098702132701874, loss=1.615094780921936
I0209 21:18:05.756707 140277062780672 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.33304890990257263, loss=1.6230270862579346
I0209 21:18:40.759901 140277054387968 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3305651545524597, loss=1.5756360292434692
I0209 21:19:15.741499 140277062780672 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.32557761669158936, loss=1.6393450498580933
I0209 21:19:50.733148 140277054387968 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3111478090286255, loss=1.657175898551941
I0209 21:20:25.735305 140277062780672 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.32613763213157654, loss=1.6382945775985718
I0209 21:21:00.728597 140277054387968 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.33530712127685547, loss=1.581669569015503
I0209 21:21:35.694125 140277062780672 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.32341083884239197, loss=1.626238465309143
I0209 21:22:10.680993 140277054387968 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.32792869210243225, loss=1.5950381755828857
I0209 21:22:33.848577 140446903760704 spec.py:321] Evaluating on the training split.
I0209 21:22:36.862358 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:26:26.922323 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 21:26:29.626908 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:29:12.464705 140446903760704 spec.py:349] Evaluating on the test split.
I0209 21:29:15.192254 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:31:58.366095 140446903760704 submission_runner.py:408] Time since start: 62172.07s, 	Step: 105668, 	{'train/accuracy': 0.6658817529678345, 'train/loss': 1.6051431894302368, 'train/bleu': 32.95288097209237, 'validation/accuracy': 0.6797559857368469, 'validation/loss': 1.4939372539520264, 'validation/bleu': 29.482852243566366, 'validation/num_examples': 3000, 'test/accuracy': 0.6930451393127441, 'test/loss': 1.4107818603515625, 'test/bleu': 29.341508113582247, 'test/num_examples': 3003, 'score': 36990.06665062904, 'total_duration': 62172.06532788277, 'accumulated_submission_time': 36990.06665062904, 'accumulated_eval_time': 25177.135360479355, 'accumulated_logging_time': 1.5139610767364502}
I0209 21:31:58.401950 140277062780672 logging_writer.py:48] [105668] accumulated_eval_time=25177.135360, accumulated_logging_time=1.513961, accumulated_submission_time=36990.066651, global_step=105668, preemption_count=0, score=36990.066651, test/accuracy=0.693045, test/bleu=29.341508, test/loss=1.410782, test/num_examples=3003, total_duration=62172.065328, train/accuracy=0.665882, train/bleu=32.952881, train/loss=1.605143, validation/accuracy=0.679756, validation/bleu=29.482852, validation/loss=1.493937, validation/num_examples=3000
I0209 21:32:09.905108 140277054387968 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.3065270185470581, loss=1.548519492149353
I0209 21:32:44.764195 140277062780672 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.3269166350364685, loss=1.6238312721252441
I0209 21:33:19.748650 140277054387968 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.34768936038017273, loss=1.6792035102844238
I0209 21:33:54.743774 140277062780672 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.31582874059677124, loss=1.6436434984207153
I0209 21:34:29.760071 140277054387968 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.3151368498802185, loss=1.6031384468078613
I0209 21:35:04.803113 140277062780672 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3271692395210266, loss=1.59995436668396
I0209 21:35:39.823350 140277054387968 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.34206104278564453, loss=1.6344512701034546
I0209 21:36:14.842684 140277062780672 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.33308103680610657, loss=1.5954420566558838
I0209 21:36:49.828339 140277054387968 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.34592899680137634, loss=1.6084309816360474
I0209 21:37:24.831072 140277062780672 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3156144320964813, loss=1.547154188156128
I0209 21:37:59.847455 140277054387968 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.321755588054657, loss=1.5776718854904175
I0209 21:38:34.883241 140277062780672 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.3050858974456787, loss=1.6010937690734863
I0209 21:39:09.879482 140277054387968 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3333386778831482, loss=1.5658786296844482
I0209 21:39:44.865311 140277062780672 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3262169361114502, loss=1.709938406944275
I0209 21:40:19.878429 140277054387968 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.33627206087112427, loss=1.594498872756958
I0209 21:40:54.869968 140277062780672 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3516595959663391, loss=1.6021723747253418
I0209 21:41:29.891378 140277054387968 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.34044376015663147, loss=1.6470106840133667
I0209 21:42:04.922590 140277062780672 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.34301963448524475, loss=1.6594284772872925
I0209 21:42:39.915057 140277054387968 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.31886401772499084, loss=1.5109666585922241
I0209 21:43:14.877476 140277062780672 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.3545683026313782, loss=1.5992939472198486
I0209 21:43:49.846680 140277054387968 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.31418290734291077, loss=1.531779408454895
I0209 21:44:24.844735 140277062780672 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.32101303339004517, loss=1.6178537607192993
I0209 21:44:59.824528 140277054387968 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3303736746311188, loss=1.5351108312606812
I0209 21:45:34.846929 140277062780672 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.31829312443733215, loss=1.6131901741027832
I0209 21:45:58.697846 140446903760704 spec.py:321] Evaluating on the training split.
I0209 21:46:01.707042 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:49:24.508369 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 21:49:27.223603 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:52:07.999559 140446903760704 spec.py:349] Evaluating on the test split.
I0209 21:52:10.708001 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 21:54:42.504033 140446903760704 submission_runner.py:408] Time since start: 63536.20s, 	Step: 108070, 	{'train/accuracy': 0.6727055311203003, 'train/loss': 1.5570759773254395, 'train/bleu': 33.9455520747962, 'validation/accuracy': 0.6823598146438599, 'validation/loss': 1.4784127473831177, 'validation/bleu': 29.72807262910545, 'validation/num_examples': 3000, 'test/accuracy': 0.6957992315292358, 'test/loss': 1.3981857299804688, 'test/bleu': 29.608836478521752, 'test/num_examples': 3003, 'score': 37830.27451658249, 'total_duration': 63536.203300237656, 'accumulated_submission_time': 37830.27451658249, 'accumulated_eval_time': 25700.941499471664, 'accumulated_logging_time': 1.5608172416687012}
I0209 21:54:42.534412 140277054387968 logging_writer.py:48] [108070] accumulated_eval_time=25700.941499, accumulated_logging_time=1.560817, accumulated_submission_time=37830.274517, global_step=108070, preemption_count=0, score=37830.274517, test/accuracy=0.695799, test/bleu=29.608836, test/loss=1.398186, test/num_examples=3003, total_duration=63536.203300, train/accuracy=0.672706, train/bleu=33.945552, train/loss=1.557076, validation/accuracy=0.682360, validation/bleu=29.728073, validation/loss=1.478413, validation/num_examples=3000
I0209 21:54:53.345333 140277062780672 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.32618334889411926, loss=1.5775238275527954
I0209 21:55:28.178393 140277054387968 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.34314650297164917, loss=1.6069356203079224
I0209 21:56:03.115061 140277062780672 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3451407849788666, loss=1.6225478649139404
I0209 21:56:38.090238 140277054387968 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3363344371318817, loss=1.6022827625274658
I0209 21:57:13.200563 140277062780672 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.32181090116500854, loss=1.5211576223373413
I0209 21:57:48.237205 140277054387968 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.33433085680007935, loss=1.5979517698287964
I0209 21:58:23.276923 140277062780672 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3036491274833679, loss=1.5401694774627686
I0209 21:58:58.258142 140277054387968 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.32454606890678406, loss=1.5083173513412476
I0209 21:59:33.237210 140277062780672 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.34840089082717896, loss=1.5756011009216309
I0209 22:00:08.246322 140277054387968 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.30669108033180237, loss=1.5803825855255127
I0209 22:00:43.244291 140277062780672 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3081912696361542, loss=1.553341031074524
I0209 22:01:18.269960 140277054387968 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.33164626359939575, loss=1.573461890220642
I0209 22:01:53.331018 140277062780672 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3153226375579834, loss=1.5616967678070068
I0209 22:02:28.402714 140277054387968 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3353146016597748, loss=1.624847173690796
I0209 22:03:03.431708 140277062780672 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.3159409165382385, loss=1.6450986862182617
I0209 22:03:38.449242 140277054387968 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.33724942803382874, loss=1.5160335302352905
I0209 22:04:13.444522 140277062780672 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.328460693359375, loss=1.5861239433288574
I0209 22:04:48.459546 140277054387968 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.33561715483665466, loss=1.5590273141860962
I0209 22:05:23.475596 140277062780672 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.33299607038497925, loss=1.5877794027328491
I0209 22:05:58.472535 140277054387968 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.30472490191459656, loss=1.5666303634643555
I0209 22:06:33.467999 140277062780672 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.32218194007873535, loss=1.5520046949386597
I0209 22:07:08.473496 140277054387968 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.32523345947265625, loss=1.6115539073944092
I0209 22:07:43.461730 140277062780672 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.3625060021877289, loss=1.4932005405426025
I0209 22:08:18.454023 140277054387968 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3422586917877197, loss=1.5565177202224731
I0209 22:08:42.673676 140446903760704 spec.py:321] Evaluating on the training split.
I0209 22:08:45.678240 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 22:12:12.809764 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 22:12:15.505615 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 22:15:00.973289 140446903760704 spec.py:349] Evaluating on the test split.
I0209 22:15:03.700544 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 22:17:56.842444 140446903760704 submission_runner.py:408] Time since start: 64930.54s, 	Step: 110471, 	{'train/accuracy': 0.6731343865394592, 'train/loss': 1.5567851066589355, 'train/bleu': 33.786392688007275, 'validation/accuracy': 0.6843188405036926, 'validation/loss': 1.4721633195877075, 'validation/bleu': 30.15936458541903, 'validation/num_examples': 3000, 'test/accuracy': 0.6989832520484924, 'test/loss': 1.3790864944458008, 'test/bleu': 29.739070535005855, 'test/num_examples': 3003, 'score': 38670.3262925148, 'total_duration': 64930.54170131683, 'accumulated_submission_time': 38670.3262925148, 'accumulated_eval_time': 26255.110206842422, 'accumulated_logging_time': 1.6010394096374512}
I0209 22:17:56.872036 140277062780672 logging_writer.py:48] [110471] accumulated_eval_time=26255.110207, accumulated_logging_time=1.601039, accumulated_submission_time=38670.326293, global_step=110471, preemption_count=0, score=38670.326293, test/accuracy=0.698983, test/bleu=29.739071, test/loss=1.379086, test/num_examples=3003, total_duration=64930.541701, train/accuracy=0.673134, train/bleu=33.786393, train/loss=1.556785, validation/accuracy=0.684319, validation/bleu=30.159365, validation/loss=1.472163, validation/num_examples=3000
I0209 22:18:07.350908 140277054387968 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3363674283027649, loss=1.6544336080551147
I0209 22:18:42.176237 140277062780672 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.338517427444458, loss=1.5395513772964478
I0209 22:19:17.123074 140277054387968 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.33003684878349304, loss=1.6255606412887573
I0209 22:19:52.091358 140277062780672 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.3397344946861267, loss=1.5743108987808228
I0209 22:20:27.069679 140277054387968 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.30807071924209595, loss=1.6013904809951782
I0209 22:21:02.054496 140277062780672 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.347936749458313, loss=1.6343281269073486
I0209 22:21:37.101983 140277054387968 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3357410132884979, loss=1.4958745241165161
I0209 22:22:12.087091 140277062780672 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.33984383940696716, loss=1.466124415397644
I0209 22:22:47.123062 140277054387968 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.35361114144325256, loss=1.5998985767364502
I0209 22:23:22.188477 140277062780672 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.33004531264305115, loss=1.5472544431686401
I0209 22:23:57.222319 140277054387968 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3274000287055969, loss=1.5841224193572998
I0209 22:24:32.249120 140277062780672 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3521171808242798, loss=1.5396195650100708
I0209 22:25:07.228724 140277054387968 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3292301297187805, loss=1.5008795261383057
I0209 22:25:42.182782 140277062780672 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.32306480407714844, loss=1.5704493522644043
I0209 22:26:17.184318 140277054387968 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3505411148071289, loss=1.5224076509475708
I0209 22:26:52.184266 140277062780672 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3190213441848755, loss=1.5186002254486084
I0209 22:27:27.138314 140277054387968 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.3299136757850647, loss=1.5460693836212158
I0209 22:28:02.119790 140277062780672 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.3266628384590149, loss=1.5331357717514038
I0209 22:28:37.103905 140277054387968 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.3189638555049896, loss=1.5746827125549316
I0209 22:29:12.100579 140277062780672 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3365465998649597, loss=1.5365500450134277
I0209 22:29:47.110848 140277054387968 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.34717369079589844, loss=1.6270171403884888
I0209 22:30:22.107308 140277062780672 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.38180413842201233, loss=1.6241050958633423
I0209 22:30:57.105500 140277054387968 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.32875120639801025, loss=1.5390843152999878
I0209 22:31:32.092278 140277062780672 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.327627956867218, loss=1.479617714881897
I0209 22:31:56.999283 140446903760704 spec.py:321] Evaluating on the training split.
I0209 22:32:00.011785 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 22:35:32.507149 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 22:35:35.242863 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 22:38:30.519252 140446903760704 spec.py:349] Evaluating on the test split.
I0209 22:38:33.228115 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 22:41:29.462095 140446903760704 submission_runner.py:408] Time since start: 66343.16s, 	Step: 112873, 	{'train/accuracy': 0.6936594247817993, 'train/loss': 1.431340217590332, 'train/bleu': 35.508655574660985, 'validation/accuracy': 0.685050368309021, 'validation/loss': 1.4566941261291504, 'validation/bleu': 30.06185313900829, 'validation/num_examples': 3000, 'test/accuracy': 0.6995874643325806, 'test/loss': 1.3684998750686646, 'test/bleu': 30.10767739633273, 'test/num_examples': 3003, 'score': 39510.36837506294, 'total_duration': 66343.16135883331, 'accumulated_submission_time': 39510.36837506294, 'accumulated_eval_time': 26827.57296895981, 'accumulated_logging_time': 1.6411964893341064}
I0209 22:41:29.493738 140277054387968 logging_writer.py:48] [112873] accumulated_eval_time=26827.572969, accumulated_logging_time=1.641196, accumulated_submission_time=39510.368375, global_step=112873, preemption_count=0, score=39510.368375, test/accuracy=0.699587, test/bleu=30.107677, test/loss=1.368500, test/num_examples=3003, total_duration=66343.161359, train/accuracy=0.693659, train/bleu=35.508656, train/loss=1.431340, validation/accuracy=0.685050, validation/bleu=30.061853, validation/loss=1.456694, validation/num_examples=3000
I0209 22:41:39.256697 140277062780672 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3469330668449402, loss=1.552841305732727
I0209 22:42:14.097013 140277054387968 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.33403480052948, loss=1.5150753259658813
I0209 22:42:49.043721 140277062780672 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.36750084161758423, loss=1.5845026969909668
I0209 22:43:24.005746 140277054387968 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.33996161818504333, loss=1.5054011344909668
I0209 22:43:58.984013 140277062780672 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.35834285616874695, loss=1.5506880283355713
I0209 22:44:33.928356 140277054387968 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3689207136631012, loss=1.5051114559173584
I0209 22:45:08.902834 140277062780672 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3441973924636841, loss=1.596830129623413
I0209 22:45:43.873846 140277054387968 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.34786489605903625, loss=1.531207799911499
I0209 22:46:18.970297 140277062780672 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.33696722984313965, loss=1.532618522644043
I0209 22:46:53.981542 140277054387968 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3265897333621979, loss=1.573637843132019
I0209 22:47:28.948138 140277062780672 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.3491305410861969, loss=1.6148697137832642
I0209 22:48:03.949569 140277054387968 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3382960259914398, loss=1.4608726501464844
I0209 22:48:38.945943 140277062780672 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3376118540763855, loss=1.5739189386367798
I0209 22:49:13.917885 140277054387968 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3437402546405792, loss=1.5340301990509033
I0209 22:49:48.897749 140277062780672 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3648080825805664, loss=1.5663394927978516
I0209 22:50:23.892478 140277054387968 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3617439270019531, loss=1.5067750215530396
I0209 22:50:58.859526 140277062780672 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.3494826555252075, loss=1.4958930015563965
I0209 22:51:33.828223 140277054387968 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.35234904289245605, loss=1.5196408033370972
I0209 22:52:08.873030 140277062780672 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.34132054448127747, loss=1.5382332801818848
I0209 22:52:43.890812 140277054387968 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3533896803855896, loss=1.5372048616409302
I0209 22:53:18.905255 140277062780672 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.31909283995628357, loss=1.4797999858856201
I0209 22:53:53.920332 140277054387968 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.32888373732566833, loss=1.4983588457107544
I0209 22:54:28.914708 140277062780672 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.32576268911361694, loss=1.4794297218322754
I0209 22:55:03.895555 140277054387968 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.348201185464859, loss=1.4719651937484741
I0209 22:55:29.508972 140446903760704 spec.py:321] Evaluating on the training split.
I0209 22:55:32.512998 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 22:59:16.294187 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 22:59:19.020471 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:02:38.647495 140446903760704 spec.py:349] Evaluating on the test split.
I0209 23:02:41.362735 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:06:07.887246 140446903760704 submission_runner.py:408] Time since start: 67821.59s, 	Step: 115275, 	{'train/accuracy': 0.6838340163230896, 'train/loss': 1.4911880493164062, 'train/bleu': 34.26181205626829, 'validation/accuracy': 0.6880261898040771, 'validation/loss': 1.4454679489135742, 'validation/bleu': 30.377481779085375, 'validation/num_examples': 3000, 'test/accuracy': 0.7013421654701233, 'test/loss': 1.357330322265625, 'test/bleu': 30.101626542932724, 'test/num_examples': 3003, 'score': 40350.29947733879, 'total_duration': 67821.58649492264, 'accumulated_submission_time': 40350.29947733879, 'accumulated_eval_time': 27465.951172590256, 'accumulated_logging_time': 1.6827702522277832}
I0209 23:06:07.919493 140277062780672 logging_writer.py:48] [115275] accumulated_eval_time=27465.951173, accumulated_logging_time=1.682770, accumulated_submission_time=40350.299477, global_step=115275, preemption_count=0, score=40350.299477, test/accuracy=0.701342, test/bleu=30.101627, test/loss=1.357330, test/num_examples=3003, total_duration=67821.586495, train/accuracy=0.683834, train/bleu=34.261812, train/loss=1.491188, validation/accuracy=0.688026, validation/bleu=30.377482, validation/loss=1.445468, validation/num_examples=3000
I0209 23:06:16.994975 140277054387968 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.36054688692092896, loss=1.5892084836959839
I0209 23:06:51.875734 140277062780672 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.35750651359558105, loss=1.53841233253479
I0209 23:07:26.847653 140277054387968 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.36271968483924866, loss=1.5288481712341309
I0209 23:08:01.831188 140277062780672 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.34377315640449524, loss=1.533497929573059
I0209 23:08:36.812767 140277054387968 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.34375429153442383, loss=1.5035263299942017
I0209 23:09:11.831728 140277062780672 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3342500329017639, loss=1.4872158765792847
I0209 23:09:46.804708 140277054387968 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.3602820336818695, loss=1.5874203443527222
I0209 23:10:21.787207 140277062780672 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.33127471804618835, loss=1.43991219997406
I0209 23:10:56.789809 140277054387968 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.3559902012348175, loss=1.4697896242141724
I0209 23:11:31.781795 140277062780672 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.3496144115924835, loss=1.5377066135406494
I0209 23:12:06.839963 140277054387968 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3535098433494568, loss=1.511470913887024
I0209 23:12:41.862319 140277062780672 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.35559290647506714, loss=1.4497976303100586
I0209 23:13:16.846391 140277054387968 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3379215598106384, loss=1.5266180038452148
I0209 23:13:51.858405 140277062780672 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.36747288703918457, loss=1.4203097820281982
I0209 23:14:26.855811 140277054387968 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.3540738821029663, loss=1.5196239948272705
I0209 23:15:01.938543 140277062780672 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.34768927097320557, loss=1.5509769916534424
I0209 23:15:36.913802 140277054387968 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.3548789322376251, loss=1.564833641052246
I0209 23:16:11.903542 140277062780672 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.38090434670448303, loss=1.5716793537139893
I0209 23:16:46.909738 140277054387968 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.36891406774520874, loss=1.5432507991790771
I0209 23:17:21.878381 140277062780672 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.3521002531051636, loss=1.5313115119934082
I0209 23:17:56.827861 140277054387968 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.3571682870388031, loss=1.4598544836044312
I0209 23:18:31.847308 140277062780672 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.3644973337650299, loss=1.5784873962402344
I0209 23:19:06.849940 140277054387968 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3450069725513458, loss=1.4421191215515137
I0209 23:19:41.873183 140277062780672 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.34491580724716187, loss=1.4646029472351074
I0209 23:20:08.187330 140446903760704 spec.py:321] Evaluating on the training split.
I0209 23:20:11.201994 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:23:53.340333 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 23:23:56.051048 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:26:39.439051 140446903760704 spec.py:349] Evaluating on the test split.
I0209 23:26:42.150980 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:29:22.904757 140446903760704 submission_runner.py:408] Time since start: 69216.60s, 	Step: 117677, 	{'train/accuracy': 0.6806775331497192, 'train/loss': 1.5084309577941895, 'train/bleu': 34.721579984247796, 'validation/accuracy': 0.6893280744552612, 'validation/loss': 1.4370354413986206, 'validation/bleu': 30.509479063813494, 'validation/num_examples': 3000, 'test/accuracy': 0.7044913172721863, 'test/loss': 1.3434144258499146, 'test/bleu': 30.434828744976627, 'test/num_examples': 3003, 'score': 41190.48154783249, 'total_duration': 69216.60397958755, 'accumulated_submission_time': 41190.48154783249, 'accumulated_eval_time': 28020.668506383896, 'accumulated_logging_time': 1.7252159118652344}
I0209 23:29:22.943608 140277054387968 logging_writer.py:48] [117677] accumulated_eval_time=28020.668506, accumulated_logging_time=1.725216, accumulated_submission_time=41190.481548, global_step=117677, preemption_count=0, score=41190.481548, test/accuracy=0.704491, test/bleu=30.434829, test/loss=1.343414, test/num_examples=3003, total_duration=69216.603980, train/accuracy=0.680678, train/bleu=34.721580, train/loss=1.508431, validation/accuracy=0.689328, validation/bleu=30.509479, validation/loss=1.437035, validation/num_examples=3000
I0209 23:29:31.320025 140277062780672 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.34380725026130676, loss=1.530548095703125
I0209 23:30:06.176116 140277054387968 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3707890510559082, loss=1.5080989599227905
I0209 23:30:41.116141 140277062780672 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.3637067675590515, loss=1.4954962730407715
I0209 23:31:16.101970 140277054387968 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.3739248514175415, loss=1.4251976013183594
I0209 23:31:51.109419 140277062780672 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.3810606598854065, loss=1.583938717842102
I0209 23:32:26.104594 140277054387968 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.37216663360595703, loss=1.5034252405166626
I0209 23:33:01.093062 140277062780672 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3710344731807709, loss=1.5239815711975098
I0209 23:33:36.086479 140277054387968 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.38041865825653076, loss=1.5020700693130493
I0209 23:34:11.088955 140277062780672 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3536909818649292, loss=1.5128388404846191
I0209 23:34:46.133624 140277054387968 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.3576723337173462, loss=1.475014328956604
I0209 23:35:21.130526 140277062780672 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.35473886132240295, loss=1.567735195159912
I0209 23:35:56.088524 140277054387968 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.3709638714790344, loss=1.4908746480941772
I0209 23:36:31.188787 140277062780672 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.3518184721469879, loss=1.4661599397659302
I0209 23:37:06.173016 140277054387968 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3855484127998352, loss=1.5572973489761353
I0209 23:37:41.142484 140277062780672 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3507258892059326, loss=1.5349681377410889
I0209 23:38:16.128059 140277054387968 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3873678147792816, loss=1.5240809917449951
I0209 23:38:51.115307 140277062780672 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.36353111267089844, loss=1.4777880907058716
I0209 23:39:26.075998 140277054387968 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3469041883945465, loss=1.4529262781143188
I0209 23:40:01.071087 140277062780672 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.38514044880867004, loss=1.4328279495239258
I0209 23:40:36.033452 140277054387968 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3532704710960388, loss=1.4589629173278809
I0209 23:41:11.032738 140277062780672 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.35917243361473083, loss=1.472521185874939
I0209 23:41:46.029987 140277054387968 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.37402474880218506, loss=1.4300169944763184
I0209 23:42:21.046893 140277062780672 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.4362012445926666, loss=1.5143660306930542
I0209 23:42:56.062227 140277054387968 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.35509973764419556, loss=1.4748059511184692
I0209 23:43:23.059084 140446903760704 spec.py:321] Evaluating on the training split.
I0209 23:43:26.073685 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:47:18.301005 140446903760704 spec.py:333] Evaluating on the validation split.
I0209 23:47:21.031269 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:50:01.287782 140446903760704 spec.py:349] Evaluating on the test split.
I0209 23:50:04.008102 140446903760704 workload.py:181] Translating evaluation dataset.
I0209 23:52:37.866633 140446903760704 submission_runner.py:408] Time since start: 70611.57s, 	Step: 120079, 	{'train/accuracy': 0.6928339004516602, 'train/loss': 1.4373910427093506, 'train/bleu': 35.37524108993434, 'validation/accuracy': 0.690034806728363, 'validation/loss': 1.4303141832351685, 'validation/bleu': 30.701788225822256, 'validation/num_examples': 3000, 'test/accuracy': 0.7062111496925354, 'test/loss': 1.3397371768951416, 'test/bleu': 30.584510235579657, 'test/num_examples': 3003, 'score': 42030.51136517525, 'total_duration': 70611.56590008736, 'accumulated_submission_time': 42030.51136517525, 'accumulated_eval_time': 28575.47600364685, 'accumulated_logging_time': 1.7752346992492676}
I0209 23:52:37.898589 140277062780672 logging_writer.py:48] [120079] accumulated_eval_time=28575.476004, accumulated_logging_time=1.775235, accumulated_submission_time=42030.511365, global_step=120079, preemption_count=0, score=42030.511365, test/accuracy=0.706211, test/bleu=30.584510, test/loss=1.339737, test/num_examples=3003, total_duration=70611.565900, train/accuracy=0.692834, train/bleu=35.375241, train/loss=1.437391, validation/accuracy=0.690035, validation/bleu=30.701788, validation/loss=1.430314, validation/num_examples=3000
I0209 23:52:45.577995 140277054387968 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.3837393820285797, loss=1.5063146352767944
I0209 23:53:20.414655 140277062780672 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.3644445538520813, loss=1.4973784685134888
I0209 23:53:55.358875 140277054387968 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.37970981001853943, loss=1.5094828605651855
I0209 23:54:30.342546 140277062780672 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.36582544445991516, loss=1.4282910823822021
I0209 23:55:05.333597 140277054387968 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3952961564064026, loss=1.5234522819519043
I0209 23:55:40.306219 140277062780672 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3562609553337097, loss=1.5349056720733643
I0209 23:56:15.298731 140277054387968 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.3767852783203125, loss=1.4360440969467163
I0209 23:56:50.318068 140277062780672 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.3476748764514923, loss=1.4754934310913086
I0209 23:57:25.430888 140277054387968 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.35671892762184143, loss=1.4651333093643188
I0209 23:58:00.389911 140277062780672 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3605005741119385, loss=1.4856503009796143
I0209 23:58:35.372934 140277054387968 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3856930732727051, loss=1.5458928346633911
I0209 23:59:10.365156 140277062780672 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.36955758929252625, loss=1.510465145111084
I0209 23:59:45.398640 140277054387968 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.37301066517829895, loss=1.5028480291366577
I0210 00:00:20.367963 140277062780672 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3659740388393402, loss=1.4631487131118774
I0210 00:00:55.362295 140277054387968 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3681245446205139, loss=1.4460389614105225
I0210 00:01:30.325588 140277062780672 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.37972313165664673, loss=1.4675418138504028
I0210 00:02:05.304445 140277054387968 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.3762223720550537, loss=1.465377926826477
I0210 00:02:40.281422 140277062780672 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.3681108057498932, loss=1.498564600944519
I0210 00:03:15.299579 140277054387968 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3683304786682129, loss=1.523926854133606
I0210 00:03:50.403896 140277062780672 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.39554983377456665, loss=1.4599355459213257
I0210 00:04:25.406111 140277054387968 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.34839826822280884, loss=1.3969125747680664
I0210 00:05:00.365051 140277062780672 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.36155417561531067, loss=1.4912810325622559
I0210 00:05:35.359691 140277054387968 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3725847005844116, loss=1.4216872453689575
I0210 00:06:10.362123 140277062780672 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.37814998626708984, loss=1.4913982152938843
I0210 00:06:38.079416 140446903760704 spec.py:321] Evaluating on the training split.
I0210 00:06:41.092292 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:10:29.469408 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 00:10:32.189809 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:13:10.486435 140446903760704 spec.py:349] Evaluating on the test split.
I0210 00:13:13.200310 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:15:44.050752 140446903760704 submission_runner.py:408] Time since start: 71997.75s, 	Step: 122481, 	{'train/accuracy': 0.688599705696106, 'train/loss': 1.4655152559280396, 'train/bleu': 35.63986574927191, 'validation/accuracy': 0.6912251710891724, 'validation/loss': 1.4246001243591309, 'validation/bleu': 30.711851471746595, 'validation/num_examples': 3000, 'test/accuracy': 0.7073150873184204, 'test/loss': 1.328243374824524, 'test/bleu': 30.721067569448703, 'test/num_examples': 3003, 'score': 42870.60580945015, 'total_duration': 71997.75002217293, 'accumulated_submission_time': 42870.60580945015, 'accumulated_eval_time': 29121.447292804718, 'accumulated_logging_time': 1.8173811435699463}
I0210 00:15:44.083302 140277054387968 logging_writer.py:48] [122481] accumulated_eval_time=29121.447293, accumulated_logging_time=1.817381, accumulated_submission_time=42870.605809, global_step=122481, preemption_count=0, score=42870.605809, test/accuracy=0.707315, test/bleu=30.721068, test/loss=1.328243, test/num_examples=3003, total_duration=71997.750022, train/accuracy=0.688600, train/bleu=35.639866, train/loss=1.465515, validation/accuracy=0.691225, validation/bleu=30.711851, validation/loss=1.424600, validation/num_examples=3000
I0210 00:15:51.060119 140277062780672 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.388284832239151, loss=1.440491795539856
I0210 00:16:25.899927 140277054387968 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.38083183765411377, loss=1.49837064743042
I0210 00:17:00.820879 140277062780672 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.36972615122795105, loss=1.4011666774749756
I0210 00:17:35.821088 140277054387968 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3798266351222992, loss=1.507120132446289
I0210 00:18:10.814349 140277062780672 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.38276445865631104, loss=1.4741804599761963
I0210 00:18:45.850862 140277054387968 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.4122251570224762, loss=1.5103110074996948
I0210 00:19:20.863795 140277062780672 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.3747326731681824, loss=1.4061833620071411
I0210 00:19:55.889715 140277054387968 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.4073046147823334, loss=1.5161488056182861
I0210 00:20:30.878334 140277062780672 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.3859446346759796, loss=1.4275325536727905
I0210 00:21:05.860324 140277054387968 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.37317198514938354, loss=1.4616577625274658
I0210 00:21:40.842672 140277062780672 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.36882340908050537, loss=1.476999282836914
I0210 00:22:15.843608 140277054387968 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.380043089389801, loss=1.4842123985290527
I0210 00:22:50.822084 140277062780672 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3990994095802307, loss=1.5055816173553467
I0210 00:23:25.860064 140277054387968 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3723604083061218, loss=1.392454981803894
I0210 00:24:00.845376 140277062780672 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3651024103164673, loss=1.4561190605163574
I0210 00:24:35.856636 140277054387968 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.38274911046028137, loss=1.4576811790466309
I0210 00:25:10.832484 140277062780672 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.4023693799972534, loss=1.4745062589645386
I0210 00:25:45.843942 140277054387968 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.3902588486671448, loss=1.388070821762085
I0210 00:26:20.814009 140277062780672 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.37832698225975037, loss=1.4939663410186768
I0210 00:26:55.796070 140277054387968 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.36348655819892883, loss=1.4401859045028687
I0210 00:27:30.795422 140277062780672 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.37203115224838257, loss=1.4464596509933472
I0210 00:28:05.762358 140277054387968 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.3720359802246094, loss=1.409786343574524
I0210 00:28:40.723822 140277062780672 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.396828293800354, loss=1.4049756526947021
I0210 00:29:15.727283 140277054387968 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.4005546271800995, loss=1.497969627380371
I0210 00:29:44.134163 140446903760704 spec.py:321] Evaluating on the training split.
I0210 00:29:47.143586 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:33:31.752750 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 00:33:34.472222 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:36:07.372868 140446903760704 spec.py:349] Evaluating on the test split.
I0210 00:36:10.069724 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:38:43.898223 140446903760704 submission_runner.py:408] Time since start: 73377.60s, 	Step: 124883, 	{'train/accuracy': 0.6913896799087524, 'train/loss': 1.4493789672851562, 'train/bleu': 35.22434842341379, 'validation/accuracy': 0.6933205723762512, 'validation/loss': 1.415845274925232, 'validation/bleu': 30.862479457424147, 'validation/num_examples': 3000, 'test/accuracy': 0.7088490128517151, 'test/loss': 1.3179659843444824, 'test/bleu': 30.963558849323057, 'test/num_examples': 3003, 'score': 43710.570806741714, 'total_duration': 73377.59749126434, 'accumulated_submission_time': 43710.570806741714, 'accumulated_eval_time': 29661.211304426193, 'accumulated_logging_time': 1.8602821826934814}
I0210 00:38:43.931231 140277062780672 logging_writer.py:48] [124883] accumulated_eval_time=29661.211304, accumulated_logging_time=1.860282, accumulated_submission_time=43710.570807, global_step=124883, preemption_count=0, score=43710.570807, test/accuracy=0.708849, test/bleu=30.963559, test/loss=1.317966, test/num_examples=3003, total_duration=73377.597491, train/accuracy=0.691390, train/bleu=35.224348, train/loss=1.449379, validation/accuracy=0.693321, validation/bleu=30.862479, validation/loss=1.415845, validation/num_examples=3000
I0210 00:38:50.224035 140277054387968 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.376011461019516, loss=1.4480772018432617
I0210 00:39:25.083224 140277062780672 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3792983889579773, loss=1.4362901449203491
I0210 00:40:00.052379 140277054387968 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.3950238525867462, loss=1.5296812057495117
I0210 00:40:35.036614 140277062780672 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.3844047784805298, loss=1.4205620288848877
I0210 00:41:10.031701 140277054387968 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3859215974807739, loss=1.4395160675048828
I0210 00:41:45.039853 140277062780672 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.36785656213760376, loss=1.5116430521011353
I0210 00:42:20.024168 140277054387968 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.38360434770584106, loss=1.4866678714752197
I0210 00:42:54.987755 140277062780672 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.38097670674324036, loss=1.4433001279830933
I0210 00:43:29.979520 140277054387968 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.3966260254383087, loss=1.4545469284057617
I0210 00:44:04.941855 140277062780672 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.38957762718200684, loss=1.460951328277588
I0210 00:44:39.933168 140277054387968 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.3715624213218689, loss=1.331774115562439
I0210 00:45:14.954783 140277062780672 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.38685107231140137, loss=1.4726909399032593
I0210 00:45:49.935515 140277054387968 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.37633568048477173, loss=1.4602874517440796
I0210 00:46:24.913123 140277062780672 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.39830702543258667, loss=1.3762664794921875
I0210 00:46:59.892635 140277054387968 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.3878073990345001, loss=1.4910914897918701
I0210 00:47:34.928654 140277062780672 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.3871874511241913, loss=1.4553649425506592
I0210 00:48:09.913575 140277054387968 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.3798658549785614, loss=1.3578344583511353
I0210 00:48:44.855288 140277062780672 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.4016883969306946, loss=1.4649819135665894
I0210 00:49:19.850853 140277054387968 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.372944176197052, loss=1.3731642961502075
I0210 00:49:54.828955 140277062780672 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.375978022813797, loss=1.4320799112319946
I0210 00:50:29.809216 140277054387968 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.3903546929359436, loss=1.5029267072677612
I0210 00:51:04.772774 140277062780672 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.3836417496204376, loss=1.392006278038025
I0210 00:51:39.755619 140277054387968 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.39730769395828247, loss=1.4521558284759521
I0210 00:52:14.754300 140277062780672 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.39099380373954773, loss=1.4678800106048584
I0210 00:52:44.193987 140446903760704 spec.py:321] Evaluating on the training split.
I0210 00:52:47.214076 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:56:09.690506 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 00:56:12.402648 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 00:58:50.113387 140446903760704 spec.py:349] Evaluating on the test split.
I0210 00:58:52.825449 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:01:35.331177 140446903760704 submission_runner.py:408] Time since start: 74749.03s, 	Step: 127286, 	{'train/accuracy': 0.6950476765632629, 'train/loss': 1.428101658821106, 'train/bleu': 35.723502797572735, 'validation/accuracy': 0.6939281225204468, 'validation/loss': 1.4126336574554443, 'validation/bleu': 30.916063388296088, 'validation/num_examples': 3000, 'test/accuracy': 0.7101272344589233, 'test/loss': 1.3143149614334106, 'test/bleu': 30.82745926563664, 'test/num_examples': 3003, 'score': 44550.7475271225, 'total_duration': 74749.03038787842, 'accumulated_submission_time': 44550.7475271225, 'accumulated_eval_time': 30192.348385572433, 'accumulated_logging_time': 1.9036564826965332}
I0210 01:01:35.366169 140277054387968 logging_writer.py:48] [127286] accumulated_eval_time=30192.348386, accumulated_logging_time=1.903656, accumulated_submission_time=44550.747527, global_step=127286, preemption_count=0, score=44550.747527, test/accuracy=0.710127, test/bleu=30.827459, test/loss=1.314315, test/num_examples=3003, total_duration=74749.030388, train/accuracy=0.695048, train/bleu=35.723503, train/loss=1.428102, validation/accuracy=0.693928, validation/bleu=30.916063, validation/loss=1.412634, validation/num_examples=3000
I0210 01:01:40.603984 140277062780672 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.3762035369873047, loss=1.3845146894454956
I0210 01:02:15.409845 140277054387968 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.3928627669811249, loss=1.3962727785110474
I0210 01:02:50.349972 140277062780672 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.38700321316719055, loss=1.4644266366958618
I0210 01:03:25.340487 140277054387968 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.38437554240226746, loss=1.4826384782791138
I0210 01:04:00.339069 140277062780672 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.3802588880062103, loss=1.5061111450195312
I0210 01:04:35.324654 140277054387968 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.39542171359062195, loss=1.423851728439331
I0210 01:05:10.314025 140277062780672 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.38843441009521484, loss=1.4744372367858887
I0210 01:05:45.271479 140277054387968 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.39134645462036133, loss=1.4168987274169922
I0210 01:06:20.245423 140277062780672 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.389795184135437, loss=1.5349568128585815
I0210 01:06:55.217445 140277054387968 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.3783018887042999, loss=1.4555189609527588
I0210 01:07:30.236459 140277062780672 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.39719778299331665, loss=1.4701385498046875
I0210 01:08:05.259632 140277054387968 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.37092259526252747, loss=1.3957841396331787
I0210 01:08:40.243417 140277062780672 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.3710728883743286, loss=1.382650375366211
I0210 01:09:15.250527 140277054387968 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.37572726607322693, loss=1.4284244775772095
I0210 01:09:50.207720 140277062780672 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.3741210103034973, loss=1.4375675916671753
I0210 01:10:25.235448 140277054387968 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.38427963852882385, loss=1.4559214115142822
I0210 01:11:00.289043 140277062780672 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.38070163130760193, loss=1.436959981918335
I0210 01:11:35.323581 140277054387968 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.39940735697746277, loss=1.425309419631958
I0210 01:12:10.369339 140277062780672 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.38372740149497986, loss=1.4977099895477295
I0210 01:12:45.469248 140277054387968 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.3909977674484253, loss=1.4732455015182495
I0210 01:13:20.465961 140277062780672 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.3848905563354492, loss=1.5004334449768066
I0210 01:13:55.447506 140277054387968 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.3921627402305603, loss=1.4849036931991577
I0210 01:14:30.430948 140277062780672 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.37829965353012085, loss=1.4040459394454956
I0210 01:15:05.408085 140277054387968 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3905808627605438, loss=1.4810081720352173
I0210 01:15:35.534322 140446903760704 spec.py:321] Evaluating on the training split.
I0210 01:15:38.543177 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:19:01.357127 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 01:19:04.082556 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:21:35.027512 140446903760704 spec.py:349] Evaluating on the test split.
I0210 01:21:37.746272 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:24:02.829957 140446903760704 submission_runner.py:408] Time since start: 76096.53s, 	Step: 129688, 	{'train/accuracy': 0.6956238150596619, 'train/loss': 1.4236267805099487, 'train/bleu': 35.90949567414748, 'validation/accuracy': 0.6945853233337402, 'validation/loss': 1.410330057144165, 'validation/bleu': 30.926567537057483, 'validation/num_examples': 3000, 'test/accuracy': 0.7101272344589233, 'test/loss': 1.311076045036316, 'test/bleu': 31.011806972890586, 'test/num_examples': 3003, 'score': 45390.827068567276, 'total_duration': 76096.52921843529, 'accumulated_submission_time': 45390.827068567276, 'accumulated_eval_time': 30699.643965244293, 'accumulated_logging_time': 1.9506874084472656}
I0210 01:24:02.863689 140277062780672 logging_writer.py:48] [129688] accumulated_eval_time=30699.643965, accumulated_logging_time=1.950687, accumulated_submission_time=45390.827069, global_step=129688, preemption_count=0, score=45390.827069, test/accuracy=0.710127, test/bleu=31.011807, test/loss=1.311076, test/num_examples=3003, total_duration=76096.529218, train/accuracy=0.695624, train/bleu=35.909496, train/loss=1.423627, validation/accuracy=0.694585, validation/bleu=30.926568, validation/loss=1.410330, validation/num_examples=3000
I0210 01:24:07.402803 140277054387968 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.4081833064556122, loss=1.5038632154464722
I0210 01:24:42.241070 140277062780672 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.37433671951293945, loss=1.4281812906265259
I0210 01:25:17.237462 140277054387968 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.3816336393356323, loss=1.4103823900222778
I0210 01:25:52.227339 140277062780672 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.3733220100402832, loss=1.4323670864105225
I0210 01:26:27.206260 140277054387968 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.39897310733795166, loss=1.4841477870941162
I0210 01:27:02.173138 140277062780672 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.37698251008987427, loss=1.4472798109054565
I0210 01:27:37.207140 140277054387968 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.384355753660202, loss=1.411661982536316
I0210 01:28:12.193296 140277062780672 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.3801412284374237, loss=1.3605084419250488
I0210 01:28:47.175215 140277054387968 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.38099205493927, loss=1.4235259294509888
I0210 01:29:22.175977 140277062780672 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.37431347370147705, loss=1.3596224784851074
I0210 01:29:57.165133 140277054387968 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.3725990056991577, loss=1.4439890384674072
I0210 01:30:32.158319 140277062780672 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.3899613916873932, loss=1.4748902320861816
I0210 01:31:07.159062 140277054387968 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.37759578227996826, loss=1.3402104377746582
I0210 01:31:42.141551 140277062780672 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.3811268210411072, loss=1.3510737419128418
I0210 01:32:17.153165 140277054387968 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.36861124634742737, loss=1.3679442405700684
I0210 01:32:52.152214 140277062780672 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.396797239780426, loss=1.427693247795105
I0210 01:33:27.284826 140277054387968 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.3629787266254425, loss=1.4330238103866577
I0210 01:34:02.360106 140277062780672 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.3620130121707916, loss=1.4562854766845703
I0210 01:34:37.359422 140277054387968 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.38601380586624146, loss=1.4535447359085083
I0210 01:35:12.379182 140277062780672 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.37421801686286926, loss=1.4078630208969116
I0210 01:35:47.376952 140277054387968 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.38181033730506897, loss=1.4310224056243896
I0210 01:36:22.375756 140277062780672 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.4006882309913635, loss=1.4758563041687012
I0210 01:36:57.354490 140277054387968 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.3771611452102661, loss=1.3958570957183838
I0210 01:37:32.338087 140277062780672 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.37877407670021057, loss=1.3974261283874512
I0210 01:38:02.836680 140446903760704 spec.py:321] Evaluating on the training split.
I0210 01:38:05.843186 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:41:33.722448 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 01:41:36.463564 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:44:09.066154 140446903760704 spec.py:349] Evaluating on the test split.
I0210 01:44:11.805533 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:46:48.979169 140446903760704 submission_runner.py:408] Time since start: 77462.68s, 	Step: 132089, 	{'train/accuracy': 0.7016637921333313, 'train/loss': 1.386156439781189, 'train/bleu': 35.79594700241151, 'validation/accuracy': 0.694386899471283, 'validation/loss': 1.4098578691482544, 'validation/bleu': 30.910722132760736, 'validation/num_examples': 3000, 'test/accuracy': 0.7106966376304626, 'test/loss': 1.3105497360229492, 'test/bleu': 30.939606512181268, 'test/num_examples': 3003, 'score': 46230.71160531044, 'total_duration': 77462.6783709526, 'accumulated_submission_time': 46230.71160531044, 'accumulated_eval_time': 31225.786337852478, 'accumulated_logging_time': 1.9961497783660889}
I0210 01:46:49.020141 140277054387968 logging_writer.py:48] [132089] accumulated_eval_time=31225.786338, accumulated_logging_time=1.996150, accumulated_submission_time=46230.711605, global_step=132089, preemption_count=0, score=46230.711605, test/accuracy=0.710697, test/bleu=30.939607, test/loss=1.310550, test/num_examples=3003, total_duration=77462.678371, train/accuracy=0.701664, train/bleu=35.795947, train/loss=1.386156, validation/accuracy=0.694387, validation/bleu=30.910722, validation/loss=1.409858, validation/num_examples=3000
I0210 01:46:53.218518 140277062780672 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.38368579745292664, loss=1.45387601852417
I0210 01:47:28.057995 140277054387968 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.3882366716861725, loss=1.4553662538528442
I0210 01:48:03.102760 140277062780672 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.3826068937778473, loss=1.4315059185028076
I0210 01:48:38.054820 140277054387968 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.40224283933639526, loss=1.4775723218917847
I0210 01:49:13.020552 140277062780672 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.38483288884162903, loss=1.3715095520019531
I0210 01:49:47.993191 140277054387968 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.3964015543460846, loss=1.4661884307861328
I0210 01:50:22.994021 140277062780672 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.3989132344722748, loss=1.488682746887207
I0210 01:50:57.970334 140277054387968 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.38075560331344604, loss=1.3801243305206299
I0210 01:51:32.983800 140277062780672 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.3793222904205322, loss=1.382179617881775
I0210 01:52:07.968015 140277054387968 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.3718094527721405, loss=1.4235581159591675
I0210 01:52:42.996066 140277062780672 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.37284404039382935, loss=1.3943084478378296
I0210 01:53:18.002197 140277054387968 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.39485296607017517, loss=1.3437575101852417
I0210 01:53:53.040473 140277062780672 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.3830239772796631, loss=1.5054261684417725
I0210 01:54:03.965674 140446903760704 spec.py:321] Evaluating on the training split.
I0210 01:54:06.982429 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 01:57:36.956037 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 01:57:39.666342 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:00:10.290841 140446903760704 spec.py:349] Evaluating on the test split.
I0210 02:00:12.995192 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:02:42.097911 140446903760704 submission_runner.py:408] Time since start: 78415.80s, 	Step: 133333, 	{'train/accuracy': 0.6967169642448425, 'train/loss': 1.42378568649292, 'train/bleu': 35.835157411181875, 'validation/accuracy': 0.69430011510849, 'validation/loss': 1.410101294517517, 'validation/bleu': 30.869154358846924, 'validation/num_examples': 3000, 'test/accuracy': 0.7108826041221619, 'test/loss': 1.3107471466064453, 'test/bleu': 30.902745284398407, 'test/num_examples': 3003, 'score': 46665.6048541069, 'total_duration': 78415.79715752602, 'accumulated_submission_time': 46665.6048541069, 'accumulated_eval_time': 31743.91850733757, 'accumulated_logging_time': 2.04841947555542}
I0210 02:02:42.131988 140277054387968 logging_writer.py:48] [133333] accumulated_eval_time=31743.918507, accumulated_logging_time=2.048419, accumulated_submission_time=46665.604854, global_step=133333, preemption_count=0, score=46665.604854, test/accuracy=0.710883, test/bleu=30.902745, test/loss=1.310747, test/num_examples=3003, total_duration=78415.797158, train/accuracy=0.696717, train/bleu=35.835157, train/loss=1.423786, validation/accuracy=0.694300, validation/bleu=30.869154, validation/loss=1.410101, validation/num_examples=3000
I0210 02:02:42.165755 140277062780672 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46665.604854
I0210 02:02:43.638770 140446903760704 checkpoints.py:490] Saving checkpoint at step: 133333
I0210 02:02:47.622693 140446903760704 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_4/checkpoint_133333
I0210 02:02:47.627478 140446903760704 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_4/checkpoint_133333.
I0210 02:02:47.705445 140446903760704 submission_runner.py:583] Tuning trial 4/5
I0210 02:02:47.705607 140446903760704 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0210 02:02:47.712600 140446903760704 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0004915465833619237, 'train/loss': 11.190376281738281, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.639936685562134, 'total_duration': 866.306262254715, 'accumulated_submission_time': 27.639936685562134, 'accumulated_eval_time': 838.6662838459015, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2401, {'train/accuracy': 0.5356904864311218, 'train/loss': 2.585400342941284, 'train/bleu': 23.428443626878675, 'validation/accuracy': 0.5370299220085144, 'validation/loss': 2.548142910003662, 'validation/bleu': 19.524479217978104, 'validation/num_examples': 3000, 'test/accuracy': 0.536575436592102, 'test/loss': 2.5754520893096924, 'test/bleu': 18.380749662236845, 'test/num_examples': 3003, 'score': 868.045458316803, 'total_duration': 2163.7909049987793, 'accumulated_submission_time': 868.045458316803, 'accumulated_eval_time': 1295.6509058475494, 'accumulated_logging_time': 0.018857955932617188, 'global_step': 2401, 'preemption_count': 0}), (4803, {'train/accuracy': 0.5805889964103699, 'train/loss': 2.216299295425415, 'train/bleu': 27.33088719134872, 'validation/accuracy': 0.5971283316612244, 'validation/loss': 2.0716443061828613, 'validation/bleu': 23.725006938623974, 'validation/num_examples': 3000, 'test/accuracy': 0.6001161932945251, 'test/loss': 2.041588306427002, 'test/bleu': 22.17443894805829, 'test/num_examples': 3003, 'score': 1708.1538841724396, 'total_duration': 3483.080261707306, 'accumulated_submission_time': 1708.1538841724396, 'accumulated_eval_time': 1774.7314743995667, 'accumulated_logging_time': 0.04454636573791504, 'global_step': 4803, 'preemption_count': 0}), (7205, {'train/accuracy': 0.5939295291900635, 'train/loss': 2.099783182144165, 'train/bleu': 27.582765593774532, 'validation/accuracy': 0.6013316512107849, 'validation/loss': 2.019782781600952, 'validation/bleu': 24.02121224259647, 'validation/num_examples': 3000, 'test/accuracy': 0.6096101403236389, 'test/loss': 1.9771912097930908, 'test/bleu': 23.194267112786342, 'test/num_examples': 3003, 'score': 2548.2274305820465, 'total_duration': 4818.948817253113, 'accumulated_submission_time': 2548.2274305820465, 'accumulated_eval_time': 2270.4233243465424, 'accumulated_logging_time': 0.07170796394348145, 'global_step': 7205, 'preemption_count': 0}), (9607, {'train/accuracy': 0.5930240750312805, 'train/loss': 2.122828960418701, 'train/bleu': 28.102238158911632, 'validation/accuracy': 0.610048234462738, 'validation/loss': 1.9808986186981201, 'validation/bleu': 24.52583438503706, 'validation/num_examples': 3000, 'test/accuracy': 0.6173610091209412, 'test/loss': 1.931437611579895, 'test/bleu': 23.49182647289432, 'test/num_examples': 3003, 'score': 3388.435542821884, 'total_duration': 6153.368245840073, 'accumulated_submission_time': 3388.435542821884, 'accumulated_eval_time': 2764.5327610969543, 'accumulated_logging_time': 0.09765505790710449, 'global_step': 9607, 'preemption_count': 0}), (12009, {'train/accuracy': 0.5922099947929382, 'train/loss': 2.1225879192352295, 'train/bleu': 28.130188839500367, 'validation/accuracy': 0.6139291524887085, 'validation/loss': 1.9412788152694702, 'validation/bleu': 25.084190063223115, 'validation/num_examples': 3000, 'test/accuracy': 0.6177793145179749, 'test/loss': 1.9158536195755005, 'test/bleu': 23.448882076309648, 'test/num_examples': 3003, 'score': 4228.665568351746, 'total_duration': 7597.773768186569, 'accumulated_submission_time': 4228.665568351746, 'accumulated_eval_time': 3368.605906009674, 'accumulated_logging_time': 0.12425565719604492, 'global_step': 12009, 'preemption_count': 0}), (14411, {'train/accuracy': 0.5989102721214294, 'train/loss': 2.0769054889678955, 'train/bleu': 27.914914125015642, 'validation/accuracy': 0.61870276927948, 'validation/loss': 1.9237356185913086, 'validation/bleu': 25.035330958622882, 'validation/num_examples': 3000, 'test/accuracy': 0.6252977848052979, 'test/loss': 1.8740822076797485, 'test/bleu': 23.781386907356108, 'test/num_examples': 3003, 'score': 5068.8694977760315, 'total_duration': 9261.455813646317, 'accumulated_submission_time': 5068.8694977760315, 'accumulated_eval_time': 4191.980131864548, 'accumulated_logging_time': 0.1532905101776123, 'global_step': 14411, 'preemption_count': 0}), (16811, {'train/accuracy': 0.5971294641494751, 'train/loss': 2.0828609466552734, 'train/bleu': 28.010472281108505, 'validation/accuracy': 0.6194343566894531, 'validation/loss': 1.906266689300537, 'validation/bleu': 25.16562009613152, 'validation/num_examples': 3000, 'test/accuracy': 0.6220208406448364, 'test/loss': 1.8677488565444946, 'test/bleu': 23.640765822774625, 'test/num_examples': 3003, 'score': 5908.888803958893, 'total_duration': 10648.903832674026, 'accumulated_submission_time': 5908.888803958893, 'accumulated_eval_time': 4739.305437326431, 'accumulated_logging_time': 0.18077826499938965, 'global_step': 16811, 'preemption_count': 0}), (19212, {'train/accuracy': 0.6081152558326721, 'train/loss': 1.9887399673461914, 'train/bleu': 29.55355088171276, 'validation/accuracy': 0.6222489476203918, 'validation/loss': 1.901801586151123, 'validation/bleu': 25.599329361585855, 'validation/num_examples': 3000, 'test/accuracy': 0.6279588937759399, 'test/loss': 1.8503223657608032, 'test/bleu': 24.208979117206603, 'test/num_examples': 3003, 'score': 6748.833927154541, 'total_duration': 12011.384460687637, 'accumulated_submission_time': 6748.833927154541, 'accumulated_eval_time': 5261.734477043152, 'accumulated_logging_time': 0.21037697792053223, 'global_step': 19212, 'preemption_count': 0}), (21613, {'train/accuracy': 0.6023226380348206, 'train/loss': 2.051201105117798, 'train/bleu': 28.267704243990025, 'validation/accuracy': 0.6232532858848572, 'validation/loss': 1.8913344144821167, 'validation/bleu': 25.35422343622899, 'validation/num_examples': 3000, 'test/accuracy': 0.6319214701652527, 'test/loss': 1.8304063081741333, 'test/bleu': 25.061395584676017, 'test/num_examples': 3003, 'score': 7588.848192691803, 'total_duration': 13354.67732167244, 'accumulated_submission_time': 7588.848192691803, 'accumulated_eval_time': 5764.907237768173, 'accumulated_logging_time': 0.24018168449401855, 'global_step': 21613, 'preemption_count': 0}), (24014, {'train/accuracy': 0.601838231086731, 'train/loss': 2.06695556640625, 'train/bleu': 28.35724378331637, 'validation/accuracy': 0.622211754322052, 'validation/loss': 1.8881958723068237, 'validation/bleu': 25.447011922234104, 'validation/num_examples': 3000, 'test/accuracy': 0.6272500157356262, 'test/loss': 1.8352961540222168, 'test/bleu': 24.594230099449156, 'test/num_examples': 3003, 'score': 8428.94619846344, 'total_duration': 14670.757593154907, 'accumulated_submission_time': 8428.94619846344, 'accumulated_eval_time': 6240.782923460007, 'accumulated_logging_time': 0.2702040672302246, 'global_step': 24014, 'preemption_count': 0}), (26415, {'train/accuracy': 0.6068945527076721, 'train/loss': 2.0167477130889893, 'train/bleu': 29.215509208704106, 'validation/accuracy': 0.6261794567108154, 'validation/loss': 1.86761474609375, 'validation/bleu': 26.067529651612723, 'validation/num_examples': 3000, 'test/accuracy': 0.6306548118591309, 'test/loss': 1.823778748512268, 'test/bleu': 24.450720016161004, 'test/num_examples': 3003, 'score': 9269.041829109192, 'total_duration': 15998.954328775406, 'accumulated_submission_time': 9269.041829109192, 'accumulated_eval_time': 6728.776242017746, 'accumulated_logging_time': 0.2997102737426758, 'global_step': 26415, 'preemption_count': 0}), (28816, {'train/accuracy': 0.6076061129570007, 'train/loss': 2.0170085430145264, 'train/bleu': 28.619133118767294, 'validation/accuracy': 0.6236252188682556, 'validation/loss': 1.8711217641830444, 'validation/bleu': 25.540494610045837, 'validation/num_examples': 3000, 'test/accuracy': 0.6310731768608093, 'test/loss': 1.814279556274414, 'test/bleu': 24.502543676121828, 'test/num_examples': 3003, 'score': 10109.073967218399, 'total_duration': 17472.045060634613, 'accumulated_submission_time': 10109.073967218399, 'accumulated_eval_time': 7361.727405786514, 'accumulated_logging_time': 0.3307063579559326, 'global_step': 28816, 'preemption_count': 0}), (31218, {'train/accuracy': 0.6038236021995544, 'train/loss': 2.032376766204834, 'train/bleu': 28.428667172127135, 'validation/accuracy': 0.6241955757141113, 'validation/loss': 1.8636945486068726, 'validation/bleu': 25.288898614166587, 'validation/num_examples': 3000, 'test/accuracy': 0.630387544631958, 'test/loss': 1.82143235206604, 'test/bleu': 24.071335933790778, 'test/num_examples': 3003, 'score': 10949.304612398148, 'total_duration': 18839.73539876938, 'accumulated_submission_time': 10949.304612398148, 'accumulated_eval_time': 7889.079682350159, 'accumulated_logging_time': 0.3619084358215332, 'global_step': 31218, 'preemption_count': 0}), (33620, {'train/accuracy': 0.609279990196228, 'train/loss': 1.99933660030365, 'train/bleu': 28.38434953125899, 'validation/accuracy': 0.6294032335281372, 'validation/loss': 1.85016667842865, 'validation/bleu': 25.796464877571452, 'validation/num_examples': 3000, 'test/accuracy': 0.6363604664802551, 'test/loss': 1.7914565801620483, 'test/bleu': 24.920774993304907, 'test/num_examples': 3003, 'score': 11789.44777727127, 'total_duration': 20178.132102012634, 'accumulated_submission_time': 11789.44777727127, 'accumulated_eval_time': 8387.22826910019, 'accumulated_logging_time': 0.3919835090637207, 'global_step': 33620, 'preemption_count': 0}), (36022, {'train/accuracy': 0.6051180958747864, 'train/loss': 2.022434711456299, 'train/bleu': 28.503332269728375, 'validation/accuracy': 0.6277913451194763, 'validation/loss': 1.8456335067749023, 'validation/bleu': 25.854004220279077, 'validation/num_examples': 3000, 'test/accuracy': 0.6338969469070435, 'test/loss': 1.800976037979126, 'test/bleu': 24.793330930167617, 'test/num_examples': 3003, 'score': 12629.596199512482, 'total_duration': 21590.399958848953, 'accumulated_submission_time': 12629.596199512482, 'accumulated_eval_time': 8959.241186380386, 'accumulated_logging_time': 0.42243266105651855, 'global_step': 36022, 'preemption_count': 0}), (38423, {'train/accuracy': 0.6163857579231262, 'train/loss': 1.9339035749435425, 'train/bleu': 29.14323735552401, 'validation/accuracy': 0.6316102743148804, 'validation/loss': 1.8279942274093628, 'validation/bleu': 26.215031467429903, 'validation/num_examples': 3000, 'test/accuracy': 0.6375108957290649, 'test/loss': 1.7810250520706177, 'test/bleu': 25.07801187053838, 'test/num_examples': 3003, 'score': 13469.625989198685, 'total_duration': 22926.05318045616, 'accumulated_submission_time': 13469.625989198685, 'accumulated_eval_time': 9454.756899356842, 'accumulated_logging_time': 0.45250535011291504, 'global_step': 38423, 'preemption_count': 0}), (40824, {'train/accuracy': 0.6103520393371582, 'train/loss': 1.9843041896820068, 'train/bleu': 29.142409587723943, 'validation/accuracy': 0.6277045607566833, 'validation/loss': 1.8335964679718018, 'validation/bleu': 25.61308660868012, 'validation/num_examples': 3000, 'test/accuracy': 0.6380454301834106, 'test/loss': 1.7811000347137451, 'test/bleu': 24.74312727864808, 'test/num_examples': 3003, 'score': 14309.583218336105, 'total_duration': 24290.329845905304, 'accumulated_submission_time': 14309.583218336105, 'accumulated_eval_time': 9978.96815109253, 'accumulated_logging_time': 0.4831573963165283, 'global_step': 40824, 'preemption_count': 0}), (43226, {'train/accuracy': 0.6160485148429871, 'train/loss': 1.9517220258712769, 'train/bleu': 29.20263873077558, 'validation/accuracy': 0.6300479769706726, 'validation/loss': 1.8143119812011719, 'validation/bleu': 22.437207824955046, 'validation/num_examples': 3000, 'test/accuracy': 0.6379989981651306, 'test/loss': 1.769517183303833, 'test/bleu': 24.556187006987688, 'test/num_examples': 3003, 'score': 15149.78208065033, 'total_duration': 25965.072471618652, 'accumulated_submission_time': 15149.78208065033, 'accumulated_eval_time': 10813.402871608734, 'accumulated_logging_time': 0.5145649909973145, 'global_step': 43226, 'preemption_count': 0}), (45628, {'train/accuracy': 0.6102014183998108, 'train/loss': 1.9810327291488647, 'train/bleu': 29.17744358049914, 'validation/accuracy': 0.6297379732131958, 'validation/loss': 1.8217298984527588, 'validation/bleu': 25.96525554826419, 'validation/num_examples': 3000, 'test/accuracy': 0.6361513137817383, 'test/loss': 1.7746611833572388, 'test/bleu': 24.85202517879502, 'test/num_examples': 3003, 'score': 15989.976817846298, 'total_duration': 27441.79203939438, 'accumulated_submission_time': 15989.976817846298, 'accumulated_eval_time': 11449.814586162567, 'accumulated_logging_time': 0.5513138771057129, 'global_step': 45628, 'preemption_count': 0}), (48030, {'train/accuracy': 0.6131874322891235, 'train/loss': 1.9688488245010376, 'train/bleu': 29.100882515421855, 'validation/accuracy': 0.6333833336830139, 'validation/loss': 1.80632483959198, 'validation/bleu': 26.431980916570602, 'validation/num_examples': 3000, 'test/accuracy': 0.6418104767799377, 'test/loss': 1.7534323930740356, 'test/bleu': 25.610204331052604, 'test/num_examples': 3003, 'score': 16830.157977104187, 'total_duration': 28879.521926641464, 'accumulated_submission_time': 16830.157977104187, 'accumulated_eval_time': 12047.255705833435, 'accumulated_logging_time': 0.5844166278839111, 'global_step': 48030, 'preemption_count': 0}), (50431, {'train/accuracy': 0.6287590861320496, 'train/loss': 1.834293246269226, 'train/bleu': 30.29873589696989, 'validation/accuracy': 0.6369046568870544, 'validation/loss': 1.7885897159576416, 'validation/bleu': 26.45663032630946, 'validation/num_examples': 3000, 'test/accuracy': 0.6455406546592712, 'test/loss': 1.7254810333251953, 'test/bleu': 25.790316454042895, 'test/num_examples': 3003, 'score': 17670.106913089752, 'total_duration': 30213.769419670105, 'accumulated_submission_time': 17670.106913089752, 'accumulated_eval_time': 12541.439943313599, 'accumulated_logging_time': 0.6211183071136475, 'global_step': 50431, 'preemption_count': 0}), (52832, {'train/accuracy': 0.6180623173713684, 'train/loss': 1.9361435174942017, 'train/bleu': 29.478945279105627, 'validation/accuracy': 0.6362723112106323, 'validation/loss': 1.7838091850280762, 'validation/bleu': 26.286487484829664, 'validation/num_examples': 3000, 'test/accuracy': 0.6466097235679626, 'test/loss': 1.7135682106018066, 'test/bleu': 25.517370648362107, 'test/num_examples': 3003, 'score': 18510.01056957245, 'total_duration': 31582.84491109848, 'accumulated_submission_time': 18510.01056957245, 'accumulated_eval_time': 13070.503627538681, 'accumulated_logging_time': 0.6545243263244629, 'global_step': 52832, 'preemption_count': 0}), (55233, {'train/accuracy': 0.6131559014320374, 'train/loss': 1.9650341272354126, 'train/bleu': 29.613710414488942, 'validation/accuracy': 0.6389381289482117, 'validation/loss': 1.767750859260559, 'validation/bleu': 26.565091468583024, 'validation/num_examples': 3000, 'test/accuracy': 0.6476556062698364, 'test/loss': 1.7083313465118408, 'test/bleu': 25.627177441123937, 'test/num_examples': 3003, 'score': 19349.917988538742, 'total_duration': 32943.01463651657, 'accumulated_submission_time': 19349.917988538742, 'accumulated_eval_time': 13590.657889842987, 'accumulated_logging_time': 0.6880850791931152, 'global_step': 55233, 'preemption_count': 0}), (57634, {'train/accuracy': 0.623281717300415, 'train/loss': 1.8893014192581177, 'train/bleu': 30.039212527666372, 'validation/accuracy': 0.639272928237915, 'validation/loss': 1.763260841369629, 'validation/bleu': 26.7444395678232, 'validation/num_examples': 3000, 'test/accuracy': 0.6502469778060913, 'test/loss': 1.700022578239441, 'test/bleu': 26.166706111756557, 'test/num_examples': 3003, 'score': 20189.931703090668, 'total_duration': 34365.222650527954, 'accumulated_submission_time': 20189.931703090668, 'accumulated_eval_time': 14172.742085933685, 'accumulated_logging_time': 0.7207436561584473, 'global_step': 57634, 'preemption_count': 0}), (60036, {'train/accuracy': 0.6204534769058228, 'train/loss': 1.9200528860092163, 'train/bleu': 30.193340853845427, 'validation/accuracy': 0.6428934335708618, 'validation/loss': 1.7423086166381836, 'validation/bleu': 26.78540750934711, 'validation/num_examples': 3000, 'test/accuracy': 0.6512114405632019, 'test/loss': 1.682421326637268, 'test/bleu': 25.748610033015733, 'test/num_examples': 3003, 'score': 21030.09056377411, 'total_duration': 35664.21192359924, 'accumulated_submission_time': 21030.09056377411, 'accumulated_eval_time': 14631.463346242905, 'accumulated_logging_time': 0.7540163993835449, 'global_step': 60036, 'preemption_count': 0}), (62438, {'train/accuracy': 0.6210818886756897, 'train/loss': 1.9125397205352783, 'train/bleu': 29.44747130858823, 'validation/accuracy': 0.6429926156997681, 'validation/loss': 1.7286237478256226, 'validation/bleu': 27.028646288240786, 'validation/num_examples': 3000, 'test/accuracy': 0.6535122990608215, 'test/loss': 1.6790618896484375, 'test/bleu': 26.31763856807123, 'test/num_examples': 3003, 'score': 21870.185875177383, 'total_duration': 37007.154800891876, 'accumulated_submission_time': 21870.185875177383, 'accumulated_eval_time': 15134.199719667435, 'accumulated_logging_time': 0.7899153232574463, 'global_step': 62438, 'preemption_count': 0}), (64838, {'train/accuracy': 0.6214345097541809, 'train/loss': 1.899621844291687, 'train/bleu': 29.87296898313337, 'validation/accuracy': 0.6450632810592651, 'validation/loss': 1.725380539894104, 'validation/bleu': 26.96320628106604, 'validation/num_examples': 3000, 'test/accuracy': 0.653872549533844, 'test/loss': 1.662144660949707, 'test/bleu': 26.29416092890208, 'test/num_examples': 3003, 'score': 22710.087017774582, 'total_duration': 38335.861196517944, 'accumulated_submission_time': 22710.087017774582, 'accumulated_eval_time': 15622.89288187027, 'accumulated_logging_time': 0.8244888782501221, 'global_step': 64838, 'preemption_count': 0}), (67239, {'train/accuracy': 0.6219140887260437, 'train/loss': 1.8977916240692139, 'train/bleu': 29.898942772496444, 'validation/accuracy': 0.6444805264472961, 'validation/loss': 1.7192747592926025, 'validation/bleu': 27.21427000478681, 'validation/num_examples': 3000, 'test/accuracy': 0.6557550430297852, 'test/loss': 1.6552984714508057, 'test/bleu': 26.504229713711247, 'test/num_examples': 3003, 'score': 23550.073257923126, 'total_duration': 39769.913810014725, 'accumulated_submission_time': 23550.073257923126, 'accumulated_eval_time': 16216.848822593689, 'accumulated_logging_time': 0.8594467639923096, 'global_step': 67239, 'preemption_count': 0}), (69640, {'train/accuracy': 0.6379268765449524, 'train/loss': 1.7811611890792847, 'train/bleu': 30.41896362964424, 'validation/accuracy': 0.649774968624115, 'validation/loss': 1.6948308944702148, 'validation/bleu': 27.45221541734196, 'validation/num_examples': 3000, 'test/accuracy': 0.6588344573974609, 'test/loss': 1.639184832572937, 'test/bleu': 26.867683681614956, 'test/num_examples': 3003, 'score': 24390.021370887756, 'total_duration': 41132.171432971954, 'accumulated_submission_time': 24390.021370887756, 'accumulated_eval_time': 16739.037479639053, 'accumulated_logging_time': 0.9037342071533203, 'global_step': 69640, 'preemption_count': 0}), (72042, {'train/accuracy': 0.6333906650543213, 'train/loss': 1.8417229652404785, 'train/bleu': 30.65259166358259, 'validation/accuracy': 0.6506924629211426, 'validation/loss': 1.6889138221740723, 'validation/bleu': 27.472684786958762, 'validation/num_examples': 3000, 'test/accuracy': 0.6625297665596008, 'test/loss': 1.6213717460632324, 'test/bleu': 27.263224574075057, 'test/num_examples': 3003, 'score': 25229.988456487656, 'total_duration': 42538.432683467865, 'accumulated_submission_time': 25229.988456487656, 'accumulated_eval_time': 17305.21870613098, 'accumulated_logging_time': 0.9400687217712402, 'global_step': 72042, 'preemption_count': 0}), (74443, {'train/accuracy': 0.6327632665634155, 'train/loss': 1.840661883354187, 'train/bleu': 30.128673062601425, 'validation/accuracy': 0.6526143550872803, 'validation/loss': 1.6699992418289185, 'validation/bleu': 27.475459978186038, 'validation/num_examples': 3000, 'test/accuracy': 0.662494957447052, 'test/loss': 1.6078910827636719, 'test/bleu': 26.571996580797872, 'test/num_examples': 3003, 'score': 26069.895349264145, 'total_duration': 43927.80186915398, 'accumulated_submission_time': 26069.895349264145, 'accumulated_eval_time': 17854.567930936813, 'accumulated_logging_time': 0.9772353172302246, 'global_step': 74443, 'preemption_count': 0}), (76844, {'train/accuracy': 0.6360828280448914, 'train/loss': 1.7997148036956787, 'train/bleu': 30.947704964285027, 'validation/accuracy': 0.6537302732467651, 'validation/loss': 1.655122995376587, 'validation/bleu': 27.672505117892413, 'validation/num_examples': 3000, 'test/accuracy': 0.6651095151901245, 'test/loss': 1.5921093225479126, 'test/bleu': 27.0778855815688, 'test/num_examples': 3003, 'score': 26909.92446255684, 'total_duration': 45335.44073653221, 'accumulated_submission_time': 26909.92446255684, 'accumulated_eval_time': 18422.05643105507, 'accumulated_logging_time': 1.0228424072265625, 'global_step': 76844, 'preemption_count': 0}), (79245, {'train/accuracy': 0.6354596018791199, 'train/loss': 1.8094682693481445, 'train/bleu': 30.61698412674778, 'validation/accuracy': 0.6570656299591064, 'validation/loss': 1.6474754810333252, 'validation/bleu': 27.837613650228427, 'validation/num_examples': 3000, 'test/accuracy': 0.666852593421936, 'test/loss': 1.5803241729736328, 'test/bleu': 27.10222512757891, 'test/num_examples': 3003, 'score': 27749.865286827087, 'total_duration': 46707.21108341217, 'accumulated_submission_time': 27749.865286827087, 'accumulated_eval_time': 18953.771744966507, 'accumulated_logging_time': 1.0598394870758057, 'global_step': 79245, 'preemption_count': 0}), (81645, {'train/accuracy': 0.6552977561950684, 'train/loss': 1.6481138467788696, 'train/bleu': 32.10947252151286, 'validation/accuracy': 0.6575492024421692, 'validation/loss': 1.6359890699386597, 'validation/bleu': 27.994461884509338, 'validation/num_examples': 3000, 'test/accuracy': 0.6707803606987, 'test/loss': 1.570162057876587, 'test/bleu': 27.857890901754264, 'test/num_examples': 3003, 'score': 28589.836373090744, 'total_duration': 48076.842170238495, 'accumulated_submission_time': 28589.836373090744, 'accumulated_eval_time': 19483.31466794014, 'accumulated_logging_time': 1.0974700450897217, 'global_step': 81645, 'preemption_count': 0}), (84048, {'train/accuracy': 0.6372441649436951, 'train/loss': 1.792824149131775, 'train/bleu': 31.231938001811475, 'validation/accuracy': 0.6590867042541504, 'validation/loss': 1.6225743293762207, 'validation/bleu': 28.227901930406095, 'validation/num_examples': 3000, 'test/accuracy': 0.6720934510231018, 'test/loss': 1.5535181760787964, 'test/bleu': 27.58812304327058, 'test/num_examples': 3003, 'score': 29429.99524831772, 'total_duration': 49503.03884482384, 'accumulated_submission_time': 29429.99524831772, 'accumulated_eval_time': 20069.23208117485, 'accumulated_logging_time': 1.141160249710083, 'global_step': 84048, 'preemption_count': 0}), (86449, {'train/accuracy': 0.6410945057868958, 'train/loss': 1.7650405168533325, 'train/bleu': 31.419086215696446, 'validation/accuracy': 0.6631659865379333, 'validation/loss': 1.604163408279419, 'validation/bleu': 28.648366919736237, 'validation/num_examples': 3000, 'test/accuracy': 0.6744524240493774, 'test/loss': 1.5341390371322632, 'test/bleu': 27.97908133401334, 'test/num_examples': 3003, 'score': 30269.94028711319, 'total_duration': 51033.38291668892, 'accumulated_submission_time': 30269.94028711319, 'accumulated_eval_time': 20759.51155924797, 'accumulated_logging_time': 1.185424566268921, 'global_step': 86449, 'preemption_count': 0}), (88851, {'train/accuracy': 0.6504623293876648, 'train/loss': 1.689774990081787, 'train/bleu': 31.84581123567893, 'validation/accuracy': 0.6630296111106873, 'validation/loss': 1.5953129529953003, 'validation/bleu': 28.532335408202147, 'validation/num_examples': 3000, 'test/accuracy': 0.6743943095207214, 'test/loss': 1.5237705707550049, 'test/bleu': 27.792506126010068, 'test/num_examples': 3003, 'score': 31109.83723974228, 'total_duration': 52418.78368616104, 'accumulated_submission_time': 31109.83723974228, 'accumulated_eval_time': 21304.902579307556, 'accumulated_logging_time': 1.2225456237792969, 'global_step': 88851, 'preemption_count': 0}), (91254, {'train/accuracy': 0.6479614973068237, 'train/loss': 1.720964789390564, 'train/bleu': 31.763669002056673, 'validation/accuracy': 0.665571391582489, 'validation/loss': 1.5849394798278809, 'validation/bleu': 28.862445004881646, 'validation/num_examples': 3000, 'test/accuracy': 0.6779152750968933, 'test/loss': 1.5046041011810303, 'test/bleu': 28.25653450368869, 'test/num_examples': 3003, 'score': 31949.886343479156, 'total_duration': 53800.26221823692, 'accumulated_submission_time': 31949.886343479156, 'accumulated_eval_time': 21846.217509269714, 'accumulated_logging_time': 1.2612571716308594, 'global_step': 91254, 'preemption_count': 0}), (93656, {'train/accuracy': 0.6468232274055481, 'train/loss': 1.723307490348816, 'train/bleu': 31.683186844159014, 'validation/accuracy': 0.6688323616981506, 'validation/loss': 1.5702892541885376, 'validation/bleu': 29.080144950437283, 'validation/num_examples': 3000, 'test/accuracy': 0.6798908114433289, 'test/loss': 1.4960421323776245, 'test/bleu': 28.425006038283964, 'test/num_examples': 3003, 'score': 32789.87203192711, 'total_duration': 55197.843589782715, 'accumulated_submission_time': 32789.87203192711, 'accumulated_eval_time': 22403.698242902756, 'accumulated_logging_time': 1.300079584121704, 'global_step': 93656, 'preemption_count': 0}), (96058, {'train/accuracy': 0.6511693596839905, 'train/loss': 1.6873743534088135, 'train/bleu': 31.714046536608137, 'validation/accuracy': 0.6717833280563354, 'validation/loss': 1.547297716140747, 'validation/bleu': 29.245886051656544, 'validation/num_examples': 3000, 'test/accuracy': 0.6813665628433228, 'test/loss': 1.4757575988769531, 'test/bleu': 28.803274428417076, 'test/num_examples': 3003, 'score': 33629.953258514404, 'total_duration': 56583.53949093819, 'accumulated_submission_time': 33629.953258514404, 'accumulated_eval_time': 22949.19913959503, 'accumulated_logging_time': 1.3382477760314941, 'global_step': 96058, 'preemption_count': 0}), (98461, {'train/accuracy': 0.6557013392448425, 'train/loss': 1.67685866355896, 'train/bleu': 32.65956777295396, 'validation/accuracy': 0.6736308336257935, 'validation/loss': 1.5366019010543823, 'validation/bleu': 29.419734290930183, 'validation/num_examples': 3000, 'test/accuracy': 0.6865260601043701, 'test/loss': 1.456973910331726, 'test/bleu': 29.064059597154397, 'test/num_examples': 3003, 'score': 34470.011729478836, 'total_duration': 57937.23023700714, 'accumulated_submission_time': 34470.011729478836, 'accumulated_eval_time': 23462.708975553513, 'accumulated_logging_time': 1.385880470275879, 'global_step': 98461, 'preemption_count': 0}), (100863, {'train/accuracy': 0.6670910716056824, 'train/loss': 1.576935052871704, 'train/bleu': 33.74426266233717, 'validation/accuracy': 0.6752303242683411, 'validation/loss': 1.5253691673278809, 'validation/bleu': 29.383349787408342, 'validation/num_examples': 3000, 'test/accuracy': 0.6878159642219543, 'test/loss': 1.442979335784912, 'test/bleu': 29.206599892438135, 'test/num_examples': 3003, 'score': 35309.92882537842, 'total_duration': 59347.370725631714, 'accumulated_submission_time': 35309.92882537842, 'accumulated_eval_time': 24032.816915750504, 'accumulated_logging_time': 1.4265773296356201, 'global_step': 100863, 'preemption_count': 0}), (103265, {'train/accuracy': 0.6593321561813354, 'train/loss': 1.6377383470535278, 'train/bleu': 32.99818121622395, 'validation/accuracy': 0.676978588104248, 'validation/loss': 1.5108222961425781, 'validation/bleu': 29.280341283493588, 'validation/num_examples': 3000, 'test/accuracy': 0.6917320489883423, 'test/loss': 1.4219063520431519, 'test/bleu': 29.722389977868755, 'test/num_examples': 3003, 'score': 36149.880427360535, 'total_duration': 60767.247957229614, 'accumulated_submission_time': 36149.880427360535, 'accumulated_eval_time': 24612.617929458618, 'accumulated_logging_time': 1.474708080291748, 'global_step': 103265, 'preemption_count': 0}), (105668, {'train/accuracy': 0.6658817529678345, 'train/loss': 1.6051431894302368, 'train/bleu': 32.95288097209237, 'validation/accuracy': 0.6797559857368469, 'validation/loss': 1.4939372539520264, 'validation/bleu': 29.482852243566366, 'validation/num_examples': 3000, 'test/accuracy': 0.6930451393127441, 'test/loss': 1.4107818603515625, 'test/bleu': 29.341508113582247, 'test/num_examples': 3003, 'score': 36990.06665062904, 'total_duration': 62172.06532788277, 'accumulated_submission_time': 36990.06665062904, 'accumulated_eval_time': 25177.135360479355, 'accumulated_logging_time': 1.5139610767364502, 'global_step': 105668, 'preemption_count': 0}), (108070, {'train/accuracy': 0.6727055311203003, 'train/loss': 1.5570759773254395, 'train/bleu': 33.9455520747962, 'validation/accuracy': 0.6823598146438599, 'validation/loss': 1.4784127473831177, 'validation/bleu': 29.72807262910545, 'validation/num_examples': 3000, 'test/accuracy': 0.6957992315292358, 'test/loss': 1.3981857299804688, 'test/bleu': 29.608836478521752, 'test/num_examples': 3003, 'score': 37830.27451658249, 'total_duration': 63536.203300237656, 'accumulated_submission_time': 37830.27451658249, 'accumulated_eval_time': 25700.941499471664, 'accumulated_logging_time': 1.5608172416687012, 'global_step': 108070, 'preemption_count': 0}), (110471, {'train/accuracy': 0.6731343865394592, 'train/loss': 1.5567851066589355, 'train/bleu': 33.786392688007275, 'validation/accuracy': 0.6843188405036926, 'validation/loss': 1.4721633195877075, 'validation/bleu': 30.15936458541903, 'validation/num_examples': 3000, 'test/accuracy': 0.6989832520484924, 'test/loss': 1.3790864944458008, 'test/bleu': 29.739070535005855, 'test/num_examples': 3003, 'score': 38670.3262925148, 'total_duration': 64930.54170131683, 'accumulated_submission_time': 38670.3262925148, 'accumulated_eval_time': 26255.110206842422, 'accumulated_logging_time': 1.6010394096374512, 'global_step': 110471, 'preemption_count': 0}), (112873, {'train/accuracy': 0.6936594247817993, 'train/loss': 1.431340217590332, 'train/bleu': 35.508655574660985, 'validation/accuracy': 0.685050368309021, 'validation/loss': 1.4566941261291504, 'validation/bleu': 30.06185313900829, 'validation/num_examples': 3000, 'test/accuracy': 0.6995874643325806, 'test/loss': 1.3684998750686646, 'test/bleu': 30.10767739633273, 'test/num_examples': 3003, 'score': 39510.36837506294, 'total_duration': 66343.16135883331, 'accumulated_submission_time': 39510.36837506294, 'accumulated_eval_time': 26827.57296895981, 'accumulated_logging_time': 1.6411964893341064, 'global_step': 112873, 'preemption_count': 0}), (115275, {'train/accuracy': 0.6838340163230896, 'train/loss': 1.4911880493164062, 'train/bleu': 34.26181205626829, 'validation/accuracy': 0.6880261898040771, 'validation/loss': 1.4454679489135742, 'validation/bleu': 30.377481779085375, 'validation/num_examples': 3000, 'test/accuracy': 0.7013421654701233, 'test/loss': 1.357330322265625, 'test/bleu': 30.101626542932724, 'test/num_examples': 3003, 'score': 40350.29947733879, 'total_duration': 67821.58649492264, 'accumulated_submission_time': 40350.29947733879, 'accumulated_eval_time': 27465.951172590256, 'accumulated_logging_time': 1.6827702522277832, 'global_step': 115275, 'preemption_count': 0}), (117677, {'train/accuracy': 0.6806775331497192, 'train/loss': 1.5084309577941895, 'train/bleu': 34.721579984247796, 'validation/accuracy': 0.6893280744552612, 'validation/loss': 1.4370354413986206, 'validation/bleu': 30.509479063813494, 'validation/num_examples': 3000, 'test/accuracy': 0.7044913172721863, 'test/loss': 1.3434144258499146, 'test/bleu': 30.434828744976627, 'test/num_examples': 3003, 'score': 41190.48154783249, 'total_duration': 69216.60397958755, 'accumulated_submission_time': 41190.48154783249, 'accumulated_eval_time': 28020.668506383896, 'accumulated_logging_time': 1.7252159118652344, 'global_step': 117677, 'preemption_count': 0}), (120079, {'train/accuracy': 0.6928339004516602, 'train/loss': 1.4373910427093506, 'train/bleu': 35.37524108993434, 'validation/accuracy': 0.690034806728363, 'validation/loss': 1.4303141832351685, 'validation/bleu': 30.701788225822256, 'validation/num_examples': 3000, 'test/accuracy': 0.7062111496925354, 'test/loss': 1.3397371768951416, 'test/bleu': 30.584510235579657, 'test/num_examples': 3003, 'score': 42030.51136517525, 'total_duration': 70611.56590008736, 'accumulated_submission_time': 42030.51136517525, 'accumulated_eval_time': 28575.47600364685, 'accumulated_logging_time': 1.7752346992492676, 'global_step': 120079, 'preemption_count': 0}), (122481, {'train/accuracy': 0.688599705696106, 'train/loss': 1.4655152559280396, 'train/bleu': 35.63986574927191, 'validation/accuracy': 0.6912251710891724, 'validation/loss': 1.4246001243591309, 'validation/bleu': 30.711851471746595, 'validation/num_examples': 3000, 'test/accuracy': 0.7073150873184204, 'test/loss': 1.328243374824524, 'test/bleu': 30.721067569448703, 'test/num_examples': 3003, 'score': 42870.60580945015, 'total_duration': 71997.75002217293, 'accumulated_submission_time': 42870.60580945015, 'accumulated_eval_time': 29121.447292804718, 'accumulated_logging_time': 1.8173811435699463, 'global_step': 122481, 'preemption_count': 0}), (124883, {'train/accuracy': 0.6913896799087524, 'train/loss': 1.4493789672851562, 'train/bleu': 35.22434842341379, 'validation/accuracy': 0.6933205723762512, 'validation/loss': 1.415845274925232, 'validation/bleu': 30.862479457424147, 'validation/num_examples': 3000, 'test/accuracy': 0.7088490128517151, 'test/loss': 1.3179659843444824, 'test/bleu': 30.963558849323057, 'test/num_examples': 3003, 'score': 43710.570806741714, 'total_duration': 73377.59749126434, 'accumulated_submission_time': 43710.570806741714, 'accumulated_eval_time': 29661.211304426193, 'accumulated_logging_time': 1.8602821826934814, 'global_step': 124883, 'preemption_count': 0}), (127286, {'train/accuracy': 0.6950476765632629, 'train/loss': 1.428101658821106, 'train/bleu': 35.723502797572735, 'validation/accuracy': 0.6939281225204468, 'validation/loss': 1.4126336574554443, 'validation/bleu': 30.916063388296088, 'validation/num_examples': 3000, 'test/accuracy': 0.7101272344589233, 'test/loss': 1.3143149614334106, 'test/bleu': 30.82745926563664, 'test/num_examples': 3003, 'score': 44550.7475271225, 'total_duration': 74749.03038787842, 'accumulated_submission_time': 44550.7475271225, 'accumulated_eval_time': 30192.348385572433, 'accumulated_logging_time': 1.9036564826965332, 'global_step': 127286, 'preemption_count': 0}), (129688, {'train/accuracy': 0.6956238150596619, 'train/loss': 1.4236267805099487, 'train/bleu': 35.90949567414748, 'validation/accuracy': 0.6945853233337402, 'validation/loss': 1.410330057144165, 'validation/bleu': 30.926567537057483, 'validation/num_examples': 3000, 'test/accuracy': 0.7101272344589233, 'test/loss': 1.311076045036316, 'test/bleu': 31.011806972890586, 'test/num_examples': 3003, 'score': 45390.827068567276, 'total_duration': 76096.52921843529, 'accumulated_submission_time': 45390.827068567276, 'accumulated_eval_time': 30699.643965244293, 'accumulated_logging_time': 1.9506874084472656, 'global_step': 129688, 'preemption_count': 0}), (132089, {'train/accuracy': 0.7016637921333313, 'train/loss': 1.386156439781189, 'train/bleu': 35.79594700241151, 'validation/accuracy': 0.694386899471283, 'validation/loss': 1.4098578691482544, 'validation/bleu': 30.910722132760736, 'validation/num_examples': 3000, 'test/accuracy': 0.7106966376304626, 'test/loss': 1.3105497360229492, 'test/bleu': 30.939606512181268, 'test/num_examples': 3003, 'score': 46230.71160531044, 'total_duration': 77462.6783709526, 'accumulated_submission_time': 46230.71160531044, 'accumulated_eval_time': 31225.786337852478, 'accumulated_logging_time': 1.9961497783660889, 'global_step': 132089, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6967169642448425, 'train/loss': 1.42378568649292, 'train/bleu': 35.835157411181875, 'validation/accuracy': 0.69430011510849, 'validation/loss': 1.410101294517517, 'validation/bleu': 30.869154358846924, 'validation/num_examples': 3000, 'test/accuracy': 0.7108826041221619, 'test/loss': 1.3107471466064453, 'test/bleu': 30.902745284398407, 'test/num_examples': 3003, 'score': 46665.6048541069, 'total_duration': 78415.79715752602, 'accumulated_submission_time': 46665.6048541069, 'accumulated_eval_time': 31743.91850733757, 'accumulated_logging_time': 2.04841947555542, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0210 02:02:47.712805 140446903760704 submission_runner.py:586] Timing: 46665.6048541069
I0210 02:02:47.712859 140446903760704 submission_runner.py:588] Total number of evals: 57
I0210 02:02:47.712900 140446903760704 submission_runner.py:589] ====================
I0210 02:02:47.712943 140446903760704 submission_runner.py:542] Using RNG seed 1540897543
I0210 02:02:47.714481 140446903760704 submission_runner.py:551] --- Tuning run 5/5 ---
I0210 02:02:47.714588 140446903760704 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_5.
I0210 02:02:47.714928 140446903760704 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_5/hparams.json.
I0210 02:02:47.715797 140446903760704 submission_runner.py:206] Initializing dataset.
I0210 02:02:47.718371 140446903760704 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 02:02:47.721322 140446903760704 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 02:02:47.759173 140446903760704 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 02:02:48.441962 140446903760704 submission_runner.py:213] Initializing model.
I0210 02:02:55.193899 140446903760704 submission_runner.py:255] Initializing optimizer.
I0210 02:02:56.000895 140446903760704 submission_runner.py:262] Initializing metrics bundle.
I0210 02:02:56.001070 140446903760704 submission_runner.py:280] Initializing checkpoint and logger.
I0210 02:02:56.002183 140446903760704 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/wmt_jax/trial_5 with prefix checkpoint_
I0210 02:02:56.002316 140446903760704 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_5/meta_data_0.json.
I0210 02:02:56.002530 140446903760704 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 02:02:56.002590 140446903760704 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 02:02:56.558628 140446903760704 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 02:02:57.106034 140446903760704 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_5/flags_0.json.
I0210 02:02:57.109484 140446903760704 submission_runner.py:314] Starting training loop.
I0210 02:03:29.091008 140277044971264 logging_writer.py:48] [0] global_step=0, grad_norm=4.788896560668945, loss=11.135162353515625
I0210 02:03:29.105495 140446903760704 spec.py:321] Evaluating on the training split.
I0210 02:03:31.802873 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:08:10.170092 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 02:08:12.899647 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:12:51.336846 140446903760704 spec.py:349] Evaluating on the test split.
I0210 02:12:54.075829 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:17:30.847336 140446903760704 submission_runner.py:408] Time since start: 873.74s, 	Step: 1, 	{'train/accuracy': 0.0005809449939988554, 'train/loss': 11.19240951538086, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 31.995970487594604, 'total_duration': 873.7377886772156, 'accumulated_submission_time': 31.995970487594604, 'accumulated_eval_time': 841.7417721748352, 'accumulated_logging_time': 0}
I0210 02:17:30.856445 140277053363968 logging_writer.py:48] [1] accumulated_eval_time=841.741772, accumulated_logging_time=0, accumulated_submission_time=31.995970, global_step=1, preemption_count=0, score=31.995970, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.191028, test/num_examples=3003, total_duration=873.737789, train/accuracy=0.000581, train/bleu=0.000000, train/loss=11.192410, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.190282, validation/num_examples=3000
I0210 02:18:06.599985 140277044971264 logging_writer.py:48] [100] global_step=100, grad_norm=0.18685811758041382, loss=8.258304595947266
I0210 02:18:42.395335 140277053363968 logging_writer.py:48] [200] global_step=200, grad_norm=0.9624199867248535, loss=7.521669387817383
I0210 02:19:18.268569 140277044971264 logging_writer.py:48] [300] global_step=300, grad_norm=0.3734639585018158, loss=6.893314361572266
I0210 02:19:54.129410 140277053363968 logging_writer.py:48] [400] global_step=400, grad_norm=0.5321248173713684, loss=6.338679313659668
I0210 02:20:30.078685 140277044971264 logging_writer.py:48] [500] global_step=500, grad_norm=0.38655486702919006, loss=5.900364398956299
I0210 02:21:05.987255 140277053363968 logging_writer.py:48] [600] global_step=600, grad_norm=0.5496124029159546, loss=5.615563869476318
I0210 02:21:41.925257 140277044971264 logging_writer.py:48] [700] global_step=700, grad_norm=0.6272759437561035, loss=5.336079120635986
I0210 02:22:17.822135 140277053363968 logging_writer.py:48] [800] global_step=800, grad_norm=0.5270278453826904, loss=5.061240196228027
I0210 02:22:53.723599 140277044971264 logging_writer.py:48] [900] global_step=900, grad_norm=0.4881947934627533, loss=4.88918399810791
I0210 02:23:29.614351 140277053363968 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7474929690361023, loss=4.609755992889404
I0210 02:24:05.485859 140277044971264 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6814027428627014, loss=4.337660789489746
I0210 02:24:41.386871 140277053363968 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6069384217262268, loss=4.1046905517578125
I0210 02:25:17.315493 140277044971264 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5249200463294983, loss=3.945490837097168
I0210 02:25:53.254904 140277053363968 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.46002233028411865, loss=3.7745182514190674
I0210 02:26:29.182390 140277044971264 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5261248350143433, loss=3.603682279586792
I0210 02:27:05.092607 140277053363968 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5167644619941711, loss=3.528944492340088
I0210 02:27:40.945614 140277044971264 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5269344449043274, loss=3.429561138153076
I0210 02:28:16.827026 140277053363968 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5184435248374939, loss=3.2781405448913574
I0210 02:28:52.755926 140277044971264 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5212904214859009, loss=3.270367383956909
I0210 02:29:28.637270 140277053363968 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.44549286365509033, loss=3.2209134101867676
I0210 02:30:04.501438 140277044971264 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.45205801725387573, loss=3.057701587677002
I0210 02:30:40.483715 140277053363968 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3876440227031708, loss=3.040194272994995
I0210 02:31:16.346958 140277044971264 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.4247182309627533, loss=3.014045000076294
I0210 02:31:31.142907 140446903760704 spec.py:321] Evaluating on the training split.
I0210 02:31:34.178600 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:34:33.966279 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 02:34:36.691474 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:37:43.400932 140446903760704 spec.py:349] Evaluating on the test split.
I0210 02:37:46.104712 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:40:28.939705 140446903760704 submission_runner.py:408] Time since start: 2251.83s, 	Step: 2343, 	{'train/accuracy': 0.5090352892875671, 'train/loss': 2.874778985977173, 'train/bleu': 22.760860535397587, 'validation/accuracy': 0.509181559085846, 'validation/loss': 2.8774378299713135, 'validation/bleu': 18.645719298845435, 'validation/num_examples': 3000, 'test/accuracy': 0.5100807547569275, 'test/loss': 2.911456823348999, 'test/bleu': 17.119625855590364, 'test/num_examples': 3003, 'score': 872.1930792331696, 'total_duration': 2251.830168247223, 'accumulated_submission_time': 872.1930792331696, 'accumulated_eval_time': 1379.5385262966156, 'accumulated_logging_time': 0.020632028579711914}
I0210 02:40:28.954828 140277053363968 logging_writer.py:48] [2343] accumulated_eval_time=1379.538526, accumulated_logging_time=0.020632, accumulated_submission_time=872.193079, global_step=2343, preemption_count=0, score=872.193079, test/accuracy=0.510081, test/bleu=17.119626, test/loss=2.911457, test/num_examples=3003, total_duration=2251.830168, train/accuracy=0.509035, train/bleu=22.760861, train/loss=2.874779, validation/accuracy=0.509182, validation/bleu=18.645719, validation/loss=2.877438, validation/num_examples=3000
I0210 02:40:49.698103 140277044971264 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3853449523448944, loss=2.9670135974884033
I0210 02:41:25.471005 140277053363968 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.30748245120048523, loss=2.955075979232788
I0210 02:42:01.375413 140277044971264 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.3189297020435333, loss=2.8719403743743896
I0210 02:42:37.237941 140277053363968 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.29831644892692566, loss=2.8704044818878174
I0210 02:43:13.075277 140277044971264 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.268766313791275, loss=2.692211866378784
I0210 02:43:48.921495 140277053363968 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2936275899410248, loss=2.8190197944641113
I0210 02:44:24.800731 140277044971264 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.27218738198280334, loss=2.7449796199798584
I0210 02:45:00.703630 140277053363968 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.24613502621650696, loss=2.64497447013855
I0210 02:45:36.620060 140277044971264 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.2677299380302429, loss=2.5420196056365967
I0210 02:46:12.448382 140277053363968 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3188421428203583, loss=2.5197975635528564
I0210 02:46:48.278464 140277044971264 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.279460608959198, loss=2.5824146270751953
I0210 02:47:24.114367 140277053363968 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.20613354444503784, loss=2.4527883529663086
I0210 02:47:59.933324 140277044971264 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.31341657042503357, loss=2.4917492866516113
I0210 02:48:35.832542 140277053363968 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.22207392752170563, loss=2.3891866207122803
I0210 02:49:11.687882 140277044971264 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.276295930147171, loss=2.462766647338867
I0210 02:49:47.581382 140277053363968 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.22223860025405884, loss=2.531439781188965
I0210 02:50:23.443370 140277044971264 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.2048281580209732, loss=2.4508044719696045
I0210 02:50:59.303973 140277053363968 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.20348887145519257, loss=2.416131019592285
I0210 02:51:35.197755 140277044971264 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.18092578649520874, loss=2.4424550533294678
I0210 02:52:11.076031 140277053363968 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.18419796228408813, loss=2.4227068424224854
I0210 02:52:46.892620 140277044971264 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.18129539489746094, loss=2.4428048133850098
I0210 02:53:22.729629 140277053363968 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.15981300175189972, loss=2.3343305587768555
I0210 02:53:58.590431 140277044971264 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.16062580049037933, loss=2.440896511077881
I0210 02:54:29.112431 140446903760704 spec.py:321] Evaluating on the training split.
I0210 02:54:32.117654 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:57:09.873865 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 02:57:12.574819 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 02:59:46.369898 140446903760704 spec.py:349] Evaluating on the test split.
I0210 02:59:49.088704 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 03:02:09.483753 140446903760704 submission_runner.py:408] Time since start: 3552.37s, 	Step: 4687, 	{'train/accuracy': 0.5762688517570496, 'train/loss': 2.2632884979248047, 'train/bleu': 27.03187390595071, 'validation/accuracy': 0.5884490013122559, 'validation/loss': 2.1593129634857178, 'validation/bleu': 23.441319934527357, 'validation/num_examples': 3000, 'test/accuracy': 0.5936436057090759, 'test/loss': 2.1284549236297607, 'test/bleu': 22.072443389635552, 'test/num_examples': 3003, 'score': 1712.265777349472, 'total_duration': 3552.3742141723633, 'accumulated_submission_time': 1712.265777349472, 'accumulated_eval_time': 1839.9097967147827, 'accumulated_logging_time': 0.046051979064941406}
I0210 03:02:09.499310 140277053363968 logging_writer.py:48] [4687] accumulated_eval_time=1839.909797, accumulated_logging_time=0.046052, accumulated_submission_time=1712.265777, global_step=4687, preemption_count=0, score=1712.265777, test/accuracy=0.593644, test/bleu=22.072443, test/loss=2.128455, test/num_examples=3003, total_duration=3552.374214, train/accuracy=0.576269, train/bleu=27.031874, train/loss=2.263288, validation/accuracy=0.588449, validation/bleu=23.441320, validation/loss=2.159313, validation/num_examples=3000
I0210 03:02:14.513716 140277044971264 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.17878298461437225, loss=2.3124852180480957
I0210 03:02:50.244524 140277053363968 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.20168666541576385, loss=2.2772774696350098
I0210 03:03:26.018550 140277044971264 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.21811933815479279, loss=2.332441568374634
I0210 03:04:01.843238 140277053363968 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.16706788539886475, loss=2.2739477157592773
I0210 03:04:37.651851 140277044971264 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.15344226360321045, loss=2.280571222305298
I0210 03:05:13.439215 140277053363968 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.1750006228685379, loss=2.261178731918335
I0210 03:05:49.254475 140277044971264 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.1683807522058487, loss=2.245896339416504
I0210 03:06:25.067561 140277053363968 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.15672554075717926, loss=2.3171298503875732
I0210 03:07:00.903373 140277044971264 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.1939181387424469, loss=2.2544097900390625
I0210 03:07:36.754384 140277053363968 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.21046458184719086, loss=2.3024115562438965
I0210 03:08:12.595712 140277044971264 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.18398059904575348, loss=2.200963020324707
I0210 03:08:48.385626 140277053363968 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.1649811863899231, loss=2.1547043323516846
I0210 03:09:24.241850 140277044971264 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.1686301976442337, loss=2.274933099746704
I0210 03:10:00.084817 140277053363968 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.15669721364974976, loss=2.106618881225586
I0210 03:10:35.938091 140277044971264 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.18010495603084564, loss=2.1691410541534424
I0210 03:11:11.763975 140277053363968 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.1797383576631546, loss=2.2718005180358887
I0210 03:11:47.619532 140277044971264 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.16840094327926636, loss=2.1866354942321777
I0210 03:12:23.458281 140277053363968 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.14328931272029877, loss=2.221851110458374
I0210 03:12:59.378681 140277044971264 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.15303055942058563, loss=2.223353147506714
I0210 03:13:35.236293 140277053363968 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.1774933636188507, loss=2.211108446121216
I0210 03:14:11.054662 140277044971264 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.1829257607460022, loss=2.236440420150757
I0210 03:14:46.973269 140277053363968 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.18841040134429932, loss=2.145052433013916
I0210 03:15:22.851687 140277044971264 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.15219439566135406, loss=2.1917436122894287
I0210 03:15:58.713365 140277053363968 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.13756655156612396, loss=2.1504664421081543
I0210 03:16:09.549413 140446903760704 spec.py:321] Evaluating on the training split.
I0210 03:16:12.556486 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 03:18:53.217077 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 03:18:55.931818 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 03:21:17.625782 140446903760704 spec.py:349] Evaluating on the test split.
I0210 03:21:20.351919 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 03:23:29.756872 140446903760704 submission_runner.py:408] Time since start: 4832.65s, 	Step: 7032, 	{'train/accuracy': 0.6044877767562866, 'train/loss': 2.0103962421417236, 'train/bleu': 29.150223633210214, 'validation/accuracy': 0.6167561411857605, 'validation/loss': 1.9335185289382935, 'validation/bleu': 25.145358707762522, 'validation/num_examples': 3000, 'test/accuracy': 0.6223810315132141, 'test/loss': 1.891671061515808, 'test/bleu': 23.716157379390612, 'test/num_examples': 3003, 'score': 2552.231223344803, 'total_duration': 4832.6473343372345, 'accumulated_submission_time': 2552.231223344803, 'accumulated_eval_time': 2280.1172001361847, 'accumulated_logging_time': 0.07169175148010254}
I0210 03:23:29.772455 140277044971264 logging_writer.py:48] [7032] accumulated_eval_time=2280.117200, accumulated_logging_time=0.071692, accumulated_submission_time=2552.231223, global_step=7032, preemption_count=0, score=2552.231223, test/accuracy=0.622381, test/bleu=23.716157, test/loss=1.891671, test/num_examples=3003, total_duration=4832.647334, train/accuracy=0.604488, train/bleu=29.150224, train/loss=2.010396, validation/accuracy=0.616756, validation/bleu=25.145359, validation/loss=1.933519, validation/num_examples=3000
I0210 03:23:54.413459 140277053363968 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.1574142724275589, loss=2.135913133621216
I0210 03:24:30.183524 140277044971264 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.17119982838630676, loss=2.1363086700439453
I0210 03:25:05.999586 140277053363968 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1557535082101822, loss=2.1074588298797607
I0210 03:25:41.789875 140277044971264 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.14584773778915405, loss=2.0482399463653564
I0210 03:26:17.634662 140277053363968 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.17599548399448395, loss=2.1198530197143555
I0210 03:26:53.425037 140277044971264 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.16963541507720947, loss=2.0993292331695557
I0210 03:27:29.251633 140277053363968 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.15262308716773987, loss=2.0958290100097656
I0210 03:28:05.058942 140277044971264 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.18618609011173248, loss=2.09741473197937
I0210 03:28:40.891089 140277053363968 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.1631770133972168, loss=2.06851863861084
I0210 03:29:16.683729 140277044971264 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.15509609878063202, loss=2.0399646759033203
I0210 03:29:52.490676 140277053363968 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.17438752949237823, loss=2.0847537517547607
I0210 03:30:28.315392 140277044971264 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.15510274469852448, loss=1.9871408939361572
I0210 03:31:04.159703 140277053363968 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.18711115419864655, loss=2.1581063270568848
I0210 03:31:39.983517 140277044971264 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.15225714445114136, loss=2.036327838897705
I0210 03:32:15.984173 140277053363968 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1826414316892624, loss=2.0480000972747803
I0210 03:32:51.806146 140277044971264 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.15705175697803497, loss=2.073080062866211
I0210 03:33:27.665499 140277053363968 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.15137642621994019, loss=2.082972288131714
I0210 03:34:03.494221 140277044971264 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.16348211467266083, loss=2.017343282699585
I0210 03:34:39.320220 140277053363968 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.2063012421131134, loss=2.0726656913757324
I0210 03:35:15.159184 140277044971264 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15900132060050964, loss=2.105734348297119
I0210 03:35:50.965269 140277053363968 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.2744244337081909, loss=2.0262861251831055
I0210 03:36:26.844735 140277044971264 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.150030717253685, loss=2.1361496448516846
I0210 03:37:02.719287 140277053363968 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.1708633154630661, loss=1.998854398727417
I0210 03:37:30.029704 140446903760704 spec.py:321] Evaluating on the training split.
I0210 03:37:33.034420 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 03:40:08.726257 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 03:40:11.430179 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 03:42:29.729499 140446903760704 spec.py:349] Evaluating on the test split.
I0210 03:42:32.437102 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 03:44:39.613472 140446903760704 submission_runner.py:408] Time since start: 6102.50s, 	Step: 9378, 	{'train/accuracy': 0.6121419668197632, 'train/loss': 1.9549943208694458, 'train/bleu': 29.574920246863407, 'validation/accuracy': 0.6282377243041992, 'validation/loss': 1.8250943422317505, 'validation/bleu': 26.20106139743751, 'validation/num_examples': 3000, 'test/accuracy': 0.637569010257721, 'test/loss': 1.7603546380996704, 'test/bleu': 25.21416047743482, 'test/num_examples': 3003, 'score': 3392.4043912887573, 'total_duration': 6102.503938674927, 'accumulated_submission_time': 3392.4043912887573, 'accumulated_eval_time': 2709.7009241580963, 'accumulated_logging_time': 0.09729146957397461}
I0210 03:44:39.629939 140277044971264 logging_writer.py:48] [9378] accumulated_eval_time=2709.700924, accumulated_logging_time=0.097291, accumulated_submission_time=3392.404391, global_step=9378, preemption_count=0, score=3392.404391, test/accuracy=0.637569, test/bleu=25.214160, test/loss=1.760355, test/num_examples=3003, total_duration=6102.503939, train/accuracy=0.612142, train/bleu=29.574920, train/loss=1.954994, validation/accuracy=0.628238, validation/bleu=26.201061, validation/loss=1.825094, validation/num_examples=3000
I0210 03:44:47.845062 140277053363968 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.1594826877117157, loss=2.0342938899993896
I0210 03:45:23.547109 140277044971264 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.15606415271759033, loss=2.041546583175659
I0210 03:45:59.316017 140277053363968 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.19949297606945038, loss=2.031689167022705
I0210 03:46:35.111750 140277044971264 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.15628844499588013, loss=1.950347661972046
I0210 03:47:10.899279 140277053363968 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.15177640318870544, loss=2.0107879638671875
I0210 03:47:46.726680 140277044971264 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.16330072283744812, loss=1.981318473815918
I0210 03:48:22.532324 140277053363968 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.19039864838123322, loss=1.961082935333252
I0210 03:48:58.348036 140277044971264 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.1635657697916031, loss=1.9509408473968506
I0210 03:49:34.162464 140277053363968 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.24637454748153687, loss=2.0077898502349854
I0210 03:50:09.985236 140277044971264 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.1885916292667389, loss=2.0000996589660645
I0210 03:50:45.792126 140277053363968 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18146315217018127, loss=1.9353910684585571
I0210 03:51:21.604944 140277044971264 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.16627497971057892, loss=2.0649445056915283
I0210 03:51:57.419071 140277053363968 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.18481875956058502, loss=2.0029289722442627
I0210 03:52:33.237926 140277044971264 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.16961900889873505, loss=2.1234114170074463
I0210 03:53:09.068029 140277053363968 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.21337038278579712, loss=2.0339207649230957
I0210 03:53:44.887994 140277044971264 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.27550259232521057, loss=1.9592686891555786
I0210 03:54:20.709701 140277053363968 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.1926233470439911, loss=2.05649995803833
I0210 03:54:56.534987 140277044971264 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.16552594304084778, loss=1.9451329708099365
I0210 03:55:32.369695 140277053363968 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.26976966857910156, loss=2.0286359786987305
I0210 03:56:08.235684 140277044971264 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.16098833084106445, loss=1.9432700872421265
I0210 03:56:44.061535 140277053363968 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.2088007926940918, loss=1.9318588972091675
I0210 03:57:19.872728 140277044971264 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.20093682408332825, loss=1.9709991216659546
I0210 03:57:55.700107 140277053363968 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.24372310936450958, loss=1.967916488647461
I0210 03:58:31.516658 140277044971264 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.1694362461566925, loss=2.1317930221557617
I0210 03:58:39.836160 140446903760704 spec.py:321] Evaluating on the training split.
I0210 03:58:42.842364 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:02:00.000956 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 04:02:02.723625 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:04:28.401738 140446903760704 spec.py:349] Evaluating on the test split.
I0210 04:04:31.098341 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:06:37.991511 140446903760704 submission_runner.py:408] Time since start: 7420.88s, 	Step: 11725, 	{'train/accuracy': 0.6145750284194946, 'train/loss': 1.9365715980529785, 'train/bleu': 29.91781370647072, 'validation/accuracy': 0.637115478515625, 'validation/loss': 1.762860894203186, 'validation/bleu': 26.47492510250528, 'validation/num_examples': 3000, 'test/accuracy': 0.6475510001182556, 'test/loss': 1.691665768623352, 'test/bleu': 25.765832245270712, 'test/num_examples': 3003, 'score': 4232.526937961578, 'total_duration': 7420.881967782974, 'accumulated_submission_time': 4232.526937961578, 'accumulated_eval_time': 3187.856215953827, 'accumulated_logging_time': 0.12502813339233398}
I0210 04:06:38.008032 140277053363968 logging_writer.py:48] [11725] accumulated_eval_time=3187.856216, accumulated_logging_time=0.125028, accumulated_submission_time=4232.526938, global_step=11725, preemption_count=0, score=4232.526938, test/accuracy=0.647551, test/bleu=25.765832, test/loss=1.691666, test/num_examples=3003, total_duration=7420.881968, train/accuracy=0.614575, train/bleu=29.917814, train/loss=1.936572, validation/accuracy=0.637115, validation/bleu=26.474925, validation/loss=1.762861, validation/num_examples=3000
I0210 04:07:05.099783 140277044971264 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.19299785792827606, loss=1.9435652494430542
I0210 04:07:40.870714 140277053363968 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.19177202880382538, loss=1.9948642253875732
I0210 04:08:16.694547 140277044971264 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.19380725920200348, loss=2.022371768951416
I0210 04:08:52.532944 140277053363968 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3246472179889679, loss=1.8829199075698853
I0210 04:09:28.366885 140277044971264 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3261016011238098, loss=1.9506897926330566
I0210 04:10:04.192284 140277053363968 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1800745278596878, loss=1.987250566482544
I0210 04:10:40.001801 140277044971264 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.1867361068725586, loss=1.9227327108383179
I0210 04:11:15.858807 140277053363968 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1837216317653656, loss=1.9609296321868896
I0210 04:11:51.693642 140277044971264 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.24742670357227325, loss=1.947903037071228
I0210 04:12:27.522347 140277053363968 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.18589825928211212, loss=1.966381549835205
I0210 04:13:03.351269 140277044971264 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.2100219428539276, loss=1.8960721492767334
I0210 04:13:39.218048 140277053363968 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.2204819768667221, loss=1.972501277923584
I0210 04:14:15.064892 140277044971264 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.22932271659374237, loss=2.014988660812378
I0210 04:14:50.895336 140277053363968 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.1650691032409668, loss=1.9739807844161987
I0210 04:15:26.718860 140277044971264 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.18398994207382202, loss=2.0148518085479736
I0210 04:16:02.577095 140277053363968 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.16636168956756592, loss=1.908424973487854
I0210 04:16:38.428852 140277044971264 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.18250243365764618, loss=1.9699087142944336
I0210 04:17:14.280178 140277053363968 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.17617689073085785, loss=1.9234946966171265
I0210 04:17:50.112387 140277044971264 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.22459672391414642, loss=1.8980934619903564
I0210 04:18:25.941191 140277053363968 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.20858336985111237, loss=1.939026951789856
I0210 04:19:01.878736 140277044971264 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.23441916704177856, loss=1.9721226692199707
I0210 04:19:37.678534 140277053363968 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2148730456829071, loss=1.9795762300491333
I0210 04:20:13.527986 140277044971264 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.19251343607902527, loss=1.8942230939865112
I0210 04:20:38.294189 140446903760704 spec.py:321] Evaluating on the training split.
I0210 04:20:41.295379 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:23:24.162374 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 04:23:26.873407 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:25:55.923828 140446903760704 spec.py:349] Evaluating on the test split.
I0210 04:25:58.630697 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:28:10.430263 140446903760704 submission_runner.py:408] Time since start: 8713.32s, 	Step: 14071, 	{'train/accuracy': 0.6287801861763, 'train/loss': 1.8265225887298584, 'train/bleu': 30.574075132997642, 'validation/accuracy': 0.6427942514419556, 'validation/loss': 1.715195655822754, 'validation/bleu': 27.20454930807229, 'validation/num_examples': 3000, 'test/accuracy': 0.6514670848846436, 'test/loss': 1.646348476409912, 'test/bleu': 26.01923512162698, 'test/num_examples': 3003, 'score': 5072.729242563248, 'total_duration': 8713.320725440979, 'accumulated_submission_time': 5072.729242563248, 'accumulated_eval_time': 3639.9922440052032, 'accumulated_logging_time': 0.15134167671203613}
I0210 04:28:10.447586 140277053363968 logging_writer.py:48] [14071] accumulated_eval_time=3639.992244, accumulated_logging_time=0.151342, accumulated_submission_time=5072.729243, global_step=14071, preemption_count=0, score=5072.729243, test/accuracy=0.651467, test/bleu=26.019235, test/loss=1.646348, test/num_examples=3003, total_duration=8713.320725, train/accuracy=0.628780, train/bleu=30.574075, train/loss=1.826523, validation/accuracy=0.642794, validation/bleu=27.204549, validation/loss=1.715196, validation/num_examples=3000
I0210 04:28:21.157969 140277044971264 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.22595669329166412, loss=1.9074195623397827
I0210 04:28:56.873028 140277053363968 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.19489040970802307, loss=1.938562273979187
I0210 04:29:32.711516 140277044971264 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.19261550903320312, loss=1.8345452547073364
I0210 04:30:08.580142 140277053363968 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.1855194866657257, loss=1.9216746091842651
I0210 04:30:44.413283 140277044971264 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.25239911675453186, loss=1.904233694076538
I0210 04:31:20.279996 140277053363968 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.20123572647571564, loss=1.923545241355896
I0210 04:31:56.163572 140277044971264 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2080586701631546, loss=1.869762897491455
I0210 04:32:32.057994 140277053363968 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.22569942474365234, loss=1.918207049369812
I0210 04:33:07.898778 140277044971264 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.18403182923793793, loss=1.938432216644287
I0210 04:33:43.760792 140277053363968 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.27031373977661133, loss=1.8427008390426636
I0210 04:34:19.614047 140277044971264 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.43428194522857666, loss=1.866886854171753
I0210 04:34:55.505984 140277053363968 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2554181218147278, loss=2.033151388168335
I0210 04:35:31.378755 140277044971264 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.18441011011600494, loss=1.9454822540283203
I0210 04:36:07.205540 140277053363968 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.27476567029953003, loss=1.8144936561584473
I0210 04:36:43.024352 140277044971264 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.18747442960739136, loss=1.9472835063934326
I0210 04:37:18.912645 140277053363968 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.21781380474567413, loss=1.9471588134765625
I0210 04:37:54.822590 140277044971264 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.22743402421474457, loss=1.854365587234497
I0210 04:38:30.723137 140277053363968 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.22701001167297363, loss=1.9239227771759033
I0210 04:39:06.613539 140277044971264 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.1991579532623291, loss=1.9929587841033936
I0210 04:39:42.467711 140277053363968 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.20067991316318512, loss=1.9199769496917725
I0210 04:40:18.330100 140277044971264 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1811235100030899, loss=1.9777582883834839
I0210 04:40:54.179445 140277053363968 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.22324155271053314, loss=1.8369495868682861
I0210 04:41:30.014590 140277044971264 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.18751463294029236, loss=1.8707791566848755
I0210 04:42:05.865729 140277053363968 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.22801747918128967, loss=1.8889256715774536
I0210 04:42:10.604198 140446903760704 spec.py:321] Evaluating on the training split.
I0210 04:42:13.607326 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:45:36.897637 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 04:45:39.578262 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:48:09.363702 140446903760704 spec.py:349] Evaluating on the test split.
I0210 04:48:12.074961 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 04:50:27.821734 140446903760704 submission_runner.py:408] Time since start: 10050.71s, 	Step: 16415, 	{'train/accuracy': 0.6283307075500488, 'train/loss': 1.8313599824905396, 'train/bleu': 30.64059289155258, 'validation/accuracy': 0.6439225673675537, 'validation/loss': 1.6987574100494385, 'validation/bleu': 26.862268504080657, 'validation/num_examples': 3000, 'test/accuracy': 0.6555575132369995, 'test/loss': 1.6271188259124756, 'test/bleu': 25.974975993637944, 'test/num_examples': 3003, 'score': 5912.799705505371, 'total_duration': 10050.71216583252, 'accumulated_submission_time': 5912.799705505371, 'accumulated_eval_time': 4137.2096972465515, 'accumulated_logging_time': 0.17881011962890625}
I0210 04:50:27.842275 140277044971264 logging_writer.py:48] [16415] accumulated_eval_time=4137.209697, accumulated_logging_time=0.178810, accumulated_submission_time=5912.799706, global_step=16415, preemption_count=0, score=5912.799706, test/accuracy=0.655558, test/bleu=25.974976, test/loss=1.627119, test/num_examples=3003, total_duration=10050.712166, train/accuracy=0.628331, train/bleu=30.640593, train/loss=1.831360, validation/accuracy=0.643923, validation/bleu=26.862269, validation/loss=1.698757, validation/num_examples=3000
I0210 04:50:58.517798 140277053363968 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.19168391823768616, loss=1.8944545984268188
I0210 04:51:34.298429 140277044971264 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.18028853833675385, loss=1.8714925050735474
I0210 04:52:10.141296 140277053363968 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.21673458814620972, loss=1.827631950378418
I0210 04:52:45.941388 140277044971264 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.329058438539505, loss=1.9375628232955933
I0210 04:53:21.772951 140277053363968 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.1709548383951187, loss=1.8538552522659302
I0210 04:53:57.621483 140277044971264 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.30413931608200073, loss=1.9551129341125488
I0210 04:54:33.437486 140277053363968 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.18689881265163422, loss=1.9208309650421143
I0210 04:55:09.290766 140277044971264 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.259031742811203, loss=1.8354740142822266
I0210 04:55:45.144020 140277053363968 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.1808958500623703, loss=1.919566035270691
I0210 04:56:21.033201 140277044971264 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.22248247265815735, loss=1.8451778888702393
I0210 04:56:56.888334 140277053363968 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.21495196223258972, loss=1.8623242378234863
I0210 04:57:32.712579 140277044971264 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.17660441994667053, loss=1.9294346570968628
I0210 04:58:08.553337 140277053363968 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.18573880195617676, loss=1.8086148500442505
I0210 04:58:44.360451 140277044971264 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.203980952501297, loss=1.89669930934906
I0210 04:59:20.214020 140277053363968 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.2163642793893814, loss=1.8815449476242065
I0210 04:59:56.191236 140277044971264 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.18332046270370483, loss=1.8806285858154297
I0210 05:00:32.026925 140277053363968 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.19860760867595673, loss=1.90254545211792
I0210 05:01:07.857867 140277044971264 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3258190453052521, loss=1.8817789554595947
I0210 05:01:43.744652 140277053363968 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.1999870091676712, loss=1.8637553453445435
I0210 05:02:19.598193 140277044971264 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2385229617357254, loss=1.852277398109436
I0210 05:02:55.441301 140277053363968 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.21204522252082825, loss=1.8945534229278564
I0210 05:03:31.301158 140277044971264 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3600625991821289, loss=1.8898067474365234
I0210 05:04:07.166378 140277053363968 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.23509778082370758, loss=1.8485281467437744
I0210 05:04:28.015240 140446903760704 spec.py:321] Evaluating on the training split.
I0210 05:04:31.017527 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:08:18.092707 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 05:08:20.823757 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:11:11.712143 140446903760704 spec.py:349] Evaluating on the test split.
I0210 05:11:14.435662 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:13:48.643913 140446903760704 submission_runner.py:408] Time since start: 11451.53s, 	Step: 18760, 	{'train/accuracy': 0.6313144564628601, 'train/loss': 1.8021981716156006, 'train/bleu': 30.330178704263602, 'validation/accuracy': 0.6500601172447205, 'validation/loss': 1.666564702987671, 'validation/bleu': 27.40282885917218, 'validation/num_examples': 3000, 'test/accuracy': 0.6605310440063477, 'test/loss': 1.594834566116333, 'test/bleu': 26.84595365712397, 'test/num_examples': 3003, 'score': 6752.885207414627, 'total_duration': 11451.534358024597, 'accumulated_submission_time': 6752.885207414627, 'accumulated_eval_time': 4697.838307380676, 'accumulated_logging_time': 0.2108919620513916}
I0210 05:13:48.661906 140277044971264 logging_writer.py:48] [18760] accumulated_eval_time=4697.838307, accumulated_logging_time=0.210892, accumulated_submission_time=6752.885207, global_step=18760, preemption_count=0, score=6752.885207, test/accuracy=0.660531, test/bleu=26.845954, test/loss=1.594835, test/num_examples=3003, total_duration=11451.534358, train/accuracy=0.631314, train/bleu=30.330179, train/loss=1.802198, validation/accuracy=0.650060, validation/bleu=27.402829, validation/loss=1.666565, validation/num_examples=3000
I0210 05:14:03.310014 140277053363968 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.26309657096862793, loss=1.8952816724777222
I0210 05:14:39.013043 140277044971264 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.1693355292081833, loss=1.792976975440979
I0210 05:15:14.827420 140277053363968 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.19731202721595764, loss=1.8936699628829956
I0210 05:15:50.659000 140277044971264 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.20182426273822784, loss=1.8861620426177979
I0210 05:16:26.469147 140277053363968 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.18071012198925018, loss=1.8231096267700195
I0210 05:17:02.271570 140277044971264 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.18525588512420654, loss=1.8299216032028198
I0210 05:17:38.108631 140277053363968 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1709691286087036, loss=1.9370931386947632
I0210 05:18:13.949126 140277044971264 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.1819419264793396, loss=1.927958607673645
I0210 05:18:49.814618 140277053363968 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1640455573797226, loss=1.8738069534301758
I0210 05:19:25.621674 140277044971264 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.1929655373096466, loss=1.9748079776763916
I0210 05:20:01.463824 140277053363968 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.1894545704126358, loss=1.8424403667449951
I0210 05:20:37.320774 140277044971264 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.2651709020137787, loss=1.885068655014038
I0210 05:21:13.152065 140277053363968 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.238292396068573, loss=1.9226616621017456
I0210 05:21:48.971910 140277044971264 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.20669114589691162, loss=1.8042584657669067
I0210 05:22:24.812411 140277053363968 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.17859894037246704, loss=1.8830039501190186
I0210 05:23:00.640564 140277044971264 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.1901792287826538, loss=1.8991706371307373
I0210 05:23:36.467095 140277053363968 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.18295076489448547, loss=1.9694169759750366
I0210 05:24:12.282321 140277044971264 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.20950745046138763, loss=1.844698429107666
I0210 05:24:48.078262 140277053363968 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.2050911784172058, loss=1.8861466646194458
I0210 05:25:23.927013 140277044971264 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.19239403307437897, loss=1.8261312246322632
I0210 05:25:59.716945 140277053363968 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.1743331104516983, loss=1.8557794094085693
I0210 05:26:35.533243 140277044971264 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.19420641660690308, loss=1.7814207077026367
I0210 05:27:11.381370 140277053363968 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.18902075290679932, loss=1.8581011295318604
I0210 05:27:47.206363 140277044971264 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.2860686779022217, loss=1.9276793003082275
I0210 05:27:48.725280 140446903760704 spec.py:321] Evaluating on the training split.
I0210 05:27:51.726877 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:31:42.820938 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 05:31:45.515226 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:34:50.732342 140446903760704 spec.py:349] Evaluating on the test split.
I0210 05:34:53.429458 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:38:39.022960 140446903760704 submission_runner.py:408] Time since start: 12941.91s, 	Step: 21106, 	{'train/accuracy': 0.6347932815551758, 'train/loss': 1.7635672092437744, 'train/bleu': 31.21414025678416, 'validation/accuracy': 0.6517339944839478, 'validation/loss': 1.6439547538757324, 'validation/bleu': 27.60810916818256, 'validation/num_examples': 3000, 'test/accuracy': 0.6628551483154297, 'test/loss': 1.5742295980453491, 'test/bleu': 26.805785981781558, 'test/num_examples': 3003, 'score': 7592.865230083466, 'total_duration': 12941.91342139244, 'accumulated_submission_time': 7592.865230083466, 'accumulated_eval_time': 5348.135931015015, 'accumulated_logging_time': 0.23898673057556152}
I0210 05:38:39.041574 140277053363968 logging_writer.py:48] [21106] accumulated_eval_time=5348.135931, accumulated_logging_time=0.238987, accumulated_submission_time=7592.865230, global_step=21106, preemption_count=0, score=7592.865230, test/accuracy=0.662855, test/bleu=26.805786, test/loss=1.574230, test/num_examples=3003, total_duration=12941.913421, train/accuracy=0.634793, train/bleu=31.214140, train/loss=1.763567, validation/accuracy=0.651734, validation/bleu=27.608109, validation/loss=1.643955, validation/num_examples=3000
I0210 05:39:12.886712 140277044971264 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.23836563527584076, loss=1.9169734716415405
I0210 05:39:48.657246 140277053363968 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.3339751064777374, loss=1.8891654014587402
I0210 05:40:24.522202 140277044971264 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.20910049974918365, loss=1.9094593524932861
I0210 05:41:00.422656 140277053363968 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2064894139766693, loss=1.934769630432129
I0210 05:41:36.251282 140277044971264 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.23036755621433258, loss=1.79599130153656
I0210 05:42:12.100759 140277053363968 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.22284391522407532, loss=1.7899169921875
I0210 05:42:47.933752 140277044971264 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.23570670187473297, loss=1.8270645141601562
I0210 05:43:23.767637 140277053363968 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.235732302069664, loss=1.9210530519485474
I0210 05:43:59.607874 140277044971264 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.31564560532569885, loss=1.822075366973877
I0210 05:44:35.429238 140277053363968 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.22895920276641846, loss=1.9504868984222412
I0210 05:45:11.237947 140277044971264 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.20287975668907166, loss=1.9047054052352905
I0210 05:45:47.049947 140277053363968 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.19464130699634552, loss=1.7941718101501465
I0210 05:46:22.862177 140277044971264 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.20601969957351685, loss=1.8091566562652588
I0210 05:46:58.698688 140277053363968 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.1904607117176056, loss=1.8156615495681763
I0210 05:47:34.567279 140277044971264 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.2281452715396881, loss=1.8110028505325317
I0210 05:48:10.392812 140277053363968 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.20869533717632294, loss=1.9390515089035034
I0210 05:48:46.228284 140277044971264 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.31685546040534973, loss=1.8307840824127197
I0210 05:49:22.045994 140277053363968 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.1817735880613327, loss=1.843699336051941
I0210 05:49:57.871860 140277044971264 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.23401515185832977, loss=1.8403749465942383
I0210 05:50:33.706014 140277053363968 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2222268283367157, loss=1.8536412715911865
I0210 05:51:09.548757 140277044971264 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.1851055771112442, loss=1.8739874362945557
I0210 05:51:45.368079 140277053363968 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.22384870052337646, loss=1.8930633068084717
I0210 05:52:21.226938 140277044971264 logging_writer.py:48] [23400] global_step=23400, grad_norm=6.382744312286377, loss=2.491070508956909
I0210 05:52:39.217614 140446903760704 spec.py:321] Evaluating on the training split.
I0210 05:52:42.226520 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:55:47.247618 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 05:55:49.948033 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 05:58:22.845309 140446903760704 spec.py:349] Evaluating on the test split.
I0210 05:58:25.563629 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 06:00:47.565912 140446903760704 submission_runner.py:408] Time since start: 14270.46s, 	Step: 23452, 	{'train/accuracy': 0.6305269598960876, 'train/loss': 1.790299415588379, 'train/bleu': 30.610692115272517, 'validation/accuracy': 0.6529738903045654, 'validation/loss': 1.6360857486724854, 'validation/bleu': 27.64664612249136, 'validation/num_examples': 3000, 'test/accuracy': 0.6636337637901306, 'test/loss': 1.5664596557617188, 'test/bleu': 27.270381340846022, 'test/num_examples': 3003, 'score': 8432.95629954338, 'total_duration': 14270.456376552582, 'accumulated_submission_time': 8432.95629954338, 'accumulated_eval_time': 5836.484181642532, 'accumulated_logging_time': 0.26767778396606445}
I0210 06:00:47.583932 140277053363968 logging_writer.py:48] [23452] accumulated_eval_time=5836.484182, accumulated_logging_time=0.267678, accumulated_submission_time=8432.956300, global_step=23452, preemption_count=0, score=8432.956300, test/accuracy=0.663634, test/bleu=27.270381, test/loss=1.566460, test/num_examples=3003, total_duration=14270.456377, train/accuracy=0.630527, train/bleu=30.610692, train/loss=1.790299, validation/accuracy=0.652974, validation/bleu=27.646646, validation/loss=1.636086, validation/num_examples=3000
I0210 06:01:05.085605 140277044971264 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.17045892775058746, loss=1.7641804218292236
I0210 06:01:40.827143 140277053363968 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.23988203704357147, loss=1.9193158149719238
I0210 06:02:16.676331 140277044971264 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2165716588497162, loss=1.8794422149658203
I0210 06:02:52.479191 140277053363968 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.20052099227905273, loss=1.828113317489624
I0210 06:03:28.323291 140277044971264 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.16866230964660645, loss=1.7859020233154297
I0210 06:04:04.141136 140277053363968 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.21708562970161438, loss=1.7933279275894165
I0210 06:04:39.951901 140277044971264 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.1979176551103592, loss=1.8165357112884521
I0210 06:05:15.767763 140277053363968 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.19598746299743652, loss=1.8536882400512695
I0210 06:05:51.599721 140277044971264 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2163906693458557, loss=1.819484829902649
I0210 06:06:27.449573 140277053363968 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.20462597906589508, loss=1.8030484914779663
I0210 06:07:03.284084 140277044971264 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.22031790018081665, loss=1.7952308654785156
I0210 06:07:39.100120 140277053363968 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.21170707046985626, loss=1.7590357065200806
I0210 06:08:14.933574 140277044971264 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.21424776315689087, loss=1.8122140169143677
I0210 06:08:50.742346 140277053363968 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.20844055712223053, loss=1.8054488897323608
I0210 06:09:26.580894 140277044971264 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.23217380046844482, loss=1.8602689504623413
I0210 06:10:02.422328 140277053363968 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.1875225305557251, loss=1.8269425630569458
I0210 06:10:38.270576 140277044971264 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.1908753663301468, loss=1.822135329246521
I0210 06:11:14.086891 140277053363968 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2261938452720642, loss=1.7682760953903198
I0210 06:11:49.932928 140277044971264 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.3154134452342987, loss=1.9687069654464722
I0210 06:12:25.802891 140277053363968 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.21681146323680878, loss=1.855024814605713
I0210 06:13:01.655020 140277044971264 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2707076370716095, loss=1.8248989582061768
I0210 06:13:37.517579 140277053363968 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.1738380640745163, loss=1.8195751905441284
I0210 06:14:13.341590 140277044971264 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.22176530957221985, loss=1.8360694646835327
I0210 06:14:47.788787 140446903760704 spec.py:321] Evaluating on the training split.
I0210 06:14:50.796725 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 06:18:00.579761 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 06:18:03.292583 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 06:20:47.986471 140446903760704 spec.py:349] Evaluating on the test split.
I0210 06:20:50.711705 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 06:23:27.461539 140446903760704 submission_runner.py:408] Time since start: 15630.35s, 	Step: 25798, 	{'train/accuracy': 0.642456591129303, 'train/loss': 1.7020819187164307, 'train/bleu': 31.552448034746874, 'validation/accuracy': 0.6566688418388367, 'validation/loss': 1.6203370094299316, 'validation/bleu': 28.153017370131508, 'validation/num_examples': 3000, 'test/accuracy': 0.6658416390419006, 'test/loss': 1.5436705350875854, 'test/bleu': 27.358169319863094, 'test/num_examples': 3003, 'score': 9273.077693939209, 'total_duration': 15630.35198712349, 'accumulated_submission_time': 9273.077693939209, 'accumulated_eval_time': 6356.156872987747, 'accumulated_logging_time': 0.29597902297973633}
I0210 06:23:27.479723 140277053363968 logging_writer.py:48] [25798] accumulated_eval_time=6356.156873, accumulated_logging_time=0.295979, accumulated_submission_time=9273.077694, global_step=25798, preemption_count=0, score=9273.077694, test/accuracy=0.665842, test/bleu=27.358169, test/loss=1.543671, test/num_examples=3003, total_duration=15630.351987, train/accuracy=0.642457, train/bleu=31.552448, train/loss=1.702082, validation/accuracy=0.656669, validation/bleu=28.153017, validation/loss=1.620337, validation/num_examples=3000
I0210 06:23:28.565840 140277044971264 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.20706161856651306, loss=1.911737084388733
I0210 06:24:04.268202 140277053363968 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.24203021824359894, loss=1.8343603610992432
I0210 06:24:40.046966 140277044971264 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.41003310680389404, loss=1.7918380498886108
I0210 06:25:15.851329 140277053363968 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.1783648282289505, loss=1.817918062210083
I0210 06:25:51.678993 140277044971264 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.22213076055049896, loss=1.8291157484054565
I0210 06:26:27.511810 140277053363968 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.1932305544614792, loss=1.819093108177185
I0210 06:27:03.359308 140277044971264 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.18291234970092773, loss=1.8443814516067505
I0210 06:27:39.175167 140277053363968 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.34735390543937683, loss=1.7880852222442627
I0210 06:28:15.022866 140277044971264 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.24834860861301422, loss=1.7640507221221924
I0210 06:28:50.842286 140277053363968 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.2172578126192093, loss=1.8565086126327515
I0210 06:29:26.661793 140277044971264 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.21435867249965668, loss=1.8730543851852417
I0210 06:30:02.479344 140277053363968 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2171809822320938, loss=1.8192893266677856
I0210 06:30:38.303232 140277044971264 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.22169137001037598, loss=1.8015774488449097
I0210 06:31:14.140963 140277053363968 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.18295864760875702, loss=1.7836172580718994
I0210 06:31:49.962658 140277044971264 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2765210270881653, loss=1.7829406261444092
I0210 06:32:25.791890 140277053363968 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.21197685599327087, loss=1.7476236820220947
I0210 06:33:01.649686 140277044971264 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.19959001243114471, loss=1.8090566396713257
I0210 06:33:37.473581 140277053363968 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.1980738490819931, loss=1.8131372928619385
I0210 06:34:13.337456 140277044971264 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.2228809893131256, loss=1.8328171968460083
I0210 06:34:49.168236 140277053363968 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.17498774826526642, loss=1.81575608253479
I0210 06:35:24.979557 140277044971264 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.1906624585390091, loss=1.8289697170257568
I0210 06:36:00.820870 140277053363968 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.2126074880361557, loss=1.7640341520309448
I0210 06:36:36.650661 140277044971264 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.21542349457740784, loss=1.9628256559371948
I0210 06:37:12.494051 140277053363968 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.17567437887191772, loss=1.7393709421157837
I0210 06:37:27.625610 140446903760704 spec.py:321] Evaluating on the training split.
I0210 06:37:30.627665 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 06:40:02.624497 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 06:40:05.328921 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 06:42:18.298018 140446903760704 spec.py:349] Evaluating on the test split.
I0210 06:42:21.010327 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 06:44:52.180776 140446903760704 submission_runner.py:408] Time since start: 16915.07s, 	Step: 28144, 	{'train/accuracy': 0.6392359733581543, 'train/loss': 1.7401916980743408, 'train/bleu': 31.22346015081715, 'validation/accuracy': 0.6564580798149109, 'validation/loss': 1.6131483316421509, 'validation/bleu': 27.81124516471783, 'validation/num_examples': 3000, 'test/accuracy': 0.6691185832023621, 'test/loss': 1.5272434949874878, 'test/bleu': 27.576740566813548, 'test/num_examples': 3003, 'score': 10113.140311479568, 'total_duration': 16915.07120656967, 'accumulated_submission_time': 10113.140311479568, 'accumulated_eval_time': 6800.711962461472, 'accumulated_logging_time': 0.3239438533782959}
I0210 06:44:52.203836 140277044971264 logging_writer.py:48] [28144] accumulated_eval_time=6800.711962, accumulated_logging_time=0.323944, accumulated_submission_time=10113.140311, global_step=28144, preemption_count=0, score=10113.140311, test/accuracy=0.669119, test/bleu=27.576741, test/loss=1.527243, test/num_examples=3003, total_duration=16915.071207, train/accuracy=0.639236, train/bleu=31.223460, train/loss=1.740192, validation/accuracy=0.656458, validation/bleu=27.811245, validation/loss=1.613148, validation/num_examples=3000
I0210 06:45:12.565071 140277053363968 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.1807689219713211, loss=1.7793554067611694
I0210 06:45:48.334229 140277044971264 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.2102949619293213, loss=1.8100528717041016
I0210 06:46:24.139783 140277053363968 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.22605468332767487, loss=1.7805590629577637
I0210 06:46:59.954783 140277044971264 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.174215078353882, loss=1.7447762489318848
I0210 06:47:35.757369 140277053363968 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.23555713891983032, loss=1.93026602268219
I0210 06:48:11.575568 140277044971264 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.20933231711387634, loss=1.8221876621246338
I0210 06:48:47.430296 140277053363968 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.19860413670539856, loss=1.8465509414672852
I0210 06:49:23.297789 140277044971264 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.2546660304069519, loss=1.9100011587142944
I0210 06:49:59.142429 140277053363968 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.18691986799240112, loss=1.7661314010620117
I0210 06:50:34.960583 140277044971264 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.18272149562835693, loss=1.783478021621704
I0210 06:51:10.782200 140277053363968 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.2637229859828949, loss=1.8935307264328003
I0210 06:51:46.617866 140277044971264 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.23065884411334991, loss=1.8233190774917603
I0210 06:52:22.467619 140277053363968 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.208489328622818, loss=1.9110045433044434
I0210 06:52:58.311368 140277044971264 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.21788647770881653, loss=1.7372627258300781
I0210 06:53:34.153759 140277053363968 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.18156002461910248, loss=1.7840325832366943
I0210 06:54:09.987753 140277044971264 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.29384616017341614, loss=1.826070785522461
I0210 06:54:45.819509 140277053363968 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.1900390386581421, loss=1.8212432861328125
I0210 06:55:21.684474 140277044971264 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.20062124729156494, loss=1.78560209274292
I0210 06:55:57.514142 140277053363968 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.24939650297164917, loss=1.8017996549606323
I0210 06:56:33.372740 140277044971264 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19754262268543243, loss=1.836818814277649
I0210 06:57:09.239365 140277053363968 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.24444739520549774, loss=1.7743812799453735
I0210 06:57:45.085301 140277044971264 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.21561329066753387, loss=1.7699265480041504
I0210 06:58:20.897080 140277053363968 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.19283074140548706, loss=1.8522071838378906
I0210 06:58:52.488197 140446903760704 spec.py:321] Evaluating on the training split.
I0210 06:58:55.506220 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:01:51.863433 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 07:01:54.560684 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:04:22.812271 140446903760704 spec.py:349] Evaluating on the test split.
I0210 07:04:25.524656 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:06:55.975622 140446903760704 submission_runner.py:408] Time since start: 18238.87s, 	Step: 30490, 	{'train/accuracy': 0.6276158094406128, 'train/loss': 1.8166896104812622, 'train/bleu': 29.779899990113677, 'validation/accuracy': 0.6502957344055176, 'validation/loss': 1.6435110569000244, 'validation/bleu': 26.92453743681419, 'validation/num_examples': 3000, 'test/accuracy': 0.6619836091995239, 'test/loss': 1.5729281902313232, 'test/bleu': 26.57815801463406, 'test/num_examples': 3003, 'score': 10953.335748672485, 'total_duration': 18238.866058826447, 'accumulated_submission_time': 10953.335748672485, 'accumulated_eval_time': 7284.199323177338, 'accumulated_logging_time': 0.3592972755432129}
I0210 07:06:55.995273 140277044971264 logging_writer.py:48] [30490] accumulated_eval_time=7284.199323, accumulated_logging_time=0.359297, accumulated_submission_time=10953.335749, global_step=30490, preemption_count=0, score=10953.335749, test/accuracy=0.661984, test/bleu=26.578158, test/loss=1.572928, test/num_examples=3003, total_duration=18238.866059, train/accuracy=0.627616, train/bleu=29.779900, train/loss=1.816690, validation/accuracy=0.650296, validation/bleu=26.924537, validation/loss=1.643511, validation/num_examples=3000
I0210 07:06:59.946357 140277053363968 logging_writer.py:48] [30500] global_step=30500, grad_norm=5.568124294281006, loss=1.9024951457977295
I0210 07:07:35.650051 140277044971264 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.176247701048851, loss=1.7457510232925415
I0210 07:08:11.580200 140277053363968 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.20330803096294403, loss=1.826279878616333
I0210 07:08:47.440993 140277044971264 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.18275272846221924, loss=1.7535018920898438
I0210 07:09:23.272142 140277053363968 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.20094572007656097, loss=1.759454369544983
I0210 07:09:59.090430 140277044971264 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.2370559275150299, loss=1.7864936590194702
I0210 07:10:34.939711 140277053363968 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.2525908052921295, loss=1.7903413772583008
I0210 07:11:10.755319 140277044971264 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.1975700557231903, loss=1.7577714920043945
I0210 07:11:46.598914 140277053363968 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.19945023953914642, loss=1.835347056388855
I0210 07:12:22.425173 140277044971264 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.20428572595119476, loss=1.8171417713165283
I0210 07:12:58.218743 140277053363968 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.28779923915863037, loss=1.7696548700332642
I0210 07:13:34.083809 140277044971264 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.21218912303447723, loss=1.7576557397842407
I0210 07:14:09.945885 140277053363968 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.23372600972652435, loss=1.8551125526428223
I0210 07:14:45.768166 140277044971264 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.6996105909347534, loss=1.845931053161621
I0210 07:15:21.624389 140277053363968 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.17459869384765625, loss=1.7898285388946533
I0210 07:15:57.458564 140277044971264 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.20423349738121033, loss=1.8009244203567505
I0210 07:16:33.309667 140277053363968 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2672747075557709, loss=1.8215456008911133
I0210 07:17:09.145294 140277044971264 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.20484967529773712, loss=1.8060518503189087
I0210 07:17:44.943885 140277053363968 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.18854422867298126, loss=1.7169955968856812
I0210 07:18:20.765999 140277044971264 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2049248218536377, loss=1.931564450263977
I0210 07:18:56.581391 140277053363968 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.24452824890613556, loss=1.7662429809570312
I0210 07:19:32.392656 140277044971264 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.20817723870277405, loss=1.7570750713348389
I0210 07:20:08.201878 140277053363968 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.2853432893753052, loss=1.7759101390838623
I0210 07:20:44.041317 140277044971264 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.21414846181869507, loss=1.8871818780899048
I0210 07:20:56.287910 140446903760704 spec.py:321] Evaluating on the training split.
I0210 07:20:59.288804 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:25:03.154079 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 07:25:05.880803 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:27:34.598715 140446903760704 spec.py:349] Evaluating on the test split.
I0210 07:27:37.306159 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:29:54.010099 140446903760704 submission_runner.py:408] Time since start: 19616.90s, 	Step: 32836, 	{'train/accuracy': 0.6437987089157104, 'train/loss': 1.7077805995941162, 'train/bleu': 31.70166116605699, 'validation/accuracy': 0.6607481837272644, 'validation/loss': 1.5932337045669556, 'validation/bleu': 28.223498811079704, 'validation/num_examples': 3000, 'test/accuracy': 0.6721515655517578, 'test/loss': 1.512355923652649, 'test/bleu': 27.20909834912664, 'test/num_examples': 3003, 'score': 11793.542217969894, 'total_duration': 19616.90053844452, 'accumulated_submission_time': 11793.542217969894, 'accumulated_eval_time': 7821.921438455582, 'accumulated_logging_time': 0.39046382904052734}
I0210 07:29:54.030564 140277053363968 logging_writer.py:48] [32836] accumulated_eval_time=7821.921438, accumulated_logging_time=0.390464, accumulated_submission_time=11793.542218, global_step=32836, preemption_count=0, score=11793.542218, test/accuracy=0.672152, test/bleu=27.209098, test/loss=1.512356, test/num_examples=3003, total_duration=19616.900538, train/accuracy=0.643799, train/bleu=31.701661, train/loss=1.707781, validation/accuracy=0.660748, validation/bleu=28.223499, validation/loss=1.593234, validation/num_examples=3000
I0210 07:30:17.210597 140277044971264 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.1847999542951584, loss=1.7966599464416504
I0210 07:30:52.945180 140277053363968 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.19966384768486023, loss=1.8129559755325317
I0210 07:31:28.769422 140277044971264 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.20532451570034027, loss=1.8430041074752808
I0210 07:32:04.626649 140277053363968 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.19837328791618347, loss=1.9087990522384644
I0210 07:32:40.448963 140277044971264 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.20254552364349365, loss=1.786157250404358
I0210 07:33:16.283417 140277053363968 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2574922442436218, loss=1.810203194618225
I0210 07:33:52.115644 140277044971264 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.26306402683258057, loss=1.88740074634552
I0210 07:34:27.986585 140277053363968 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.21605008840560913, loss=1.7826391458511353
I0210 07:35:03.858121 140277044971264 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.1965259611606598, loss=1.7577998638153076
I0210 07:35:39.697608 140277053363968 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.19982381165027618, loss=1.786940574645996
I0210 07:36:15.682469 140277044971264 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.21045687794685364, loss=1.8281630277633667
I0210 07:36:51.506356 140277053363968 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.19987884163856506, loss=1.7865384817123413
I0210 07:37:27.334017 140277044971264 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.22584056854248047, loss=1.7697700262069702
I0210 07:38:03.164323 140277053363968 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.2182566076517105, loss=1.880591630935669
I0210 07:38:38.977282 140277044971264 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2036319524049759, loss=1.8579702377319336
I0210 07:39:14.768054 140277053363968 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.20549514889717102, loss=1.743331789970398
I0210 07:39:50.583333 140277044971264 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.2096051722764969, loss=1.8462212085723877
I0210 07:40:26.397841 140277053363968 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2095591425895691, loss=1.76705002784729
I0210 07:41:02.208861 140277044971264 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.17861855030059814, loss=1.7584953308105469
I0210 07:41:38.028129 140277053363968 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.17480260133743286, loss=1.8298704624176025
I0210 07:42:13.876489 140277044971264 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.2969330847263336, loss=1.8720502853393555
I0210 07:42:49.727501 140277053363968 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.19638358056545258, loss=1.7742489576339722
I0210 07:43:25.552850 140277044971264 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.22838003933429718, loss=1.8214552402496338
I0210 07:43:54.285283 140446903760704 spec.py:321] Evaluating on the training split.
I0210 07:43:57.289237 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:47:26.854577 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 07:47:29.553891 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:50:44.691398 140446903760704 spec.py:349] Evaluating on the test split.
I0210 07:50:47.377832 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 07:54:15.287826 140446903760704 submission_runner.py:408] Time since start: 21078.18s, 	Step: 35182, 	{'train/accuracy': 0.644922137260437, 'train/loss': 1.7093086242675781, 'train/bleu': 31.362425620829725, 'validation/accuracy': 0.6612069010734558, 'validation/loss': 1.584161639213562, 'validation/bleu': 28.028519514767073, 'validation/num_examples': 3000, 'test/accuracy': 0.6742432117462158, 'test/loss': 1.504552960395813, 'test/bleu': 27.942357040831503, 'test/num_examples': 3003, 'score': 12633.712213754654, 'total_duration': 21078.17825651169, 'accumulated_submission_time': 12633.712213754654, 'accumulated_eval_time': 8442.923906326294, 'accumulated_logging_time': 0.4212970733642578}
I0210 07:54:15.307395 140277053363968 logging_writer.py:48] [35182] accumulated_eval_time=8442.923906, accumulated_logging_time=0.421297, accumulated_submission_time=12633.712214, global_step=35182, preemption_count=0, score=12633.712214, test/accuracy=0.674243, test/bleu=27.942357, test/loss=1.504553, test/num_examples=3003, total_duration=21078.178257, train/accuracy=0.644922, train/bleu=31.362426, train/loss=1.709309, validation/accuracy=0.661207, validation/bleu=28.028520, validation/loss=1.584162, validation/num_examples=3000
I0210 07:54:22.092727 140277044971264 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.19877874851226807, loss=1.8223342895507812
I0210 07:54:57.799932 140277053363968 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.2067435383796692, loss=1.7589337825775146
I0210 07:55:33.580815 140277044971264 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.19075869023799896, loss=1.7660542726516724
I0210 07:56:09.381262 140277053363968 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.20864130556583405, loss=1.8752772808074951
I0210 07:56:45.248028 140277044971264 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.20241133868694305, loss=1.7918756008148193
I0210 07:57:21.125282 140277053363968 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.20265692472457886, loss=1.8315094709396362
I0210 07:57:56.965409 140277044971264 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.18528145551681519, loss=1.7253471612930298
I0210 07:58:32.790574 140277053363968 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.19548748433589935, loss=1.791165828704834
I0210 07:59:08.600669 140277044971264 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.18758243322372437, loss=1.731472134590149
I0210 07:59:44.399278 140277053363968 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.22477005422115326, loss=1.833808422088623
I0210 08:00:20.204319 140277044971264 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.22839629650115967, loss=1.7811291217803955
I0210 08:00:56.008113 140277053363968 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.18528877198696136, loss=1.7700027227401733
I0210 08:01:31.806597 140277044971264 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.1817188709974289, loss=1.7631089687347412
I0210 08:02:07.696198 140277053363968 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.2013964056968689, loss=1.8402996063232422
I0210 08:02:43.544106 140277044971264 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.26814284920692444, loss=1.7714993953704834
I0210 08:03:19.436803 140277053363968 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.2677665650844574, loss=1.7551368474960327
I0210 08:03:55.271005 140277044971264 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.21745000779628754, loss=1.7603782415390015
I0210 08:04:31.099380 140277053363968 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2686859369277954, loss=1.8481076955795288
I0210 08:05:06.925657 140277044971264 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.22690384089946747, loss=1.822250485420227
I0210 08:05:42.818578 140277053363968 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.2645880877971649, loss=1.6859079599380493
I0210 08:06:18.641834 140277044971264 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.18875156342983246, loss=1.8020962476730347
I0210 08:06:54.459839 140277053363968 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.1883041262626648, loss=1.7778584957122803
I0210 08:07:30.308416 140277044971264 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.20083296298980713, loss=1.814846396446228
I0210 08:08:06.139424 140277053363968 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.20760327577590942, loss=1.8339948654174805
I0210 08:08:15.534979 140446903760704 spec.py:321] Evaluating on the training split.
I0210 08:08:18.527952 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 08:11:20.599133 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 08:11:23.288074 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 08:14:02.891674 140446903760704 spec.py:349] Evaluating on the test split.
I0210 08:14:05.620223 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 08:16:26.681223 140446903760704 submission_runner.py:408] Time since start: 22409.57s, 	Step: 37528, 	{'train/accuracy': 0.6506013870239258, 'train/loss': 1.6590782403945923, 'train/bleu': 32.2079672733254, 'validation/accuracy': 0.6631659865379333, 'validation/loss': 1.5785056352615356, 'validation/bleu': 28.21083540443361, 'validation/num_examples': 3000, 'test/accuracy': 0.6755679845809937, 'test/loss': 1.4972996711730957, 'test/bleu': 27.54270861029842, 'test/num_examples': 3003, 'score': 13473.850688695908, 'total_duration': 22409.57164978981, 'accumulated_submission_time': 13473.850688695908, 'accumulated_eval_time': 8934.070060491562, 'accumulated_logging_time': 0.45255208015441895}
I0210 08:16:26.705388 140277044971264 logging_writer.py:48] [37528] accumulated_eval_time=8934.070060, accumulated_logging_time=0.452552, accumulated_submission_time=13473.850689, global_step=37528, preemption_count=0, score=13473.850689, test/accuracy=0.675568, test/bleu=27.542709, test/loss=1.497300, test/num_examples=3003, total_duration=22409.571650, train/accuracy=0.650601, train/bleu=32.207967, train/loss=1.659078, validation/accuracy=0.663166, validation/bleu=28.210835, validation/loss=1.578506, validation/num_examples=3000
I0210 08:16:52.780577 140277053363968 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.1970183104276657, loss=1.772024154663086
I0210 08:17:28.505213 140277044971264 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.21234682202339172, loss=1.7855345010757446
I0210 08:18:04.304982 140277053363968 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.19926868379116058, loss=1.8272039890289307
I0210 08:18:40.153994 140277044971264 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.1883506029844284, loss=1.8506462574005127
I0210 08:19:16.011653 140277053363968 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.20681656897068024, loss=1.8030829429626465
I0210 08:19:51.969780 140277044971264 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.19508470594882965, loss=1.8002517223358154
I0210 08:20:27.789783 140277053363968 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.20846271514892578, loss=1.768792986869812
I0210 08:21:03.619316 140277044971264 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.19020003080368042, loss=1.785981297492981
I0210 08:21:39.463327 140277053363968 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.24266639351844788, loss=1.7621550559997559
I0210 08:22:15.352047 140277044971264 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.20442168414592743, loss=1.7699716091156006
I0210 08:22:51.179462 140277053363968 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2453363686800003, loss=1.840258240699768
I0210 08:23:27.047250 140277044971264 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.21066166460514069, loss=1.7034052610397339
I0210 08:24:02.901945 140277053363968 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.2314700484275818, loss=1.7507555484771729
I0210 08:24:38.772185 140277044971264 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.1804153174161911, loss=1.7361502647399902
I0210 08:25:14.644117 140277053363968 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.2010209858417511, loss=1.8159641027450562
I0210 08:25:50.523558 140277044971264 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.209530770778656, loss=1.7341152429580688
I0210 08:26:26.352443 140277053363968 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.2253606915473938, loss=1.834831953048706
I0210 08:27:02.188549 140277044971264 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.1985417902469635, loss=1.7741234302520752
I0210 08:27:38.017750 140277053363968 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.19237801432609558, loss=1.7960715293884277
I0210 08:28:13.853273 140277044971264 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.19065366685390472, loss=1.7428921461105347
I0210 08:28:49.694581 140277053363968 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.1921360343694687, loss=1.7712910175323486
I0210 08:29:25.537263 140277044971264 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.242905393242836, loss=1.7583647966384888
I0210 08:30:01.364391 140277053363968 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1987312138080597, loss=1.8086001873016357
I0210 08:30:26.894283 140446903760704 spec.py:321] Evaluating on the training split.
I0210 08:30:29.915869 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 08:34:41.469679 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 08:34:44.187906 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 08:37:25.485593 140446903760704 spec.py:349] Evaluating on the test split.
I0210 08:37:28.211569 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 08:40:04.683445 140446903760704 submission_runner.py:408] Time since start: 23827.57s, 	Step: 39873, 	{'train/accuracy': 0.6454939842224121, 'train/loss': 1.692597508430481, 'train/bleu': 31.815291168303688, 'validation/accuracy': 0.6619632840156555, 'validation/loss': 1.576145052909851, 'validation/bleu': 28.208675423715423, 'validation/num_examples': 3000, 'test/accuracy': 0.6739526987075806, 'test/loss': 1.4942594766616821, 'test/bleu': 27.546707690715404, 'test/num_examples': 3003, 'score': 14313.951741695404, 'total_duration': 23827.573871850967, 'accumulated_submission_time': 14313.951741695404, 'accumulated_eval_time': 9511.859144210815, 'accumulated_logging_time': 0.4880993366241455}
I0210 08:40:04.708783 140277044971264 logging_writer.py:48] [39873] accumulated_eval_time=9511.859144, accumulated_logging_time=0.488099, accumulated_submission_time=14313.951742, global_step=39873, preemption_count=0, score=14313.951742, test/accuracy=0.673953, test/bleu=27.546708, test/loss=1.494259, test/num_examples=3003, total_duration=23827.573872, train/accuracy=0.645494, train/bleu=31.815291, train/loss=1.692598, validation/accuracy=0.661963, validation/bleu=28.208675, validation/loss=1.576145, validation/num_examples=3000
I0210 08:40:14.709168 140277053363968 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7070040702819824, loss=1.7459166049957275
I0210 08:40:50.410073 140277044971264 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.20769436657428741, loss=1.814437985420227
I0210 08:41:26.217886 140277053363968 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.21025025844573975, loss=1.7183655500411987
I0210 08:42:02.152709 140277044971264 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.18001390993595123, loss=1.7274861335754395
I0210 08:42:37.950425 140277053363968 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.31304365396499634, loss=1.7814899682998657
I0210 08:43:13.763188 140277044971264 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.2090938240289688, loss=1.7711718082427979
I0210 08:43:49.580332 140277053363968 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.19100606441497803, loss=1.6964699029922485
I0210 08:44:25.399527 140277044971264 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.2090083211660385, loss=1.8008313179016113
I0210 08:45:01.213167 140277053363968 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.21194715797901154, loss=1.7663956880569458
I0210 08:45:37.052368 140277044971264 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.33698421716690063, loss=1.6981240510940552
I0210 08:46:12.895345 140277053363968 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.2204725444316864, loss=1.7949285507202148
I0210 08:46:48.749614 140277044971264 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.1927967518568039, loss=1.785131812095642
I0210 08:47:24.596065 140277053363968 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.18881721794605255, loss=1.8034008741378784
I0210 08:48:00.445026 140277044971264 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.20473337173461914, loss=1.7928292751312256
I0210 08:48:36.297549 140277053363968 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.2054193764925003, loss=1.74367356300354
I0210 08:49:12.127521 140277044971264 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2674359083175659, loss=1.6642637252807617
I0210 08:49:47.942378 140277053363968 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.19854623079299927, loss=1.727022647857666
I0210 08:50:23.772737 140277044971264 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.208482563495636, loss=1.8045132160186768
I0210 08:50:59.610140 140277053363968 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.22113469243049622, loss=1.792249321937561
I0210 08:51:35.450430 140277044971264 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.19534017145633698, loss=1.7284666299819946
I0210 08:52:11.307913 140277053363968 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.2756468653678894, loss=1.7806979417800903
I0210 08:52:47.150012 140277044971264 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1885911524295807, loss=1.7474919557571411
I0210 08:53:22.990285 140277053363968 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.20012153685092926, loss=1.7362161874771118
I0210 08:53:58.835566 140277044971264 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.20398461818695068, loss=1.8962939977645874
I0210 08:54:05.008774 140446903760704 spec.py:321] Evaluating on the training split.
I0210 08:54:08.023732 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 08:57:40.471713 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 08:57:43.182395 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:00:15.283222 140446903760704 spec.py:349] Evaluating on the test split.
I0210 09:00:17.988799 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:02:40.183920 140446903760704 submission_runner.py:408] Time since start: 25183.07s, 	Step: 42219, 	{'train/accuracy': 0.6474224925041199, 'train/loss': 1.68617844581604, 'train/bleu': 31.77898337621006, 'validation/accuracy': 0.6637239456176758, 'validation/loss': 1.564249873161316, 'validation/bleu': 28.69145620875173, 'validation/num_examples': 3000, 'test/accuracy': 0.6774853467941284, 'test/loss': 1.4813222885131836, 'test/bleu': 27.985960600248518, 'test/num_examples': 3003, 'score': 15154.16379904747, 'total_duration': 25183.074380159378, 'accumulated_submission_time': 15154.16379904747, 'accumulated_eval_time': 10027.034233808517, 'accumulated_logging_time': 0.5258986949920654}
I0210 09:02:40.204494 140277053363968 logging_writer.py:48] [42219] accumulated_eval_time=10027.034234, accumulated_logging_time=0.525899, accumulated_submission_time=15154.163799, global_step=42219, preemption_count=0, score=15154.163799, test/accuracy=0.677485, test/bleu=27.985961, test/loss=1.481322, test/num_examples=3003, total_duration=25183.074380, train/accuracy=0.647422, train/bleu=31.778983, train/loss=1.686178, validation/accuracy=0.663724, validation/bleu=28.691456, validation/loss=1.564250, validation/num_examples=3000
I0210 09:03:09.469488 140277044971264 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.19051402807235718, loss=1.8014241456985474
I0210 09:03:45.222266 140277053363968 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.20989494025707245, loss=1.7386586666107178
I0210 09:04:21.070861 140277044971264 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.1971171796321869, loss=1.792393684387207
I0210 09:04:56.897995 140277053363968 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.21676084399223328, loss=1.736238956451416
I0210 09:05:32.722856 140277044971264 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.20730040967464447, loss=1.8235584497451782
I0210 09:06:08.549414 140277053363968 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.19443291425704956, loss=1.7823230028152466
I0210 09:06:44.381646 140277044971264 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.20146813988685608, loss=1.7152736186981201
I0210 09:07:20.268799 140277053363968 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.19936732947826385, loss=1.6662026643753052
I0210 09:07:56.171061 140277044971264 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2942022979259491, loss=1.7650461196899414
I0210 09:08:32.082993 140277053363968 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.20233024656772614, loss=1.7377190589904785
I0210 09:09:07.976189 140277044971264 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.22838640213012695, loss=1.7091506719589233
I0210 09:09:43.945378 140277053363968 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.1980123221874237, loss=1.7670609951019287
I0210 09:10:19.810096 140277044971264 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.1926426738500595, loss=1.7009981870651245
I0210 09:10:55.633343 140277053363968 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.19215503334999084, loss=1.7901403903961182
I0210 09:11:31.462496 140277044971264 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.21419599652290344, loss=1.820165991783142
I0210 09:12:07.305752 140277053363968 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2161385416984558, loss=1.700784683227539
I0210 09:12:43.163475 140277044971264 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.20755062997341156, loss=1.7272735834121704
I0210 09:13:19.001938 140277053363968 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.20084728300571442, loss=1.7042604684829712
I0210 09:13:54.819375 140277044971264 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.20490597188472748, loss=1.777588129043579
I0210 09:14:30.628965 140277053363968 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.20431563258171082, loss=1.7612733840942383
I0210 09:15:06.449901 140277044971264 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.21367931365966797, loss=1.6545929908752441
I0210 09:15:42.319104 140277053363968 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2085883468389511, loss=1.6952143907546997
I0210 09:16:18.171535 140277044971264 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.1817801594734192, loss=1.7732535600662231
I0210 09:16:40.473042 140446903760704 spec.py:321] Evaluating on the training split.
I0210 09:16:43.506233 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:20:21.050853 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 09:20:23.759466 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:23:00.229202 140446903760704 spec.py:349] Evaluating on the test split.
I0210 09:23:02.958470 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:25:17.527725 140446903760704 submission_runner.py:408] Time since start: 26540.42s, 	Step: 44564, 	{'train/accuracy': 0.6538242101669312, 'train/loss': 1.6220608949661255, 'train/bleu': 32.21654232899015, 'validation/accuracy': 0.6659061908721924, 'validation/loss': 1.5523436069488525, 'validation/bleu': 28.64157721411009, 'validation/num_examples': 3000, 'test/accuracy': 0.6764511466026306, 'test/loss': 1.4741398096084595, 'test/bleu': 27.830552615538394, 'test/num_examples': 3003, 'score': 15994.344855546951, 'total_duration': 26540.418180942535, 'accumulated_submission_time': 15994.344855546951, 'accumulated_eval_time': 10544.088871002197, 'accumulated_logging_time': 0.5563359260559082}
I0210 09:25:17.548815 140277053363968 logging_writer.py:48] [44564] accumulated_eval_time=10544.088871, accumulated_logging_time=0.556336, accumulated_submission_time=15994.344856, global_step=44564, preemption_count=0, score=15994.344856, test/accuracy=0.676451, test/bleu=27.830553, test/loss=1.474140, test/num_examples=3003, total_duration=26540.418181, train/accuracy=0.653824, train/bleu=32.216542, train/loss=1.622061, validation/accuracy=0.665906, validation/bleu=28.641577, validation/loss=1.552344, validation/num_examples=3000
I0210 09:25:30.778215 140277044971264 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.19741740822792053, loss=1.7549200057983398
I0210 09:26:06.531759 140277053363968 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.19853289425373077, loss=1.7301571369171143
I0210 09:26:42.323297 140277044971264 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.21881134808063507, loss=1.8020477294921875
I0210 09:27:18.163452 140277053363968 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.21190641820430756, loss=1.7016944885253906
I0210 09:27:53.994123 140277044971264 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.21989934146404266, loss=1.6709587574005127
I0210 09:28:29.822438 140277053363968 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.19387169182300568, loss=1.6988542079925537
I0210 09:29:05.643351 140277044971264 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2140287309885025, loss=1.754275918006897
I0210 09:29:41.453311 140277053363968 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.17984242737293243, loss=1.7599623203277588
I0210 09:30:17.247435 140277044971264 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.18061819672584534, loss=1.7331146001815796
I0210 09:30:53.088180 140277053363968 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.18596363067626953, loss=1.8239543437957764
I0210 09:31:28.900712 140277044971264 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.19486865401268005, loss=1.7279479503631592
I0210 09:32:04.746677 140277053363968 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.1984645128250122, loss=1.8331561088562012
I0210 09:32:40.549086 140277044971264 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2263612598180771, loss=1.7144920825958252
I0210 09:33:16.400666 140277053363968 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2324654459953308, loss=1.6956347227096558
I0210 09:33:52.246583 140277044971264 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.1898587942123413, loss=1.7211402654647827
I0210 09:34:28.110166 140277053363968 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.18889643251895905, loss=1.7908278703689575
I0210 09:35:03.938498 140277044971264 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.19885008037090302, loss=1.717148780822754
I0210 09:35:39.745296 140277053363968 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2557907998561859, loss=1.7414153814315796
I0210 09:36:15.591565 140277044971264 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.20374616980552673, loss=1.8410249948501587
I0210 09:36:51.409720 140277053363968 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.2786468267440796, loss=1.8322380781173706
I0210 09:37:27.257882 140277044971264 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2323756068944931, loss=1.8029762506484985
I0210 09:38:03.087495 140277053363968 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.20009595155715942, loss=1.8063380718231201
I0210 09:38:38.885746 140277044971264 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.20646001398563385, loss=1.7727957963943481
I0210 09:39:14.701498 140277053363968 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.19471412897109985, loss=1.7331660985946655
I0210 09:39:17.649390 140446903760704 spec.py:321] Evaluating on the training split.
I0210 09:39:20.654368 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:43:01.284975 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 09:43:04.028409 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:45:33.696324 140446903760704 spec.py:349] Evaluating on the test split.
I0210 09:45:36.413838 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 09:47:58.221738 140446903760704 submission_runner.py:408] Time since start: 27901.11s, 	Step: 46910, 	{'train/accuracy': 0.6494661569595337, 'train/loss': 1.669074296951294, 'train/bleu': 31.6695985126084, 'validation/accuracy': 0.6667740941047668, 'validation/loss': 1.5472114086151123, 'validation/bleu': 28.777955986349998, 'validation/num_examples': 3000, 'test/accuracy': 0.6811225414276123, 'test/loss': 1.4597563743591309, 'test/bleu': 28.346774455501244, 'test/num_examples': 3003, 'score': 16834.357803106308, 'total_duration': 27901.11219573021, 'accumulated_submission_time': 16834.357803106308, 'accumulated_eval_time': 11064.661159992218, 'accumulated_logging_time': 0.5887689590454102}
I0210 09:47:58.243507 140277044971264 logging_writer.py:48] [46910] accumulated_eval_time=11064.661160, accumulated_logging_time=0.588769, accumulated_submission_time=16834.357803, global_step=46910, preemption_count=0, score=16834.357803, test/accuracy=0.681123, test/bleu=28.346774, test/loss=1.459756, test/num_examples=3003, total_duration=27901.112196, train/accuracy=0.649466, train/bleu=31.669599, train/loss=1.669074, validation/accuracy=0.666774, validation/bleu=28.777956, validation/loss=1.547211, validation/num_examples=3000
I0210 09:48:30.712490 140277053363968 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.18364432454109192, loss=1.7439839839935303
I0210 09:49:06.496809 140277044971264 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.21822026371955872, loss=1.6996550559997559
I0210 09:49:42.322989 140277053363968 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2480674684047699, loss=1.7701389789581299
I0210 09:50:18.137562 140277044971264 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.19636426866054535, loss=1.7085154056549072
I0210 09:50:53.952210 140277053363968 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.19903425872325897, loss=1.7648561000823975
I0210 09:51:29.761057 140277044971264 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.20119774341583252, loss=1.7211308479309082
I0210 09:52:05.606448 140277053363968 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.23944558203220367, loss=1.7561618089675903
I0210 09:52:41.418848 140277044971264 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19756467640399933, loss=1.806276559829712
I0210 09:53:17.225936 140277053363968 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.21087151765823364, loss=1.7367746829986572
I0210 09:53:53.063617 140277044971264 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.24434158205986023, loss=1.735624074935913
I0210 09:54:28.888147 140277053363968 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2665998339653015, loss=1.786726951599121
I0210 09:55:04.725557 140277044971264 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.274283230304718, loss=1.7428722381591797
I0210 09:55:40.571978 140277053363968 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.18924370408058167, loss=1.8090808391571045
I0210 09:56:16.464753 140277044971264 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2397134006023407, loss=1.7538211345672607
I0210 09:56:52.269725 140277053363968 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.2618396580219269, loss=1.6988352537155151
I0210 09:57:28.120170 140277044971264 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.191865935921669, loss=1.718976616859436
I0210 09:58:03.934291 140277053363968 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.19957594573497772, loss=1.815483570098877
I0210 09:58:39.740315 140277044971264 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.22315336763858795, loss=1.7746944427490234
I0210 09:59:15.575915 140277053363968 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.19824917614459991, loss=1.6873769760131836
I0210 09:59:51.386490 140277044971264 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.1968536525964737, loss=1.6789565086364746
I0210 10:00:27.208029 140277053363968 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.18392594158649445, loss=1.7585792541503906
I0210 10:01:03.030729 140277044971264 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.24798721075057983, loss=1.7126288414001465
I0210 10:01:38.886655 140277053363968 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.20198330283164978, loss=1.6643399000167847
I0210 10:01:58.336832 140446903760704 spec.py:321] Evaluating on the training split.
I0210 10:02:01.360691 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:04:43.210815 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 10:04:45.904267 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:07:15.254806 140446903760704 spec.py:349] Evaluating on the test split.
I0210 10:07:17.967508 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:09:48.699762 140446903760704 submission_runner.py:408] Time since start: 29211.59s, 	Step: 49256, 	{'train/accuracy': 0.6462664604187012, 'train/loss': 1.6856175661087036, 'train/bleu': 31.678658230344087, 'validation/accuracy': 0.6664517521858215, 'validation/loss': 1.5448410511016846, 'validation/bleu': 28.842956598526044, 'validation/num_examples': 3000, 'test/accuracy': 0.6809831261634827, 'test/loss': 1.4532852172851562, 'test/bleu': 28.45048140232037, 'test/num_examples': 3003, 'score': 17674.36550784111, 'total_duration': 29211.590218305588, 'accumulated_submission_time': 17674.36550784111, 'accumulated_eval_time': 11535.024050235748, 'accumulated_logging_time': 0.6217913627624512}
I0210 10:09:48.721494 140277044971264 logging_writer.py:48] [49256] accumulated_eval_time=11535.024050, accumulated_logging_time=0.621791, accumulated_submission_time=17674.365508, global_step=49256, preemption_count=0, score=17674.365508, test/accuracy=0.680983, test/bleu=28.450481, test/loss=1.453285, test/num_examples=3003, total_duration=29211.590218, train/accuracy=0.646266, train/bleu=31.678658, train/loss=1.685618, validation/accuracy=0.666452, validation/bleu=28.842957, validation/loss=1.544841, validation/num_examples=3000
I0210 10:10:04.777944 140277053363968 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.18368160724639893, loss=1.6734602451324463
I0210 10:10:40.480269 140277044971264 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1960928738117218, loss=1.7164161205291748
I0210 10:11:16.294790 140277053363968 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.20943757891654968, loss=1.8017078638076782
I0210 10:11:52.139308 140277044971264 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.26799851655960083, loss=1.7281354665756226
I0210 10:12:27.950088 140277053363968 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.23519796133041382, loss=1.8174388408660889
I0210 10:13:03.773804 140277044971264 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.20173785090446472, loss=1.6808303594589233
I0210 10:13:39.608850 140277053363968 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.23209324479103088, loss=1.769863247871399
I0210 10:14:15.447598 140277044971264 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.22755111753940582, loss=1.7541534900665283
I0210 10:14:51.268997 140277053363968 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.22861552238464355, loss=1.8523262739181519
I0210 10:15:27.132239 140277044971264 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.19493727385997772, loss=1.7403247356414795
I0210 10:16:02.986428 140277053363968 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2244243621826172, loss=1.6685696840286255
I0210 10:16:38.796028 140277044971264 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.1906622350215912, loss=1.7971124649047852
I0210 10:17:14.623248 140277053363968 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.20128537714481354, loss=1.7780791521072388
I0210 10:17:50.466016 140277044971264 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.19689558446407318, loss=1.7024879455566406
I0210 10:18:26.269641 140277053363968 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.22529077529907227, loss=1.7143884897232056
I0210 10:19:02.112197 140277044971264 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.20625711977481842, loss=1.737838864326477
I0210 10:19:37.923302 140277053363968 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2486700564622879, loss=1.7764900922775269
I0210 10:20:13.756306 140277044971264 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.21541929244995117, loss=1.7288439273834229
I0210 10:20:49.570183 140277053363968 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.18223805725574493, loss=1.714596152305603
I0210 10:21:25.401351 140277044971264 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.22912588715553284, loss=1.7849591970443726
I0210 10:22:01.223044 140277053363968 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.36184680461883545, loss=1.747483491897583
I0210 10:22:37.039415 140277044971264 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.25153228640556335, loss=1.6853342056274414
I0210 10:23:12.873698 140277053363968 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1998843550682068, loss=1.7266554832458496
I0210 10:23:48.703489 140277044971264 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.19044770300388336, loss=1.8125964403152466
I0210 10:23:48.710538 140446903760704 spec.py:321] Evaluating on the training split.
I0210 10:23:51.437020 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:28:01.876599 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 10:28:04.587240 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:30:44.278705 140446903760704 spec.py:349] Evaluating on the test split.
I0210 10:30:46.987964 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:33:32.384634 140446903760704 submission_runner.py:408] Time since start: 30635.28s, 	Step: 51601, 	{'train/accuracy': 0.6567646265029907, 'train/loss': 1.613392949104309, 'train/bleu': 32.20008454597276, 'validation/accuracy': 0.668398380279541, 'validation/loss': 1.5337615013122559, 'validation/bleu': 28.792989343388353, 'validation/num_examples': 3000, 'test/accuracy': 0.6818430423736572, 'test/loss': 1.4524085521697998, 'test/bleu': 28.224619330030805, 'test/num_examples': 3003, 'score': 18514.27074623108, 'total_duration': 30635.275083065033, 'accumulated_submission_time': 18514.27074623108, 'accumulated_eval_time': 12118.698055744171, 'accumulated_logging_time': 0.6539661884307861}
I0210 10:33:32.406975 140277053363968 logging_writer.py:48] [51601] accumulated_eval_time=12118.698056, accumulated_logging_time=0.653966, accumulated_submission_time=18514.270746, global_step=51601, preemption_count=0, score=18514.270746, test/accuracy=0.681843, test/bleu=28.224619, test/loss=1.452409, test/num_examples=3003, total_duration=30635.275083, train/accuracy=0.656765, train/bleu=32.200085, train/loss=1.613393, validation/accuracy=0.668398, validation/bleu=28.792989, validation/loss=1.533762, validation/num_examples=3000
I0210 10:34:08.082545 140277044971264 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.19027765095233917, loss=1.72489333152771
I0210 10:34:43.865550 140277053363968 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.20728322863578796, loss=1.8608804941177368
I0210 10:35:19.663659 140277044971264 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.21748113632202148, loss=1.6650453805923462
I0210 10:35:55.519964 140277053363968 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.18828219175338745, loss=1.696092963218689
I0210 10:36:31.367735 140277044971264 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.19314423203468323, loss=1.8151732683181763
I0210 10:37:07.185961 140277053363968 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.468683123588562, loss=1.7270233631134033
I0210 10:37:42.984952 140277044971264 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.21946100890636444, loss=1.7183499336242676
I0210 10:38:18.822054 140277053363968 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.20564500987529755, loss=1.788766860961914
I0210 10:38:54.728225 140277044971264 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.20508559048175812, loss=1.8190033435821533
I0210 10:39:30.580855 140277053363968 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.23593242466449738, loss=1.7736718654632568
I0210 10:40:06.378067 140277044971264 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.4099421203136444, loss=1.771034598350525
I0210 10:40:42.176285 140277053363968 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.2027355432510376, loss=1.781114101409912
I0210 10:41:18.008074 140277044971264 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.2075972706079483, loss=1.8096120357513428
I0210 10:41:53.836211 140277053363968 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.22257934510707855, loss=1.7883001565933228
I0210 10:42:29.644518 140277044971264 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.23155266046524048, loss=1.7799938917160034
I0210 10:43:05.482560 140277053363968 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.2004924714565277, loss=1.7123280763626099
I0210 10:43:41.356421 140277044971264 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2299557328224182, loss=1.689343810081482
I0210 10:44:17.176283 140277053363968 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.21272659301757812, loss=1.754429578781128
I0210 10:44:53.014536 140277044971264 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.21179050207138062, loss=1.7092175483703613
I0210 10:45:28.838112 140277053363968 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.19592604041099548, loss=1.7657122611999512
I0210 10:46:04.671658 140277044971264 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.24803100526332855, loss=1.7452200651168823
I0210 10:46:40.524235 140277053363968 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1914733201265335, loss=1.7702844142913818
I0210 10:47:16.360015 140277044971264 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.21344606578350067, loss=1.6697849035263062
I0210 10:47:32.556924 140446903760704 spec.py:321] Evaluating on the training split.
I0210 10:47:35.572338 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:50:58.020683 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 10:51:00.728807 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:54:08.698673 140446903760704 spec.py:349] Evaluating on the test split.
I0210 10:54:11.407791 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 10:57:30.375023 140446903760704 submission_runner.py:408] Time since start: 32073.27s, 	Step: 53947, 	{'train/accuracy': 0.6505134701728821, 'train/loss': 1.660967469215393, 'train/bleu': 31.867379464977144, 'validation/accuracy': 0.6687207818031311, 'validation/loss': 1.5278306007385254, 'validation/bleu': 28.76152895014794, 'validation/num_examples': 3000, 'test/accuracy': 0.6840276718139648, 'test/loss': 1.4374014139175415, 'test/bleu': 28.765265595968746, 'test/num_examples': 3003, 'score': 19354.333768606186, 'total_duration': 32073.26548433304, 'accumulated_submission_time': 19354.333768606186, 'accumulated_eval_time': 12716.516110658646, 'accumulated_logging_time': 0.6878187656402588}
I0210 10:57:30.396876 140277053363968 logging_writer.py:48] [53947] accumulated_eval_time=12716.516111, accumulated_logging_time=0.687819, accumulated_submission_time=19354.333769, global_step=53947, preemption_count=0, score=19354.333769, test/accuracy=0.684028, test/bleu=28.765266, test/loss=1.437401, test/num_examples=3003, total_duration=32073.265484, train/accuracy=0.650513, train/bleu=31.867379, train/loss=1.660967, validation/accuracy=0.668721, validation/bleu=28.761529, validation/loss=1.527831, validation/num_examples=3000
I0210 10:57:49.693004 140277044971264 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.1933976411819458, loss=1.680898666381836
I0210 10:58:25.416917 140277053363968 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1933349221944809, loss=1.6455073356628418
I0210 10:59:01.231351 140277044971264 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.2165965884923935, loss=1.735577940940857
I0210 10:59:37.090920 140277053363968 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2047761231660843, loss=1.6600326299667358
I0210 11:00:12.915935 140277044971264 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.20703858137130737, loss=1.6578404903411865
I0210 11:00:48.728357 140277053363968 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.19704951345920563, loss=1.7255967855453491
I0210 11:01:24.535769 140277044971264 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.20112471282482147, loss=1.761426568031311
I0210 11:02:00.413790 140277053363968 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.2632441520690918, loss=1.7208149433135986
I0210 11:02:36.242207 140277044971264 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1934366375207901, loss=1.6636393070220947
I0210 11:03:12.069991 140277053363968 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.231063574552536, loss=1.7691197395324707
I0210 11:03:47.898877 140277044971264 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2018888145685196, loss=1.7030773162841797
I0210 11:04:23.720363 140277053363968 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.3721626102924347, loss=1.8521173000335693
I0210 11:04:59.553782 140277044971264 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.17741890251636505, loss=1.6586928367614746
I0210 11:05:35.382010 140277053363968 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.18400894105434418, loss=1.7764753103256226
I0210 11:06:11.247415 140277044971264 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.19019535183906555, loss=1.7279990911483765
I0210 11:06:47.071285 140277053363968 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.18110539019107819, loss=1.693056344985962
I0210 11:07:22.894251 140277044971264 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2021266222000122, loss=1.752212405204773
I0210 11:07:58.719179 140277053363968 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.20424482226371765, loss=1.6677554845809937
I0210 11:08:34.528868 140277044971264 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.20545022189617157, loss=1.6727662086486816
I0210 11:09:10.362668 140277053363968 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.20477497577667236, loss=1.8053306341171265
I0210 11:09:46.326349 140277044971264 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.18938083946704865, loss=1.678445816040039
I0210 11:10:22.152554 140277053363968 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.25775185227394104, loss=1.6659547090530396
I0210 11:10:58.004939 140277044971264 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.19981960952281952, loss=1.7844736576080322
I0210 11:11:30.667752 140446903760704 spec.py:321] Evaluating on the training split.
I0210 11:11:33.687737 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 11:15:08.449393 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 11:15:11.154815 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 11:18:27.624900 140446903760704 spec.py:349] Evaluating on the test split.
I0210 11:18:30.338346 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 11:21:12.025465 140446903760704 submission_runner.py:408] Time since start: 33494.92s, 	Step: 56293, 	{'train/accuracy': 0.6667506098747253, 'train/loss': 1.5589523315429688, 'train/bleu': 33.09192433508889, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.5208386182785034, 'validation/bleu': 28.633689483150718, 'validation/num_examples': 3000, 'test/accuracy': 0.6826448440551758, 'test/loss': 1.4398152828216553, 'test/bleu': 28.341602362376708, 'test/num_examples': 3003, 'score': 20194.51906824112, 'total_duration': 33494.9159014225, 'accumulated_submission_time': 20194.51906824112, 'accumulated_eval_time': 13297.87374830246, 'accumulated_logging_time': 0.7198841571807861}
I0210 11:21:12.051680 140277053363968 logging_writer.py:48] [56293] accumulated_eval_time=13297.873748, accumulated_logging_time=0.719884, accumulated_submission_time=20194.519068, global_step=56293, preemption_count=0, score=20194.519068, test/accuracy=0.682645, test/bleu=28.341602, test/loss=1.439815, test/num_examples=3003, total_duration=33494.915901, train/accuracy=0.666751, train/bleu=33.091924, train/loss=1.558952, validation/accuracy=0.668783, validation/bleu=28.633689, validation/loss=1.520839, validation/num_examples=3000
I0210 11:21:14.928288 140277044971264 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.25425922870635986, loss=1.7603195905685425
I0210 11:21:50.621835 140277053363968 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.19906383752822876, loss=1.6985312700271606
I0210 11:22:26.401683 140277044971264 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2129562795162201, loss=1.7375036478042603
I0210 11:23:02.197257 140277053363968 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.21296660602092743, loss=1.6812996864318848
I0210 11:23:38.037903 140277044971264 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.20825353264808655, loss=1.7835041284561157
I0210 11:24:13.833446 140277053363968 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.1969606727361679, loss=1.804934024810791
I0210 11:24:49.660502 140277044971264 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2482895851135254, loss=1.840851902961731
I0210 11:25:25.480734 140277053363968 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.18710313737392426, loss=1.677634358406067
I0210 11:26:01.412735 140277044971264 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19565944373607635, loss=1.6760231256484985
I0210 11:26:37.236624 140277053363968 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.19129319489002228, loss=1.7251646518707275
I0210 11:27:13.049438 140277044971264 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.19681072235107422, loss=1.7209275960922241
I0210 11:27:48.854132 140277053363968 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.20781950652599335, loss=1.6939665079116821
I0210 11:28:24.658850 140277044971264 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.18848039209842682, loss=1.8293871879577637
I0210 11:29:00.480992 140277053363968 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19158577919006348, loss=1.7363128662109375
I0210 11:29:36.309768 140277044971264 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.19932103157043457, loss=1.7809094190597534
I0210 11:30:12.164759 140277053363968 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.20549078285694122, loss=1.7708852291107178
I0210 11:30:47.974681 140277044971264 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.20668447017669678, loss=1.6155441999435425
I0210 11:31:23.795279 140277053363968 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.19972072541713715, loss=1.6711159944534302
I0210 11:31:59.665358 140277044971264 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.1886889934539795, loss=1.6874736547470093
I0210 11:32:35.595422 140277053363968 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.21678373217582703, loss=1.618438720703125
I0210 11:33:11.412158 140277044971264 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.19036690890789032, loss=1.6713542938232422
I0210 11:33:47.268765 140277053363968 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.22080212831497192, loss=1.6911122798919678
I0210 11:34:23.076145 140277044971264 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.1952989101409912, loss=1.7323689460754395
I0210 11:34:58.904662 140277053363968 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.1856776773929596, loss=1.7661854028701782
I0210 11:35:12.235867 140446903760704 spec.py:321] Evaluating on the training split.
I0210 11:35:15.248507 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 11:39:05.996692 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 11:39:08.689788 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 11:42:23.704189 140446903760704 spec.py:349] Evaluating on the test split.
I0210 11:42:26.405936 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 11:45:56.019015 140446903760704 submission_runner.py:408] Time since start: 34978.91s, 	Step: 58639, 	{'train/accuracy': 0.655954897403717, 'train/loss': 1.618160605430603, 'train/bleu': 32.188351151900314, 'validation/accuracy': 0.6726884841918945, 'validation/loss': 1.5124878883361816, 'validation/bleu': 29.072180110225716, 'validation/num_examples': 3000, 'test/accuracy': 0.6835163831710815, 'test/loss': 1.4296272993087769, 'test/bleu': 28.599861865485746, 'test/num_examples': 3003, 'score': 21034.617770195007, 'total_duration': 34978.909477710724, 'accumulated_submission_time': 21034.617770195007, 'accumulated_eval_time': 13941.65684556961, 'accumulated_logging_time': 0.7570688724517822}
I0210 11:45:56.042357 140277044971264 logging_writer.py:48] [58639] accumulated_eval_time=13941.656846, accumulated_logging_time=0.757069, accumulated_submission_time=21034.617770, global_step=58639, preemption_count=0, score=21034.617770, test/accuracy=0.683516, test/bleu=28.599862, test/loss=1.429627, test/num_examples=3003, total_duration=34978.909478, train/accuracy=0.655955, train/bleu=32.188351, train/loss=1.618161, validation/accuracy=0.672688, validation/bleu=29.072180, validation/loss=1.512488, validation/num_examples=3000
I0210 11:46:18.144733 140277053363968 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.20893873274326324, loss=1.7078500986099243
I0210 11:46:53.848428 140277044971264 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.18816401064395905, loss=1.6934884786605835
I0210 11:47:29.651041 140277053363968 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.2117239534854889, loss=1.6186712980270386
I0210 11:48:05.470647 140277044971264 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.20553560554981232, loss=1.7548127174377441
I0210 11:48:41.326668 140277053363968 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.20653977990150452, loss=1.716558575630188
I0210 11:49:17.192381 140277044971264 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.22650110721588135, loss=1.7150019407272339
I0210 11:49:52.988383 140277053363968 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2396508753299713, loss=1.7537201642990112
I0210 11:50:28.833960 140277044971264 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.21260638535022736, loss=1.7024767398834229
I0210 11:51:04.714246 140277053363968 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.35535043478012085, loss=1.7317290306091309
I0210 11:51:40.541908 140277044971264 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.20369592308998108, loss=1.7168196439743042
I0210 11:52:16.394815 140277053363968 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.20238107442855835, loss=1.6879959106445312
I0210 11:52:52.251733 140277044971264 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.18985575437545776, loss=1.6651666164398193
I0210 11:53:28.100054 140277053363968 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.19565561413764954, loss=1.661481499671936
I0210 11:54:03.917638 140277044971264 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.20826652646064758, loss=1.7398662567138672
I0210 11:54:39.729861 140277053363968 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.20452208817005157, loss=1.7195110321044922
I0210 11:55:15.602446 140277044971264 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.19018954038619995, loss=1.6594475507736206
I0210 11:55:51.427230 140277053363968 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.20312118530273438, loss=1.7279987335205078
I0210 11:56:27.259972 140277044971264 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.200331449508667, loss=1.7419134378433228
I0210 11:57:03.086835 140277053363968 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.19463196396827698, loss=1.7543554306030273
I0210 11:57:38.917514 140277044971264 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.1947942078113556, loss=1.7091221809387207
I0210 11:58:14.783434 140277053363968 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.21117012202739716, loss=1.7241768836975098
I0210 11:58:50.618294 140277044971264 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.21225076913833618, loss=1.7259178161621094
I0210 11:59:26.443870 140277053363968 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.206997811794281, loss=1.7797279357910156
I0210 11:59:56.283975 140446903760704 spec.py:321] Evaluating on the training split.
I0210 11:59:59.302136 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:03:13.444170 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 12:03:16.148666 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:05:43.551340 140446903760704 spec.py:349] Evaluating on the test split.
I0210 12:05:46.257837 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:08:00.579277 140446903760704 submission_runner.py:408] Time since start: 36303.47s, 	Step: 60985, 	{'train/accuracy': 0.6542706489562988, 'train/loss': 1.6405000686645508, 'train/bleu': 32.1757717300823, 'validation/accuracy': 0.6717957258224487, 'validation/loss': 1.5104609727859497, 'validation/bleu': 29.062029670361643, 'validation/num_examples': 3000, 'test/accuracy': 0.6864215135574341, 'test/loss': 1.420398235321045, 'test/bleu': 29.007746720886413, 'test/num_examples': 3003, 'score': 21874.773897647858, 'total_duration': 36303.46973657608, 'accumulated_submission_time': 21874.773897647858, 'accumulated_eval_time': 14425.952094316483, 'accumulated_logging_time': 0.7913601398468018}
I0210 12:08:00.602942 140277044971264 logging_writer.py:48] [60985] accumulated_eval_time=14425.952094, accumulated_logging_time=0.791360, accumulated_submission_time=21874.773898, global_step=60985, preemption_count=0, score=21874.773898, test/accuracy=0.686422, test/bleu=29.007747, test/loss=1.420398, test/num_examples=3003, total_duration=36303.469737, train/accuracy=0.654271, train/bleu=32.175772, train/loss=1.640500, validation/accuracy=0.671796, validation/bleu=29.062030, validation/loss=1.510461, validation/num_examples=3000
I0210 12:08:06.320486 140277053363968 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.20335757732391357, loss=1.7213062047958374
I0210 12:08:42.036270 140277044971264 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.5883747935295105, loss=1.6454862356185913
I0210 12:09:17.847031 140277053363968 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.23324967920780182, loss=1.6989140510559082
I0210 12:09:53.680364 140277044971264 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.18883852660655975, loss=1.7517372369766235
I0210 12:10:29.603587 140277053363968 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.20022788643836975, loss=1.7016334533691406
I0210 12:11:05.415644 140277044971264 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.19389654695987701, loss=1.6990152597427368
I0210 12:11:41.233173 140277053363968 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2026892453432083, loss=1.6693106889724731
I0210 12:12:17.088891 140277044971264 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.21999862790107727, loss=1.6787564754486084
I0210 12:12:52.918854 140277053363968 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.20190013945102692, loss=1.7487725019454956
I0210 12:13:28.738395 140277044971264 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2331535965204239, loss=1.6958807706832886
I0210 12:14:04.566640 140277053363968 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.23222534358501434, loss=1.7599190473556519
I0210 12:14:40.359982 140277044971264 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.3302367925643921, loss=1.6421120166778564
I0210 12:15:16.180653 140277053363968 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.1991003453731537, loss=1.7231132984161377
I0210 12:15:51.995918 140277044971264 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.19844022393226624, loss=1.622588038444519
I0210 12:16:27.808215 140277053363968 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.19211846590042114, loss=1.6397615671157837
I0210 12:17:03.605581 140277044971264 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.1923685520887375, loss=1.6815217733383179
I0210 12:17:39.529636 140277053363968 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.18142688274383545, loss=1.6783770322799683
I0210 12:18:15.416423 140277044971264 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.2233104556798935, loss=1.6569907665252686
I0210 12:18:51.322423 140277053363968 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.22344349324703217, loss=1.7119845151901245
I0210 12:19:27.204847 140277044971264 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.21059440076351166, loss=1.619251012802124
I0210 12:20:03.012496 140277053363968 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2315118908882141, loss=1.7683842182159424
I0210 12:20:38.804343 140277044971264 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.1986469030380249, loss=1.6526808738708496
I0210 12:21:14.646558 140277053363968 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.1931404322385788, loss=1.694904088973999
I0210 12:21:50.488963 140277044971264 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.21106569468975067, loss=1.7405056953430176
I0210 12:22:00.597380 140446903760704 spec.py:321] Evaluating on the training split.
I0210 12:22:03.637746 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:25:31.634897 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 12:25:34.339601 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:28:07.625586 140446903760704 spec.py:349] Evaluating on the test split.
I0210 12:28:10.332165 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:30:29.487771 140446903760704 submission_runner.py:408] Time since start: 37652.38s, 	Step: 63330, 	{'train/accuracy': 0.6675124168395996, 'train/loss': 1.544390082359314, 'train/bleu': 32.92653022817832, 'validation/accuracy': 0.6732712388038635, 'validation/loss': 1.5051460266113281, 'validation/bleu': 29.133460583030878, 'validation/num_examples': 3000, 'test/accuracy': 0.6854802370071411, 'test/loss': 1.4173305034637451, 'test/bleu': 28.617106854066893, 'test/num_examples': 3003, 'score': 22714.68177628517, 'total_duration': 37652.378227472305, 'accumulated_submission_time': 22714.68177628517, 'accumulated_eval_time': 14934.84242773056, 'accumulated_logging_time': 0.8252365589141846}
I0210 12:30:29.513033 140277053363968 logging_writer.py:48] [63330] accumulated_eval_time=14934.842428, accumulated_logging_time=0.825237, accumulated_submission_time=22714.681776, global_step=63330, preemption_count=0, score=22714.681776, test/accuracy=0.685480, test/bleu=28.617107, test/loss=1.417331, test/num_examples=3003, total_duration=37652.378227, train/accuracy=0.667512, train/bleu=32.926530, train/loss=1.544390, validation/accuracy=0.673271, validation/bleu=29.133461, validation/loss=1.505146, validation/num_examples=3000
I0210 12:30:54.834993 140277044971264 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.20393621921539307, loss=1.8113303184509277
I0210 12:31:30.577894 140277053363968 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2504338026046753, loss=1.6624101400375366
I0210 12:32:06.414820 140277044971264 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.23182615637779236, loss=1.6480681896209717
I0210 12:32:42.240849 140277053363968 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.2119964063167572, loss=1.6379212141036987
I0210 12:33:18.098733 140277044971264 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.24901792407035828, loss=1.8049756288528442
I0210 12:33:53.934752 140277053363968 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.19008894264698029, loss=1.7000130414962769
I0210 12:34:29.750287 140277044971264 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.26779505610466003, loss=1.7197400331497192
I0210 12:35:05.586623 140277053363968 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.21049097180366516, loss=1.7911949157714844
I0210 12:35:41.438012 140277044971264 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.31915831565856934, loss=1.744875192642212
I0210 12:36:17.272643 140277053363968 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.20430736243724823, loss=1.6417301893234253
I0210 12:36:53.107183 140277044971264 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.23516155779361725, loss=1.7163296937942505
I0210 12:37:28.924544 140277053363968 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2246599942445755, loss=1.7053297758102417
I0210 12:38:04.731429 140277044971264 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.43282780051231384, loss=1.6302120685577393
I0210 12:38:40.552260 140277053363968 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.29262575507164, loss=1.6007784605026245
I0210 12:39:16.393020 140277044971264 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.20173320174217224, loss=1.6946017742156982
I0210 12:39:52.216864 140277053363968 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.22859477996826172, loss=1.6657825708389282
I0210 12:40:28.031464 140277044971264 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.19011029601097107, loss=1.7202391624450684
I0210 12:41:03.865121 140277053363968 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.19387367367744446, loss=1.6850212812423706
I0210 12:41:39.702486 140277044971264 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.21948711574077606, loss=1.7521347999572754
I0210 12:42:15.524351 140277053363968 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.7155961990356445, loss=1.6961584091186523
I0210 12:42:51.338496 140277044971264 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.19805939495563507, loss=1.7570159435272217
I0210 12:43:27.154128 140277053363968 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.19659078121185303, loss=1.6465919017791748
I0210 12:44:02.993144 140277044971264 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2087613046169281, loss=1.716566801071167
I0210 12:44:29.566766 140446903760704 spec.py:321] Evaluating on the training split.
I0210 12:44:32.573569 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:48:37.417218 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 12:48:40.118268 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:51:36.858529 140446903760704 spec.py:349] Evaluating on the test split.
I0210 12:51:39.569972 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 12:54:45.642429 140446903760704 submission_runner.py:408] Time since start: 39108.53s, 	Step: 65676, 	{'train/accuracy': 0.6562884449958801, 'train/loss': 1.612204670906067, 'train/bleu': 32.68944922577982, 'validation/accuracy': 0.6751435399055481, 'validation/loss': 1.4910575151443481, 'validation/bleu': 29.30038312727096, 'validation/num_examples': 3000, 'test/accuracy': 0.6907327175140381, 'test/loss': 1.3964228630065918, 'test/bleu': 29.165284759238112, 'test/num_examples': 3003, 'score': 23554.652008533478, 'total_duration': 39108.53287887573, 'accumulated_submission_time': 23554.652008533478, 'accumulated_eval_time': 15550.918023347855, 'accumulated_logging_time': 0.8605771064758301}
I0210 12:54:45.668663 140277053363968 logging_writer.py:48] [65676] accumulated_eval_time=15550.918023, accumulated_logging_time=0.860577, accumulated_submission_time=23554.652009, global_step=65676, preemption_count=0, score=23554.652009, test/accuracy=0.690733, test/bleu=29.165285, test/loss=1.396423, test/num_examples=3003, total_duration=39108.532879, train/accuracy=0.656288, train/bleu=32.689449, train/loss=1.612205, validation/accuracy=0.675144, validation/bleu=29.300383, validation/loss=1.491058, validation/num_examples=3000
I0210 12:54:54.604905 140277044971264 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.21194054186344147, loss=1.683082938194275
I0210 12:55:30.346028 140277053363968 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.18686474859714508, loss=1.6707959175109863
I0210 12:56:06.149478 140277044971264 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.20533700287342072, loss=1.6594127416610718
I0210 12:56:41.946003 140277053363968 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.19802409410476685, loss=1.679612159729004
I0210 12:57:17.769240 140277044971264 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2060995101928711, loss=1.6538724899291992
I0210 12:57:53.616552 140277053363968 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.20152749121189117, loss=1.6602739095687866
I0210 12:58:29.442891 140277044971264 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.22494779527187347, loss=1.5978206396102905
I0210 12:59:05.278170 140277053363968 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.21201953291893005, loss=1.6218619346618652
I0210 12:59:41.088246 140277044971264 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.21515360474586487, loss=1.7144910097122192
I0210 13:00:16.921136 140277053363968 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.22777311503887177, loss=1.6514497995376587
I0210 13:00:52.707942 140277044971264 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.20160959661006927, loss=1.6537481546401978
I0210 13:01:28.539785 140277053363968 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2203613519668579, loss=1.5999209880828857
I0210 13:02:04.400365 140277044971264 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2075798511505127, loss=1.6998448371887207
I0210 13:02:40.190481 140277053363968 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.22476139664649963, loss=1.7258487939834595
I0210 13:03:15.983538 140277044971264 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.47882044315338135, loss=1.602580189704895
I0210 13:03:51.835224 140277053363968 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.1994682401418686, loss=1.638267993927002
I0210 13:04:27.650114 140277044971264 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.20203812420368195, loss=1.6361478567123413
I0210 13:05:03.463983 140277053363968 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.19758149981498718, loss=1.7445851564407349
I0210 13:05:39.294791 140277044971264 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.22702430188655853, loss=1.6442288160324097
I0210 13:06:15.109740 140277053363968 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.1926979422569275, loss=1.6808468103408813
I0210 13:06:50.924832 140277044971264 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.20321610569953918, loss=1.7558386325836182
I0210 13:07:26.758658 140277053363968 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2054091840982437, loss=1.7269114255905151
I0210 13:08:02.649248 140277044971264 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2072218507528305, loss=1.6383745670318604
I0210 13:08:38.480587 140277053363968 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.18766605854034424, loss=1.5912569761276245
I0210 13:08:45.725428 140446903760704 spec.py:321] Evaluating on the training split.
I0210 13:08:48.732795 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 13:12:41.449808 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 13:12:44.145228 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 13:15:28.206637 140446903760704 spec.py:349] Evaluating on the test split.
I0210 13:15:30.908497 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 13:17:53.228431 140446903760704 submission_runner.py:408] Time since start: 40496.12s, 	Step: 68022, 	{'train/accuracy': 0.6579658389091492, 'train/loss': 1.613708734512329, 'train/bleu': 32.35109639577502, 'validation/accuracy': 0.6769165992736816, 'validation/loss': 1.4885090589523315, 'validation/bleu': 29.626793474166714, 'validation/num_examples': 3000, 'test/accuracy': 0.6890709400177002, 'test/loss': 1.3990836143493652, 'test/bleu': 29.196933871882052, 'test/num_examples': 3003, 'score': 24394.624833345413, 'total_duration': 40496.11887073517, 'accumulated_submission_time': 24394.624833345413, 'accumulated_eval_time': 16098.42095041275, 'accumulated_logging_time': 0.8969278335571289}
I0210 13:17:53.253270 140277044971264 logging_writer.py:48] [68022] accumulated_eval_time=16098.420950, accumulated_logging_time=0.896928, accumulated_submission_time=24394.624833, global_step=68022, preemption_count=0, score=24394.624833, test/accuracy=0.689071, test/bleu=29.196934, test/loss=1.399084, test/num_examples=3003, total_duration=40496.118871, train/accuracy=0.657966, train/bleu=32.351096, train/loss=1.613709, validation/accuracy=0.676917, validation/bleu=29.626793, validation/loss=1.488509, validation/num_examples=3000
I0210 13:18:21.412339 140277053363968 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.21157626807689667, loss=1.7225629091262817
I0210 13:18:57.203308 140277044971264 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.21069557964801788, loss=1.7275758981704712
I0210 13:19:33.057902 140277053363968 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.1992253214120865, loss=1.6334142684936523
I0210 13:20:08.870913 140277044971264 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.1927858591079712, loss=1.710434913635254
I0210 13:20:44.692065 140277053363968 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.1853691190481186, loss=1.6584519147872925
I0210 13:21:20.532702 140277044971264 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.1822204738855362, loss=1.6730471849441528
I0210 13:21:56.356409 140277053363968 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.20114780962467194, loss=1.727433443069458
I0210 13:22:32.170478 140277044971264 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.1902063488960266, loss=1.7547613382339478
I0210 13:23:07.979577 140277053363968 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.19153544306755066, loss=1.5799164772033691
I0210 13:23:43.838259 140277044971264 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2134101241827011, loss=1.7018672227859497
I0210 13:24:19.657564 140277053363968 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.18253864347934723, loss=1.682136058807373
I0210 13:24:55.514546 140277044971264 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.19538868963718414, loss=1.676933765411377
I0210 13:25:31.353329 140277053363968 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.18411770462989807, loss=1.6605799198150635
I0210 13:26:07.222275 140277044971264 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.21083378791809082, loss=1.6115543842315674
I0210 13:26:43.037883 140277053363968 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.2020852118730545, loss=1.670129656791687
I0210 13:27:18.926185 140277044971264 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.20338886976242065, loss=1.6049375534057617
I0210 13:27:54.790909 140277053363968 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.20450182259082794, loss=1.6742514371871948
I0210 13:28:30.595293 140277044971264 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.20578213036060333, loss=1.671391248703003
I0210 13:29:06.412519 140277053363968 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.20491521060466766, loss=1.7044475078582764
I0210 13:29:42.220466 140277044971264 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.19967569410800934, loss=1.6078321933746338
I0210 13:30:18.034671 140277053363968 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.1968526393175125, loss=1.6201462745666504
I0210 13:30:53.832621 140277044971264 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.22216206789016724, loss=1.672050952911377
I0210 13:31:29.655051 140277053363968 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.24908819794654846, loss=1.6371917724609375
I0210 13:31:53.372751 140446903760704 spec.py:321] Evaluating on the training split.
I0210 13:31:56.382085 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 13:36:16.145612 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 13:36:18.857949 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 13:40:07.851504 140446903760704 spec.py:349] Evaluating on the test split.
I0210 13:40:10.558746 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 13:43:51.810409 140446903760704 submission_runner.py:408] Time since start: 42054.70s, 	Step: 70368, 	{'train/accuracy': 0.6629226207733154, 'train/loss': 1.5664702653884888, 'train/bleu': 32.78073814231867, 'validation/accuracy': 0.6758130788803101, 'validation/loss': 1.479118824005127, 'validation/bleu': 29.19225225701453, 'validation/num_examples': 3000, 'test/accuracy': 0.6903492212295532, 'test/loss': 1.3912490606307983, 'test/bleu': 28.780128995637558, 'test/num_examples': 3003, 'score': 25234.659603357315, 'total_duration': 42054.70087099075, 'accumulated_submission_time': 25234.659603357315, 'accumulated_eval_time': 16816.858570575714, 'accumulated_logging_time': 0.9333441257476807}
I0210 13:43:51.835819 140277044971264 logging_writer.py:48] [70368] accumulated_eval_time=16816.858571, accumulated_logging_time=0.933344, accumulated_submission_time=25234.659603, global_step=70368, preemption_count=0, score=25234.659603, test/accuracy=0.690349, test/bleu=28.780129, test/loss=1.391249, test/num_examples=3003, total_duration=42054.700871, train/accuracy=0.662923, train/bleu=32.780738, train/loss=1.566470, validation/accuracy=0.675813, validation/bleu=29.192252, validation/loss=1.479119, validation/num_examples=3000
I0210 13:44:03.614823 140277053363968 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.19794373214244843, loss=1.6558294296264648
I0210 13:44:39.264141 140277044971264 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.1964518129825592, loss=1.6115473508834839
I0210 13:45:15.077987 140277053363968 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.19129060208797455, loss=1.6866047382354736
I0210 13:45:50.885426 140277044971264 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2050352245569229, loss=1.7132694721221924
I0210 13:46:26.770753 140277053363968 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.19936923682689667, loss=1.6519289016723633
I0210 13:47:02.589535 140277044971264 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.21155479550361633, loss=1.7669991254806519
I0210 13:47:38.393458 140277053363968 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20130227506160736, loss=1.6873723268508911
I0210 13:48:14.226062 140277044971264 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2084600329399109, loss=1.6006100177764893
I0210 13:48:50.058644 140277053363968 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.20608174800872803, loss=1.6584751605987549
I0210 13:49:25.870554 140277044971264 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.2458079755306244, loss=1.7318958044052124
I0210 13:50:01.681990 140277053363968 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.19895119965076447, loss=1.5898910760879517
I0210 13:50:37.529844 140277044971264 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.22056365013122559, loss=1.6025969982147217
I0210 13:51:13.370356 140277053363968 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.20341160893440247, loss=1.6530500650405884
I0210 13:51:49.202270 140277044971264 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.3402160704135895, loss=1.6794819831848145
I0210 13:52:25.077695 140277053363968 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.192934051156044, loss=1.7503080368041992
I0210 13:53:00.907031 140277044971264 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.20297466218471527, loss=1.6076287031173706
I0210 13:53:36.710967 140277053363968 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.7300505638122559, loss=1.686643362045288
I0210 13:54:12.527530 140277044971264 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.20520196855068207, loss=1.690103530883789
I0210 13:54:48.362512 140277053363968 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.20004110038280487, loss=1.6306363344192505
I0210 13:55:24.172811 140277044971264 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.20865288376808167, loss=1.6287461519241333
I0210 13:55:59.982022 140277053363968 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.19518911838531494, loss=1.641078233718872
I0210 13:56:35.856814 140277044971264 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.19537346065044403, loss=1.6735210418701172
I0210 13:57:11.694941 140277053363968 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.20104998350143433, loss=1.5163229703903198
I0210 13:57:47.524996 140277044971264 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.19627273082733154, loss=1.7017467021942139
I0210 13:57:51.908809 140446903760704 spec.py:321] Evaluating on the training split.
I0210 13:57:54.926253 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:01:06.245856 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 14:01:08.949765 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:04:23.852288 140446903760704 spec.py:349] Evaluating on the test split.
I0210 14:04:26.576748 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:07:28.943002 140446903760704 submission_runner.py:408] Time since start: 43471.83s, 	Step: 72714, 	{'train/accuracy': 0.6606534719467163, 'train/loss': 1.593955636024475, 'train/bleu': 32.511623246878834, 'validation/accuracy': 0.6776605248451233, 'validation/loss': 1.4730331897735596, 'validation/bleu': 29.498651552431973, 'validation/num_examples': 3000, 'test/accuracy': 0.6935099959373474, 'test/loss': 1.380598545074463, 'test/bleu': 29.474507063397496, 'test/num_examples': 3003, 'score': 26074.649913072586, 'total_duration': 43471.833458423615, 'accumulated_submission_time': 26074.649913072586, 'accumulated_eval_time': 17393.892706871033, 'accumulated_logging_time': 0.9688091278076172}
I0210 14:07:28.968914 140277053363968 logging_writer.py:48] [72714] accumulated_eval_time=17393.892707, accumulated_logging_time=0.968809, accumulated_submission_time=26074.649913, global_step=72714, preemption_count=0, score=26074.649913, test/accuracy=0.693510, test/bleu=29.474507, test/loss=1.380599, test/num_examples=3003, total_duration=43471.833458, train/accuracy=0.660653, train/bleu=32.511623, train/loss=1.593956, validation/accuracy=0.677661, validation/bleu=29.498652, validation/loss=1.473033, validation/num_examples=3000
I0210 14:07:59.980841 140277044971264 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.19580912590026855, loss=1.647779107093811
I0210 14:08:35.740695 140277053363968 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.20379360020160675, loss=1.6544924974441528
I0210 14:09:11.533574 140277044971264 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.19416099786758423, loss=1.6620982885360718
I0210 14:09:47.347369 140277053363968 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.21983890235424042, loss=1.6161718368530273
I0210 14:10:23.126559 140277044971264 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.20736117660999298, loss=1.6614903211593628
I0210 14:10:58.956247 140277053363968 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.20262756943702698, loss=1.5835481882095337
I0210 14:11:34.791970 140277044971264 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2105126976966858, loss=1.678403615951538
I0210 14:12:10.629919 140277053363968 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.19553451240062714, loss=1.616392970085144
I0210 14:12:46.440703 140277044971264 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.19602206349372864, loss=1.6264694929122925
I0210 14:13:22.234581 140277053363968 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.2178335338830948, loss=1.6599364280700684
I0210 14:13:58.032026 140277044971264 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.19735285639762878, loss=1.6290971040725708
I0210 14:14:33.869242 140277053363968 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.19988341629505157, loss=1.6562772989273071
I0210 14:15:09.661751 140277044971264 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.20735186338424683, loss=1.6347624063491821
I0210 14:15:45.472109 140277053363968 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.20039008557796478, loss=1.6411046981811523
I0210 14:16:21.304569 140277044971264 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.21008838713169098, loss=1.6809985637664795
I0210 14:16:57.113647 140277053363968 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.2246374934911728, loss=1.6129413843154907
I0210 14:17:32.905133 140277044971264 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.1981545090675354, loss=1.727942943572998
I0210 14:18:08.711229 140277053363968 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.18320967257022858, loss=1.660239577293396
I0210 14:18:44.527637 140277044971264 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.19121894240379333, loss=1.669358491897583
I0210 14:19:20.404101 140277053363968 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.21275582909584045, loss=1.6775906085968018
I0210 14:19:56.258006 140277044971264 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.1997908502817154, loss=1.6240339279174805
I0210 14:20:32.080716 140277053363968 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.20722687244415283, loss=1.5520625114440918
I0210 14:21:07.938777 140277044971264 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.22502610087394714, loss=1.6352918148040771
I0210 14:21:29.137511 140446903760704 spec.py:321] Evaluating on the training split.
I0210 14:21:32.155716 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:24:39.618581 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 14:24:42.329934 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:27:07.394476 140446903760704 spec.py:349] Evaluating on the test split.
I0210 14:27:10.092374 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:29:31.600921 140446903760704 submission_runner.py:408] Time since start: 44794.49s, 	Step: 75061, 	{'train/accuracy': 0.6858612298965454, 'train/loss': 1.4403479099273682, 'train/bleu': 34.89402750189174, 'validation/accuracy': 0.677734911441803, 'validation/loss': 1.4711121320724487, 'validation/bleu': 29.468598279995035, 'validation/num_examples': 3000, 'test/accuracy': 0.6918134093284607, 'test/loss': 1.3813815116882324, 'test/bleu': 29.141061383739522, 'test/num_examples': 3003, 'score': 26914.733607769012, 'total_duration': 44794.491381406784, 'accumulated_submission_time': 26914.733607769012, 'accumulated_eval_time': 17876.356063604355, 'accumulated_logging_time': 1.006338119506836}
I0210 14:29:31.626402 140277053363968 logging_writer.py:48] [75061] accumulated_eval_time=17876.356064, accumulated_logging_time=1.006338, accumulated_submission_time=26914.733608, global_step=75061, preemption_count=0, score=26914.733608, test/accuracy=0.691813, test/bleu=29.141061, test/loss=1.381382, test/num_examples=3003, total_duration=44794.491381, train/accuracy=0.685861, train/bleu=34.894028, train/loss=1.440348, validation/accuracy=0.677735, validation/bleu=29.468598, validation/loss=1.471112, validation/num_examples=3000
I0210 14:29:45.902951 140277044971264 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.1916264295578003, loss=1.620102047920227
I0210 14:30:21.639912 140277053363968 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.23214119672775269, loss=1.6884558200836182
I0210 14:30:57.471866 140277044971264 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.21776437759399414, loss=1.5915915966033936
I0210 14:31:33.281427 140277053363968 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.17885024845600128, loss=1.515084147453308
I0210 14:32:09.169705 140277044971264 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.20946814119815826, loss=1.5775749683380127
I0210 14:32:45.000565 140277053363968 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.19500423967838287, loss=1.6955870389938354
I0210 14:33:20.848418 140277044971264 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.18857091665267944, loss=1.5881009101867676
I0210 14:33:56.693076 140277053363968 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2274063527584076, loss=1.6362215280532837
I0210 14:34:32.551271 140277044971264 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.21841122210025787, loss=1.6716482639312744
I0210 14:35:08.378961 140277053363968 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20265011489391327, loss=1.654833436012268
I0210 14:35:44.246036 140277044971264 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.23683157563209534, loss=1.6618852615356445
I0210 14:36:20.070463 140277053363968 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.20218105614185333, loss=1.6595745086669922
I0210 14:36:55.906519 140277044971264 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.20182019472122192, loss=1.7195085287094116
I0210 14:37:31.721912 140277053363968 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.22532245516777039, loss=1.700236439704895
I0210 14:38:07.594768 140277044971264 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.19097602367401123, loss=1.6420763731002808
I0210 14:38:43.472640 140277053363968 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.21162404119968414, loss=1.6320335865020752
I0210 14:39:19.329941 140277044971264 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.23387733101844788, loss=1.4924242496490479
I0210 14:39:55.225028 140277053363968 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.19718152284622192, loss=1.6642416715621948
I0210 14:40:31.063774 140277044971264 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.19911953806877136, loss=1.6678673028945923
I0210 14:41:06.904805 140277053363968 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.2094973474740982, loss=1.7009358406066895
I0210 14:41:42.759705 140277044971264 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.19359466433525085, loss=1.6492079496383667
I0210 14:42:18.597260 140277053363968 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2011990249156952, loss=1.6623306274414062
I0210 14:42:54.443914 140277044971264 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.20151737332344055, loss=1.697955846786499
I0210 14:43:30.379428 140277053363968 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.20231479406356812, loss=1.6731839179992676
I0210 14:43:31.904502 140446903760704 spec.py:321] Evaluating on the training split.
I0210 14:43:34.927194 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:47:35.191793 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 14:47:37.883221 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:50:55.407332 140446903760704 spec.py:349] Evaluating on the test split.
I0210 14:50:58.119562 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 14:54:22.288252 140446903760704 submission_runner.py:408] Time since start: 46285.18s, 	Step: 77406, 	{'train/accuracy': 0.665233850479126, 'train/loss': 1.5531893968582153, 'train/bleu': 33.34801089317181, 'validation/accuracy': 0.6795451641082764, 'validation/loss': 1.4581555128097534, 'validation/bleu': 29.55528459819339, 'validation/num_examples': 3000, 'test/accuracy': 0.694428026676178, 'test/loss': 1.3683438301086426, 'test/bleu': 29.284875716363665, 'test/num_examples': 3003, 'score': 27754.925392866135, 'total_duration': 46285.17871427536, 'accumulated_submission_time': 27754.925392866135, 'accumulated_eval_time': 18526.739768743515, 'accumulated_logging_time': 1.0434327125549316}
I0210 14:54:22.315346 140277044971264 logging_writer.py:48] [77406] accumulated_eval_time=18526.739769, accumulated_logging_time=1.043433, accumulated_submission_time=27754.925393, global_step=77406, preemption_count=0, score=27754.925393, test/accuracy=0.694428, test/bleu=29.284876, test/loss=1.368344, test/num_examples=3003, total_duration=46285.178714, train/accuracy=0.665234, train/bleu=33.348011, train/loss=1.553189, validation/accuracy=0.679545, validation/bleu=29.555285, validation/loss=1.458156, validation/num_examples=3000
I0210 14:54:56.220442 140277053363968 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.211501806974411, loss=1.6227141618728638
I0210 14:55:31.991213 140277044971264 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.21359515190124512, loss=1.5880573987960815
I0210 14:56:07.823355 140277053363968 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.18793267011642456, loss=1.6022309064865112
I0210 14:56:43.679764 140277044971264 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.207166388630867, loss=1.5771448612213135
I0210 14:57:19.519881 140277053363968 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.2082216590642929, loss=1.7057660818099976
I0210 14:57:55.389053 140277044971264 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.23421511054039001, loss=1.665083885192871
I0210 14:58:31.292368 140277053363968 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2037220150232315, loss=1.6214178800582886
I0210 14:59:07.274757 140277044971264 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.18863336741924286, loss=1.6352044343948364
I0210 14:59:43.112372 140277053363968 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.21069547533988953, loss=1.6371878385543823
I0210 15:00:18.932770 140277044971264 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.23178702592849731, loss=1.5835883617401123
I0210 15:00:54.790740 140277053363968 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.19463826715946198, loss=1.5951327085494995
I0210 15:01:30.632288 140277044971264 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.21276019513607025, loss=1.7232617139816284
I0210 15:02:06.475677 140277053363968 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.24580053985118866, loss=1.6818749904632568
I0210 15:02:42.302076 140277044971264 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.20346786081790924, loss=1.6378260850906372
I0210 15:03:18.173230 140277053363968 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.19956223666667938, loss=1.6373271942138672
I0210 15:03:54.019742 140277044971264 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.20777612924575806, loss=1.686524748802185
I0210 15:04:29.864929 140277053363968 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.19012610614299774, loss=1.6188679933547974
I0210 15:05:05.701948 140277044971264 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2129959613084793, loss=1.6146540641784668
I0210 15:05:41.563499 140277053363968 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.19205787777900696, loss=1.6306926012039185
I0210 15:06:17.413595 140277044971264 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.23471994698047638, loss=1.6320337057113647
I0210 15:06:53.232293 140277053363968 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2172200083732605, loss=1.5721343755722046
I0210 15:07:29.069577 140277044971264 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.2216048538684845, loss=1.59796142578125
I0210 15:08:04.933132 140277053363968 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.177610397338867, loss=1.8054137229919434
I0210 15:08:22.580578 140446903760704 spec.py:321] Evaluating on the training split.
I0210 15:08:25.590354 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 15:12:30.703186 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 15:12:33.419266 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 15:15:21.305826 140446903760704 spec.py:349] Evaluating on the test split.
I0210 15:15:23.999771 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 15:17:53.487430 140446903760704 submission_runner.py:408] Time since start: 47696.38s, 	Step: 79751, 	{'train/accuracy': 0.6607846617698669, 'train/loss': 1.5785906314849854, 'train/bleu': 33.05226990077929, 'validation/accuracy': 0.6790120601654053, 'validation/loss': 1.4651505947113037, 'validation/bleu': 29.44278157741172, 'validation/num_examples': 3000, 'test/accuracy': 0.6931613683700562, 'test/loss': 1.3743035793304443, 'test/bleu': 29.269250344436912, 'test/num_examples': 3003, 'score': 28595.104697227478, 'total_duration': 47696.377888441086, 'accumulated_submission_time': 28595.104697227478, 'accumulated_eval_time': 19097.646564006805, 'accumulated_logging_time': 1.0817084312438965}
I0210 15:17:53.514226 140277044971264 logging_writer.py:48] [79751] accumulated_eval_time=19097.646564, accumulated_logging_time=1.081708, accumulated_submission_time=28595.104697, global_step=79751, preemption_count=0, score=28595.104697, test/accuracy=0.693161, test/bleu=29.269250, test/loss=1.374304, test/num_examples=3003, total_duration=47696.377888, train/accuracy=0.660785, train/bleu=33.052270, train/loss=1.578591, validation/accuracy=0.679012, validation/bleu=29.442782, validation/loss=1.465151, validation/num_examples=3000
I0210 15:18:11.350618 140277053363968 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.33772191405296326, loss=1.6168147325515747
I0210 15:18:47.064220 140277044971264 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.1917952448129654, loss=1.656334638595581
I0210 15:19:22.874103 140277053363968 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2087063491344452, loss=1.5709861516952515
I0210 15:19:58.686779 140277044971264 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2278371900320053, loss=1.6697336435317993
I0210 15:20:34.513046 140277053363968 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.19310572743415833, loss=1.6246610879898071
I0210 15:21:10.334500 140277044971264 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2007385492324829, loss=1.6537576913833618
I0210 15:21:46.135185 140277053363968 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.1987328976392746, loss=1.6331533193588257
I0210 15:22:21.952067 140277044971264 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.192567840218544, loss=1.6319830417633057
I0210 15:22:57.767794 140277053363968 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.21194659173488617, loss=1.6442699432373047
I0210 15:23:33.580162 140277044971264 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.21352128684520721, loss=1.663609504699707
I0210 15:24:09.412147 140277053363968 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.19643904268741608, loss=1.5982991456985474
I0210 15:24:45.213750 140277044971264 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.1955064833164215, loss=1.69732666015625
I0210 15:25:21.060384 140277053363968 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.19672872126102448, loss=1.6514195203781128
I0210 15:25:56.898452 140277044971264 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.1886383444070816, loss=1.5772117376327515
I0210 15:26:32.721918 140277053363968 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.20552286505699158, loss=1.578454613685608
I0210 15:27:08.572126 140277044971264 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.20569153130054474, loss=1.534010648727417
I0210 15:27:44.377615 140277053363968 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.2019512802362442, loss=1.7593121528625488
I0210 15:28:20.225070 140277044971264 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.19699187576770782, loss=1.587851643562317
I0210 15:28:56.082746 140277053363968 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.20421968400478363, loss=1.6623179912567139
I0210 15:29:31.934728 140277044971264 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.1947333663702011, loss=1.5975737571716309
I0210 15:30:07.766472 140277053363968 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.19363506138324738, loss=1.6622318029403687
I0210 15:30:43.600284 140277044971264 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.18817637860774994, loss=1.6112086772918701
I0210 15:31:19.408640 140277053363968 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.24578963220119476, loss=1.5745468139648438
I0210 15:31:53.487673 140446903760704 spec.py:321] Evaluating on the training split.
I0210 15:31:56.501336 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 15:35:30.182764 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 15:35:32.890754 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 15:38:27.834858 140446903760704 spec.py:349] Evaluating on the test split.
I0210 15:38:30.537503 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 15:40:57.983645 140446903760704 submission_runner.py:408] Time since start: 49080.87s, 	Step: 82097, 	{'train/accuracy': 0.6755383014678955, 'train/loss': 1.49465811252594, 'train/bleu': 33.659196883772914, 'validation/accuracy': 0.6815910339355469, 'validation/loss': 1.449007272720337, 'validation/bleu': 29.689568520414724, 'validation/num_examples': 3000, 'test/accuracy': 0.6955435872077942, 'test/loss': 1.3564544916152954, 'test/bleu': 29.427192194045116, 'test/num_examples': 3003, 'score': 29434.99210190773, 'total_duration': 49080.874106407166, 'accumulated_submission_time': 29434.99210190773, 'accumulated_eval_time': 19642.142485380173, 'accumulated_logging_time': 1.1191542148590088}
I0210 15:40:58.010289 140277044971264 logging_writer.py:48] [82097] accumulated_eval_time=19642.142485, accumulated_logging_time=1.119154, accumulated_submission_time=29434.992102, global_step=82097, preemption_count=0, score=29434.992102, test/accuracy=0.695544, test/bleu=29.427192, test/loss=1.356454, test/num_examples=3003, total_duration=49080.874106, train/accuracy=0.675538, train/bleu=33.659197, train/loss=1.494658, validation/accuracy=0.681591, validation/bleu=29.689569, validation/loss=1.449007, validation/num_examples=3000
I0210 15:40:59.460265 140277053363968 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.19733504951000214, loss=1.6050044298171997
I0210 15:41:35.102390 140277044971264 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.20334620773792267, loss=1.564619779586792
I0210 15:42:10.929114 140277053363968 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.20139047503471375, loss=1.6874256134033203
I0210 15:42:46.804171 140277044971264 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.18826115131378174, loss=1.599048137664795
I0210 15:43:22.718904 140277053363968 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.20023496448993683, loss=1.5739694833755493
I0210 15:43:58.515650 140277044971264 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.201568603515625, loss=1.5914746522903442
I0210 15:44:34.334626 140277053363968 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.1938321888446808, loss=1.6108297109603882
I0210 15:45:10.151544 140277044971264 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.21953020989894867, loss=1.5710902214050293
I0210 15:45:45.958781 140277053363968 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.20833231508731842, loss=1.6335254907608032
I0210 15:46:21.826854 140277044971264 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.20676983892917633, loss=1.5544121265411377
I0210 15:46:57.693260 140277053363968 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.19962109625339508, loss=1.575939655303955
I0210 15:47:33.523229 140277044971264 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.21114322543144226, loss=1.6024067401885986
I0210 15:48:09.326433 140277053363968 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.18988585472106934, loss=1.5688949823379517
I0210 15:48:45.128380 140277044971264 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.22017371654510498, loss=1.5994460582733154
I0210 15:49:20.999479 140277053363968 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.2014172375202179, loss=1.597489356994629
I0210 15:49:56.850737 140277044971264 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.2106478214263916, loss=1.6321427822113037
I0210 15:50:32.681194 140277053363968 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.21082012355327606, loss=1.67874276638031
I0210 15:51:08.536719 140277044971264 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2087583690881729, loss=1.6816954612731934
I0210 15:51:44.397751 140277053363968 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.1959536224603653, loss=1.6524372100830078
I0210 15:52:20.235316 140277044971264 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21022994816303253, loss=1.592989206314087
I0210 15:52:56.068361 140277053363968 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.2301156222820282, loss=1.6211477518081665
I0210 15:53:31.910907 140277044971264 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.20539933443069458, loss=1.6299681663513184
I0210 15:54:07.724215 140277053363968 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.20977944135665894, loss=1.6663283109664917
I0210 15:54:43.580620 140277044971264 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.20567555725574493, loss=1.6435964107513428
I0210 15:54:57.997482 140446903760704 spec.py:321] Evaluating on the training split.
I0210 15:55:01.002470 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 15:59:13.720172 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 15:59:16.424354 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:02:33.569946 140446903760704 spec.py:349] Evaluating on the test split.
I0210 16:02:36.272452 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:06:05.643393 140446903760704 submission_runner.py:408] Time since start: 50588.53s, 	Step: 84442, 	{'train/accuracy': 0.6705901026725769, 'train/loss': 1.5163570642471313, 'train/bleu': 33.776368870019915, 'validation/accuracy': 0.6826077699661255, 'validation/loss': 1.4386560916900635, 'validation/bleu': 29.743246304174004, 'validation/num_examples': 3000, 'test/accuracy': 0.697402834892273, 'test/loss': 1.3449287414550781, 'test/bleu': 29.554429663796398, 'test/num_examples': 3003, 'score': 30274.89132285118, 'total_duration': 50588.53385710716, 'accumulated_submission_time': 30274.89132285118, 'accumulated_eval_time': 20309.78834581375, 'accumulated_logging_time': 1.156053066253662}
I0210 16:06:05.670717 140277053363968 logging_writer.py:48] [84442] accumulated_eval_time=20309.788346, accumulated_logging_time=1.156053, accumulated_submission_time=30274.891323, global_step=84442, preemption_count=0, score=30274.891323, test/accuracy=0.697403, test/bleu=29.554430, test/loss=1.344929, test/num_examples=3003, total_duration=50588.533857, train/accuracy=0.670590, train/bleu=33.776369, train/loss=1.516357, validation/accuracy=0.682608, validation/bleu=29.743246, validation/loss=1.438656, validation/num_examples=3000
I0210 16:06:26.698723 140277044971264 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.20643502473831177, loss=1.6745163202285767
I0210 16:07:02.578859 140277053363968 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.19830983877182007, loss=1.6108283996582031
I0210 16:07:38.381665 140277044971264 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.5055400133132935, loss=1.5522900819778442
I0210 16:08:14.170650 140277053363968 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.22491295635700226, loss=1.6023242473602295
I0210 16:08:50.058827 140277044971264 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.20251621305942535, loss=1.6000629663467407
I0210 16:09:25.869382 140277053363968 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2070537954568863, loss=1.6301734447479248
I0210 16:10:01.727860 140277044971264 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.2074119597673416, loss=1.6023485660552979
I0210 16:10:37.550842 140277053363968 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.1938224732875824, loss=1.6184461116790771
I0210 16:11:13.384740 140277044971264 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.21460691094398499, loss=1.6804417371749878
I0210 16:11:49.244735 140277053363968 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.20525269210338593, loss=1.618245005607605
I0210 16:12:25.073093 140277044971264 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.20890066027641296, loss=1.6852312088012695
I0210 16:13:00.915151 140277053363968 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2108820676803589, loss=1.6311546564102173
I0210 16:13:36.724995 140277044971264 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.21799448132514954, loss=1.6252373456954956
I0210 16:14:12.547585 140277053363968 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.20727680623531342, loss=1.6086323261260986
I0210 16:14:48.380733 140277044971264 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.2000451385974884, loss=1.641835331916809
I0210 16:15:24.203105 140277053363968 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.2189599573612213, loss=1.6631970405578613
I0210 16:16:00.010483 140277044971264 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.21475647389888763, loss=1.681013584136963
I0210 16:16:35.842308 140277053363968 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.2212231606245041, loss=1.5934985876083374
I0210 16:17:11.662328 140277044971264 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.20532821118831635, loss=1.638100504875183
I0210 16:17:47.456743 140277053363968 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.22169865667819977, loss=1.6300629377365112
I0210 16:18:23.300089 140277044971264 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2131803184747696, loss=1.639815092086792
I0210 16:18:59.100338 140277053363968 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.21033355593681335, loss=1.6385974884033203
I0210 16:19:34.922445 140277044971264 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.19825628399848938, loss=1.6048214435577393
I0210 16:20:05.801939 140446903760704 spec.py:321] Evaluating on the training split.
I0210 16:20:08.814634 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:23:47.541100 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 16:23:50.248680 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:26:15.453097 140446903760704 spec.py:349] Evaluating on the test split.
I0210 16:26:18.148010 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:28:41.049717 140446903760704 submission_runner.py:408] Time since start: 51943.94s, 	Step: 86788, 	{'train/accuracy': 0.6693463325500488, 'train/loss': 1.5310808420181274, 'train/bleu': 33.519672285560624, 'validation/accuracy': 0.682632565498352, 'validation/loss': 1.4379298686981201, 'validation/bleu': 30.200129200007808, 'validation/num_examples': 3000, 'test/accuracy': 0.6986927390098572, 'test/loss': 1.3387260437011719, 'test/bleu': 29.753769352763033, 'test/num_examples': 3003, 'score': 31114.93718099594, 'total_duration': 51943.9401807785, 'accumulated_submission_time': 31114.93718099594, 'accumulated_eval_time': 20825.03607749939, 'accumulated_logging_time': 1.1935102939605713}
I0210 16:28:41.076488 140277053363968 logging_writer.py:48] [86788] accumulated_eval_time=20825.036077, accumulated_logging_time=1.193510, accumulated_submission_time=31114.937181, global_step=86788, preemption_count=0, score=31114.937181, test/accuracy=0.698693, test/bleu=29.753769, test/loss=1.338726, test/num_examples=3003, total_duration=51943.940181, train/accuracy=0.669346, train/bleu=33.519672, train/loss=1.531081, validation/accuracy=0.682633, validation/bleu=30.200129, validation/loss=1.437930, validation/num_examples=3000
I0210 16:28:45.720741 140277044971264 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.21128934621810913, loss=1.5472257137298584
I0210 16:29:21.411053 140277053363968 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.20294184982776642, loss=1.5827385187149048
I0210 16:29:57.222242 140277044971264 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2177465558052063, loss=1.5606462955474854
I0210 16:30:33.042771 140277053363968 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.2051161676645279, loss=1.6167058944702148
I0210 16:31:08.850780 140277044971264 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.20897698402404785, loss=1.5864988565444946
I0210 16:31:44.662392 140277053363968 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.215376615524292, loss=1.5658336877822876
I0210 16:32:20.542905 140277044971264 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.20327970385551453, loss=1.5810024738311768
I0210 16:32:56.437295 140277053363968 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.2153787910938263, loss=1.5252996683120728
I0210 16:33:32.274704 140277044971264 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.2086443156003952, loss=1.6152950525283813
I0210 16:34:08.101324 140277053363968 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.19234740734100342, loss=1.5762237310409546
I0210 16:34:43.965403 140277044971264 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.20296631753444672, loss=1.5327115058898926
I0210 16:35:19.771708 140277053363968 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.2103906273841858, loss=1.6076548099517822
I0210 16:35:55.597285 140277044971264 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.21317824721336365, loss=1.5994786024093628
I0210 16:36:31.453785 140277053363968 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.2018243372440338, loss=1.6143662929534912
I0210 16:37:07.274355 140277044971264 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.2109813243150711, loss=1.6311495304107666
I0210 16:37:43.072156 140277053363968 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.20028485357761383, loss=1.601285696029663
I0210 16:38:18.911379 140277044971264 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.21695512533187866, loss=1.5644428730010986
I0210 16:38:54.806035 140277053363968 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21515679359436035, loss=1.5691146850585938
I0210 16:39:30.658650 140277044971264 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.21672631800174713, loss=1.6346317529678345
I0210 16:40:06.524971 140277053363968 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.23331500589847565, loss=1.6479219198226929
I0210 16:40:42.460762 140277044971264 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.20421554148197174, loss=1.5993572473526
I0210 16:41:18.344550 140277053363968 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2335875928401947, loss=1.573392629623413
I0210 16:41:54.248996 140277044971264 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.19735774397850037, loss=1.5492674112319946
I0210 16:42:30.080272 140277053363968 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.21344392001628876, loss=1.570639967918396
I0210 16:42:41.257063 140446903760704 spec.py:321] Evaluating on the training split.
I0210 16:42:44.272605 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:46:34.579486 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 16:46:37.289688 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:49:41.480304 140446903760704 spec.py:349] Evaluating on the test split.
I0210 16:49:44.193373 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 16:52:41.488373 140446903760704 submission_runner.py:408] Time since start: 53384.38s, 	Step: 89133, 	{'train/accuracy': 0.6757782101631165, 'train/loss': 1.485595941543579, 'train/bleu': 34.190875248660575, 'validation/accuracy': 0.6854719519615173, 'validation/loss': 1.4247512817382812, 'validation/bleu': 29.772108184170435, 'validation/num_examples': 3000, 'test/accuracy': 0.6976584792137146, 'test/loss': 1.333077311515808, 'test/bleu': 29.528959397441458, 'test/num_examples': 3003, 'score': 31955.03034901619, 'total_duration': 53384.378826379776, 'accumulated_submission_time': 31955.03034901619, 'accumulated_eval_time': 21425.267327308655, 'accumulated_logging_time': 1.2301530838012695}
I0210 16:52:41.522397 140277044971264 logging_writer.py:48] [89133] accumulated_eval_time=21425.267327, accumulated_logging_time=1.230153, accumulated_submission_time=31955.030349, global_step=89133, preemption_count=0, score=31955.030349, test/accuracy=0.697658, test/bleu=29.528959, test/loss=1.333077, test/num_examples=3003, total_duration=53384.378826, train/accuracy=0.675778, train/bleu=34.190875, train/loss=1.485596, validation/accuracy=0.685472, validation/bleu=29.772108, validation/loss=1.424751, validation/num_examples=3000
I0210 16:53:05.763319 140277053363968 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.19464880228042603, loss=1.6211587190628052
I0210 16:53:41.484547 140277044971264 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.21454225480556488, loss=1.5071676969528198
I0210 16:54:17.311053 140277053363968 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.20419242978096008, loss=1.503116250038147
I0210 16:54:53.163501 140277044971264 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.22733299434185028, loss=1.6447752714157104
I0210 16:55:28.986658 140277053363968 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.21633265912532806, loss=1.6220752000808716
I0210 16:56:04.796185 140277044971264 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.20227372646331787, loss=1.630819320678711
I0210 16:56:40.628898 140277053363968 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.22620336711406708, loss=1.5712093114852905
I0210 16:57:16.472887 140277044971264 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.20834626257419586, loss=1.6487597227096558
I0210 16:57:52.305439 140277053363968 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.20892095565795898, loss=1.6297316551208496
I0210 16:58:28.144094 140277044971264 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.21284927427768707, loss=1.6484363079071045
I0210 16:59:03.978379 140277053363968 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2131355255842209, loss=1.5987834930419922
I0210 16:59:39.779204 140277044971264 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.21560101211071014, loss=1.6563180685043335
I0210 17:00:15.594412 140277053363968 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.21009710431098938, loss=1.6212458610534668
I0210 17:00:51.445922 140277044971264 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.23108436167240143, loss=1.5565203428268433
I0210 17:01:27.287151 140277053363968 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.20608054101467133, loss=1.6404141187667847
I0210 17:02:03.102665 140277044971264 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.2112933099269867, loss=1.5874075889587402
I0210 17:02:38.921338 140277053363968 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.20120124518871307, loss=1.660101056098938
I0210 17:03:14.749132 140277044971264 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.21502169966697693, loss=1.5753264427185059
I0210 17:03:50.558146 140277053363968 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.2080152928829193, loss=1.5737968683242798
I0210 17:04:26.386947 140277044971264 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.20400092005729675, loss=1.5037959814071655
I0210 17:05:02.234549 140277053363968 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.2122153490781784, loss=1.606688141822815
I0210 17:05:38.082132 140277044971264 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.19732949137687683, loss=1.5336209535598755
I0210 17:06:13.906718 140277053363968 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.2080356925725937, loss=1.5513302087783813
I0210 17:06:41.542672 140446903760704 spec.py:321] Evaluating on the training split.
I0210 17:06:44.559783 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 17:10:36.730985 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 17:10:39.442504 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 17:14:04.228416 140446903760704 spec.py:349] Evaluating on the test split.
I0210 17:14:06.934525 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 17:17:08.286104 140446903760704 submission_runner.py:408] Time since start: 54851.18s, 	Step: 91479, 	{'train/accuracy': 0.6764565110206604, 'train/loss': 1.4868851900100708, 'train/bleu': 33.79142281553153, 'validation/accuracy': 0.6856207251548767, 'validation/loss': 1.4216893911361694, 'validation/bleu': 30.081463864085272, 'validation/num_examples': 3000, 'test/accuracy': 0.7033525109291077, 'test/loss': 1.3221434354782104, 'test/bleu': 30.261509158958816, 'test/num_examples': 3003, 'score': 32794.96316599846, 'total_duration': 54851.176552057266, 'accumulated_submission_time': 32794.96316599846, 'accumulated_eval_time': 22052.010696411133, 'accumulated_logging_time': 1.276489496231079}
I0210 17:17:08.313768 140277044971264 logging_writer.py:48] [91479] accumulated_eval_time=22052.010696, accumulated_logging_time=1.276489, accumulated_submission_time=32794.963166, global_step=91479, preemption_count=0, score=32794.963166, test/accuracy=0.703353, test/bleu=30.261509, test/loss=1.322143, test/num_examples=3003, total_duration=54851.176552, train/accuracy=0.676457, train/bleu=33.791423, train/loss=1.486885, validation/accuracy=0.685621, validation/bleu=30.081464, validation/loss=1.421689, validation/num_examples=3000
I0210 17:17:16.186705 140277053363968 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.21479646861553192, loss=1.558669924736023
I0210 17:17:51.886105 140277044971264 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.2143925577402115, loss=1.6445363759994507
I0210 17:18:27.706360 140277053363968 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.20562130212783813, loss=1.575323462486267
I0210 17:19:03.489745 140277044971264 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.21322789788246155, loss=1.557405710220337
I0210 17:19:39.356755 140277053363968 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.23030690848827362, loss=1.522229790687561
I0210 17:20:15.170607 140277044971264 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.22892232239246368, loss=1.6261886358261108
I0210 17:20:50.983128 140277053363968 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.22637036442756653, loss=1.627055287361145
I0210 17:21:26.777315 140277044971264 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.21516409516334534, loss=1.5899202823638916
I0210 17:22:02.597596 140277053363968 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21667616069316864, loss=1.530933141708374
I0210 17:22:38.441710 140277044971264 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.2269069403409958, loss=1.6432454586029053
I0210 17:23:14.302371 140277053363968 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.22893068194389343, loss=1.5695173740386963
I0210 17:23:50.088627 140277044971264 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.22411686182022095, loss=1.5587565898895264
I0210 17:24:25.897746 140277053363968 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.21160945296287537, loss=1.5443763732910156
I0210 17:25:01.752541 140277044971264 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.21750831604003906, loss=1.5684008598327637
I0210 17:25:37.626541 140277053363968 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.20006106793880463, loss=1.5361946821212769
I0210 17:26:13.511653 140277044971264 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.22398926317691803, loss=1.6266188621520996
I0210 17:26:49.357478 140277053363968 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.21775518357753754, loss=1.647955298423767
I0210 17:27:25.198762 140277044971264 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.19944874942302704, loss=1.593401312828064
I0210 17:28:00.990485 140277053363968 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.21418794989585876, loss=1.5825748443603516
I0210 17:28:36.776551 140277044971264 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.2078508734703064, loss=1.5885872840881348
I0210 17:29:12.605766 140277053363968 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.21833980083465576, loss=1.5313215255737305
I0210 17:29:48.438678 140277044971264 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.2143431454896927, loss=1.5582634210586548
I0210 17:30:24.267120 140277053363968 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.1981065422296524, loss=1.5223808288574219
I0210 17:31:00.096845 140277044971264 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.20016418397426605, loss=1.4963451623916626
I0210 17:31:08.435781 140446903760704 spec.py:321] Evaluating on the training split.
I0210 17:31:11.465259 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 17:34:49.752654 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 17:34:52.449256 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 17:37:59.029091 140446903760704 spec.py:349] Evaluating on the test split.
I0210 17:38:01.732545 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 17:40:49.792306 140446903760704 submission_runner.py:408] Time since start: 56272.68s, 	Step: 93825, 	{'train/accuracy': 0.690422534942627, 'train/loss': 1.4100230932235718, 'train/bleu': 34.952902904917686, 'validation/accuracy': 0.685050368309021, 'validation/loss': 1.4175060987472534, 'validation/bleu': 30.129531487347982, 'validation/num_examples': 3000, 'test/accuracy': 0.7028295993804932, 'test/loss': 1.319978952407837, 'test/bleu': 30.10693494571689, 'test/num_examples': 3003, 'score': 33634.999903678894, 'total_duration': 56272.6827609539, 'accumulated_submission_time': 33634.999903678894, 'accumulated_eval_time': 22633.36717224121, 'accumulated_logging_time': 1.315727710723877}
I0210 17:40:49.820997 140277053363968 logging_writer.py:48] [93825] accumulated_eval_time=22633.367172, accumulated_logging_time=1.315728, accumulated_submission_time=33634.999904, global_step=93825, preemption_count=0, score=33634.999904, test/accuracy=0.702830, test/bleu=30.106935, test/loss=1.319979, test/num_examples=3003, total_duration=56272.682761, train/accuracy=0.690423, train/bleu=34.952903, train/loss=1.410023, validation/accuracy=0.685050, validation/bleu=30.129531, validation/loss=1.417506, validation/num_examples=3000
I0210 17:41:16.910315 140277044971264 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.22908030450344086, loss=1.6105093955993652
I0210 17:41:52.667054 140277053363968 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.21466895937919617, loss=1.5209795236587524
I0210 17:42:28.498771 140277044971264 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.20118485391139984, loss=1.5403319597244263
I0210 17:43:04.321953 140277053363968 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.21159407496452332, loss=1.585231900215149
I0210 17:43:40.154771 140277044971264 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.2059461921453476, loss=1.5994477272033691
I0210 17:44:15.999430 140277053363968 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.2093505710363388, loss=1.569710612297058
I0210 17:44:51.846058 140277044971264 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21227747201919556, loss=1.5897539854049683
I0210 17:45:27.709318 140277053363968 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.20455613732337952, loss=1.6208771467208862
I0210 17:46:03.533150 140277044971264 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.2264283299446106, loss=1.5585079193115234
I0210 17:46:39.425537 140277053363968 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.20894436538219452, loss=1.532495141029358
I0210 17:47:15.272479 140277044971264 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.20802992582321167, loss=1.5552817583084106
I0210 17:47:51.065502 140277053363968 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.21560095250606537, loss=1.5323007106781006
I0210 17:48:26.888399 140277044971264 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.2196817845106125, loss=1.5895566940307617
I0210 17:49:02.698490 140277053363968 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.21209049224853516, loss=1.6171786785125732
I0210 17:49:38.552435 140277044971264 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.21563275158405304, loss=1.5942682027816772
I0210 17:50:14.370462 140277053363968 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.21793000400066376, loss=1.4845285415649414
I0210 17:50:50.164713 140277044971264 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.21699023246765137, loss=1.4877277612686157
I0210 17:51:25.970437 140277053363968 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.21423687040805817, loss=1.5333434343338013
I0210 17:52:01.837578 140277044971264 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.21634523570537567, loss=1.6420176029205322
I0210 17:52:37.712436 140277053363968 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21528907120227814, loss=1.6173102855682373
I0210 17:53:13.554996 140277044971264 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.2417127639055252, loss=1.642187476158142
I0210 17:53:49.360682 140277053363968 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.22250959277153015, loss=1.5444031953811646
I0210 17:54:25.192598 140277044971264 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.20970474183559418, loss=1.5358014106750488
I0210 17:54:50.028639 140446903760704 spec.py:321] Evaluating on the training split.
I0210 17:54:53.038850 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 17:59:17.727053 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 17:59:20.433663 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:03:39.632385 140446903760704 spec.py:349] Evaluating on the test split.
I0210 18:03:42.334279 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:08:08.833730 140446903760704 submission_runner.py:408] Time since start: 57911.72s, 	Step: 96171, 	{'train/accuracy': 0.6796119809150696, 'train/loss': 1.4651366472244263, 'train/bleu': 34.32243432293985, 'validation/accuracy': 0.6871210336685181, 'validation/loss': 1.4065537452697754, 'validation/bleu': 30.200335772061464, 'validation/num_examples': 3000, 'test/accuracy': 0.7031084895133972, 'test/loss': 1.311916708946228, 'test/bleu': 30.018684147444766, 'test/num_examples': 3003, 'score': 34475.117656469345, 'total_duration': 57911.7241795063, 'accumulated_submission_time': 34475.117656469345, 'accumulated_eval_time': 23432.17221236229, 'accumulated_logging_time': 1.3558223247528076}
I0210 18:08:08.862250 140277053363968 logging_writer.py:48] [96171] accumulated_eval_time=23432.172212, accumulated_logging_time=1.355822, accumulated_submission_time=34475.117656, global_step=96171, preemption_count=0, score=34475.117656, test/accuracy=0.703108, test/bleu=30.018684, test/loss=1.311917, test/num_examples=3003, total_duration=57911.724180, train/accuracy=0.679612, train/bleu=34.322434, train/loss=1.465137, validation/accuracy=0.687121, validation/bleu=30.200336, validation/loss=1.406554, validation/num_examples=3000
I0210 18:08:19.566437 140277044971264 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.21246500313282013, loss=1.520949363708496
I0210 18:08:55.246445 140277053363968 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.227308988571167, loss=1.609743356704712
I0210 18:09:31.038871 140277044971264 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.2252519279718399, loss=1.6145355701446533
I0210 18:10:06.849251 140277053363968 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.21536016464233398, loss=1.5323129892349243
I0210 18:10:42.641613 140277044971264 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.23328569531440735, loss=1.6248129606246948
I0210 18:11:18.439879 140277053363968 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.27825793623924255, loss=1.5311684608459473
I0210 18:11:54.289146 140277044971264 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.22263500094413757, loss=1.6289489269256592
I0210 18:12:30.096304 140277053363968 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.21196310222148895, loss=1.569352626800537
I0210 18:13:05.951445 140277044971264 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.21729543805122375, loss=1.5676379203796387
I0210 18:13:41.789568 140277053363968 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.20895808935165405, loss=1.5077704191207886
I0210 18:14:17.673123 140277044971264 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.21957743167877197, loss=1.5090928077697754
I0210 18:14:53.478401 140277053363968 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.21804367005825043, loss=1.575577974319458
I0210 18:15:29.298856 140277044971264 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.2130366563796997, loss=1.5579400062561035
I0210 18:16:05.115134 140277053363968 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.21410511434078217, loss=1.5745909214019775
I0210 18:16:40.928575 140277044971264 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.22521236538887024, loss=1.5649164915084839
I0210 18:17:16.728401 140277053363968 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.21499501168727875, loss=1.544703722000122
I0210 18:17:52.587245 140277044971264 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.21268656849861145, loss=1.5944617986679077
I0210 18:18:28.436171 140277053363968 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.21524138748645782, loss=1.5351824760437012
I0210 18:19:04.270277 140277044971264 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.21651913225650787, loss=1.6294652223587036
I0210 18:19:40.134927 140277053363968 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.23488961160182953, loss=1.548242211341858
I0210 18:20:15.964348 140277044971264 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.21600273251533508, loss=1.5868463516235352
I0210 18:20:51.930977 140277053363968 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.2176491767168045, loss=1.6287425756454468
I0210 18:21:27.744669 140277044971264 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21219664812088013, loss=1.4784427881240845
I0210 18:22:03.571043 140277053363968 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.22905392944812775, loss=1.564192771911621
I0210 18:22:09.025028 140446903760704 spec.py:321] Evaluating on the training split.
I0210 18:22:12.033826 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:26:11.729892 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 18:26:14.430577 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:28:47.095455 140446903760704 spec.py:349] Evaluating on the test split.
I0210 18:28:49.796617 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:31:03.640403 140446903760704 submission_runner.py:408] Time since start: 59286.53s, 	Step: 98517, 	{'train/accuracy': 0.6749527454376221, 'train/loss': 1.4922711849212646, 'train/bleu': 33.76268337396561, 'validation/accuracy': 0.6880509853363037, 'validation/loss': 1.4029872417449951, 'validation/bleu': 30.562362559763177, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.301902413368225, 'test/bleu': 30.582308832194634, 'test/num_examples': 3003, 'score': 35315.19370055199, 'total_duration': 59286.53086400032, 'accumulated_submission_time': 35315.19370055199, 'accumulated_eval_time': 23966.787534713745, 'accumulated_logging_time': 1.394505500793457}
I0210 18:31:03.669970 140277044971264 logging_writer.py:48] [98517] accumulated_eval_time=23966.787535, accumulated_logging_time=1.394506, accumulated_submission_time=35315.193701, global_step=98517, preemption_count=0, score=35315.193701, test/accuracy=0.705816, test/bleu=30.582309, test/loss=1.301902, test/num_examples=3003, total_duration=59286.530864, train/accuracy=0.674953, train/bleu=33.762683, train/loss=1.492271, validation/accuracy=0.688051, validation/bleu=30.562363, validation/loss=1.402987, validation/num_examples=3000
I0210 18:31:33.655480 140277053363968 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.2353539615869522, loss=1.5431846380233765
I0210 18:32:09.429811 140277044971264 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.22741751372814178, loss=1.5185534954071045
I0210 18:32:45.231841 140277053363968 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.22026117146015167, loss=1.6407074928283691
I0210 18:33:21.067729 140277044971264 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.22222135961055756, loss=1.6059221029281616
I0210 18:33:56.897002 140277053363968 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2089255005121231, loss=1.5543859004974365
I0210 18:34:32.751657 140277044971264 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.2010219693183899, loss=1.5339150428771973
I0210 18:35:08.598538 140277053363968 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.2319038361310959, loss=1.5175734758377075
I0210 18:35:44.422558 140277044971264 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.22241519391536713, loss=1.5415500402450562
I0210 18:36:20.330820 140277053363968 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.2177862524986267, loss=1.5160764455795288
I0210 18:36:56.179678 140277044971264 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.22060570120811462, loss=1.535334825515747
I0210 18:37:32.004194 140277053363968 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.22133506834506989, loss=1.5459480285644531
I0210 18:38:07.831989 140277044971264 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.21167534589767456, loss=1.474528193473816
I0210 18:38:43.647929 140277053363968 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.2207697182893753, loss=1.5625351667404175
I0210 18:39:19.505884 140277044971264 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.2301228940486908, loss=1.4871219396591187
I0210 18:39:55.345896 140277053363968 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.21911215782165527, loss=1.493300199508667
I0210 18:40:31.213468 140277044971264 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.21726244688034058, loss=1.596614122390747
I0210 18:41:07.089895 140277053363968 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.22226737439632416, loss=1.5365570783615112
I0210 18:41:42.938952 140277044971264 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.20507633686065674, loss=1.5036667585372925
I0210 18:42:18.769077 140277053363968 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.2235124409198761, loss=1.562213659286499
I0210 18:42:54.631491 140277044971264 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.2229335904121399, loss=1.5464266538619995
I0210 18:43:30.477863 140277053363968 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21569959819316864, loss=1.5205954313278198
I0210 18:44:06.311430 140277044971264 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.23204410076141357, loss=1.5126467943191528
I0210 18:44:42.123821 140277053363968 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.23152399063110352, loss=1.5166904926300049
I0210 18:45:03.718323 140446903760704 spec.py:321] Evaluating on the training split.
I0210 18:45:06.718111 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:48:27.962535 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 18:48:30.682306 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:51:20.080966 140446903760704 spec.py:349] Evaluating on the test split.
I0210 18:51:22.793898 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 18:54:05.827289 140446903760704 submission_runner.py:408] Time since start: 60668.72s, 	Step: 100862, 	{'train/accuracy': 0.6901666522026062, 'train/loss': 1.3958227634429932, 'train/bleu': 34.54494358756864, 'validation/accuracy': 0.6898488402366638, 'validation/loss': 1.3990224599838257, 'validation/bleu': 30.523842063106216, 'validation/num_examples': 3000, 'test/accuracy': 0.7054442167282104, 'test/loss': 1.299425721168518, 'test/bleu': 30.249200470461076, 'test/num_examples': 3003, 'score': 36155.15829825401, 'total_duration': 60668.71771264076, 'accumulated_submission_time': 36155.15829825401, 'accumulated_eval_time': 24508.896410226822, 'accumulated_logging_time': 1.4340672492980957}
I0210 18:54:05.864146 140277044971264 logging_writer.py:48] [100862] accumulated_eval_time=24508.896410, accumulated_logging_time=1.434067, accumulated_submission_time=36155.158298, global_step=100862, preemption_count=0, score=36155.158298, test/accuracy=0.705444, test/bleu=30.249200, test/loss=1.299426, test/num_examples=3003, total_duration=60668.717713, train/accuracy=0.690167, train/bleu=34.544944, train/loss=1.395823, validation/accuracy=0.689849, validation/bleu=30.523842, validation/loss=1.399022, validation/num_examples=3000
I0210 18:54:19.791899 140277053363968 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.2166571021080017, loss=1.5568159818649292
I0210 18:54:55.532027 140277044971264 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.22467026114463806, loss=1.5714325904846191
I0210 18:55:31.363603 140277053363968 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.2104959785938263, loss=1.5454641580581665
I0210 18:56:07.221204 140277044971264 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.2322152554988861, loss=1.5592650175094604
I0210 18:56:43.058101 140277053363968 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.2292868196964264, loss=1.6017217636108398
I0210 18:57:18.899431 140277044971264 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.2206381857395172, loss=1.5647175312042236
I0210 18:57:54.757584 140277053363968 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.21545004844665527, loss=1.552558183670044
I0210 18:58:30.600857 140277044971264 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.2323891818523407, loss=1.528617262840271
I0210 18:59:06.443689 140277053363968 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.22116857767105103, loss=1.522135615348816
I0210 18:59:42.263069 140277044971264 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.22071538865566254, loss=1.5630460977554321
I0210 19:00:18.099467 140277053363968 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.20907829701900482, loss=1.6159130334854126
I0210 19:00:53.936214 140277044971264 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.2141418755054474, loss=1.5614968538284302
I0210 19:01:29.751046 140277053363968 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.22260715067386627, loss=1.5305887460708618
I0210 19:02:05.601422 140277044971264 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.2266174554824829, loss=1.579039454460144
I0210 19:02:41.443174 140277053363968 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.2324213981628418, loss=1.4973654747009277
I0210 19:03:17.306318 140277044971264 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.22045208513736725, loss=1.5395690202713013
I0210 19:03:53.140195 140277053363968 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.22551441192626953, loss=1.5276378393173218
I0210 19:04:28.996917 140277044971264 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.23074346780776978, loss=1.5300933122634888
I0210 19:05:04.837875 140277053363968 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.2296387106180191, loss=1.6488988399505615
I0210 19:05:40.666841 140277044971264 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.22928361594676971, loss=1.5684360265731812
I0210 19:06:16.477760 140277053363968 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.2282448709011078, loss=1.5529323816299438
I0210 19:06:52.334295 140277044971264 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2215011864900589, loss=1.5405793190002441
I0210 19:07:28.165536 140277053363968 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.22912535071372986, loss=1.517823338508606
I0210 19:08:03.957112 140277044971264 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.22030103206634521, loss=1.6036136150360107
I0210 19:08:05.828960 140446903760704 spec.py:321] Evaluating on the training split.
I0210 19:08:08.834113 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 19:12:07.582165 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 19:12:10.284369 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 19:14:49.206897 140446903760704 spec.py:349] Evaluating on the test split.
I0210 19:14:51.927588 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 19:17:34.069490 140446903760704 submission_runner.py:408] Time since start: 62076.96s, 	Step: 103207, 	{'train/accuracy': 0.6826054453849792, 'train/loss': 1.445312738418579, 'train/bleu': 34.78132076309567, 'validation/accuracy': 0.6900224089622498, 'validation/loss': 1.391385555267334, 'validation/bleu': 30.546865210499295, 'validation/num_examples': 3000, 'test/accuracy': 0.708686351776123, 'test/loss': 1.2871750593185425, 'test/bleu': 30.83900984318424, 'test/num_examples': 3003, 'score': 36995.03522968292, 'total_duration': 62076.9599506855, 'accumulated_submission_time': 36995.03522968292, 'accumulated_eval_time': 25077.136883974075, 'accumulated_logging_time': 1.4827287197113037}
I0210 19:17:34.099379 140277053363968 logging_writer.py:48] [103207] accumulated_eval_time=25077.136884, accumulated_logging_time=1.482729, accumulated_submission_time=36995.035230, global_step=103207, preemption_count=0, score=36995.035230, test/accuracy=0.708686, test/bleu=30.839010, test/loss=1.287175, test/num_examples=3003, total_duration=62076.959951, train/accuracy=0.682605, train/bleu=34.781321, train/loss=1.445313, validation/accuracy=0.690022, validation/bleu=30.546865, validation/loss=1.391386, validation/num_examples=3000
I0210 19:18:07.675487 140277044971264 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.22596149146556854, loss=1.5938771963119507
I0210 19:18:43.487412 140277053363968 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.21909655630588531, loss=1.5662177801132202
I0210 19:19:19.342249 140277044971264 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.2161521017551422, loss=1.502472996711731
I0210 19:19:55.353180 140277053363968 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.21368518471717834, loss=1.5157972574234009
I0210 19:20:31.147068 140277044971264 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.2329016476869583, loss=1.476442813873291
I0210 19:21:06.985474 140277053363968 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22545170783996582, loss=1.5842969417572021
I0210 19:21:42.839454 140277044971264 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.21457776427268982, loss=1.5358847379684448
I0210 19:22:18.688613 140277053363968 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.22077332437038422, loss=1.5594444274902344
I0210 19:22:54.541965 140277044971264 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.20787400007247925, loss=1.4601072072982788
I0210 19:23:30.396967 140277053363968 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.2197699397802353, loss=1.5374938249588013
I0210 19:24:06.222805 140277044971264 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.2124386578798294, loss=1.516477346420288
I0210 19:24:42.060749 140277053363968 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.22215783596038818, loss=1.5992214679718018
I0210 19:25:17.878437 140277044971264 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.21704509854316711, loss=1.5383756160736084
I0210 19:25:53.726622 140277053363968 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.2147669643163681, loss=1.4828370809555054
I0210 19:26:29.548985 140277044971264 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.2185332328081131, loss=1.5166265964508057
I0210 19:27:05.363459 140277053363968 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.23480886220932007, loss=1.5148577690124512
I0210 19:27:41.171512 140277044971264 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.22044330835342407, loss=1.5115865468978882
I0210 19:28:17.029984 140277053363968 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.21720285713672638, loss=1.472243309020996
I0210 19:28:52.860224 140277044971264 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.2234954535961151, loss=1.5517070293426514
I0210 19:29:28.719686 140277053363968 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.2152179777622223, loss=1.5599271059036255
I0210 19:30:04.600940 140277044971264 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.2241332232952118, loss=1.5418336391448975
I0210 19:30:40.463058 140277053363968 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.23323799669742584, loss=1.4913774728775024
I0210 19:31:16.328411 140277044971264 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.2432534545660019, loss=1.526925802230835
I0210 19:31:34.322185 140446903760704 spec.py:321] Evaluating on the training split.
I0210 19:31:37.364303 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 19:35:45.472568 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 19:35:48.187291 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 19:38:47.406858 140446903760704 spec.py:349] Evaluating on the test split.
I0210 19:38:50.122317 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 19:41:11.499152 140446903760704 submission_runner.py:408] Time since start: 63494.39s, 	Step: 105552, 	{'train/accuracy': 0.6831424832344055, 'train/loss': 1.4466559886932373, 'train/bleu': 34.30571893606282, 'validation/accuracy': 0.6902456283569336, 'validation/loss': 1.3884084224700928, 'validation/bleu': 30.71525097595463, 'validation/num_examples': 3000, 'test/accuracy': 0.706187903881073, 'test/loss': 1.2858394384384155, 'test/bleu': 30.671070000288662, 'test/num_examples': 3003, 'score': 37835.16900038719, 'total_duration': 63494.38958978653, 'accumulated_submission_time': 37835.16900038719, 'accumulated_eval_time': 25654.313775777817, 'accumulated_logging_time': 1.522960901260376}
I0210 19:41:11.529360 140277053363968 logging_writer.py:48] [105552] accumulated_eval_time=25654.313776, accumulated_logging_time=1.522961, accumulated_submission_time=37835.169000, global_step=105552, preemption_count=0, score=37835.169000, test/accuracy=0.706188, test/bleu=30.671070, test/loss=1.285839, test/num_examples=3003, total_duration=63494.389590, train/accuracy=0.683142, train/bleu=34.305719, train/loss=1.446656, validation/accuracy=0.690246, validation/bleu=30.715251, validation/loss=1.388408, validation/num_examples=3000
I0210 19:41:28.997930 140277044971264 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.21531113982200623, loss=1.5046086311340332
I0210 19:42:04.724972 140277053363968 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.2116938680410385, loss=1.4613429307937622
I0210 19:42:40.531788 140277044971264 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.22405466437339783, loss=1.5283944606781006
I0210 19:43:16.346745 140277053363968 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.22332783043384552, loss=1.5903676748275757
I0210 19:43:52.147285 140277044971264 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.22658665478229523, loss=1.547865629196167
I0210 19:44:27.984970 140277053363968 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.230091392993927, loss=1.4974744319915771
I0210 19:45:03.824901 140277044971264 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.2278091013431549, loss=1.5113831758499146
I0210 19:45:39.642765 140277053363968 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.2323254942893982, loss=1.5421580076217651
I0210 19:46:15.448893 140277044971264 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.21865147352218628, loss=1.5018919706344604
I0210 19:46:51.253969 140277053363968 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.21960386633872986, loss=1.515538215637207
I0210 19:47:27.096763 140277044971264 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22020478546619415, loss=1.4551787376403809
I0210 19:48:02.891260 140277053363968 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.21728450059890747, loss=1.49008309841156
I0210 19:48:38.715820 140277044971264 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.21288542449474335, loss=1.5139062404632568
I0210 19:49:14.552476 140277053363968 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.22423678636550903, loss=1.4742929935455322
I0210 19:49:50.392152 140277044971264 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.23526360094547272, loss=1.6246007680892944
I0210 19:50:26.229007 140277053363968 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.22916372120380402, loss=1.5088233947753906
I0210 19:51:02.027963 140277044971264 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.22721222043037415, loss=1.5220305919647217
I0210 19:51:37.843372 140277053363968 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.24389073252677917, loss=1.559224247932434
I0210 19:52:13.675288 140277044971264 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.24398696422576904, loss=1.5679904222488403
I0210 19:52:49.488696 140277053363968 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.22746680676937103, loss=1.4278054237365723
I0210 19:53:25.308006 140277044971264 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.24285000562667847, loss=1.5027484893798828
I0210 19:54:01.166922 140277053363968 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.22992098331451416, loss=1.4478806257247925
I0210 19:54:37.001132 140277044971264 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.21795371174812317, loss=1.5363236665725708
I0210 19:55:11.792429 140446903760704 spec.py:321] Evaluating on the training split.
I0210 19:55:14.802557 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 19:59:26.273594 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 19:59:28.996099 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:02:06.760795 140446903760704 spec.py:349] Evaluating on the test split.
I0210 20:02:09.477115 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:05:08.345151 140446903760704 submission_runner.py:408] Time since start: 64931.24s, 	Step: 107899, 	{'train/accuracy': 0.6916837692260742, 'train/loss': 1.393021583557129, 'train/bleu': 35.40145042693628, 'validation/accuracy': 0.691299557685852, 'validation/loss': 1.383009910583496, 'validation/bleu': 30.558788910901555, 'validation/num_examples': 3000, 'test/accuracy': 0.7079077363014221, 'test/loss': 1.2825238704681396, 'test/bleu': 30.42644247752539, 'test/num_examples': 3003, 'score': 38675.34786558151, 'total_duration': 64931.23558783531, 'accumulated_submission_time': 38675.34786558151, 'accumulated_eval_time': 26250.866422891617, 'accumulated_logging_time': 1.5631628036499023}
I0210 20:05:08.374624 140277053363968 logging_writer.py:48] [107899] accumulated_eval_time=26250.866423, accumulated_logging_time=1.563163, accumulated_submission_time=38675.347866, global_step=107899, preemption_count=0, score=38675.347866, test/accuracy=0.707908, test/bleu=30.426442, test/loss=1.282524, test/num_examples=3003, total_duration=64931.235588, train/accuracy=0.691684, train/bleu=35.401450, train/loss=1.393022, validation/accuracy=0.691300, validation/bleu=30.558789, validation/loss=1.383010, validation/num_examples=3000
I0210 20:05:09.116379 140277044971264 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.20996129512786865, loss=1.451849102973938
I0210 20:05:44.813811 140277053363968 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.23288923501968384, loss=1.516542673110962
I0210 20:06:20.563534 140277044971264 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.21717165410518646, loss=1.494807481765747
I0210 20:06:56.379279 140277053363968 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.2409965842962265, loss=1.5297974348068237
I0210 20:07:32.249896 140277044971264 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.2195422202348709, loss=1.5355756282806396
I0210 20:08:08.120566 140277053363968 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.21921603381633759, loss=1.5124702453613281
I0210 20:08:43.928882 140277044971264 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.2217951864004135, loss=1.4347044229507446
I0210 20:09:19.739572 140277053363968 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.22625230252742767, loss=1.5152137279510498
I0210 20:09:55.564377 140277044971264 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.22067208588123322, loss=1.4707940816879272
I0210 20:10:31.412664 140277053363968 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.22114750742912292, loss=1.4222095012664795
I0210 20:11:07.234600 140277044971264 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.22466909885406494, loss=1.4853442907333374
I0210 20:11:43.058439 140277053363968 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.22330597043037415, loss=1.494632601737976
I0210 20:12:18.872055 140277044971264 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.21940770745277405, loss=1.4748417139053345
I0210 20:12:54.693399 140277053363968 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.22713305056095123, loss=1.4974161386489868
I0210 20:13:30.506426 140277044971264 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.23249079287052155, loss=1.480387806892395
I0210 20:14:06.368343 140277053363968 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.23051096498966217, loss=1.5433859825134277
I0210 20:14:42.188945 140277044971264 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.238645538687706, loss=1.560870885848999
I0210 20:15:18.029663 140277053363968 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.21823714673519135, loss=1.4403839111328125
I0210 20:15:53.829931 140277044971264 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.224979430437088, loss=1.5065795183181763
I0210 20:16:29.638476 140277053363968 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.22448118031024933, loss=1.494188904762268
I0210 20:17:05.446413 140277044971264 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.22612479329109192, loss=1.5125566720962524
I0210 20:17:41.280951 140277053363968 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.22157080471515656, loss=1.4952974319458008
I0210 20:18:17.117380 140277044971264 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.22525186836719513, loss=1.4805020093917847
I0210 20:18:52.915039 140277053363968 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.23891159892082214, loss=1.5312750339508057
I0210 20:19:08.405172 140446903760704 spec.py:321] Evaluating on the training split.
I0210 20:19:11.418263 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:23:14.679716 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 20:23:17.378372 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:26:15.293786 140446903760704 spec.py:349] Evaluating on the test split.
I0210 20:26:17.994953 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:29:29.927801 140446903760704 submission_runner.py:408] Time since start: 66392.82s, 	Step: 110245, 	{'train/accuracy': 0.6882879137992859, 'train/loss': 1.4097886085510254, 'train/bleu': 35.19981584636805, 'validation/accuracy': 0.6923038959503174, 'validation/loss': 1.3791635036468506, 'validation/bleu': 30.563474260944655, 'validation/num_examples': 3000, 'test/accuracy': 0.7103015780448914, 'test/loss': 1.2744451761245728, 'test/bleu': 30.49992268140961, 'test/num_examples': 3003, 'score': 39515.29098367691, 'total_duration': 66392.81823420525, 'accumulated_submission_time': 39515.29098367691, 'accumulated_eval_time': 26872.388967752457, 'accumulated_logging_time': 1.603606939315796}
I0210 20:29:29.965494 140277044971264 logging_writer.py:48] [110245] accumulated_eval_time=26872.388968, accumulated_logging_time=1.603607, accumulated_submission_time=39515.290984, global_step=110245, preemption_count=0, score=39515.290984, test/accuracy=0.710302, test/bleu=30.499923, test/loss=1.274445, test/num_examples=3003, total_duration=66392.818234, train/accuracy=0.688288, train/bleu=35.199816, train/loss=1.409789, validation/accuracy=0.692304, validation/bleu=30.563474, validation/loss=1.379164, validation/num_examples=3000
I0210 20:29:49.961865 140277053363968 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.22750158607959747, loss=1.4197016954421997
I0210 20:30:25.714289 140277044971264 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.24315758049488068, loss=1.4892719984054565
I0210 20:31:01.521132 140277053363968 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.22780703008174896, loss=1.5829346179962158
I0210 20:31:37.363351 140277044971264 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.2235332876443863, loss=1.4704118967056274
I0210 20:32:13.211760 140277053363968 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.24034763872623444, loss=1.5512398481369019
I0210 20:32:49.050177 140277044971264 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.2429470717906952, loss=1.5051416158676147
I0210 20:33:24.862901 140277053363968 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.22409410774707794, loss=1.531583547592163
I0210 20:34:00.709668 140277044971264 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.23275911808013916, loss=1.5615044832229614
I0210 20:34:36.526595 140277053363968 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.22180145978927612, loss=1.4402695894241333
I0210 20:35:12.337293 140277044971264 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.22499696910381317, loss=1.4024699926376343
I0210 20:35:48.172124 140277053363968 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.22525736689567566, loss=1.527528166770935
I0210 20:36:23.995207 140277044971264 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.24090638756752014, loss=1.4738945960998535
I0210 20:36:59.821699 140277053363968 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.24252668023109436, loss=1.5239170789718628
I0210 20:37:35.651437 140277044971264 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.23537111282348633, loss=1.4678666591644287
I0210 20:38:11.470179 140277053363968 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.22792914509773254, loss=1.4370628595352173
I0210 20:38:47.285877 140277044971264 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.22150813043117523, loss=1.504367709159851
I0210 20:39:23.108288 140277053363968 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.23855164647102356, loss=1.4536703824996948
I0210 20:39:58.937050 140277044971264 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.22830264270305634, loss=1.458751916885376
I0210 20:40:34.776195 140277053363968 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.24167896807193756, loss=1.4908561706542969
I0210 20:41:10.613230 140277044971264 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.2270476222038269, loss=1.4652245044708252
I0210 20:41:46.446863 140277053363968 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.22345291078090668, loss=1.517021656036377
I0210 20:42:22.278337 140277044971264 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.2255290299654007, loss=1.477327823638916
I0210 20:42:58.148611 140277053363968 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.24925720691680908, loss=1.5580594539642334
I0210 20:43:30.109026 140446903760704 spec.py:321] Evaluating on the training split.
I0210 20:43:33.119049 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:47:37.625403 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 20:47:40.338772 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:50:36.764412 140446903760704 spec.py:349] Evaluating on the test split.
I0210 20:50:39.459171 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 20:53:32.675640 140446903760704 submission_runner.py:408] Time since start: 67835.57s, 	Step: 112591, 	{'train/accuracy': 0.6941087245941162, 'train/loss': 1.3747334480285645, 'train/bleu': 35.41065961993999, 'validation/accuracy': 0.6923410892486572, 'validation/loss': 1.3770735263824463, 'validation/bleu': 30.704859704017625, 'validation/num_examples': 3000, 'test/accuracy': 0.7099180817604065, 'test/loss': 1.2725780010223389, 'test/bleu': 30.61117920436823, 'test/num_examples': 3003, 'score': 40355.34690570831, 'total_duration': 67835.56606578827, 'accumulated_submission_time': 40355.34690570831, 'accumulated_eval_time': 27474.955493688583, 'accumulated_logging_time': 1.654353380203247}
I0210 20:53:32.711956 140277044971264 logging_writer.py:48] [112591] accumulated_eval_time=27474.955494, accumulated_logging_time=1.654353, accumulated_submission_time=40355.346906, global_step=112591, preemption_count=0, score=40355.346906, test/accuracy=0.709918, test/bleu=30.611179, test/loss=1.272578, test/num_examples=3003, total_duration=67835.566066, train/accuracy=0.694109, train/bleu=35.410660, train/loss=1.374733, validation/accuracy=0.692341, validation/bleu=30.704860, validation/loss=1.377074, validation/num_examples=3000
I0210 20:53:36.288149 140277053363968 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.23456676304340363, loss=1.5472214221954346
I0210 20:54:12.045499 140277044971264 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.21714599430561066, loss=1.4802703857421875
I0210 20:54:47.847483 140277053363968 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.22089187800884247, loss=1.4230320453643799
I0210 20:55:23.670921 140277044971264 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.23109965026378632, loss=1.4883432388305664
I0210 20:55:59.480228 140277053363968 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.2242770493030548, loss=1.4598767757415771
I0210 20:56:35.303373 140277044971264 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.2539895474910736, loss=1.5202685594558716
I0210 20:57:11.160525 140277053363968 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.23098620772361755, loss=1.4523398876190186
I0210 20:57:47.034761 140277044971264 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.230768084526062, loss=1.5033265352249146
I0210 20:58:22.871727 140277053363968 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.2275930941104889, loss=1.4473451375961304
I0210 20:58:58.702741 140277044971264 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.23162797093391418, loss=1.5395290851593018
I0210 20:59:34.502990 140277053363968 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.22574837505817413, loss=1.48550546169281
I0210 21:00:10.343745 140277044971264 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.2371581494808197, loss=1.483416199684143
I0210 21:00:46.134325 140277053363968 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.24473673105239868, loss=1.5248197317123413
I0210 21:01:21.976696 140277044971264 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.22980034351348877, loss=1.5503178834915161
I0210 21:01:57.813675 140277053363968 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.23216187953948975, loss=1.4136911630630493
I0210 21:02:33.640863 140277044971264 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.23840101063251495, loss=1.5250706672668457
I0210 21:03:09.460323 140277053363968 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.2340291291475296, loss=1.4733954668045044
I0210 21:03:45.313971 140277044971264 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.2295851707458496, loss=1.512565016746521
I0210 21:04:21.158528 140277053363968 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.23690751194953918, loss=1.4591550827026367
I0210 21:04:56.975986 140277044971264 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.2155623733997345, loss=1.440780520439148
I0210 21:05:32.808182 140277053363968 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.23325252532958984, loss=1.4703620672225952
I0210 21:06:08.644201 140277044971264 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.24044904112815857, loss=1.4873062372207642
I0210 21:06:44.575529 140277053363968 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.2353435605764389, loss=1.4889947175979614
I0210 21:07:20.390798 140277044971264 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.22332723438739777, loss=1.433185338973999
I0210 21:07:33.010263 140446903760704 spec.py:321] Evaluating on the training split.
I0210 21:07:36.020887 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 21:11:59.072018 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 21:12:01.787400 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 21:15:51.134638 140446903760704 spec.py:349] Evaluating on the test split.
I0210 21:15:53.845710 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 21:19:57.104213 140446903760704 submission_runner.py:408] Time since start: 69419.99s, 	Step: 114937, 	{'train/accuracy': 0.6922072172164917, 'train/loss': 1.393733263015747, 'train/bleu': 35.2817522823305, 'validation/accuracy': 0.693717360496521, 'validation/loss': 1.3703649044036865, 'validation/bleu': 30.42187428646859, 'validation/num_examples': 3000, 'test/accuracy': 0.7107896208763123, 'test/loss': 1.2660441398620605, 'test/bleu': 30.542745113871938, 'test/num_examples': 3003, 'score': 41195.55699467659, 'total_duration': 69419.99466729164, 'accumulated_submission_time': 41195.55699467659, 'accumulated_eval_time': 28219.049389600754, 'accumulated_logging_time': 1.7018797397613525}
I0210 21:19:57.137322 140277053363968 logging_writer.py:48] [114937] accumulated_eval_time=28219.049390, accumulated_logging_time=1.701880, accumulated_submission_time=41195.556995, global_step=114937, preemption_count=0, score=41195.556995, test/accuracy=0.710790, test/bleu=30.542745, test/loss=1.266044, test/num_examples=3003, total_duration=69419.994667, train/accuracy=0.692207, train/bleu=35.281752, train/loss=1.393733, validation/accuracy=0.693717, validation/bleu=30.421874, validation/loss=1.370365, validation/num_examples=3000
I0210 21:20:19.967570 140277044971264 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.23491519689559937, loss=1.4536656141281128
I0210 21:20:55.713634 140277053363968 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.23302091658115387, loss=1.4425171613693237
I0210 21:21:31.633448 140277044971264 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.23878803849220276, loss=1.4311919212341309
I0210 21:22:07.451967 140277053363968 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.2437308430671692, loss=1.5503607988357544
I0210 21:22:43.242189 140277044971264 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.23444612324237823, loss=1.4962575435638428
I0210 21:23:19.068639 140277053363968 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.22851254045963287, loss=1.4861996173858643
I0210 21:23:54.917589 140277044971264 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.23396959900856018, loss=1.4997215270996094
I0210 21:24:30.748252 140277053363968 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.22179973125457764, loss=1.460321068763733
I0210 21:25:06.594696 140277044971264 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.2278091460466385, loss=1.4467551708221436
I0210 21:25:42.390256 140277053363968 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.23429572582244873, loss=1.5444891452789307
I0210 21:26:18.211800 140277044971264 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.23451374471187592, loss=1.400378942489624
I0210 21:26:54.040593 140277053363968 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.22649352252483368, loss=1.4287641048431396
I0210 21:27:29.846046 140277044971264 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.22683805227279663, loss=1.4954522848129272
I0210 21:28:05.688556 140277053363968 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.2267962396144867, loss=1.4659340381622314
I0210 21:28:41.533397 140277044971264 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.22404000163078308, loss=1.4088258743286133
I0210 21:29:17.390791 140277053363968 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.23226496577262878, loss=1.4825892448425293
I0210 21:29:53.194633 140277044971264 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.23239289224147797, loss=1.3836171627044678
I0210 21:30:29.017399 140277053363968 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.2370452880859375, loss=1.4781817197799683
I0210 21:31:04.837551 140277044971264 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.22914531826972961, loss=1.5136809349060059
I0210 21:31:40.648893 140277053363968 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.2360856980085373, loss=1.5181405544281006
I0210 21:32:16.463336 140277044971264 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.2332286387681961, loss=1.5302599668502808
I0210 21:32:52.278687 140277053363968 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.23519298434257507, loss=1.5058399438858032
I0210 21:33:28.173209 140277044971264 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.2320854812860489, loss=1.4993879795074463
I0210 21:33:57.426110 140446903760704 spec.py:321] Evaluating on the training split.
I0210 21:34:00.452355 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 21:37:56.914237 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 21:37:59.623789 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 21:40:58.042808 140446903760704 spec.py:349] Evaluating on the test split.
I0210 21:41:00.752109 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 21:43:48.904704 140446903760704 submission_runner.py:408] Time since start: 70851.80s, 	Step: 117283, 	{'train/accuracy': 0.690304696559906, 'train/loss': 1.3954484462738037, 'train/bleu': 35.20293102881795, 'validation/accuracy': 0.69430011510849, 'validation/loss': 1.3707165718078613, 'validation/bleu': 30.820244597158045, 'validation/num_examples': 3000, 'test/accuracy': 0.7114868760108948, 'test/loss': 1.2633055448532104, 'test/bleu': 30.761119658345095, 'test/num_examples': 3003, 'score': 42035.75880694389, 'total_duration': 70851.79516196251, 'accumulated_submission_time': 42035.75880694389, 'accumulated_eval_time': 28810.52793598175, 'accumulated_logging_time': 1.7455673217773438}
I0210 21:43:48.937558 140277053363968 logging_writer.py:48] [117283] accumulated_eval_time=28810.527936, accumulated_logging_time=1.745567, accumulated_submission_time=42035.758807, global_step=117283, preemption_count=0, score=42035.758807, test/accuracy=0.711487, test/bleu=30.761120, test/loss=1.263306, test/num_examples=3003, total_duration=70851.795162, train/accuracy=0.690305, train/bleu=35.202931, train/loss=1.395448, validation/accuracy=0.694300, validation/bleu=30.820245, validation/loss=1.370717, validation/num_examples=3000
I0210 21:43:55.379275 140277044971264 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.2328435629606247, loss=1.4269193410873413
I0210 21:44:31.067247 140277053363968 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.2483927458524704, loss=1.5543471574783325
I0210 21:45:06.835825 140277044971264 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.22775386273860931, loss=1.4099961519241333
I0210 21:45:42.656410 140277053363968 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.22689200937747955, loss=1.430196762084961
I0210 21:46:18.503883 140277044971264 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.23418577015399933, loss=1.5043376684188843
I0210 21:46:54.339323 140277053363968 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.23488019406795502, loss=1.47938072681427
I0210 21:47:30.172590 140277044971264 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.23914562165737152, loss=1.4604558944702148
I0210 21:48:06.037018 140277053363968 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.23429934680461884, loss=1.4033751487731934
I0210 21:48:41.874874 140277044971264 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.23694764077663422, loss=1.54456627368927
I0210 21:49:17.684634 140277053363968 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.22942285239696503, loss=1.4777779579162598
I0210 21:49:53.497160 140277044971264 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.22641953825950623, loss=1.4908283948898315
I0210 21:50:29.356820 140277053363968 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.22131317853927612, loss=1.4770448207855225
I0210 21:51:05.171046 140277044971264 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.240871861577034, loss=1.4854003190994263
I0210 21:51:41.004015 140277053363968 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.23201397061347961, loss=1.4483745098114014
I0210 21:52:16.849934 140277044971264 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.23878712952136993, loss=1.54037606716156
I0210 21:52:52.680024 140277053363968 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.24465177953243256, loss=1.4580048322677612
I0210 21:53:28.531341 140277044971264 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.23220491409301758, loss=1.4452413320541382
I0210 21:54:04.338639 140277053363968 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.24153903126716614, loss=1.5311925411224365
I0210 21:54:40.166135 140277044971264 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.2430342733860016, loss=1.5223361253738403
I0210 21:55:16.056927 140277053363968 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.22830010950565338, loss=1.5012328624725342
I0210 21:55:51.879666 140277044971264 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.22242110967636108, loss=1.4584928750991821
I0210 21:56:27.715029 140277053363968 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.22949185967445374, loss=1.4313855171203613
I0210 21:57:03.550082 140277044971264 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23995894193649292, loss=1.4129362106323242
I0210 21:57:39.402033 140277053363968 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.22924749553203583, loss=1.4412992000579834
I0210 21:57:49.186777 140446903760704 spec.py:321] Evaluating on the training split.
I0210 21:57:52.204293 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:02:11.216675 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 22:02:13.930921 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:05:37.048084 140446903760704 spec.py:349] Evaluating on the test split.
I0210 22:05:39.767801 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:09:31.505890 140446903760704 submission_runner.py:408] Time since start: 72394.40s, 	Step: 119629, 	{'train/accuracy': 0.6958056688308716, 'train/loss': 1.3685253858566284, 'train/bleu': 35.7102083223731, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.36842942237854, 'validation/bleu': 30.65897634626175, 'validation/num_examples': 3000, 'test/accuracy': 0.7119168043136597, 'test/loss': 1.2605042457580566, 'test/bleu': 30.859965177480586, 'test/num_examples': 3003, 'score': 42875.923796892166, 'total_duration': 72394.39634847641, 'accumulated_submission_time': 42875.923796892166, 'accumulated_eval_time': 29512.847000598907, 'accumulated_logging_time': 1.788480281829834}
I0210 22:09:31.538291 140277044971264 logging_writer.py:48] [119629] accumulated_eval_time=29512.847001, accumulated_logging_time=1.788480, accumulated_submission_time=42875.923797, global_step=119629, preemption_count=0, score=42875.923797, test/accuracy=0.711917, test/bleu=30.859965, test/loss=1.260504, test/num_examples=3003, total_duration=72394.396348, train/accuracy=0.695806, train/bleu=35.710208, train/loss=1.368525, validation/accuracy=0.693618, validation/bleu=30.658976, validation/loss=1.368429, validation/num_examples=3000
I0210 22:09:57.221780 140277053363968 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.2294473946094513, loss=1.45307195186615
I0210 22:10:32.971546 140277044971264 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.2437230348587036, loss=1.4194053411483765
I0210 22:11:08.810894 140277053363968 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.23687909543514252, loss=1.480751395225525
I0210 22:11:44.611163 140277044971264 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.2387222796678543, loss=1.4575673341751099
I0210 22:12:20.442669 140277053363968 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.24073076248168945, loss=1.4913524389266968
I0210 22:12:56.297066 140277044971264 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.2421993613243103, loss=1.4807215929031372
I0210 22:13:32.115550 140277053363968 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.2366994321346283, loss=1.4918352365493774
I0210 22:14:07.932321 140277044971264 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.23672975599765778, loss=1.410567045211792
I0210 22:14:43.763841 140277053363968 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.2537117600440979, loss=1.510606288909912
I0210 22:15:19.615523 140277044971264 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.23698313534259796, loss=1.5157371759414673
I0210 22:15:55.452059 140277053363968 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.23786839842796326, loss=1.4193364381790161
I0210 22:16:31.309612 140277044971264 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.23992840945720673, loss=1.4596083164215088
I0210 22:17:07.233180 140277053363968 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.2234639823436737, loss=1.461514949798584
I0210 22:17:43.125348 140277044971264 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.23691906034946442, loss=1.477070927619934
I0210 22:18:19.027400 140277053363968 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.2364608645439148, loss=1.536096453666687
I0210 22:18:54.846479 140277044971264 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.23276811838150024, loss=1.5012001991271973
I0210 22:19:30.703764 140277053363968 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.23551712930202484, loss=1.489007830619812
I0210 22:20:06.545639 140277044971264 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.2283426970243454, loss=1.4418922662734985
I0210 22:20:42.414271 140277053363968 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.23924924433231354, loss=1.4417378902435303
I0210 22:21:18.302934 140277044971264 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.2425193190574646, loss=1.459275722503662
I0210 22:21:54.186457 140277053363968 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.22903811931610107, loss=1.455705165863037
I0210 22:22:30.033013 140277044971264 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23515519499778748, loss=1.4909412860870361
I0210 22:23:05.863180 140277053363968 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.23362450301647186, loss=1.5196058750152588
I0210 22:23:31.733718 140446903760704 spec.py:321] Evaluating on the training split.
I0210 22:23:34.736563 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:27:13.174134 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 22:27:15.873912 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:30:09.022840 140446903760704 spec.py:349] Evaluating on the test split.
I0210 22:30:11.730113 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:33:03.051939 140446903760704 submission_runner.py:408] Time since start: 73805.94s, 	Step: 121974, 	{'train/accuracy': 0.6915479302406311, 'train/loss': 1.388578176498413, 'train/bleu': 35.36310249116494, 'validation/accuracy': 0.6942381262779236, 'validation/loss': 1.3653781414031982, 'validation/bleu': 30.687726654480677, 'validation/num_examples': 3000, 'test/accuracy': 0.7125210762023926, 'test/loss': 1.2577069997787476, 'test/bleu': 30.773637579945547, 'test/num_examples': 3003, 'score': 43716.03023672104, 'total_duration': 73805.94238114357, 'accumulated_submission_time': 43716.03023672104, 'accumulated_eval_time': 30084.165147781372, 'accumulated_logging_time': 1.8323473930358887}
I0210 22:33:03.085275 140277044971264 logging_writer.py:48] [121974] accumulated_eval_time=30084.165148, accumulated_logging_time=1.832347, accumulated_submission_time=43716.030237, global_step=121974, preemption_count=0, score=43716.030237, test/accuracy=0.712521, test/bleu=30.773638, test/loss=1.257707, test/num_examples=3003, total_duration=73805.942381, train/accuracy=0.691548, train/bleu=35.363102, train/loss=1.388578, validation/accuracy=0.694238, validation/bleu=30.687727, validation/loss=1.365378, validation/num_examples=3000
I0210 22:33:12.715172 140277053363968 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.2440243512392044, loss=1.447885513305664
I0210 22:33:48.431343 140277044971264 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.22660361230373383, loss=1.3921736478805542
I0210 22:34:24.233679 140277053363968 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.23766301572322845, loss=1.4913889169692993
I0210 22:35:00.049567 140277044971264 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.24108678102493286, loss=1.4143661260604858
I0210 22:35:35.872969 140277053363968 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.24541205167770386, loss=1.4910551309585571
I0210 22:36:11.727353 140277044971264 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.23742710053920746, loss=1.4396708011627197
I0210 22:36:47.720386 140277053363968 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.22953110933303833, loss=1.4890758991241455
I0210 22:37:23.587185 140277044971264 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.22208145260810852, loss=1.4021947383880615
I0210 22:37:59.405479 140277053363968 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.231512188911438, loss=1.5051887035369873
I0210 22:38:35.249305 140277044971264 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.24962352216243744, loss=1.4736578464508057
I0210 22:39:11.081288 140277053363968 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.2521432638168335, loss=1.5029778480529785
I0210 22:39:46.893183 140277044971264 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.2311946600675583, loss=1.4134061336517334
I0210 22:40:22.763412 140277053363968 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.24402782320976257, loss=1.5191434621810913
I0210 22:40:58.607810 140277044971264 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.23369161784648895, loss=1.4317649602890015
I0210 22:41:34.478934 140277053363968 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.235538512468338, loss=1.4651466608047485
I0210 22:42:10.347985 140277044971264 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.24540290236473083, loss=1.4885510206222534
I0210 22:42:46.239196 140277053363968 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.2384505718946457, loss=1.484959363937378
I0210 22:43:22.241724 140277044971264 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.24027277529239655, loss=1.5052051544189453
I0210 22:43:58.117394 140277053363968 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.23252038657665253, loss=1.40604567527771
I0210 22:44:33.961557 140277044971264 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.24130283296108246, loss=1.4614906311035156
I0210 22:45:09.784029 140277053363968 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.23167793452739716, loss=1.463905692100525
I0210 22:45:45.649101 140277044971264 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.232398122549057, loss=1.4798450469970703
I0210 22:46:21.528356 140277053363968 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.222721129655838, loss=1.4046701192855835
I0210 22:46:57.381584 140277044971264 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.23209674656391144, loss=1.497468113899231
I0210 22:47:03.208831 140446903760704 spec.py:321] Evaluating on the training split.
I0210 22:47:06.223758 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:51:02.342082 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 22:51:05.043628 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:54:11.066738 140446903760704 spec.py:349] Evaluating on the test split.
I0210 22:54:13.791461 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 22:57:20.096989 140446903760704 submission_runner.py:408] Time since start: 75262.99s, 	Step: 124318, 	{'train/accuracy': 0.6958664655685425, 'train/loss': 1.3725371360778809, 'train/bleu': 35.70916601186907, 'validation/accuracy': 0.6946473121643066, 'validation/loss': 1.364732027053833, 'validation/bleu': 30.91054033191716, 'validation/num_examples': 3000, 'test/accuracy': 0.7131369709968567, 'test/loss': 1.2557777166366577, 'test/bleu': 30.845955240528713, 'test/num_examples': 3003, 'score': 44556.06577014923, 'total_duration': 75262.98744797707, 'accumulated_submission_time': 44556.06577014923, 'accumulated_eval_time': 30701.053248643875, 'accumulated_logging_time': 1.8761370182037354}
I0210 22:57:20.133187 140277053363968 logging_writer.py:48] [124318] accumulated_eval_time=30701.053249, accumulated_logging_time=1.876137, accumulated_submission_time=44556.065770, global_step=124318, preemption_count=0, score=44556.065770, test/accuracy=0.713137, test/bleu=30.845955, test/loss=1.255778, test/num_examples=3003, total_duration=75262.987448, train/accuracy=0.695866, train/bleu=35.709166, train/loss=1.372537, validation/accuracy=0.694647, validation/bleu=30.910540, validation/loss=1.364732, validation/num_examples=3000
I0210 22:57:49.730818 140277044971264 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.23403926193714142, loss=1.4606553316116333
I0210 22:58:25.467729 140277053363968 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.22235827147960663, loss=1.4585434198379517
I0210 22:59:01.260303 140277044971264 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.240403413772583, loss=1.4192527532577515
I0210 22:59:37.214660 140277053363968 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.2379152476787567, loss=1.412970781326294
I0210 23:00:13.093993 140277044971264 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.24043691158294678, loss=1.5077190399169922
I0210 23:00:48.921270 140277053363968 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.23341472446918488, loss=1.4473294019699097
I0210 23:01:24.760899 140277044971264 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.2357909083366394, loss=1.4501146078109741
I0210 23:02:00.605675 140277053363968 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.2394241839647293, loss=1.5217500925064087
I0210 23:02:36.438237 140277044971264 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.23315750062465668, loss=1.430155634880066
I0210 23:03:12.286325 140277053363968 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.23973914980888367, loss=1.449724555015564
I0210 23:03:48.128736 140277044971264 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.23980289697647095, loss=1.5171160697937012
I0210 23:04:23.940839 140277053363968 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.23332954943180084, loss=1.4922832250595093
I0210 23:04:59.783997 140277044971264 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.22865797579288483, loss=1.4512934684753418
I0210 23:05:35.583303 140277053363968 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.23508065938949585, loss=1.4660505056381226
I0210 23:06:11.489908 140277044971264 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.24730262160301208, loss=1.4659497737884521
I0210 23:06:47.323414 140277053363968 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.23264136910438538, loss=1.3490759134292603
I0210 23:07:23.159247 140277044971264 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.24251800775527954, loss=1.494343638420105
I0210 23:07:59.002575 140277053363968 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.22347550094127655, loss=1.4812620878219604
I0210 23:08:34.817405 140277044971264 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.23583336174488068, loss=1.3957599401474
I0210 23:09:10.608175 140277053363968 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.24912585318088531, loss=1.5136313438415527
I0210 23:09:46.480597 140277044971264 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.23338720202445984, loss=1.4758820533752441
I0210 23:10:22.311613 140277053363968 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.23112905025482178, loss=1.3878393173217773
I0210 23:10:58.183643 140277044971264 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.2443208396434784, loss=1.481427550315857
I0210 23:11:20.160879 140446903760704 spec.py:321] Evaluating on the training split.
I0210 23:11:23.192480 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 23:15:01.494177 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 23:15:04.207733 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 23:18:05.961823 140446903760704 spec.py:349] Evaluating on the test split.
I0210 23:18:08.679835 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 23:21:39.909513 140446903760704 submission_runner.py:408] Time since start: 76722.80s, 	Step: 126663, 	{'train/accuracy': 0.6963878273963928, 'train/loss': 1.3677339553833008, 'train/bleu': 35.3685362955417, 'validation/accuracy': 0.6950316429138184, 'validation/loss': 1.3643397092819214, 'validation/bleu': 30.77219468305123, 'validation/num_examples': 3000, 'test/accuracy': 0.7128929495811462, 'test/loss': 1.2559876441955566, 'test/bleu': 30.87501995821473, 'test/num_examples': 3003, 'score': 45396.00702667236, 'total_duration': 76722.7999753952, 'accumulated_submission_time': 45396.00702667236, 'accumulated_eval_time': 31320.80184316635, 'accumulated_logging_time': 1.9225962162017822}
I0210 23:21:39.942557 140277053363968 logging_writer.py:48] [126663] accumulated_eval_time=31320.801843, accumulated_logging_time=1.922596, accumulated_submission_time=45396.007027, global_step=126663, preemption_count=0, score=45396.007027, test/accuracy=0.712893, test/bleu=30.875020, test/loss=1.255988, test/num_examples=3003, total_duration=76722.799975, train/accuracy=0.696388, train/bleu=35.368536, train/loss=1.367734, validation/accuracy=0.695032, validation/bleu=30.772195, validation/loss=1.364340, validation/num_examples=3000
I0210 23:21:53.500016 140277044971264 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.22516921162605286, loss=1.3928031921386719
I0210 23:22:29.154912 140277053363968 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.23438170552253723, loss=1.4464200735092163
I0210 23:23:04.947842 140277044971264 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.2412874698638916, loss=1.5206758975982666
I0210 23:23:40.728528 140277053363968 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.22948330640792847, loss=1.410465955734253
I0210 23:24:16.557617 140277044971264 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.2387140691280365, loss=1.477330207824707
I0210 23:24:52.406240 140277053363968 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.2394874542951584, loss=1.4939637184143066
I0210 23:25:28.241620 140277044971264 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.2423374503850937, loss=1.4035499095916748
I0210 23:26:04.075202 140277053363968 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.2517233192920685, loss=1.425391435623169
I0210 23:26:39.953985 140277044971264 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.23883560299873352, loss=1.4832178354263306
I0210 23:27:15.772757 140277053363968 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.23887822031974792, loss=1.5033063888549805
I0210 23:27:51.559515 140277044971264 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.23040986061096191, loss=1.529618263244629
I0210 23:28:27.351285 140277053363968 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.2340814620256424, loss=1.4497332572937012
I0210 23:29:03.174398 140277044971264 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.22866135835647583, loss=1.498477578163147
I0210 23:29:38.996853 140277053363968 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.23864981532096863, loss=1.441518783569336
I0210 23:30:14.816677 140277044971264 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.2313365787267685, loss=1.5636085271835327
I0210 23:30:50.650742 140277053363968 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.23688600957393646, loss=1.479067325592041
I0210 23:31:26.469300 140277044971264 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.23416170477867126, loss=1.4988688230514526
I0210 23:32:02.328024 140277053363968 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.2357264757156372, loss=1.4232866764068604
I0210 23:32:38.146075 140277044971264 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.23276354372501373, loss=1.4115651845932007
I0210 23:33:13.973623 140277053363968 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.22475549578666687, loss=1.4533729553222656
I0210 23:33:49.801735 140277044971264 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.22669266164302826, loss=1.466381549835205
I0210 23:34:25.646894 140277053363968 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.24337027966976166, loss=1.4871866703033447
I0210 23:35:01.485588 140277044971264 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.2291145622730255, loss=1.4571747779846191
I0210 23:35:37.302166 140277053363968 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.24391016364097595, loss=1.448444128036499
I0210 23:35:40.246692 140446903760704 spec.py:321] Evaluating on the training split.
I0210 23:35:43.253525 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 23:39:46.129784 140446903760704 spec.py:333] Evaluating on the validation split.
I0210 23:39:48.864987 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 23:43:03.863193 140446903760704 spec.py:349] Evaluating on the test split.
I0210 23:43:06.594174 140446903760704 workload.py:181] Translating evaluation dataset.
I0210 23:46:40.969571 140446903760704 submission_runner.py:408] Time since start: 78223.86s, 	Step: 129010, 	{'train/accuracy': 0.6947799324989319, 'train/loss': 1.3747010231018066, 'train/bleu': 35.44969392992167, 'validation/accuracy': 0.6953044533729553, 'validation/loss': 1.3621292114257812, 'validation/bleu': 30.786053998723386, 'validation/num_examples': 3000, 'test/accuracy': 0.7133809924125671, 'test/loss': 1.2536063194274902, 'test/bleu': 30.837718893970138, 'test/num_examples': 3003, 'score': 46236.22338271141, 'total_duration': 78223.86001873016, 'accumulated_submission_time': 46236.22338271141, 'accumulated_eval_time': 31981.524652004242, 'accumulated_logging_time': 1.9672255516052246}
I0210 23:46:41.002686 140277044971264 logging_writer.py:48] [129010] accumulated_eval_time=31981.524652, accumulated_logging_time=1.967226, accumulated_submission_time=46236.223383, global_step=129010, preemption_count=0, score=46236.223383, test/accuracy=0.713381, test/bleu=30.837719, test/loss=1.253606, test/num_examples=3003, total_duration=78223.860019, train/accuracy=0.694780, train/bleu=35.449694, train/loss=1.374701, validation/accuracy=0.695304, validation/bleu=30.786054, validation/loss=1.362129, validation/num_examples=3000
I0210 23:47:13.440708 140277053363968 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.2311979979276657, loss=1.5257867574691772
I0210 23:47:49.177134 140277044971264 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.2391783446073532, loss=1.500693440437317
I0210 23:48:24.972050 140277053363968 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.23968979716300964, loss=1.5363959074020386
I0210 23:49:00.764871 140277044971264 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.2518017292022705, loss=1.5172535181045532
I0210 23:49:36.571708 140277053363968 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.2302149087190628, loss=1.4313585758209229
I0210 23:50:12.393956 140277044971264 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.2294602245092392, loss=1.5167185068130493
I0210 23:50:48.225175 140277053363968 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.22954119741916656, loss=1.5306849479675293
I0210 23:51:24.056125 140277044971264 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.2333347350358963, loss=1.4507635831832886
I0210 23:51:59.858714 140277053363968 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.2338036298751831, loss=1.4427871704101562
I0210 23:52:35.704636 140277044971264 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.22633616626262665, loss=1.4700952768325806
I0210 23:53:11.520027 140277053363968 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.24452751874923706, loss=1.5061978101730347
I0210 23:53:47.329748 140277044971264 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.22463975846767426, loss=1.4833917617797852
I0210 23:54:23.181314 140277053363968 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.24077939987182617, loss=1.4534472227096558
I0210 23:54:58.991693 140277044971264 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.22640614211559296, loss=1.3911986351013184
I0210 23:55:34.830863 140277053363968 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.23335373401641846, loss=1.4557194709777832
I0210 23:56:10.647099 140277044971264 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.24072204530239105, loss=1.3988673686981201
I0210 23:56:46.467852 140277053363968 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.23806163668632507, loss=1.4710192680358887
I0210 23:57:22.283405 140277044971264 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.2432478964328766, loss=1.5066170692443848
I0210 23:57:58.119797 140277053363968 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.2428203672170639, loss=1.3695887327194214
I0210 23:58:33.960908 140277044971264 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.24300234019756317, loss=1.388023018836975
I0210 23:59:09.802535 140277053363968 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.22458770871162415, loss=1.3993438482284546
I0210 23:59:45.623555 140277044971264 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.23947642743587494, loss=1.4638683795928955
I0211 00:00:21.439557 140277053363968 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.22734816372394562, loss=1.4718270301818848
I0211 00:00:41.201935 140446903760704 spec.py:321] Evaluating on the training split.
I0211 00:00:44.211634 140446903760704 workload.py:181] Translating evaluation dataset.
I0211 00:04:57.935432 140446903760704 spec.py:333] Evaluating on the validation split.
I0211 00:05:00.651374 140446903760704 workload.py:181] Translating evaluation dataset.
I0211 00:08:07.246859 140446903760704 spec.py:349] Evaluating on the test split.
I0211 00:08:09.959984 140446903760704 workload.py:181] Translating evaluation dataset.
I0211 00:11:36.503381 140446903760704 submission_runner.py:408] Time since start: 79719.39s, 	Step: 131357, 	{'train/accuracy': 0.6980651617050171, 'train/loss': 1.3623944520950317, 'train/bleu': 35.745533977811, 'validation/accuracy': 0.6951928734779358, 'validation/loss': 1.3623720407485962, 'validation/bleu': 30.816823338684074, 'validation/num_examples': 3000, 'test/accuracy': 0.7132763862609863, 'test/loss': 1.2537087202072144, 'test/bleu': 30.857126776061257, 'test/num_examples': 3003, 'score': 47076.33667135239, 'total_duration': 79719.39379668236, 'accumulated_submission_time': 47076.33667135239, 'accumulated_eval_time': 32636.826003074646, 'accumulated_logging_time': 2.012143135070801}
I0211 00:11:36.539327 140277044971264 logging_writer.py:48] [131357] accumulated_eval_time=32636.826003, accumulated_logging_time=2.012143, accumulated_submission_time=47076.336671, global_step=131357, preemption_count=0, score=47076.336671, test/accuracy=0.713276, test/bleu=30.857127, test/loss=1.253709, test/num_examples=3003, total_duration=79719.393797, train/accuracy=0.698065, train/bleu=35.745534, train/loss=1.362394, validation/accuracy=0.695193, validation/bleu=30.816823, validation/loss=1.362372, validation/num_examples=3000
I0211 00:11:52.246362 140277053363968 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.23556186258792877, loss=1.482901930809021
I0211 00:12:27.944697 140277044971264 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.23309986293315887, loss=1.4772605895996094
I0211 00:13:03.712274 140277053363968 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.24226202070713043, loss=1.4369754791259766
I0211 00:13:39.522454 140277044971264 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.2312496453523636, loss=1.4703295230865479
I0211 00:14:15.361443 140277053363968 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.23734821379184723, loss=1.5133002996444702
I0211 00:14:51.189918 140277044971264 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.23060771822929382, loss=1.4255547523498535
I0211 00:15:27.014215 140277053363968 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.2225581258535385, loss=1.4377237558364868
I0211 00:16:02.829616 140277044971264 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.240070179104805, loss=1.4913591146469116
I0211 00:16:38.686127 140277053363968 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.2455245703458786, loss=1.4826178550720215
I0211 00:17:14.524559 140277044971264 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.2378643900156021, loss=1.4719619750976562
I0211 00:17:50.362629 140277053363968 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.23707321286201477, loss=1.5127588510513306
I0211 00:18:26.203840 140277044971264 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.23154138028621674, loss=1.4090690612792969
I0211 00:19:02.041715 140277053363968 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.24261683225631714, loss=1.494104027748108
I0211 00:19:37.895674 140277044971264 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.22968712449073792, loss=1.5253218412399292
I0211 00:20:13.719571 140277053363968 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.22074146568775177, loss=1.4171987771987915
I0211 00:20:49.534777 140277044971264 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.2235494703054428, loss=1.4206938743591309
I0211 00:21:25.387228 140277053363968 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.23782403767108917, loss=1.4628651142120361
I0211 00:22:01.219596 140277044971264 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.23701703548431396, loss=1.4334042072296143
I0211 00:22:37.088686 140277053363968 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.23096658289432526, loss=1.3800336122512817
I0211 00:23:12.914615 140277044971264 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.23601046204566956, loss=1.5506621599197388
I0211 00:23:24.110878 140446903760704 spec.py:321] Evaluating on the training split.
I0211 00:23:27.118896 140446903760704 workload.py:181] Translating evaluation dataset.
I0211 00:27:19.868412 140446903760704 spec.py:333] Evaluating on the validation split.
I0211 00:27:22.575994 140446903760704 workload.py:181] Translating evaluation dataset.
I0211 00:30:28.469256 140446903760704 spec.py:349] Evaluating on the test split.
I0211 00:30:31.180289 140446903760704 workload.py:181] Translating evaluation dataset.
I0211 00:33:57.879122 140446903760704 submission_runner.py:408] Time since start: 81060.77s, 	Step: 133333, 	{'train/accuracy': 0.6977786421775818, 'train/loss': 1.358901858329773, 'train/bleu': 35.85408950071874, 'validation/accuracy': 0.6951680779457092, 'validation/loss': 1.3623775243759155, 'validation/bleu': 30.784543671591038, 'validation/num_examples': 3000, 'test/accuracy': 0.7132763862609863, 'test/loss': 1.253868579864502, 'test/bleu': 30.852166892997438, 'test/num_examples': 3003, 'score': 47783.83299946785, 'total_duration': 81060.76957821846, 'accumulated_submission_time': 47783.83299946785, 'accumulated_eval_time': 33270.594187021255, 'accumulated_logging_time': 2.060373544692993}
I0211 00:33:57.915293 140277053363968 logging_writer.py:48] [133333] accumulated_eval_time=33270.594187, accumulated_logging_time=2.060374, accumulated_submission_time=47783.832999, global_step=133333, preemption_count=0, score=47783.832999, test/accuracy=0.713276, test/bleu=30.852167, test/loss=1.253869, test/num_examples=3003, total_duration=81060.769578, train/accuracy=0.697779, train/bleu=35.854090, train/loss=1.358902, validation/accuracy=0.695168, validation/bleu=30.784544, validation/loss=1.362378, validation/num_examples=3000
I0211 00:33:57.948590 140277044971264 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47783.832999
I0211 00:33:59.188016 140446903760704 checkpoints.py:490] Saving checkpoint at step: 133333
I0211 00:34:03.349000 140446903760704 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/wmt_jax/trial_5/checkpoint_133333
I0211 00:34:03.354781 140446903760704 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/wmt_jax/trial_5/checkpoint_133333.
I0211 00:34:03.417455 140446903760704 submission_runner.py:583] Tuning trial 5/5
I0211 00:34:03.417618 140446903760704 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0211 00:34:03.427308 140446903760704 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005809449939988554, 'train/loss': 11.19240951538086, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.190281867980957, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.191027641296387, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 31.995970487594604, 'total_duration': 873.7377886772156, 'accumulated_submission_time': 31.995970487594604, 'accumulated_eval_time': 841.7417721748352, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2343, {'train/accuracy': 0.5090352892875671, 'train/loss': 2.874778985977173, 'train/bleu': 22.760860535397587, 'validation/accuracy': 0.509181559085846, 'validation/loss': 2.8774378299713135, 'validation/bleu': 18.645719298845435, 'validation/num_examples': 3000, 'test/accuracy': 0.5100807547569275, 'test/loss': 2.911456823348999, 'test/bleu': 17.119625855590364, 'test/num_examples': 3003, 'score': 872.1930792331696, 'total_duration': 2251.830168247223, 'accumulated_submission_time': 872.1930792331696, 'accumulated_eval_time': 1379.5385262966156, 'accumulated_logging_time': 0.020632028579711914, 'global_step': 2343, 'preemption_count': 0}), (4687, {'train/accuracy': 0.5762688517570496, 'train/loss': 2.2632884979248047, 'train/bleu': 27.03187390595071, 'validation/accuracy': 0.5884490013122559, 'validation/loss': 2.1593129634857178, 'validation/bleu': 23.441319934527357, 'validation/num_examples': 3000, 'test/accuracy': 0.5936436057090759, 'test/loss': 2.1284549236297607, 'test/bleu': 22.072443389635552, 'test/num_examples': 3003, 'score': 1712.265777349472, 'total_duration': 3552.3742141723633, 'accumulated_submission_time': 1712.265777349472, 'accumulated_eval_time': 1839.9097967147827, 'accumulated_logging_time': 0.046051979064941406, 'global_step': 4687, 'preemption_count': 0}), (7032, {'train/accuracy': 0.6044877767562866, 'train/loss': 2.0103962421417236, 'train/bleu': 29.150223633210214, 'validation/accuracy': 0.6167561411857605, 'validation/loss': 1.9335185289382935, 'validation/bleu': 25.145358707762522, 'validation/num_examples': 3000, 'test/accuracy': 0.6223810315132141, 'test/loss': 1.891671061515808, 'test/bleu': 23.716157379390612, 'test/num_examples': 3003, 'score': 2552.231223344803, 'total_duration': 4832.6473343372345, 'accumulated_submission_time': 2552.231223344803, 'accumulated_eval_time': 2280.1172001361847, 'accumulated_logging_time': 0.07169175148010254, 'global_step': 7032, 'preemption_count': 0}), (9378, {'train/accuracy': 0.6121419668197632, 'train/loss': 1.9549943208694458, 'train/bleu': 29.574920246863407, 'validation/accuracy': 0.6282377243041992, 'validation/loss': 1.8250943422317505, 'validation/bleu': 26.20106139743751, 'validation/num_examples': 3000, 'test/accuracy': 0.637569010257721, 'test/loss': 1.7603546380996704, 'test/bleu': 25.21416047743482, 'test/num_examples': 3003, 'score': 3392.4043912887573, 'total_duration': 6102.503938674927, 'accumulated_submission_time': 3392.4043912887573, 'accumulated_eval_time': 2709.7009241580963, 'accumulated_logging_time': 0.09729146957397461, 'global_step': 9378, 'preemption_count': 0}), (11725, {'train/accuracy': 0.6145750284194946, 'train/loss': 1.9365715980529785, 'train/bleu': 29.91781370647072, 'validation/accuracy': 0.637115478515625, 'validation/loss': 1.762860894203186, 'validation/bleu': 26.47492510250528, 'validation/num_examples': 3000, 'test/accuracy': 0.6475510001182556, 'test/loss': 1.691665768623352, 'test/bleu': 25.765832245270712, 'test/num_examples': 3003, 'score': 4232.526937961578, 'total_duration': 7420.881967782974, 'accumulated_submission_time': 4232.526937961578, 'accumulated_eval_time': 3187.856215953827, 'accumulated_logging_time': 0.12502813339233398, 'global_step': 11725, 'preemption_count': 0}), (14071, {'train/accuracy': 0.6287801861763, 'train/loss': 1.8265225887298584, 'train/bleu': 30.574075132997642, 'validation/accuracy': 0.6427942514419556, 'validation/loss': 1.715195655822754, 'validation/bleu': 27.20454930807229, 'validation/num_examples': 3000, 'test/accuracy': 0.6514670848846436, 'test/loss': 1.646348476409912, 'test/bleu': 26.01923512162698, 'test/num_examples': 3003, 'score': 5072.729242563248, 'total_duration': 8713.320725440979, 'accumulated_submission_time': 5072.729242563248, 'accumulated_eval_time': 3639.9922440052032, 'accumulated_logging_time': 0.15134167671203613, 'global_step': 14071, 'preemption_count': 0}), (16415, {'train/accuracy': 0.6283307075500488, 'train/loss': 1.8313599824905396, 'train/bleu': 30.64059289155258, 'validation/accuracy': 0.6439225673675537, 'validation/loss': 1.6987574100494385, 'validation/bleu': 26.862268504080657, 'validation/num_examples': 3000, 'test/accuracy': 0.6555575132369995, 'test/loss': 1.6271188259124756, 'test/bleu': 25.974975993637944, 'test/num_examples': 3003, 'score': 5912.799705505371, 'total_duration': 10050.71216583252, 'accumulated_submission_time': 5912.799705505371, 'accumulated_eval_time': 4137.2096972465515, 'accumulated_logging_time': 0.17881011962890625, 'global_step': 16415, 'preemption_count': 0}), (18760, {'train/accuracy': 0.6313144564628601, 'train/loss': 1.8021981716156006, 'train/bleu': 30.330178704263602, 'validation/accuracy': 0.6500601172447205, 'validation/loss': 1.666564702987671, 'validation/bleu': 27.40282885917218, 'validation/num_examples': 3000, 'test/accuracy': 0.6605310440063477, 'test/loss': 1.594834566116333, 'test/bleu': 26.84595365712397, 'test/num_examples': 3003, 'score': 6752.885207414627, 'total_duration': 11451.534358024597, 'accumulated_submission_time': 6752.885207414627, 'accumulated_eval_time': 4697.838307380676, 'accumulated_logging_time': 0.2108919620513916, 'global_step': 18760, 'preemption_count': 0}), (21106, {'train/accuracy': 0.6347932815551758, 'train/loss': 1.7635672092437744, 'train/bleu': 31.21414025678416, 'validation/accuracy': 0.6517339944839478, 'validation/loss': 1.6439547538757324, 'validation/bleu': 27.60810916818256, 'validation/num_examples': 3000, 'test/accuracy': 0.6628551483154297, 'test/loss': 1.5742295980453491, 'test/bleu': 26.805785981781558, 'test/num_examples': 3003, 'score': 7592.865230083466, 'total_duration': 12941.91342139244, 'accumulated_submission_time': 7592.865230083466, 'accumulated_eval_time': 5348.135931015015, 'accumulated_logging_time': 0.23898673057556152, 'global_step': 21106, 'preemption_count': 0}), (23452, {'train/accuracy': 0.6305269598960876, 'train/loss': 1.790299415588379, 'train/bleu': 30.610692115272517, 'validation/accuracy': 0.6529738903045654, 'validation/loss': 1.6360857486724854, 'validation/bleu': 27.64664612249136, 'validation/num_examples': 3000, 'test/accuracy': 0.6636337637901306, 'test/loss': 1.5664596557617188, 'test/bleu': 27.270381340846022, 'test/num_examples': 3003, 'score': 8432.95629954338, 'total_duration': 14270.456376552582, 'accumulated_submission_time': 8432.95629954338, 'accumulated_eval_time': 5836.484181642532, 'accumulated_logging_time': 0.26767778396606445, 'global_step': 23452, 'preemption_count': 0}), (25798, {'train/accuracy': 0.642456591129303, 'train/loss': 1.7020819187164307, 'train/bleu': 31.552448034746874, 'validation/accuracy': 0.6566688418388367, 'validation/loss': 1.6203370094299316, 'validation/bleu': 28.153017370131508, 'validation/num_examples': 3000, 'test/accuracy': 0.6658416390419006, 'test/loss': 1.5436705350875854, 'test/bleu': 27.358169319863094, 'test/num_examples': 3003, 'score': 9273.077693939209, 'total_duration': 15630.35198712349, 'accumulated_submission_time': 9273.077693939209, 'accumulated_eval_time': 6356.156872987747, 'accumulated_logging_time': 0.29597902297973633, 'global_step': 25798, 'preemption_count': 0}), (28144, {'train/accuracy': 0.6392359733581543, 'train/loss': 1.7401916980743408, 'train/bleu': 31.22346015081715, 'validation/accuracy': 0.6564580798149109, 'validation/loss': 1.6131483316421509, 'validation/bleu': 27.81124516471783, 'validation/num_examples': 3000, 'test/accuracy': 0.6691185832023621, 'test/loss': 1.5272434949874878, 'test/bleu': 27.576740566813548, 'test/num_examples': 3003, 'score': 10113.140311479568, 'total_duration': 16915.07120656967, 'accumulated_submission_time': 10113.140311479568, 'accumulated_eval_time': 6800.711962461472, 'accumulated_logging_time': 0.3239438533782959, 'global_step': 28144, 'preemption_count': 0}), (30490, {'train/accuracy': 0.6276158094406128, 'train/loss': 1.8166896104812622, 'train/bleu': 29.779899990113677, 'validation/accuracy': 0.6502957344055176, 'validation/loss': 1.6435110569000244, 'validation/bleu': 26.92453743681419, 'validation/num_examples': 3000, 'test/accuracy': 0.6619836091995239, 'test/loss': 1.5729281902313232, 'test/bleu': 26.57815801463406, 'test/num_examples': 3003, 'score': 10953.335748672485, 'total_duration': 18238.866058826447, 'accumulated_submission_time': 10953.335748672485, 'accumulated_eval_time': 7284.199323177338, 'accumulated_logging_time': 0.3592972755432129, 'global_step': 30490, 'preemption_count': 0}), (32836, {'train/accuracy': 0.6437987089157104, 'train/loss': 1.7077805995941162, 'train/bleu': 31.70166116605699, 'validation/accuracy': 0.6607481837272644, 'validation/loss': 1.5932337045669556, 'validation/bleu': 28.223498811079704, 'validation/num_examples': 3000, 'test/accuracy': 0.6721515655517578, 'test/loss': 1.512355923652649, 'test/bleu': 27.20909834912664, 'test/num_examples': 3003, 'score': 11793.542217969894, 'total_duration': 19616.90053844452, 'accumulated_submission_time': 11793.542217969894, 'accumulated_eval_time': 7821.921438455582, 'accumulated_logging_time': 0.39046382904052734, 'global_step': 32836, 'preemption_count': 0}), (35182, {'train/accuracy': 0.644922137260437, 'train/loss': 1.7093086242675781, 'train/bleu': 31.362425620829725, 'validation/accuracy': 0.6612069010734558, 'validation/loss': 1.584161639213562, 'validation/bleu': 28.028519514767073, 'validation/num_examples': 3000, 'test/accuracy': 0.6742432117462158, 'test/loss': 1.504552960395813, 'test/bleu': 27.942357040831503, 'test/num_examples': 3003, 'score': 12633.712213754654, 'total_duration': 21078.17825651169, 'accumulated_submission_time': 12633.712213754654, 'accumulated_eval_time': 8442.923906326294, 'accumulated_logging_time': 0.4212970733642578, 'global_step': 35182, 'preemption_count': 0}), (37528, {'train/accuracy': 0.6506013870239258, 'train/loss': 1.6590782403945923, 'train/bleu': 32.2079672733254, 'validation/accuracy': 0.6631659865379333, 'validation/loss': 1.5785056352615356, 'validation/bleu': 28.21083540443361, 'validation/num_examples': 3000, 'test/accuracy': 0.6755679845809937, 'test/loss': 1.4972996711730957, 'test/bleu': 27.54270861029842, 'test/num_examples': 3003, 'score': 13473.850688695908, 'total_duration': 22409.57164978981, 'accumulated_submission_time': 13473.850688695908, 'accumulated_eval_time': 8934.070060491562, 'accumulated_logging_time': 0.45255208015441895, 'global_step': 37528, 'preemption_count': 0}), (39873, {'train/accuracy': 0.6454939842224121, 'train/loss': 1.692597508430481, 'train/bleu': 31.815291168303688, 'validation/accuracy': 0.6619632840156555, 'validation/loss': 1.576145052909851, 'validation/bleu': 28.208675423715423, 'validation/num_examples': 3000, 'test/accuracy': 0.6739526987075806, 'test/loss': 1.4942594766616821, 'test/bleu': 27.546707690715404, 'test/num_examples': 3003, 'score': 14313.951741695404, 'total_duration': 23827.573871850967, 'accumulated_submission_time': 14313.951741695404, 'accumulated_eval_time': 9511.859144210815, 'accumulated_logging_time': 0.4880993366241455, 'global_step': 39873, 'preemption_count': 0}), (42219, {'train/accuracy': 0.6474224925041199, 'train/loss': 1.68617844581604, 'train/bleu': 31.77898337621006, 'validation/accuracy': 0.6637239456176758, 'validation/loss': 1.564249873161316, 'validation/bleu': 28.69145620875173, 'validation/num_examples': 3000, 'test/accuracy': 0.6774853467941284, 'test/loss': 1.4813222885131836, 'test/bleu': 27.985960600248518, 'test/num_examples': 3003, 'score': 15154.16379904747, 'total_duration': 25183.074380159378, 'accumulated_submission_time': 15154.16379904747, 'accumulated_eval_time': 10027.034233808517, 'accumulated_logging_time': 0.5258986949920654, 'global_step': 42219, 'preemption_count': 0}), (44564, {'train/accuracy': 0.6538242101669312, 'train/loss': 1.6220608949661255, 'train/bleu': 32.21654232899015, 'validation/accuracy': 0.6659061908721924, 'validation/loss': 1.5523436069488525, 'validation/bleu': 28.64157721411009, 'validation/num_examples': 3000, 'test/accuracy': 0.6764511466026306, 'test/loss': 1.4741398096084595, 'test/bleu': 27.830552615538394, 'test/num_examples': 3003, 'score': 15994.344855546951, 'total_duration': 26540.418180942535, 'accumulated_submission_time': 15994.344855546951, 'accumulated_eval_time': 10544.088871002197, 'accumulated_logging_time': 0.5563359260559082, 'global_step': 44564, 'preemption_count': 0}), (46910, {'train/accuracy': 0.6494661569595337, 'train/loss': 1.669074296951294, 'train/bleu': 31.6695985126084, 'validation/accuracy': 0.6667740941047668, 'validation/loss': 1.5472114086151123, 'validation/bleu': 28.777955986349998, 'validation/num_examples': 3000, 'test/accuracy': 0.6811225414276123, 'test/loss': 1.4597563743591309, 'test/bleu': 28.346774455501244, 'test/num_examples': 3003, 'score': 16834.357803106308, 'total_duration': 27901.11219573021, 'accumulated_submission_time': 16834.357803106308, 'accumulated_eval_time': 11064.661159992218, 'accumulated_logging_time': 0.5887689590454102, 'global_step': 46910, 'preemption_count': 0}), (49256, {'train/accuracy': 0.6462664604187012, 'train/loss': 1.6856175661087036, 'train/bleu': 31.678658230344087, 'validation/accuracy': 0.6664517521858215, 'validation/loss': 1.5448410511016846, 'validation/bleu': 28.842956598526044, 'validation/num_examples': 3000, 'test/accuracy': 0.6809831261634827, 'test/loss': 1.4532852172851562, 'test/bleu': 28.45048140232037, 'test/num_examples': 3003, 'score': 17674.36550784111, 'total_duration': 29211.590218305588, 'accumulated_submission_time': 17674.36550784111, 'accumulated_eval_time': 11535.024050235748, 'accumulated_logging_time': 0.6217913627624512, 'global_step': 49256, 'preemption_count': 0}), (51601, {'train/accuracy': 0.6567646265029907, 'train/loss': 1.613392949104309, 'train/bleu': 32.20008454597276, 'validation/accuracy': 0.668398380279541, 'validation/loss': 1.5337615013122559, 'validation/bleu': 28.792989343388353, 'validation/num_examples': 3000, 'test/accuracy': 0.6818430423736572, 'test/loss': 1.4524085521697998, 'test/bleu': 28.224619330030805, 'test/num_examples': 3003, 'score': 18514.27074623108, 'total_duration': 30635.275083065033, 'accumulated_submission_time': 18514.27074623108, 'accumulated_eval_time': 12118.698055744171, 'accumulated_logging_time': 0.6539661884307861, 'global_step': 51601, 'preemption_count': 0}), (53947, {'train/accuracy': 0.6505134701728821, 'train/loss': 1.660967469215393, 'train/bleu': 31.867379464977144, 'validation/accuracy': 0.6687207818031311, 'validation/loss': 1.5278306007385254, 'validation/bleu': 28.76152895014794, 'validation/num_examples': 3000, 'test/accuracy': 0.6840276718139648, 'test/loss': 1.4374014139175415, 'test/bleu': 28.765265595968746, 'test/num_examples': 3003, 'score': 19354.333768606186, 'total_duration': 32073.26548433304, 'accumulated_submission_time': 19354.333768606186, 'accumulated_eval_time': 12716.516110658646, 'accumulated_logging_time': 0.6878187656402588, 'global_step': 53947, 'preemption_count': 0}), (56293, {'train/accuracy': 0.6667506098747253, 'train/loss': 1.5589523315429688, 'train/bleu': 33.09192433508889, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.5208386182785034, 'validation/bleu': 28.633689483150718, 'validation/num_examples': 3000, 'test/accuracy': 0.6826448440551758, 'test/loss': 1.4398152828216553, 'test/bleu': 28.341602362376708, 'test/num_examples': 3003, 'score': 20194.51906824112, 'total_duration': 33494.9159014225, 'accumulated_submission_time': 20194.51906824112, 'accumulated_eval_time': 13297.87374830246, 'accumulated_logging_time': 0.7198841571807861, 'global_step': 56293, 'preemption_count': 0}), (58639, {'train/accuracy': 0.655954897403717, 'train/loss': 1.618160605430603, 'train/bleu': 32.188351151900314, 'validation/accuracy': 0.6726884841918945, 'validation/loss': 1.5124878883361816, 'validation/bleu': 29.072180110225716, 'validation/num_examples': 3000, 'test/accuracy': 0.6835163831710815, 'test/loss': 1.4296272993087769, 'test/bleu': 28.599861865485746, 'test/num_examples': 3003, 'score': 21034.617770195007, 'total_duration': 34978.909477710724, 'accumulated_submission_time': 21034.617770195007, 'accumulated_eval_time': 13941.65684556961, 'accumulated_logging_time': 0.7570688724517822, 'global_step': 58639, 'preemption_count': 0}), (60985, {'train/accuracy': 0.6542706489562988, 'train/loss': 1.6405000686645508, 'train/bleu': 32.1757717300823, 'validation/accuracy': 0.6717957258224487, 'validation/loss': 1.5104609727859497, 'validation/bleu': 29.062029670361643, 'validation/num_examples': 3000, 'test/accuracy': 0.6864215135574341, 'test/loss': 1.420398235321045, 'test/bleu': 29.007746720886413, 'test/num_examples': 3003, 'score': 21874.773897647858, 'total_duration': 36303.46973657608, 'accumulated_submission_time': 21874.773897647858, 'accumulated_eval_time': 14425.952094316483, 'accumulated_logging_time': 0.7913601398468018, 'global_step': 60985, 'preemption_count': 0}), (63330, {'train/accuracy': 0.6675124168395996, 'train/loss': 1.544390082359314, 'train/bleu': 32.92653022817832, 'validation/accuracy': 0.6732712388038635, 'validation/loss': 1.5051460266113281, 'validation/bleu': 29.133460583030878, 'validation/num_examples': 3000, 'test/accuracy': 0.6854802370071411, 'test/loss': 1.4173305034637451, 'test/bleu': 28.617106854066893, 'test/num_examples': 3003, 'score': 22714.68177628517, 'total_duration': 37652.378227472305, 'accumulated_submission_time': 22714.68177628517, 'accumulated_eval_time': 14934.84242773056, 'accumulated_logging_time': 0.8252365589141846, 'global_step': 63330, 'preemption_count': 0}), (65676, {'train/accuracy': 0.6562884449958801, 'train/loss': 1.612204670906067, 'train/bleu': 32.68944922577982, 'validation/accuracy': 0.6751435399055481, 'validation/loss': 1.4910575151443481, 'validation/bleu': 29.30038312727096, 'validation/num_examples': 3000, 'test/accuracy': 0.6907327175140381, 'test/loss': 1.3964228630065918, 'test/bleu': 29.165284759238112, 'test/num_examples': 3003, 'score': 23554.652008533478, 'total_duration': 39108.53287887573, 'accumulated_submission_time': 23554.652008533478, 'accumulated_eval_time': 15550.918023347855, 'accumulated_logging_time': 0.8605771064758301, 'global_step': 65676, 'preemption_count': 0}), (68022, {'train/accuracy': 0.6579658389091492, 'train/loss': 1.613708734512329, 'train/bleu': 32.35109639577502, 'validation/accuracy': 0.6769165992736816, 'validation/loss': 1.4885090589523315, 'validation/bleu': 29.626793474166714, 'validation/num_examples': 3000, 'test/accuracy': 0.6890709400177002, 'test/loss': 1.3990836143493652, 'test/bleu': 29.196933871882052, 'test/num_examples': 3003, 'score': 24394.624833345413, 'total_duration': 40496.11887073517, 'accumulated_submission_time': 24394.624833345413, 'accumulated_eval_time': 16098.42095041275, 'accumulated_logging_time': 0.8969278335571289, 'global_step': 68022, 'preemption_count': 0}), (70368, {'train/accuracy': 0.6629226207733154, 'train/loss': 1.5664702653884888, 'train/bleu': 32.78073814231867, 'validation/accuracy': 0.6758130788803101, 'validation/loss': 1.479118824005127, 'validation/bleu': 29.19225225701453, 'validation/num_examples': 3000, 'test/accuracy': 0.6903492212295532, 'test/loss': 1.3912490606307983, 'test/bleu': 28.780128995637558, 'test/num_examples': 3003, 'score': 25234.659603357315, 'total_duration': 42054.70087099075, 'accumulated_submission_time': 25234.659603357315, 'accumulated_eval_time': 16816.858570575714, 'accumulated_logging_time': 0.9333441257476807, 'global_step': 70368, 'preemption_count': 0}), (72714, {'train/accuracy': 0.6606534719467163, 'train/loss': 1.593955636024475, 'train/bleu': 32.511623246878834, 'validation/accuracy': 0.6776605248451233, 'validation/loss': 1.4730331897735596, 'validation/bleu': 29.498651552431973, 'validation/num_examples': 3000, 'test/accuracy': 0.6935099959373474, 'test/loss': 1.380598545074463, 'test/bleu': 29.474507063397496, 'test/num_examples': 3003, 'score': 26074.649913072586, 'total_duration': 43471.833458423615, 'accumulated_submission_time': 26074.649913072586, 'accumulated_eval_time': 17393.892706871033, 'accumulated_logging_time': 0.9688091278076172, 'global_step': 72714, 'preemption_count': 0}), (75061, {'train/accuracy': 0.6858612298965454, 'train/loss': 1.4403479099273682, 'train/bleu': 34.89402750189174, 'validation/accuracy': 0.677734911441803, 'validation/loss': 1.4711121320724487, 'validation/bleu': 29.468598279995035, 'validation/num_examples': 3000, 'test/accuracy': 0.6918134093284607, 'test/loss': 1.3813815116882324, 'test/bleu': 29.141061383739522, 'test/num_examples': 3003, 'score': 26914.733607769012, 'total_duration': 44794.491381406784, 'accumulated_submission_time': 26914.733607769012, 'accumulated_eval_time': 17876.356063604355, 'accumulated_logging_time': 1.006338119506836, 'global_step': 75061, 'preemption_count': 0}), (77406, {'train/accuracy': 0.665233850479126, 'train/loss': 1.5531893968582153, 'train/bleu': 33.34801089317181, 'validation/accuracy': 0.6795451641082764, 'validation/loss': 1.4581555128097534, 'validation/bleu': 29.55528459819339, 'validation/num_examples': 3000, 'test/accuracy': 0.694428026676178, 'test/loss': 1.3683438301086426, 'test/bleu': 29.284875716363665, 'test/num_examples': 3003, 'score': 27754.925392866135, 'total_duration': 46285.17871427536, 'accumulated_submission_time': 27754.925392866135, 'accumulated_eval_time': 18526.739768743515, 'accumulated_logging_time': 1.0434327125549316, 'global_step': 77406, 'preemption_count': 0}), (79751, {'train/accuracy': 0.6607846617698669, 'train/loss': 1.5785906314849854, 'train/bleu': 33.05226990077929, 'validation/accuracy': 0.6790120601654053, 'validation/loss': 1.4651505947113037, 'validation/bleu': 29.44278157741172, 'validation/num_examples': 3000, 'test/accuracy': 0.6931613683700562, 'test/loss': 1.3743035793304443, 'test/bleu': 29.269250344436912, 'test/num_examples': 3003, 'score': 28595.104697227478, 'total_duration': 47696.377888441086, 'accumulated_submission_time': 28595.104697227478, 'accumulated_eval_time': 19097.646564006805, 'accumulated_logging_time': 1.0817084312438965, 'global_step': 79751, 'preemption_count': 0}), (82097, {'train/accuracy': 0.6755383014678955, 'train/loss': 1.49465811252594, 'train/bleu': 33.659196883772914, 'validation/accuracy': 0.6815910339355469, 'validation/loss': 1.449007272720337, 'validation/bleu': 29.689568520414724, 'validation/num_examples': 3000, 'test/accuracy': 0.6955435872077942, 'test/loss': 1.3564544916152954, 'test/bleu': 29.427192194045116, 'test/num_examples': 3003, 'score': 29434.99210190773, 'total_duration': 49080.874106407166, 'accumulated_submission_time': 29434.99210190773, 'accumulated_eval_time': 19642.142485380173, 'accumulated_logging_time': 1.1191542148590088, 'global_step': 82097, 'preemption_count': 0}), (84442, {'train/accuracy': 0.6705901026725769, 'train/loss': 1.5163570642471313, 'train/bleu': 33.776368870019915, 'validation/accuracy': 0.6826077699661255, 'validation/loss': 1.4386560916900635, 'validation/bleu': 29.743246304174004, 'validation/num_examples': 3000, 'test/accuracy': 0.697402834892273, 'test/loss': 1.3449287414550781, 'test/bleu': 29.554429663796398, 'test/num_examples': 3003, 'score': 30274.89132285118, 'total_duration': 50588.53385710716, 'accumulated_submission_time': 30274.89132285118, 'accumulated_eval_time': 20309.78834581375, 'accumulated_logging_time': 1.156053066253662, 'global_step': 84442, 'preemption_count': 0}), (86788, {'train/accuracy': 0.6693463325500488, 'train/loss': 1.5310808420181274, 'train/bleu': 33.519672285560624, 'validation/accuracy': 0.682632565498352, 'validation/loss': 1.4379298686981201, 'validation/bleu': 30.200129200007808, 'validation/num_examples': 3000, 'test/accuracy': 0.6986927390098572, 'test/loss': 1.3387260437011719, 'test/bleu': 29.753769352763033, 'test/num_examples': 3003, 'score': 31114.93718099594, 'total_duration': 51943.9401807785, 'accumulated_submission_time': 31114.93718099594, 'accumulated_eval_time': 20825.03607749939, 'accumulated_logging_time': 1.1935102939605713, 'global_step': 86788, 'preemption_count': 0}), (89133, {'train/accuracy': 0.6757782101631165, 'train/loss': 1.485595941543579, 'train/bleu': 34.190875248660575, 'validation/accuracy': 0.6854719519615173, 'validation/loss': 1.4247512817382812, 'validation/bleu': 29.772108184170435, 'validation/num_examples': 3000, 'test/accuracy': 0.6976584792137146, 'test/loss': 1.333077311515808, 'test/bleu': 29.528959397441458, 'test/num_examples': 3003, 'score': 31955.03034901619, 'total_duration': 53384.378826379776, 'accumulated_submission_time': 31955.03034901619, 'accumulated_eval_time': 21425.267327308655, 'accumulated_logging_time': 1.2301530838012695, 'global_step': 89133, 'preemption_count': 0}), (91479, {'train/accuracy': 0.6764565110206604, 'train/loss': 1.4868851900100708, 'train/bleu': 33.79142281553153, 'validation/accuracy': 0.6856207251548767, 'validation/loss': 1.4216893911361694, 'validation/bleu': 30.081463864085272, 'validation/num_examples': 3000, 'test/accuracy': 0.7033525109291077, 'test/loss': 1.3221434354782104, 'test/bleu': 30.261509158958816, 'test/num_examples': 3003, 'score': 32794.96316599846, 'total_duration': 54851.176552057266, 'accumulated_submission_time': 32794.96316599846, 'accumulated_eval_time': 22052.010696411133, 'accumulated_logging_time': 1.276489496231079, 'global_step': 91479, 'preemption_count': 0}), (93825, {'train/accuracy': 0.690422534942627, 'train/loss': 1.4100230932235718, 'train/bleu': 34.952902904917686, 'validation/accuracy': 0.685050368309021, 'validation/loss': 1.4175060987472534, 'validation/bleu': 30.129531487347982, 'validation/num_examples': 3000, 'test/accuracy': 0.7028295993804932, 'test/loss': 1.319978952407837, 'test/bleu': 30.10693494571689, 'test/num_examples': 3003, 'score': 33634.999903678894, 'total_duration': 56272.6827609539, 'accumulated_submission_time': 33634.999903678894, 'accumulated_eval_time': 22633.36717224121, 'accumulated_logging_time': 1.315727710723877, 'global_step': 93825, 'preemption_count': 0}), (96171, {'train/accuracy': 0.6796119809150696, 'train/loss': 1.4651366472244263, 'train/bleu': 34.32243432293985, 'validation/accuracy': 0.6871210336685181, 'validation/loss': 1.4065537452697754, 'validation/bleu': 30.200335772061464, 'validation/num_examples': 3000, 'test/accuracy': 0.7031084895133972, 'test/loss': 1.311916708946228, 'test/bleu': 30.018684147444766, 'test/num_examples': 3003, 'score': 34475.117656469345, 'total_duration': 57911.7241795063, 'accumulated_submission_time': 34475.117656469345, 'accumulated_eval_time': 23432.17221236229, 'accumulated_logging_time': 1.3558223247528076, 'global_step': 96171, 'preemption_count': 0}), (98517, {'train/accuracy': 0.6749527454376221, 'train/loss': 1.4922711849212646, 'train/bleu': 33.76268337396561, 'validation/accuracy': 0.6880509853363037, 'validation/loss': 1.4029872417449951, 'validation/bleu': 30.562362559763177, 'validation/num_examples': 3000, 'test/accuracy': 0.7058160901069641, 'test/loss': 1.301902413368225, 'test/bleu': 30.582308832194634, 'test/num_examples': 3003, 'score': 35315.19370055199, 'total_duration': 59286.53086400032, 'accumulated_submission_time': 35315.19370055199, 'accumulated_eval_time': 23966.787534713745, 'accumulated_logging_time': 1.394505500793457, 'global_step': 98517, 'preemption_count': 0}), (100862, {'train/accuracy': 0.6901666522026062, 'train/loss': 1.3958227634429932, 'train/bleu': 34.54494358756864, 'validation/accuracy': 0.6898488402366638, 'validation/loss': 1.3990224599838257, 'validation/bleu': 30.523842063106216, 'validation/num_examples': 3000, 'test/accuracy': 0.7054442167282104, 'test/loss': 1.299425721168518, 'test/bleu': 30.249200470461076, 'test/num_examples': 3003, 'score': 36155.15829825401, 'total_duration': 60668.71771264076, 'accumulated_submission_time': 36155.15829825401, 'accumulated_eval_time': 24508.896410226822, 'accumulated_logging_time': 1.4340672492980957, 'global_step': 100862, 'preemption_count': 0}), (103207, {'train/accuracy': 0.6826054453849792, 'train/loss': 1.445312738418579, 'train/bleu': 34.78132076309567, 'validation/accuracy': 0.6900224089622498, 'validation/loss': 1.391385555267334, 'validation/bleu': 30.546865210499295, 'validation/num_examples': 3000, 'test/accuracy': 0.708686351776123, 'test/loss': 1.2871750593185425, 'test/bleu': 30.83900984318424, 'test/num_examples': 3003, 'score': 36995.03522968292, 'total_duration': 62076.9599506855, 'accumulated_submission_time': 36995.03522968292, 'accumulated_eval_time': 25077.136883974075, 'accumulated_logging_time': 1.4827287197113037, 'global_step': 103207, 'preemption_count': 0}), (105552, {'train/accuracy': 0.6831424832344055, 'train/loss': 1.4466559886932373, 'train/bleu': 34.30571893606282, 'validation/accuracy': 0.6902456283569336, 'validation/loss': 1.3884084224700928, 'validation/bleu': 30.71525097595463, 'validation/num_examples': 3000, 'test/accuracy': 0.706187903881073, 'test/loss': 1.2858394384384155, 'test/bleu': 30.671070000288662, 'test/num_examples': 3003, 'score': 37835.16900038719, 'total_duration': 63494.38958978653, 'accumulated_submission_time': 37835.16900038719, 'accumulated_eval_time': 25654.313775777817, 'accumulated_logging_time': 1.522960901260376, 'global_step': 105552, 'preemption_count': 0}), (107899, {'train/accuracy': 0.6916837692260742, 'train/loss': 1.393021583557129, 'train/bleu': 35.40145042693628, 'validation/accuracy': 0.691299557685852, 'validation/loss': 1.383009910583496, 'validation/bleu': 30.558788910901555, 'validation/num_examples': 3000, 'test/accuracy': 0.7079077363014221, 'test/loss': 1.2825238704681396, 'test/bleu': 30.42644247752539, 'test/num_examples': 3003, 'score': 38675.34786558151, 'total_duration': 64931.23558783531, 'accumulated_submission_time': 38675.34786558151, 'accumulated_eval_time': 26250.866422891617, 'accumulated_logging_time': 1.5631628036499023, 'global_step': 107899, 'preemption_count': 0}), (110245, {'train/accuracy': 0.6882879137992859, 'train/loss': 1.4097886085510254, 'train/bleu': 35.19981584636805, 'validation/accuracy': 0.6923038959503174, 'validation/loss': 1.3791635036468506, 'validation/bleu': 30.563474260944655, 'validation/num_examples': 3000, 'test/accuracy': 0.7103015780448914, 'test/loss': 1.2744451761245728, 'test/bleu': 30.49992268140961, 'test/num_examples': 3003, 'score': 39515.29098367691, 'total_duration': 66392.81823420525, 'accumulated_submission_time': 39515.29098367691, 'accumulated_eval_time': 26872.388967752457, 'accumulated_logging_time': 1.603606939315796, 'global_step': 110245, 'preemption_count': 0}), (112591, {'train/accuracy': 0.6941087245941162, 'train/loss': 1.3747334480285645, 'train/bleu': 35.41065961993999, 'validation/accuracy': 0.6923410892486572, 'validation/loss': 1.3770735263824463, 'validation/bleu': 30.704859704017625, 'validation/num_examples': 3000, 'test/accuracy': 0.7099180817604065, 'test/loss': 1.2725780010223389, 'test/bleu': 30.61117920436823, 'test/num_examples': 3003, 'score': 40355.34690570831, 'total_duration': 67835.56606578827, 'accumulated_submission_time': 40355.34690570831, 'accumulated_eval_time': 27474.955493688583, 'accumulated_logging_time': 1.654353380203247, 'global_step': 112591, 'preemption_count': 0}), (114937, {'train/accuracy': 0.6922072172164917, 'train/loss': 1.393733263015747, 'train/bleu': 35.2817522823305, 'validation/accuracy': 0.693717360496521, 'validation/loss': 1.3703649044036865, 'validation/bleu': 30.42187428646859, 'validation/num_examples': 3000, 'test/accuracy': 0.7107896208763123, 'test/loss': 1.2660441398620605, 'test/bleu': 30.542745113871938, 'test/num_examples': 3003, 'score': 41195.55699467659, 'total_duration': 69419.99466729164, 'accumulated_submission_time': 41195.55699467659, 'accumulated_eval_time': 28219.049389600754, 'accumulated_logging_time': 1.7018797397613525, 'global_step': 114937, 'preemption_count': 0}), (117283, {'train/accuracy': 0.690304696559906, 'train/loss': 1.3954484462738037, 'train/bleu': 35.20293102881795, 'validation/accuracy': 0.69430011510849, 'validation/loss': 1.3707165718078613, 'validation/bleu': 30.820244597158045, 'validation/num_examples': 3000, 'test/accuracy': 0.7114868760108948, 'test/loss': 1.2633055448532104, 'test/bleu': 30.761119658345095, 'test/num_examples': 3003, 'score': 42035.75880694389, 'total_duration': 70851.79516196251, 'accumulated_submission_time': 42035.75880694389, 'accumulated_eval_time': 28810.52793598175, 'accumulated_logging_time': 1.7455673217773438, 'global_step': 117283, 'preemption_count': 0}), (119629, {'train/accuracy': 0.6958056688308716, 'train/loss': 1.3685253858566284, 'train/bleu': 35.7102083223731, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.36842942237854, 'validation/bleu': 30.65897634626175, 'validation/num_examples': 3000, 'test/accuracy': 0.7119168043136597, 'test/loss': 1.2605042457580566, 'test/bleu': 30.859965177480586, 'test/num_examples': 3003, 'score': 42875.923796892166, 'total_duration': 72394.39634847641, 'accumulated_submission_time': 42875.923796892166, 'accumulated_eval_time': 29512.847000598907, 'accumulated_logging_time': 1.788480281829834, 'global_step': 119629, 'preemption_count': 0}), (121974, {'train/accuracy': 0.6915479302406311, 'train/loss': 1.388578176498413, 'train/bleu': 35.36310249116494, 'validation/accuracy': 0.6942381262779236, 'validation/loss': 1.3653781414031982, 'validation/bleu': 30.687726654480677, 'validation/num_examples': 3000, 'test/accuracy': 0.7125210762023926, 'test/loss': 1.2577069997787476, 'test/bleu': 30.773637579945547, 'test/num_examples': 3003, 'score': 43716.03023672104, 'total_duration': 73805.94238114357, 'accumulated_submission_time': 43716.03023672104, 'accumulated_eval_time': 30084.165147781372, 'accumulated_logging_time': 1.8323473930358887, 'global_step': 121974, 'preemption_count': 0}), (124318, {'train/accuracy': 0.6958664655685425, 'train/loss': 1.3725371360778809, 'train/bleu': 35.70916601186907, 'validation/accuracy': 0.6946473121643066, 'validation/loss': 1.364732027053833, 'validation/bleu': 30.91054033191716, 'validation/num_examples': 3000, 'test/accuracy': 0.7131369709968567, 'test/loss': 1.2557777166366577, 'test/bleu': 30.845955240528713, 'test/num_examples': 3003, 'score': 44556.06577014923, 'total_duration': 75262.98744797707, 'accumulated_submission_time': 44556.06577014923, 'accumulated_eval_time': 30701.053248643875, 'accumulated_logging_time': 1.8761370182037354, 'global_step': 124318, 'preemption_count': 0}), (126663, {'train/accuracy': 0.6963878273963928, 'train/loss': 1.3677339553833008, 'train/bleu': 35.3685362955417, 'validation/accuracy': 0.6950316429138184, 'validation/loss': 1.3643397092819214, 'validation/bleu': 30.77219468305123, 'validation/num_examples': 3000, 'test/accuracy': 0.7128929495811462, 'test/loss': 1.2559876441955566, 'test/bleu': 30.87501995821473, 'test/num_examples': 3003, 'score': 45396.00702667236, 'total_duration': 76722.7999753952, 'accumulated_submission_time': 45396.00702667236, 'accumulated_eval_time': 31320.80184316635, 'accumulated_logging_time': 1.9225962162017822, 'global_step': 126663, 'preemption_count': 0}), (129010, {'train/accuracy': 0.6947799324989319, 'train/loss': 1.3747010231018066, 'train/bleu': 35.44969392992167, 'validation/accuracy': 0.6953044533729553, 'validation/loss': 1.3621292114257812, 'validation/bleu': 30.786053998723386, 'validation/num_examples': 3000, 'test/accuracy': 0.7133809924125671, 'test/loss': 1.2536063194274902, 'test/bleu': 30.837718893970138, 'test/num_examples': 3003, 'score': 46236.22338271141, 'total_duration': 78223.86001873016, 'accumulated_submission_time': 46236.22338271141, 'accumulated_eval_time': 31981.524652004242, 'accumulated_logging_time': 1.9672255516052246, 'global_step': 129010, 'preemption_count': 0}), (131357, {'train/accuracy': 0.6980651617050171, 'train/loss': 1.3623944520950317, 'train/bleu': 35.745533977811, 'validation/accuracy': 0.6951928734779358, 'validation/loss': 1.3623720407485962, 'validation/bleu': 30.816823338684074, 'validation/num_examples': 3000, 'test/accuracy': 0.7132763862609863, 'test/loss': 1.2537087202072144, 'test/bleu': 30.857126776061257, 'test/num_examples': 3003, 'score': 47076.33667135239, 'total_duration': 79719.39379668236, 'accumulated_submission_time': 47076.33667135239, 'accumulated_eval_time': 32636.826003074646, 'accumulated_logging_time': 2.012143135070801, 'global_step': 131357, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6977786421775818, 'train/loss': 1.358901858329773, 'train/bleu': 35.85408950071874, 'validation/accuracy': 0.6951680779457092, 'validation/loss': 1.3623775243759155, 'validation/bleu': 30.784543671591038, 'validation/num_examples': 3000, 'test/accuracy': 0.7132763862609863, 'test/loss': 1.253868579864502, 'test/bleu': 30.852166892997438, 'test/num_examples': 3003, 'score': 47783.83299946785, 'total_duration': 81060.76957821846, 'accumulated_submission_time': 47783.83299946785, 'accumulated_eval_time': 33270.594187021255, 'accumulated_logging_time': 2.060373544692993, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0211 00:34:03.427576 140446903760704 submission_runner.py:586] Timing: 47783.83299946785
I0211 00:34:03.427639 140446903760704 submission_runner.py:588] Total number of evals: 58
I0211 00:34:03.427690 140446903760704 submission_runner.py:589] ====================
I0211 00:34:03.428579 140446903760704 submission_runner.py:673] Final wmt score: 46665.6048541069
