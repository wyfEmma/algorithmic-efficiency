python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_2 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=3827130657 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_01-26-2024-19-02-57.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0126 19:03:18.905308 140187804313408 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax.
I0126 19:03:19.973264 140187804313408 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0126 19:03:19.974560 140187804313408 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0126 19:03:19.974741 140187804313408 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0126 19:03:19.976011 140187804313408 submission_runner.py:542] Using RNG seed 3827130657
I0126 19:03:21.062430 140187804313408 submission_runner.py:551] --- Tuning run 1/5 ---
I0126 19:03:21.062704 140187804313408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_1.
I0126 19:03:21.063243 140187804313408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_1/hparams.json.
I0126 19:03:21.258507 140187804313408 submission_runner.py:206] Initializing dataset.
I0126 19:03:21.275210 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:03:21.286109 140187804313408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:03:21.678270 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:03:22.816600 140187804313408 submission_runner.py:213] Initializing model.
I0126 19:03:32.620765 140187804313408 submission_runner.py:255] Initializing optimizer.
I0126 19:03:34.315548 140187804313408 submission_runner.py:262] Initializing metrics bundle.
I0126 19:03:34.315733 140187804313408 submission_runner.py:280] Initializing checkpoint and logger.
I0126 19:03:34.316891 140187804313408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0126 19:03:34.317030 140187804313408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0126 19:03:34.623982 140187804313408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0126 19:03:34.907756 140187804313408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_1/flags_0.json.
I0126 19:03:34.917058 140187804313408 submission_runner.py:314] Starting training loop.
I0126 19:04:26.830973 140025784133376 logging_writer.py:48] [0] global_step=0, grad_norm=0.5919651985168457, loss=6.933516979217529
I0126 19:04:26.847105 140187804313408 spec.py:321] Evaluating on the training split.
I0126 19:04:27.773819 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:27.782943 140187804313408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:04:27.863874 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:41.163465 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 19:04:42.534806 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:42.560835 140187804313408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:04:42.623020 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:58.564564 140187804313408 spec.py:349] Evaluating on the test split.
I0126 19:04:59.354250 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0126 19:04:59.359512 140187804313408 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0126 19:04:59.399051 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0126 19:05:03.735063 140187804313408 submission_runner.py:408] Time since start: 88.82s, 	Step: 1, 	{'train/accuracy': 0.0010363520123064518, 'train/loss': 6.91261625289917, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 51.929956912994385, 'total_duration': 88.81795763969421, 'accumulated_submission_time': 51.929956912994385, 'accumulated_eval_time': 36.88790965080261, 'accumulated_logging_time': 0}
I0126 19:05:03.753075 140003686463232 logging_writer.py:48] [1] accumulated_eval_time=36.887910, accumulated_logging_time=0, accumulated_submission_time=51.929957, global_step=1, preemption_count=0, score=51.929957, test/accuracy=0.000600, test/loss=6.912549, test/num_examples=10000, total_duration=88.817958, train/accuracy=0.001036, train/loss=6.912616, validation/accuracy=0.000760, validation/loss=6.913175, validation/num_examples=50000
I0126 19:05:37.644290 140003678070528 logging_writer.py:48] [100] global_step=100, grad_norm=0.5884626507759094, loss=6.899717330932617
I0126 19:06:11.646423 140003686463232 logging_writer.py:48] [200] global_step=200, grad_norm=0.5819929242134094, loss=6.867395877838135
I0126 19:06:45.725513 140003678070528 logging_writer.py:48] [300] global_step=300, grad_norm=0.62214595079422, loss=6.8104376792907715
I0126 19:07:19.827893 140003686463232 logging_writer.py:48] [400] global_step=400, grad_norm=0.6753132939338684, loss=6.713794708251953
I0126 19:07:53.920746 140003678070528 logging_writer.py:48] [500] global_step=500, grad_norm=0.7096460461616516, loss=6.602458953857422
I0126 19:08:28.035246 140003686463232 logging_writer.py:48] [600] global_step=600, grad_norm=0.7531196475028992, loss=6.518093109130859
I0126 19:09:02.189874 140003678070528 logging_writer.py:48] [700] global_step=700, grad_norm=0.7649732232093811, loss=6.465991020202637
I0126 19:09:36.324966 140003686463232 logging_writer.py:48] [800] global_step=800, grad_norm=0.8881122469902039, loss=6.391731262207031
I0126 19:10:10.455856 140003678070528 logging_writer.py:48] [900] global_step=900, grad_norm=1.9567850828170776, loss=6.2281646728515625
I0126 19:10:44.563760 140003686463232 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.7885996103286743, loss=6.170497894287109
I0126 19:11:18.660850 140003678070528 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4164201021194458, loss=6.028444290161133
I0126 19:11:52.791105 140003686463232 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.6268216371536255, loss=6.000195503234863
I0126 19:12:26.929373 140003678070528 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.740270972251892, loss=5.934002876281738
I0126 19:13:01.068241 140003686463232 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7124648094177246, loss=5.83317232131958
I0126 19:13:33.923912 140187804313408 spec.py:321] Evaluating on the training split.
I0126 19:13:41.446313 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 19:13:49.705294 140187804313408 spec.py:349] Evaluating on the test split.
I0126 19:13:52.040627 140187804313408 submission_runner.py:408] Time since start: 617.12s, 	Step: 1498, 	{'train/accuracy': 0.07172751426696777, 'train/loss': 5.377313137054443, 'validation/accuracy': 0.0648999959230423, 'validation/loss': 5.448593616485596, 'validation/num_examples': 50000, 'test/accuracy': 0.04650000110268593, 'test/loss': 5.657770156860352, 'test/num_examples': 10000, 'score': 562.0372688770294, 'total_duration': 617.1234951019287, 'accumulated_submission_time': 562.0372688770294, 'accumulated_eval_time': 55.00457406044006, 'accumulated_logging_time': 0.02753734588623047}
I0126 19:13:52.057201 140003694855936 logging_writer.py:48] [1498] accumulated_eval_time=55.004574, accumulated_logging_time=0.027537, accumulated_submission_time=562.037269, global_step=1498, preemption_count=0, score=562.037269, test/accuracy=0.046500, test/loss=5.657770, test/num_examples=10000, total_duration=617.123495, train/accuracy=0.071728, train/loss=5.377313, validation/accuracy=0.064900, validation/loss=5.448594, validation/num_examples=50000
I0126 19:13:53.109917 140003703248640 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.861758232116699, loss=5.851533889770508
I0126 19:14:27.194623 140003694855936 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.073251724243164, loss=5.688718318939209
I0126 19:15:01.311756 140003703248640 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.3801002502441406, loss=5.639028072357178
I0126 19:15:35.467067 140003694855936 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.4518070220947266, loss=5.582479476928711
I0126 19:16:09.589154 140003703248640 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.528557777404785, loss=5.586120128631592
I0126 19:16:43.739299 140003694855936 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.3546085357666016, loss=5.545998573303223
I0126 19:17:17.858666 140003703248640 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.9188644886016846, loss=5.489565372467041
I0126 19:17:51.951225 140003694855936 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.7264976501464844, loss=5.410717964172363
I0126 19:18:26.075996 140003703248640 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.16541862487793, loss=5.329685688018799
I0126 19:19:00.195641 140003694855936 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.224274158477783, loss=5.410288333892822
I0126 19:19:34.310786 140003703248640 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.362463474273682, loss=5.2928547859191895
I0126 19:20:08.424287 140003694855936 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.732656240463257, loss=5.20192813873291
I0126 19:20:42.553178 140003703248640 logging_writer.py:48] [2700] global_step=2700, grad_norm=6.936979293823242, loss=5.240112781524658
I0126 19:21:16.695940 140003694855936 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.375875473022461, loss=5.130969047546387
I0126 19:21:50.870502 140003703248640 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.3831915855407715, loss=5.082301616668701
I0126 19:22:22.355959 140187804313408 spec.py:321] Evaluating on the training split.
I0126 19:22:29.610938 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 19:22:38.128886 140187804313408 spec.py:349] Evaluating on the test split.
I0126 19:22:40.767999 140187804313408 submission_runner.py:408] Time since start: 1145.85s, 	Step: 2994, 	{'train/accuracy': 0.17580117285251617, 'train/loss': 4.277118682861328, 'validation/accuracy': 0.15845999121665955, 'validation/loss': 4.395031929016113, 'validation/num_examples': 50000, 'test/accuracy': 0.11050000786781311, 'test/loss': 4.828097820281982, 'test/num_examples': 10000, 'score': 1072.2711553573608, 'total_duration': 1145.8508818149567, 'accumulated_submission_time': 1072.2711553573608, 'accumulated_eval_time': 73.41657638549805, 'accumulated_logging_time': 0.05562090873718262}
I0126 19:22:40.784026 140026151130880 logging_writer.py:48] [2994] accumulated_eval_time=73.416576, accumulated_logging_time=0.055621, accumulated_submission_time=1072.271155, global_step=2994, preemption_count=0, score=1072.271155, test/accuracy=0.110500, test/loss=4.828098, test/num_examples=10000, total_duration=1145.850882, train/accuracy=0.175801, train/loss=4.277119, validation/accuracy=0.158460, validation/loss=4.395032, validation/num_examples=50000
I0126 19:22:43.185789 140026159523584 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.677891254425049, loss=5.00310754776001
I0126 19:23:17.271528 140026151130880 logging_writer.py:48] [3100] global_step=3100, grad_norm=6.679555892944336, loss=5.01997709274292
I0126 19:23:51.359967 140026159523584 logging_writer.py:48] [3200] global_step=3200, grad_norm=6.225565433502197, loss=5.013828277587891
I0126 19:24:25.517969 140026151130880 logging_writer.py:48] [3300] global_step=3300, grad_norm=5.755523681640625, loss=4.918829441070557
I0126 19:24:59.639415 140026159523584 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.021462917327881, loss=4.901810169219971
I0126 19:25:33.747042 140026151130880 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.178967475891113, loss=4.856604099273682
I0126 19:26:07.875061 140026159523584 logging_writer.py:48] [3600] global_step=3600, grad_norm=5.7418742179870605, loss=4.824374198913574
I0126 19:26:42.042659 140026151130880 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.229619026184082, loss=4.742892265319824
I0126 19:27:16.186918 140026159523584 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.022139549255371, loss=4.645009517669678
I0126 19:27:50.362197 140026151130880 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.898063659667969, loss=4.633932590484619
I0126 19:28:24.491064 140026159523584 logging_writer.py:48] [4000] global_step=4000, grad_norm=6.8403120040893555, loss=4.669504165649414
I0126 19:28:58.636104 140026151130880 logging_writer.py:48] [4100] global_step=4100, grad_norm=5.319889545440674, loss=4.647063255310059
I0126 19:29:32.782826 140026159523584 logging_writer.py:48] [4200] global_step=4200, grad_norm=7.880003929138184, loss=4.608963966369629
I0126 19:30:06.936161 140026151130880 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.9618961811065674, loss=4.474211692810059
I0126 19:30:41.037502 140026159523584 logging_writer.py:48] [4400] global_step=4400, grad_norm=7.255220890045166, loss=4.460317134857178
I0126 19:31:10.811909 140187804313408 spec.py:321] Evaluating on the training split.
I0126 19:31:18.034233 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 19:31:26.683100 140187804313408 spec.py:349] Evaluating on the test split.
I0126 19:31:28.984435 140187804313408 submission_runner.py:408] Time since start: 1674.07s, 	Step: 4489, 	{'train/accuracy': 0.27919721603393555, 'train/loss': 3.5287153720855713, 'validation/accuracy': 0.2509799897670746, 'validation/loss': 3.671311378479004, 'validation/num_examples': 50000, 'test/accuracy': 0.18370001018047333, 'test/loss': 4.219701766967773, 'test/num_examples': 10000, 'score': 1582.2347013950348, 'total_duration': 1674.067317724228, 'accumulated_submission_time': 1582.2347013950348, 'accumulated_eval_time': 91.58906817436218, 'accumulated_logging_time': 0.08123278617858887}
I0126 19:31:29.007074 140026042091264 logging_writer.py:48] [4489] accumulated_eval_time=91.589068, accumulated_logging_time=0.081233, accumulated_submission_time=1582.234701, global_step=4489, preemption_count=0, score=1582.234701, test/accuracy=0.183700, test/loss=4.219702, test/num_examples=10000, total_duration=1674.067318, train/accuracy=0.279197, train/loss=3.528715, validation/accuracy=0.250980, validation/loss=3.671311, validation/num_examples=50000
I0126 19:31:33.104648 140026050483968 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.725560665130615, loss=4.4413299560546875
I0126 19:32:07.207279 140026042091264 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.43543815612793, loss=4.386270046234131
I0126 19:32:41.305464 140026050483968 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.833035945892334, loss=4.394768714904785
I0126 19:33:15.440770 140026042091264 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.362995147705078, loss=4.349085807800293
I0126 19:33:49.573194 140026050483968 logging_writer.py:48] [4900] global_step=4900, grad_norm=5.903903007507324, loss=4.302934646606445
I0126 19:34:23.698948 140026042091264 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.7214813232421875, loss=4.168231964111328
I0126 19:34:57.813600 140026050483968 logging_writer.py:48] [5100] global_step=5100, grad_norm=7.8385114669799805, loss=4.21583366394043
I0126 19:35:31.938042 140026042091264 logging_writer.py:48] [5200] global_step=5200, grad_norm=6.53201961517334, loss=4.17575740814209
I0126 19:36:06.061106 140026050483968 logging_writer.py:48] [5300] global_step=5300, grad_norm=6.413189888000488, loss=4.324619293212891
I0126 19:36:40.157418 140026042091264 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.210958003997803, loss=4.09226655960083
I0126 19:37:14.304581 140026050483968 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.6356682777404785, loss=4.151790142059326
I0126 19:37:48.409856 140026042091264 logging_writer.py:48] [5600] global_step=5600, grad_norm=6.963917255401611, loss=4.114117622375488
I0126 19:38:22.515616 140026050483968 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.6448309421539307, loss=4.011347770690918
I0126 19:38:56.636803 140026042091264 logging_writer.py:48] [5800] global_step=5800, grad_norm=7.577235221862793, loss=4.109172821044922
I0126 19:39:30.739092 140026050483968 logging_writer.py:48] [5900] global_step=5900, grad_norm=6.230823040008545, loss=4.142294883728027
I0126 19:39:59.148323 140187804313408 spec.py:321] Evaluating on the training split.
I0126 19:40:06.643279 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 19:40:14.952139 140187804313408 spec.py:349] Evaluating on the test split.
I0126 19:40:17.264210 140187804313408 submission_runner.py:408] Time since start: 2202.35s, 	Step: 5985, 	{'train/accuracy': 0.36918047070503235, 'train/loss': 2.9538686275482178, 'validation/accuracy': 0.34627997875213623, 'validation/loss': 3.0891966819763184, 'validation/num_examples': 50000, 'test/accuracy': 0.26190000772476196, 'test/loss': 3.715914726257324, 'test/num_examples': 10000, 'score': 2092.3124701976776, 'total_duration': 2202.3470935821533, 'accumulated_submission_time': 2092.3124701976776, 'accumulated_eval_time': 109.70492148399353, 'accumulated_logging_time': 0.11393284797668457}
I0126 19:40:17.281462 140026151130880 logging_writer.py:48] [5985] accumulated_eval_time=109.704921, accumulated_logging_time=0.113933, accumulated_submission_time=2092.312470, global_step=5985, preemption_count=0, score=2092.312470, test/accuracy=0.261900, test/loss=3.715915, test/num_examples=10000, total_duration=2202.347094, train/accuracy=0.369180, train/loss=2.953869, validation/accuracy=0.346280, validation/loss=3.089197, validation/num_examples=50000
I0126 19:40:22.746228 140026159523584 logging_writer.py:48] [6000] global_step=6000, grad_norm=5.417344093322754, loss=4.0156121253967285
I0126 19:40:56.813618 140026151130880 logging_writer.py:48] [6100] global_step=6100, grad_norm=5.071869850158691, loss=4.058651924133301
I0126 19:41:30.968380 140026159523584 logging_writer.py:48] [6200] global_step=6200, grad_norm=5.255810737609863, loss=3.9050419330596924
I0126 19:42:05.101075 140026151130880 logging_writer.py:48] [6300] global_step=6300, grad_norm=8.691950798034668, loss=3.941265344619751
I0126 19:42:39.212861 140026159523584 logging_writer.py:48] [6400] global_step=6400, grad_norm=6.652378082275391, loss=3.901573419570923
I0126 19:43:13.331742 140026151130880 logging_writer.py:48] [6500] global_step=6500, grad_norm=6.2290263175964355, loss=3.811035394668579
I0126 19:43:47.470610 140026159523584 logging_writer.py:48] [6600] global_step=6600, grad_norm=8.65705680847168, loss=3.9495301246643066
I0126 19:44:21.565626 140026151130880 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.934433460235596, loss=3.7452871799468994
I0126 19:44:55.695145 140026159523584 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.392856597900391, loss=3.8337979316711426
I0126 19:45:29.844271 140026151130880 logging_writer.py:48] [6900] global_step=6900, grad_norm=5.986684799194336, loss=3.6988918781280518
I0126 19:46:03.951100 140026159523584 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.542160987854004, loss=3.681399345397949
I0126 19:46:38.064870 140026151130880 logging_writer.py:48] [7100] global_step=7100, grad_norm=5.997280120849609, loss=3.7386250495910645
I0126 19:47:12.169874 140026159523584 logging_writer.py:48] [7200] global_step=7200, grad_norm=6.130944728851318, loss=3.8040220737457275
I0126 19:47:46.307906 140026151130880 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.6314871311187744, loss=3.668544292449951
I0126 19:48:20.401630 140026159523584 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.405890941619873, loss=3.6511034965515137
I0126 19:48:47.429941 140187804313408 spec.py:321] Evaluating on the training split.
I0126 19:48:54.699670 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 19:49:03.281636 140187804313408 spec.py:349] Evaluating on the test split.
I0126 19:49:05.577492 140187804313408 submission_runner.py:408] Time since start: 2730.66s, 	Step: 7481, 	{'train/accuracy': 0.4307437837123871, 'train/loss': 2.613607168197632, 'validation/accuracy': 0.3971000015735626, 'validation/loss': 2.790703296661377, 'validation/num_examples': 50000, 'test/accuracy': 0.2989000082015991, 'test/loss': 3.4827613830566406, 'test/num_examples': 10000, 'score': 2602.396719932556, 'total_duration': 2730.6603696346283, 'accumulated_submission_time': 2602.396719932556, 'accumulated_eval_time': 127.85242891311646, 'accumulated_logging_time': 0.14101171493530273}
I0126 19:49:05.597543 140026050483968 logging_writer.py:48] [7481] accumulated_eval_time=127.852429, accumulated_logging_time=0.141012, accumulated_submission_time=2602.396720, global_step=7481, preemption_count=0, score=2602.396720, test/accuracy=0.298900, test/loss=3.482761, test/num_examples=10000, total_duration=2730.660370, train/accuracy=0.430744, train/loss=2.613607, validation/accuracy=0.397100, validation/loss=2.790703, validation/num_examples=50000
I0126 19:49:12.442076 140026058876672 logging_writer.py:48] [7500] global_step=7500, grad_norm=6.075757026672363, loss=3.6120073795318604
I0126 19:49:46.493227 140026050483968 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.704527378082275, loss=3.6343047618865967
I0126 19:50:20.591385 140026058876672 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.462488174438477, loss=3.7985146045684814
I0126 19:50:54.702038 140026050483968 logging_writer.py:48] [7800] global_step=7800, grad_norm=5.880280017852783, loss=3.594994306564331
I0126 19:51:28.820883 140026058876672 logging_writer.py:48] [7900] global_step=7900, grad_norm=7.005244731903076, loss=3.716761589050293
I0126 19:52:02.918095 140026050483968 logging_writer.py:48] [8000] global_step=8000, grad_norm=4.650908470153809, loss=3.567553997039795
I0126 19:52:37.024884 140026058876672 logging_writer.py:48] [8100] global_step=8100, grad_norm=7.040465354919434, loss=3.5626471042633057
I0126 19:53:11.102877 140026050483968 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.488724708557129, loss=3.4963278770446777
I0126 19:53:45.232243 140026058876672 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.1811842918396, loss=3.6001150608062744
I0126 19:54:19.342530 140026050483968 logging_writer.py:48] [8400] global_step=8400, grad_norm=6.457899570465088, loss=3.4788055419921875
I0126 19:54:53.431809 140026058876672 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.335597991943359, loss=3.466364860534668
I0126 19:55:27.515324 140026050483968 logging_writer.py:48] [8600] global_step=8600, grad_norm=7.735893726348877, loss=3.574483871459961
I0126 19:56:01.607439 140026058876672 logging_writer.py:48] [8700] global_step=8700, grad_norm=6.286797523498535, loss=3.521584987640381
I0126 19:56:35.719715 140026050483968 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.466212749481201, loss=3.47537899017334
I0126 19:57:09.806254 140026058876672 logging_writer.py:48] [8900] global_step=8900, grad_norm=5.391483306884766, loss=3.4652369022369385
I0126 19:57:35.839687 140187804313408 spec.py:321] Evaluating on the training split.
I0126 19:57:43.337936 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 19:57:51.813339 140187804313408 spec.py:349] Evaluating on the test split.
I0126 19:57:54.115670 140187804313408 submission_runner.py:408] Time since start: 3259.20s, 	Step: 8978, 	{'train/accuracy': 0.49228712916374207, 'train/loss': 2.275744676589966, 'validation/accuracy': 0.4618600010871887, 'validation/loss': 2.433438301086426, 'validation/num_examples': 50000, 'test/accuracy': 0.357200026512146, 'test/loss': 3.100750684738159, 'test/num_examples': 10000, 'score': 3112.5727632045746, 'total_duration': 3259.198537349701, 'accumulated_submission_time': 3112.5727632045746, 'accumulated_eval_time': 146.1283574104309, 'accumulated_logging_time': 0.17496347427368164}
I0126 19:57:54.132994 140026042091264 logging_writer.py:48] [8978] accumulated_eval_time=146.128357, accumulated_logging_time=0.174963, accumulated_submission_time=3112.572763, global_step=8978, preemption_count=0, score=3112.572763, test/accuracy=0.357200, test/loss=3.100751, test/num_examples=10000, total_duration=3259.198537, train/accuracy=0.492287, train/loss=2.275745, validation/accuracy=0.461860, validation/loss=2.433438, validation/num_examples=50000
I0126 19:58:01.988736 140026050483968 logging_writer.py:48] [9000] global_step=9000, grad_norm=6.047358512878418, loss=3.429074287414551
I0126 19:58:36.036131 140026042091264 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.06921124458313, loss=3.4291653633117676
I0126 19:59:10.096402 140026050483968 logging_writer.py:48] [9200] global_step=9200, grad_norm=6.119688987731934, loss=3.5042781829833984
I0126 19:59:44.205658 140026042091264 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.946896076202393, loss=3.3898158073425293
I0126 20:00:18.327498 140026050483968 logging_writer.py:48] [9400] global_step=9400, grad_norm=8.532670021057129, loss=3.3934431076049805
I0126 20:00:52.421507 140026042091264 logging_writer.py:48] [9500] global_step=9500, grad_norm=4.261840343475342, loss=3.4348106384277344
I0126 20:01:26.535347 140026050483968 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.802608966827393, loss=3.298281192779541
I0126 20:02:00.628377 140026042091264 logging_writer.py:48] [9700] global_step=9700, grad_norm=6.1879191398620605, loss=3.4674503803253174
I0126 20:02:34.708328 140026050483968 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.279059410095215, loss=3.3631303310394287
I0126 20:03:08.811486 140026042091264 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.996440887451172, loss=3.367342472076416
I0126 20:03:42.910235 140026050483968 logging_writer.py:48] [10000] global_step=10000, grad_norm=6.40277624130249, loss=3.272825002670288
I0126 20:04:16.980944 140026042091264 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.7889580726623535, loss=3.5001766681671143
I0126 20:04:51.092004 140026050483968 logging_writer.py:48] [10200] global_step=10200, grad_norm=6.288000106811523, loss=3.463392972946167
I0126 20:05:25.148429 140026042091264 logging_writer.py:48] [10300] global_step=10300, grad_norm=4.688681125640869, loss=3.261662006378174
I0126 20:05:59.227583 140026050483968 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.9352495670318604, loss=3.3239009380340576
I0126 20:06:24.251626 140187804313408 spec.py:321] Evaluating on the training split.
I0126 20:06:31.692947 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 20:06:40.385587 140187804313408 spec.py:349] Evaluating on the test split.
I0126 20:06:42.689244 140187804313408 submission_runner.py:408] Time since start: 3787.77s, 	Step: 10475, 	{'train/accuracy': 0.5693957209587097, 'train/loss': 1.8849722146987915, 'validation/accuracy': 0.5029999613761902, 'validation/loss': 2.2190101146698, 'validation/num_examples': 50000, 'test/accuracy': 0.39510002732276917, 'test/loss': 2.8704304695129395, 'test/num_examples': 10000, 'score': 3622.6284663677216, 'total_duration': 3787.772131204605, 'accumulated_submission_time': 3622.6284663677216, 'accumulated_eval_time': 164.56593775749207, 'accumulated_logging_time': 0.20221304893493652}
I0126 20:06:42.714560 140026159523584 logging_writer.py:48] [10475] accumulated_eval_time=164.565938, accumulated_logging_time=0.202213, accumulated_submission_time=3622.628466, global_step=10475, preemption_count=0, score=3622.628466, test/accuracy=0.395100, test/loss=2.870430, test/num_examples=10000, total_duration=3787.772131, train/accuracy=0.569396, train/loss=1.884972, validation/accuracy=0.503000, validation/loss=2.219010, validation/num_examples=50000
I0126 20:06:51.556184 140026167916288 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.338120460510254, loss=3.253288745880127
I0126 20:07:25.576080 140026159523584 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.510358810424805, loss=3.1702914237976074
I0126 20:07:59.634980 140026167916288 logging_writer.py:48] [10700] global_step=10700, grad_norm=6.38612174987793, loss=3.3298463821411133
I0126 20:08:33.700156 140026159523584 logging_writer.py:48] [10800] global_step=10800, grad_norm=4.219602108001709, loss=3.278679370880127
I0126 20:09:07.775360 140026167916288 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.967636823654175, loss=3.3029706478118896
I0126 20:09:41.848347 140026159523584 logging_writer.py:48] [11000] global_step=11000, grad_norm=5.317691326141357, loss=3.3002517223358154
I0126 20:10:15.913940 140026167916288 logging_writer.py:48] [11100] global_step=11100, grad_norm=6.987340450286865, loss=3.176374673843384
I0126 20:10:49.983000 140026159523584 logging_writer.py:48] [11200] global_step=11200, grad_norm=6.789555072784424, loss=3.199215888977051
I0126 20:11:24.062845 140026167916288 logging_writer.py:48] [11300] global_step=11300, grad_norm=6.940995216369629, loss=3.2305309772491455
I0126 20:11:58.085617 140026159523584 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.4795002937316895, loss=3.134665012359619
I0126 20:12:32.164150 140026167916288 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.8769712448120117, loss=3.1810741424560547
I0126 20:13:06.245335 140026159523584 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.529537200927734, loss=3.1175365447998047
I0126 20:13:40.282432 140026167916288 logging_writer.py:48] [11700] global_step=11700, grad_norm=5.6142497062683105, loss=3.1971139907836914
I0126 20:14:14.351835 140026159523584 logging_writer.py:48] [11800] global_step=11800, grad_norm=6.106864929199219, loss=3.1820449829101562
I0126 20:14:48.409581 140026167916288 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.553673267364502, loss=3.173311471939087
I0126 20:15:12.998845 140187804313408 spec.py:321] Evaluating on the training split.
I0126 20:15:20.360630 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 20:15:28.955398 140187804313408 spec.py:349] Evaluating on the test split.
I0126 20:15:31.265181 140187804313408 submission_runner.py:408] Time since start: 4316.35s, 	Step: 11974, 	{'train/accuracy': 0.5763313174247742, 'train/loss': 1.9071600437164307, 'validation/accuracy': 0.5275999903678894, 'validation/loss': 2.1289572715759277, 'validation/num_examples': 50000, 'test/accuracy': 0.4093000292778015, 'test/loss': 2.793846368789673, 'test/num_examples': 10000, 'score': 4132.8440663814545, 'total_duration': 4316.348064184189, 'accumulated_submission_time': 4132.8440663814545, 'accumulated_eval_time': 182.83223509788513, 'accumulated_logging_time': 0.2424328327178955}
I0126 20:15:31.288379 140026042091264 logging_writer.py:48] [11974] accumulated_eval_time=182.832235, accumulated_logging_time=0.242433, accumulated_submission_time=4132.844066, global_step=11974, preemption_count=0, score=4132.844066, test/accuracy=0.409300, test/loss=2.793846, test/num_examples=10000, total_duration=4316.348064, train/accuracy=0.576331, train/loss=1.907160, validation/accuracy=0.527600, validation/loss=2.128957, validation/num_examples=50000
I0126 20:15:40.466733 140026050483968 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.312479019165039, loss=3.24310564994812
I0126 20:16:14.485546 140026042091264 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.910430431365967, loss=3.1286468505859375
I0126 20:16:48.543106 140026050483968 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.610931873321533, loss=3.2323994636535645
I0126 20:17:22.600095 140026042091264 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.540145397186279, loss=3.1670477390289307
I0126 20:17:56.677752 140026050483968 logging_writer.py:48] [12400] global_step=12400, grad_norm=7.48927116394043, loss=3.0768744945526123
I0126 20:18:30.745042 140026042091264 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.352994441986084, loss=3.1421215534210205
I0126 20:19:04.939857 140026050483968 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.770852565765381, loss=3.17075252532959
I0126 20:19:38.971654 140026042091264 logging_writer.py:48] [12700] global_step=12700, grad_norm=5.044092178344727, loss=3.063199281692505
I0126 20:20:13.040366 140026050483968 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.798257350921631, loss=3.18239688873291
I0126 20:20:47.108741 140026042091264 logging_writer.py:48] [12900] global_step=12900, grad_norm=7.1598663330078125, loss=3.1738502979278564
I0126 20:21:21.157582 140026050483968 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.546345233917236, loss=3.055391311645508
I0126 20:21:55.199970 140026042091264 logging_writer.py:48] [13100] global_step=13100, grad_norm=6.084322929382324, loss=3.133787155151367
I0126 20:22:29.275352 140026050483968 logging_writer.py:48] [13200] global_step=13200, grad_norm=6.332658767700195, loss=3.0568318367004395
I0126 20:23:03.301280 140026042091264 logging_writer.py:48] [13300] global_step=13300, grad_norm=5.885891914367676, loss=3.134817600250244
I0126 20:23:37.328216 140026050483968 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.264165878295898, loss=3.133049488067627
I0126 20:24:01.593674 140187804313408 spec.py:321] Evaluating on the training split.
I0126 20:24:09.186231 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 20:24:17.868491 140187804313408 spec.py:349] Evaluating on the test split.
I0126 20:24:20.143217 140187804313408 submission_runner.py:408] Time since start: 4845.23s, 	Step: 13473, 	{'train/accuracy': 0.5816326141357422, 'train/loss': 1.8359532356262207, 'validation/accuracy': 0.535539984703064, 'validation/loss': 2.061918258666992, 'validation/num_examples': 50000, 'test/accuracy': 0.4231000244617462, 'test/loss': 2.7273426055908203, 'test/num_examples': 10000, 'score': 4643.087154865265, 'total_duration': 4845.225155115128, 'accumulated_submission_time': 4643.087154865265, 'accumulated_eval_time': 201.3808000087738, 'accumulated_logging_time': 0.27495527267456055}
I0126 20:24:20.163637 140026042091264 logging_writer.py:48] [13473] accumulated_eval_time=201.380800, accumulated_logging_time=0.274955, accumulated_submission_time=4643.087155, global_step=13473, preemption_count=0, score=4643.087155, test/accuracy=0.423100, test/loss=2.727343, test/num_examples=10000, total_duration=4845.225155, train/accuracy=0.581633, train/loss=1.835953, validation/accuracy=0.535540, validation/loss=2.061918, validation/num_examples=50000
I0126 20:24:29.690452 140026050483968 logging_writer.py:48] [13500] global_step=13500, grad_norm=6.477536678314209, loss=3.076951026916504
I0126 20:25:03.739618 140026042091264 logging_writer.py:48] [13600] global_step=13600, grad_norm=8.468356132507324, loss=3.0745856761932373
I0126 20:25:37.749020 140026050483968 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.689301490783691, loss=3.0862741470336914
I0126 20:26:11.766766 140026042091264 logging_writer.py:48] [13800] global_step=13800, grad_norm=4.309243202209473, loss=3.0836586952209473
I0126 20:26:45.822823 140026050483968 logging_writer.py:48] [13900] global_step=13900, grad_norm=5.265732765197754, loss=3.0579373836517334
I0126 20:27:19.843501 140026042091264 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.137474060058594, loss=3.0738868713378906
I0126 20:27:53.877651 140026050483968 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.292250633239746, loss=3.050060987472534
I0126 20:28:27.897592 140026042091264 logging_writer.py:48] [14200] global_step=14200, grad_norm=5.557734489440918, loss=3.115891456604004
I0126 20:29:01.974538 140026050483968 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.479909896850586, loss=3.022315740585327
I0126 20:29:36.007722 140026042091264 logging_writer.py:48] [14400] global_step=14400, grad_norm=7.7898688316345215, loss=3.13922381401062
I0126 20:30:10.032881 140026050483968 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.776465892791748, loss=3.1531176567077637
I0126 20:30:44.082777 140026042091264 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.303415298461914, loss=3.057133197784424
I0126 20:31:18.116032 140026050483968 logging_writer.py:48] [14700] global_step=14700, grad_norm=5.56661319732666, loss=2.9820520877838135
I0126 20:31:52.129812 140026042091264 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.22451114654541, loss=2.995555877685547
I0126 20:32:26.190303 140026050483968 logging_writer.py:48] [14900] global_step=14900, grad_norm=4.687906742095947, loss=3.0129950046539307
I0126 20:32:50.441074 140187804313408 spec.py:321] Evaluating on the training split.
I0126 20:32:58.476237 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 20:33:07.321734 140187804313408 spec.py:349] Evaluating on the test split.
I0126 20:33:09.611474 140187804313408 submission_runner.py:408] Time since start: 5374.69s, 	Step: 14973, 	{'train/accuracy': 0.5989516973495483, 'train/loss': 1.7546666860580444, 'validation/accuracy': 0.5537399649620056, 'validation/loss': 1.9646070003509521, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.6510255336761475, 'test/num_examples': 10000, 'score': 5153.3032858371735, 'total_duration': 5374.694349765778, 'accumulated_submission_time': 5153.3032858371735, 'accumulated_eval_time': 220.55117321014404, 'accumulated_logging_time': 0.30461955070495605}
I0126 20:33:09.643257 140026042091264 logging_writer.py:48] [14973] accumulated_eval_time=220.551173, accumulated_logging_time=0.304620, accumulated_submission_time=5153.303286, global_step=14973, preemption_count=0, score=5153.303286, test/accuracy=0.431300, test/loss=2.651026, test/num_examples=10000, total_duration=5374.694350, train/accuracy=0.598952, train/loss=1.754667, validation/accuracy=0.553740, validation/loss=1.964607, validation/num_examples=50000
I0126 20:33:19.181593 140026050483968 logging_writer.py:48] [15000] global_step=15000, grad_norm=5.041775703430176, loss=3.1142656803131104
I0126 20:33:53.150123 140026042091264 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.69556999206543, loss=3.0217599868774414
I0126 20:34:27.192643 140026050483968 logging_writer.py:48] [15200] global_step=15200, grad_norm=4.43284273147583, loss=3.007606267929077
I0126 20:35:01.187794 140026042091264 logging_writer.py:48] [15300] global_step=15300, grad_norm=5.656130313873291, loss=3.060645580291748
I0126 20:35:35.204644 140026050483968 logging_writer.py:48] [15400] global_step=15400, grad_norm=5.794323921203613, loss=2.9635186195373535
I0126 20:36:09.228299 140026042091264 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.302029132843018, loss=3.0559604167938232
I0126 20:36:43.249108 140026050483968 logging_writer.py:48] [15600] global_step=15600, grad_norm=5.170292377471924, loss=3.0175223350524902
I0126 20:37:17.310461 140026042091264 logging_writer.py:48] [15700] global_step=15700, grad_norm=6.0849223136901855, loss=3.0207996368408203
I0126 20:37:51.336191 140026050483968 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.535536289215088, loss=3.0397253036499023
I0126 20:38:25.388571 140026042091264 logging_writer.py:48] [15900] global_step=15900, grad_norm=4.3230133056640625, loss=2.9990267753601074
I0126 20:38:59.437475 140026050483968 logging_writer.py:48] [16000] global_step=16000, grad_norm=6.849791526794434, loss=3.0577147006988525
I0126 20:39:33.475125 140026042091264 logging_writer.py:48] [16100] global_step=16100, grad_norm=4.70114803314209, loss=3.034986972808838
I0126 20:40:07.480637 140026050483968 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.710892200469971, loss=3.065101146697998
I0126 20:40:41.496497 140026042091264 logging_writer.py:48] [16300] global_step=16300, grad_norm=6.111588954925537, loss=3.0616674423217773
I0126 20:41:15.527291 140026050483968 logging_writer.py:48] [16400] global_step=16400, grad_norm=5.790919780731201, loss=3.0826478004455566
I0126 20:41:39.767160 140187804313408 spec.py:321] Evaluating on the training split.
I0126 20:41:47.177170 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 20:41:57.734521 140187804313408 spec.py:349] Evaluating on the test split.
I0126 20:41:59.986896 140187804313408 submission_runner.py:408] Time since start: 5905.07s, 	Step: 16473, 	{'train/accuracy': 0.59375, 'train/loss': 1.8033223152160645, 'validation/accuracy': 0.5557399988174438, 'validation/loss': 1.9902366399765015, 'validation/num_examples': 50000, 'test/accuracy': 0.4360000193119049, 'test/loss': 2.6539673805236816, 'test/num_examples': 10000, 'score': 5663.362612962723, 'total_duration': 5905.069729804993, 'accumulated_submission_time': 5663.362612962723, 'accumulated_eval_time': 240.77084040641785, 'accumulated_logging_time': 0.34645605087280273}
I0126 20:42:00.011744 140026159523584 logging_writer.py:48] [16473] accumulated_eval_time=240.770840, accumulated_logging_time=0.346456, accumulated_submission_time=5663.362613, global_step=16473, preemption_count=0, score=5663.362613, test/accuracy=0.436000, test/loss=2.653967, test/num_examples=10000, total_duration=5905.069730, train/accuracy=0.593750, train/loss=1.803322, validation/accuracy=0.555740, validation/loss=1.990237, validation/num_examples=50000
I0126 20:42:09.510343 140026167916288 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.752538204193115, loss=2.9943976402282715
I0126 20:42:43.464927 140026159523584 logging_writer.py:48] [16600] global_step=16600, grad_norm=5.830123424530029, loss=3.025496006011963
I0126 20:43:17.483944 140026167916288 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.454415798187256, loss=2.9928431510925293
I0126 20:43:51.551652 140026159523584 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.685934543609619, loss=3.0117228031158447
I0126 20:44:25.519730 140026167916288 logging_writer.py:48] [16900] global_step=16900, grad_norm=4.500191688537598, loss=2.982905864715576
I0126 20:44:59.561825 140026159523584 logging_writer.py:48] [17000] global_step=17000, grad_norm=5.140146732330322, loss=3.0672600269317627
I0126 20:45:33.577085 140026167916288 logging_writer.py:48] [17100] global_step=17100, grad_norm=4.4022979736328125, loss=3.0495781898498535
I0126 20:46:07.576722 140026159523584 logging_writer.py:48] [17200] global_step=17200, grad_norm=5.197539806365967, loss=2.9541072845458984
I0126 20:46:41.599377 140026167916288 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.120326280593872, loss=2.9368741512298584
I0126 20:47:15.618182 140026159523584 logging_writer.py:48] [17400] global_step=17400, grad_norm=4.2042927742004395, loss=3.120098352432251
I0126 20:47:49.632097 140026167916288 logging_writer.py:48] [17500] global_step=17500, grad_norm=5.783239364624023, loss=2.908508777618408
I0126 20:48:23.651398 140026159523584 logging_writer.py:48] [17600] global_step=17600, grad_norm=6.3360795974731445, loss=2.9443612098693848
I0126 20:48:57.653756 140026167916288 logging_writer.py:48] [17700] global_step=17700, grad_norm=4.589481830596924, loss=2.9321224689483643
I0126 20:49:31.690853 140026159523584 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.504002094268799, loss=2.9891605377197266
I0126 20:50:05.684350 140026167916288 logging_writer.py:48] [17900] global_step=17900, grad_norm=5.538050651550293, loss=2.938774585723877
I0126 20:50:30.242665 140187804313408 spec.py:321] Evaluating on the training split.
I0126 20:50:38.287531 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 20:50:49.340165 140187804313408 spec.py:349] Evaluating on the test split.
I0126 20:50:51.638937 140187804313408 submission_runner.py:408] Time since start: 6436.72s, 	Step: 17974, 	{'train/accuracy': 0.6002271771430969, 'train/loss': 1.7657809257507324, 'validation/accuracy': 0.5660600066184998, 'validation/loss': 1.9302324056625366, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.6314282417297363, 'test/num_examples': 10000, 'score': 6173.529769182205, 'total_duration': 6436.721809148788, 'accumulated_submission_time': 6173.529769182205, 'accumulated_eval_time': 262.16706109046936, 'accumulated_logging_time': 0.3816823959350586}
I0126 20:50:51.671724 140026058876672 logging_writer.py:48] [17974] accumulated_eval_time=262.167061, accumulated_logging_time=0.381682, accumulated_submission_time=6173.529769, global_step=17974, preemption_count=0, score=6173.529769, test/accuracy=0.438800, test/loss=2.631428, test/num_examples=10000, total_duration=6436.721809, train/accuracy=0.600227, train/loss=1.765781, validation/accuracy=0.566060, validation/loss=1.930232, validation/num_examples=50000
I0126 20:51:00.874847 140026067269376 logging_writer.py:48] [18000] global_step=18000, grad_norm=7.06389045715332, loss=2.9806225299835205
I0126 20:51:34.840998 140026058876672 logging_writer.py:48] [18100] global_step=18100, grad_norm=5.424322128295898, loss=3.1007139682769775
I0126 20:52:08.846355 140026067269376 logging_writer.py:48] [18200] global_step=18200, grad_norm=5.153900623321533, loss=3.011902332305908
I0126 20:52:42.837312 140026058876672 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.776179313659668, loss=3.0183935165405273
I0126 20:53:16.848278 140026067269376 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.308542490005493, loss=2.947852611541748
I0126 20:53:50.865514 140026058876672 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.4427263736724854, loss=2.9797120094299316
I0126 20:54:24.869117 140026067269376 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.87288761138916, loss=2.9049668312072754
I0126 20:54:58.901722 140026058876672 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.7539210319519043, loss=3.1367602348327637
I0126 20:55:32.953976 140026067269376 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.992184162139893, loss=2.9422311782836914
I0126 20:56:07.110836 140026058876672 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.311196804046631, loss=2.9560883045196533
I0126 20:56:41.128461 140026067269376 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.95857310295105, loss=2.9567692279815674
I0126 20:57:15.113920 140026058876672 logging_writer.py:48] [19100] global_step=19100, grad_norm=4.823800563812256, loss=2.980787992477417
I0126 20:57:49.132021 140026067269376 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.5978381633758545, loss=2.972179651260376
I0126 20:58:23.123116 140026058876672 logging_writer.py:48] [19300] global_step=19300, grad_norm=4.354915618896484, loss=3.033252477645874
I0126 20:58:57.119000 140026067269376 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.122372150421143, loss=3.026331663131714
I0126 20:59:21.706602 140187804313408 spec.py:321] Evaluating on the training split.
I0126 20:59:30.142936 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 20:59:44.323212 140187804313408 spec.py:349] Evaluating on the test split.
I0126 20:59:46.544861 140187804313408 submission_runner.py:408] Time since start: 6971.63s, 	Step: 19474, 	{'train/accuracy': 0.6611328125, 'train/loss': 1.4854481220245361, 'validation/accuracy': 0.5753799676895142, 'validation/loss': 1.8646032810211182, 'validation/num_examples': 50000, 'test/accuracy': 0.45090001821517944, 'test/loss': 2.5466179847717285, 'test/num_examples': 10000, 'score': 6683.501819372177, 'total_duration': 6971.627715110779, 'accumulated_submission_time': 6683.501819372177, 'accumulated_eval_time': 287.0052680969238, 'accumulated_logging_time': 0.423555850982666}
I0126 20:59:46.576645 140026058876672 logging_writer.py:48] [19474] accumulated_eval_time=287.005268, accumulated_logging_time=0.423556, accumulated_submission_time=6683.501819, global_step=19474, preemption_count=0, score=6683.501819, test/accuracy=0.450900, test/loss=2.546618, test/num_examples=10000, total_duration=6971.627715, train/accuracy=0.661133, train/loss=1.485448, validation/accuracy=0.575380, validation/loss=1.864603, validation/num_examples=50000
I0126 20:59:55.748391 140026159523584 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.759834289550781, loss=3.0838940143585205
I0126 21:00:29.697699 140026058876672 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.5009403228759766, loss=3.0684375762939453
I0126 21:01:03.652288 140026159523584 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.409160614013672, loss=2.9272918701171875
I0126 21:01:37.661645 140026058876672 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.296720027923584, loss=2.877135992050171
I0126 21:02:11.687142 140026159523584 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.321223735809326, loss=2.973360061645508
I0126 21:02:45.705012 140026058876672 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.546351432800293, loss=2.8683786392211914
I0126 21:03:19.721716 140026159523584 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.761502265930176, loss=2.919895648956299
I0126 21:03:53.747090 140026058876672 logging_writer.py:48] [20200] global_step=20200, grad_norm=5.781681060791016, loss=3.0437607765197754
I0126 21:04:27.740170 140026159523584 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.9075217247009277, loss=2.9061992168426514
I0126 21:05:01.742046 140026058876672 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.4821157455444336, loss=2.9209601879119873
I0126 21:05:35.718618 140026159523584 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.116556644439697, loss=2.997696876525879
I0126 21:06:09.707912 140026058876672 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.804197311401367, loss=2.892871856689453
I0126 21:06:43.722399 140026159523584 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.5172195434570312, loss=2.8678784370422363
I0126 21:07:17.718665 140026058876672 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.0347440242767334, loss=2.9054248332977295
I0126 21:07:51.714847 140026159523584 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.5980300903320312, loss=2.8346478939056396
I0126 21:08:16.716784 140187804313408 spec.py:321] Evaluating on the training split.
I0126 21:08:24.329823 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 21:08:37.283892 140187804313408 spec.py:349] Evaluating on the test split.
I0126 21:08:39.564019 140187804313408 submission_runner.py:408] Time since start: 7504.65s, 	Step: 20975, 	{'train/accuracy': 0.6342673897743225, 'train/loss': 1.583891749382019, 'validation/accuracy': 0.5761199593544006, 'validation/loss': 1.8759549856185913, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.530134439468384, 'test/num_examples': 10000, 'score': 7193.579409122467, 'total_duration': 7504.646897554398, 'accumulated_submission_time': 7193.579409122467, 'accumulated_eval_time': 309.8524570465088, 'accumulated_logging_time': 0.46407437324523926}
I0126 21:08:39.583450 140026075662080 logging_writer.py:48] [20975] accumulated_eval_time=309.852457, accumulated_logging_time=0.464074, accumulated_submission_time=7193.579409, global_step=20975, preemption_count=0, score=7193.579409, test/accuracy=0.455400, test/loss=2.530134, test/num_examples=10000, total_duration=7504.646898, train/accuracy=0.634267, train/loss=1.583892, validation/accuracy=0.576120, validation/loss=1.875955, validation/num_examples=50000
I0126 21:08:48.386219 140026151130880 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.74817156791687, loss=3.0300545692443848
I0126 21:09:22.330582 140026075662080 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.732286214828491, loss=2.915809392929077
I0126 21:09:56.319372 140026151130880 logging_writer.py:48] [21200] global_step=21200, grad_norm=4.794797897338867, loss=2.9613840579986572
I0126 21:10:30.295930 140026075662080 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.955122947692871, loss=2.957878589630127
I0126 21:11:04.287246 140026151130880 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.855057954788208, loss=2.9765677452087402
I0126 21:11:38.331104 140026075662080 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.8367438316345215, loss=2.993034601211548
I0126 21:12:12.333748 140026151130880 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.935307741165161, loss=2.9546971321105957
I0126 21:12:46.332459 140026075662080 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.311840534210205, loss=2.8962581157684326
I0126 21:13:20.313314 140026151130880 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.69634747505188, loss=2.8974199295043945
I0126 21:13:54.331065 140026075662080 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.223458766937256, loss=2.907848358154297
I0126 21:14:28.377708 140026151130880 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.204550266265869, loss=2.859945774078369
I0126 21:15:02.362564 140026075662080 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.261802673339844, loss=2.9638795852661133
I0126 21:15:36.355392 140026151130880 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.832975387573242, loss=2.939281702041626
I0126 21:16:10.341725 140026075662080 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.9108340740203857, loss=2.8975043296813965
I0126 21:16:44.350290 140026151130880 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.5362300872802734, loss=2.9626407623291016
I0126 21:17:09.583422 140187804313408 spec.py:321] Evaluating on the training split.
I0126 21:17:17.142498 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 21:17:28.618899 140187804313408 spec.py:349] Evaluating on the test split.
I0126 21:17:30.878936 140187804313408 submission_runner.py:408] Time since start: 8035.96s, 	Step: 22476, 	{'train/accuracy': 0.6265146732330322, 'train/loss': 1.6444129943847656, 'validation/accuracy': 0.5791199803352356, 'validation/loss': 1.8640754222869873, 'validation/num_examples': 50000, 'test/accuracy': 0.45260003209114075, 'test/loss': 2.587834596633911, 'test/num_examples': 10000, 'score': 7703.516690969467, 'total_duration': 8035.961810588837, 'accumulated_submission_time': 7703.516690969467, 'accumulated_eval_time': 331.14792466163635, 'accumulated_logging_time': 0.49283337593078613}
I0126 21:17:30.900133 140026042091264 logging_writer.py:48] [22476] accumulated_eval_time=331.147925, accumulated_logging_time=0.492833, accumulated_submission_time=7703.516691, global_step=22476, preemption_count=0, score=7703.516691, test/accuracy=0.452600, test/loss=2.587835, test/num_examples=10000, total_duration=8035.961811, train/accuracy=0.626515, train/loss=1.644413, validation/accuracy=0.579120, validation/loss=1.864075, validation/num_examples=50000
I0126 21:17:39.387187 140026050483968 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.1034274101257324, loss=2.799154281616211
I0126 21:18:13.301102 140026042091264 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.384188175201416, loss=2.9413177967071533
I0126 21:18:47.283974 140026050483968 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.0463123321533203, loss=2.998666763305664
I0126 21:19:21.271288 140026042091264 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.0338847637176514, loss=2.925930976867676
I0126 21:19:55.267067 140026050483968 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.8024418354034424, loss=2.819706916809082
I0126 21:20:29.272125 140026042091264 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.7274630069732666, loss=2.89068341255188
I0126 21:21:03.269400 140026050483968 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.6499574184417725, loss=2.859642505645752
I0126 21:21:37.258569 140026042091264 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.9500977993011475, loss=2.908761739730835
I0126 21:22:11.282395 140026050483968 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.691087245941162, loss=2.8551828861236572
I0126 21:22:45.255202 140026042091264 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.1852123737335205, loss=2.921757221221924
I0126 21:23:19.254563 140026050483968 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.624263048171997, loss=2.8786046504974365
I0126 21:23:53.232908 140026042091264 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.4833834171295166, loss=2.977908134460449
I0126 21:24:27.246935 140026050483968 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.2924540042877197, loss=2.9514217376708984
I0126 21:25:01.214610 140026042091264 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.6460182666778564, loss=2.932478666305542
I0126 21:25:35.218138 140026050483968 logging_writer.py:48] [23900] global_step=23900, grad_norm=4.014513969421387, loss=2.907074213027954
I0126 21:26:01.192815 140187804313408 spec.py:321] Evaluating on the training split.
I0126 21:26:08.848788 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 21:26:21.812773 140187804313408 spec.py:349] Evaluating on the test split.
I0126 21:26:24.084613 140187804313408 submission_runner.py:408] Time since start: 8569.17s, 	Step: 23978, 	{'train/accuracy': 0.6319355964660645, 'train/loss': 1.6008868217468262, 'validation/accuracy': 0.582260012626648, 'validation/loss': 1.830167293548584, 'validation/num_examples': 50000, 'test/accuracy': 0.4561000168323517, 'test/loss': 2.5494673252105713, 'test/num_examples': 10000, 'score': 8213.743519306183, 'total_duration': 8569.167505025864, 'accumulated_submission_time': 8213.743519306183, 'accumulated_eval_time': 354.0397229194641, 'accumulated_logging_time': 0.5262980461120605}
I0126 21:26:24.105705 140026151130880 logging_writer.py:48] [23978] accumulated_eval_time=354.039723, accumulated_logging_time=0.526298, accumulated_submission_time=8213.743519, global_step=23978, preemption_count=0, score=8213.743519, test/accuracy=0.456100, test/loss=2.549467, test/num_examples=10000, total_duration=8569.167505, train/accuracy=0.631936, train/loss=1.600887, validation/accuracy=0.582260, validation/loss=1.830167, validation/num_examples=50000
I0126 21:26:31.899544 140026159523584 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.3361942768096924, loss=2.946510076522827
I0126 21:27:05.829843 140026151130880 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.0140838623046875, loss=2.9074299335479736
I0126 21:27:39.807068 140026159523584 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.4321796894073486, loss=2.8671345710754395
I0126 21:28:13.788287 140026151130880 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.255411148071289, loss=2.960397720336914
I0126 21:28:47.799060 140026159523584 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.2583229541778564, loss=2.9364559650421143
I0126 21:29:21.782556 140026151130880 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.360703706741333, loss=2.9693057537078857
I0126 21:29:55.733287 140026159523584 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.0959019660949707, loss=2.9045965671539307
I0126 21:30:29.777150 140026151130880 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.7245945930480957, loss=2.811309814453125
I0126 21:31:03.766437 140026159523584 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.336134672164917, loss=2.8608107566833496
I0126 21:31:37.769401 140026151130880 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.866135835647583, loss=2.8712968826293945
I0126 21:32:11.757341 140026159523584 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.3591344356536865, loss=2.902031898498535
I0126 21:32:45.744420 140026151130880 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.7100131511688232, loss=2.9319894313812256
I0126 21:33:19.748769 140026159523584 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.4249536991119385, loss=2.975761651992798
I0126 21:33:53.735460 140026151130880 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.31874942779541, loss=2.934413433074951
I0126 21:34:27.741339 140026159523584 logging_writer.py:48] [25400] global_step=25400, grad_norm=4.404600620269775, loss=2.930527925491333
I0126 21:34:54.343456 140187804313408 spec.py:321] Evaluating on the training split.
I0126 21:35:02.342597 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 21:35:14.930174 140187804313408 spec.py:349] Evaluating on the test split.
I0126 21:35:17.212424 140187804313408 submission_runner.py:408] Time since start: 9102.30s, 	Step: 25480, 	{'train/accuracy': 0.6276904940605164, 'train/loss': 1.620834469795227, 'validation/accuracy': 0.586080014705658, 'validation/loss': 1.8189243078231812, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.476848602294922, 'test/num_examples': 10000, 'score': 8723.919181346893, 'total_duration': 9102.295284986496, 'accumulated_submission_time': 8723.919181346893, 'accumulated_eval_time': 376.90863513946533, 'accumulated_logging_time': 0.5557169914245605}
I0126 21:35:17.235026 140026058876672 logging_writer.py:48] [25480] accumulated_eval_time=376.908635, accumulated_logging_time=0.555717, accumulated_submission_time=8723.919181, global_step=25480, preemption_count=0, score=8723.919181, test/accuracy=0.468700, test/loss=2.476849, test/num_examples=10000, total_duration=9102.295285, train/accuracy=0.627690, train/loss=1.620834, validation/accuracy=0.586080, validation/loss=1.818924, validation/num_examples=50000
I0126 21:35:24.367373 140026067269376 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.586646318435669, loss=2.92940092086792
I0126 21:35:58.228444 140026058876672 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.2085354328155518, loss=2.8617751598358154
I0126 21:36:32.190401 140026067269376 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.8288586139678955, loss=2.8919389247894287
I0126 21:37:06.197723 140026058876672 logging_writer.py:48] [25800] global_step=25800, grad_norm=4.906302452087402, loss=2.849118947982788
I0126 21:37:40.166992 140026067269376 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.6572539806365967, loss=2.854916572570801
I0126 21:38:14.155968 140026058876672 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.5907557010650635, loss=2.989903211593628
I0126 21:38:48.157454 140026067269376 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.268592119216919, loss=2.8105711936950684
I0126 21:39:22.309222 140026058876672 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.318164110183716, loss=2.8739912509918213
I0126 21:39:56.301728 140026067269376 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.8666515350341797, loss=2.9284496307373047
I0126 21:40:30.277668 140026058876672 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.252199172973633, loss=2.832787036895752
I0126 21:41:04.283802 140026067269376 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.416459560394287, loss=2.9144835472106934
I0126 21:41:38.257202 140026058876672 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.187577724456787, loss=2.836845636367798
I0126 21:42:12.251030 140026067269376 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.9147415161132812, loss=2.8907742500305176
I0126 21:42:46.201463 140026058876672 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.8679511547088623, loss=2.848619222640991
I0126 21:43:20.210796 140026067269376 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.2322521209716797, loss=2.7034432888031006
I0126 21:43:47.517292 140187804313408 spec.py:321] Evaluating on the training split.
I0126 21:43:55.589818 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 21:44:06.471460 140187804313408 spec.py:349] Evaluating on the test split.
I0126 21:44:08.755927 140187804313408 submission_runner.py:408] Time since start: 9633.84s, 	Step: 26982, 	{'train/accuracy': 0.6330516338348389, 'train/loss': 1.5673516988754272, 'validation/accuracy': 0.5966199636459351, 'validation/loss': 1.7551690340042114, 'validation/num_examples': 50000, 'test/accuracy': 0.469400018453598, 'test/loss': 2.4492757320404053, 'test/num_examples': 10000, 'score': 9234.135885238647, 'total_duration': 9633.838790893555, 'accumulated_submission_time': 9234.135885238647, 'accumulated_eval_time': 398.14722084999084, 'accumulated_logging_time': 0.58970046043396}
I0126 21:44:08.778863 140026050483968 logging_writer.py:48] [26982] accumulated_eval_time=398.147221, accumulated_logging_time=0.589700, accumulated_submission_time=9234.135885, global_step=26982, preemption_count=0, score=9234.135885, test/accuracy=0.469400, test/loss=2.449276, test/num_examples=10000, total_duration=9633.838791, train/accuracy=0.633052, train/loss=1.567352, validation/accuracy=0.596620, validation/loss=1.755169, validation/num_examples=50000
I0126 21:44:16.126044 140026058876672 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.0001423358917236, loss=2.9029502868652344
I0126 21:44:50.052263 140026050483968 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.1757285594940186, loss=2.8102798461914062
I0126 21:45:24.066729 140026058876672 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.4719159603118896, loss=2.9049689769744873
I0126 21:45:58.071155 140026050483968 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.9811909198760986, loss=2.8899312019348145
I0126 21:46:32.041527 140026058876672 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.025294065475464, loss=2.8347463607788086
I0126 21:47:06.041795 140026050483968 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.921326160430908, loss=2.809666156768799
I0126 21:47:40.024355 140026058876672 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.5656962394714355, loss=2.847012519836426
I0126 21:48:14.015640 140026050483968 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.6790807247161865, loss=2.805474281311035
I0126 21:48:48.022017 140026058876672 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.3535408973693848, loss=2.918076753616333
I0126 21:49:22.036084 140026050483968 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.366222858428955, loss=2.8949549198150635
I0126 21:49:56.008976 140026058876672 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.5740060806274414, loss=2.8901259899139404
I0126 21:50:29.986374 140026050483968 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.436803340911865, loss=2.8531699180603027
I0126 21:51:03.965523 140026058876672 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.1451499462127686, loss=2.885059356689453
I0126 21:51:37.942569 140026050483968 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.1399385929107666, loss=2.8513143062591553
I0126 21:52:11.914258 140026058876672 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.9848310947418213, loss=2.803987979888916
I0126 21:52:38.855457 140187804313408 spec.py:321] Evaluating on the training split.
I0126 21:52:47.008733 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 21:52:59.184258 140187804313408 spec.py:349] Evaluating on the test split.
I0126 21:53:01.546760 140187804313408 submission_runner.py:408] Time since start: 10166.63s, 	Step: 28481, 	{'train/accuracy': 0.6598572731018066, 'train/loss': 1.473987102508545, 'validation/accuracy': 0.5931400060653687, 'validation/loss': 1.779598593711853, 'validation/num_examples': 50000, 'test/accuracy': 0.4660000205039978, 'test/loss': 2.4653191566467285, 'test/num_examples': 10000, 'score': 9743.258620500565, 'total_duration': 10166.62963104248, 'accumulated_submission_time': 9743.258620500565, 'accumulated_eval_time': 420.8384771347046, 'accumulated_logging_time': 1.5133922100067139}
I0126 21:53:01.569411 140026075662080 logging_writer.py:48] [28481] accumulated_eval_time=420.838477, accumulated_logging_time=1.513392, accumulated_submission_time=9743.258621, global_step=28481, preemption_count=0, score=9743.258621, test/accuracy=0.466000, test/loss=2.465319, test/num_examples=10000, total_duration=10166.629631, train/accuracy=0.659857, train/loss=1.473987, validation/accuracy=0.593140, validation/loss=1.779599, validation/num_examples=50000
I0126 21:53:08.362251 140026151130880 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.0405681133270264, loss=2.7238311767578125
I0126 21:53:42.258323 140026075662080 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.262010335922241, loss=2.9139060974121094
I0126 21:54:16.207948 140026151130880 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.7362723350524902, loss=2.9004247188568115
I0126 21:54:50.144468 140026075662080 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.3342702388763428, loss=2.784005880355835
I0126 21:55:24.137253 140026151130880 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.8614399433135986, loss=2.8551597595214844
I0126 21:55:58.106109 140026075662080 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.9259932041168213, loss=2.757617235183716
I0126 21:56:32.065646 140026151130880 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.824230909347534, loss=2.859121084213257
I0126 21:57:06.075492 140026075662080 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.0760769844055176, loss=2.8271236419677734
I0126 21:57:40.046706 140026151130880 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.1074860095977783, loss=2.9115424156188965
I0126 21:58:14.026265 140026075662080 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.047194242477417, loss=2.909259796142578
I0126 21:58:47.994506 140026151130880 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.5170814990997314, loss=2.8762245178222656
I0126 21:59:21.989813 140026075662080 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.0546212196350098, loss=2.8918426036834717
I0126 21:59:55.974431 140026151130880 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.0002217292785645, loss=2.7729239463806152
I0126 22:00:29.953793 140026075662080 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.916548728942871, loss=2.8288369178771973
I0126 22:01:03.929236 140026151130880 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.049670696258545, loss=2.9017553329467773
I0126 22:01:31.576045 140187804313408 spec.py:321] Evaluating on the training split.
I0126 22:01:39.601700 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 22:01:50.961748 140187804313408 spec.py:349] Evaluating on the test split.
I0126 22:01:53.261518 140187804313408 submission_runner.py:408] Time since start: 10698.34s, 	Step: 29983, 	{'train/accuracy': 0.6408442258834839, 'train/loss': 1.5831868648529053, 'validation/accuracy': 0.5834800004959106, 'validation/loss': 1.8493436574935913, 'validation/num_examples': 50000, 'test/accuracy': 0.4546000361442566, 'test/loss': 2.542966604232788, 'test/num_examples': 10000, 'score': 10253.200542211533, 'total_duration': 10698.343393564224, 'accumulated_submission_time': 10253.200542211533, 'accumulated_eval_time': 442.5229060649872, 'accumulated_logging_time': 1.5470540523529053}
I0126 22:01:53.284377 140026058876672 logging_writer.py:48] [29983] accumulated_eval_time=442.522906, accumulated_logging_time=1.547054, accumulated_submission_time=10253.200542, global_step=29983, preemption_count=0, score=10253.200542, test/accuracy=0.454600, test/loss=2.542967, test/num_examples=10000, total_duration=10698.343394, train/accuracy=0.640844, train/loss=1.583187, validation/accuracy=0.583480, validation/loss=1.849344, validation/num_examples=50000
I0126 22:01:59.376016 140026067269376 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.0958092212677, loss=2.9137539863586426
I0126 22:02:33.309745 140026058876672 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.8589000701904297, loss=2.8384270668029785
I0126 22:03:07.246597 140026067269376 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.2966666221618652, loss=2.905388355255127
I0126 22:03:41.218366 140026058876672 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.594942569732666, loss=2.8583767414093018
I0126 22:04:15.444652 140026067269376 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.593708038330078, loss=2.7820732593536377
I0126 22:04:49.404886 140026058876672 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.311403274536133, loss=2.838719367980957
I0126 22:05:23.403606 140026067269376 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.8326332569122314, loss=2.8183975219726562
I0126 22:05:57.357023 140026058876672 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.9349985122680664, loss=2.80421781539917
I0126 22:06:31.328778 140026067269376 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.971891403198242, loss=2.8300178050994873
I0126 22:07:05.316458 140026058876672 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.1885881423950195, loss=2.878404140472412
I0126 22:07:39.345067 140026067269376 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.031079053878784, loss=2.860544204711914
I0126 22:08:13.335110 140026058876672 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.20982027053833, loss=2.817272186279297
I0126 22:08:47.311803 140026067269376 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.8656227588653564, loss=2.808567762374878
I0126 22:09:21.289043 140026058876672 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.312229633331299, loss=2.8034439086914062
I0126 22:09:55.260903 140026067269376 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.7885217666625977, loss=2.854771614074707
I0126 22:10:23.356454 140187804313408 spec.py:321] Evaluating on the training split.
I0126 22:10:31.441841 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 22:10:44.375713 140187804313408 spec.py:349] Evaluating on the test split.
I0126 22:10:46.747543 140187804313408 submission_runner.py:408] Time since start: 11231.83s, 	Step: 31484, 	{'train/accuracy': 0.650390625, 'train/loss': 1.5332188606262207, 'validation/accuracy': 0.6009599566459656, 'validation/loss': 1.777511715888977, 'validation/num_examples': 50000, 'test/accuracy': 0.47510001063346863, 'test/loss': 2.450975179672241, 'test/num_examples': 10000, 'score': 10763.207682132721, 'total_duration': 11231.830409526825, 'accumulated_submission_time': 10763.207682132721, 'accumulated_eval_time': 465.9139356613159, 'accumulated_logging_time': 1.5822596549987793}
I0126 22:10:46.770672 140026058876672 logging_writer.py:48] [31484] accumulated_eval_time=465.913936, accumulated_logging_time=1.582260, accumulated_submission_time=10763.207682, global_step=31484, preemption_count=0, score=10763.207682, test/accuracy=0.475100, test/loss=2.450975, test/num_examples=10000, total_duration=11231.830410, train/accuracy=0.650391, train/loss=1.533219, validation/accuracy=0.600960, validation/loss=1.777512, validation/num_examples=50000
I0126 22:10:52.554281 140026151130880 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.833932876586914, loss=2.888495922088623
I0126 22:11:26.451591 140026058876672 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.613941192626953, loss=2.701788902282715
I0126 22:12:00.376946 140026151130880 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.868307113647461, loss=2.7814948558807373
I0126 22:12:34.352586 140026058876672 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.609816789627075, loss=2.83958101272583
I0126 22:13:08.314027 140026151130880 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.1383652687072754, loss=2.810866355895996
I0126 22:13:42.295863 140026058876672 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.6020610332489014, loss=2.7450368404388428
I0126 22:14:16.281437 140026151130880 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.6150057315826416, loss=2.8474130630493164
I0126 22:14:50.217664 140026058876672 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.1289703845977783, loss=2.8485758304595947
I0126 22:15:24.206423 140026151130880 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.4877707958221436, loss=2.7046287059783936
I0126 22:15:58.174920 140026058876672 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.8535118103027344, loss=2.848358631134033
I0126 22:16:32.182140 140026151130880 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.019984722137451, loss=2.7523584365844727
I0126 22:17:06.185160 140026058876672 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.8395774364471436, loss=2.832012176513672
I0126 22:17:40.129174 140026151130880 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.847024917602539, loss=2.8828482627868652
I0126 22:18:14.060919 140026058876672 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.050798177719116, loss=2.8363888263702393
I0126 22:18:48.052340 140026151130880 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.163862705230713, loss=2.7476649284362793
I0126 22:19:17.073958 140187804313408 spec.py:321] Evaluating on the training split.
I0126 22:19:25.269382 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 22:19:36.624767 140187804313408 spec.py:349] Evaluating on the test split.
I0126 22:19:38.939888 140187804313408 submission_runner.py:408] Time since start: 11764.02s, 	Step: 32987, 	{'train/accuracy': 0.6478196382522583, 'train/loss': 1.5455231666564941, 'validation/accuracy': 0.5983999967575073, 'validation/loss': 1.7723729610443115, 'validation/num_examples': 50000, 'test/accuracy': 0.47190001606941223, 'test/loss': 2.464462995529175, 'test/num_examples': 10000, 'score': 11273.447633743286, 'total_duration': 11764.022417783737, 'accumulated_submission_time': 11273.447633743286, 'accumulated_eval_time': 487.7795009613037, 'accumulated_logging_time': 1.6160292625427246}
I0126 22:19:38.962926 140026042091264 logging_writer.py:48] [32987] accumulated_eval_time=487.779501, accumulated_logging_time=1.616029, accumulated_submission_time=11273.447634, global_step=32987, preemption_count=0, score=11273.447634, test/accuracy=0.471900, test/loss=2.464463, test/num_examples=10000, total_duration=11764.022418, train/accuracy=0.647820, train/loss=1.545523, validation/accuracy=0.598400, validation/loss=1.772373, validation/num_examples=50000
I0126 22:19:43.728342 140026050483968 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.6491217613220215, loss=2.7894375324249268
I0126 22:20:17.625915 140026042091264 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.214628219604492, loss=2.86303448677063
I0126 22:20:51.526239 140026050483968 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.81447172164917, loss=2.820672035217285
I0126 22:21:25.484743 140026042091264 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.52538800239563, loss=2.7905094623565674
I0126 22:21:59.460427 140026050483968 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.148494243621826, loss=2.87695050239563
I0126 22:22:33.431550 140026042091264 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.1201772689819336, loss=2.851095676422119
I0126 22:23:07.434862 140026050483968 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.598557710647583, loss=2.978175640106201
I0126 22:23:41.403276 140026042091264 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.179494857788086, loss=2.7038745880126953
I0126 22:24:15.371632 140026050483968 logging_writer.py:48] [33800] global_step=33800, grad_norm=4.316969871520996, loss=2.843315601348877
I0126 22:24:49.330655 140026042091264 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.150477409362793, loss=2.6837239265441895
I0126 22:25:23.325543 140026050483968 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.939138650894165, loss=2.858905792236328
I0126 22:25:57.255570 140026042091264 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.3855011463165283, loss=2.804762363433838
I0126 22:26:31.223074 140026050483968 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.7565605640411377, loss=2.704948663711548
I0126 22:27:05.219403 140026042091264 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.7895126342773438, loss=2.8592031002044678
I0126 22:27:39.213434 140026050483968 logging_writer.py:48] [34400] global_step=34400, grad_norm=4.430564880371094, loss=2.7546238899230957
I0126 22:28:09.262361 140187804313408 spec.py:321] Evaluating on the training split.
I0126 22:28:17.529326 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 22:28:27.647742 140187804313408 spec.py:349] Evaluating on the test split.
I0126 22:28:29.933989 140187804313408 submission_runner.py:408] Time since start: 12295.02s, 	Step: 34490, 	{'train/accuracy': 0.6456871628761292, 'train/loss': 1.5215743780136108, 'validation/accuracy': 0.6019399762153625, 'validation/loss': 1.7319201231002808, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.4039146900177, 'test/num_examples': 10000, 'score': 11783.682542085648, 'total_duration': 12295.01685166359, 'accumulated_submission_time': 11783.682542085648, 'accumulated_eval_time': 508.4510877132416, 'accumulated_logging_time': 1.65091872215271}
I0126 22:28:29.957417 140026050483968 logging_writer.py:48] [34490] accumulated_eval_time=508.451088, accumulated_logging_time=1.650919, accumulated_submission_time=11783.682542, global_step=34490, preemption_count=0, score=11783.682542, test/accuracy=0.484400, test/loss=2.403915, test/num_examples=10000, total_duration=12295.016852, train/accuracy=0.645687, train/loss=1.521574, validation/accuracy=0.601940, validation/loss=1.731920, validation/num_examples=50000
I0126 22:28:33.692136 140026058876672 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.9461843967437744, loss=2.7351412773132324
I0126 22:29:07.632884 140026050483968 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.7681992053985596, loss=2.7300002574920654
I0126 22:29:41.518947 140026058876672 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.5492351055145264, loss=2.7950093746185303
I0126 22:30:15.463275 140026050483968 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.5813679695129395, loss=2.9430675506591797
I0126 22:30:49.399967 140026058876672 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.1165759563446045, loss=2.809025287628174
I0126 22:31:23.353732 140026050483968 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.244032859802246, loss=2.8109827041625977
I0126 22:31:57.330996 140026058876672 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.886679172515869, loss=2.814077854156494
I0126 22:32:31.290991 140026050483968 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.3744664192199707, loss=2.801363229751587
I0126 22:33:05.228068 140026058876672 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.4379937648773193, loss=2.7912509441375732
I0126 22:33:39.180325 140026050483968 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.287763833999634, loss=2.7733116149902344
I0126 22:34:13.143091 140026058876672 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.8069839477539062, loss=2.789987325668335
I0126 22:34:47.109755 140026050483968 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.0731215476989746, loss=2.864565134048462
I0126 22:35:21.123722 140026058876672 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.6704623699188232, loss=2.733433961868286
I0126 22:35:55.064598 140026050483968 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.3420217037200928, loss=2.7943949699401855
I0126 22:36:29.015783 140026058876672 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.593411922454834, loss=2.7875332832336426
I0126 22:37:00.031723 140187804313408 spec.py:321] Evaluating on the training split.
I0126 22:37:08.304425 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 22:37:20.024256 140187804313408 spec.py:349] Evaluating on the test split.
I0126 22:37:22.284803 140187804313408 submission_runner.py:408] Time since start: 12827.37s, 	Step: 35993, 	{'train/accuracy': 0.6405652165412903, 'train/loss': 1.5603916645050049, 'validation/accuracy': 0.5936599969863892, 'validation/loss': 1.7699402570724487, 'validation/num_examples': 50000, 'test/accuracy': 0.4715000092983246, 'test/loss': 2.450953722000122, 'test/num_examples': 10000, 'score': 12293.693517684937, 'total_duration': 12827.367676734924, 'accumulated_submission_time': 12293.693517684937, 'accumulated_eval_time': 530.7041218280792, 'accumulated_logging_time': 1.6843512058258057}
I0126 22:37:22.308631 140026067269376 logging_writer.py:48] [35993] accumulated_eval_time=530.704122, accumulated_logging_time=1.684351, accumulated_submission_time=12293.693518, global_step=35993, preemption_count=0, score=12293.693518, test/accuracy=0.471500, test/loss=2.450954, test/num_examples=10000, total_duration=12827.367677, train/accuracy=0.640565, train/loss=1.560392, validation/accuracy=0.593660, validation/loss=1.769940, validation/num_examples=50000
I0126 22:37:25.033763 140026075662080 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.201761245727539, loss=2.7518131732940674
I0126 22:37:58.904070 140026067269376 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.00597882270813, loss=2.868264675140381
I0126 22:38:32.849150 140026075662080 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.458444595336914, loss=2.787688732147217
I0126 22:39:06.804979 140026067269376 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.057537078857422, loss=2.8714489936828613
I0126 22:39:40.762618 140026075662080 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.4325804710388184, loss=2.8488166332244873
I0126 22:40:14.733555 140026067269376 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.237230062484741, loss=2.8198587894439697
I0126 22:40:48.694880 140026075662080 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.1337151527404785, loss=2.8502163887023926
I0126 22:41:22.823168 140026067269376 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.906583309173584, loss=2.731527328491211
I0126 22:41:56.804450 140026075662080 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.1922671794891357, loss=2.7538552284240723
I0126 22:42:30.788147 140026067269376 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.155482530593872, loss=2.782769203186035
I0126 22:43:04.766006 140026075662080 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.107537269592285, loss=2.888618230819702
I0126 22:43:38.754706 140026067269376 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.168848752975464, loss=2.8442234992980957
I0126 22:44:12.718956 140026075662080 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.8717076778411865, loss=2.7857666015625
I0126 22:44:46.679890 140026067269376 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.439595937728882, loss=2.780843734741211
I0126 22:45:20.664838 140026075662080 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.2894554138183594, loss=2.7862815856933594
I0126 22:45:52.380450 140187804313408 spec.py:321] Evaluating on the training split.
I0126 22:45:59.324896 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 22:46:11.643995 140187804313408 spec.py:349] Evaluating on the test split.
I0126 22:46:13.909808 140187804313408 submission_runner.py:408] Time since start: 13358.99s, 	Step: 37495, 	{'train/accuracy': 0.6504703164100647, 'train/loss': 1.510545015335083, 'validation/accuracy': 0.608460009098053, 'validation/loss': 1.7156388759613037, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.3600785732269287, 'test/num_examples': 10000, 'score': 12803.700054168701, 'total_duration': 13358.992667198181, 'accumulated_submission_time': 12803.700054168701, 'accumulated_eval_time': 552.2334206104279, 'accumulated_logging_time': 1.7202045917510986}
I0126 22:46:13.934069 140026042091264 logging_writer.py:48] [37495] accumulated_eval_time=552.233421, accumulated_logging_time=1.720205, accumulated_submission_time=12803.700054, global_step=37495, preemption_count=0, score=12803.700054, test/accuracy=0.491600, test/loss=2.360079, test/num_examples=10000, total_duration=13358.992667, train/accuracy=0.650470, train/loss=1.510545, validation/accuracy=0.608460, validation/loss=1.715639, validation/num_examples=50000
I0126 22:46:15.975685 140026050483968 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.0691959857940674, loss=2.892313003540039
I0126 22:46:49.837877 140026042091264 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.128234624862671, loss=2.7432782649993896
I0126 22:47:23.788590 140026050483968 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.936448097229004, loss=2.821577310562134
I0126 22:47:57.807536 140026042091264 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.8408868312835693, loss=2.7994790077209473
I0126 22:48:31.775826 140026050483968 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.7796597480773926, loss=2.8756792545318604
I0126 22:49:05.714549 140026042091264 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.1966700553894043, loss=2.861729621887207
I0126 22:49:39.649866 140026050483968 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.8235793113708496, loss=2.818028450012207
I0126 22:50:13.591043 140026042091264 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.877809524536133, loss=2.799938917160034
I0126 22:50:47.561712 140026050483968 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.523313283920288, loss=2.8878555297851562
I0126 22:51:21.508355 140026042091264 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.133383274078369, loss=2.774181842803955
I0126 22:51:55.459875 140026050483968 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.2348310947418213, loss=2.8474650382995605
I0126 22:52:29.398255 140026042091264 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.8379127979278564, loss=2.782893180847168
I0126 22:53:03.334217 140026050483968 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.9216291904449463, loss=2.741209030151367
I0126 22:53:37.330238 140026042091264 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.9016690254211426, loss=2.7559597492218018
I0126 22:54:11.259294 140026050483968 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.782697916030884, loss=2.776020050048828
I0126 22:54:43.988102 140187804313408 spec.py:321] Evaluating on the training split.
I0126 22:54:50.685221 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 22:55:01.812325 140187804313408 spec.py:349] Evaluating on the test split.
I0126 22:55:04.112128 140187804313408 submission_runner.py:408] Time since start: 13889.19s, 	Step: 38998, 	{'train/accuracy': 0.6715760231018066, 'train/loss': 1.4409266710281372, 'validation/accuracy': 0.6063599586486816, 'validation/loss': 1.7327011823654175, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.3913426399230957, 'test/num_examples': 10000, 'score': 13313.690217733383, 'total_duration': 13889.194969892502, 'accumulated_submission_time': 13313.690217733383, 'accumulated_eval_time': 572.3573670387268, 'accumulated_logging_time': 1.755732774734497}
I0126 22:55:04.137004 140026151130880 logging_writer.py:48] [38998] accumulated_eval_time=572.357367, accumulated_logging_time=1.755733, accumulated_submission_time=13313.690218, global_step=38998, preemption_count=0, score=13313.690218, test/accuracy=0.484100, test/loss=2.391343, test/num_examples=10000, total_duration=13889.194970, train/accuracy=0.671576, train/loss=1.440927, validation/accuracy=0.606360, validation/loss=1.732701, validation/num_examples=50000
I0126 22:55:05.159306 140026159523584 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.8516411781311035, loss=2.681264638900757
I0126 22:55:39.086087 140026151130880 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.859983205795288, loss=2.7598118782043457
I0126 22:56:13.035353 140026159523584 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.2336366176605225, loss=2.775611639022827
I0126 22:56:46.994514 140026151130880 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.445305585861206, loss=2.7773807048797607
I0126 22:57:20.910629 140026159523584 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.003864049911499, loss=2.854454517364502
I0126 22:57:54.859144 140026151130880 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.865943193435669, loss=2.779484510421753
I0126 22:58:28.842262 140026159523584 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.1560187339782715, loss=2.7058706283569336
I0126 22:59:02.821060 140026151130880 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.549738883972168, loss=2.801652193069458
I0126 22:59:36.821526 140026159523584 logging_writer.py:48] [39800] global_step=39800, grad_norm=4.2070631980896, loss=2.825857639312744
I0126 23:00:10.969891 140026151130880 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.1507198810577393, loss=2.7913007736206055
I0126 23:00:44.967384 140026159523584 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.649016857147217, loss=2.877236843109131
I0126 23:01:18.913804 140026151130880 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.424382448196411, loss=2.7482919692993164
I0126 23:01:52.858838 140026159523584 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.6626217365264893, loss=2.716273546218872
I0126 23:02:26.825438 140026151130880 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.201472520828247, loss=2.8185698986053467
I0126 23:03:00.783050 140026159523584 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.418835401535034, loss=2.814861297607422
I0126 23:03:34.157031 140187804313408 spec.py:321] Evaluating on the training split.
I0126 23:03:40.772117 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 23:03:49.918256 140187804313408 spec.py:349] Evaluating on the test split.
I0126 23:03:52.243381 140187804313408 submission_runner.py:408] Time since start: 14417.33s, 	Step: 40500, 	{'train/accuracy': 0.6689851880073547, 'train/loss': 1.4142094850540161, 'validation/accuracy': 0.6127600073814392, 'validation/loss': 1.6750468015670776, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.3593156337738037, 'test/num_examples': 10000, 'score': 13823.646218061447, 'total_duration': 14417.326255083084, 'accumulated_submission_time': 13823.646218061447, 'accumulated_eval_time': 590.4436695575714, 'accumulated_logging_time': 1.7907118797302246}
I0126 23:03:52.267579 140026042091264 logging_writer.py:48] [40500] accumulated_eval_time=590.443670, accumulated_logging_time=1.790712, accumulated_submission_time=13823.646218, global_step=40500, preemption_count=0, score=13823.646218, test/accuracy=0.488500, test/loss=2.359316, test/num_examples=10000, total_duration=14417.326255, train/accuracy=0.668985, train/loss=1.414209, validation/accuracy=0.612760, validation/loss=1.675047, validation/num_examples=50000
I0126 23:03:52.624554 140026050483968 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.763352632522583, loss=2.77005672454834
I0126 23:04:26.522649 140026042091264 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.0812370777130127, loss=2.691347599029541
I0126 23:05:00.410148 140026050483968 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.8817174434661865, loss=2.77433180809021
I0126 23:05:34.375718 140026042091264 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.753791570663452, loss=2.8267204761505127
I0126 23:06:08.374587 140026050483968 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.0998356342315674, loss=2.7880890369415283
I0126 23:06:42.345067 140026042091264 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.9616096019744873, loss=2.703278064727783
I0126 23:07:16.303595 140026050483968 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.4837465286254883, loss=2.8434388637542725
I0126 23:07:50.271404 140026042091264 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.7177276611328125, loss=2.6936731338500977
I0126 23:08:24.199109 140026050483968 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.458073616027832, loss=2.7510595321655273
I0126 23:08:58.148527 140026042091264 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.2685275077819824, loss=2.7637388706207275
I0126 23:09:32.099290 140026050483968 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.4942872524261475, loss=2.8085780143737793
I0126 23:10:06.007602 140026042091264 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.905914545059204, loss=2.7026896476745605
I0126 23:10:39.962449 140026050483968 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.0253493785858154, loss=2.7742297649383545
I0126 23:11:13.894919 140026042091264 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.427589178085327, loss=2.7532734870910645
I0126 23:11:47.849909 140026050483968 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.276549816131592, loss=2.7950010299682617
I0126 23:12:21.846773 140026042091264 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.9647202491760254, loss=2.6765146255493164
I0126 23:12:22.326967 140187804313408 spec.py:321] Evaluating on the training split.
I0126 23:12:28.812397 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 23:12:40.519995 140187804313408 spec.py:349] Evaluating on the test split.
I0126 23:12:42.808920 140187804313408 submission_runner.py:408] Time since start: 14947.89s, 	Step: 42003, 	{'train/accuracy': 0.661531388759613, 'train/loss': 1.4491329193115234, 'validation/accuracy': 0.6156399846076965, 'validation/loss': 1.6673866510391235, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.3523242473602295, 'test/num_examples': 10000, 'score': 14333.642135620117, 'total_duration': 14947.89179635048, 'accumulated_submission_time': 14333.642135620117, 'accumulated_eval_time': 610.9255712032318, 'accumulated_logging_time': 1.8257253170013428}
I0126 23:12:42.833643 140026075662080 logging_writer.py:48] [42003] accumulated_eval_time=610.925571, accumulated_logging_time=1.825725, accumulated_submission_time=14333.642136, global_step=42003, preemption_count=0, score=14333.642136, test/accuracy=0.494900, test/loss=2.352324, test/num_examples=10000, total_duration=14947.891796, train/accuracy=0.661531, train/loss=1.449133, validation/accuracy=0.615640, validation/loss=1.667387, validation/num_examples=50000
I0126 23:13:16.028468 140026151130880 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.999505043029785, loss=2.8161511421203613
I0126 23:13:49.950751 140026075662080 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.1095340251922607, loss=2.797152280807495
I0126 23:14:23.855898 140026151130880 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.863755702972412, loss=2.775804042816162
I0126 23:14:57.828177 140026075662080 logging_writer.py:48] [42400] global_step=42400, grad_norm=4.274326324462891, loss=2.7637393474578857
I0126 23:15:31.774953 140026151130880 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.089317798614502, loss=2.7878074645996094
I0126 23:16:05.721139 140026075662080 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.4377682209014893, loss=2.718304395675659
I0126 23:16:39.675438 140026151130880 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.97033953666687, loss=2.818870782852173
I0126 23:17:13.626459 140026075662080 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.2258379459381104, loss=2.6974260807037354
I0126 23:17:47.599829 140026151130880 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.1353955268859863, loss=2.6897478103637695
I0126 23:18:21.533846 140026075662080 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.419745445251465, loss=2.7029473781585693
I0126 23:18:55.565824 140026151130880 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.0190203189849854, loss=2.7114977836608887
I0126 23:19:29.533843 140026075662080 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.8725852966308594, loss=2.6609439849853516
I0126 23:20:03.517512 140026151130880 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.87501859664917, loss=2.7413134574890137
I0126 23:20:37.477866 140026075662080 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.8060741424560547, loss=2.836212158203125
I0126 23:21:11.407355 140026151130880 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.186936855316162, loss=2.856186866760254
I0126 23:21:12.882521 140187804313408 spec.py:321] Evaluating on the training split.
I0126 23:21:19.169851 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 23:21:29.394915 140187804313408 spec.py:349] Evaluating on the test split.
I0126 23:21:31.666020 140187804313408 submission_runner.py:408] Time since start: 15476.75s, 	Step: 43506, 	{'train/accuracy': 0.6548947691917419, 'train/loss': 1.4952327013015747, 'validation/accuracy': 0.6090599894523621, 'validation/loss': 1.7058863639831543, 'validation/num_examples': 50000, 'test/accuracy': 0.4839000105857849, 'test/loss': 2.3706626892089844, 'test/num_examples': 10000, 'score': 14843.627077817917, 'total_duration': 15476.748891830444, 'accumulated_submission_time': 14843.627077817917, 'accumulated_eval_time': 629.7090165615082, 'accumulated_logging_time': 1.8625688552856445}
I0126 23:21:31.691433 140026067269376 logging_writer.py:48] [43506] accumulated_eval_time=629.709017, accumulated_logging_time=1.862569, accumulated_submission_time=14843.627078, global_step=43506, preemption_count=0, score=14843.627078, test/accuracy=0.483900, test/loss=2.370663, test/num_examples=10000, total_duration=15476.748892, train/accuracy=0.654895, train/loss=1.495233, validation/accuracy=0.609060, validation/loss=1.705886, validation/num_examples=50000
I0126 23:22:03.880341 140026167916288 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.181701421737671, loss=2.771681547164917
I0126 23:22:37.808784 140026067269376 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.1634936332702637, loss=2.7615420818328857
I0126 23:23:11.774119 140026167916288 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.824491262435913, loss=2.706291913986206
I0126 23:23:45.706239 140026067269376 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.6162900924682617, loss=2.8195950984954834
I0126 23:24:19.646999 140026167916288 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.433345317840576, loss=2.8411669731140137
I0126 23:24:53.687892 140026067269376 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.970283031463623, loss=2.7755117416381836
I0126 23:25:27.631863 140026167916288 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.183624744415283, loss=2.7067840099334717
I0126 23:26:01.567124 140026067269376 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.8263280391693115, loss=2.67954158782959
I0126 23:26:35.554725 140026167916288 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.119323253631592, loss=2.7718019485473633
I0126 23:27:09.534661 140026067269376 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.6493849754333496, loss=2.7625484466552734
I0126 23:27:43.457247 140026167916288 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.0273356437683105, loss=2.724579334259033
I0126 23:28:17.401598 140026067269376 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.890752077102661, loss=2.7271065711975098
I0126 23:28:51.368315 140026167916288 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.0657455921173096, loss=2.682622194290161
I0126 23:29:25.287559 140026067269376 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.1445460319519043, loss=2.754831552505493
I0126 23:29:59.217131 140026167916288 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.3461620807647705, loss=2.774160861968994
I0126 23:30:01.733054 140187804313408 spec.py:321] Evaluating on the training split.
I0126 23:30:08.105063 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 23:30:18.141026 140187804313408 spec.py:349] Evaluating on the test split.
I0126 23:30:20.394810 140187804313408 submission_runner.py:408] Time since start: 16005.48s, 	Step: 45009, 	{'train/accuracy': 0.6541972160339355, 'train/loss': 1.492714524269104, 'validation/accuracy': 0.6135199666023254, 'validation/loss': 1.686639666557312, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.3645849227905273, 'test/num_examples': 10000, 'score': 15353.602659702301, 'total_duration': 16005.477684020996, 'accumulated_submission_time': 15353.602659702301, 'accumulated_eval_time': 648.3707220554352, 'accumulated_logging_time': 1.9011075496673584}
I0126 23:30:20.419759 140026058876672 logging_writer.py:48] [45009] accumulated_eval_time=648.370722, accumulated_logging_time=1.901108, accumulated_submission_time=15353.602660, global_step=45009, preemption_count=0, score=15353.602660, test/accuracy=0.488900, test/loss=2.364585, test/num_examples=10000, total_duration=16005.477684, train/accuracy=0.654197, train/loss=1.492715, validation/accuracy=0.613520, validation/loss=1.686640, validation/num_examples=50000
I0126 23:30:51.633009 140026075662080 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.990827798843384, loss=2.857661724090576
I0126 23:31:25.537286 140026058876672 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.8594298362731934, loss=2.8074209690093994
I0126 23:31:59.432203 140026075662080 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.9004628658294678, loss=2.748107671737671
I0126 23:32:33.380591 140026058876672 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.5129334926605225, loss=2.7157859802246094
I0126 23:33:07.341938 140026075662080 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.0366692543029785, loss=2.7508630752563477
I0126 23:33:41.277187 140026058876672 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.2729573249816895, loss=2.760444402694702
I0126 23:34:15.211450 140026075662080 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.726858139038086, loss=2.728515625
I0126 23:34:49.162758 140026058876672 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.089768171310425, loss=2.8295865058898926
I0126 23:35:23.100692 140026075662080 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.3500590324401855, loss=2.830293655395508
I0126 23:35:57.056382 140026058876672 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.298215866088867, loss=2.7639613151550293
I0126 23:36:30.999419 140026075662080 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.6659128665924072, loss=2.7273426055908203
I0126 23:37:04.963695 140026058876672 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.9542551040649414, loss=2.7861852645874023
I0126 23:37:38.910867 140026075662080 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.521756172180176, loss=2.6932461261749268
I0126 23:38:12.850823 140026058876672 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.148512363433838, loss=2.711907386779785
I0126 23:38:46.777121 140026075662080 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.237252950668335, loss=2.732908248901367
I0126 23:38:50.659394 140187804313408 spec.py:321] Evaluating on the training split.
I0126 23:38:57.007596 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 23:39:06.273725 140187804313408 spec.py:349] Evaluating on the test split.
I0126 23:39:08.726386 140187804313408 submission_runner.py:408] Time since start: 16533.81s, 	Step: 46513, 	{'train/accuracy': 0.6575055718421936, 'train/loss': 1.4960453510284424, 'validation/accuracy': 0.6189199686050415, 'validation/loss': 1.6851189136505127, 'validation/num_examples': 50000, 'test/accuracy': 0.4930000305175781, 'test/loss': 2.3379759788513184, 'test/num_examples': 10000, 'score': 15863.77936911583, 'total_duration': 16533.809260606766, 'accumulated_submission_time': 15863.77936911583, 'accumulated_eval_time': 666.4376637935638, 'accumulated_logging_time': 1.9357657432556152}
I0126 23:39:08.752823 140026067269376 logging_writer.py:48] [46513] accumulated_eval_time=666.437664, accumulated_logging_time=1.935766, accumulated_submission_time=15863.779369, global_step=46513, preemption_count=0, score=15863.779369, test/accuracy=0.493000, test/loss=2.337976, test/num_examples=10000, total_duration=16533.809261, train/accuracy=0.657506, train/loss=1.496045, validation/accuracy=0.618920, validation/loss=1.685119, validation/num_examples=50000
I0126 23:39:38.616473 140026167916288 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.9840354919433594, loss=2.7660560607910156
I0126 23:40:12.470096 140026067269376 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.846529722213745, loss=2.75795316696167
I0126 23:40:46.386673 140026167916288 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.3376455307006836, loss=2.759836435317993
I0126 23:41:20.319596 140026067269376 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.4926748275756836, loss=2.75223970413208
I0126 23:41:54.254425 140026167916288 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.201664447784424, loss=2.7708380222320557
I0126 23:42:28.142433 140026067269376 logging_writer.py:48] [47100] global_step=47100, grad_norm=4.3568572998046875, loss=2.7071285247802734
I0126 23:43:02.068786 140026167916288 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.5271730422973633, loss=2.7292675971984863
I0126 23:43:36.085769 140026067269376 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.9949183464050293, loss=2.6421563625335693
I0126 23:44:10.001748 140026167916288 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.288987398147583, loss=2.6368985176086426
I0126 23:44:43.938724 140026067269376 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.4778501987457275, loss=2.7587132453918457
I0126 23:45:17.897988 140026167916288 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.5213189125061035, loss=2.76442289352417
I0126 23:45:51.794447 140026067269376 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.794811487197876, loss=2.7655084133148193
I0126 23:46:25.720904 140026167916288 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.9107890129089355, loss=2.6743428707122803
I0126 23:46:59.682153 140026067269376 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.8036746978759766, loss=2.719971179962158
I0126 23:47:33.620864 140026167916288 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.0797150135040283, loss=2.7591586112976074
I0126 23:47:38.841786 140187804313408 spec.py:321] Evaluating on the training split.
I0126 23:47:45.199392 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 23:47:55.125826 140187804313408 spec.py:349] Evaluating on the test split.
I0126 23:47:57.470754 140187804313408 submission_runner.py:408] Time since start: 17062.55s, 	Step: 48017, 	{'train/accuracy': 0.6881377696990967, 'train/loss': 1.3390893936157227, 'validation/accuracy': 0.6141799688339233, 'validation/loss': 1.6819976568222046, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.3249881267547607, 'test/num_examples': 10000, 'score': 16373.80011177063, 'total_duration': 17062.553616285324, 'accumulated_submission_time': 16373.80011177063, 'accumulated_eval_time': 685.0665671825409, 'accumulated_logging_time': 1.9751687049865723}
I0126 23:47:57.498960 140026042091264 logging_writer.py:48] [48017] accumulated_eval_time=685.066567, accumulated_logging_time=1.975169, accumulated_submission_time=16373.800112, global_step=48017, preemption_count=0, score=16373.800112, test/accuracy=0.498600, test/loss=2.324988, test/num_examples=10000, total_duration=17062.553616, train/accuracy=0.688138, train/loss=1.339089, validation/accuracy=0.614180, validation/loss=1.681998, validation/num_examples=50000
I0126 23:48:25.979912 140026050483968 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.206228256225586, loss=2.7364068031311035
I0126 23:48:59.879714 140026042091264 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.7902679443359375, loss=2.6900475025177
I0126 23:49:33.851495 140026050483968 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.5872442722320557, loss=2.859919548034668
I0126 23:50:07.788631 140026042091264 logging_writer.py:48] [48400] global_step=48400, grad_norm=2.8317489624023438, loss=2.705564260482788
I0126 23:50:41.733807 140026050483968 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.410883665084839, loss=2.7185099124908447
I0126 23:51:15.663340 140026042091264 logging_writer.py:48] [48600] global_step=48600, grad_norm=2.90140438079834, loss=2.7518811225891113
I0126 23:51:49.627181 140026050483968 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.135185480117798, loss=2.711578369140625
I0126 23:52:23.543577 140026042091264 logging_writer.py:48] [48800] global_step=48800, grad_norm=2.917560577392578, loss=2.7332603931427
I0126 23:52:57.493723 140026050483968 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.0387556552886963, loss=2.740663528442383
I0126 23:53:31.407929 140026042091264 logging_writer.py:48] [49000] global_step=49000, grad_norm=2.87680983543396, loss=2.700514793395996
I0126 23:54:05.352714 140026050483968 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.366743564605713, loss=2.769054651260376
I0126 23:54:39.300781 140026042091264 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.0268349647521973, loss=2.7164595127105713
I0126 23:55:13.274882 140026050483968 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.5077459812164307, loss=2.7408223152160645
I0126 23:55:47.291701 140026042091264 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.8517305850982666, loss=2.6661927700042725
I0126 23:56:21.216928 140026050483968 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.1151490211486816, loss=2.6275932788848877
I0126 23:56:27.474408 140187804313408 spec.py:321] Evaluating on the training split.
I0126 23:56:33.752953 140187804313408 spec.py:333] Evaluating on the validation split.
I0126 23:56:43.583803 140187804313408 spec.py:349] Evaluating on the test split.
I0126 23:56:45.912936 140187804313408 submission_runner.py:408] Time since start: 17591.00s, 	Step: 49520, 	{'train/accuracy': 0.6678889989852905, 'train/loss': 1.4150813817977905, 'validation/accuracy': 0.6149399876594543, 'validation/loss': 1.6707707643508911, 'validation/num_examples': 50000, 'test/accuracy': 0.4930000305175781, 'test/loss': 2.3653693199157715, 'test/num_examples': 10000, 'score': 16883.71029162407, 'total_duration': 17590.99581003189, 'accumulated_submission_time': 16883.71029162407, 'accumulated_eval_time': 703.5050427913666, 'accumulated_logging_time': 2.0140111446380615}
I0126 23:56:45.942686 140026042091264 logging_writer.py:48] [49520] accumulated_eval_time=703.505043, accumulated_logging_time=2.014011, accumulated_submission_time=16883.710292, global_step=49520, preemption_count=0, score=16883.710292, test/accuracy=0.493000, test/loss=2.365369, test/num_examples=10000, total_duration=17590.995810, train/accuracy=0.667889, train/loss=1.415081, validation/accuracy=0.614940, validation/loss=1.670771, validation/num_examples=50000
I0126 23:57:13.358755 140026050483968 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.308121919631958, loss=2.7167482376098633
I0126 23:57:47.295308 140026042091264 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.555877208709717, loss=2.7102744579315186
I0126 23:58:21.220978 140026050483968 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.738264322280884, loss=2.6949965953826904
I0126 23:58:55.122245 140026042091264 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.07614803314209, loss=2.8184704780578613
I0126 23:59:29.057992 140026050483968 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.4216232299804688, loss=2.777933359146118
I0127 00:00:02.988847 140026042091264 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.2239792346954346, loss=2.7709498405456543
I0127 00:00:36.942095 140026050483968 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.165510892868042, loss=2.7628865242004395
I0127 00:01:10.892886 140026042091264 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.9116926193237305, loss=2.6236541271209717
I0127 00:01:45.010067 140026050483968 logging_writer.py:48] [50400] global_step=50400, grad_norm=2.9309706687927246, loss=2.692619800567627
I0127 00:02:18.888130 140026042091264 logging_writer.py:48] [50500] global_step=50500, grad_norm=2.9505650997161865, loss=2.7305595874786377
I0127 00:02:52.827892 140026050483968 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.0188486576080322, loss=2.729708433151245
I0127 00:03:26.750090 140026042091264 logging_writer.py:48] [50700] global_step=50700, grad_norm=2.8647677898406982, loss=2.6811089515686035
I0127 00:04:00.661701 140026050483968 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.373114824295044, loss=2.815275192260742
I0127 00:04:34.611067 140026042091264 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.0155751705169678, loss=2.7530064582824707
I0127 00:05:08.540583 140026050483968 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.2945027351379395, loss=2.744213342666626
I0127 00:05:16.155884 140187804313408 spec.py:321] Evaluating on the training split.
I0127 00:05:22.345874 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 00:05:31.842282 140187804313408 spec.py:349] Evaluating on the test split.
I0127 00:05:34.164046 140187804313408 submission_runner.py:408] Time since start: 18119.25s, 	Step: 51024, 	{'train/accuracy': 0.6670718789100647, 'train/loss': 1.4345439672470093, 'validation/accuracy': 0.6176199913024902, 'validation/loss': 1.679994821548462, 'validation/num_examples': 50000, 'test/accuracy': 0.5006999969482422, 'test/loss': 2.3009283542633057, 'test/num_examples': 10000, 'score': 17393.85679912567, 'total_duration': 18119.246876478195, 'accumulated_submission_time': 17393.85679912567, 'accumulated_eval_time': 721.513111114502, 'accumulated_logging_time': 2.0556466579437256}
I0127 00:05:34.194643 140026050483968 logging_writer.py:48] [51024] accumulated_eval_time=721.513111, accumulated_logging_time=2.055647, accumulated_submission_time=17393.856799, global_step=51024, preemption_count=0, score=17393.856799, test/accuracy=0.500700, test/loss=2.300928, test/num_examples=10000, total_duration=18119.246876, train/accuracy=0.667072, train/loss=1.434544, validation/accuracy=0.617620, validation/loss=1.679995, validation/num_examples=50000
I0127 00:06:00.285278 140026067269376 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.9557058811187744, loss=2.7519774436950684
I0127 00:06:34.211302 140026050483968 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.9579222202301025, loss=2.6751132011413574
I0127 00:07:08.156415 140026067269376 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.1644699573516846, loss=2.7524831295013428
I0127 00:07:42.089740 140026050483968 logging_writer.py:48] [51400] global_step=51400, grad_norm=4.287642478942871, loss=2.7056238651275635
I0127 00:08:16.145031 140026067269376 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.373114585876465, loss=2.6884615421295166
I0127 00:08:50.074115 140026050483968 logging_writer.py:48] [51600] global_step=51600, grad_norm=2.9816033840179443, loss=2.659409284591675
I0127 00:09:23.987251 140026067269376 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.9718663692474365, loss=2.7536122798919678
I0127 00:09:57.926086 140026050483968 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.05652117729187, loss=2.767704486846924
I0127 00:10:31.813698 140026067269376 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.3422086238861084, loss=2.7750611305236816
I0127 00:11:05.763061 140026050483968 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.943591594696045, loss=2.6618781089782715
I0127 00:11:39.732734 140026067269376 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.501939535140991, loss=2.695128917694092
I0127 00:12:13.666171 140026050483968 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.3092997074127197, loss=2.750213146209717
I0127 00:12:47.590732 140026067269376 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.7333874702453613, loss=2.7511839866638184
I0127 00:13:21.543348 140026050483968 logging_writer.py:48] [52400] global_step=52400, grad_norm=2.8515827655792236, loss=2.6533257961273193
I0127 00:13:55.533519 140026067269376 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.255620002746582, loss=2.661669969558716
I0127 00:14:04.500186 140187804313408 spec.py:321] Evaluating on the training split.
I0127 00:14:10.725911 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 00:14:20.710355 140187804313408 spec.py:349] Evaluating on the test split.
I0127 00:14:23.016189 140187804313408 submission_runner.py:408] Time since start: 18648.10s, 	Step: 52528, 	{'train/accuracy': 0.6692044138908386, 'train/loss': 1.4314494132995605, 'validation/accuracy': 0.6190999746322632, 'validation/loss': 1.6480151414871216, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.28476881980896, 'test/num_examples': 10000, 'score': 17904.098637342453, 'total_duration': 18648.099063634872, 'accumulated_submission_time': 17904.098637342453, 'accumulated_eval_time': 740.029061794281, 'accumulated_logging_time': 2.0966885089874268}
I0127 00:14:23.042519 140026151130880 logging_writer.py:48] [52528] accumulated_eval_time=740.029062, accumulated_logging_time=2.096689, accumulated_submission_time=17904.098637, global_step=52528, preemption_count=0, score=17904.098637, test/accuracy=0.499900, test/loss=2.284769, test/num_examples=10000, total_duration=18648.099064, train/accuracy=0.669204, train/loss=1.431449, validation/accuracy=0.619100, validation/loss=1.648015, validation/num_examples=50000
I0127 00:14:47.723339 140026159523584 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.4586844444274902, loss=2.7777087688446045
I0127 00:15:21.631832 140026151130880 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.205151081085205, loss=2.750683546066284
I0127 00:15:55.547423 140026159523584 logging_writer.py:48] [52800] global_step=52800, grad_norm=2.68422269821167, loss=2.714573621749878
I0127 00:16:29.463696 140026151130880 logging_writer.py:48] [52900] global_step=52900, grad_norm=3.451950788497925, loss=2.832681894302368
I0127 00:17:03.377898 140026159523584 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.203559637069702, loss=2.8047704696655273
I0127 00:17:37.274703 140026151130880 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.452735185623169, loss=2.6851234436035156
I0127 00:18:11.168452 140026159523584 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.251471757888794, loss=2.800870418548584
I0127 00:18:45.114697 140026151130880 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.214993715286255, loss=2.6991419792175293
I0127 00:19:19.003309 140026159523584 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.204190254211426, loss=2.7495481967926025
I0127 00:19:52.948462 140026151130880 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.0964176654815674, loss=2.7319023609161377
I0127 00:20:26.944610 140026159523584 logging_writer.py:48] [53600] global_step=53600, grad_norm=2.96770977973938, loss=2.752589702606201
I0127 00:21:00.888627 140026151130880 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.266646385192871, loss=2.771179676055908
I0127 00:21:34.851920 140026159523584 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.0557703971862793, loss=2.712115526199341
I0127 00:22:08.794879 140026151130880 logging_writer.py:48] [53900] global_step=53900, grad_norm=2.8575730323791504, loss=2.74263072013855
I0127 00:22:42.732892 140026159523584 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.756385087966919, loss=2.708167314529419
I0127 00:22:53.043467 140187804313408 spec.py:321] Evaluating on the training split.
I0127 00:22:59.236852 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 00:23:08.053698 140187804313408 spec.py:349] Evaluating on the test split.
I0127 00:23:10.367431 140187804313408 submission_runner.py:408] Time since start: 19175.45s, 	Step: 54032, 	{'train/accuracy': 0.6640425324440002, 'train/loss': 1.432912826538086, 'validation/accuracy': 0.620199978351593, 'validation/loss': 1.6460167169570923, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.3091156482696533, 'test/num_examples': 10000, 'score': 18414.036471366882, 'total_duration': 19175.45029401779, 'accumulated_submission_time': 18414.036471366882, 'accumulated_eval_time': 757.3529677391052, 'accumulated_logging_time': 2.1330432891845703}
I0127 00:23:10.394220 140026050483968 logging_writer.py:48] [54032] accumulated_eval_time=757.352968, accumulated_logging_time=2.133043, accumulated_submission_time=18414.036471, global_step=54032, preemption_count=0, score=18414.036471, test/accuracy=0.492300, test/loss=2.309116, test/num_examples=10000, total_duration=19175.450294, train/accuracy=0.664043, train/loss=1.432913, validation/accuracy=0.620200, validation/loss=1.646017, validation/num_examples=50000
I0127 00:23:33.799790 140026067269376 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.5948686599731445, loss=2.7819416522979736
I0127 00:24:07.714547 140026050483968 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.2789723873138428, loss=2.7176756858825684
I0127 00:24:41.633414 140026067269376 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.0523135662078857, loss=2.714325189590454
I0127 00:25:15.582252 140026050483968 logging_writer.py:48] [54400] global_step=54400, grad_norm=2.9051713943481445, loss=2.7838659286499023
I0127 00:25:49.547361 140026067269376 logging_writer.py:48] [54500] global_step=54500, grad_norm=2.967127799987793, loss=2.6981499195098877
I0127 00:26:23.536224 140026050483968 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.093212842941284, loss=2.72167706489563
I0127 00:26:57.474772 140026067269376 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.4643642902374268, loss=2.754133462905884
I0127 00:27:31.416188 140026050483968 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.3194055557250977, loss=2.7265563011169434
I0127 00:28:05.365905 140026067269376 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.4760329723358154, loss=2.752241611480713
I0127 00:28:39.318252 140026050483968 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.061716318130493, loss=2.6571598052978516
I0127 00:29:13.273953 140026067269376 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.194507360458374, loss=2.764235019683838
I0127 00:29:47.212209 140026050483968 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.964033365249634, loss=2.639122247695923
I0127 00:30:21.132765 140026067269376 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.1387667655944824, loss=2.6591854095458984
I0127 00:30:55.096513 140026050483968 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.309734582901001, loss=2.660569429397583
I0127 00:31:29.036351 140026067269376 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.0764005184173584, loss=2.6934046745300293
I0127 00:31:40.380161 140187804313408 spec.py:321] Evaluating on the training split.
I0127 00:31:46.559638 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 00:31:55.470037 140187804313408 spec.py:349] Evaluating on the test split.
I0127 00:31:57.843243 140187804313408 submission_runner.py:408] Time since start: 19702.93s, 	Step: 55535, 	{'train/accuracy': 0.6664939522743225, 'train/loss': 1.427367925643921, 'validation/accuracy': 0.6240000128746033, 'validation/loss': 1.622045636177063, 'validation/num_examples': 50000, 'test/accuracy': 0.5022000074386597, 'test/loss': 2.2728426456451416, 'test/num_examples': 10000, 'score': 18923.95730996132, 'total_duration': 19702.92580795288, 'accumulated_submission_time': 18923.95730996132, 'accumulated_eval_time': 774.8156905174255, 'accumulated_logging_time': 2.171757459640503}
I0127 00:31:57.869327 140026050483968 logging_writer.py:48] [55535] accumulated_eval_time=774.815691, accumulated_logging_time=2.171757, accumulated_submission_time=18923.957310, global_step=55535, preemption_count=0, score=18923.957310, test/accuracy=0.502200, test/loss=2.272843, test/num_examples=10000, total_duration=19702.925808, train/accuracy=0.666494, train/loss=1.427368, validation/accuracy=0.624000, validation/loss=1.622046, validation/num_examples=50000
I0127 00:32:20.229191 140026058876672 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.014796018600464, loss=2.7113165855407715
I0127 00:32:54.159630 140026050483968 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.7021398544311523, loss=2.7263948917388916
I0127 00:33:28.095418 140026058876672 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.579848527908325, loss=2.678743600845337
I0127 00:34:02.017795 140026050483968 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.379204511642456, loss=2.7334108352661133
I0127 00:34:35.927974 140026058876672 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.9614310264587402, loss=2.703138828277588
I0127 00:35:09.875064 140026050483968 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.3290352821350098, loss=2.764950752258301
I0127 00:35:43.799105 140026058876672 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.682171583175659, loss=2.757309913635254
I0127 00:36:17.703095 140026050483968 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.2066402435302734, loss=2.6621532440185547
I0127 00:36:51.636110 140026058876672 logging_writer.py:48] [56400] global_step=56400, grad_norm=2.9474401473999023, loss=2.6396360397338867
I0127 00:37:25.518580 140026050483968 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.9304609298706055, loss=2.722999095916748
I0127 00:37:59.437768 140026058876672 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.011320114135742, loss=2.7023916244506836
I0127 00:38:33.369584 140026050483968 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.0909290313720703, loss=2.684102773666382
I0127 00:39:07.281450 140026058876672 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.21927809715271, loss=2.748598337173462
I0127 00:39:41.165967 140026050483968 logging_writer.py:48] [56900] global_step=56900, grad_norm=2.9237983226776123, loss=2.7965524196624756
I0127 00:40:15.098763 140026058876672 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.243922710418701, loss=2.7788589000701904
I0127 00:40:28.096261 140187804313408 spec.py:321] Evaluating on the training split.
I0127 00:40:34.372045 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 00:40:43.253633 140187804313408 spec.py:349] Evaluating on the test split.
I0127 00:40:45.519670 140187804313408 submission_runner.py:408] Time since start: 20230.60s, 	Step: 57040, 	{'train/accuracy': 0.7063934803009033, 'train/loss': 1.2701016664505005, 'validation/accuracy': 0.6281599998474121, 'validation/loss': 1.6143368482589722, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2856898307800293, 'test/num_examples': 10000, 'score': 19434.118763685226, 'total_duration': 20230.6025326252, 'accumulated_submission_time': 19434.118763685226, 'accumulated_eval_time': 792.2390511035919, 'accumulated_logging_time': 2.209519863128662}
I0127 00:40:45.550351 140026050483968 logging_writer.py:48] [57040] accumulated_eval_time=792.239051, accumulated_logging_time=2.209520, accumulated_submission_time=19434.118764, global_step=57040, preemption_count=0, score=19434.118764, test/accuracy=0.505400, test/loss=2.285690, test/num_examples=10000, total_duration=20230.602533, train/accuracy=0.706393, train/loss=1.270102, validation/accuracy=0.628160, validation/loss=1.614337, validation/num_examples=50000
I0127 00:41:06.219412 140026058876672 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.082603693008423, loss=2.649313449859619
I0127 00:41:40.142824 140026050483968 logging_writer.py:48] [57200] global_step=57200, grad_norm=2.8929972648620605, loss=2.6150870323181152
I0127 00:42:14.067777 140026058876672 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.8769047260284424, loss=2.6960830688476562
I0127 00:42:47.989251 140026050483968 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.688016891479492, loss=2.7678208351135254
I0127 00:43:21.915270 140026058876672 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.153291940689087, loss=2.7495834827423096
I0127 00:43:55.893194 140026050483968 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.049027681350708, loss=2.7008543014526367
I0127 00:44:29.848659 140026058876672 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.0736124515533447, loss=2.7347774505615234
I0127 00:45:03.805752 140026050483968 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.265084981918335, loss=2.6883084774017334
I0127 00:45:37.738218 140026058876672 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.5345609188079834, loss=2.744222640991211
I0127 00:46:11.683994 140026050483968 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.027301073074341, loss=2.7310690879821777
I0127 00:46:45.620417 140026058876672 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.1523666381835938, loss=2.7945964336395264
I0127 00:47:19.549156 140026050483968 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.918022632598877, loss=2.8537731170654297
I0127 00:47:53.492515 140026058876672 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.8544814586639404, loss=2.7248172760009766
I0127 00:48:27.433873 140026050483968 logging_writer.py:48] [58400] global_step=58400, grad_norm=4.219939708709717, loss=2.659088134765625
I0127 00:49:01.402684 140026058876672 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.002284049987793, loss=2.7275960445404053
I0127 00:49:15.805930 140187804313408 spec.py:321] Evaluating on the training split.
I0127 00:49:22.010709 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 00:49:30.859864 140187804313408 spec.py:349] Evaluating on the test split.
I0127 00:49:33.171478 140187804313408 submission_runner.py:408] Time since start: 20758.25s, 	Step: 58544, 	{'train/accuracy': 0.6819196343421936, 'train/loss': 1.3621110916137695, 'validation/accuracy': 0.6212799549102783, 'validation/loss': 1.638480544090271, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.3111507892608643, 'test/num_examples': 10000, 'score': 19944.30988574028, 'total_duration': 20758.254354953766, 'accumulated_submission_time': 19944.30988574028, 'accumulated_eval_time': 809.6045508384705, 'accumulated_logging_time': 2.2512757778167725}
I0127 00:49:33.200496 140026050483968 logging_writer.py:48] [58544] accumulated_eval_time=809.604551, accumulated_logging_time=2.251276, accumulated_submission_time=19944.309886, global_step=58544, preemption_count=0, score=19944.309886, test/accuracy=0.492800, test/loss=2.311151, test/num_examples=10000, total_duration=20758.254355, train/accuracy=0.681920, train/loss=1.362111, validation/accuracy=0.621280, validation/loss=1.638481, validation/num_examples=50000
I0127 00:49:52.519208 140026151130880 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.9981913566589355, loss=2.5930192470550537
I0127 00:50:26.403491 140026050483968 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.8551783561706543, loss=2.6647417545318604
I0127 00:51:00.513423 140026151130880 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.0630171298980713, loss=2.6146326065063477
I0127 00:51:34.452416 140026050483968 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.324618101119995, loss=2.710759162902832
I0127 00:52:08.374030 140026151130880 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.116584300994873, loss=2.6685075759887695
I0127 00:52:42.292326 140026050483968 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.812685489654541, loss=2.647653579711914
I0127 00:53:16.218011 140026151130880 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.117673635482788, loss=2.845731019973755
I0127 00:53:50.094191 140026050483968 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.991053819656372, loss=2.6080784797668457
I0127 00:54:24.022063 140026151130880 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.1278271675109863, loss=2.7686684131622314
I0127 00:54:57.946179 140026050483968 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.5513038635253906, loss=2.7387824058532715
I0127 00:55:31.885093 140026151130880 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.076677083969116, loss=2.712144613265991
I0127 00:56:05.825714 140026050483968 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.9479010105133057, loss=2.6041085720062256
I0127 00:56:39.749819 140026151130880 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.8138623237609863, loss=2.65183162689209
I0127 00:57:13.795377 140026050483968 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.9029693603515625, loss=2.719998836517334
I0127 00:57:47.716447 140026151130880 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.2786078453063965, loss=2.655261993408203
I0127 00:58:03.463349 140187804313408 spec.py:321] Evaluating on the training split.
I0127 00:58:09.653749 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 00:58:18.429791 140187804313408 spec.py:349] Evaluating on the test split.
I0127 00:58:20.724724 140187804313408 submission_runner.py:408] Time since start: 21285.81s, 	Step: 60048, 	{'train/accuracy': 0.6807836294174194, 'train/loss': 1.359082579612732, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.601298213005066, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.2946648597717285, 'test/num_examples': 10000, 'score': 20454.509740829468, 'total_duration': 21285.80756020546, 'accumulated_submission_time': 20454.509740829468, 'accumulated_eval_time': 826.8658409118652, 'accumulated_logging_time': 2.2905819416046143}
I0127 00:58:20.759809 140026075662080 logging_writer.py:48] [60048] accumulated_eval_time=826.865841, accumulated_logging_time=2.290582, accumulated_submission_time=20454.509741, global_step=60048, preemption_count=0, score=20454.509741, test/accuracy=0.505300, test/loss=2.294665, test/num_examples=10000, total_duration=21285.807560, train/accuracy=0.680784, train/loss=1.359083, validation/accuracy=0.632200, validation/loss=1.601298, validation/num_examples=50000
I0127 00:58:38.741487 140026167916288 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.6456024646759033, loss=2.757594585418701
I0127 00:59:12.626785 140026075662080 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.752467393875122, loss=2.659454822540283
I0127 00:59:46.498107 140026167916288 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.669647455215454, loss=2.710294246673584
I0127 01:00:20.431307 140026075662080 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.781740665435791, loss=2.6741175651550293
I0127 01:00:54.370972 140026167916288 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.023190498352051, loss=2.60707426071167
I0127 01:01:28.273122 140026075662080 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.6947011947631836, loss=2.7088396549224854
I0127 01:02:02.236944 140026167916288 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.0431854724884033, loss=2.624305009841919
I0127 01:02:36.179205 140026075662080 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.3973093032836914, loss=2.7532522678375244
I0127 01:03:10.161938 140026167916288 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.2015864849090576, loss=2.7196669578552246
I0127 01:03:44.085694 140026075662080 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.0483951568603516, loss=2.6775736808776855
I0127 01:04:18.005023 140026167916288 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.2760322093963623, loss=2.6275882720947266
I0127 01:04:51.923934 140026075662080 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.5806217193603516, loss=2.7320404052734375
I0127 01:05:25.823916 140026167916288 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.534583806991577, loss=2.6741480827331543
I0127 01:05:59.770332 140026075662080 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.2606029510498047, loss=2.6975255012512207
I0127 01:06:33.678449 140026167916288 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.2622125148773193, loss=2.7830636501312256
I0127 01:06:50.782810 140187804313408 spec.py:321] Evaluating on the training split.
I0127 01:06:56.969102 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 01:07:05.866582 140187804313408 spec.py:349] Evaluating on the test split.
I0127 01:07:08.175613 140187804313408 submission_runner.py:408] Time since start: 21813.26s, 	Step: 61552, 	{'train/accuracy': 0.6801857352256775, 'train/loss': 1.3753252029418945, 'validation/accuracy': 0.6317799687385559, 'validation/loss': 1.5977727174758911, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2585179805755615, 'test/num_examples': 10000, 'score': 20964.469042301178, 'total_duration': 21813.258487939835, 'accumulated_submission_time': 20964.469042301178, 'accumulated_eval_time': 844.2586009502411, 'accumulated_logging_time': 2.3358445167541504}
I0127 01:07:08.205985 140026042091264 logging_writer.py:48] [61552] accumulated_eval_time=844.258601, accumulated_logging_time=2.335845, accumulated_submission_time=20964.469042, global_step=61552, preemption_count=0, score=20964.469042, test/accuracy=0.504100, test/loss=2.258518, test/num_examples=10000, total_duration=21813.258488, train/accuracy=0.680186, train/loss=1.375325, validation/accuracy=0.631780, validation/loss=1.597773, validation/num_examples=50000
I0127 01:07:24.784556 140026050483968 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.248789072036743, loss=2.745004177093506
I0127 01:07:58.642745 140026042091264 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.0049264430999756, loss=2.612414836883545
I0127 01:08:32.560534 140026050483968 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.426759958267212, loss=2.675168991088867
I0127 01:09:06.461932 140026042091264 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.4410648345947266, loss=2.6523637771606445
I0127 01:09:40.417131 140026050483968 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.9884183406829834, loss=2.665860176086426
I0127 01:10:14.332655 140026042091264 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.0617828369140625, loss=2.7061767578125
I0127 01:10:48.226141 140026050483968 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.0617988109588623, loss=2.7062742710113525
I0127 01:11:22.134760 140026042091264 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.1230270862579346, loss=2.654351234436035
I0127 01:11:56.035392 140026050483968 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.482586145401001, loss=2.705143690109253
I0127 01:12:29.949124 140026042091264 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.437603235244751, loss=2.6978085041046143
I0127 01:13:03.879262 140026050483968 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.1461806297302246, loss=2.62674617767334
I0127 01:13:37.816885 140026042091264 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.537572145462036, loss=2.8750619888305664
I0127 01:14:11.736118 140026050483968 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.901327610015869, loss=2.6276447772979736
I0127 01:14:45.653202 140026042091264 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.274362802505493, loss=2.6325433254241943
I0127 01:15:19.570930 140026050483968 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.525094509124756, loss=2.78941011428833
I0127 01:15:38.418308 140187804313408 spec.py:321] Evaluating on the training split.
I0127 01:15:44.640634 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 01:15:53.456078 140187804313408 spec.py:349] Evaluating on the test split.
I0127 01:15:55.761182 140187804313408 submission_runner.py:408] Time since start: 22340.84s, 	Step: 63057, 	{'train/accuracy': 0.6864038705825806, 'train/loss': 1.3631954193115234, 'validation/accuracy': 0.6362400054931641, 'validation/loss': 1.5933818817138672, 'validation/num_examples': 50000, 'test/accuracy': 0.5072000026702881, 'test/loss': 2.271667242050171, 'test/num_examples': 10000, 'score': 21474.617376327515, 'total_duration': 22340.84404706955, 'accumulated_submission_time': 21474.617376327515, 'accumulated_eval_time': 861.6014168262482, 'accumulated_logging_time': 2.3771815299987793}
I0127 01:15:55.790409 140026159523584 logging_writer.py:48] [63057] accumulated_eval_time=861.601417, accumulated_logging_time=2.377182, accumulated_submission_time=21474.617376, global_step=63057, preemption_count=0, score=21474.617376, test/accuracy=0.507200, test/loss=2.271667, test/num_examples=10000, total_duration=22340.844047, train/accuracy=0.686404, train/loss=1.363195, validation/accuracy=0.636240, validation/loss=1.593382, validation/num_examples=50000
I0127 01:16:10.669422 140026167916288 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.244642734527588, loss=2.6693806648254395
I0127 01:16:44.528767 140026159523584 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.1844990253448486, loss=2.6679232120513916
I0127 01:17:18.450186 140026167916288 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.193077802658081, loss=2.621316432952881
I0127 01:17:52.338057 140026159523584 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.392543315887451, loss=2.724003314971924
I0127 01:18:26.270978 140026167916288 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.11537504196167, loss=2.579846143722534
I0127 01:19:00.209241 140026159523584 logging_writer.py:48] [63600] global_step=63600, grad_norm=2.9328243732452393, loss=2.542722225189209
I0127 01:19:34.106715 140026167916288 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.790407180786133, loss=2.6804935932159424
I0127 01:20:08.047547 140026159523584 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.5883731842041016, loss=2.700181007385254
I0127 01:20:41.983596 140026167916288 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.260805368423462, loss=2.5591249465942383
I0127 01:21:15.876168 140026159523584 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.904658794403076, loss=2.7015762329101562
I0127 01:21:49.915183 140026167916288 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.6646158695220947, loss=2.7861924171447754
I0127 01:22:23.832008 140026159523584 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.3013715744018555, loss=2.663569450378418
I0127 01:22:57.747598 140026167916288 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.1256632804870605, loss=2.6276211738586426
I0127 01:23:31.628089 140026159523584 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.2637152671813965, loss=2.635380268096924
I0127 01:24:05.553115 140026167916288 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.7015392780303955, loss=2.7956786155700684
I0127 01:24:26.064126 140187804313408 spec.py:321] Evaluating on the training split.
I0127 01:24:32.400743 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 01:24:41.266734 140187804313408 spec.py:349] Evaluating on the test split.
I0127 01:24:43.538034 140187804313408 submission_runner.py:408] Time since start: 22868.62s, 	Step: 64562, 	{'train/accuracy': 0.6725525856018066, 'train/loss': 1.4098135232925415, 'validation/accuracy': 0.6275399923324585, 'validation/loss': 1.6258821487426758, 'validation/num_examples': 50000, 'test/accuracy': 0.4985000193119049, 'test/loss': 2.332926034927368, 'test/num_examples': 10000, 'score': 21984.827194929123, 'total_duration': 22868.620908498764, 'accumulated_submission_time': 21984.827194929123, 'accumulated_eval_time': 879.075288772583, 'accumulated_logging_time': 2.4166557788848877}
I0127 01:24:43.566650 140026075662080 logging_writer.py:48] [64562] accumulated_eval_time=879.075289, accumulated_logging_time=2.416656, accumulated_submission_time=21984.827195, global_step=64562, preemption_count=0, score=21984.827195, test/accuracy=0.498500, test/loss=2.332926, test/num_examples=10000, total_duration=22868.620908, train/accuracy=0.672553, train/loss=1.409814, validation/accuracy=0.627540, validation/loss=1.625882, validation/num_examples=50000
I0127 01:24:56.784281 140026151130880 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.544548749923706, loss=2.6144232749938965
I0127 01:25:30.641467 140026075662080 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.684391975402832, loss=2.6968252658843994
I0127 01:26:04.507346 140026151130880 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.009641170501709, loss=2.594648838043213
I0127 01:26:38.416449 140026075662080 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.8196611404418945, loss=2.70879864692688
I0127 01:27:12.312980 140026151130880 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.361577272415161, loss=2.6283161640167236
I0127 01:27:46.358539 140026075662080 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.845688819885254, loss=2.7226266860961914
I0127 01:28:20.279288 140026151130880 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.8335444927215576, loss=2.57004451751709
I0127 01:28:54.169363 140026075662080 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.230494499206543, loss=2.776362657546997
I0127 01:29:28.093546 140026151130880 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.2450368404388428, loss=2.6433849334716797
I0127 01:30:02.056041 140026075662080 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.1729860305786133, loss=2.569810390472412
I0127 01:30:35.960475 140026151130880 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.7269513607025146, loss=2.7094531059265137
I0127 01:31:09.883351 140026075662080 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.2731146812438965, loss=2.762643814086914
I0127 01:31:43.798716 140026151130880 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.473679780960083, loss=2.623438596725464
I0127 01:32:17.699291 140026075662080 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.662822723388672, loss=2.646357536315918
I0127 01:32:51.614386 140026151130880 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.0366599559783936, loss=2.6356582641601562
I0127 01:33:13.762884 140187804313408 spec.py:321] Evaluating on the training split.
I0127 01:33:19.899000 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 01:33:28.910558 140187804313408 spec.py:349] Evaluating on the test split.
I0127 01:33:31.279705 140187804313408 submission_runner.py:408] Time since start: 23396.36s, 	Step: 66067, 	{'train/accuracy': 0.7122927308082581, 'train/loss': 1.2576779127120972, 'validation/accuracy': 0.624459981918335, 'validation/loss': 1.6419285535812378, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.2767369747161865, 'test/num_examples': 10000, 'score': 22494.960673332214, 'total_duration': 23396.362580537796, 'accumulated_submission_time': 22494.960673332214, 'accumulated_eval_time': 896.5920617580414, 'accumulated_logging_time': 2.4555139541625977}
I0127 01:33:31.308941 140026058876672 logging_writer.py:48] [66067] accumulated_eval_time=896.592062, accumulated_logging_time=2.455514, accumulated_submission_time=22494.960673, global_step=66067, preemption_count=0, score=22494.960673, test/accuracy=0.506200, test/loss=2.276737, test/num_examples=10000, total_duration=23396.362581, train/accuracy=0.712293, train/loss=1.257678, validation/accuracy=0.624460, validation/loss=1.641929, validation/num_examples=50000
I0127 01:33:42.826009 140026067269376 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.2469234466552734, loss=2.6752800941467285
I0127 01:34:16.699702 140026058876672 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.6469788551330566, loss=2.6390113830566406
I0127 01:34:50.607763 140026067269376 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.0496160984039307, loss=2.6826066970825195
I0127 01:35:24.516854 140026058876672 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.447437047958374, loss=2.7742443084716797
I0127 01:35:58.446200 140026067269376 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.568798065185547, loss=2.640340805053711
I0127 01:36:32.379784 140026058876672 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.114886999130249, loss=2.6661031246185303
I0127 01:37:06.296926 140026067269376 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.3941915035247803, loss=2.6380608081817627
I0127 01:37:40.213705 140026058876672 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.093992233276367, loss=2.578002691268921
I0127 01:38:14.134437 140026067269376 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.372202157974243, loss=2.7115979194641113
I0127 01:38:48.057581 140026058876672 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.2082531452178955, loss=2.593729019165039
I0127 01:39:21.988190 140026067269376 logging_writer.py:48] [67100] global_step=67100, grad_norm=4.0441765785217285, loss=2.702792167663574
I0127 01:39:55.898265 140026058876672 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.204568386077881, loss=2.67759370803833
I0127 01:40:29.852527 140026067269376 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.3273119926452637, loss=2.6770925521850586
I0127 01:41:03.753115 140026058876672 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.0767619609832764, loss=2.5691356658935547
I0127 01:41:37.679968 140026067269376 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.3025994300842285, loss=2.6573703289031982
I0127 01:42:01.608864 140187804313408 spec.py:321] Evaluating on the training split.
I0127 01:42:08.146270 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 01:42:16.828633 140187804313408 spec.py:349] Evaluating on the test split.
I0127 01:42:19.120901 140187804313408 submission_runner.py:408] Time since start: 23924.20s, 	Step: 67572, 	{'train/accuracy': 0.6955117583274841, 'train/loss': 1.2980284690856934, 'validation/accuracy': 0.6328999996185303, 'validation/loss': 1.5780168771743774, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.217724323272705, 'test/num_examples': 10000, 'score': 23005.19539809227, 'total_duration': 23924.20377588272, 'accumulated_submission_time': 23005.19539809227, 'accumulated_eval_time': 914.1040511131287, 'accumulated_logging_time': 2.495955228805542}
I0127 01:42:19.149885 140026050483968 logging_writer.py:48] [67572] accumulated_eval_time=914.104051, accumulated_logging_time=2.495955, accumulated_submission_time=23005.195398, global_step=67572, preemption_count=0, score=23005.195398, test/accuracy=0.518600, test/loss=2.217724, test/num_examples=10000, total_duration=23924.203776, train/accuracy=0.695512, train/loss=1.298028, validation/accuracy=0.632900, validation/loss=1.578017, validation/num_examples=50000
I0127 01:42:28.961789 140026058876672 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.1223483085632324, loss=2.6815977096557617
I0127 01:43:02.817644 140026050483968 logging_writer.py:48] [67700] global_step=67700, grad_norm=4.075494766235352, loss=2.639185667037964
I0127 01:43:36.716194 140026058876672 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.3652291297912598, loss=2.747851848602295
I0127 01:44:10.621409 140026050483968 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.255289316177368, loss=2.7137632369995117
I0127 01:44:44.506856 140026058876672 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.3386638164520264, loss=2.6419732570648193
I0127 01:45:18.399532 140026050483968 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.9620962142944336, loss=2.6873555183410645
I0127 01:45:52.327299 140026058876672 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.905912160873413, loss=2.5368056297302246
I0127 01:46:26.290539 140026050483968 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.071155309677124, loss=2.6607015132904053
I0127 01:47:00.170687 140026058876672 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.0665454864501953, loss=2.6578969955444336
I0127 01:47:34.091035 140026050483968 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.541966438293457, loss=2.578014850616455
I0127 01:48:07.991779 140026058876672 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.2197601795196533, loss=2.6315996646881104
I0127 01:48:41.909763 140026050483968 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.182565450668335, loss=2.6669223308563232
I0127 01:49:15.799286 140026058876672 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.1133408546447754, loss=2.615934133529663
I0127 01:49:49.698103 140026050483968 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.1655051708221436, loss=2.664573907852173
I0127 01:50:23.614867 140026058876672 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.238471269607544, loss=2.65963077545166
I0127 01:50:49.167216 140187804313408 spec.py:321] Evaluating on the training split.
I0127 01:50:55.214911 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 01:51:05.203550 140187804313408 spec.py:349] Evaluating on the test split.
I0127 01:51:08.141237 140187804313408 submission_runner.py:408] Time since start: 24453.22s, 	Step: 69077, 	{'train/accuracy': 0.6938177347183228, 'train/loss': 1.3356457948684692, 'validation/accuracy': 0.634719967842102, 'validation/loss': 1.5932949781417847, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.25681209564209, 'test/num_examples': 10000, 'score': 23515.14908361435, 'total_duration': 24453.224115133286, 'accumulated_submission_time': 23515.14908361435, 'accumulated_eval_time': 933.078031539917, 'accumulated_logging_time': 2.535719633102417}
I0127 01:51:08.170208 140026159523584 logging_writer.py:48] [69077] accumulated_eval_time=933.078032, accumulated_logging_time=2.535720, accumulated_submission_time=23515.149084, global_step=69077, preemption_count=0, score=23515.149084, test/accuracy=0.508200, test/loss=2.256812, test/num_examples=10000, total_duration=24453.224115, train/accuracy=0.693818, train/loss=1.335646, validation/accuracy=0.634720, validation/loss=1.593295, validation/num_examples=50000
I0127 01:51:16.302064 140026167916288 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.591066360473633, loss=2.60904860496521
I0127 01:51:50.166507 140026159523584 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.3435404300689697, loss=2.544921875
I0127 01:52:24.132057 140026167916288 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.1493313312530518, loss=2.6701369285583496
I0127 01:52:58.057305 140026159523584 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.529019355773926, loss=2.7039742469787598
I0127 01:53:31.930857 140026167916288 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.763711452484131, loss=2.6991543769836426
I0127 01:54:05.850054 140026159523584 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.38374924659729, loss=2.6815361976623535
I0127 01:54:39.740232 140026167916288 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.7880403995513916, loss=2.646928548812866
I0127 01:55:13.655321 140026159523584 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.203237295150757, loss=2.689397096633911
I0127 01:55:47.595418 140026167916288 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.5385608673095703, loss=2.586061477661133
I0127 01:56:21.505261 140026159523584 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.008498191833496, loss=2.640040636062622
I0127 01:56:55.440201 140026167916288 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.8031210899353027, loss=2.6671366691589355
I0127 01:57:29.349905 140026159523584 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.951136589050293, loss=2.566767692565918
I0127 01:58:03.277800 140026167916288 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.5862298011779785, loss=2.6076762676239014
I0127 01:58:37.239640 140026159523584 logging_writer.py:48] [70400] global_step=70400, grad_norm=4.3538923263549805, loss=2.6966145038604736
I0127 01:59:11.117032 140026167916288 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.618600368499756, loss=2.5996761322021484
I0127 01:59:38.397790 140187804313408 spec.py:321] Evaluating on the training split.
I0127 01:59:44.615988 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 01:59:54.545114 140187804313408 spec.py:349] Evaluating on the test split.
I0127 01:59:56.828949 140187804313408 submission_runner.py:408] Time since start: 24981.91s, 	Step: 70582, 	{'train/accuracy': 0.6916653513908386, 'train/loss': 1.3394980430603027, 'validation/accuracy': 0.6408199667930603, 'validation/loss': 1.581676959991455, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2319271564483643, 'test/num_examples': 10000, 'score': 24025.312272787094, 'total_duration': 24981.911822795868, 'accumulated_submission_time': 24025.312272787094, 'accumulated_eval_time': 951.5091438293457, 'accumulated_logging_time': 2.5747079849243164}
I0127 01:59:56.861335 140026067269376 logging_writer.py:48] [70582] accumulated_eval_time=951.509144, accumulated_logging_time=2.574708, accumulated_submission_time=24025.312273, global_step=70582, preemption_count=0, score=24025.312273, test/accuracy=0.516300, test/loss=2.231927, test/num_examples=10000, total_duration=24981.911823, train/accuracy=0.691665, train/loss=1.339498, validation/accuracy=0.640820, validation/loss=1.581677, validation/num_examples=50000
I0127 02:00:03.326748 140026075662080 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.267181396484375, loss=2.6539173126220703
I0127 02:00:37.180073 140026067269376 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.543714761734009, loss=2.702731132507324
I0127 02:01:11.064679 140026075662080 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.435297966003418, loss=2.6899614334106445
I0127 02:01:44.963584 140026067269376 logging_writer.py:48] [70900] global_step=70900, grad_norm=4.086978912353516, loss=2.723236322402954
I0127 02:02:18.878400 140026075662080 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.3283631801605225, loss=2.609365224838257
I0127 02:02:52.832660 140026067269376 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.130284309387207, loss=2.5152599811553955
I0127 02:03:26.724848 140026075662080 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.6658406257629395, loss=2.643686294555664
I0127 02:04:00.653425 140026067269376 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.290644407272339, loss=2.708667039871216
I0127 02:04:34.577534 140026075662080 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.2595419883728027, loss=2.608030319213867
I0127 02:05:08.565682 140026067269376 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.849130868911743, loss=2.712986946105957
I0127 02:05:42.470519 140026075662080 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.9901061058044434, loss=2.7113349437713623
I0127 02:06:16.373808 140026067269376 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.238090991973877, loss=2.667545795440674
I0127 02:06:50.294856 140026075662080 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.667557954788208, loss=2.5920958518981934
I0127 02:07:24.210880 140026067269376 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.2478041648864746, loss=2.664090871810913
I0127 02:07:58.137362 140026075662080 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.5065066814422607, loss=2.5706541538238525
I0127 02:08:27.127758 140187804313408 spec.py:321] Evaluating on the training split.
I0127 02:08:33.199563 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 02:08:41.985736 140187804313408 spec.py:349] Evaluating on the test split.
I0127 02:08:44.351960 140187804313408 submission_runner.py:408] Time since start: 25509.43s, 	Step: 72087, 	{'train/accuracy': 0.6868821382522583, 'train/loss': 1.351656198501587, 'validation/accuracy': 0.6393600106239319, 'validation/loss': 1.5750479698181152, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.226827621459961, 'test/num_examples': 10000, 'score': 24535.51449584961, 'total_duration': 25509.434837818146, 'accumulated_submission_time': 24535.51449584961, 'accumulated_eval_time': 968.7333037853241, 'accumulated_logging_time': 2.617192506790161}
I0127 02:08:44.383040 140026042091264 logging_writer.py:48] [72087] accumulated_eval_time=968.733304, accumulated_logging_time=2.617193, accumulated_submission_time=24535.514496, global_step=72087, preemption_count=0, score=24535.514496, test/accuracy=0.518500, test/loss=2.226828, test/num_examples=10000, total_duration=25509.434838, train/accuracy=0.686882, train/loss=1.351656, validation/accuracy=0.639360, validation/loss=1.575048, validation/num_examples=50000
I0127 02:08:49.141151 140026050483968 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.057405710220337, loss=2.6481382846832275
I0127 02:09:23.003746 140026042091264 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.313861131668091, loss=2.638667345046997
I0127 02:09:56.911596 140026050483968 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.7046265602111816, loss=2.673191785812378
I0127 02:10:30.773192 140026042091264 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.9596898555755615, loss=2.701198101043701
I0127 02:11:04.744917 140026050483968 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.1946048736572266, loss=2.622331142425537
I0127 02:11:38.638622 140026042091264 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.448103427886963, loss=2.552873134613037
I0127 02:12:12.565960 140026050483968 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.7814900875091553, loss=2.5614070892333984
I0127 02:12:46.443657 140026042091264 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.3175435066223145, loss=2.6292314529418945
I0127 02:13:20.377964 140026050483968 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.1049649715423584, loss=2.5624442100524902
I0127 02:13:54.278188 140026042091264 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.489882469177246, loss=2.6942834854125977
I0127 02:14:28.214323 140026050483968 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.5837934017181396, loss=2.6611862182617188
I0127 02:15:02.148870 140026042091264 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.2902772426605225, loss=2.6587183475494385
I0127 02:15:36.064170 140026050483968 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.0646297931671143, loss=2.495976686477661
I0127 02:16:09.993225 140026042091264 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.040266990661621, loss=2.5921623706817627
I0127 02:16:43.905349 140026050483968 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.1062750816345215, loss=2.6092605590820312
I0127 02:17:14.625193 140187804313408 spec.py:321] Evaluating on the training split.
I0127 02:17:20.719818 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 02:17:30.110895 140187804313408 spec.py:349] Evaluating on the test split.
I0127 02:17:32.386938 140187804313408 submission_runner.py:408] Time since start: 26037.47s, 	Step: 73592, 	{'train/accuracy': 0.6874202489852905, 'train/loss': 1.3545960187911987, 'validation/accuracy': 0.6374799609184265, 'validation/loss': 1.5858798027038574, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.2407429218292236, 'test/num_examples': 10000, 'score': 25045.693425178528, 'total_duration': 26037.469779729843, 'accumulated_submission_time': 25045.693425178528, 'accumulated_eval_time': 986.4949653148651, 'accumulated_logging_time': 2.6583826541900635}
I0127 02:17:32.420268 140026151130880 logging_writer.py:48] [73592] accumulated_eval_time=986.494965, accumulated_logging_time=2.658383, accumulated_submission_time=25045.693425, global_step=73592, preemption_count=0, score=25045.693425, test/accuracy=0.513700, test/loss=2.240743, test/num_examples=10000, total_duration=26037.469780, train/accuracy=0.687420, train/loss=1.354596, validation/accuracy=0.637480, validation/loss=1.585880, validation/num_examples=50000
I0127 02:17:35.476240 140026167916288 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.344808578491211, loss=2.5232136249542236
I0127 02:18:09.255005 140026151130880 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.1059513092041016, loss=2.5552287101745605
I0127 02:18:43.144240 140026167916288 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.2688651084899902, loss=2.5518293380737305
I0127 02:19:16.999377 140026151130880 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.4654176235198975, loss=2.6930201053619385
I0127 02:19:50.905082 140026167916288 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.121577262878418, loss=2.5391054153442383
I0127 02:20:24.787826 140026151130880 logging_writer.py:48] [74100] global_step=74100, grad_norm=4.064323902130127, loss=2.4908132553100586
I0127 02:20:58.704008 140026167916288 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.7782962322235107, loss=2.715693473815918
I0127 02:21:32.584075 140026151130880 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.011303424835205, loss=2.7054495811462402
I0127 02:22:06.503847 140026167916288 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.3039069175720215, loss=2.572211980819702
I0127 02:22:40.436432 140026151130880 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.4099504947662354, loss=2.6171741485595703
I0127 02:23:14.352659 140026167916288 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.305474042892456, loss=2.781909227371216
I0127 02:23:48.292003 140026151130880 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.849975109100342, loss=2.6771368980407715
I0127 02:24:22.212963 140026167916288 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.518751382827759, loss=2.712284803390503
I0127 02:24:56.095317 140026151130880 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.2771193981170654, loss=2.6196224689483643
I0127 02:25:30.002657 140026167916288 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.2991116046905518, loss=2.6467204093933105
I0127 02:26:02.666676 140187804313408 spec.py:321] Evaluating on the training split.
I0127 02:26:09.353774 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 02:26:18.794364 140187804313408 spec.py:349] Evaluating on the test split.
I0127 02:26:21.084263 140187804313408 submission_runner.py:408] Time since start: 26566.17s, 	Step: 75098, 	{'train/accuracy': 0.7245495915412903, 'train/loss': 1.1946289539337158, 'validation/accuracy': 0.6363799571990967, 'validation/loss': 1.5921239852905273, 'validation/num_examples': 50000, 'test/accuracy': 0.5095000267028809, 'test/loss': 2.269789695739746, 'test/num_examples': 10000, 'score': 25555.87436771393, 'total_duration': 26566.167140960693, 'accumulated_submission_time': 25555.87436771393, 'accumulated_eval_time': 1004.9125220775604, 'accumulated_logging_time': 2.7030186653137207}
I0127 02:26:21.115255 140026042091264 logging_writer.py:48] [75098] accumulated_eval_time=1004.912522, accumulated_logging_time=2.703019, accumulated_submission_time=25555.874368, global_step=75098, preemption_count=0, score=25555.874368, test/accuracy=0.509500, test/loss=2.269790, test/num_examples=10000, total_duration=26566.167141, train/accuracy=0.724550, train/loss=1.194629, validation/accuracy=0.636380, validation/loss=1.592124, validation/num_examples=50000
I0127 02:26:22.132889 140026050483968 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.562553882598877, loss=2.613757371902466
I0127 02:26:56.010733 140026042091264 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.3810935020446777, loss=2.677900552749634
I0127 02:27:29.923205 140026050483968 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.137885093688965, loss=2.6682794094085693
I0127 02:28:03.850269 140026042091264 logging_writer.py:48] [75400] global_step=75400, grad_norm=4.1045379638671875, loss=2.623830795288086
I0127 02:28:37.772074 140026050483968 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.668079376220703, loss=2.657899856567383
I0127 02:29:11.684228 140026042091264 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.3938119411468506, loss=2.6502485275268555
I0127 02:29:45.669003 140026050483968 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.244886875152588, loss=2.606508255004883
I0127 02:30:19.584622 140026042091264 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.970294713973999, loss=2.669276237487793
I0127 02:30:53.520757 140026050483968 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.220000982284546, loss=2.7353532314300537
I0127 02:31:27.413745 140026042091264 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.172062873840332, loss=2.640202283859253
I0127 02:32:01.328975 140026050483968 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.8495094776153564, loss=2.6719810962677
I0127 02:32:35.211627 140026042091264 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.4645628929138184, loss=2.607470989227295
I0127 02:33:09.137407 140026050483968 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.348771810531616, loss=2.5900588035583496
I0127 02:33:43.039779 140026042091264 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.5401127338409424, loss=2.6493518352508545
I0127 02:34:16.956058 140026050483968 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.6133906841278076, loss=2.5862622261047363
I0127 02:34:50.887567 140026042091264 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.4884588718414307, loss=2.633347988128662
I0127 02:34:51.370714 140187804313408 spec.py:321] Evaluating on the training split.
I0127 02:34:57.407284 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 02:35:07.028489 140187804313408 spec.py:349] Evaluating on the test split.
I0127 02:35:09.336093 140187804313408 submission_runner.py:408] Time since start: 27094.42s, 	Step: 76603, 	{'train/accuracy': 0.7009127736091614, 'train/loss': 1.2936850786209106, 'validation/accuracy': 0.6375199556350708, 'validation/loss': 1.5906641483306885, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.208439588546753, 'test/num_examples': 10000, 'score': 26066.066779613495, 'total_duration': 27094.418971776962, 'accumulated_submission_time': 26066.066779613495, 'accumulated_eval_time': 1022.8778517246246, 'accumulated_logging_time': 2.7443735599517822}
I0127 02:35:09.369563 140026151130880 logging_writer.py:48] [76603] accumulated_eval_time=1022.877852, accumulated_logging_time=2.744374, accumulated_submission_time=26066.066780, global_step=76603, preemption_count=0, score=26066.066780, test/accuracy=0.521200, test/loss=2.208440, test/num_examples=10000, total_duration=27094.418972, train/accuracy=0.700913, train/loss=1.293685, validation/accuracy=0.637520, validation/loss=1.590664, validation/num_examples=50000
I0127 02:35:42.732321 140026159523584 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.2312331199645996, loss=2.615069627761841
I0127 02:36:16.601670 140026151130880 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.468165636062622, loss=2.593921184539795
I0127 02:36:50.502742 140026159523584 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.6052136421203613, loss=2.5683257579803467
I0127 02:37:24.608603 140026151130880 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.4288418292999268, loss=2.6480064392089844
I0127 02:37:58.514491 140026159523584 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.351574659347534, loss=2.5921523571014404
I0127 02:38:32.406004 140026151130880 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.634427785873413, loss=2.568321704864502
I0127 02:39:06.344176 140026159523584 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.4900588989257812, loss=2.639336347579956
I0127 02:39:40.254309 140026151130880 logging_writer.py:48] [77400] global_step=77400, grad_norm=4.015191555023193, loss=2.63139271736145
I0127 02:40:14.172682 140026159523584 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.4087908267974854, loss=2.5464584827423096
I0127 02:40:48.087722 140026151130880 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.853940010070801, loss=2.6030209064483643
I0127 02:41:22.004470 140026159523584 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.3662331104278564, loss=2.531700849533081
I0127 02:41:55.989914 140026151130880 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.492783784866333, loss=2.662989616394043
I0127 02:42:29.934536 140026159523584 logging_writer.py:48] [77900] global_step=77900, grad_norm=4.091135501861572, loss=2.7208151817321777
I0127 02:43:03.846420 140026151130880 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.3552825450897217, loss=2.509869337081909
I0127 02:43:37.755578 140026159523584 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.430112838745117, loss=2.5236949920654297
I0127 02:43:39.596604 140187804313408 spec.py:321] Evaluating on the training split.
I0127 02:43:45.657966 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 02:43:54.422390 140187804313408 spec.py:349] Evaluating on the test split.
I0127 02:43:56.703809 140187804313408 submission_runner.py:408] Time since start: 27621.79s, 	Step: 78107, 	{'train/accuracy': 0.7107182741165161, 'train/loss': 1.2404513359069824, 'validation/accuracy': 0.6543399691581726, 'validation/loss': 1.5085474252700806, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.185645580291748, 'test/num_examples': 10000, 'score': 26576.229032993317, 'total_duration': 27621.786662578583, 'accumulated_submission_time': 26576.229032993317, 'accumulated_eval_time': 1039.984982252121, 'accumulated_logging_time': 2.788038969039917}
I0127 02:43:56.735193 140026067269376 logging_writer.py:48] [78107] accumulated_eval_time=1039.984982, accumulated_logging_time=2.788039, accumulated_submission_time=26576.229033, global_step=78107, preemption_count=0, score=26576.229033, test/accuracy=0.525800, test/loss=2.185646, test/num_examples=10000, total_duration=27621.786663, train/accuracy=0.710718, train/loss=1.240451, validation/accuracy=0.654340, validation/loss=1.508547, validation/num_examples=50000
I0127 02:44:28.565879 140026075662080 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.354367971420288, loss=2.6507928371429443
I0127 02:45:02.440468 140026067269376 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.1301326751708984, loss=2.569575309753418
I0127 02:45:36.355998 140026075662080 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.932377815246582, loss=2.6571645736694336
I0127 02:46:10.277166 140026067269376 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.7310221195220947, loss=2.6956844329833984
I0127 02:46:44.204877 140026075662080 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.5258989334106445, loss=2.6060686111450195
I0127 02:47:18.091115 140026067269376 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.6162405014038086, loss=2.539519786834717
I0127 02:47:52.008632 140026075662080 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.7005467414855957, loss=2.580789089202881
I0127 02:48:26.018983 140026067269376 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.585618257522583, loss=2.588550090789795
I0127 02:48:59.901763 140026075662080 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.795912981033325, loss=2.676248073577881
I0127 02:49:33.865289 140026067269376 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.356198310852051, loss=2.5981218814849854
I0127 02:50:07.795852 140026075662080 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.347668409347534, loss=2.5640079975128174
I0127 02:50:41.697782 140026067269376 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.455057382583618, loss=2.5658748149871826
I0127 02:51:15.597258 140026075662080 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.321171522140503, loss=2.550368547439575
I0127 02:51:49.486872 140026067269376 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.4476959705352783, loss=2.6357035636901855
I0127 02:52:23.409627 140026075662080 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.3642046451568604, loss=2.5993919372558594
I0127 02:52:26.946555 140187804313408 spec.py:321] Evaluating on the training split.
I0127 02:52:33.145664 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 02:52:41.869114 140187804313408 spec.py:349] Evaluating on the test split.
I0127 02:52:44.154330 140187804313408 submission_runner.py:408] Time since start: 28149.24s, 	Step: 79612, 	{'train/accuracy': 0.7019491195678711, 'train/loss': 1.2742984294891357, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.5245815515518188, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.1862270832061768, 'test/num_examples': 10000, 'score': 27086.374658584595, 'total_duration': 28149.237203598022, 'accumulated_submission_time': 27086.374658584595, 'accumulated_eval_time': 1057.1927177906036, 'accumulated_logging_time': 2.82971453666687}
I0127 02:52:44.187967 140026050483968 logging_writer.py:48] [79612] accumulated_eval_time=1057.192718, accumulated_logging_time=2.829715, accumulated_submission_time=27086.374659, global_step=79612, preemption_count=0, score=27086.374659, test/accuracy=0.526000, test/loss=2.186227, test/num_examples=10000, total_duration=28149.237204, train/accuracy=0.701949, train/loss=1.274298, validation/accuracy=0.646920, validation/loss=1.524582, validation/num_examples=50000
I0127 02:53:14.325721 140026058876672 logging_writer.py:48] [79700] global_step=79700, grad_norm=4.50594425201416, loss=2.6753337383270264
I0127 02:53:48.233356 140026050483968 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.3294992446899414, loss=2.5577750205993652
I0127 02:54:22.177976 140026058876672 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.4113950729370117, loss=2.5345630645751953
I0127 02:54:56.083424 140026050483968 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.2058494091033936, loss=2.4989211559295654
I0127 02:55:30.004479 140026058876672 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.3852381706237793, loss=2.59083890914917
I0127 02:56:03.899511 140026050483968 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.5533287525177, loss=2.6012625694274902
I0127 02:56:37.803655 140026058876672 logging_writer.py:48] [80300] global_step=80300, grad_norm=3.1682493686676025, loss=2.6180953979492188
I0127 02:57:11.677243 140026050483968 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.3250162601470947, loss=2.577752113342285
I0127 02:57:45.600662 140026058876672 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.410283327102661, loss=2.6068077087402344
I0127 02:58:19.499904 140026050483968 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.5461950302124023, loss=2.64192533493042
I0127 02:58:53.413306 140026058876672 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.6832168102264404, loss=2.635018825531006
I0127 02:59:27.304511 140026050483968 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.4450128078460693, loss=2.5922558307647705
I0127 03:00:01.218675 140026058876672 logging_writer.py:48] [80900] global_step=80900, grad_norm=3.291027545928955, loss=2.5222725868225098
I0127 03:00:35.199725 140026050483968 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.4969356060028076, loss=2.525247812271118
I0127 03:01:09.107472 140026058876672 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.4435677528381348, loss=2.5148394107818604
I0127 03:01:14.339704 140187804313408 spec.py:321] Evaluating on the training split.
I0127 03:01:20.383586 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 03:01:29.278552 140187804313408 spec.py:349] Evaluating on the test split.
I0127 03:01:31.650408 140187804313408 submission_runner.py:408] Time since start: 28676.73s, 	Step: 81117, 	{'train/accuracy': 0.6994778513908386, 'train/loss': 1.2843109369277954, 'validation/accuracy': 0.6515199542045593, 'validation/loss': 1.5109398365020752, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.175584554672241, 'test/num_examples': 10000, 'score': 27596.460722208023, 'total_duration': 28676.73327088356, 'accumulated_submission_time': 27596.460722208023, 'accumulated_eval_time': 1074.5033564567566, 'accumulated_logging_time': 2.8747684955596924}
I0127 03:01:31.684262 140026067269376 logging_writer.py:48] [81117] accumulated_eval_time=1074.503356, accumulated_logging_time=2.874768, accumulated_submission_time=27596.460722, global_step=81117, preemption_count=0, score=27596.460722, test/accuracy=0.525300, test/loss=2.175585, test/num_examples=10000, total_duration=28676.733271, train/accuracy=0.699478, train/loss=1.284311, validation/accuracy=0.651520, validation/loss=1.510940, validation/num_examples=50000
I0127 03:02:00.124372 140026075662080 logging_writer.py:48] [81200] global_step=81200, grad_norm=3.717317819595337, loss=2.484079360961914
I0127 03:02:34.027498 140026067269376 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.443585157394409, loss=2.520960569381714
I0127 03:03:07.949289 140026075662080 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.3884787559509277, loss=2.6137571334838867
I0127 03:03:41.866995 140026067269376 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.5024971961975098, loss=2.5702693462371826
I0127 03:04:15.753170 140026075662080 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.7605714797973633, loss=2.5301599502563477
I0127 03:04:49.662893 140026067269376 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.6618025302886963, loss=2.662480115890503
I0127 03:05:23.542324 140026075662080 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.3443543910980225, loss=2.585635185241699
I0127 03:05:57.456751 140026067269376 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.3333020210266113, loss=2.62003493309021
I0127 03:06:31.413807 140026075662080 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.5554966926574707, loss=2.604088544845581
I0127 03:07:05.304596 140026067269376 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.40862774848938, loss=2.540883779525757
I0127 03:07:39.203802 140026075662080 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.6377058029174805, loss=2.5651581287384033
I0127 03:08:13.099742 140026067269376 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.456791639328003, loss=2.628518581390381
I0127 03:08:47.043287 140026075662080 logging_writer.py:48] [82400] global_step=82400, grad_norm=4.1410136222839355, loss=2.5893373489379883
I0127 03:09:20.979146 140026067269376 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.399141550064087, loss=2.5908336639404297
I0127 03:09:54.908196 140026075662080 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.68151593208313, loss=2.542032480239868
I0127 03:10:01.847075 140187804313408 spec.py:321] Evaluating on the training split.
I0127 03:10:08.168472 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 03:10:16.838250 140187804313408 spec.py:349] Evaluating on the test split.
I0127 03:10:19.131902 140187804313408 submission_runner.py:408] Time since start: 29204.21s, 	Step: 82622, 	{'train/accuracy': 0.6990393400192261, 'train/loss': 1.291176676750183, 'validation/accuracy': 0.6479799747467041, 'validation/loss': 1.5192651748657227, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1800341606140137, 'test/num_examples': 10000, 'score': 28106.558556318283, 'total_duration': 29204.21426296234, 'accumulated_submission_time': 28106.558556318283, 'accumulated_eval_time': 1091.787621974945, 'accumulated_logging_time': 2.9192492961883545}
I0127 03:10:19.166509 140026050483968 logging_writer.py:48] [82622] accumulated_eval_time=1091.787622, accumulated_logging_time=2.919249, accumulated_submission_time=28106.558556, global_step=82622, preemption_count=0, score=28106.558556, test/accuracy=0.525100, test/loss=2.180034, test/num_examples=10000, total_duration=29204.214263, train/accuracy=0.699039, train/loss=1.291177, validation/accuracy=0.647980, validation/loss=1.519265, validation/num_examples=50000
I0127 03:10:45.883595 140026058876672 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.5117733478546143, loss=2.642803430557251
I0127 03:11:19.776588 140026050483968 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.2299036979675293, loss=2.543541193008423
I0127 03:11:53.660175 140026058876672 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.301283121109009, loss=2.627748727798462
I0127 03:12:27.586298 140026050483968 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.4808831214904785, loss=2.5901291370391846
I0127 03:13:01.559816 140026058876672 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.4059829711914062, loss=2.556779146194458
I0127 03:13:35.444579 140026050483968 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.7771646976470947, loss=2.5623767375946045
I0127 03:14:09.384730 140026058876672 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.4061129093170166, loss=2.5821917057037354
I0127 03:14:43.283207 140026050483968 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.9240834712982178, loss=2.6534318923950195
I0127 03:15:17.205959 140026058876672 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.746903657913208, loss=2.607208013534546
I0127 03:15:51.136042 140026050483968 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.346494674682617, loss=2.6196348667144775
I0127 03:16:25.027125 140026058876672 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.702064037322998, loss=2.609381914138794
I0127 03:16:58.903487 140026050483968 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.010138034820557, loss=2.51888370513916
I0127 03:17:32.830939 140026058876672 logging_writer.py:48] [83900] global_step=83900, grad_norm=3.469348192214966, loss=2.5556252002716064
I0127 03:18:06.714205 140026050483968 logging_writer.py:48] [84000] global_step=84000, grad_norm=4.062318801879883, loss=2.5797600746154785
I0127 03:18:40.699715 140026058876672 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.25945782661438, loss=2.6296184062957764
I0127 03:18:49.288384 140187804313408 spec.py:321] Evaluating on the training split.
I0127 03:18:55.443240 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 03:19:04.565601 140187804313408 spec.py:349] Evaluating on the test split.
I0127 03:19:06.909296 140187804313408 submission_runner.py:408] Time since start: 29731.99s, 	Step: 84127, 	{'train/accuracy': 0.7421077489852905, 'train/loss': 1.1094766855239868, 'validation/accuracy': 0.6539199948310852, 'validation/loss': 1.498048186302185, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.172151565551758, 'test/num_examples': 10000, 'score': 28616.616693496704, 'total_duration': 29731.992182970047, 'accumulated_submission_time': 28616.616693496704, 'accumulated_eval_time': 1109.4084930419922, 'accumulated_logging_time': 2.964075803756714}
I0127 03:19:06.935736 140026050483968 logging_writer.py:48] [84127] accumulated_eval_time=1109.408493, accumulated_logging_time=2.964076, accumulated_submission_time=28616.616693, global_step=84127, preemption_count=0, score=28616.616693, test/accuracy=0.523100, test/loss=2.172152, test/num_examples=10000, total_duration=29731.992183, train/accuracy=0.742108, train/loss=1.109477, validation/accuracy=0.653920, validation/loss=1.498048, validation/num_examples=50000
I0127 03:19:32.019012 140026159523584 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.5281736850738525, loss=2.499859571456909
I0127 03:20:05.910022 140026050483968 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.70880126953125, loss=2.5925133228302
I0127 03:20:39.838949 140026159523584 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.4406685829162598, loss=2.564548969268799
I0127 03:21:13.739295 140026050483968 logging_writer.py:48] [84500] global_step=84500, grad_norm=4.38657808303833, loss=2.582545518875122
I0127 03:21:47.635635 140026159523584 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.5981783866882324, loss=2.6261770725250244
I0127 03:22:21.537695 140026050483968 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.5762853622436523, loss=2.554506778717041
I0127 03:22:55.451203 140026159523584 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.298593044281006, loss=2.536740779876709
I0127 03:23:29.368094 140026050483968 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.9533212184906006, loss=2.523257255554199
I0127 03:24:03.245939 140026159523584 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.9706366062164307, loss=2.577066421508789
I0127 03:24:37.169358 140026050483968 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.842358112335205, loss=2.563056230545044
I0127 03:25:11.159327 140026159523584 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.302449941635132, loss=2.501516103744507
I0127 03:25:45.064671 140026050483968 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.374790906906128, loss=2.6007533073425293
I0127 03:26:18.946705 140026159523584 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.9488143920898438, loss=2.506706476211548
I0127 03:26:52.840232 140026050483968 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.114386558532715, loss=2.5767104625701904
I0127 03:27:26.780268 140026159523584 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.5497524738311768, loss=2.5162196159362793
I0127 03:27:37.103932 140187804313408 spec.py:321] Evaluating on the training split.
I0127 03:27:43.137531 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 03:27:52.157868 140187804313408 spec.py:349] Evaluating on the test split.
I0127 03:27:54.432282 140187804313408 submission_runner.py:408] Time since start: 30259.52s, 	Step: 85632, 	{'train/accuracy': 0.72562575340271, 'train/loss': 1.1676472425460815, 'validation/accuracy': 0.6571399569511414, 'validation/loss': 1.469819188117981, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.140961170196533, 'test/num_examples': 10000, 'score': 29126.722969293594, 'total_duration': 30259.5151386261, 'accumulated_submission_time': 29126.722969293594, 'accumulated_eval_time': 1126.7367713451385, 'accumulated_logging_time': 2.999528408050537}
I0127 03:27:54.476351 140026042091264 logging_writer.py:48] [85632] accumulated_eval_time=1126.736771, accumulated_logging_time=2.999528, accumulated_submission_time=29126.722969, global_step=85632, preemption_count=0, score=29126.722969, test/accuracy=0.533900, test/loss=2.140961, test/num_examples=10000, total_duration=30259.515139, train/accuracy=0.725626, train/loss=1.167647, validation/accuracy=0.657140, validation/loss=1.469819, validation/num_examples=50000
I0127 03:28:17.834242 140026050483968 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.227163791656494, loss=2.5935773849487305
I0127 03:28:51.659695 140026042091264 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.6051809787750244, loss=2.576613426208496
I0127 03:29:25.526862 140026050483968 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.6191184520721436, loss=2.5610005855560303
I0127 03:29:59.414263 140026042091264 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.8439176082611084, loss=2.55613374710083
I0127 03:30:33.341794 140026050483968 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.6625256538391113, loss=2.534921407699585
I0127 03:31:07.312667 140026042091264 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.82405948638916, loss=2.6443562507629395
I0127 03:31:41.198340 140026050483968 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.3465964794158936, loss=2.5729589462280273
I0127 03:32:15.126761 140026042091264 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.838069438934326, loss=2.6390457153320312
I0127 03:32:48.992601 140026050483968 logging_writer.py:48] [86500] global_step=86500, grad_norm=3.660529851913452, loss=2.644357204437256
I0127 03:33:22.912844 140026042091264 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.4921183586120605, loss=2.626110076904297
I0127 03:33:56.808010 140026050483968 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.411048412322998, loss=2.659731388092041
I0127 03:34:30.736929 140026042091264 logging_writer.py:48] [86800] global_step=86800, grad_norm=3.9998066425323486, loss=2.6125264167785645
I0127 03:35:04.653324 140026050483968 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.7138493061065674, loss=2.508768081665039
I0127 03:35:38.556851 140026042091264 logging_writer.py:48] [87000] global_step=87000, grad_norm=4.01687479019165, loss=2.5413575172424316
I0127 03:36:12.489969 140026050483968 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.149709939956665, loss=2.589813709259033
I0127 03:36:24.472414 140187804313408 spec.py:321] Evaluating on the training split.
I0127 03:36:30.506854 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 03:36:39.224156 140187804313408 spec.py:349] Evaluating on the test split.
I0127 03:36:41.623206 140187804313408 submission_runner.py:408] Time since start: 30786.71s, 	Step: 87137, 	{'train/accuracy': 0.7182118892669678, 'train/loss': 1.213655948638916, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.4903264045715332, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.1381936073303223, 'test/num_examples': 10000, 'score': 29636.656057357788, 'total_duration': 30786.70604276657, 'accumulated_submission_time': 29636.656057357788, 'accumulated_eval_time': 1143.8874711990356, 'accumulated_logging_time': 3.0535805225372314}
I0127 03:36:41.661538 140026159523584 logging_writer.py:48] [87137] accumulated_eval_time=1143.887471, accumulated_logging_time=3.053581, accumulated_submission_time=29636.656057, global_step=87137, preemption_count=0, score=29636.656057, test/accuracy=0.533900, test/loss=2.138194, test/num_examples=10000, total_duration=30786.706043, train/accuracy=0.718212, train/loss=1.213656, validation/accuracy=0.656680, validation/loss=1.490326, validation/num_examples=50000
I0127 03:37:03.332061 140026167916288 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.8011157512664795, loss=2.5487148761749268
I0127 03:37:37.219274 140026159523584 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.867917537689209, loss=2.541017770767212
I0127 03:38:11.138768 140026167916288 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.532348394393921, loss=2.5842976570129395
I0127 03:38:45.060488 140026159523584 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.4965715408325195, loss=2.596158504486084
I0127 03:39:18.963302 140026167916288 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.5940089225769043, loss=2.5518805980682373
I0127 03:39:52.904991 140026159523584 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.8498454093933105, loss=2.572019100189209
I0127 03:40:26.816048 140026167916288 logging_writer.py:48] [87800] global_step=87800, grad_norm=3.4810783863067627, loss=2.552366256713867
I0127 03:41:00.715436 140026159523584 logging_writer.py:48] [87900] global_step=87900, grad_norm=3.753805160522461, loss=2.5973739624023438
I0127 03:41:34.632023 140026167916288 logging_writer.py:48] [88000] global_step=88000, grad_norm=3.7676494121551514, loss=2.618662118911743
I0127 03:42:08.503271 140026159523584 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.5047876834869385, loss=2.5910019874572754
I0127 03:42:42.421767 140026167916288 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.826437473297119, loss=2.6163346767425537
I0127 03:43:16.320720 140026159523584 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.506213426589966, loss=2.506885290145874
I0127 03:43:50.409502 140026167916288 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.553184986114502, loss=2.6096129417419434
I0127 03:44:24.304226 140026159523584 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.8259010314941406, loss=2.6591243743896484
I0127 03:44:58.226614 140026167916288 logging_writer.py:48] [88600] global_step=88600, grad_norm=3.458163022994995, loss=2.4875409603118896
I0127 03:45:11.956295 140187804313408 spec.py:321] Evaluating on the training split.
I0127 03:45:18.047270 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 03:45:27.006943 140187804313408 spec.py:349] Evaluating on the test split.
I0127 03:45:29.278812 140187804313408 submission_runner.py:408] Time since start: 31314.36s, 	Step: 88642, 	{'train/accuracy': 0.7137675285339355, 'train/loss': 1.2036211490631104, 'validation/accuracy': 0.6636999845504761, 'validation/loss': 1.4569283723831177, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.1352744102478027, 'test/num_examples': 10000, 'score': 30146.884006261826, 'total_duration': 31314.36168217659, 'accumulated_submission_time': 30146.884006261826, 'accumulated_eval_time': 1161.2099361419678, 'accumulated_logging_time': 3.103337049484253}
I0127 03:45:29.310357 140026075662080 logging_writer.py:48] [88642] accumulated_eval_time=1161.209936, accumulated_logging_time=3.103337, accumulated_submission_time=30146.884006, global_step=88642, preemption_count=0, score=30146.884006, test/accuracy=0.532400, test/loss=2.135274, test/num_examples=10000, total_duration=31314.361682, train/accuracy=0.713768, train/loss=1.203621, validation/accuracy=0.663700, validation/loss=1.456928, validation/num_examples=50000
I0127 03:45:49.301436 140026151130880 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.929250717163086, loss=2.458167552947998
I0127 03:46:23.159164 140026075662080 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.47481107711792, loss=2.6331071853637695
I0127 03:46:57.043240 140026151130880 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.6036860942840576, loss=2.52907133102417
I0127 03:47:30.950521 140026075662080 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.628997564315796, loss=2.5730819702148438
I0127 03:48:04.843275 140026151130880 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.435964107513428, loss=2.5123066902160645
I0127 03:48:38.783822 140026075662080 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.4904961585998535, loss=2.53127384185791
I0127 03:49:12.646703 140026151130880 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.4742136001586914, loss=2.574561357498169
I0127 03:49:46.645599 140026075662080 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.5802981853485107, loss=2.6188862323760986
I0127 03:50:20.561993 140026151130880 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.724128484725952, loss=2.588770866394043
I0127 03:50:54.451327 140026075662080 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.287895202636719, loss=2.666341781616211
I0127 03:51:28.333783 140026151130880 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.5204885005950928, loss=2.4996025562286377
I0127 03:52:02.248358 140026075662080 logging_writer.py:48] [89800] global_step=89800, grad_norm=3.8312573432922363, loss=2.592562198638916
I0127 03:52:36.150024 140026151130880 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.802877902984619, loss=2.6021721363067627
I0127 03:53:10.088887 140026075662080 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.7769649028778076, loss=2.6042277812957764
I0127 03:53:43.988670 140026151130880 logging_writer.py:48] [90100] global_step=90100, grad_norm=3.541649580001831, loss=2.567631959915161
I0127 03:53:59.384247 140187804313408 spec.py:321] Evaluating on the training split.
I0127 03:54:05.685051 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 03:54:14.593560 140187804313408 spec.py:349] Evaluating on the test split.
I0127 03:54:16.945757 140187804313408 submission_runner.py:408] Time since start: 31842.03s, 	Step: 90147, 	{'train/accuracy': 0.7099409699440002, 'train/loss': 1.2612212896347046, 'validation/accuracy': 0.6620799899101257, 'validation/loss': 1.5005483627319336, 'validation/num_examples': 50000, 'test/accuracy': 0.5302000045776367, 'test/loss': 2.188394784927368, 'test/num_examples': 10000, 'score': 30656.89363193512, 'total_duration': 31842.028629779816, 'accumulated_submission_time': 30656.89363193512, 'accumulated_eval_time': 1178.7714076042175, 'accumulated_logging_time': 3.1447083950042725}
I0127 03:54:16.980424 140026058876672 logging_writer.py:48] [90147] accumulated_eval_time=1178.771408, accumulated_logging_time=3.144708, accumulated_submission_time=30656.893632, global_step=90147, preemption_count=0, score=30656.893632, test/accuracy=0.530200, test/loss=2.188395, test/num_examples=10000, total_duration=31842.028630, train/accuracy=0.709941, train/loss=1.261221, validation/accuracy=0.662080, validation/loss=1.500548, validation/num_examples=50000
I0127 03:54:35.266918 140026067269376 logging_writer.py:48] [90200] global_step=90200, grad_norm=4.709497451782227, loss=2.5823814868927
I0127 03:55:09.136488 140026058876672 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.5159294605255127, loss=2.6170637607574463
I0127 03:55:43.080478 140026067269376 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.3648550510406494, loss=2.4885668754577637
I0127 03:56:16.969956 140026058876672 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.067188262939453, loss=2.543732166290283
I0127 03:56:50.890151 140026067269376 logging_writer.py:48] [90600] global_step=90600, grad_norm=3.701890707015991, loss=2.527247428894043
I0127 03:57:24.818311 140026058876672 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.667569160461426, loss=2.604785919189453
I0127 03:57:58.750936 140026067269376 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.4554762840271, loss=2.4865994453430176
I0127 03:58:32.652962 140026058876672 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.5296719074249268, loss=2.480492353439331
I0127 03:59:06.566871 140026067269376 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.474465847015381, loss=2.4839212894439697
I0127 03:59:40.462659 140026058876672 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.8912112712860107, loss=2.5888705253601074
I0127 04:00:14.395200 140026067269376 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.5677738189697266, loss=2.543893814086914
I0127 04:00:48.327583 140026058876672 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.5910816192626953, loss=2.537338972091675
I0127 04:01:22.232137 140026067269376 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.57195782661438, loss=2.5118417739868164
I0127 04:01:56.196195 140026058876672 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.160109043121338, loss=2.4917054176330566
I0127 04:02:30.106689 140026067269376 logging_writer.py:48] [91600] global_step=91600, grad_norm=3.7082369327545166, loss=2.5907113552093506
I0127 04:02:47.184757 140187804313408 spec.py:321] Evaluating on the training split.
I0127 04:02:53.202443 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 04:03:02.300554 140187804313408 spec.py:349] Evaluating on the test split.
I0127 04:03:04.605803 140187804313408 submission_runner.py:408] Time since start: 32369.69s, 	Step: 91652, 	{'train/accuracy': 0.7141063213348389, 'train/loss': 1.2344672679901123, 'validation/accuracy': 0.6605799794197083, 'validation/loss': 1.4747819900512695, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.1402695178985596, 'test/num_examples': 10000, 'score': 31167.033656597137, 'total_duration': 32369.68866419792, 'accumulated_submission_time': 31167.033656597137, 'accumulated_eval_time': 1196.192389011383, 'accumulated_logging_time': 3.1903398036956787}
I0127 04:03:04.640097 140026151130880 logging_writer.py:48] [91652] accumulated_eval_time=1196.192389, accumulated_logging_time=3.190340, accumulated_submission_time=31167.033657, global_step=91652, preemption_count=0, score=31167.033657, test/accuracy=0.532800, test/loss=2.140270, test/num_examples=10000, total_duration=32369.688664, train/accuracy=0.714106, train/loss=1.234467, validation/accuracy=0.660580, validation/loss=1.474782, validation/num_examples=50000
I0127 04:03:21.215798 140026167916288 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.249724388122559, loss=2.5693359375
I0127 04:03:55.094117 140026151130880 logging_writer.py:48] [91800] global_step=91800, grad_norm=3.5101864337921143, loss=2.625762462615967
I0127 04:04:28.989688 140026167916288 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.096652984619141, loss=2.605055809020996
I0127 04:05:02.899010 140026151130880 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.6296181678771973, loss=2.5879483222961426
I0127 04:05:36.826915 140026167916288 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.012789726257324, loss=2.533592462539673
I0127 04:06:10.729307 140026151130880 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.05514669418335, loss=2.479278326034546
I0127 04:06:44.637694 140026167916288 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.4917256832122803, loss=2.5097498893737793
I0127 04:07:18.529834 140026151130880 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.7632479667663574, loss=2.5806517601013184
I0127 04:07:52.440344 140026167916288 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.6146435737609863, loss=2.533163547515869
I0127 04:08:26.372059 140026151130880 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.9864861965179443, loss=2.444622278213501
I0127 04:09:00.306127 140026167916288 logging_writer.py:48] [92700] global_step=92700, grad_norm=3.489356517791748, loss=2.512637138366699
I0127 04:09:34.185109 140026151130880 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.144328594207764, loss=2.46126651763916
I0127 04:10:08.077305 140026167916288 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.7966763973236084, loss=2.605884313583374
I0127 04:10:41.985543 140026151130880 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.049746990203857, loss=2.6247642040252686
I0127 04:11:15.883453 140026167916288 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.054458141326904, loss=2.478597402572632
I0127 04:11:34.686636 140187804313408 spec.py:321] Evaluating on the training split.
I0127 04:11:40.799220 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 04:11:49.834322 140187804313408 spec.py:349] Evaluating on the test split.
I0127 04:11:52.079228 140187804313408 submission_runner.py:408] Time since start: 32897.16s, 	Step: 93157, 	{'train/accuracy': 0.7379822731018066, 'train/loss': 1.1373311281204224, 'validation/accuracy': 0.6595999598503113, 'validation/loss': 1.4904597997665405, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.143916606903076, 'test/num_examples': 10000, 'score': 31677.014585733414, 'total_duration': 32897.162084817886, 'accumulated_submission_time': 31677.014585733414, 'accumulated_eval_time': 1213.5849130153656, 'accumulated_logging_time': 3.2358620166778564}
I0127 04:11:52.112338 140026058876672 logging_writer.py:48] [93157] accumulated_eval_time=1213.584913, accumulated_logging_time=3.235862, accumulated_submission_time=31677.014586, global_step=93157, preemption_count=0, score=31677.014586, test/accuracy=0.534800, test/loss=2.143917, test/num_examples=10000, total_duration=32897.162085, train/accuracy=0.737982, train/loss=1.137331, validation/accuracy=0.659600, validation/loss=1.490460, validation/num_examples=50000
I0127 04:12:07.018674 140026067269376 logging_writer.py:48] [93200] global_step=93200, grad_norm=4.437147617340088, loss=2.586038827896118
I0127 04:12:40.896496 140026058876672 logging_writer.py:48] [93300] global_step=93300, grad_norm=3.6613335609436035, loss=2.4570984840393066
I0127 04:13:14.811975 140026067269376 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.6046276092529297, loss=2.6377522945404053
I0127 04:13:48.756016 140026058876672 logging_writer.py:48] [93500] global_step=93500, grad_norm=3.8957133293151855, loss=2.4871582984924316
I0127 04:14:22.702243 140026067269376 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.68449330329895, loss=2.483527183532715
I0127 04:14:56.617658 140026058876672 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.9640910625457764, loss=2.547041654586792
I0127 04:15:30.539309 140026067269376 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.137187480926514, loss=2.5890321731567383
I0127 04:16:04.466409 140026058876672 logging_writer.py:48] [93900] global_step=93900, grad_norm=3.9537932872772217, loss=2.441998243331909
I0127 04:16:38.354029 140026067269376 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.84494948387146, loss=2.5767788887023926
I0127 04:17:12.264999 140026058876672 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.587407350540161, loss=2.5630643367767334
I0127 04:17:46.164875 140026067269376 logging_writer.py:48] [94200] global_step=94200, grad_norm=3.9411959648132324, loss=2.5852155685424805
I0127 04:18:20.097417 140026058876672 logging_writer.py:48] [94300] global_step=94300, grad_norm=3.9601099491119385, loss=2.4808530807495117
I0127 04:18:54.007310 140026067269376 logging_writer.py:48] [94400] global_step=94400, grad_norm=3.8789658546447754, loss=2.443470001220703
I0127 04:19:27.918018 140026058876672 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.3877947330474854, loss=2.5256052017211914
I0127 04:20:01.853106 140026067269376 logging_writer.py:48] [94600] global_step=94600, grad_norm=3.851182699203491, loss=2.4959945678710938
I0127 04:20:22.409485 140187804313408 spec.py:321] Evaluating on the training split.
I0127 04:20:28.416654 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 04:20:37.394038 140187804313408 spec.py:349] Evaluating on the test split.
I0127 04:20:39.664786 140187804313408 submission_runner.py:408] Time since start: 33424.75s, 	Step: 94662, 	{'train/accuracy': 0.7356903553009033, 'train/loss': 1.137479305267334, 'validation/accuracy': 0.6634199619293213, 'validation/loss': 1.4608635902404785, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.139446258544922, 'test/num_examples': 10000, 'score': 32187.246658563614, 'total_duration': 33424.74766254425, 'accumulated_submission_time': 32187.246658563614, 'accumulated_eval_time': 1230.840161561966, 'accumulated_logging_time': 3.2798256874084473}
I0127 04:20:39.699684 140026151130880 logging_writer.py:48] [94662] accumulated_eval_time=1230.840162, accumulated_logging_time=3.279826, accumulated_submission_time=32187.246659, global_step=94662, preemption_count=0, score=32187.246659, test/accuracy=0.534500, test/loss=2.139446, test/num_examples=10000, total_duration=33424.747663, train/accuracy=0.735690, train/loss=1.137479, validation/accuracy=0.663420, validation/loss=1.460864, validation/num_examples=50000
I0127 04:20:52.934553 140026159523584 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.8866653442382812, loss=2.4644150733947754
I0127 04:21:26.787089 140026151130880 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.5355634689331055, loss=2.6201891899108887
I0127 04:22:00.663725 140026159523584 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.076558589935303, loss=2.4910035133361816
I0127 04:22:34.626642 140026151130880 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.657858371734619, loss=2.5587728023529053
I0127 04:23:08.545365 140026159523584 logging_writer.py:48] [95100] global_step=95100, grad_norm=3.359027624130249, loss=2.517982006072998
I0127 04:23:42.438481 140026151130880 logging_writer.py:48] [95200] global_step=95200, grad_norm=3.8230574131011963, loss=2.4056396484375
I0127 04:24:16.371251 140026159523584 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.9712395668029785, loss=2.554119110107422
I0127 04:24:50.276529 140026151130880 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.09997034072876, loss=2.4900662899017334
I0127 04:25:24.170428 140026159523584 logging_writer.py:48] [95500] global_step=95500, grad_norm=3.921060085296631, loss=2.441025733947754
I0127 04:25:58.097333 140026151130880 logging_writer.py:48] [95600] global_step=95600, grad_norm=3.668346405029297, loss=2.5008296966552734
I0127 04:26:32.056804 140026159523584 logging_writer.py:48] [95700] global_step=95700, grad_norm=3.759477376937866, loss=2.4959917068481445
I0127 04:27:05.983942 140026151130880 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.099213600158691, loss=2.5697178840637207
I0127 04:27:39.870472 140026159523584 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.5908608436584473, loss=2.4646825790405273
I0127 04:28:13.776429 140026151130880 logging_writer.py:48] [96000] global_step=96000, grad_norm=3.7771270275115967, loss=2.663750410079956
I0127 04:28:47.707237 140026159523584 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.170780658721924, loss=2.665642261505127
I0127 04:29:09.910220 140187804313408 spec.py:321] Evaluating on the training split.
I0127 04:29:16.041524 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 04:29:24.853968 140187804313408 spec.py:349] Evaluating on the test split.
I0127 04:29:27.140629 140187804313408 submission_runner.py:408] Time since start: 33952.22s, 	Step: 96167, 	{'train/accuracy': 0.7338966727256775, 'train/loss': 1.1363016366958618, 'validation/accuracy': 0.6714999675750732, 'validation/loss': 1.4211866855621338, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.080425262451172, 'test/num_examples': 10000, 'score': 32697.393936157227, 'total_duration': 33952.223504543304, 'accumulated_submission_time': 32697.393936157227, 'accumulated_eval_time': 1248.0705358982086, 'accumulated_logging_time': 3.32486629486084}
I0127 04:29:27.176430 140026067269376 logging_writer.py:48] [96167] accumulated_eval_time=1248.070536, accumulated_logging_time=3.324866, accumulated_submission_time=32697.393936, global_step=96167, preemption_count=0, score=32697.393936, test/accuracy=0.542000, test/loss=2.080425, test/num_examples=10000, total_duration=33952.223505, train/accuracy=0.733897, train/loss=1.136302, validation/accuracy=0.671500, validation/loss=1.421187, validation/num_examples=50000
I0127 04:29:38.719383 140026075662080 logging_writer.py:48] [96200] global_step=96200, grad_norm=3.7274246215820312, loss=2.554617404937744
I0127 04:30:12.566823 140026067269376 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.194692611694336, loss=2.5571646690368652
I0127 04:30:46.437007 140026075662080 logging_writer.py:48] [96400] global_step=96400, grad_norm=3.3616724014282227, loss=2.4468886852264404
I0127 04:31:20.305821 140026067269376 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.287490367889404, loss=2.534494161605835
I0127 04:31:54.221910 140026075662080 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.923830509185791, loss=2.546053409576416
I0127 04:32:28.137604 140026067269376 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.282931804656982, loss=2.5839481353759766
I0127 04:33:02.116775 140026075662080 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.008004665374756, loss=2.481394052505493
I0127 04:33:36.019117 140026067269376 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.3304030895233154, loss=2.4881865978240967
I0127 04:34:09.931281 140026075662080 logging_writer.py:48] [97000] global_step=97000, grad_norm=4.123739242553711, loss=2.4892935752868652
I0127 04:34:43.843359 140026067269376 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.9388396739959717, loss=2.5120248794555664
I0127 04:35:17.772741 140026075662080 logging_writer.py:48] [97200] global_step=97200, grad_norm=3.741417407989502, loss=2.464643955230713
I0127 04:35:51.658674 140026067269376 logging_writer.py:48] [97300] global_step=97300, grad_norm=3.8046648502349854, loss=2.5092525482177734
I0127 04:36:25.581117 140026075662080 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.052030086517334, loss=2.4729790687561035
I0127 04:36:59.479056 140026067269376 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.812448501586914, loss=2.4142770767211914
I0127 04:37:33.370758 140026075662080 logging_writer.py:48] [97600] global_step=97600, grad_norm=3.5298945903778076, loss=2.5815086364746094
I0127 04:37:57.241684 140187804313408 spec.py:321] Evaluating on the training split.
I0127 04:38:03.400190 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 04:38:12.424842 140187804313408 spec.py:349] Evaluating on the test split.
I0127 04:38:14.725566 140187804313408 submission_runner.py:408] Time since start: 34479.81s, 	Step: 97672, 	{'train/accuracy': 0.7274991869926453, 'train/loss': 1.1664454936981201, 'validation/accuracy': 0.6618399620056152, 'validation/loss': 1.453052043914795, 'validation/num_examples': 50000, 'test/accuracy': 0.5360000133514404, 'test/loss': 2.12292218208313, 'test/num_examples': 10000, 'score': 33207.3950073719, 'total_duration': 34479.80842757225, 'accumulated_submission_time': 33207.3950073719, 'accumulated_eval_time': 1265.554355621338, 'accumulated_logging_time': 3.372291326522827}
I0127 04:38:14.761255 140026058876672 logging_writer.py:48] [97672] accumulated_eval_time=1265.554356, accumulated_logging_time=3.372291, accumulated_submission_time=33207.395007, global_step=97672, preemption_count=0, score=33207.395007, test/accuracy=0.536000, test/loss=2.122922, test/num_examples=10000, total_duration=34479.808428, train/accuracy=0.727499, train/loss=1.166445, validation/accuracy=0.661840, validation/loss=1.453052, validation/num_examples=50000
I0127 04:38:24.600220 140026159523584 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.158326148986816, loss=2.4697771072387695
I0127 04:38:58.523350 140026058876672 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.8841819763183594, loss=2.545891046524048
I0127 04:39:32.409847 140026159523584 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.74272346496582, loss=2.428837537765503
I0127 04:40:06.306527 140026058876672 logging_writer.py:48] [98000] global_step=98000, grad_norm=3.7196433544158936, loss=2.5761234760284424
I0127 04:40:40.206340 140026159523584 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.571277618408203, loss=2.5027589797973633
I0127 04:41:14.077033 140026058876672 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.484135866165161, loss=2.5360541343688965
I0127 04:41:47.965318 140026159523584 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.2548136711120605, loss=2.461390972137451
I0127 04:42:21.875309 140026058876672 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.823613405227661, loss=2.42346453666687
I0127 04:42:55.787481 140026159523584 logging_writer.py:48] [98500] global_step=98500, grad_norm=3.7163727283477783, loss=2.539153575897217
I0127 04:43:29.673916 140026058876672 logging_writer.py:48] [98600] global_step=98600, grad_norm=4.128726959228516, loss=2.5593104362487793
I0127 04:44:03.557586 140026159523584 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.176965713500977, loss=2.4822092056274414
I0127 04:44:37.468943 140026058876672 logging_writer.py:48] [98800] global_step=98800, grad_norm=3.8612465858459473, loss=2.5481317043304443
I0127 04:45:11.370858 140026159523584 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.037229537963867, loss=2.570467472076416
I0127 04:45:45.301223 140026058876672 logging_writer.py:48] [99000] global_step=99000, grad_norm=3.6157360076904297, loss=2.400972604751587
I0127 04:46:19.208485 140026159523584 logging_writer.py:48] [99100] global_step=99100, grad_norm=3.938859224319458, loss=2.424844980239868
I0127 04:46:44.802628 140187804313408 spec.py:321] Evaluating on the training split.
I0127 04:46:50.825498 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 04:46:59.870895 140187804313408 spec.py:349] Evaluating on the test split.
I0127 04:47:02.194540 140187804313408 submission_runner.py:408] Time since start: 35007.28s, 	Step: 99177, 	{'train/accuracy': 0.7198660373687744, 'train/loss': 1.1820178031921387, 'validation/accuracy': 0.6629999876022339, 'validation/loss': 1.4383418560028076, 'validation/num_examples': 50000, 'test/accuracy': 0.538100004196167, 'test/loss': 2.1251816749572754, 'test/num_examples': 10000, 'score': 33717.37340140343, 'total_duration': 35007.27703619003, 'accumulated_submission_time': 33717.37340140343, 'accumulated_eval_time': 1282.9458377361298, 'accumulated_logging_time': 3.418320894241333}
I0127 04:47:02.228266 140026042091264 logging_writer.py:48] [99177] accumulated_eval_time=1282.945838, accumulated_logging_time=3.418321, accumulated_submission_time=33717.373401, global_step=99177, preemption_count=0, score=33717.373401, test/accuracy=0.538100, test/loss=2.125182, test/num_examples=10000, total_duration=35007.277036, train/accuracy=0.719866, train/loss=1.182018, validation/accuracy=0.663000, validation/loss=1.438342, validation/num_examples=50000
I0127 04:47:10.365551 140026050483968 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.528580188751221, loss=2.4679982662200928
I0127 04:47:44.205527 140026042091264 logging_writer.py:48] [99300] global_step=99300, grad_norm=4.249222278594971, loss=2.592454195022583
I0127 04:48:18.049640 140026050483968 logging_writer.py:48] [99400] global_step=99400, grad_norm=3.9855563640594482, loss=2.501756429672241
I0127 04:48:51.956114 140026042091264 logging_writer.py:48] [99500] global_step=99500, grad_norm=3.5672988891601562, loss=2.5180423259735107
I0127 04:49:25.851783 140026050483968 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.9285361766815186, loss=2.462669849395752
I0127 04:49:59.801314 140026042091264 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.0925703048706055, loss=2.58240008354187
I0127 04:50:33.692179 140026050483968 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.536905288696289, loss=2.4614052772521973
I0127 04:51:07.706644 140026042091264 logging_writer.py:48] [99900] global_step=99900, grad_norm=3.7325809001922607, loss=2.478740692138672
I0127 04:51:41.593844 140026050483968 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.0555877685546875, loss=2.5401906967163086
I0127 04:52:15.524272 140026042091264 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.386351108551025, loss=2.4515340328216553
I0127 04:52:49.397365 140026050483968 logging_writer.py:48] [100200] global_step=100200, grad_norm=3.4161453247070312, loss=2.4007444381713867
I0127 04:53:23.330146 140026042091264 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.3266377449035645, loss=2.516702651977539
I0127 04:53:57.253163 140026050483968 logging_writer.py:48] [100400] global_step=100400, grad_norm=3.8622183799743652, loss=2.4406890869140625
I0127 04:54:31.138647 140026042091264 logging_writer.py:48] [100500] global_step=100500, grad_norm=5.246373176574707, loss=2.547706127166748
I0127 04:55:05.082231 140026050483968 logging_writer.py:48] [100600] global_step=100600, grad_norm=3.586299180984497, loss=2.4399523735046387
I0127 04:55:32.362720 140187804313408 spec.py:321] Evaluating on the training split.
I0127 04:55:38.442402 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 04:55:47.199434 140187804313408 spec.py:349] Evaluating on the test split.
I0127 04:55:49.498480 140187804313408 submission_runner.py:408] Time since start: 35534.58s, 	Step: 100682, 	{'train/accuracy': 0.7286949753761292, 'train/loss': 1.1676808595657349, 'validation/accuracy': 0.6708599925041199, 'validation/loss': 1.4283438920974731, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.0684101581573486, 'test/num_examples': 10000, 'score': 34227.44268536568, 'total_duration': 35534.58135795593, 'accumulated_submission_time': 34227.44268536568, 'accumulated_eval_time': 1300.0815467834473, 'accumulated_logging_time': 3.4637675285339355}
I0127 04:55:49.533217 140026151130880 logging_writer.py:48] [100682] accumulated_eval_time=1300.081547, accumulated_logging_time=3.463768, accumulated_submission_time=34227.442685, global_step=100682, preemption_count=0, score=34227.442685, test/accuracy=0.549600, test/loss=2.068410, test/num_examples=10000, total_duration=35534.581358, train/accuracy=0.728695, train/loss=1.167681, validation/accuracy=0.670860, validation/loss=1.428344, validation/num_examples=50000
I0127 04:55:56.010765 140026159523584 logging_writer.py:48] [100700] global_step=100700, grad_norm=3.781721591949463, loss=2.482095241546631
I0127 04:56:29.856789 140026151130880 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.208528995513916, loss=2.5202343463897705
I0127 04:57:03.724708 140026159523584 logging_writer.py:48] [100900] global_step=100900, grad_norm=3.5157155990600586, loss=2.5162267684936523
I0127 04:57:37.683384 140026151130880 logging_writer.py:48] [101000] global_step=101000, grad_norm=3.9031527042388916, loss=2.4699783325195312
I0127 04:58:11.563057 140026159523584 logging_writer.py:48] [101100] global_step=101100, grad_norm=3.6662840843200684, loss=2.5614473819732666
I0127 04:58:45.478439 140026151130880 logging_writer.py:48] [101200] global_step=101200, grad_norm=3.9238338470458984, loss=2.433818817138672
I0127 04:59:19.393864 140026159523584 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.049907207489014, loss=2.480403423309326
I0127 04:59:53.318393 140026151130880 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.519705057144165, loss=2.495521306991577
I0127 05:00:27.222235 140026159523584 logging_writer.py:48] [101500] global_step=101500, grad_norm=3.7815699577331543, loss=2.425180673599243
I0127 05:01:01.133655 140026151130880 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.035865306854248, loss=2.462862491607666
I0127 05:01:35.039183 140026159523584 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.341773509979248, loss=2.538201332092285
I0127 05:02:08.947404 140026151130880 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.205801486968994, loss=2.4765639305114746
I0127 05:02:42.848585 140026159523584 logging_writer.py:48] [101900] global_step=101900, grad_norm=4.058688163757324, loss=2.6141085624694824
I0127 05:03:16.780530 140026151130880 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.486818790435791, loss=2.5259950160980225
I0127 05:03:50.747552 140026159523584 logging_writer.py:48] [102100] global_step=102100, grad_norm=3.5696377754211426, loss=2.3412272930145264
I0127 05:04:19.698507 140187804313408 spec.py:321] Evaluating on the training split.
I0127 05:04:25.735535 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 05:04:34.412352 140187804313408 spec.py:349] Evaluating on the test split.
I0127 05:04:36.730977 140187804313408 submission_runner.py:408] Time since start: 36061.81s, 	Step: 102187, 	{'train/accuracy': 0.748046875, 'train/loss': 1.0852841138839722, 'validation/accuracy': 0.6756599545478821, 'validation/loss': 1.4007805585861206, 'validation/num_examples': 50000, 'test/accuracy': 0.5542000532150269, 'test/loss': 2.0193703174591064, 'test/num_examples': 10000, 'score': 34737.54347777367, 'total_duration': 36061.81385445595, 'accumulated_submission_time': 34737.54347777367, 'accumulated_eval_time': 1317.113971233368, 'accumulated_logging_time': 3.5098297595977783}
I0127 05:04:36.765841 140026058876672 logging_writer.py:48] [102187] accumulated_eval_time=1317.113971, accumulated_logging_time=3.509830, accumulated_submission_time=34737.543478, global_step=102187, preemption_count=0, score=34737.543478, test/accuracy=0.554200, test/loss=2.019370, test/num_examples=10000, total_duration=36061.813854, train/accuracy=0.748047, train/loss=1.085284, validation/accuracy=0.675660, validation/loss=1.400781, validation/num_examples=50000
I0127 05:04:41.506646 140026067269376 logging_writer.py:48] [102200] global_step=102200, grad_norm=3.744929313659668, loss=2.511320114135742
I0127 05:05:15.359536 140026058876672 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.038853645324707, loss=2.493324041366577
I0127 05:05:49.223958 140026067269376 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.08301305770874, loss=2.384221315383911
I0127 05:06:23.140477 140026058876672 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.491696357727051, loss=2.4652326107025146
I0127 05:06:57.046542 140026067269376 logging_writer.py:48] [102600] global_step=102600, grad_norm=3.6941137313842773, loss=2.374377727508545
I0127 05:07:30.970030 140026058876672 logging_writer.py:48] [102700] global_step=102700, grad_norm=3.557563304901123, loss=2.368365526199341
I0127 05:08:04.917666 140026067269376 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.258756160736084, loss=2.5401806831359863
I0127 05:08:38.828656 140026058876672 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.745309829711914, loss=2.5563559532165527
I0127 05:09:12.747122 140026067269376 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.057251930236816, loss=2.3900933265686035
I0127 05:09:46.719316 140026058876672 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.3047332763671875, loss=2.4628312587738037
I0127 05:10:20.624090 140026067269376 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.1023454666137695, loss=2.445744276046753
I0127 05:10:54.558665 140026058876672 logging_writer.py:48] [103300] global_step=103300, grad_norm=3.799137592315674, loss=2.374857187271118
I0127 05:11:28.504126 140026067269376 logging_writer.py:48] [103400] global_step=103400, grad_norm=3.7527616024017334, loss=2.4053092002868652
I0127 05:12:02.421793 140026058876672 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.246884346008301, loss=2.5410680770874023
I0127 05:12:36.341438 140026067269376 logging_writer.py:48] [103600] global_step=103600, grad_norm=3.52408504486084, loss=2.478447198867798
I0127 05:13:06.988320 140187804313408 spec.py:321] Evaluating on the training split.
I0127 05:13:13.087815 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 05:13:21.781931 140187804313408 spec.py:349] Evaluating on the test split.
I0127 05:13:24.086842 140187804313408 submission_runner.py:408] Time since start: 36589.17s, 	Step: 103692, 	{'train/accuracy': 0.7497408986091614, 'train/loss': 1.068345069885254, 'validation/accuracy': 0.6728799939155579, 'validation/loss': 1.414284348487854, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.0770750045776367, 'test/num_examples': 10000, 'score': 35247.70157814026, 'total_duration': 36589.16969251633, 'accumulated_submission_time': 35247.70157814026, 'accumulated_eval_time': 1334.212421655655, 'accumulated_logging_time': 3.555453062057495}
I0127 05:13:24.124575 140026050483968 logging_writer.py:48] [103692] accumulated_eval_time=1334.212422, accumulated_logging_time=3.555453, accumulated_submission_time=35247.701578, global_step=103692, preemption_count=0, score=35247.701578, test/accuracy=0.542100, test/loss=2.077075, test/num_examples=10000, total_duration=36589.169693, train/accuracy=0.749741, train/loss=1.068345, validation/accuracy=0.672880, validation/loss=1.414284, validation/num_examples=50000
I0127 05:13:27.192499 140026058876672 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.5082902908325195, loss=2.527287006378174
I0127 05:14:01.007313 140026050483968 logging_writer.py:48] [103800] global_step=103800, grad_norm=3.8596243858337402, loss=2.3896689414978027
I0127 05:14:34.883943 140026058876672 logging_writer.py:48] [103900] global_step=103900, grad_norm=3.944746732711792, loss=2.4885833263397217
I0127 05:15:08.742640 140026050483968 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.528372764587402, loss=2.499431610107422
I0127 05:15:42.692821 140026058876672 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.140153408050537, loss=2.4934773445129395
I0127 05:16:16.551830 140026050483968 logging_writer.py:48] [104200] global_step=104200, grad_norm=3.662510633468628, loss=2.40281343460083
I0127 05:16:50.459665 140026058876672 logging_writer.py:48] [104300] global_step=104300, grad_norm=3.886672019958496, loss=2.4511194229125977
I0127 05:17:24.348240 140026050483968 logging_writer.py:48] [104400] global_step=104400, grad_norm=3.960904359817505, loss=2.5329184532165527
I0127 05:17:58.270431 140026058876672 logging_writer.py:48] [104500] global_step=104500, grad_norm=4.368973731994629, loss=2.49652099609375
I0127 05:18:32.124138 140026050483968 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.442819595336914, loss=2.4307949542999268
I0127 05:19:06.032194 140026058876672 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.390934944152832, loss=2.5008416175842285
I0127 05:19:39.932463 140026050483968 logging_writer.py:48] [104800] global_step=104800, grad_norm=3.9928290843963623, loss=2.4572720527648926
I0127 05:20:13.823344 140026058876672 logging_writer.py:48] [104900] global_step=104900, grad_norm=3.630460500717163, loss=2.4229631423950195
I0127 05:20:47.766901 140026050483968 logging_writer.py:48] [105000] global_step=105000, grad_norm=4.965823173522949, loss=2.427457094192505
I0127 05:21:21.683667 140026058876672 logging_writer.py:48] [105100] global_step=105100, grad_norm=3.7177207469940186, loss=2.4663565158843994
I0127 05:21:54.403185 140187804313408 spec.py:321] Evaluating on the training split.
I0127 05:22:00.639457 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 05:22:09.631346 140187804313408 spec.py:349] Evaluating on the test split.
I0127 05:22:11.942959 140187804313408 submission_runner.py:408] Time since start: 37117.03s, 	Step: 105198, 	{'train/accuracy': 0.7359095811843872, 'train/loss': 1.1194480657577515, 'validation/accuracy': 0.6708999872207642, 'validation/loss': 1.4225313663482666, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.1047375202178955, 'test/num_examples': 10000, 'score': 35757.91668009758, 'total_duration': 37117.02583503723, 'accumulated_submission_time': 35757.91668009758, 'accumulated_eval_time': 1351.7521600723267, 'accumulated_logging_time': 3.603590250015259}
I0127 05:22:11.978437 140026050483968 logging_writer.py:48] [105198] accumulated_eval_time=1351.752160, accumulated_logging_time=3.603590, accumulated_submission_time=35757.916680, global_step=105198, preemption_count=0, score=35757.916680, test/accuracy=0.543900, test/loss=2.104738, test/num_examples=10000, total_duration=37117.025835, train/accuracy=0.735910, train/loss=1.119448, validation/accuracy=0.670900, validation/loss=1.422531, validation/num_examples=50000
I0127 05:22:13.007335 140026159523584 logging_writer.py:48] [105200] global_step=105200, grad_norm=3.976410388946533, loss=2.4253041744232178
I0127 05:22:46.894701 140026050483968 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.407947063446045, loss=2.568415880203247
I0127 05:23:20.763855 140026159523584 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.12398099899292, loss=2.4464151859283447
I0127 05:23:54.698034 140026050483968 logging_writer.py:48] [105500] global_step=105500, grad_norm=3.623220682144165, loss=2.3315987586975098
I0127 05:24:28.635546 140026159523584 logging_writer.py:48] [105600] global_step=105600, grad_norm=4.120917797088623, loss=2.4623796939849854
I0127 05:25:02.548964 140026050483968 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.075385570526123, loss=2.4577622413635254
I0127 05:25:36.456468 140026159523584 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.080421447753906, loss=2.4593772888183594
I0127 05:26:10.357378 140026050483968 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.119755744934082, loss=2.476402997970581
I0127 05:26:44.271761 140026159523584 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.043107509613037, loss=2.420222282409668
I0127 05:27:18.164986 140026050483968 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.849368095397949, loss=2.4559977054595947
I0127 05:27:52.063186 140026159523584 logging_writer.py:48] [106200] global_step=106200, grad_norm=3.7156360149383545, loss=2.44626522064209
I0127 05:28:26.022181 140026050483968 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.474815368652344, loss=2.5514090061187744
I0127 05:28:59.953018 140026159523584 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.12320613861084, loss=2.4521219730377197
I0127 05:29:33.860570 140026050483968 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.015237331390381, loss=2.431058645248413
I0127 05:30:07.761609 140026159523584 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.045912265777588, loss=2.4966564178466797
I0127 05:30:41.660105 140026050483968 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.2242889404296875, loss=2.4442214965820312
I0127 05:30:42.153833 140187804313408 spec.py:321] Evaluating on the training split.
I0127 05:30:48.318142 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 05:30:57.294192 140187804313408 spec.py:349] Evaluating on the test split.
I0127 05:30:59.575995 140187804313408 submission_runner.py:408] Time since start: 37644.66s, 	Step: 106703, 	{'train/accuracy': 0.7438616156578064, 'train/loss': 1.0795286893844604, 'validation/accuracy': 0.6801799535751343, 'validation/loss': 1.3656352758407593, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.037686824798584, 'test/num_examples': 10000, 'score': 36268.02729392052, 'total_duration': 37644.65886926651, 'accumulated_submission_time': 36268.02729392052, 'accumulated_eval_time': 1369.1742932796478, 'accumulated_logging_time': 3.651136636734009}
I0127 05:30:59.612402 140026050483968 logging_writer.py:48] [106703] accumulated_eval_time=1369.174293, accumulated_logging_time=3.651137, accumulated_submission_time=36268.027294, global_step=106703, preemption_count=0, score=36268.027294, test/accuracy=0.557400, test/loss=2.037687, test/num_examples=10000, total_duration=37644.658869, train/accuracy=0.743862, train/loss=1.079529, validation/accuracy=0.680180, validation/loss=1.365635, validation/num_examples=50000
I0127 05:31:32.793018 140026058876672 logging_writer.py:48] [106800] global_step=106800, grad_norm=4.159449577331543, loss=2.4995481967926025
I0127 05:32:06.653103 140026050483968 logging_writer.py:48] [106900] global_step=106900, grad_norm=3.5899763107299805, loss=2.4078760147094727
I0127 05:32:40.572122 140026058876672 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.9609718322753906, loss=2.5036158561706543
I0127 05:33:14.444041 140026050483968 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.981335639953613, loss=2.4487690925598145
I0127 05:33:48.312453 140026058876672 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.701204299926758, loss=2.5008366107940674
I0127 05:34:22.292498 140026050483968 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.004021167755127, loss=2.4064605236053467
I0127 05:34:56.187115 140026058876672 logging_writer.py:48] [107400] global_step=107400, grad_norm=3.945077657699585, loss=2.381207227706909
I0127 05:35:31.190624 140026050483968 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.269820213317871, loss=2.3734192848205566
I0127 05:36:05.058106 140026058876672 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.1860480308532715, loss=2.4670138359069824
I0127 05:36:38.962745 140026050483968 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.373486518859863, loss=2.435650110244751
I0127 05:37:12.885842 140026058876672 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.275124549865723, loss=2.53029465675354
I0127 05:37:46.813598 140026050483968 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.194716930389404, loss=2.3363778591156006
I0127 05:38:20.710977 140026058876672 logging_writer.py:48] [108000] global_step=108000, grad_norm=3.9619808197021484, loss=2.546088695526123
I0127 05:38:54.624103 140026050483968 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.606565952301025, loss=2.3992109298706055
I0127 05:39:28.516141 140026058876672 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.187807083129883, loss=2.4537107944488525
I0127 05:39:29.673080 140187804313408 spec.py:321] Evaluating on the training split.
I0127 05:39:35.754026 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 05:39:44.460482 140187804313408 spec.py:349] Evaluating on the test split.
I0127 05:39:46.780083 140187804313408 submission_runner.py:408] Time since start: 38171.86s, 	Step: 108205, 	{'train/accuracy': 0.7301897406578064, 'train/loss': 1.1509002447128296, 'validation/accuracy': 0.6698799729347229, 'validation/loss': 1.42633056640625, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.072263479232788, 'test/num_examples': 10000, 'score': 36778.0198366642, 'total_duration': 38171.862953186035, 'accumulated_submission_time': 36778.0198366642, 'accumulated_eval_time': 1386.2812361717224, 'accumulated_logging_time': 3.69752836227417}
I0127 05:39:46.818389 140026050483968 logging_writer.py:48] [108205] accumulated_eval_time=1386.281236, accumulated_logging_time=3.697528, accumulated_submission_time=36778.019837, global_step=108205, preemption_count=0, score=36778.019837, test/accuracy=0.543300, test/loss=2.072263, test/num_examples=10000, total_duration=38171.862953, train/accuracy=0.730190, train/loss=1.150900, validation/accuracy=0.669880, validation/loss=1.426331, validation/num_examples=50000
I0127 05:40:19.404190 140026058876672 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.429504871368408, loss=2.5189361572265625
I0127 05:40:53.250367 140026050483968 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.2096028327941895, loss=2.4667062759399414
I0127 05:41:27.151970 140026058876672 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.948782920837402, loss=2.4086685180664062
I0127 05:42:01.106348 140026050483968 logging_writer.py:48] [108600] global_step=108600, grad_norm=3.825096368789673, loss=2.394965648651123
I0127 05:42:35.028330 140026058876672 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.056812286376953, loss=2.4521923065185547
I0127 05:43:08.928710 140026050483968 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.6153788566589355, loss=2.4832448959350586
I0127 05:43:42.848963 140026058876672 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.2400946617126465, loss=2.398799419403076
I0127 05:44:16.751452 140026050483968 logging_writer.py:48] [109000] global_step=109000, grad_norm=3.982302188873291, loss=2.4137871265411377
I0127 05:44:50.661056 140026058876672 logging_writer.py:48] [109100] global_step=109100, grad_norm=4.604796886444092, loss=2.4570064544677734
I0127 05:45:24.568672 140026050483968 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.068877696990967, loss=2.3927724361419678
I0127 05:45:58.468211 140026058876672 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.158542633056641, loss=2.4121828079223633
I0127 05:46:32.446652 140026050483968 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.334082126617432, loss=2.4192428588867188
I0127 05:47:06.368909 140026058876672 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.6562819480896, loss=2.478745937347412
I0127 05:47:40.264519 140026050483968 logging_writer.py:48] [109600] global_step=109600, grad_norm=3.655007839202881, loss=2.375256061553955
I0127 05:48:14.163684 140026058876672 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.2302565574646, loss=2.475661277770996
I0127 05:48:17.021444 140187804313408 spec.py:321] Evaluating on the training split.
I0127 05:48:23.097609 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 05:48:32.078620 140187804313408 spec.py:349] Evaluating on the test split.
I0127 05:48:34.351463 140187804313408 submission_runner.py:408] Time since start: 38699.43s, 	Step: 109710, 	{'train/accuracy': 0.7444196343421936, 'train/loss': 1.0855257511138916, 'validation/accuracy': 0.6843999624252319, 'validation/loss': 1.3646774291992188, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.0237014293670654, 'test/num_examples': 10000, 'score': 37288.15925168991, 'total_duration': 38699.43433403969, 'accumulated_submission_time': 37288.15925168991, 'accumulated_eval_time': 1403.6111969947815, 'accumulated_logging_time': 3.7452445030212402}
I0127 05:48:34.390991 140026159523584 logging_writer.py:48] [109710] accumulated_eval_time=1403.611197, accumulated_logging_time=3.745245, accumulated_submission_time=37288.159252, global_step=109710, preemption_count=0, score=37288.159252, test/accuracy=0.553900, test/loss=2.023701, test/num_examples=10000, total_duration=38699.434334, train/accuracy=0.744420, train/loss=1.085526, validation/accuracy=0.684400, validation/loss=1.364677, validation/num_examples=50000
I0127 05:49:05.204269 140026176308992 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.044627666473389, loss=2.4793026447296143
I0127 05:49:39.074885 140026159523584 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.005114555358887, loss=2.4338879585266113
I0127 05:50:12.958691 140026176308992 logging_writer.py:48] [110000] global_step=110000, grad_norm=3.948591947555542, loss=2.474423408508301
I0127 05:50:46.876486 140026159523584 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.091310501098633, loss=2.37640643119812
I0127 05:51:20.763914 140026176308992 logging_writer.py:48] [110200] global_step=110200, grad_norm=3.9887471199035645, loss=2.3795018196105957
I0127 05:51:54.696584 140026159523584 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.081305027008057, loss=2.398878812789917
I0127 05:52:28.587208 140026176308992 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.913119792938232, loss=2.4669954776763916
I0127 05:53:02.556414 140026159523584 logging_writer.py:48] [110500] global_step=110500, grad_norm=3.9996414184570312, loss=2.4198544025421143
I0127 05:53:36.467909 140026176308992 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.34523868560791, loss=2.5240516662597656
I0127 05:54:10.376289 140026159523584 logging_writer.py:48] [110700] global_step=110700, grad_norm=3.884894371032715, loss=2.449475049972534
I0127 05:54:44.289702 140026176308992 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.175046443939209, loss=2.458223819732666
I0127 05:55:18.188816 140026159523584 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.339505195617676, loss=2.457821846008301
I0127 05:55:52.116206 140026176308992 logging_writer.py:48] [111000] global_step=111000, grad_norm=3.967386484146118, loss=2.3882617950439453
I0127 05:56:26.001752 140026159523584 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.786189079284668, loss=2.423027276992798
I0127 05:56:59.935924 140026176308992 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.529335975646973, loss=2.5388383865356445
I0127 05:57:04.472740 140187804313408 spec.py:321] Evaluating on the training split.
I0127 05:57:10.711513 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 05:57:19.607829 140187804313408 spec.py:349] Evaluating on the test split.
I0127 05:57:21.924772 140187804313408 submission_runner.py:408] Time since start: 39227.01s, 	Step: 111215, 	{'train/accuracy': 0.7463129758834839, 'train/loss': 1.0860766172409058, 'validation/accuracy': 0.6835599541664124, 'validation/loss': 1.3672128915786743, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.0336456298828125, 'test/num_examples': 10000, 'score': 37798.17662835121, 'total_duration': 39227.00764942169, 'accumulated_submission_time': 37798.17662835121, 'accumulated_eval_time': 1421.063180923462, 'accumulated_logging_time': 3.7959115505218506}
I0127 05:57:21.960674 140026067269376 logging_writer.py:48] [111215] accumulated_eval_time=1421.063181, accumulated_logging_time=3.795912, accumulated_submission_time=37798.176628, global_step=111215, preemption_count=0, score=37798.176628, test/accuracy=0.553900, test/loss=2.033646, test/num_examples=10000, total_duration=39227.007649, train/accuracy=0.746313, train/loss=1.086077, validation/accuracy=0.683560, validation/loss=1.367213, validation/num_examples=50000
I0127 05:57:51.087841 140026075662080 logging_writer.py:48] [111300] global_step=111300, grad_norm=3.920287609100342, loss=2.399348735809326
I0127 05:58:24.991149 140026067269376 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.370989799499512, loss=2.315944194793701
I0127 05:58:58.925277 140026075662080 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.166919708251953, loss=2.343444347381592
I0127 05:59:32.822553 140026067269376 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.309573173522949, loss=2.459879159927368
I0127 06:00:06.700058 140026075662080 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.0536675453186035, loss=2.3914616107940674
I0127 06:00:40.613508 140026067269376 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.169595241546631, loss=2.3691017627716064
I0127 06:01:14.494142 140026075662080 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.3736419677734375, loss=2.428290605545044
I0127 06:01:48.407044 140026067269376 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.803853511810303, loss=2.476898193359375
I0127 06:02:22.287161 140026075662080 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.20088529586792, loss=2.455921173095703
I0127 06:02:56.234263 140026067269376 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.020308017730713, loss=2.251110076904297
I0127 06:03:30.147079 140026075662080 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.912905693054199, loss=2.3999173641204834
I0127 06:04:04.043683 140026067269376 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.1668877601623535, loss=2.479983329772949
I0127 06:04:37.965502 140026075662080 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.513906955718994, loss=2.453432321548462
I0127 06:05:11.915006 140026067269376 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.117376327514648, loss=2.386625051498413
I0127 06:05:45.819379 140026075662080 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.31704568862915, loss=2.3834095001220703
I0127 06:05:52.055924 140187804313408 spec.py:321] Evaluating on the training split.
I0127 06:05:58.261870 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 06:06:07.243833 140187804313408 spec.py:349] Evaluating on the test split.
I0127 06:06:09.580227 140187804313408 submission_runner.py:408] Time since start: 39754.66s, 	Step: 112720, 	{'train/accuracy': 0.7732580900192261, 'train/loss': 0.9794655442237854, 'validation/accuracy': 0.6888999938964844, 'validation/loss': 1.3610857725143433, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 2.027822256088257, 'test/num_examples': 10000, 'score': 38308.20759654045, 'total_duration': 39754.66309762001, 'accumulated_submission_time': 38308.20759654045, 'accumulated_eval_time': 1438.5874259471893, 'accumulated_logging_time': 3.8423855304718018}
I0127 06:06:09.625057 140026050483968 logging_writer.py:48] [112720] accumulated_eval_time=1438.587426, accumulated_logging_time=3.842386, accumulated_submission_time=38308.207597, global_step=112720, preemption_count=0, score=38308.207597, test/accuracy=0.556700, test/loss=2.027822, test/num_examples=10000, total_duration=39754.663098, train/accuracy=0.773258, train/loss=0.979466, validation/accuracy=0.688900, validation/loss=1.361086, validation/num_examples=50000
I0127 06:06:37.027881 140026058876672 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.491449356079102, loss=2.411691188812256
I0127 06:07:10.911229 140026050483968 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.457342147827148, loss=2.468013048171997
I0127 06:07:44.809889 140026058876672 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.240532398223877, loss=2.423283815383911
I0127 06:08:18.709515 140026050483968 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.0077409744262695, loss=2.45833683013916
I0127 06:08:52.591454 140026058876672 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.918643474578857, loss=2.3384978771209717
I0127 06:09:26.519189 140026050483968 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.382397651672363, loss=2.384622573852539
I0127 06:10:00.456022 140026058876672 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.5915207862854, loss=2.444202423095703
I0127 06:10:34.363174 140026050483968 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.6949238777160645, loss=2.5078117847442627
I0127 06:11:08.352153 140026058876672 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.350430965423584, loss=2.2865793704986572
I0127 06:11:42.260147 140026050483968 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.577518463134766, loss=2.44325590133667
I0127 06:12:16.159112 140026058876672 logging_writer.py:48] [113800] global_step=113800, grad_norm=4.192428112030029, loss=2.3451521396636963
I0127 06:12:50.060812 140026050483968 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.639267921447754, loss=2.4098691940307617
I0127 06:13:23.966131 140026058876672 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.26392126083374, loss=2.363186836242676
I0127 06:13:57.844868 140026050483968 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.359783172607422, loss=2.37311053276062
I0127 06:14:31.773373 140026058876672 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.928391456604004, loss=2.5424063205718994
I0127 06:14:39.699284 140187804313408 spec.py:321] Evaluating on the training split.
I0127 06:14:45.931885 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 06:14:54.687293 140187804313408 spec.py:349] Evaluating on the test split.
I0127 06:14:56.969202 140187804313408 submission_runner.py:408] Time since start: 40282.05s, 	Step: 114225, 	{'train/accuracy': 0.7552614808082581, 'train/loss': 1.0510189533233643, 'validation/accuracy': 0.6833999752998352, 'validation/loss': 1.376708984375, 'validation/num_examples': 50000, 'test/accuracy': 0.557200014591217, 'test/loss': 2.0630359649658203, 'test/num_examples': 10000, 'score': 38818.21781897545, 'total_duration': 40282.05206513405, 'accumulated_submission_time': 38818.21781897545, 'accumulated_eval_time': 1455.8572933673859, 'accumulated_logging_time': 3.897749662399292}
I0127 06:14:57.012069 140026067269376 logging_writer.py:48] [114225] accumulated_eval_time=1455.857293, accumulated_logging_time=3.897750, accumulated_submission_time=38818.217819, global_step=114225, preemption_count=0, score=38818.217819, test/accuracy=0.557200, test/loss=2.063036, test/num_examples=10000, total_duration=40282.052065, train/accuracy=0.755261, train/loss=1.051019, validation/accuracy=0.683400, validation/loss=1.376709, validation/num_examples=50000
I0127 06:15:22.783109 140026075662080 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.008652210235596, loss=2.345823287963867
I0127 06:15:56.659015 140026067269376 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.27490234375, loss=2.4169809818267822
I0127 06:16:30.575059 140026075662080 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.30459451675415, loss=2.4020543098449707
I0127 06:17:04.503867 140026067269376 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.775064468383789, loss=2.377934217453003
I0127 06:17:38.437214 140026075662080 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.667629241943359, loss=2.41338849067688
I0127 06:18:12.375741 140026067269376 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.697245121002197, loss=2.4220521450042725
I0127 06:18:46.248347 140026075662080 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.088781833648682, loss=2.332685947418213
I0127 06:19:20.167017 140026067269376 logging_writer.py:48] [115000] global_step=115000, grad_norm=5.297969818115234, loss=2.3649749755859375
I0127 06:19:54.078515 140026075662080 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.746627330780029, loss=2.313992977142334
I0127 06:20:28.005223 140026067269376 logging_writer.py:48] [115200] global_step=115200, grad_norm=3.983231544494629, loss=2.354478120803833
I0127 06:21:01.901159 140026075662080 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.331354141235352, loss=2.3167080879211426
I0127 06:21:35.810299 140026067269376 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.057974338531494, loss=2.3310494422912598
I0127 06:22:09.738898 140026075662080 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.206114768981934, loss=2.3902342319488525
I0127 06:22:43.648724 140026067269376 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.7456583976745605, loss=2.330986738204956
I0127 06:23:17.566570 140026075662080 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.503324031829834, loss=2.3768506050109863
I0127 06:23:27.286327 140187804313408 spec.py:321] Evaluating on the training split.
I0127 06:23:33.433696 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 06:23:42.159459 140187804313408 spec.py:349] Evaluating on the test split.
I0127 06:23:44.377579 140187804313408 submission_runner.py:408] Time since start: 40809.46s, 	Step: 115730, 	{'train/accuracy': 0.7583107352256775, 'train/loss': 1.042015790939331, 'validation/accuracy': 0.6886799931526184, 'validation/loss': 1.344862461090088, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.0068650245666504, 'test/num_examples': 10000, 'score': 39328.428878068924, 'total_duration': 40809.460448265076, 'accumulated_submission_time': 39328.428878068924, 'accumulated_eval_time': 1472.9484844207764, 'accumulated_logging_time': 3.9506664276123047}
I0127 06:23:44.416866 140026159523584 logging_writer.py:48] [115730] accumulated_eval_time=1472.948484, accumulated_logging_time=3.950666, accumulated_submission_time=39328.428878, global_step=115730, preemption_count=0, score=39328.428878, test/accuracy=0.561100, test/loss=2.006865, test/num_examples=10000, total_duration=40809.460448, train/accuracy=0.758311, train/loss=1.042016, validation/accuracy=0.688680, validation/loss=1.344862, validation/num_examples=50000
I0127 06:24:08.455372 140026176308992 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.551599979400635, loss=2.3353285789489746
I0127 06:24:42.294016 140026159523584 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.360775470733643, loss=2.3118410110473633
I0127 06:25:16.213224 140026176308992 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.308072090148926, loss=2.3194997310638428
I0127 06:25:50.113681 140026159523584 logging_writer.py:48] [116100] global_step=116100, grad_norm=4.535228729248047, loss=2.4382872581481934
I0127 06:26:24.028762 140026176308992 logging_writer.py:48] [116200] global_step=116200, grad_norm=4.396334171295166, loss=2.3757686614990234
I0127 06:26:57.930769 140026159523584 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.745738983154297, loss=2.4467790126800537
I0127 06:27:31.849333 140026176308992 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.218836784362793, loss=2.336942672729492
I0127 06:28:05.725733 140026159523584 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.766848564147949, loss=2.5105199813842773
I0127 06:28:39.677085 140026176308992 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.666255474090576, loss=2.4507522583007812
I0127 06:29:13.555478 140026159523584 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.400184631347656, loss=2.3762879371643066
I0127 06:29:47.523413 140026176308992 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.796417236328125, loss=2.347409963607788
I0127 06:30:21.443623 140026159523584 logging_writer.py:48] [116900] global_step=116900, grad_norm=5.239429473876953, loss=2.399599313735962
I0127 06:30:55.317803 140026176308992 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.271702289581299, loss=2.365508556365967
I0127 06:31:29.194376 140026159523584 logging_writer.py:48] [117100] global_step=117100, grad_norm=5.018250942230225, loss=2.389284372329712
I0127 06:32:03.100965 140026176308992 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.580930233001709, loss=2.3955392837524414
I0127 06:32:14.438140 140187804313408 spec.py:321] Evaluating on the training split.
I0127 06:32:20.496074 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 06:32:29.367421 140187804313408 spec.py:349] Evaluating on the test split.
I0127 06:32:31.699805 140187804313408 submission_runner.py:408] Time since start: 41336.78s, 	Step: 117235, 	{'train/accuracy': 0.7524114847183228, 'train/loss': 1.0644334554672241, 'validation/accuracy': 0.6860799789428711, 'validation/loss': 1.3524702787399292, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.0201923847198486, 'test/num_examples': 10000, 'score': 39838.38658428192, 'total_duration': 41336.782682180405, 'accumulated_submission_time': 39838.38658428192, 'accumulated_eval_time': 1490.2100987434387, 'accumulated_logging_time': 4.000177621841431}
I0127 06:32:31.736487 140026042091264 logging_writer.py:48] [117235] accumulated_eval_time=1490.210099, accumulated_logging_time=4.000178, accumulated_submission_time=39838.386584, global_step=117235, preemption_count=0, score=39838.386584, test/accuracy=0.561800, test/loss=2.020192, test/num_examples=10000, total_duration=41336.782682, train/accuracy=0.752411, train/loss=1.064433, validation/accuracy=0.686080, validation/loss=1.352470, validation/num_examples=50000
I0127 06:32:54.098144 140026067269376 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.208919048309326, loss=2.3753714561462402
I0127 06:33:27.963668 140026042091264 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.413414478302002, loss=2.353325843811035
I0127 06:34:01.897615 140026067269376 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.245613098144531, loss=2.39019775390625
I0127 06:34:35.864033 140026042091264 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.588047981262207, loss=2.3756182193756104
I0127 06:35:09.799427 140026067269376 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.730867385864258, loss=2.310420274734497
I0127 06:35:43.764286 140026042091264 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.6971435546875, loss=2.3969318866729736
I0127 06:36:17.696490 140026067269376 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.4199090003967285, loss=2.3685054779052734
I0127 06:36:51.609985 140026042091264 logging_writer.py:48] [118000] global_step=118000, grad_norm=5.49035120010376, loss=2.337956428527832
I0127 06:37:25.533767 140026067269376 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.6463189125061035, loss=2.3569679260253906
I0127 06:37:59.450809 140026042091264 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.888104438781738, loss=2.408369779586792
I0127 06:38:33.342784 140026067269376 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.227669715881348, loss=2.2676303386688232
I0127 06:39:07.251211 140026042091264 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.595451831817627, loss=2.3855361938476562
I0127 06:39:41.204550 140026067269376 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.550756931304932, loss=2.4413700103759766
I0127 06:40:15.136053 140026042091264 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.391235828399658, loss=2.3738222122192383
I0127 06:40:49.068988 140026067269376 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.383045196533203, loss=2.309485912322998
I0127 06:41:01.768478 140187804313408 spec.py:321] Evaluating on the training split.
I0127 06:41:07.908085 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 06:41:16.637013 140187804313408 spec.py:349] Evaluating on the test split.
I0127 06:41:18.947398 140187804313408 submission_runner.py:408] Time since start: 41864.03s, 	Step: 118739, 	{'train/accuracy': 0.7526506781578064, 'train/loss': 1.0996969938278198, 'validation/accuracy': 0.6875199675559998, 'validation/loss': 1.3770368099212646, 'validation/num_examples': 50000, 'test/accuracy': 0.565500020980835, 'test/loss': 2.018036365509033, 'test/num_examples': 10000, 'score': 40348.35553407669, 'total_duration': 41864.03025341034, 'accumulated_submission_time': 40348.35553407669, 'accumulated_eval_time': 1507.3889908790588, 'accumulated_logging_time': 4.0468316078186035}
I0127 06:41:18.987417 140026075662080 logging_writer.py:48] [118739] accumulated_eval_time=1507.388991, accumulated_logging_time=4.046832, accumulated_submission_time=40348.355534, global_step=118739, preemption_count=0, score=40348.355534, test/accuracy=0.565500, test/loss=2.018036, test/num_examples=10000, total_duration=41864.030253, train/accuracy=0.752651, train/loss=1.099697, validation/accuracy=0.687520, validation/loss=1.377037, validation/num_examples=50000
I0127 06:41:39.984456 140026151130880 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.666704177856445, loss=2.4416544437408447
I0127 06:42:13.963355 140026075662080 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.420248031616211, loss=2.3164126873016357
I0127 06:42:47.813728 140026151130880 logging_writer.py:48] [119000] global_step=119000, grad_norm=4.190904140472412, loss=2.3732240200042725
I0127 06:43:21.714349 140026075662080 logging_writer.py:48] [119100] global_step=119100, grad_norm=4.724602222442627, loss=2.3887343406677246
I0127 06:43:55.636325 140026151130880 logging_writer.py:48] [119200] global_step=119200, grad_norm=5.018944263458252, loss=2.379319190979004
I0127 06:44:29.582388 140026075662080 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.263012886047363, loss=2.3143630027770996
I0127 06:45:03.469693 140026151130880 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.878970146179199, loss=2.379859447479248
I0127 06:45:37.403697 140026075662080 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.157686233520508, loss=2.300095558166504
I0127 06:46:11.306639 140026151130880 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.723892688751221, loss=2.412769317626953
I0127 06:46:45.205948 140026075662080 logging_writer.py:48] [119700] global_step=119700, grad_norm=4.533367156982422, loss=2.394273519515991
I0127 06:47:19.150934 140026151130880 logging_writer.py:48] [119800] global_step=119800, grad_norm=4.302761077880859, loss=2.3021128177642822
I0127 06:47:53.111781 140026075662080 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.502476692199707, loss=2.3801000118255615
I0127 06:48:27.000575 140026151130880 logging_writer.py:48] [120000] global_step=120000, grad_norm=4.573281288146973, loss=2.335176467895508
I0127 06:49:00.904750 140026075662080 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.734741687774658, loss=2.299431324005127
I0127 06:49:34.804144 140026151130880 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.8370137214660645, loss=2.372720718383789
I0127 06:49:49.186987 140187804313408 spec.py:321] Evaluating on the training split.
I0127 06:49:55.276736 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 06:50:04.255268 140187804313408 spec.py:349] Evaluating on the test split.
I0127 06:50:06.600647 140187804313408 submission_runner.py:408] Time since start: 42391.68s, 	Step: 120244, 	{'train/accuracy': 0.7641701102256775, 'train/loss': 1.0034244060516357, 'validation/accuracy': 0.699999988079071, 'validation/loss': 1.2891603708267212, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.9515352249145508, 'test/num_examples': 10000, 'score': 40858.48894238472, 'total_duration': 42391.6834628582, 'accumulated_submission_time': 40858.48894238472, 'accumulated_eval_time': 1524.8025405406952, 'accumulated_logging_time': 4.097309589385986}
I0127 06:50:06.643988 140026042091264 logging_writer.py:48] [120244] accumulated_eval_time=1524.802541, accumulated_logging_time=4.097310, accumulated_submission_time=40858.488942, global_step=120244, preemption_count=0, score=40858.488942, test/accuracy=0.566200, test/loss=1.951535, test/num_examples=10000, total_duration=42391.683463, train/accuracy=0.764170, train/loss=1.003424, validation/accuracy=0.700000, validation/loss=1.289160, validation/num_examples=50000
I0127 06:50:25.961277 140026050483968 logging_writer.py:48] [120300] global_step=120300, grad_norm=4.4207048416137695, loss=2.3216500282287598
I0127 06:50:59.835214 140026042091264 logging_writer.py:48] [120400] global_step=120400, grad_norm=4.613088130950928, loss=2.443376064300537
I0127 06:51:33.717102 140026050483968 logging_writer.py:48] [120500] global_step=120500, grad_norm=4.650065898895264, loss=2.3231799602508545
I0127 06:52:07.649654 140026042091264 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.753737449645996, loss=2.252019166946411
I0127 06:52:41.512079 140026050483968 logging_writer.py:48] [120700] global_step=120700, grad_norm=4.454890727996826, loss=2.3958418369293213
I0127 06:53:15.403327 140026042091264 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.471037864685059, loss=2.3229010105133057
I0127 06:53:49.314588 140026050483968 logging_writer.py:48] [120900] global_step=120900, grad_norm=5.052595615386963, loss=2.332432270050049
I0127 06:54:23.388047 140026042091264 logging_writer.py:48] [121000] global_step=121000, grad_norm=4.434714317321777, loss=2.319566488265991
I0127 06:54:57.296164 140026050483968 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.092274188995361, loss=2.2732090950012207
I0127 06:55:31.225037 140026042091264 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.795506000518799, loss=2.3899989128112793
I0127 06:56:05.107917 140026050483968 logging_writer.py:48] [121300] global_step=121300, grad_norm=4.955556869506836, loss=2.3887252807617188
I0127 06:56:39.046176 140026042091264 logging_writer.py:48] [121400] global_step=121400, grad_norm=4.259450435638428, loss=2.309758186340332
I0127 06:57:12.924696 140026050483968 logging_writer.py:48] [121500] global_step=121500, grad_norm=4.816622257232666, loss=2.2453112602233887
I0127 06:57:46.840410 140026042091264 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.607376575469971, loss=2.367065191268921
I0127 06:58:20.733724 140026050483968 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.6441802978515625, loss=2.336594343185425
I0127 06:58:36.822147 140187804313408 spec.py:321] Evaluating on the training split.
I0127 06:58:42.919072 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 06:58:51.933603 140187804313408 spec.py:349] Evaluating on the test split.
I0127 06:58:54.260111 140187804313408 submission_runner.py:408] Time since start: 42919.34s, 	Step: 121749, 	{'train/accuracy': 0.7843789458274841, 'train/loss': 0.9068344831466675, 'validation/accuracy': 0.6967200040817261, 'validation/loss': 1.3004614114761353, 'validation/num_examples': 50000, 'test/accuracy': 0.5667999982833862, 'test/loss': 1.9649931192398071, 'test/num_examples': 10000, 'score': 41368.59728837013, 'total_duration': 42919.342970609665, 'accumulated_submission_time': 41368.59728837013, 'accumulated_eval_time': 1542.2404384613037, 'accumulated_logging_time': 4.155265808105469}
I0127 06:58:54.297498 140026167916288 logging_writer.py:48] [121749] accumulated_eval_time=1542.240438, accumulated_logging_time=4.155266, accumulated_submission_time=41368.597288, global_step=121749, preemption_count=0, score=41368.597288, test/accuracy=0.566800, test/loss=1.964993, test/num_examples=10000, total_duration=42919.342971, train/accuracy=0.784379, train/loss=0.906834, validation/accuracy=0.696720, validation/loss=1.300461, validation/num_examples=50000
I0127 06:59:11.898891 140026176308992 logging_writer.py:48] [121800] global_step=121800, grad_norm=4.398266792297363, loss=2.215672492980957
I0127 06:59:45.756068 140026167916288 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.453064918518066, loss=2.3686623573303223
I0127 07:00:19.747690 140026176308992 logging_writer.py:48] [122000] global_step=122000, grad_norm=5.187708854675293, loss=2.36999249458313
I0127 07:00:53.635627 140026167916288 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.496100902557373, loss=2.282156467437744
I0127 07:01:27.528025 140026176308992 logging_writer.py:48] [122200] global_step=122200, grad_norm=4.966671943664551, loss=2.3860695362091064
I0127 07:02:01.412852 140026167916288 logging_writer.py:48] [122300] global_step=122300, grad_norm=4.547307014465332, loss=2.3242626190185547
I0127 07:02:35.324009 140026176308992 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.626219272613525, loss=2.358168125152588
I0127 07:03:09.220767 140026167916288 logging_writer.py:48] [122500] global_step=122500, grad_norm=4.691232681274414, loss=2.257876396179199
I0127 07:03:43.124002 140026176308992 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.5577216148376465, loss=2.368439197540283
I0127 07:04:17.038454 140026167916288 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.064599514007568, loss=2.282700777053833
I0127 07:04:50.935828 140026176308992 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.611791610717773, loss=2.3029067516326904
I0127 07:05:24.841995 140026167916288 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.964198112487793, loss=2.3382568359375
I0127 07:05:58.757196 140026176308992 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.019435882568359, loss=2.2309820652008057
I0127 07:06:32.728420 140026167916288 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.87018346786499, loss=2.346250534057617
I0127 07:07:06.616451 140026176308992 logging_writer.py:48] [123200] global_step=123200, grad_norm=4.381294250488281, loss=2.289951801300049
I0127 07:07:24.407954 140187804313408 spec.py:321] Evaluating on the training split.
I0127 07:07:30.479238 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 07:07:39.438979 140187804313408 spec.py:349] Evaluating on the test split.
I0127 07:07:41.768327 140187804313408 submission_runner.py:408] Time since start: 43446.85s, 	Step: 123254, 	{'train/accuracy': 0.78324294090271, 'train/loss': 0.9465728998184204, 'validation/accuracy': 0.7019199728965759, 'validation/loss': 1.2940689325332642, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9503682851791382, 'test/num_examples': 10000, 'score': 41878.64205765724, 'total_duration': 43446.8511133194, 'accumulated_submission_time': 41878.64205765724, 'accumulated_eval_time': 1559.6006872653961, 'accumulated_logging_time': 4.203623294830322}
I0127 07:07:41.807650 140026042091264 logging_writer.py:48] [123254] accumulated_eval_time=1559.600687, accumulated_logging_time=4.203623, accumulated_submission_time=41878.642058, global_step=123254, preemption_count=0, score=41878.642058, test/accuracy=0.574900, test/loss=1.950368, test/num_examples=10000, total_duration=43446.851113, train/accuracy=0.783243, train/loss=0.946573, validation/accuracy=0.701920, validation/loss=1.294069, validation/num_examples=50000
I0127 07:07:57.760249 140026050483968 logging_writer.py:48] [123300] global_step=123300, grad_norm=4.28789758682251, loss=2.367544651031494
I0127 07:08:31.605790 140026042091264 logging_writer.py:48] [123400] global_step=123400, grad_norm=4.472567081451416, loss=2.3308680057525635
I0127 07:09:05.489931 140026050483968 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.658864974975586, loss=2.3141286373138428
I0127 07:09:39.391380 140026042091264 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.384178638458252, loss=2.3762664794921875
I0127 07:10:13.295551 140026050483968 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.864699840545654, loss=2.3943235874176025
I0127 07:10:47.213567 140026042091264 logging_writer.py:48] [123800] global_step=123800, grad_norm=4.876719951629639, loss=2.2811243534088135
I0127 07:11:21.117387 140026050483968 logging_writer.py:48] [123900] global_step=123900, grad_norm=4.796325206756592, loss=2.3376882076263428
I0127 07:11:55.028200 140026042091264 logging_writer.py:48] [124000] global_step=124000, grad_norm=4.629236221313477, loss=2.3070335388183594
I0127 07:12:28.959001 140026050483968 logging_writer.py:48] [124100] global_step=124100, grad_norm=4.453520774841309, loss=2.3466267585754395
I0127 07:13:02.940900 140026042091264 logging_writer.py:48] [124200] global_step=124200, grad_norm=4.571990966796875, loss=2.361433267593384
I0127 07:13:36.853240 140026050483968 logging_writer.py:48] [124300] global_step=124300, grad_norm=4.6598639488220215, loss=2.3364479541778564
I0127 07:14:10.764372 140026042091264 logging_writer.py:48] [124400] global_step=124400, grad_norm=5.025425910949707, loss=2.30588436126709
I0127 07:14:44.699750 140026050483968 logging_writer.py:48] [124500] global_step=124500, grad_norm=4.567148685455322, loss=2.2815804481506348
I0127 07:15:18.636439 140026042091264 logging_writer.py:48] [124600] global_step=124600, grad_norm=4.790897846221924, loss=2.2691171169281006
I0127 07:15:52.524919 140026050483968 logging_writer.py:48] [124700] global_step=124700, grad_norm=4.607850074768066, loss=2.286505699157715
I0127 07:16:12.001408 140187804313408 spec.py:321] Evaluating on the training split.
I0127 07:16:18.211465 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 07:16:27.105574 140187804313408 spec.py:349] Evaluating on the test split.
I0127 07:16:29.404609 140187804313408 submission_runner.py:408] Time since start: 43974.49s, 	Step: 124759, 	{'train/accuracy': 0.7840202450752258, 'train/loss': 0.9154542088508606, 'validation/accuracy': 0.7075799703598022, 'validation/loss': 1.2590969800949097, 'validation/num_examples': 50000, 'test/accuracy': 0.578000009059906, 'test/loss': 1.9135336875915527, 'test/num_examples': 10000, 'score': 42388.77073264122, 'total_duration': 43974.48748540878, 'accumulated_submission_time': 42388.77073264122, 'accumulated_eval_time': 1577.0038568973541, 'accumulated_logging_time': 4.252768278121948}
I0127 07:16:29.441691 140026075662080 logging_writer.py:48] [124759] accumulated_eval_time=1577.003857, accumulated_logging_time=4.252768, accumulated_submission_time=42388.770733, global_step=124759, preemption_count=0, score=42388.770733, test/accuracy=0.578000, test/loss=1.913534, test/num_examples=10000, total_duration=43974.487485, train/accuracy=0.784020, train/loss=0.915454, validation/accuracy=0.707580, validation/loss=1.259097, validation/num_examples=50000
I0127 07:16:43.674927 140026151130880 logging_writer.py:48] [124800] global_step=124800, grad_norm=4.82763671875, loss=2.2290775775909424
I0127 07:17:17.550456 140026075662080 logging_writer.py:48] [124900] global_step=124900, grad_norm=4.850259304046631, loss=2.3421337604522705
I0127 07:17:51.428120 140026151130880 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.01019811630249, loss=2.2876980304718018
I0127 07:18:25.365161 140026075662080 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.474386215209961, loss=2.254716634750366
I0127 07:18:59.299625 140026151130880 logging_writer.py:48] [125200] global_step=125200, grad_norm=4.471950531005859, loss=2.237572431564331
I0127 07:19:33.210114 140026075662080 logging_writer.py:48] [125300] global_step=125300, grad_norm=4.700170040130615, loss=2.3034160137176514
I0127 07:20:07.079353 140026151130880 logging_writer.py:48] [125400] global_step=125400, grad_norm=4.958733081817627, loss=2.3410849571228027
I0127 07:20:41.039320 140026075662080 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.205319881439209, loss=2.3172965049743652
I0127 07:21:14.964068 140026151130880 logging_writer.py:48] [125600] global_step=125600, grad_norm=4.672982692718506, loss=2.289327383041382
I0127 07:21:48.868304 140026075662080 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.051142692565918, loss=2.3151051998138428
I0127 07:22:22.783850 140026151130880 logging_writer.py:48] [125800] global_step=125800, grad_norm=4.942299842834473, loss=2.313987970352173
I0127 07:22:56.688376 140026075662080 logging_writer.py:48] [125900] global_step=125900, grad_norm=4.664368152618408, loss=2.2867226600646973
I0127 07:23:30.596707 140026151130880 logging_writer.py:48] [126000] global_step=126000, grad_norm=4.986155986785889, loss=2.320631504058838
I0127 07:24:04.497750 140026075662080 logging_writer.py:48] [126100] global_step=126100, grad_norm=4.538898468017578, loss=2.2719645500183105
I0127 07:24:38.386093 140026151130880 logging_writer.py:48] [126200] global_step=126200, grad_norm=4.862328052520752, loss=2.3068675994873047
I0127 07:24:59.631598 140187804313408 spec.py:321] Evaluating on the training split.
I0127 07:25:05.924134 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 07:25:14.815932 140187804313408 spec.py:349] Evaluating on the test split.
I0127 07:25:17.128600 140187804313408 submission_runner.py:408] Time since start: 44502.21s, 	Step: 126264, 	{'train/accuracy': 0.7784199714660645, 'train/loss': 0.9533615708351135, 'validation/accuracy': 0.7020599842071533, 'validation/loss': 1.2897897958755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5743000507354736, 'test/loss': 1.9358104467391968, 'test/num_examples': 10000, 'score': 42898.89861416817, 'total_duration': 44502.21145987511, 'accumulated_submission_time': 42898.89861416817, 'accumulated_eval_time': 1594.5007934570312, 'accumulated_logging_time': 4.299294471740723}
I0127 07:25:17.179883 140026050483968 logging_writer.py:48] [126264] accumulated_eval_time=1594.500793, accumulated_logging_time=4.299294, accumulated_submission_time=42898.898614, global_step=126264, preemption_count=0, score=42898.898614, test/accuracy=0.574300, test/loss=1.935810, test/num_examples=10000, total_duration=44502.211460, train/accuracy=0.778420, train/loss=0.953362, validation/accuracy=0.702060, validation/loss=1.289790, validation/num_examples=50000
I0127 07:25:29.735658 140026058876672 logging_writer.py:48] [126300] global_step=126300, grad_norm=4.997949600219727, loss=2.27651047706604
I0127 07:26:03.540102 140026050483968 logging_writer.py:48] [126400] global_step=126400, grad_norm=4.825022220611572, loss=2.2621781826019287
I0127 07:26:37.452527 140026058876672 logging_writer.py:48] [126500] global_step=126500, grad_norm=4.627394199371338, loss=2.1860740184783936
I0127 07:27:11.364296 140026050483968 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.115971565246582, loss=2.3519177436828613
I0127 07:27:45.293129 140026058876672 logging_writer.py:48] [126700] global_step=126700, grad_norm=4.734691619873047, loss=2.3153162002563477
I0127 07:28:19.162680 140026050483968 logging_writer.py:48] [126800] global_step=126800, grad_norm=5.069713592529297, loss=2.341481924057007
I0127 07:28:53.068366 140026058876672 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.1081671714782715, loss=2.2948451042175293
I0127 07:29:26.978090 140026050483968 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.132742404937744, loss=2.2606351375579834
I0127 07:30:00.864304 140026058876672 logging_writer.py:48] [127100] global_step=127100, grad_norm=4.79905366897583, loss=2.336836099624634
I0127 07:30:34.988862 140026050483968 logging_writer.py:48] [127200] global_step=127200, grad_norm=4.630134105682373, loss=2.1996419429779053
I0127 07:31:09.065441 140026058876672 logging_writer.py:48] [127300] global_step=127300, grad_norm=4.758152961730957, loss=2.284566879272461
I0127 07:31:42.974026 140026050483968 logging_writer.py:48] [127400] global_step=127400, grad_norm=4.888067722320557, loss=2.259422540664673
I0127 07:32:16.884626 140026058876672 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.316736698150635, loss=2.2602412700653076
I0127 07:32:50.771838 140026050483968 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.005551338195801, loss=2.250153064727783
I0127 07:33:24.682924 140026058876672 logging_writer.py:48] [127700] global_step=127700, grad_norm=4.498688220977783, loss=2.2050347328186035
I0127 07:33:47.231045 140187804313408 spec.py:321] Evaluating on the training split.
I0127 07:33:53.309704 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 07:34:02.484956 140187804313408 spec.py:349] Evaluating on the test split.
I0127 07:34:04.798646 140187804313408 submission_runner.py:408] Time since start: 45029.88s, 	Step: 127768, 	{'train/accuracy': 0.7771444320678711, 'train/loss': 0.9476613402366638, 'validation/accuracy': 0.7056399583816528, 'validation/loss': 1.2707682847976685, 'validation/num_examples': 50000, 'test/accuracy': 0.5785000324249268, 'test/loss': 1.9376139640808105, 'test/num_examples': 10000, 'score': 43408.88532400131, 'total_duration': 45029.88151431084, 'accumulated_submission_time': 43408.88532400131, 'accumulated_eval_time': 1612.0683364868164, 'accumulated_logging_time': 4.361209392547607}
I0127 07:34:04.836554 140026058876672 logging_writer.py:48] [127768] accumulated_eval_time=1612.068336, accumulated_logging_time=4.361209, accumulated_submission_time=43408.885324, global_step=127768, preemption_count=0, score=43408.885324, test/accuracy=0.578500, test/loss=1.937614, test/num_examples=10000, total_duration=45029.881514, train/accuracy=0.777144, train/loss=0.947661, validation/accuracy=0.705640, validation/loss=1.270768, validation/num_examples=50000
I0127 07:34:16.019431 140026151130880 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.106462001800537, loss=2.3619015216827393
I0127 07:34:49.888515 140026058876672 logging_writer.py:48] [127900] global_step=127900, grad_norm=4.542870998382568, loss=2.2096219062805176
I0127 07:35:23.804598 140026151130880 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.286779880523682, loss=2.321568727493286
I0127 07:35:57.696288 140026058876672 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.362543106079102, loss=2.2313735485076904
I0127 07:36:31.610885 140026151130880 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.110106468200684, loss=2.2683396339416504
I0127 07:37:05.549194 140026058876672 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.279693126678467, loss=2.2864248752593994
I0127 07:37:39.555449 140026151130880 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.202923774719238, loss=2.2188501358032227
I0127 07:38:13.438063 140026058876672 logging_writer.py:48] [128500] global_step=128500, grad_norm=4.855005264282227, loss=2.290083885192871
I0127 07:38:47.363606 140026151130880 logging_writer.py:48] [128600] global_step=128600, grad_norm=4.7222771644592285, loss=2.225182056427002
I0127 07:39:21.264513 140026058876672 logging_writer.py:48] [128700] global_step=128700, grad_norm=5.533535957336426, loss=2.224698066711426
I0127 07:39:55.168931 140026151130880 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.368877410888672, loss=2.3943710327148438
I0127 07:40:29.117354 140026058876672 logging_writer.py:48] [128900] global_step=128900, grad_norm=4.940589904785156, loss=2.2923150062561035
I0127 07:41:03.055416 140026151130880 logging_writer.py:48] [129000] global_step=129000, grad_norm=5.43998908996582, loss=2.29103946685791
I0127 07:41:36.944905 140026058876672 logging_writer.py:48] [129100] global_step=129100, grad_norm=4.6988325119018555, loss=2.2560341358184814
I0127 07:42:10.850232 140026151130880 logging_writer.py:48] [129200] global_step=129200, grad_norm=5.118694305419922, loss=2.2324256896972656
I0127 07:42:35.073671 140187804313408 spec.py:321] Evaluating on the training split.
I0127 07:42:41.228111 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 07:42:49.894993 140187804313408 spec.py:349] Evaluating on the test split.
I0127 07:42:52.220759 140187804313408 submission_runner.py:408] Time since start: 45557.30s, 	Step: 129273, 	{'train/accuracy': 0.7789580225944519, 'train/loss': 0.9515725374221802, 'validation/accuracy': 0.7039799690246582, 'validation/loss': 1.283861756324768, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9563974142074585, 'test/num_examples': 10000, 'score': 43919.058978796005, 'total_duration': 45557.30363321304, 'accumulated_submission_time': 43919.058978796005, 'accumulated_eval_time': 1629.2153718471527, 'accumulated_logging_time': 4.409427642822266}
I0127 07:42:52.264131 140026042091264 logging_writer.py:48] [129273] accumulated_eval_time=1629.215372, accumulated_logging_time=4.409428, accumulated_submission_time=43919.058979, global_step=129273, preemption_count=0, score=43919.058979, test/accuracy=0.572900, test/loss=1.956397, test/num_examples=10000, total_duration=45557.303633, train/accuracy=0.778958, train/loss=0.951573, validation/accuracy=0.703980, validation/loss=1.283862, validation/num_examples=50000
I0127 07:43:01.724993 140026050483968 logging_writer.py:48] [129300] global_step=129300, grad_norm=5.613831043243408, loss=2.380864143371582
I0127 07:43:35.712989 140026042091264 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.416019916534424, loss=2.285764694213867
I0127 07:44:09.651071 140026050483968 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.548020839691162, loss=2.353738307952881
I0127 07:44:43.571082 140026042091264 logging_writer.py:48] [129600] global_step=129600, grad_norm=4.610960006713867, loss=2.261704683303833
I0127 07:45:17.541841 140026050483968 logging_writer.py:48] [129700] global_step=129700, grad_norm=4.8693928718566895, loss=2.1989192962646484
I0127 07:45:51.457784 140026042091264 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.801608562469482, loss=2.3009300231933594
I0127 07:46:25.355345 140026050483968 logging_writer.py:48] [129900] global_step=129900, grad_norm=4.9968485832214355, loss=2.2190425395965576
I0127 07:46:59.298757 140026042091264 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.441566467285156, loss=2.268026828765869
I0127 07:47:33.222754 140026050483968 logging_writer.py:48] [130100] global_step=130100, grad_norm=5.060995578765869, loss=2.2005374431610107
I0127 07:48:07.134277 140026042091264 logging_writer.py:48] [130200] global_step=130200, grad_norm=5.427159786224365, loss=2.2136759757995605
I0127 07:48:41.057710 140026050483968 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.286771774291992, loss=2.3117661476135254
I0127 07:49:14.987834 140026042091264 logging_writer.py:48] [130400] global_step=130400, grad_norm=5.119128704071045, loss=2.2723546028137207
I0127 07:49:48.936766 140026050483968 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.31838846206665, loss=2.370760917663574
I0127 07:50:22.840195 140026042091264 logging_writer.py:48] [130600] global_step=130600, grad_norm=4.931728839874268, loss=2.344921350479126
I0127 07:50:56.774081 140026050483968 logging_writer.py:48] [130700] global_step=130700, grad_norm=4.718886375427246, loss=2.1922154426574707
I0127 07:51:22.347415 140187804313408 spec.py:321] Evaluating on the training split.
I0127 07:51:28.492308 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 07:51:37.490362 140187804313408 spec.py:349] Evaluating on the test split.
I0127 07:51:39.799102 140187804313408 submission_runner.py:408] Time since start: 46084.88s, 	Step: 130777, 	{'train/accuracy': 0.8083147406578064, 'train/loss': 0.815697968006134, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.2330472469329834, 'validation/num_examples': 50000, 'test/accuracy': 0.5872000455856323, 'test/loss': 1.8761094808578491, 'test/num_examples': 10000, 'score': 44429.077756643295, 'total_duration': 46084.88196539879, 'accumulated_submission_time': 44429.077756643295, 'accumulated_eval_time': 1646.6670017242432, 'accumulated_logging_time': 4.463230848312378}
I0127 07:51:39.842372 140026042091264 logging_writer.py:48] [130777] accumulated_eval_time=1646.667002, accumulated_logging_time=4.463231, accumulated_submission_time=44429.077757, global_step=130777, preemption_count=0, score=44429.077757, test/accuracy=0.587200, test/loss=1.876109, test/num_examples=10000, total_duration=46084.881965, train/accuracy=0.808315, train/loss=0.815698, validation/accuracy=0.713520, validation/loss=1.233047, validation/num_examples=50000
I0127 07:51:47.952834 140026050483968 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.232184410095215, loss=2.2274272441864014
I0127 07:52:21.849502 140026042091264 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.3849029541015625, loss=2.2945938110351562
I0127 07:52:55.761952 140026050483968 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.463231086730957, loss=2.2489399909973145
I0127 07:53:29.666172 140026042091264 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.23043966293335, loss=2.2314743995666504
I0127 07:54:03.558850 140026050483968 logging_writer.py:48] [131200] global_step=131200, grad_norm=4.874256134033203, loss=2.211639642715454
I0127 07:54:37.461811 140026042091264 logging_writer.py:48] [131300] global_step=131300, grad_norm=5.276926517486572, loss=2.2460851669311523
I0127 07:55:11.375608 140026050483968 logging_writer.py:48] [131400] global_step=131400, grad_norm=5.604053020477295, loss=2.244767427444458
I0127 07:55:45.391388 140026042091264 logging_writer.py:48] [131500] global_step=131500, grad_norm=4.980528831481934, loss=2.141171455383301
I0127 07:56:19.289077 140026050483968 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.55555534362793, loss=2.2522692680358887
I0127 07:56:53.220969 140026042091264 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.057231903076172, loss=2.152737617492676
I0127 07:57:27.126210 140026050483968 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.057864189147949, loss=2.1284332275390625
I0127 07:58:01.041338 140026042091264 logging_writer.py:48] [131900] global_step=131900, grad_norm=4.806088447570801, loss=2.248814821243286
I0127 07:58:34.960413 140026050483968 logging_writer.py:48] [132000] global_step=132000, grad_norm=4.973300457000732, loss=2.234769582748413
I0127 07:59:08.880464 140026042091264 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.2373833656311035, loss=2.3008790016174316
I0127 07:59:42.821325 140026050483968 logging_writer.py:48] [132200] global_step=132200, grad_norm=4.979663372039795, loss=2.302578926086426
I0127 08:00:10.094367 140187804313408 spec.py:321] Evaluating on the training split.
I0127 08:00:16.193807 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 08:00:25.250739 140187804313408 spec.py:349] Evaluating on the test split.
I0127 08:00:27.499324 140187804313408 submission_runner.py:408] Time since start: 46612.58s, 	Step: 132282, 	{'train/accuracy': 0.8037906289100647, 'train/loss': 0.8384227156639099, 'validation/accuracy': 0.7140399813652039, 'validation/loss': 1.224155306816101, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8553305864334106, 'test/num_examples': 10000, 'score': 44939.26374220848, 'total_duration': 46612.582179546356, 'accumulated_submission_time': 44939.26374220848, 'accumulated_eval_time': 1664.0718927383423, 'accumulated_logging_time': 4.518594264984131}
I0127 08:00:27.540851 140026159523584 logging_writer.py:48] [132282] accumulated_eval_time=1664.071893, accumulated_logging_time=4.518594, accumulated_submission_time=44939.263742, global_step=132282, preemption_count=0, score=44939.263742, test/accuracy=0.589300, test/loss=1.855331, test/num_examples=10000, total_duration=46612.582180, train/accuracy=0.803791, train/loss=0.838423, validation/accuracy=0.714040, validation/loss=1.224155, validation/num_examples=50000
I0127 08:00:33.989149 140026167916288 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.224681377410889, loss=2.2866311073303223
I0127 08:01:07.859318 140026159523584 logging_writer.py:48] [132400] global_step=132400, grad_norm=4.9651641845703125, loss=2.239224672317505
I0127 08:01:41.747043 140026167916288 logging_writer.py:48] [132500] global_step=132500, grad_norm=5.563508987426758, loss=2.265214443206787
I0127 08:02:15.715558 140026159523584 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.6392951011657715, loss=2.212162971496582
I0127 08:02:49.598805 140026167916288 logging_writer.py:48] [132700] global_step=132700, grad_norm=5.158884048461914, loss=2.2136921882629395
I0127 08:03:23.504766 140026159523584 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.419349670410156, loss=2.2372004985809326
I0127 08:03:57.401833 140026167916288 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.176459789276123, loss=2.2593607902526855
I0127 08:04:31.282793 140026159523584 logging_writer.py:48] [133000] global_step=133000, grad_norm=5.372392654418945, loss=2.3002548217773438
I0127 08:05:05.204040 140026167916288 logging_writer.py:48] [133100] global_step=133100, grad_norm=5.717319488525391, loss=2.2017323970794678
I0127 08:05:39.137105 140026159523584 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.151527404785156, loss=2.2395894527435303
I0127 08:06:13.048897 140026167916288 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.067322254180908, loss=2.180387496948242
I0127 08:06:46.977953 140026159523584 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.359608173370361, loss=2.219801664352417
I0127 08:07:20.837624 140026167916288 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.521465301513672, loss=2.187058925628662
I0127 08:07:54.811114 140026159523584 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.408185005187988, loss=2.2123847007751465
I0127 08:08:28.702558 140026167916288 logging_writer.py:48] [133700] global_step=133700, grad_norm=4.91672420501709, loss=2.135152816772461
I0127 08:08:57.684808 140187804313408 spec.py:321] Evaluating on the training split.
I0127 08:09:03.889375 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 08:09:12.826903 140187804313408 spec.py:349] Evaluating on the test split.
I0127 08:09:15.148724 140187804313408 submission_runner.py:408] Time since start: 47140.23s, 	Step: 133787, 	{'train/accuracy': 0.8019172549247742, 'train/loss': 0.8607712388038635, 'validation/accuracy': 0.715499997138977, 'validation/loss': 1.2324215173721313, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.8591670989990234, 'test/num_examples': 10000, 'score': 45449.338752985, 'total_duration': 47140.231604099274, 'accumulated_submission_time': 45449.338752985, 'accumulated_eval_time': 1681.5357718467712, 'accumulated_logging_time': 4.574284315109253}
I0127 08:09:15.189713 140026058876672 logging_writer.py:48] [133787] accumulated_eval_time=1681.535772, accumulated_logging_time=4.574284, accumulated_submission_time=45449.338753, global_step=133787, preemption_count=0, score=45449.338753, test/accuracy=0.592900, test/loss=1.859167, test/num_examples=10000, total_duration=47140.231604, train/accuracy=0.801917, train/loss=0.860771, validation/accuracy=0.715500, validation/loss=1.232422, validation/num_examples=50000
I0127 08:09:19.920820 140026067269376 logging_writer.py:48] [133800] global_step=133800, grad_norm=5.10436487197876, loss=2.275484800338745
I0127 08:09:53.777714 140026058876672 logging_writer.py:48] [133900] global_step=133900, grad_norm=4.928086757659912, loss=2.23020601272583
I0127 08:10:27.668114 140026067269376 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.6506524085998535, loss=2.2931926250457764
I0127 08:11:01.537423 140026058876672 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.896180152893066, loss=2.298539876937866
I0127 08:11:35.449620 140026067269376 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.72742223739624, loss=2.2475147247314453
I0127 08:12:09.333592 140026058876672 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.517000675201416, loss=2.2146315574645996
I0127 08:12:43.232583 140026067269376 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.347916603088379, loss=2.169116497039795
I0127 08:13:17.134814 140026058876672 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.128696918487549, loss=2.1558022499084473
I0127 08:13:51.015888 140026067269376 logging_writer.py:48] [134600] global_step=134600, grad_norm=5.160247325897217, loss=2.1569457054138184
I0127 08:14:25.006589 140026058876672 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.818028450012207, loss=2.237253427505493
I0127 08:14:58.906201 140026067269376 logging_writer.py:48] [134800] global_step=134800, grad_norm=5.306553840637207, loss=2.239806652069092
I0127 08:15:32.848428 140026058876672 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.080873489379883, loss=2.2622246742248535
I0127 08:16:06.738750 140026067269376 logging_writer.py:48] [135000] global_step=135000, grad_norm=5.357486724853516, loss=2.1587717533111572
I0127 08:16:40.679416 140026058876672 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.39502477645874, loss=2.24473237991333
I0127 08:17:14.608676 140026067269376 logging_writer.py:48] [135200] global_step=135200, grad_norm=5.699190616607666, loss=2.264024257659912
I0127 08:17:45.270768 140187804313408 spec.py:321] Evaluating on the training split.
I0127 08:17:51.301505 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 08:18:00.068508 140187804313408 spec.py:349] Evaluating on the test split.
I0127 08:18:02.415570 140187804313408 submission_runner.py:408] Time since start: 47667.50s, 	Step: 135292, 	{'train/accuracy': 0.8028938174247742, 'train/loss': 0.844454824924469, 'validation/accuracy': 0.7201399803161621, 'validation/loss': 1.20967698097229, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8514662981033325, 'test/num_examples': 10000, 'score': 45959.35618376732, 'total_duration': 47667.49844503403, 'accumulated_submission_time': 45959.35618376732, 'accumulated_eval_time': 1698.6805226802826, 'accumulated_logging_time': 4.625326871871948}
I0127 08:18:02.455789 140026042091264 logging_writer.py:48] [135292] accumulated_eval_time=1698.680523, accumulated_logging_time=4.625327, accumulated_submission_time=45959.356184, global_step=135292, preemption_count=0, score=45959.356184, test/accuracy=0.594400, test/loss=1.851466, test/num_examples=10000, total_duration=47667.498445, train/accuracy=0.802894, train/loss=0.844455, validation/accuracy=0.720140, validation/loss=1.209677, validation/num_examples=50000
I0127 08:18:05.496219 140026151130880 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.416314601898193, loss=2.252962112426758
I0127 08:18:39.340957 140026042091264 logging_writer.py:48] [135400] global_step=135400, grad_norm=5.318693161010742, loss=2.2546937465667725
I0127 08:19:13.244066 140026151130880 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.09621524810791, loss=2.161254405975342
I0127 08:19:47.141120 140026042091264 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.3164238929748535, loss=2.2255752086639404
I0127 08:20:21.124375 140026151130880 logging_writer.py:48] [135700] global_step=135700, grad_norm=5.442934513092041, loss=2.213825225830078
I0127 08:20:55.017367 140026042091264 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.406680107116699, loss=2.197737455368042
I0127 08:21:28.918791 140026151130880 logging_writer.py:48] [135900] global_step=135900, grad_norm=4.797821044921875, loss=2.1369194984436035
I0127 08:22:02.814965 140026042091264 logging_writer.py:48] [136000] global_step=136000, grad_norm=4.928188323974609, loss=2.17199444770813
I0127 08:22:36.736015 140026151130880 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.126610279083252, loss=2.2105817794799805
I0127 08:23:10.641081 140026042091264 logging_writer.py:48] [136200] global_step=136200, grad_norm=5.2548956871032715, loss=2.1368675231933594
I0127 08:23:44.571171 140026151130880 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.178229331970215, loss=2.125579833984375
I0127 08:24:18.468429 140026042091264 logging_writer.py:48] [136400] global_step=136400, grad_norm=5.026782512664795, loss=2.1751999855041504
I0127 08:24:52.369013 140026151130880 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.13667106628418, loss=2.2617640495300293
I0127 08:25:26.279903 140026042091264 logging_writer.py:48] [136600] global_step=136600, grad_norm=5.547479152679443, loss=2.21895170211792
I0127 08:26:00.181329 140026151130880 logging_writer.py:48] [136700] global_step=136700, grad_norm=5.7056474685668945, loss=2.1533148288726807
I0127 08:26:32.625298 140187804313408 spec.py:321] Evaluating on the training split.
I0127 08:26:38.668175 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 08:26:47.393875 140187804313408 spec.py:349] Evaluating on the test split.
I0127 08:26:49.722889 140187804313408 submission_runner.py:408] Time since start: 48194.81s, 	Step: 136797, 	{'train/accuracy': 0.8018972873687744, 'train/loss': 0.8656412959098816, 'validation/accuracy': 0.7197399735450745, 'validation/loss': 1.2183958292007446, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.8558731079101562, 'test/num_examples': 10000, 'score': 46469.46252202988, 'total_duration': 48194.805307626724, 'accumulated_submission_time': 46469.46252202988, 'accumulated_eval_time': 1715.7776033878326, 'accumulated_logging_time': 4.675408601760864}
I0127 08:26:49.768311 140026167916288 logging_writer.py:48] [136797] accumulated_eval_time=1715.777603, accumulated_logging_time=4.675409, accumulated_submission_time=46469.462522, global_step=136797, preemption_count=0, score=46469.462522, test/accuracy=0.592700, test/loss=1.855873, test/num_examples=10000, total_duration=48194.805308, train/accuracy=0.801897, train/loss=0.865641, validation/accuracy=0.719740, validation/loss=1.218396, validation/num_examples=50000
I0127 08:26:51.138053 140026176308992 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.214852333068848, loss=2.1268715858459473
I0127 08:27:24.990657 140026167916288 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.154865741729736, loss=2.074145793914795
I0127 08:27:58.866307 140026176308992 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.120267391204834, loss=2.290771484375
I0127 08:28:32.744024 140026167916288 logging_writer.py:48] [137100] global_step=137100, grad_norm=5.7240753173828125, loss=2.214226722717285
I0127 08:29:06.637084 140026176308992 logging_writer.py:48] [137200] global_step=137200, grad_norm=6.27016019821167, loss=2.22314190864563
I0127 08:29:40.557528 140026167916288 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.346155643463135, loss=2.101501941680908
I0127 08:30:14.452442 140026176308992 logging_writer.py:48] [137400] global_step=137400, grad_norm=5.444846153259277, loss=2.1965177059173584
I0127 08:30:48.366003 140026167916288 logging_writer.py:48] [137500] global_step=137500, grad_norm=5.435645580291748, loss=2.1337101459503174
I0127 08:31:22.265056 140026176308992 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.567656993865967, loss=2.1927103996276855
I0127 08:31:56.182011 140026167916288 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.776443004608154, loss=2.2378408908843994
I0127 08:32:30.043028 140026176308992 logging_writer.py:48] [137800] global_step=137800, grad_norm=5.97406005859375, loss=2.1693925857543945
I0127 08:33:04.005164 140026167916288 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.108036994934082, loss=2.203423500061035
I0127 08:33:37.896850 140026176308992 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.764305591583252, loss=2.1489202976226807
I0127 08:34:11.764949 140026167916288 logging_writer.py:48] [138100] global_step=138100, grad_norm=5.443096160888672, loss=2.2416746616363525
I0127 08:34:45.653626 140026176308992 logging_writer.py:48] [138200] global_step=138200, grad_norm=5.48352575302124, loss=2.1724328994750977
I0127 08:35:19.559352 140026167916288 logging_writer.py:48] [138300] global_step=138300, grad_norm=5.752431869506836, loss=2.2187211513519287
I0127 08:35:20.048366 140187804313408 spec.py:321] Evaluating on the training split.
I0127 08:35:26.144955 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 08:35:35.092400 140187804313408 spec.py:349] Evaluating on the test split.
I0127 08:35:37.379933 140187804313408 submission_runner.py:408] Time since start: 48722.46s, 	Step: 138303, 	{'train/accuracy': 0.8061822056770325, 'train/loss': 0.8341156244277954, 'validation/accuracy': 0.7231799960136414, 'validation/loss': 1.196844220161438, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.8155568838119507, 'test/num_examples': 10000, 'score': 46979.67847561836, 'total_duration': 48722.46280956268, 'accumulated_submission_time': 46979.67847561836, 'accumulated_eval_time': 1733.1091315746307, 'accumulated_logging_time': 4.73111629486084}
I0127 08:35:37.420224 140026075662080 logging_writer.py:48] [138303] accumulated_eval_time=1733.109132, accumulated_logging_time=4.731116, accumulated_submission_time=46979.678476, global_step=138303, preemption_count=0, score=46979.678476, test/accuracy=0.602200, test/loss=1.815557, test/num_examples=10000, total_duration=48722.462810, train/accuracy=0.806182, train/loss=0.834116, validation/accuracy=0.723180, validation/loss=1.196844, validation/num_examples=50000
I0127 08:36:10.620824 140026176308992 logging_writer.py:48] [138400] global_step=138400, grad_norm=5.257725238800049, loss=2.163299798965454
I0127 08:36:44.459002 140026075662080 logging_writer.py:48] [138500] global_step=138500, grad_norm=5.623497009277344, loss=2.1431753635406494
I0127 08:37:18.335218 140026176308992 logging_writer.py:48] [138600] global_step=138600, grad_norm=5.4374237060546875, loss=2.1376922130584717
I0127 08:37:52.266697 140026075662080 logging_writer.py:48] [138700] global_step=138700, grad_norm=5.419795036315918, loss=2.1198344230651855
I0127 08:38:26.168795 140026176308992 logging_writer.py:48] [138800] global_step=138800, grad_norm=5.065165042877197, loss=2.1610324382781982
I0127 08:39:00.148301 140026075662080 logging_writer.py:48] [138900] global_step=138900, grad_norm=5.415761947631836, loss=2.176304817199707
I0127 08:39:34.075040 140026176308992 logging_writer.py:48] [139000] global_step=139000, grad_norm=5.528193473815918, loss=2.223787784576416
I0127 08:40:07.991045 140026075662080 logging_writer.py:48] [139100] global_step=139100, grad_norm=5.418331623077393, loss=2.179719924926758
I0127 08:40:41.919583 140026176308992 logging_writer.py:48] [139200] global_step=139200, grad_norm=5.9004292488098145, loss=2.1602630615234375
I0127 08:41:15.859561 140026075662080 logging_writer.py:48] [139300] global_step=139300, grad_norm=5.701276779174805, loss=2.1923747062683105
I0127 08:41:49.782658 140026176308992 logging_writer.py:48] [139400] global_step=139400, grad_norm=5.301414489746094, loss=2.1225929260253906
I0127 08:42:23.682750 140026075662080 logging_writer.py:48] [139500] global_step=139500, grad_norm=5.644302845001221, loss=2.1328258514404297
I0127 08:42:57.603292 140026176308992 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.41795015335083, loss=2.18060564994812
I0127 08:43:31.495847 140026075662080 logging_writer.py:48] [139700] global_step=139700, grad_norm=5.457915782928467, loss=2.1947569847106934
I0127 08:44:05.424328 140026176308992 logging_writer.py:48] [139800] global_step=139800, grad_norm=5.561614036560059, loss=2.1632840633392334
I0127 08:44:07.591019 140187804313408 spec.py:321] Evaluating on the training split.
I0127 08:44:13.700726 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 08:44:22.517797 140187804313408 spec.py:349] Evaluating on the test split.
I0127 08:44:24.806473 140187804313408 submission_runner.py:408] Time since start: 49249.89s, 	Step: 139808, 	{'train/accuracy': 0.8362762928009033, 'train/loss': 0.7129075527191162, 'validation/accuracy': 0.7239599823951721, 'validation/loss': 1.1824004650115967, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.808569073677063, 'test/num_examples': 10000, 'score': 47489.7852473259, 'total_duration': 49249.88934969902, 'accumulated_submission_time': 47489.7852473259, 'accumulated_eval_time': 1750.324533700943, 'accumulated_logging_time': 4.7820470333099365}
I0127 08:44:24.847369 140026042091264 logging_writer.py:48] [139808] accumulated_eval_time=1750.324534, accumulated_logging_time=4.782047, accumulated_submission_time=47489.785247, global_step=139808, preemption_count=0, score=47489.785247, test/accuracy=0.601900, test/loss=1.808569, test/num_examples=10000, total_duration=49249.889350, train/accuracy=0.836276, train/loss=0.712908, validation/accuracy=0.723960, validation/loss=1.182400, validation/num_examples=50000
I0127 08:44:56.402781 140026050483968 logging_writer.py:48] [139900] global_step=139900, grad_norm=5.7862067222595215, loss=2.1577558517456055
I0127 08:45:30.283598 140026042091264 logging_writer.py:48] [140000] global_step=140000, grad_norm=5.6264495849609375, loss=2.179464817047119
I0127 08:46:04.178720 140026050483968 logging_writer.py:48] [140100] global_step=140100, grad_norm=5.715020656585693, loss=2.1576528549194336
I0127 08:46:38.079554 140026042091264 logging_writer.py:48] [140200] global_step=140200, grad_norm=5.5722270011901855, loss=2.1476221084594727
I0127 08:47:12.003341 140026050483968 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.488579750061035, loss=2.167821168899536
I0127 08:47:45.894869 140026042091264 logging_writer.py:48] [140400] global_step=140400, grad_norm=6.093377590179443, loss=2.1869277954101562
I0127 08:48:19.803826 140026050483968 logging_writer.py:48] [140500] global_step=140500, grad_norm=6.569947719573975, loss=2.1740684509277344
I0127 08:48:53.712598 140026042091264 logging_writer.py:48] [140600] global_step=140600, grad_norm=5.408058166503906, loss=2.10853910446167
I0127 08:49:27.640060 140026050483968 logging_writer.py:48] [140700] global_step=140700, grad_norm=6.303810119628906, loss=2.177337646484375
I0127 08:50:01.529524 140026042091264 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.0254364013671875, loss=2.1543986797332764
I0127 08:50:35.434714 140026050483968 logging_writer.py:48] [140900] global_step=140900, grad_norm=6.3826212882995605, loss=2.2155685424804688
I0127 08:51:09.389068 140026042091264 logging_writer.py:48] [141000] global_step=141000, grad_norm=5.8154497146606445, loss=2.1860618591308594
I0127 08:51:43.284931 140026050483968 logging_writer.py:48] [141100] global_step=141100, grad_norm=5.394862174987793, loss=2.1520497798919678
I0127 08:52:17.154943 140026042091264 logging_writer.py:48] [141200] global_step=141200, grad_norm=5.678049087524414, loss=2.1174471378326416
I0127 08:52:51.063576 140026050483968 logging_writer.py:48] [141300] global_step=141300, grad_norm=5.97076416015625, loss=2.124596357345581
I0127 08:52:54.933525 140187804313408 spec.py:321] Evaluating on the training split.
I0127 08:53:01.088197 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 08:53:10.163006 140187804313408 spec.py:349] Evaluating on the test split.
I0127 08:53:12.430531 140187804313408 submission_runner.py:408] Time since start: 49777.51s, 	Step: 141313, 	{'train/accuracy': 0.8279455900192261, 'train/loss': 0.7402110695838928, 'validation/accuracy': 0.727679967880249, 'validation/loss': 1.1678305864334106, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.8135945796966553, 'test/num_examples': 10000, 'score': 47999.80777215958, 'total_duration': 49777.51341342926, 'accumulated_submission_time': 47999.80777215958, 'accumulated_eval_time': 1767.8215091228485, 'accumulated_logging_time': 4.8334338665008545}
I0127 08:53:12.474370 140026050483968 logging_writer.py:48] [141313] accumulated_eval_time=1767.821509, accumulated_logging_time=4.833434, accumulated_submission_time=47999.807772, global_step=141313, preemption_count=0, score=47999.807772, test/accuracy=0.605600, test/loss=1.813595, test/num_examples=10000, total_duration=49777.513413, train/accuracy=0.827946, train/loss=0.740211, validation/accuracy=0.727680, validation/loss=1.167831, validation/num_examples=50000
I0127 08:53:42.312981 140026151130880 logging_writer.py:48] [141400] global_step=141400, grad_norm=5.16378116607666, loss=2.094041585922241
I0127 08:54:16.190402 140026050483968 logging_writer.py:48] [141500] global_step=141500, grad_norm=6.243452072143555, loss=2.104811668395996
I0127 08:54:50.091324 140026151130880 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.025623798370361, loss=2.1030845642089844
I0127 08:55:23.986331 140026050483968 logging_writer.py:48] [141700] global_step=141700, grad_norm=5.668252468109131, loss=2.183305501937866
I0127 08:55:57.904462 140026151130880 logging_writer.py:48] [141800] global_step=141800, grad_norm=5.932690620422363, loss=2.1658544540405273
I0127 08:56:31.814903 140026050483968 logging_writer.py:48] [141900] global_step=141900, grad_norm=5.972758769989014, loss=2.087740182876587
I0127 08:57:05.757066 140026151130880 logging_writer.py:48] [142000] global_step=142000, grad_norm=5.874500751495361, loss=2.0502943992614746
I0127 08:57:39.705329 140026050483968 logging_writer.py:48] [142100] global_step=142100, grad_norm=5.377716541290283, loss=2.1059627532958984
I0127 08:58:13.636377 140026151130880 logging_writer.py:48] [142200] global_step=142200, grad_norm=6.074958324432373, loss=2.1217174530029297
I0127 08:58:47.545570 140026050483968 logging_writer.py:48] [142300] global_step=142300, grad_norm=5.863874435424805, loss=2.2037298679351807
I0127 08:59:21.466482 140026151130880 logging_writer.py:48] [142400] global_step=142400, grad_norm=5.90083646774292, loss=2.100562334060669
I0127 08:59:55.397450 140026050483968 logging_writer.py:48] [142500] global_step=142500, grad_norm=5.6063103675842285, loss=2.125847339630127
I0127 09:00:29.304854 140026151130880 logging_writer.py:48] [142600] global_step=142600, grad_norm=5.8476128578186035, loss=2.0949013233184814
I0127 09:01:03.233017 140026050483968 logging_writer.py:48] [142700] global_step=142700, grad_norm=6.753626346588135, loss=2.132437229156494
I0127 09:01:37.192090 140026151130880 logging_writer.py:48] [142800] global_step=142800, grad_norm=5.48087739944458, loss=2.1274735927581787
I0127 09:01:42.761382 140187804313408 spec.py:321] Evaluating on the training split.
I0127 09:01:48.859417 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 09:01:57.622082 140187804313408 spec.py:349] Evaluating on the test split.
I0127 09:01:59.897412 140187804313408 submission_runner.py:408] Time since start: 50304.98s, 	Step: 142818, 	{'train/accuracy': 0.8234215378761292, 'train/loss': 0.757717490196228, 'validation/accuracy': 0.729479968547821, 'validation/loss': 1.166062593460083, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.7834153175354004, 'test/num_examples': 10000, 'score': 48510.02785205841, 'total_duration': 50304.98026776314, 'accumulated_submission_time': 48510.02785205841, 'accumulated_eval_time': 1784.9574799537659, 'accumulated_logging_time': 4.888729572296143}
I0127 09:01:59.944069 140026067269376 logging_writer.py:48] [142818] accumulated_eval_time=1784.957480, accumulated_logging_time=4.888730, accumulated_submission_time=48510.027852, global_step=142818, preemption_count=0, score=48510.027852, test/accuracy=0.604000, test/loss=1.783415, test/num_examples=10000, total_duration=50304.980268, train/accuracy=0.823422, train/loss=0.757717, validation/accuracy=0.729480, validation/loss=1.166063, validation/num_examples=50000
I0127 09:02:28.035073 140026075662080 logging_writer.py:48] [142900] global_step=142900, grad_norm=6.085147380828857, loss=2.2575433254241943
I0127 09:03:01.937786 140026067269376 logging_writer.py:48] [143000] global_step=143000, grad_norm=5.7569050788879395, loss=2.147380828857422
I0127 09:03:35.844737 140026075662080 logging_writer.py:48] [143100] global_step=143100, grad_norm=6.055605888366699, loss=2.0758566856384277
I0127 09:04:09.747427 140026067269376 logging_writer.py:48] [143200] global_step=143200, grad_norm=5.711363315582275, loss=2.0668458938598633
I0127 09:04:43.612568 140026075662080 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.068598747253418, loss=2.1535189151763916
I0127 09:05:17.503374 140026067269376 logging_writer.py:48] [143400] global_step=143400, grad_norm=5.940159797668457, loss=2.1063854694366455
I0127 09:05:51.431870 140026075662080 logging_writer.py:48] [143500] global_step=143500, grad_norm=5.971693992614746, loss=2.078152656555176
I0127 09:06:25.343680 140026067269376 logging_writer.py:48] [143600] global_step=143600, grad_norm=5.742143630981445, loss=2.1748549938201904
I0127 09:06:59.275231 140026075662080 logging_writer.py:48] [143700] global_step=143700, grad_norm=5.883481025695801, loss=2.124725818634033
I0127 09:07:33.191971 140026067269376 logging_writer.py:48] [143800] global_step=143800, grad_norm=5.90303897857666, loss=2.0574822425842285
I0127 09:08:07.109664 140026075662080 logging_writer.py:48] [143900] global_step=143900, grad_norm=6.429196357727051, loss=2.105907917022705
I0127 09:08:41.032003 140026067269376 logging_writer.py:48] [144000] global_step=144000, grad_norm=6.320889949798584, loss=2.179612636566162
I0127 09:09:14.916800 140026075662080 logging_writer.py:48] [144100] global_step=144100, grad_norm=6.559700012207031, loss=2.173380136489868
I0127 09:09:48.899681 140026067269376 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.073679447174072, loss=2.118513345718384
I0127 09:10:22.808452 140026075662080 logging_writer.py:48] [144300] global_step=144300, grad_norm=5.852452754974365, loss=2.1137101650238037
I0127 09:10:30.097982 140187804313408 spec.py:321] Evaluating on the training split.
I0127 09:10:36.372918 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 09:10:45.201085 140187804313408 spec.py:349] Evaluating on the test split.
I0127 09:10:47.501676 140187804313408 submission_runner.py:408] Time since start: 50832.58s, 	Step: 144323, 	{'train/accuracy': 0.832429826259613, 'train/loss': 0.7265508770942688, 'validation/accuracy': 0.7337599992752075, 'validation/loss': 1.1390953063964844, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.7489664554595947, 'test/num_examples': 10000, 'score': 49020.1178920269, 'total_duration': 50832.58455181122, 'accumulated_submission_time': 49020.1178920269, 'accumulated_eval_time': 1802.3611364364624, 'accumulated_logging_time': 4.945554733276367}
I0127 09:10:47.544230 140026050483968 logging_writer.py:48] [144323] accumulated_eval_time=1802.361136, accumulated_logging_time=4.945555, accumulated_submission_time=49020.117892, global_step=144323, preemption_count=0, score=49020.117892, test/accuracy=0.617800, test/loss=1.748966, test/num_examples=10000, total_duration=50832.584552, train/accuracy=0.832430, train/loss=0.726551, validation/accuracy=0.733760, validation/loss=1.139095, validation/num_examples=50000
I0127 09:11:13.915461 140026058876672 logging_writer.py:48] [144400] global_step=144400, grad_norm=5.50372838973999, loss=2.1248795986175537
I0127 09:11:47.830097 140026050483968 logging_writer.py:48] [144500] global_step=144500, grad_norm=5.364564418792725, loss=2.096147298812866
I0127 09:12:21.719428 140026058876672 logging_writer.py:48] [144600] global_step=144600, grad_norm=5.741308689117432, loss=2.1123180389404297
I0127 09:12:55.645012 140026050483968 logging_writer.py:48] [144700] global_step=144700, grad_norm=6.8424530029296875, loss=2.157780647277832
I0127 09:13:29.563305 140026058876672 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.643800735473633, loss=2.1442923545837402
I0127 09:14:03.485599 140026050483968 logging_writer.py:48] [144900] global_step=144900, grad_norm=6.284236907958984, loss=2.1653332710266113
I0127 09:14:37.398062 140026058876672 logging_writer.py:48] [145000] global_step=145000, grad_norm=6.446435928344727, loss=2.124230146408081
I0127 09:15:11.325463 140026050483968 logging_writer.py:48] [145100] global_step=145100, grad_norm=5.654735088348389, loss=2.090188980102539
I0127 09:15:45.302128 140026058876672 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.081845760345459, loss=2.0917506217956543
I0127 09:16:19.224117 140026050483968 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.291576385498047, loss=2.117877244949341
I0127 09:16:53.176543 140026058876672 logging_writer.py:48] [145400] global_step=145400, grad_norm=5.82375431060791, loss=2.036674737930298
I0127 09:17:27.105242 140026050483968 logging_writer.py:48] [145500] global_step=145500, grad_norm=5.685153484344482, loss=2.053378105163574
I0127 09:18:01.027096 140026058876672 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.299025058746338, loss=2.044086456298828
I0127 09:18:34.953154 140026050483968 logging_writer.py:48] [145700] global_step=145700, grad_norm=6.659217834472656, loss=2.085526704788208
I0127 09:19:08.858991 140026058876672 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.235352516174316, loss=2.175055980682373
I0127 09:19:17.830249 140187804313408 spec.py:321] Evaluating on the training split.
I0127 09:19:23.922967 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 09:19:32.804453 140187804313408 spec.py:349] Evaluating on the test split.
I0127 09:19:35.083278 140187804313408 submission_runner.py:408] Time since start: 51360.17s, 	Step: 145828, 	{'train/accuracy': 0.8336455225944519, 'train/loss': 0.722388744354248, 'validation/accuracy': 0.7360000014305115, 'validation/loss': 1.13362717628479, 'validation/num_examples': 50000, 'test/accuracy': 0.6071000099182129, 'test/loss': 1.7726635932922363, 'test/num_examples': 10000, 'score': 49530.3389377594, 'total_duration': 51360.16606426239, 'accumulated_submission_time': 49530.3389377594, 'accumulated_eval_time': 1819.614022731781, 'accumulated_logging_time': 4.998095750808716}
I0127 09:19:35.124962 140026058876672 logging_writer.py:48] [145828] accumulated_eval_time=1819.614023, accumulated_logging_time=4.998096, accumulated_submission_time=49530.338938, global_step=145828, preemption_count=0, score=49530.338938, test/accuracy=0.607100, test/loss=1.772664, test/num_examples=10000, total_duration=51360.166064, train/accuracy=0.833646, train/loss=0.722389, validation/accuracy=0.736000, validation/loss=1.133627, validation/num_examples=50000
I0127 09:19:59.844148 140026067269376 logging_writer.py:48] [145900] global_step=145900, grad_norm=5.422291278839111, loss=2.0795345306396484
I0127 09:20:33.714694 140026058876672 logging_writer.py:48] [146000] global_step=146000, grad_norm=6.011080265045166, loss=2.1418275833129883
I0127 09:21:07.644950 140026067269376 logging_writer.py:48] [146100] global_step=146100, grad_norm=6.306201934814453, loss=2.0629775524139404
I0127 09:21:41.517868 140026058876672 logging_writer.py:48] [146200] global_step=146200, grad_norm=5.935089588165283, loss=2.1075515747070312
I0127 09:22:15.475276 140026067269376 logging_writer.py:48] [146300] global_step=146300, grad_norm=5.898000717163086, loss=2.060781478881836
I0127 09:22:49.354874 140026058876672 logging_writer.py:48] [146400] global_step=146400, grad_norm=6.490942001342773, loss=2.0791361331939697
I0127 09:23:23.280764 140026067269376 logging_writer.py:48] [146500] global_step=146500, grad_norm=6.19254207611084, loss=2.083482503890991
I0127 09:23:57.156140 140026058876672 logging_writer.py:48] [146600] global_step=146600, grad_norm=5.934210777282715, loss=2.10134220123291
I0127 09:24:31.088160 140026067269376 logging_writer.py:48] [146700] global_step=146700, grad_norm=6.040417671203613, loss=2.06099796295166
I0127 09:25:04.982709 140026058876672 logging_writer.py:48] [146800] global_step=146800, grad_norm=6.428039073944092, loss=2.0893795490264893
I0127 09:25:38.890759 140026067269376 logging_writer.py:48] [146900] global_step=146900, grad_norm=5.717761039733887, loss=2.123192071914673
I0127 09:26:12.785151 140026058876672 logging_writer.py:48] [147000] global_step=147000, grad_norm=6.1645965576171875, loss=2.139150857925415
I0127 09:26:46.704658 140026067269376 logging_writer.py:48] [147100] global_step=147100, grad_norm=5.898373603820801, loss=2.0878896713256836
I0127 09:27:20.571288 140026058876672 logging_writer.py:48] [147200] global_step=147200, grad_norm=5.886660099029541, loss=2.0578737258911133
I0127 09:27:54.503584 140026067269376 logging_writer.py:48] [147300] global_step=147300, grad_norm=5.7514238357543945, loss=2.0014352798461914
I0127 09:28:05.216763 140187804313408 spec.py:321] Evaluating on the training split.
I0127 09:28:11.270850 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 09:28:20.050429 140187804313408 spec.py:349] Evaluating on the test split.
I0127 09:28:22.393955 140187804313408 submission_runner.py:408] Time since start: 51887.48s, 	Step: 147333, 	{'train/accuracy': 0.8375119566917419, 'train/loss': 0.7116343975067139, 'validation/accuracy': 0.7391600012779236, 'validation/loss': 1.1251534223556519, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.7439031600952148, 'test/num_examples': 10000, 'score': 50040.367628097534, 'total_duration': 51887.47683286667, 'accumulated_submission_time': 50040.367628097534, 'accumulated_eval_time': 1836.7911660671234, 'accumulated_logging_time': 5.049542188644409}
I0127 09:28:22.439724 140026159523584 logging_writer.py:48] [147333] accumulated_eval_time=1836.791166, accumulated_logging_time=5.049542, accumulated_submission_time=50040.367628, global_step=147333, preemption_count=0, score=50040.367628, test/accuracy=0.615900, test/loss=1.743903, test/num_examples=10000, total_duration=51887.476833, train/accuracy=0.837512, train/loss=0.711634, validation/accuracy=0.739160, validation/loss=1.125153, validation/num_examples=50000
I0127 09:28:45.484515 140026167916288 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.097975730895996, loss=2.0550625324249268
I0127 09:29:19.352169 140026159523584 logging_writer.py:48] [147500] global_step=147500, grad_norm=5.754580497741699, loss=2.060936450958252
I0127 09:29:53.282403 140026167916288 logging_writer.py:48] [147600] global_step=147600, grad_norm=6.326064109802246, loss=2.0807886123657227
I0127 09:30:27.196966 140026159523584 logging_writer.py:48] [147700] global_step=147700, grad_norm=6.535181999206543, loss=2.109924077987671
I0127 09:31:01.097872 140026167916288 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.92075777053833, loss=2.1032607555389404
I0127 09:31:34.989916 140026159523584 logging_writer.py:48] [147900] global_step=147900, grad_norm=6.230408191680908, loss=2.1299796104431152
I0127 09:32:08.905354 140026167916288 logging_writer.py:48] [148000] global_step=148000, grad_norm=5.918160915374756, loss=2.0534117221832275
I0127 09:32:42.804536 140026159523584 logging_writer.py:48] [148100] global_step=148100, grad_norm=5.930842399597168, loss=2.0307183265686035
I0127 09:33:16.751292 140026167916288 logging_writer.py:48] [148200] global_step=148200, grad_norm=6.460635662078857, loss=2.0064988136291504
I0127 09:33:50.643170 140026159523584 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.088584899902344, loss=1.9668869972229004
I0127 09:34:24.738497 140026167916288 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.743505477905273, loss=2.0380852222442627
I0127 09:34:58.624793 140026159523584 logging_writer.py:48] [148500] global_step=148500, grad_norm=5.891897201538086, loss=2.0663046836853027
I0127 09:35:32.555898 140026167916288 logging_writer.py:48] [148600] global_step=148600, grad_norm=6.1743645668029785, loss=2.051236152648926
I0127 09:36:06.511474 140026159523584 logging_writer.py:48] [148700] global_step=148700, grad_norm=5.9549970626831055, loss=2.092398166656494
I0127 09:36:40.440517 140026167916288 logging_writer.py:48] [148800] global_step=148800, grad_norm=6.41605281829834, loss=1.9867874383926392
I0127 09:36:52.465108 140187804313408 spec.py:321] Evaluating on the training split.
I0127 09:36:58.579789 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 09:37:07.558509 140187804313408 spec.py:349] Evaluating on the test split.
I0127 09:37:09.946773 140187804313408 submission_runner.py:408] Time since start: 52415.03s, 	Step: 148837, 	{'train/accuracy': 0.8622449040412903, 'train/loss': 0.6133614182472229, 'validation/accuracy': 0.7362200021743774, 'validation/loss': 1.1291574239730835, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.7641743421554565, 'test/num_examples': 10000, 'score': 50550.328521966934, 'total_duration': 52415.02962350845, 'accumulated_submission_time': 50550.328521966934, 'accumulated_eval_time': 1854.272777080536, 'accumulated_logging_time': 5.106694459915161}
I0127 09:37:09.989845 140026050483968 logging_writer.py:48] [148837] accumulated_eval_time=1854.272777, accumulated_logging_time=5.106694, accumulated_submission_time=50550.328522, global_step=148837, preemption_count=0, score=50550.328522, test/accuracy=0.615600, test/loss=1.764174, test/num_examples=10000, total_duration=52415.029624, train/accuracy=0.862245, train/loss=0.613361, validation/accuracy=0.736220, validation/loss=1.129157, validation/num_examples=50000
I0127 09:37:31.673873 140026058876672 logging_writer.py:48] [148900] global_step=148900, grad_norm=5.976118564605713, loss=2.0164482593536377
I0127 09:38:05.536034 140026050483968 logging_writer.py:48] [149000] global_step=149000, grad_norm=6.363506317138672, loss=2.1314094066619873
I0127 09:38:39.432641 140026058876672 logging_writer.py:48] [149100] global_step=149100, grad_norm=6.192903995513916, loss=2.098721742630005
I0127 09:39:13.327238 140026050483968 logging_writer.py:48] [149200] global_step=149200, grad_norm=5.855014801025391, loss=2.005807876586914
I0127 09:39:47.234668 140026058876672 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.612080097198486, loss=2.0768580436706543
I0127 09:40:21.346736 140026050483968 logging_writer.py:48] [149400] global_step=149400, grad_norm=5.579790115356445, loss=2.0426454544067383
I0127 09:40:55.209736 140026058876672 logging_writer.py:48] [149500] global_step=149500, grad_norm=6.6326003074646, loss=2.0771682262420654
I0127 09:41:29.148070 140026050483968 logging_writer.py:48] [149600] global_step=149600, grad_norm=5.781462669372559, loss=2.0859766006469727
I0127 09:42:03.075660 140026058876672 logging_writer.py:48] [149700] global_step=149700, grad_norm=5.890028953552246, loss=2.0642237663269043
I0127 09:42:36.972328 140026050483968 logging_writer.py:48] [149800] global_step=149800, grad_norm=7.391788482666016, loss=2.0939879417419434
I0127 09:43:10.884780 140026058876672 logging_writer.py:48] [149900] global_step=149900, grad_norm=6.13355827331543, loss=2.0291101932525635
I0127 09:43:44.772751 140026050483968 logging_writer.py:48] [150000] global_step=150000, grad_norm=7.061862468719482, loss=2.0505008697509766
I0127 09:44:18.664686 140026058876672 logging_writer.py:48] [150100] global_step=150100, grad_norm=6.397164344787598, loss=2.0185093879699707
I0127 09:44:52.576556 140026050483968 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.273865699768066, loss=1.9714884757995605
I0127 09:45:26.504279 140026058876672 logging_writer.py:48] [150300] global_step=150300, grad_norm=6.352560997009277, loss=2.0343260765075684
I0127 09:45:40.181110 140187804313408 spec.py:321] Evaluating on the training split.
I0127 09:45:46.265983 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 09:45:55.160877 140187804313408 spec.py:349] Evaluating on the test split.
I0127 09:45:57.461702 140187804313408 submission_runner.py:408] Time since start: 52942.54s, 	Step: 150342, 	{'train/accuracy': 0.8563655614852905, 'train/loss': 0.637763261795044, 'validation/accuracy': 0.7451800107955933, 'validation/loss': 1.1021682024002075, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.7354857921600342, 'test/num_examples': 10000, 'score': 51060.45466089249, 'total_duration': 52942.5445561409, 'accumulated_submission_time': 51060.45466089249, 'accumulated_eval_time': 1871.5532939434052, 'accumulated_logging_time': 5.160580635070801}
I0127 09:45:57.502279 140026050483968 logging_writer.py:48] [150342] accumulated_eval_time=1871.553294, accumulated_logging_time=5.160581, accumulated_submission_time=51060.454661, global_step=150342, preemption_count=0, score=51060.454661, test/accuracy=0.617700, test/loss=1.735486, test/num_examples=10000, total_duration=52942.544556, train/accuracy=0.856366, train/loss=0.637763, validation/accuracy=0.745180, validation/loss=1.102168, validation/num_examples=50000
I0127 09:46:17.515612 140026151130880 logging_writer.py:48] [150400] global_step=150400, grad_norm=6.207460403442383, loss=1.9671753644943237
I0127 09:46:51.497094 140026050483968 logging_writer.py:48] [150500] global_step=150500, grad_norm=6.120126724243164, loss=2.0684103965759277
I0127 09:47:25.417316 140026151130880 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.799393177032471, loss=2.1190860271453857
I0127 09:47:59.349050 140026050483968 logging_writer.py:48] [150700] global_step=150700, grad_norm=6.463006496429443, loss=1.9972832202911377
I0127 09:48:33.272885 140026151130880 logging_writer.py:48] [150800] global_step=150800, grad_norm=6.689637660980225, loss=1.9780527353286743
I0127 09:49:07.212123 140026050483968 logging_writer.py:48] [150900] global_step=150900, grad_norm=7.074612617492676, loss=2.161297559738159
I0127 09:49:41.126354 140026151130880 logging_writer.py:48] [151000] global_step=151000, grad_norm=5.944238185882568, loss=1.9736392498016357
I0127 09:50:15.047724 140026050483968 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.551972389221191, loss=2.0652403831481934
I0127 09:50:48.943123 140026151130880 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.3171186447143555, loss=2.014799118041992
I0127 09:51:22.872096 140026050483968 logging_writer.py:48] [151300] global_step=151300, grad_norm=5.85405969619751, loss=2.0261549949645996
I0127 09:51:56.762576 140026151130880 logging_writer.py:48] [151400] global_step=151400, grad_norm=6.059118270874023, loss=2.061811923980713
I0127 09:52:30.682355 140026050483968 logging_writer.py:48] [151500] global_step=151500, grad_norm=6.820746421813965, loss=2.0482170581817627
I0127 09:53:04.666215 140026151130880 logging_writer.py:48] [151600] global_step=151600, grad_norm=6.750731945037842, loss=2.0882978439331055
I0127 09:53:38.570872 140026050483968 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.05513334274292, loss=2.006477117538452
I0127 09:54:12.519191 140026151130880 logging_writer.py:48] [151800] global_step=151800, grad_norm=6.514369964599609, loss=2.0285587310791016
I0127 09:54:27.592529 140187804313408 spec.py:321] Evaluating on the training split.
I0127 09:54:34.272379 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 09:54:43.401078 140187804313408 spec.py:349] Evaluating on the test split.
I0127 09:54:45.672352 140187804313408 submission_runner.py:408] Time since start: 53470.76s, 	Step: 151846, 	{'train/accuracy': 0.8573222160339355, 'train/loss': 0.6447144746780396, 'validation/accuracy': 0.7456799745559692, 'validation/loss': 1.0978809595108032, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.743034839630127, 'test/num_examples': 10000, 'score': 51570.480170726776, 'total_duration': 53470.75521993637, 'accumulated_submission_time': 51570.480170726776, 'accumulated_eval_time': 1889.6330585479736, 'accumulated_logging_time': 5.211879253387451}
I0127 09:54:45.715706 140026067269376 logging_writer.py:48] [151846] accumulated_eval_time=1889.633059, accumulated_logging_time=5.211879, accumulated_submission_time=51570.480171, global_step=151846, preemption_count=0, score=51570.480171, test/accuracy=0.617700, test/loss=1.743035, test/num_examples=10000, total_duration=53470.755220, train/accuracy=0.857322, train/loss=0.644714, validation/accuracy=0.745680, validation/loss=1.097881, validation/num_examples=50000
I0127 09:55:04.340333 140026075662080 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.163012981414795, loss=2.0625061988830566
I0127 09:55:38.208192 140026067269376 logging_writer.py:48] [152000] global_step=152000, grad_norm=6.687175750732422, loss=2.096529483795166
I0127 09:56:12.088159 140026075662080 logging_writer.py:48] [152100] global_step=152100, grad_norm=6.672936916351318, loss=2.0917599201202393
I0127 09:56:45.998682 140026067269376 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.0055341720581055, loss=2.0548832416534424
I0127 09:57:19.903489 140026075662080 logging_writer.py:48] [152300] global_step=152300, grad_norm=6.263744354248047, loss=2.05924129486084
I0127 09:57:53.834354 140026067269376 logging_writer.py:48] [152400] global_step=152400, grad_norm=6.474575042724609, loss=1.9937106370925903
I0127 09:58:27.776889 140026075662080 logging_writer.py:48] [152500] global_step=152500, grad_norm=6.641602039337158, loss=1.9861444234848022
I0127 09:59:01.708190 140026067269376 logging_writer.py:48] [152600] global_step=152600, grad_norm=5.886133670806885, loss=2.066558361053467
I0127 09:59:35.600142 140026075662080 logging_writer.py:48] [152700] global_step=152700, grad_norm=6.305741786956787, loss=1.9851152896881104
I0127 10:00:09.516082 140026067269376 logging_writer.py:48] [152800] global_step=152800, grad_norm=6.7402472496032715, loss=2.0195298194885254
I0127 10:00:43.432684 140026075662080 logging_writer.py:48] [152900] global_step=152900, grad_norm=6.613620281219482, loss=2.0515267848968506
I0127 10:01:17.314102 140026067269376 logging_writer.py:48] [153000] global_step=153000, grad_norm=6.930911540985107, loss=2.0377001762390137
I0127 10:01:51.243709 140026075662080 logging_writer.py:48] [153100] global_step=153100, grad_norm=6.4018874168396, loss=2.0426084995269775
I0127 10:02:25.130676 140026067269376 logging_writer.py:48] [153200] global_step=153200, grad_norm=6.443772792816162, loss=1.9633560180664062
I0127 10:02:59.081151 140026075662080 logging_writer.py:48] [153300] global_step=153300, grad_norm=6.981008052825928, loss=2.0627193450927734
I0127 10:03:15.843621 140187804313408 spec.py:321] Evaluating on the training split.
I0127 10:03:21.969587 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 10:03:30.904672 140187804313408 spec.py:349] Evaluating on the test split.
I0127 10:03:33.212162 140187804313408 submission_runner.py:408] Time since start: 53998.30s, 	Step: 153351, 	{'train/accuracy': 0.8552295565605164, 'train/loss': 0.6356821060180664, 'validation/accuracy': 0.7467799782752991, 'validation/loss': 1.086501955986023, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.7114746570587158, 'test/num_examples': 10000, 'score': 52080.545378923416, 'total_duration': 53998.295038700104, 'accumulated_submission_time': 52080.545378923416, 'accumulated_eval_time': 1907.0015578269958, 'accumulated_logging_time': 5.265218734741211}
I0127 10:03:33.254132 140026050483968 logging_writer.py:48] [153351] accumulated_eval_time=1907.001558, accumulated_logging_time=5.265219, accumulated_submission_time=52080.545379, global_step=153351, preemption_count=0, score=52080.545379, test/accuracy=0.624300, test/loss=1.711475, test/num_examples=10000, total_duration=53998.295039, train/accuracy=0.855230, train/loss=0.635682, validation/accuracy=0.746780, validation/loss=1.086502, validation/num_examples=50000
I0127 10:03:50.237047 140026058876672 logging_writer.py:48] [153400] global_step=153400, grad_norm=6.483532905578613, loss=2.0847015380859375
I0127 10:04:24.147383 140026050483968 logging_writer.py:48] [153500] global_step=153500, grad_norm=6.267752647399902, loss=2.042867422103882
I0127 10:04:58.133863 140026058876672 logging_writer.py:48] [153600] global_step=153600, grad_norm=6.390479564666748, loss=2.011443853378296
I0127 10:05:32.039100 140026050483968 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.401498317718506, loss=1.9534287452697754
I0127 10:06:05.960646 140026058876672 logging_writer.py:48] [153800] global_step=153800, grad_norm=6.36201286315918, loss=1.9558101892471313
I0127 10:06:39.874201 140026050483968 logging_writer.py:48] [153900] global_step=153900, grad_norm=6.185624122619629, loss=1.9705724716186523
I0127 10:07:13.813742 140026058876672 logging_writer.py:48] [154000] global_step=154000, grad_norm=6.721349239349365, loss=2.0035400390625
I0127 10:07:47.730410 140026050483968 logging_writer.py:48] [154100] global_step=154100, grad_norm=6.866756916046143, loss=2.0987393856048584
I0127 10:08:21.670460 140026058876672 logging_writer.py:48] [154200] global_step=154200, grad_norm=6.3227996826171875, loss=1.9348946809768677
I0127 10:08:55.581662 140026050483968 logging_writer.py:48] [154300] global_step=154300, grad_norm=6.918083667755127, loss=2.0410311222076416
I0127 10:09:29.473259 140026058876672 logging_writer.py:48] [154400] global_step=154400, grad_norm=6.673347473144531, loss=1.8990275859832764
I0127 10:10:03.398698 140026050483968 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.313526153564453, loss=2.0484838485717773
I0127 10:10:37.280423 140026058876672 logging_writer.py:48] [154600] global_step=154600, grad_norm=5.859514236450195, loss=1.9189406633377075
I0127 10:11:11.248594 140026050483968 logging_writer.py:48] [154700] global_step=154700, grad_norm=6.414256572723389, loss=1.9280264377593994
I0127 10:11:45.173297 140026058876672 logging_writer.py:48] [154800] global_step=154800, grad_norm=6.23227596282959, loss=2.0580101013183594
I0127 10:12:03.281224 140187804313408 spec.py:321] Evaluating on the training split.
I0127 10:12:09.297050 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 10:12:18.169669 140187804313408 spec.py:349] Evaluating on the test split.
I0127 10:12:20.434765 140187804313408 submission_runner.py:408] Time since start: 54525.52s, 	Step: 154855, 	{'train/accuracy': 0.8594945669174194, 'train/loss': 0.6123570203781128, 'validation/accuracy': 0.7492199540138245, 'validation/loss': 1.0768321752548218, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.6894590854644775, 'test/num_examples': 10000, 'score': 52590.51016449928, 'total_duration': 54525.517634391785, 'accumulated_submission_time': 52590.51016449928, 'accumulated_eval_time': 1924.1550514698029, 'accumulated_logging_time': 5.317140579223633}
I0127 10:12:20.479527 140026075662080 logging_writer.py:48] [154855] accumulated_eval_time=1924.155051, accumulated_logging_time=5.317141, accumulated_submission_time=52590.510164, global_step=154855, preemption_count=0, score=52590.510164, test/accuracy=0.626400, test/loss=1.689459, test/num_examples=10000, total_duration=54525.517634, train/accuracy=0.859495, train/loss=0.612357, validation/accuracy=0.749220, validation/loss=1.076832, validation/num_examples=50000
I0127 10:12:36.043967 140026167916288 logging_writer.py:48] [154900] global_step=154900, grad_norm=6.686344146728516, loss=1.899965763092041
I0127 10:13:09.913343 140026075662080 logging_writer.py:48] [155000] global_step=155000, grad_norm=6.84406852722168, loss=1.9895305633544922
I0127 10:13:43.832270 140026167916288 logging_writer.py:48] [155100] global_step=155100, grad_norm=6.884700298309326, loss=1.972702980041504
I0127 10:14:17.789283 140026075662080 logging_writer.py:48] [155200] global_step=155200, grad_norm=6.765017509460449, loss=2.012375593185425
I0127 10:14:51.704791 140026167916288 logging_writer.py:48] [155300] global_step=155300, grad_norm=6.880112648010254, loss=2.037806749343872
I0127 10:15:25.615633 140026075662080 logging_writer.py:48] [155400] global_step=155400, grad_norm=6.8418707847595215, loss=1.9933557510375977
I0127 10:15:59.546243 140026167916288 logging_writer.py:48] [155500] global_step=155500, grad_norm=6.503964424133301, loss=1.9166607856750488
I0127 10:16:33.433264 140026075662080 logging_writer.py:48] [155600] global_step=155600, grad_norm=6.721812725067139, loss=1.9878051280975342
I0127 10:17:07.563845 140026167916288 logging_writer.py:48] [155700] global_step=155700, grad_norm=6.5578932762146, loss=2.016273260116577
I0127 10:17:41.426172 140026075662080 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.520502090454102, loss=2.0229909420013428
I0127 10:18:15.359321 140026167916288 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.170796871185303, loss=2.03928279876709
I0127 10:18:49.246790 140026075662080 logging_writer.py:48] [156000] global_step=156000, grad_norm=6.536003589630127, loss=1.9823838472366333
I0127 10:19:23.177432 140026167916288 logging_writer.py:48] [156100] global_step=156100, grad_norm=6.154617786407471, loss=1.934617280960083
I0127 10:19:57.047954 140026075662080 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.085198402404785, loss=1.9829314947128296
I0127 10:20:30.997173 140026167916288 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.164404392242432, loss=1.999843955039978
I0127 10:20:50.448033 140187804313408 spec.py:321] Evaluating on the training split.
I0127 10:20:56.543947 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 10:21:05.692338 140187804313408 spec.py:349] Evaluating on the test split.
I0127 10:21:08.473545 140187804313408 submission_runner.py:408] Time since start: 55053.56s, 	Step: 156359, 	{'train/accuracy': 0.861348032951355, 'train/loss': 0.6097769141197205, 'validation/accuracy': 0.7504799962043762, 'validation/loss': 1.0702511072158813, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.6836930513381958, 'test/num_examples': 10000, 'score': 53100.41500091553, 'total_duration': 55053.556411504745, 'accumulated_submission_time': 53100.41500091553, 'accumulated_eval_time': 1942.180507183075, 'accumulated_logging_time': 5.372523307800293}
I0127 10:21:08.509463 140026058876672 logging_writer.py:48] [156359] accumulated_eval_time=1942.180507, accumulated_logging_time=5.372523, accumulated_submission_time=53100.415001, global_step=156359, preemption_count=0, score=53100.415001, test/accuracy=0.628600, test/loss=1.683693, test/num_examples=10000, total_duration=55053.556412, train/accuracy=0.861348, train/loss=0.609777, validation/accuracy=0.750480, validation/loss=1.070251, validation/num_examples=50000
I0127 10:21:22.738059 140026067269376 logging_writer.py:48] [156400] global_step=156400, grad_norm=6.6538519859313965, loss=1.9742097854614258
I0127 10:21:56.564757 140026058876672 logging_writer.py:48] [156500] global_step=156500, grad_norm=6.330656051635742, loss=1.9555952548980713
I0127 10:22:30.486882 140026067269376 logging_writer.py:48] [156600] global_step=156600, grad_norm=6.570887565612793, loss=2.001462936401367
I0127 10:23:04.435300 140026058876672 logging_writer.py:48] [156700] global_step=156700, grad_norm=6.808892726898193, loss=1.9739948511123657
I0127 10:23:38.463442 140026067269376 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.074760913848877, loss=1.9538369178771973
I0127 10:24:12.398297 140026058876672 logging_writer.py:48] [156900] global_step=156900, grad_norm=6.885613441467285, loss=1.9231128692626953
I0127 10:24:46.356406 140026067269376 logging_writer.py:48] [157000] global_step=157000, grad_norm=7.261640548706055, loss=1.9883689880371094
I0127 10:25:20.279389 140026058876672 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.583667278289795, loss=2.022320508956909
I0127 10:25:54.212545 140026067269376 logging_writer.py:48] [157200] global_step=157200, grad_norm=7.072617053985596, loss=1.9152588844299316
I0127 10:26:28.164604 140026058876672 logging_writer.py:48] [157300] global_step=157300, grad_norm=6.594588279724121, loss=1.9832545518875122
I0127 10:27:02.105815 140026067269376 logging_writer.py:48] [157400] global_step=157400, grad_norm=6.975288391113281, loss=1.9765413999557495
I0127 10:27:36.009065 140026058876672 logging_writer.py:48] [157500] global_step=157500, grad_norm=6.943084239959717, loss=1.908009648323059
I0127 10:28:09.945915 140026067269376 logging_writer.py:48] [157600] global_step=157600, grad_norm=6.633910179138184, loss=1.978576898574829
I0127 10:28:43.875461 140026058876672 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.043360233306885, loss=1.9935988187789917
I0127 10:29:17.794973 140026067269376 logging_writer.py:48] [157800] global_step=157800, grad_norm=6.845682621002197, loss=1.944392204284668
I0127 10:29:38.710547 140187804313408 spec.py:321] Evaluating on the training split.
I0127 10:29:44.814447 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 10:29:53.784408 140187804313408 spec.py:349] Evaluating on the test split.
I0127 10:29:56.098263 140187804313408 submission_runner.py:408] Time since start: 55581.18s, 	Step: 157863, 	{'train/accuracy': 0.8867984414100647, 'train/loss': 0.5364611744880676, 'validation/accuracy': 0.7528600096702576, 'validation/loss': 1.0770390033721924, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.6905221939086914, 'test/num_examples': 10000, 'score': 53610.552248716354, 'total_duration': 55581.18113279343, 'accumulated_submission_time': 53610.552248716354, 'accumulated_eval_time': 1959.5681657791138, 'accumulated_logging_time': 5.417553663253784}
I0127 10:29:56.145041 140026058876672 logging_writer.py:48] [157863] accumulated_eval_time=1959.568166, accumulated_logging_time=5.417554, accumulated_submission_time=53610.552249, global_step=157863, preemption_count=0, score=53610.552249, test/accuracy=0.627600, test/loss=1.690522, test/num_examples=10000, total_duration=55581.181133, train/accuracy=0.886798, train/loss=0.536461, validation/accuracy=0.752860, validation/loss=1.077039, validation/num_examples=50000
I0127 10:30:09.034518 140026159523584 logging_writer.py:48] [157900] global_step=157900, grad_norm=7.450242042541504, loss=2.018383264541626
I0127 10:30:42.872146 140026058876672 logging_writer.py:48] [158000] global_step=158000, grad_norm=7.10711145401001, loss=1.9063903093338013
I0127 10:31:16.779458 140026159523584 logging_writer.py:48] [158100] global_step=158100, grad_norm=7.177637100219727, loss=1.9524656534194946
I0127 10:31:50.675843 140026058876672 logging_writer.py:48] [158200] global_step=158200, grad_norm=7.08253812789917, loss=1.9625375270843506
I0127 10:32:24.536132 140026159523584 logging_writer.py:48] [158300] global_step=158300, grad_norm=6.706003189086914, loss=1.9919277429580688
I0127 10:32:58.467574 140026058876672 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.098618030548096, loss=1.9974782466888428
I0127 10:33:32.428196 140026159523584 logging_writer.py:48] [158500] global_step=158500, grad_norm=7.351057529449463, loss=2.0923337936401367
I0127 10:34:06.319635 140026058876672 logging_writer.py:48] [158600] global_step=158600, grad_norm=6.896575927734375, loss=1.92682683467865
I0127 10:34:40.251704 140026159523584 logging_writer.py:48] [158700] global_step=158700, grad_norm=6.986037254333496, loss=1.9679286479949951
I0127 10:35:14.159037 140026058876672 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.225819110870361, loss=1.985048770904541
I0127 10:35:48.225577 140026159523584 logging_writer.py:48] [158900] global_step=158900, grad_norm=6.610015869140625, loss=1.992983102798462
I0127 10:36:22.107273 140026058876672 logging_writer.py:48] [159000] global_step=159000, grad_norm=7.823366165161133, loss=1.9883517026901245
I0127 10:36:56.015516 140026159523584 logging_writer.py:48] [159100] global_step=159100, grad_norm=7.561598777770996, loss=1.9595452547073364
I0127 10:37:29.915340 140026058876672 logging_writer.py:48] [159200] global_step=159200, grad_norm=6.808052062988281, loss=1.9416110515594482
I0127 10:38:03.830277 140026159523584 logging_writer.py:48] [159300] global_step=159300, grad_norm=6.996287822723389, loss=1.97001314163208
I0127 10:38:26.355527 140187804313408 spec.py:321] Evaluating on the training split.
I0127 10:38:32.519519 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 10:38:41.501932 140187804313408 spec.py:349] Evaluating on the test split.
I0127 10:38:43.824518 140187804313408 submission_runner.py:408] Time since start: 56108.91s, 	Step: 159368, 	{'train/accuracy': 0.8807397484779358, 'train/loss': 0.5454962849617004, 'validation/accuracy': 0.7541199922561646, 'validation/loss': 1.0671308040618896, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.6812326908111572, 'test/num_examples': 10000, 'score': 54120.69530892372, 'total_duration': 56108.90738940239, 'accumulated_submission_time': 54120.69530892372, 'accumulated_eval_time': 1977.037132024765, 'accumulated_logging_time': 5.476731061935425}
I0127 10:38:43.874310 140026058876672 logging_writer.py:48] [159368] accumulated_eval_time=1977.037132, accumulated_logging_time=5.476731, accumulated_submission_time=54120.695309, global_step=159368, preemption_count=0, score=54120.695309, test/accuracy=0.630400, test/loss=1.681233, test/num_examples=10000, total_duration=56108.907389, train/accuracy=0.880740, train/loss=0.545496, validation/accuracy=0.754120, validation/loss=1.067131, validation/num_examples=50000
I0127 10:38:55.054635 140026067269376 logging_writer.py:48] [159400] global_step=159400, grad_norm=6.42026424407959, loss=1.944032073020935
I0127 10:39:28.965130 140026058876672 logging_writer.py:48] [159500] global_step=159500, grad_norm=6.556851863861084, loss=1.9625277519226074
I0127 10:40:02.907619 140026067269376 logging_writer.py:48] [159600] global_step=159600, grad_norm=7.174050807952881, loss=1.9374628067016602
I0127 10:40:36.837080 140026058876672 logging_writer.py:48] [159700] global_step=159700, grad_norm=7.3567094802856445, loss=1.9443402290344238
I0127 10:41:10.748200 140026067269376 logging_writer.py:48] [159800] global_step=159800, grad_norm=6.814488410949707, loss=1.963357925415039
I0127 10:41:44.663360 140026058876672 logging_writer.py:48] [159900] global_step=159900, grad_norm=6.99188232421875, loss=1.9797934293746948
I0127 10:42:18.698422 140026067269376 logging_writer.py:48] [160000] global_step=160000, grad_norm=6.861777305603027, loss=2.021131992340088
I0127 10:42:52.629689 140026058876672 logging_writer.py:48] [160100] global_step=160100, grad_norm=7.040738105773926, loss=1.936692476272583
I0127 10:43:26.549948 140026067269376 logging_writer.py:48] [160200] global_step=160200, grad_norm=6.599379062652588, loss=1.8969954252243042
I0127 10:44:00.464666 140026058876672 logging_writer.py:48] [160300] global_step=160300, grad_norm=7.634988784790039, loss=1.9912617206573486
I0127 10:44:34.369649 140026067269376 logging_writer.py:48] [160400] global_step=160400, grad_norm=7.601545810699463, loss=1.9478107690811157
I0127 10:45:08.292932 140026058876672 logging_writer.py:48] [160500] global_step=160500, grad_norm=7.1773681640625, loss=1.9552628993988037
I0127 10:45:42.231603 140026067269376 logging_writer.py:48] [160600] global_step=160600, grad_norm=6.891651153564453, loss=1.9032317399978638
I0127 10:46:16.186051 140026058876672 logging_writer.py:48] [160700] global_step=160700, grad_norm=7.427582740783691, loss=1.9188305139541626
I0127 10:46:50.127559 140026067269376 logging_writer.py:48] [160800] global_step=160800, grad_norm=6.855623245239258, loss=1.9153058528900146
I0127 10:47:14.036275 140187804313408 spec.py:321] Evaluating on the training split.
I0127 10:47:20.106193 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 10:47:28.934795 140187804313408 spec.py:349] Evaluating on the test split.
I0127 10:47:31.192569 140187804313408 submission_runner.py:408] Time since start: 56636.28s, 	Step: 160872, 	{'train/accuracy': 0.882254421710968, 'train/loss': 0.5351816415786743, 'validation/accuracy': 0.7583799958229065, 'validation/loss': 1.0473202466964722, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.6694732904434204, 'test/num_examples': 10000, 'score': 54630.79351758957, 'total_duration': 56636.27542257309, 'accumulated_submission_time': 54630.79351758957, 'accumulated_eval_time': 1994.1933534145355, 'accumulated_logging_time': 5.536860942840576}
I0127 10:47:31.235468 140026151130880 logging_writer.py:48] [160872] accumulated_eval_time=1994.193353, accumulated_logging_time=5.536861, accumulated_submission_time=54630.793518, global_step=160872, preemption_count=0, score=54630.793518, test/accuracy=0.632000, test/loss=1.669473, test/num_examples=10000, total_duration=56636.275423, train/accuracy=0.882254, train/loss=0.535182, validation/accuracy=0.758380, validation/loss=1.047320, validation/num_examples=50000
I0127 10:47:41.056329 140026159523584 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.113852024078369, loss=1.894090175628662
I0127 10:48:14.989867 140026151130880 logging_writer.py:48] [161000] global_step=161000, grad_norm=6.641897678375244, loss=1.9251296520233154
I0127 10:48:48.909065 140026159523584 logging_writer.py:48] [161100] global_step=161100, grad_norm=7.127837181091309, loss=1.8535798788070679
I0127 10:49:22.826791 140026151130880 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.24166202545166, loss=1.9563119411468506
I0127 10:49:56.744321 140026159523584 logging_writer.py:48] [161300] global_step=161300, grad_norm=6.738456726074219, loss=1.951897382736206
I0127 10:50:30.660260 140026151130880 logging_writer.py:48] [161400] global_step=161400, grad_norm=7.925612926483154, loss=1.9199402332305908
I0127 10:51:04.579492 140026159523584 logging_writer.py:48] [161500] global_step=161500, grad_norm=8.122515678405762, loss=1.8921624422073364
I0127 10:51:38.514188 140026151130880 logging_writer.py:48] [161600] global_step=161600, grad_norm=7.681222438812256, loss=1.885412573814392
I0127 10:52:12.435352 140026159523584 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.393649578094482, loss=1.9103682041168213
I0127 10:52:46.358536 140026151130880 logging_writer.py:48] [161800] global_step=161800, grad_norm=7.354171276092529, loss=1.893298864364624
I0127 10:53:20.221798 140026159523584 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.149814605712891, loss=1.8810240030288696
I0127 10:53:54.134131 140026151130880 logging_writer.py:48] [162000] global_step=162000, grad_norm=7.3003034591674805, loss=1.8974905014038086
I0127 10:54:28.109309 140026159523584 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.144917011260986, loss=1.975355625152588
I0127 10:55:02.030901 140026151130880 logging_writer.py:48] [162200] global_step=162200, grad_norm=6.9661970138549805, loss=1.9743579626083374
I0127 10:55:35.962631 140026159523584 logging_writer.py:48] [162300] global_step=162300, grad_norm=7.327320098876953, loss=1.894598364830017
I0127 10:56:01.199578 140187804313408 spec.py:321] Evaluating on the training split.
I0127 10:56:07.494502 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 10:56:16.498921 140187804313408 spec.py:349] Evaluating on the test split.
I0127 10:56:18.836660 140187804313408 submission_runner.py:408] Time since start: 57163.92s, 	Step: 162376, 	{'train/accuracy': 0.8803212642669678, 'train/loss': 0.5474730134010315, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 1.0637322664260864, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.6772630214691162, 'test/num_examples': 10000, 'score': 55140.68953895569, 'total_duration': 57163.919515132904, 'accumulated_submission_time': 55140.68953895569, 'accumulated_eval_time': 2011.8303670883179, 'accumulated_logging_time': 5.594317197799683}
I0127 10:56:18.884399 140026067269376 logging_writer.py:48] [162376] accumulated_eval_time=2011.830367, accumulated_logging_time=5.594317, accumulated_submission_time=55140.689539, global_step=162376, preemption_count=0, score=55140.689539, test/accuracy=0.637800, test/loss=1.677263, test/num_examples=10000, total_duration=57163.919515, train/accuracy=0.880321, train/loss=0.547473, validation/accuracy=0.754560, validation/loss=1.063732, validation/num_examples=50000
I0127 10:56:27.353086 140026075662080 logging_writer.py:48] [162400] global_step=162400, grad_norm=7.281028747558594, loss=1.9610544443130493
I0127 10:57:01.241948 140026067269376 logging_writer.py:48] [162500] global_step=162500, grad_norm=7.382462024688721, loss=1.894836664199829
I0127 10:57:35.093134 140026075662080 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.290848255157471, loss=1.893946647644043
I0127 10:58:08.989366 140026067269376 logging_writer.py:48] [162700] global_step=162700, grad_norm=6.662451267242432, loss=1.8833311796188354
I0127 10:58:42.917299 140026075662080 logging_writer.py:48] [162800] global_step=162800, grad_norm=7.273500919342041, loss=1.8803755044937134
I0127 10:59:16.837076 140026067269376 logging_writer.py:48] [162900] global_step=162900, grad_norm=7.5301032066345215, loss=1.8688795566558838
I0127 10:59:50.748567 140026075662080 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.411191463470459, loss=1.9039182662963867
I0127 11:00:24.856930 140026067269376 logging_writer.py:48] [163100] global_step=163100, grad_norm=7.792447566986084, loss=1.9276070594787598
I0127 11:00:58.745054 140026075662080 logging_writer.py:48] [163200] global_step=163200, grad_norm=7.328280448913574, loss=1.8736382722854614
I0127 11:01:32.670508 140026067269376 logging_writer.py:48] [163300] global_step=163300, grad_norm=6.774786472320557, loss=1.9725468158721924
I0127 11:02:06.577219 140026075662080 logging_writer.py:48] [163400] global_step=163400, grad_norm=7.167571067810059, loss=1.891329288482666
I0127 11:02:40.510162 140026067269376 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.489239692687988, loss=1.8980740308761597
I0127 11:03:14.432688 140026075662080 logging_writer.py:48] [163600] global_step=163600, grad_norm=7.070474147796631, loss=1.8999545574188232
I0127 11:03:48.330747 140026067269376 logging_writer.py:48] [163700] global_step=163700, grad_norm=7.368054389953613, loss=1.9296176433563232
I0127 11:04:22.275313 140026075662080 logging_writer.py:48] [163800] global_step=163800, grad_norm=7.176645755767822, loss=1.8707841634750366
I0127 11:04:48.868567 140187804313408 spec.py:321] Evaluating on the training split.
I0127 11:04:54.942852 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 11:05:03.765095 140187804313408 spec.py:349] Evaluating on the test split.
I0127 11:05:06.125051 140187804313408 submission_runner.py:408] Time since start: 57691.21s, 	Step: 163880, 	{'train/accuracy': 0.8883529901504517, 'train/loss': 0.5204653143882751, 'validation/accuracy': 0.7597799897193909, 'validation/loss': 1.0434074401855469, 'validation/num_examples': 50000, 'test/accuracy': 0.6351000070571899, 'test/loss': 1.6635215282440186, 'test/num_examples': 10000, 'score': 55650.608660936356, 'total_duration': 57691.20793008804, 'accumulated_submission_time': 55650.608660936356, 'accumulated_eval_time': 2029.086805820465, 'accumulated_logging_time': 5.652773380279541}
I0127 11:05:06.172532 140026058876672 logging_writer.py:48] [163880] accumulated_eval_time=2029.086806, accumulated_logging_time=5.652773, accumulated_submission_time=55650.608661, global_step=163880, preemption_count=0, score=55650.608661, test/accuracy=0.635100, test/loss=1.663522, test/num_examples=10000, total_duration=57691.207930, train/accuracy=0.888353, train/loss=0.520465, validation/accuracy=0.759780, validation/loss=1.043407, validation/num_examples=50000
I0127 11:05:13.298647 140026067269376 logging_writer.py:48] [163900] global_step=163900, grad_norm=7.94945764541626, loss=1.9170689582824707
I0127 11:05:47.190617 140026058876672 logging_writer.py:48] [164000] global_step=164000, grad_norm=7.055152893066406, loss=1.8497588634490967
I0127 11:06:21.101452 140026067269376 logging_writer.py:48] [164100] global_step=164100, grad_norm=6.760568141937256, loss=1.8829421997070312
I0127 11:06:55.136838 140026058876672 logging_writer.py:48] [164200] global_step=164200, grad_norm=6.925742149353027, loss=1.8986551761627197
I0127 11:07:29.040581 140026067269376 logging_writer.py:48] [164300] global_step=164300, grad_norm=7.448813438415527, loss=1.9381990432739258
I0127 11:08:02.963658 140026058876672 logging_writer.py:48] [164400] global_step=164400, grad_norm=7.4172844886779785, loss=1.894158124923706
I0127 11:08:36.867904 140026067269376 logging_writer.py:48] [164500] global_step=164500, grad_norm=7.352170467376709, loss=1.8745578527450562
I0127 11:09:10.806653 140026058876672 logging_writer.py:48] [164600] global_step=164600, grad_norm=7.370941638946533, loss=1.8873342275619507
I0127 11:09:44.706058 140026067269376 logging_writer.py:48] [164700] global_step=164700, grad_norm=6.8842692375183105, loss=1.8365578651428223
I0127 11:10:18.637788 140026058876672 logging_writer.py:48] [164800] global_step=164800, grad_norm=7.0730462074279785, loss=1.9142996072769165
I0127 11:10:52.518589 140026067269376 logging_writer.py:48] [164900] global_step=164900, grad_norm=7.127713203430176, loss=1.936816692352295
I0127 11:11:26.454483 140026058876672 logging_writer.py:48] [165000] global_step=165000, grad_norm=7.163964748382568, loss=1.8772072792053223
I0127 11:12:00.387853 140026067269376 logging_writer.py:48] [165100] global_step=165100, grad_norm=7.437894821166992, loss=1.9182958602905273
I0127 11:12:34.347095 140026058876672 logging_writer.py:48] [165200] global_step=165200, grad_norm=6.596614360809326, loss=1.7828282117843628
I0127 11:13:08.265988 140026067269376 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.750193119049072, loss=1.94318425655365
I0127 11:13:36.201220 140187804313408 spec.py:321] Evaluating on the training split.
I0127 11:13:42.282992 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 11:13:51.005918 140187804313408 spec.py:349] Evaluating on the test split.
I0127 11:13:53.346121 140187804313408 submission_runner.py:408] Time since start: 58218.43s, 	Step: 165384, 	{'train/accuracy': 0.8879145383834839, 'train/loss': 0.5141134858131409, 'validation/accuracy': 0.7617999911308289, 'validation/loss': 1.0365240573883057, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.6468669176101685, 'test/num_examples': 10000, 'score': 56160.56981110573, 'total_duration': 58218.42898082733, 'accumulated_submission_time': 56160.56981110573, 'accumulated_eval_time': 2046.2316403388977, 'accumulated_logging_time': 5.71399450302124}
I0127 11:13:53.392352 140026050483968 logging_writer.py:48] [165384] accumulated_eval_time=2046.231640, accumulated_logging_time=5.713995, accumulated_submission_time=56160.569811, global_step=165384, preemption_count=0, score=56160.569811, test/accuracy=0.636700, test/loss=1.646867, test/num_examples=10000, total_duration=58218.428981, train/accuracy=0.887915, train/loss=0.514113, validation/accuracy=0.761800, validation/loss=1.036524, validation/num_examples=50000
I0127 11:13:59.156165 140026058876672 logging_writer.py:48] [165400] global_step=165400, grad_norm=7.378777980804443, loss=1.8386932611465454
I0127 11:14:33.042676 140026050483968 logging_writer.py:48] [165500] global_step=165500, grad_norm=7.726687431335449, loss=1.8698852062225342
I0127 11:15:06.967858 140026058876672 logging_writer.py:48] [165600] global_step=165600, grad_norm=6.928041934967041, loss=1.8948273658752441
I0127 11:15:40.881533 140026050483968 logging_writer.py:48] [165700] global_step=165700, grad_norm=6.901962757110596, loss=1.8475513458251953
I0127 11:16:14.802317 140026058876672 logging_writer.py:48] [165800] global_step=165800, grad_norm=7.821813583374023, loss=1.855629324913025
I0127 11:16:48.721076 140026050483968 logging_writer.py:48] [165900] global_step=165900, grad_norm=7.847108364105225, loss=1.879854440689087
I0127 11:17:22.616266 140026058876672 logging_writer.py:48] [166000] global_step=166000, grad_norm=7.173822402954102, loss=1.8742144107818604
I0127 11:17:56.528349 140026050483968 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.818319797515869, loss=1.7926304340362549
I0127 11:18:30.433041 140026058876672 logging_writer.py:48] [166200] global_step=166200, grad_norm=7.327397346496582, loss=1.8738632202148438
I0127 11:19:04.415797 140026050483968 logging_writer.py:48] [166300] global_step=166300, grad_norm=8.338043212890625, loss=1.9406636953353882
I0127 11:19:38.346784 140026058876672 logging_writer.py:48] [166400] global_step=166400, grad_norm=7.393373012542725, loss=1.883823275566101
I0127 11:20:12.280025 140026050483968 logging_writer.py:48] [166500] global_step=166500, grad_norm=7.001880168914795, loss=1.8613829612731934
I0127 11:20:46.234456 140026058876672 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.6979899406433105, loss=1.9705042839050293
I0127 11:21:20.133833 140026050483968 logging_writer.py:48] [166700] global_step=166700, grad_norm=8.146358489990234, loss=1.8733224868774414
I0127 11:21:54.061125 140026058876672 logging_writer.py:48] [166800] global_step=166800, grad_norm=7.331029415130615, loss=1.8597053289413452
I0127 11:22:23.350498 140187804313408 spec.py:321] Evaluating on the training split.
I0127 11:22:29.401299 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 11:22:38.356540 140187804313408 spec.py:349] Evaluating on the test split.
I0127 11:22:40.632052 140187804313408 submission_runner.py:408] Time since start: 58745.71s, 	Step: 166888, 	{'train/accuracy': 0.9036790132522583, 'train/loss': 0.4611811637878418, 'validation/accuracy': 0.7634199857711792, 'validation/loss': 1.030709147453308, 'validation/num_examples': 50000, 'test/accuracy': 0.6419000029563904, 'test/loss': 1.6486574411392212, 'test/num_examples': 10000, 'score': 56670.46351933479, 'total_duration': 58745.71489930153, 'accumulated_submission_time': 56670.46351933479, 'accumulated_eval_time': 2063.513118505478, 'accumulated_logging_time': 5.7704408168792725}
I0127 11:22:40.697505 140026159523584 logging_writer.py:48] [166888] accumulated_eval_time=2063.513119, accumulated_logging_time=5.770441, accumulated_submission_time=56670.463519, global_step=166888, preemption_count=0, score=56670.463519, test/accuracy=0.641900, test/loss=1.648657, test/num_examples=10000, total_duration=58745.714899, train/accuracy=0.903679, train/loss=0.461181, validation/accuracy=0.763420, validation/loss=1.030709, validation/num_examples=50000
I0127 11:22:45.096700 140026167916288 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.043891906738281, loss=1.9052015542984009
I0127 11:23:18.934021 140026159523584 logging_writer.py:48] [167000] global_step=167000, grad_norm=7.263787269592285, loss=1.9038389921188354
I0127 11:23:52.806989 140026167916288 logging_writer.py:48] [167100] global_step=167100, grad_norm=7.235081672668457, loss=1.795609951019287
I0127 11:24:26.719080 140026159523584 logging_writer.py:48] [167200] global_step=167200, grad_norm=7.648473262786865, loss=1.8222464323043823
I0127 11:25:00.686542 140026167916288 logging_writer.py:48] [167300] global_step=167300, grad_norm=8.857078552246094, loss=1.8787331581115723
I0127 11:25:34.577146 140026159523584 logging_writer.py:48] [167400] global_step=167400, grad_norm=8.21392822265625, loss=1.847306489944458
I0127 11:26:08.496608 140026167916288 logging_writer.py:48] [167500] global_step=167500, grad_norm=6.971644401550293, loss=1.9021832942962646
I0127 11:26:42.468503 140026159523584 logging_writer.py:48] [167600] global_step=167600, grad_norm=7.34712553024292, loss=1.8087949752807617
I0127 11:27:16.324456 140026167916288 logging_writer.py:48] [167700] global_step=167700, grad_norm=8.653308868408203, loss=1.912638545036316
I0127 11:27:50.290184 140026159523584 logging_writer.py:48] [167800] global_step=167800, grad_norm=7.1365275382995605, loss=1.8618242740631104
I0127 11:28:24.246894 140026167916288 logging_writer.py:48] [167900] global_step=167900, grad_norm=7.905021667480469, loss=1.9126691818237305
I0127 11:28:58.143615 140026159523584 logging_writer.py:48] [168000] global_step=168000, grad_norm=7.3981122970581055, loss=1.815669059753418
I0127 11:29:32.069573 140026167916288 logging_writer.py:48] [168100] global_step=168100, grad_norm=7.618997097015381, loss=1.9633347988128662
I0127 11:30:05.965945 140026159523584 logging_writer.py:48] [168200] global_step=168200, grad_norm=8.368803024291992, loss=1.8745781183242798
I0127 11:30:39.896960 140026167916288 logging_writer.py:48] [168300] global_step=168300, grad_norm=7.7421698570251465, loss=1.9003318548202515
I0127 11:31:10.938338 140187804313408 spec.py:321] Evaluating on the training split.
I0127 11:31:17.010633 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 11:31:25.969864 140187804313408 spec.py:349] Evaluating on the test split.
I0127 11:31:28.311780 140187804313408 submission_runner.py:408] Time since start: 59273.39s, 	Step: 168393, 	{'train/accuracy': 0.9052734375, 'train/loss': 0.4601774215698242, 'validation/accuracy': 0.7659199833869934, 'validation/loss': 1.0219361782073975, 'validation/num_examples': 50000, 'test/accuracy': 0.6416000127792358, 'test/loss': 1.6349159479141235, 'test/num_examples': 10000, 'score': 57180.640429496765, 'total_duration': 59273.3946313858, 'accumulated_submission_time': 57180.640429496765, 'accumulated_eval_time': 2080.8864829540253, 'accumulated_logging_time': 5.8460328578948975}
I0127 11:31:28.365333 140026058876672 logging_writer.py:48] [168393] accumulated_eval_time=2080.886483, accumulated_logging_time=5.846033, accumulated_submission_time=57180.640429, global_step=168393, preemption_count=0, score=57180.640429, test/accuracy=0.641600, test/loss=1.634916, test/num_examples=10000, total_duration=59273.394631, train/accuracy=0.905273, train/loss=0.460177, validation/accuracy=0.765920, validation/loss=1.021936, validation/num_examples=50000
I0127 11:31:31.079834 140026067269376 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.8234333992004395, loss=1.891847848892212
I0127 11:32:04.941141 140026058876672 logging_writer.py:48] [168500] global_step=168500, grad_norm=7.313815593719482, loss=1.8286056518554688
I0127 11:32:38.826438 140026067269376 logging_writer.py:48] [168600] global_step=168600, grad_norm=8.218310356140137, loss=1.8155285120010376
I0127 11:33:12.748225 140026058876672 logging_writer.py:48] [168700] global_step=168700, grad_norm=6.916595458984375, loss=1.8990988731384277
I0127 11:33:46.632416 140026067269376 logging_writer.py:48] [168800] global_step=168800, grad_norm=8.303924560546875, loss=1.8908848762512207
I0127 11:34:20.570369 140026058876672 logging_writer.py:48] [168900] global_step=168900, grad_norm=7.649956703186035, loss=1.8467357158660889
I0127 11:34:54.465878 140026067269376 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.620940208435059, loss=1.8693122863769531
I0127 11:35:28.403568 140026058876672 logging_writer.py:48] [169100] global_step=169100, grad_norm=7.473098278045654, loss=1.8847659826278687
I0127 11:36:02.318441 140026067269376 logging_writer.py:48] [169200] global_step=169200, grad_norm=7.007213115692139, loss=1.8046109676361084
I0127 11:36:36.250752 140026058876672 logging_writer.py:48] [169300] global_step=169300, grad_norm=6.849884033203125, loss=1.8230623006820679
I0127 11:37:10.230026 140026067269376 logging_writer.py:48] [169400] global_step=169400, grad_norm=7.456007480621338, loss=1.8656587600708008
I0127 11:37:44.141386 140026058876672 logging_writer.py:48] [169500] global_step=169500, grad_norm=7.261194705963135, loss=1.8961235284805298
I0127 11:38:18.072552 140026067269376 logging_writer.py:48] [169600] global_step=169600, grad_norm=7.271721839904785, loss=1.8000514507293701
I0127 11:38:51.996320 140026058876672 logging_writer.py:48] [169700] global_step=169700, grad_norm=7.251142978668213, loss=1.8692753314971924
I0127 11:39:25.905561 140026067269376 logging_writer.py:48] [169800] global_step=169800, grad_norm=7.131124973297119, loss=1.8797249794006348
I0127 11:39:58.624363 140187804313408 spec.py:321] Evaluating on the training split.
I0127 11:40:04.859812 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 11:40:13.828897 140187804313408 spec.py:349] Evaluating on the test split.
I0127 11:40:16.136004 140187804313408 submission_runner.py:408] Time since start: 59801.22s, 	Step: 169898, 	{'train/accuracy': 0.9046555757522583, 'train/loss': 0.45881563425064087, 'validation/accuracy': 0.7649399638175964, 'validation/loss': 1.0206623077392578, 'validation/num_examples': 50000, 'test/accuracy': 0.6459000110626221, 'test/loss': 1.622611403465271, 'test/num_examples': 10000, 'score': 57690.83558368683, 'total_duration': 59801.218878507614, 'accumulated_submission_time': 57690.83558368683, 'accumulated_eval_time': 2098.3980734348297, 'accumulated_logging_time': 5.909879684448242}
I0127 11:40:16.181388 140026050483968 logging_writer.py:48] [169898] accumulated_eval_time=2098.398073, accumulated_logging_time=5.909880, accumulated_submission_time=57690.835584, global_step=169898, preemption_count=0, score=57690.835584, test/accuracy=0.645900, test/loss=1.622611, test/num_examples=10000, total_duration=59801.218879, train/accuracy=0.904656, train/loss=0.458816, validation/accuracy=0.764940, validation/loss=1.020662, validation/num_examples=50000
I0127 11:40:17.194689 140026151130880 logging_writer.py:48] [169900] global_step=169900, grad_norm=7.929098129272461, loss=1.8366682529449463
I0127 11:40:51.058916 140026050483968 logging_writer.py:48] [170000] global_step=170000, grad_norm=6.7385406494140625, loss=1.8599817752838135
I0127 11:41:24.937677 140026151130880 logging_writer.py:48] [170100] global_step=170100, grad_norm=7.222888946533203, loss=1.8140292167663574
I0127 11:41:58.858506 140026050483968 logging_writer.py:48] [170200] global_step=170200, grad_norm=7.182392120361328, loss=1.7851375341415405
I0127 11:42:32.768515 140026151130880 logging_writer.py:48] [170300] global_step=170300, grad_norm=7.376770973205566, loss=1.834846019744873
I0127 11:43:06.691419 140026050483968 logging_writer.py:48] [170400] global_step=170400, grad_norm=7.464912414550781, loss=1.7756893634796143
I0127 11:43:40.783562 140026151130880 logging_writer.py:48] [170500] global_step=170500, grad_norm=7.096928119659424, loss=1.8849073648452759
I0127 11:44:14.692570 140026050483968 logging_writer.py:48] [170600] global_step=170600, grad_norm=7.411852836608887, loss=1.7690272331237793
I0127 11:44:48.598550 140026151130880 logging_writer.py:48] [170700] global_step=170700, grad_norm=7.111948490142822, loss=1.8393890857696533
I0127 11:45:22.505861 140026050483968 logging_writer.py:48] [170800] global_step=170800, grad_norm=7.186059474945068, loss=1.7693010568618774
I0127 11:45:56.403744 140026151130880 logging_writer.py:48] [170900] global_step=170900, grad_norm=7.684720993041992, loss=1.8038444519042969
I0127 11:46:30.318624 140026050483968 logging_writer.py:48] [171000] global_step=171000, grad_norm=8.025445938110352, loss=1.8063511848449707
I0127 11:47:04.251156 140026151130880 logging_writer.py:48] [171100] global_step=171100, grad_norm=7.336914539337158, loss=1.8316068649291992
I0127 11:47:38.142003 140026050483968 logging_writer.py:48] [171200] global_step=171200, grad_norm=7.951133728027344, loss=1.8210318088531494
I0127 11:48:12.061069 140026151130880 logging_writer.py:48] [171300] global_step=171300, grad_norm=8.270142555236816, loss=1.8572523593902588
I0127 11:48:45.958851 140026050483968 logging_writer.py:48] [171400] global_step=171400, grad_norm=8.303609848022461, loss=1.8331186771392822
I0127 11:48:46.444108 140187804313408 spec.py:321] Evaluating on the training split.
I0127 11:48:52.485061 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 11:49:01.588401 140187804313408 spec.py:349] Evaluating on the test split.
I0127 11:49:04.074430 140187804313408 submission_runner.py:408] Time since start: 60329.16s, 	Step: 171403, 	{'train/accuracy': 0.9075254797935486, 'train/loss': 0.4488637149333954, 'validation/accuracy': 0.7670800089836121, 'validation/loss': 1.009906530380249, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.6306302547454834, 'test/num_examples': 10000, 'score': 58201.03377509117, 'total_duration': 60329.157285928726, 'accumulated_submission_time': 58201.03377509117, 'accumulated_eval_time': 2116.0283353328705, 'accumulated_logging_time': 5.96552038192749}
I0127 11:49:04.123207 140026067269376 logging_writer.py:48] [171403] accumulated_eval_time=2116.028335, accumulated_logging_time=5.965520, accumulated_submission_time=58201.033775, global_step=171403, preemption_count=0, score=58201.033775, test/accuracy=0.645600, test/loss=1.630630, test/num_examples=10000, total_duration=60329.157286, train/accuracy=0.907525, train/loss=0.448864, validation/accuracy=0.767080, validation/loss=1.009907, validation/num_examples=50000
I0127 11:49:37.403171 140026075662080 logging_writer.py:48] [171500] global_step=171500, grad_norm=8.041868209838867, loss=1.8122210502624512
I0127 11:50:11.309856 140026067269376 logging_writer.py:48] [171600] global_step=171600, grad_norm=7.06218147277832, loss=1.797580599784851
I0127 11:50:45.215441 140026075662080 logging_writer.py:48] [171700] global_step=171700, grad_norm=7.357846260070801, loss=1.8154023885726929
I0127 11:51:19.161701 140026067269376 logging_writer.py:48] [171800] global_step=171800, grad_norm=7.901629447937012, loss=1.7290287017822266
I0127 11:51:53.054944 140026075662080 logging_writer.py:48] [171900] global_step=171900, grad_norm=7.677393436431885, loss=1.8510879278182983
I0127 11:52:26.993661 140026067269376 logging_writer.py:48] [172000] global_step=172000, grad_norm=7.199026584625244, loss=1.9092392921447754
I0127 11:53:00.913363 140026075662080 logging_writer.py:48] [172100] global_step=172100, grad_norm=8.013542175292969, loss=1.808544397354126
I0127 11:53:34.829025 140026067269376 logging_writer.py:48] [172200] global_step=172200, grad_norm=6.874134540557861, loss=1.821898102760315
I0127 11:54:08.752725 140026075662080 logging_writer.py:48] [172300] global_step=172300, grad_norm=7.588504314422607, loss=1.8120726346969604
I0127 11:54:42.641036 140026067269376 logging_writer.py:48] [172400] global_step=172400, grad_norm=7.242465972900391, loss=1.8133389949798584
I0127 11:55:16.551358 140026075662080 logging_writer.py:48] [172500] global_step=172500, grad_norm=7.279773235321045, loss=1.8121947050094604
I0127 11:55:50.588908 140026067269376 logging_writer.py:48] [172600] global_step=172600, grad_norm=7.65477180480957, loss=1.7926207780838013
I0127 11:56:24.514333 140026075662080 logging_writer.py:48] [172700] global_step=172700, grad_norm=8.028923988342285, loss=1.7772884368896484
I0127 11:56:58.427332 140026067269376 logging_writer.py:48] [172800] global_step=172800, grad_norm=8.260858535766602, loss=1.8704828023910522
I0127 11:57:32.329920 140026075662080 logging_writer.py:48] [172900] global_step=172900, grad_norm=7.333858013153076, loss=1.8496533632278442
I0127 11:57:34.168420 140187804313408 spec.py:321] Evaluating on the training split.
I0127 11:57:40.175578 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 11:57:49.231692 140187804313408 spec.py:349] Evaluating on the test split.
I0127 11:57:51.540462 140187804313408 submission_runner.py:408] Time since start: 60856.62s, 	Step: 172907, 	{'train/accuracy': 0.9103754758834839, 'train/loss': 0.4398549795150757, 'validation/accuracy': 0.7702599763870239, 'validation/loss': 1.0039763450622559, 'validation/num_examples': 50000, 'test/accuracy': 0.6478000283241272, 'test/loss': 1.6156542301177979, 'test/num_examples': 10000, 'score': 58711.011095523834, 'total_duration': 60856.62333703041, 'accumulated_submission_time': 58711.011095523834, 'accumulated_eval_time': 2133.4003245830536, 'accumulated_logging_time': 6.02742600440979}
I0127 11:57:51.590839 140026159523584 logging_writer.py:48] [172907] accumulated_eval_time=2133.400325, accumulated_logging_time=6.027426, accumulated_submission_time=58711.011096, global_step=172907, preemption_count=0, score=58711.011096, test/accuracy=0.647800, test/loss=1.615654, test/num_examples=10000, total_duration=60856.623337, train/accuracy=0.910375, train/loss=0.439855, validation/accuracy=0.770260, validation/loss=1.003976, validation/num_examples=50000
I0127 11:58:23.453530 140026167916288 logging_writer.py:48] [173000] global_step=173000, grad_norm=7.633696556091309, loss=1.78983473777771
I0127 11:58:57.329053 140026159523584 logging_writer.py:48] [173100] global_step=173100, grad_norm=7.633703231811523, loss=1.8542330265045166
I0127 11:59:31.252793 140026167916288 logging_writer.py:48] [173200] global_step=173200, grad_norm=8.444219589233398, loss=1.7956414222717285
I0127 12:00:05.154468 140026159523584 logging_writer.py:48] [173300] global_step=173300, grad_norm=8.104816436767578, loss=1.8316066265106201
I0127 12:00:39.092062 140026167916288 logging_writer.py:48] [173400] global_step=173400, grad_norm=7.977518558502197, loss=1.7997095584869385
I0127 12:01:13.004444 140026159523584 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.098631858825684, loss=1.8501603603363037
I0127 12:01:46.967959 140026167916288 logging_writer.py:48] [173600] global_step=173600, grad_norm=7.941333293914795, loss=1.811871886253357
I0127 12:02:20.893404 140026159523584 logging_writer.py:48] [173700] global_step=173700, grad_norm=8.317412376403809, loss=1.8389252424240112
I0127 12:02:54.769977 140026167916288 logging_writer.py:48] [173800] global_step=173800, grad_norm=7.656453609466553, loss=1.7908110618591309
I0127 12:03:28.710949 140026159523584 logging_writer.py:48] [173900] global_step=173900, grad_norm=8.882854461669922, loss=1.8707104921340942
I0127 12:04:02.593142 140026167916288 logging_writer.py:48] [174000] global_step=174000, grad_norm=8.069390296936035, loss=1.772430419921875
I0127 12:04:36.530321 140026159523584 logging_writer.py:48] [174100] global_step=174100, grad_norm=7.505932807922363, loss=1.8508771657943726
I0127 12:05:10.438352 140026167916288 logging_writer.py:48] [174200] global_step=174200, grad_norm=7.640811920166016, loss=1.8409534692764282
I0127 12:05:44.359766 140026159523584 logging_writer.py:48] [174300] global_step=174300, grad_norm=9.072524070739746, loss=1.8439218997955322
I0127 12:06:18.267381 140026167916288 logging_writer.py:48] [174400] global_step=174400, grad_norm=7.826278209686279, loss=1.734745979309082
I0127 12:06:21.802652 140187804313408 spec.py:321] Evaluating on the training split.
I0127 12:06:27.826050 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 12:06:36.695508 140187804313408 spec.py:349] Evaluating on the test split.
I0127 12:06:39.076684 140187804313408 submission_runner.py:408] Time since start: 61384.16s, 	Step: 174412, 	{'train/accuracy': 0.9120894074440002, 'train/loss': 0.4342247247695923, 'validation/accuracy': 0.7703799605369568, 'validation/loss': 1.0012692213058472, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6078853607177734, 'test/num_examples': 10000, 'score': 59221.15896511078, 'total_duration': 61384.15951418877, 'accumulated_submission_time': 59221.15896511078, 'accumulated_eval_time': 2150.6742584705353, 'accumulated_logging_time': 6.087927341461182}
I0127 12:06:39.121618 140026042091264 logging_writer.py:48] [174412] accumulated_eval_time=2150.674258, accumulated_logging_time=6.087927, accumulated_submission_time=59221.158965, global_step=174412, preemption_count=0, score=59221.158965, test/accuracy=0.650900, test/loss=1.607885, test/num_examples=10000, total_duration=61384.159514, train/accuracy=0.912089, train/loss=0.434225, validation/accuracy=0.770380, validation/loss=1.001269, validation/num_examples=50000
I0127 12:07:09.235105 140026050483968 logging_writer.py:48] [174500] global_step=174500, grad_norm=7.227959632873535, loss=1.7434555292129517
I0127 12:07:43.143036 140026042091264 logging_writer.py:48] [174600] global_step=174600, grad_norm=7.320182800292969, loss=1.759097933769226
I0127 12:08:17.077226 140026050483968 logging_writer.py:48] [174700] global_step=174700, grad_norm=6.85640811920166, loss=1.7016220092773438
I0127 12:08:51.005455 140026042091264 logging_writer.py:48] [174800] global_step=174800, grad_norm=7.459934711456299, loss=1.7279603481292725
I0127 12:09:24.928024 140026050483968 logging_writer.py:48] [174900] global_step=174900, grad_norm=8.579655647277832, loss=1.8318955898284912
I0127 12:09:58.856527 140026042091264 logging_writer.py:48] [175000] global_step=175000, grad_norm=7.896464824676514, loss=1.7963370084762573
I0127 12:10:32.800261 140026050483968 logging_writer.py:48] [175100] global_step=175100, grad_norm=7.642576217651367, loss=1.8344758749008179
I0127 12:11:06.724827 140026042091264 logging_writer.py:48] [175200] global_step=175200, grad_norm=8.393542289733887, loss=1.765289306640625
I0127 12:11:40.653887 140026050483968 logging_writer.py:48] [175300] global_step=175300, grad_norm=6.824997425079346, loss=1.722157597541809
I0127 12:12:14.561475 140026042091264 logging_writer.py:48] [175400] global_step=175400, grad_norm=7.781236171722412, loss=1.7809674739837646
I0127 12:12:48.473558 140026050483968 logging_writer.py:48] [175500] global_step=175500, grad_norm=7.430577278137207, loss=1.776039481163025
I0127 12:13:22.431771 140026042091264 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.675870895385742, loss=1.811244010925293
I0127 12:13:56.349031 140026050483968 logging_writer.py:48] [175700] global_step=175700, grad_norm=7.788251876831055, loss=1.7803317308425903
I0127 12:14:30.331706 140026042091264 logging_writer.py:48] [175800] global_step=175800, grad_norm=8.42795181274414, loss=1.8059165477752686
I0127 12:15:04.271469 140026050483968 logging_writer.py:48] [175900] global_step=175900, grad_norm=8.256399154663086, loss=1.794071078300476
I0127 12:15:09.173259 140187804313408 spec.py:321] Evaluating on the training split.
I0127 12:15:15.246412 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 12:15:23.996594 140187804313408 spec.py:349] Evaluating on the test split.
I0127 12:15:26.349382 140187804313408 submission_runner.py:408] Time since start: 61911.43s, 	Step: 175916, 	{'train/accuracy': 0.9157963991165161, 'train/loss': 0.4226926863193512, 'validation/accuracy': 0.7699999809265137, 'validation/loss': 1.0016485452651978, 'validation/num_examples': 50000, 'test/accuracy': 0.6498000025749207, 'test/loss': 1.614419937133789, 'test/num_examples': 10000, 'score': 59731.146606206894, 'total_duration': 61911.43213367462, 'accumulated_submission_time': 59731.146606206894, 'accumulated_eval_time': 2167.850204706192, 'accumulated_logging_time': 6.143632411956787}
I0127 12:15:26.398195 140026167916288 logging_writer.py:48] [175916] accumulated_eval_time=2167.850205, accumulated_logging_time=6.143632, accumulated_submission_time=59731.146606, global_step=175916, preemption_count=0, score=59731.146606, test/accuracy=0.649800, test/loss=1.614420, test/num_examples=10000, total_duration=61911.432134, train/accuracy=0.915796, train/loss=0.422693, validation/accuracy=0.770000, validation/loss=1.001649, validation/num_examples=50000
I0127 12:15:55.209145 140026184701696 logging_writer.py:48] [176000] global_step=176000, grad_norm=7.310638427734375, loss=1.7717775106430054
I0127 12:16:29.136564 140026167916288 logging_writer.py:48] [176100] global_step=176100, grad_norm=7.851513385772705, loss=1.7645856142044067
I0127 12:17:03.065224 140026184701696 logging_writer.py:48] [176200] global_step=176200, grad_norm=7.2466535568237305, loss=1.7755588293075562
I0127 12:17:36.984496 140026167916288 logging_writer.py:48] [176300] global_step=176300, grad_norm=8.512974739074707, loss=1.7844862937927246
I0127 12:18:10.919420 140026184701696 logging_writer.py:48] [176400] global_step=176400, grad_norm=7.57521915435791, loss=1.707026481628418
I0127 12:18:44.850937 140026167916288 logging_writer.py:48] [176500] global_step=176500, grad_norm=6.966330051422119, loss=1.7601147890090942
I0127 12:19:18.762172 140026184701696 logging_writer.py:48] [176600] global_step=176600, grad_norm=8.014874458312988, loss=1.7876392602920532
I0127 12:19:52.686955 140026167916288 logging_writer.py:48] [176700] global_step=176700, grad_norm=7.764400005340576, loss=1.7599456310272217
I0127 12:20:26.662611 140026184701696 logging_writer.py:48] [176800] global_step=176800, grad_norm=7.389079570770264, loss=1.8298782110214233
I0127 12:21:00.598266 140026167916288 logging_writer.py:48] [176900] global_step=176900, grad_norm=8.321337699890137, loss=1.7829984426498413
I0127 12:21:34.496975 140026184701696 logging_writer.py:48] [177000] global_step=177000, grad_norm=7.6160149574279785, loss=1.7915006875991821
I0127 12:22:08.438367 140026167916288 logging_writer.py:48] [177100] global_step=177100, grad_norm=8.103553771972656, loss=1.812464952468872
I0127 12:22:42.349804 140026184701696 logging_writer.py:48] [177200] global_step=177200, grad_norm=8.021571159362793, loss=1.871878743171692
I0127 12:23:16.487839 140026167916288 logging_writer.py:48] [177300] global_step=177300, grad_norm=7.563212871551514, loss=1.7686725854873657
I0127 12:23:50.429554 140026184701696 logging_writer.py:48] [177400] global_step=177400, grad_norm=7.806105136871338, loss=1.7770472764968872
I0127 12:23:56.669763 140187804313408 spec.py:321] Evaluating on the training split.
I0127 12:24:02.907777 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 12:24:11.872691 140187804313408 spec.py:349] Evaluating on the test split.
I0127 12:24:14.209330 140187804313408 submission_runner.py:408] Time since start: 62439.29s, 	Step: 177420, 	{'train/accuracy': 0.9207788109779358, 'train/loss': 0.40458759665489197, 'validation/accuracy': 0.7719599604606628, 'validation/loss': 0.9974290132522583, 'validation/num_examples': 50000, 'test/accuracy': 0.6514000296592712, 'test/loss': 1.6060065031051636, 'test/num_examples': 10000, 'score': 60241.354083538055, 'total_duration': 62439.29220581055, 'accumulated_submission_time': 60241.354083538055, 'accumulated_eval_time': 2185.3897194862366, 'accumulated_logging_time': 6.202255487442017}
I0127 12:24:14.263719 140026042091264 logging_writer.py:48] [177420] accumulated_eval_time=2185.389719, accumulated_logging_time=6.202255, accumulated_submission_time=60241.354084, global_step=177420, preemption_count=0, score=60241.354084, test/accuracy=0.651400, test/loss=1.606007, test/num_examples=10000, total_duration=62439.292206, train/accuracy=0.920779, train/loss=0.404588, validation/accuracy=0.771960, validation/loss=0.997429, validation/num_examples=50000
I0127 12:24:41.693575 140026050483968 logging_writer.py:48] [177500] global_step=177500, grad_norm=6.903619289398193, loss=1.7535779476165771
I0127 12:25:15.575347 140026042091264 logging_writer.py:48] [177600] global_step=177600, grad_norm=7.562516689300537, loss=1.7528258562088013
I0127 12:25:49.494883 140026050483968 logging_writer.py:48] [177700] global_step=177700, grad_norm=8.208888053894043, loss=1.7485593557357788
I0127 12:26:23.466553 140026042091264 logging_writer.py:48] [177800] global_step=177800, grad_norm=7.776823043823242, loss=1.7463303804397583
I0127 12:26:57.413498 140026050483968 logging_writer.py:48] [177900] global_step=177900, grad_norm=7.836831569671631, loss=1.7818430662155151
I0127 12:27:31.300797 140026042091264 logging_writer.py:48] [178000] global_step=178000, grad_norm=8.816300392150879, loss=1.864530086517334
I0127 12:28:05.215205 140026050483968 logging_writer.py:48] [178100] global_step=178100, grad_norm=7.830196857452393, loss=1.751702904701233
I0127 12:28:39.131035 140026042091264 logging_writer.py:48] [178200] global_step=178200, grad_norm=7.6668243408203125, loss=1.8324052095413208
I0127 12:29:13.061668 140026050483968 logging_writer.py:48] [178300] global_step=178300, grad_norm=8.407273292541504, loss=1.8475233316421509
I0127 12:29:46.968327 140026042091264 logging_writer.py:48] [178400] global_step=178400, grad_norm=7.592644691467285, loss=1.763817310333252
I0127 12:30:20.886287 140026050483968 logging_writer.py:48] [178500] global_step=178500, grad_norm=7.539210319519043, loss=1.7736284732818604
I0127 12:30:54.850558 140026042091264 logging_writer.py:48] [178600] global_step=178600, grad_norm=8.6419038772583, loss=1.8325626850128174
I0127 12:31:28.764856 140026050483968 logging_writer.py:48] [178700] global_step=178700, grad_norm=7.8740363121032715, loss=1.727618932723999
I0127 12:32:02.704260 140026042091264 logging_writer.py:48] [178800] global_step=178800, grad_norm=7.481883525848389, loss=1.766113519668579
I0127 12:32:36.668509 140026050483968 logging_writer.py:48] [178900] global_step=178900, grad_norm=8.137811660766602, loss=1.849880337715149
I0127 12:32:44.268172 140187804313408 spec.py:321] Evaluating on the training split.
I0127 12:32:50.353943 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 12:32:59.351743 140187804313408 spec.py:349] Evaluating on the test split.
I0127 12:33:01.677226 140187804313408 submission_runner.py:408] Time since start: 62966.76s, 	Step: 178924, 	{'train/accuracy': 0.9169324040412903, 'train/loss': 0.40575864911079407, 'validation/accuracy': 0.7729399800300598, 'validation/loss': 0.9932107329368591, 'validation/num_examples': 50000, 'test/accuracy': 0.6512000560760498, 'test/loss': 1.60286545753479, 'test/num_examples': 10000, 'score': 60751.29431128502, 'total_duration': 62966.76009917259, 'accumulated_submission_time': 60751.29431128502, 'accumulated_eval_time': 2202.798728942871, 'accumulated_logging_time': 6.266868829727173}
I0127 12:33:01.743575 140026042091264 logging_writer.py:48] [178924] accumulated_eval_time=2202.798729, accumulated_logging_time=6.266869, accumulated_submission_time=60751.294311, global_step=178924, preemption_count=0, score=60751.294311, test/accuracy=0.651200, test/loss=1.602865, test/num_examples=10000, total_duration=62966.760099, train/accuracy=0.916932, train/loss=0.405759, validation/accuracy=0.772940, validation/loss=0.993211, validation/num_examples=50000
I0127 12:33:27.822157 140026050483968 logging_writer.py:48] [179000] global_step=179000, grad_norm=7.637482643127441, loss=1.8588769435882568
I0127 12:34:01.666637 140026042091264 logging_writer.py:48] [179100] global_step=179100, grad_norm=7.769998550415039, loss=1.7887349128723145
I0127 12:34:35.585153 140026050483968 logging_writer.py:48] [179200] global_step=179200, grad_norm=8.508171081542969, loss=1.8149769306182861
I0127 12:35:09.486232 140026042091264 logging_writer.py:48] [179300] global_step=179300, grad_norm=8.179459571838379, loss=1.7914307117462158
I0127 12:35:43.421047 140026050483968 logging_writer.py:48] [179400] global_step=179400, grad_norm=7.823482513427734, loss=1.8012163639068604
I0127 12:36:17.323638 140026042091264 logging_writer.py:48] [179500] global_step=179500, grad_norm=8.888666152954102, loss=1.8343271017074585
I0127 12:36:51.270943 140026050483968 logging_writer.py:48] [179600] global_step=179600, grad_norm=7.634566307067871, loss=1.7776243686676025
I0127 12:37:25.170479 140026042091264 logging_writer.py:48] [179700] global_step=179700, grad_norm=7.854852676391602, loss=1.7704920768737793
I0127 12:37:59.114965 140026050483968 logging_writer.py:48] [179800] global_step=179800, grad_norm=8.515233993530273, loss=1.7846158742904663
I0127 12:38:32.994681 140026042091264 logging_writer.py:48] [179900] global_step=179900, grad_norm=7.124691963195801, loss=1.7613345384597778
I0127 12:39:06.996178 140026050483968 logging_writer.py:48] [180000] global_step=180000, grad_norm=8.354175567626953, loss=1.7854185104370117
I0127 12:39:40.925963 140026042091264 logging_writer.py:48] [180100] global_step=180100, grad_norm=7.903351783752441, loss=1.7708979845046997
I0127 12:40:14.874854 140026050483968 logging_writer.py:48] [180200] global_step=180200, grad_norm=7.531160354614258, loss=1.7671823501586914
I0127 12:40:48.793661 140026042091264 logging_writer.py:48] [180300] global_step=180300, grad_norm=8.230056762695312, loss=1.7647557258605957
I0127 12:41:22.680061 140026050483968 logging_writer.py:48] [180400] global_step=180400, grad_norm=8.403834342956543, loss=1.778406023979187
I0127 12:41:32.011820 140187804313408 spec.py:321] Evaluating on the training split.
I0127 12:41:38.113399 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 12:41:46.982214 140187804313408 spec.py:349] Evaluating on the test split.
I0127 12:41:49.272382 140187804313408 submission_runner.py:408] Time since start: 63494.36s, 	Step: 180429, 	{'train/accuracy': 0.9206991195678711, 'train/loss': 0.40181708335876465, 'validation/accuracy': 0.7720800042152405, 'validation/loss': 0.9955241084098816, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.6055456399917603, 'test/num_examples': 10000, 'score': 61261.49818825722, 'total_duration': 63494.355257987976, 'accumulated_submission_time': 61261.49818825722, 'accumulated_eval_time': 2220.0592410564423, 'accumulated_logging_time': 6.343884229660034}
I0127 12:41:49.319954 140026050483968 logging_writer.py:48] [180429] accumulated_eval_time=2220.059241, accumulated_logging_time=6.343884, accumulated_submission_time=61261.498188, global_step=180429, preemption_count=0, score=61261.498188, test/accuracy=0.651900, test/loss=1.605546, test/num_examples=10000, total_duration=63494.355258, train/accuracy=0.920699, train/loss=0.401817, validation/accuracy=0.772080, validation/loss=0.995524, validation/num_examples=50000
I0127 12:42:13.734426 140026058876672 logging_writer.py:48] [180500] global_step=180500, grad_norm=7.60492467880249, loss=1.8059043884277344
I0127 12:42:47.621501 140026050483968 logging_writer.py:48] [180600] global_step=180600, grad_norm=7.656253814697266, loss=1.7812035083770752
I0127 12:43:21.508170 140026058876672 logging_writer.py:48] [180700] global_step=180700, grad_norm=7.81361722946167, loss=1.7695876359939575
I0127 12:43:55.439361 140026050483968 logging_writer.py:48] [180800] global_step=180800, grad_norm=7.884774684906006, loss=1.7537723779678345
I0127 12:44:29.329396 140026058876672 logging_writer.py:48] [180900] global_step=180900, grad_norm=7.879122734069824, loss=1.8050285577774048
I0127 12:45:03.302059 140026050483968 logging_writer.py:48] [181000] global_step=181000, grad_norm=7.536798000335693, loss=1.6893222332000732
I0127 12:45:37.221537 140026058876672 logging_writer.py:48] [181100] global_step=181100, grad_norm=8.48144817352295, loss=1.808110237121582
I0127 12:46:11.116460 140026050483968 logging_writer.py:48] [181200] global_step=181200, grad_norm=7.309296131134033, loss=1.7688348293304443
I0127 12:46:45.054579 140026058876672 logging_writer.py:48] [181300] global_step=181300, grad_norm=7.932340621948242, loss=1.7954622507095337
I0127 12:47:19.011365 140026050483968 logging_writer.py:48] [181400] global_step=181400, grad_norm=7.842724800109863, loss=1.7774198055267334
I0127 12:47:52.924086 140026058876672 logging_writer.py:48] [181500] global_step=181500, grad_norm=8.931660652160645, loss=1.7881553173065186
I0127 12:48:26.853384 140026050483968 logging_writer.py:48] [181600] global_step=181600, grad_norm=7.634679794311523, loss=1.8203128576278687
I0127 12:49:00.737046 140026058876672 logging_writer.py:48] [181700] global_step=181700, grad_norm=7.861557960510254, loss=1.7535722255706787
I0127 12:49:34.639803 140026050483968 logging_writer.py:48] [181800] global_step=181800, grad_norm=7.020387172698975, loss=1.7230679988861084
I0127 12:50:08.526349 140026058876672 logging_writer.py:48] [181900] global_step=181900, grad_norm=8.106372833251953, loss=1.754422903060913
I0127 12:50:19.535709 140187804313408 spec.py:321] Evaluating on the training split.
I0127 12:50:25.570429 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 12:50:34.388085 140187804313408 spec.py:349] Evaluating on the test split.
I0127 12:50:36.704456 140187804313408 submission_runner.py:408] Time since start: 64021.79s, 	Step: 181934, 	{'train/accuracy': 0.91898512840271, 'train/loss': 0.40230587124824524, 'validation/accuracy': 0.7721999883651733, 'validation/loss': 0.9920402765274048, 'validation/num_examples': 50000, 'test/accuracy': 0.653700053691864, 'test/loss': 1.6013190746307373, 'test/num_examples': 10000, 'score': 61771.65120244026, 'total_duration': 64021.787281513214, 'accumulated_submission_time': 61771.65120244026, 'accumulated_eval_time': 2237.2278864383698, 'accumulated_logging_time': 6.401177883148193}
I0127 12:50:36.756084 140026075662080 logging_writer.py:48] [181934] accumulated_eval_time=2237.227886, accumulated_logging_time=6.401178, accumulated_submission_time=61771.651202, global_step=181934, preemption_count=0, score=61771.651202, test/accuracy=0.653700, test/loss=1.601319, test/num_examples=10000, total_duration=64021.787282, train/accuracy=0.918985, train/loss=0.402306, validation/accuracy=0.772200, validation/loss=0.992040, validation/num_examples=50000
I0127 12:50:59.558742 140026159523584 logging_writer.py:48] [182000] global_step=182000, grad_norm=7.7536396980285645, loss=1.7638928890228271
I0127 12:51:33.440793 140026075662080 logging_writer.py:48] [182100] global_step=182100, grad_norm=7.581351280212402, loss=1.7759647369384766
I0127 12:52:07.347742 140026159523584 logging_writer.py:48] [182200] global_step=182200, grad_norm=7.082845211029053, loss=1.7970452308654785
I0127 12:52:41.263862 140026075662080 logging_writer.py:48] [182300] global_step=182300, grad_norm=7.36033296585083, loss=1.7584707736968994
I0127 12:53:15.203766 140026159523584 logging_writer.py:48] [182400] global_step=182400, grad_norm=7.6692795753479, loss=1.775352954864502
I0127 12:53:49.118790 140026075662080 logging_writer.py:48] [182500] global_step=182500, grad_norm=8.171821594238281, loss=1.7275749444961548
I0127 12:54:23.052516 140026159523584 logging_writer.py:48] [182600] global_step=182600, grad_norm=8.431207656860352, loss=1.8187156915664673
I0127 12:54:56.984496 140026075662080 logging_writer.py:48] [182700] global_step=182700, grad_norm=7.392416477203369, loss=1.7607470750808716
I0127 12:55:30.852781 140026159523584 logging_writer.py:48] [182800] global_step=182800, grad_norm=8.101473808288574, loss=1.867169737815857
I0127 12:56:04.805566 140026075662080 logging_writer.py:48] [182900] global_step=182900, grad_norm=7.826923370361328, loss=1.7807278633117676
I0127 12:56:38.764869 140026159523584 logging_writer.py:48] [183000] global_step=183000, grad_norm=7.765092849731445, loss=1.7922921180725098
I0127 12:57:12.743540 140026075662080 logging_writer.py:48] [183100] global_step=183100, grad_norm=7.489945888519287, loss=1.7437459230422974
I0127 12:57:46.644291 140026159523584 logging_writer.py:48] [183200] global_step=183200, grad_norm=8.188780784606934, loss=1.8179857730865479
I0127 12:58:20.573966 140026075662080 logging_writer.py:48] [183300] global_step=183300, grad_norm=8.114648818969727, loss=1.738657832145691
I0127 12:58:54.487854 140026159523584 logging_writer.py:48] [183400] global_step=183400, grad_norm=7.767033576965332, loss=1.7056100368499756
I0127 12:59:06.847990 140187804313408 spec.py:321] Evaluating on the training split.
I0127 12:59:12.878039 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 12:59:21.834481 140187804313408 spec.py:349] Evaluating on the test split.
I0127 12:59:24.120923 140187804313408 submission_runner.py:408] Time since start: 64549.20s, 	Step: 183438, 	{'train/accuracy': 0.9219945669174194, 'train/loss': 0.39710769057273865, 'validation/accuracy': 0.772819995880127, 'validation/loss': 0.9916077852249146, 'validation/num_examples': 50000, 'test/accuracy': 0.6526000499725342, 'test/loss': 1.6008130311965942, 'test/num_examples': 10000, 'score': 62281.68023562431, 'total_duration': 64549.20379114151, 'accumulated_submission_time': 62281.68023562431, 'accumulated_eval_time': 2254.5007655620575, 'accumulated_logging_time': 6.462839603424072}
I0127 12:59:24.171995 140026058876672 logging_writer.py:48] [183438] accumulated_eval_time=2254.500766, accumulated_logging_time=6.462840, accumulated_submission_time=62281.680236, global_step=183438, preemption_count=0, score=62281.680236, test/accuracy=0.652600, test/loss=1.600813, test/num_examples=10000, total_duration=64549.203791, train/accuracy=0.921995, train/loss=0.397108, validation/accuracy=0.772820, validation/loss=0.991608, validation/num_examples=50000
I0127 12:59:45.547257 140026067269376 logging_writer.py:48] [183500] global_step=183500, grad_norm=8.388040542602539, loss=1.833632230758667
I0127 13:00:19.440921 140026058876672 logging_writer.py:48] [183600] global_step=183600, grad_norm=8.372841835021973, loss=1.827305793762207
I0127 13:00:53.365078 140026067269376 logging_writer.py:48] [183700] global_step=183700, grad_norm=8.227224349975586, loss=1.8159940242767334
I0127 13:01:27.293632 140026058876672 logging_writer.py:48] [183800] global_step=183800, grad_norm=7.8921613693237305, loss=1.7685136795043945
I0127 13:02:01.213918 140026067269376 logging_writer.py:48] [183900] global_step=183900, grad_norm=8.35634708404541, loss=1.8707139492034912
I0127 13:02:35.154190 140026058876672 logging_writer.py:48] [184000] global_step=184000, grad_norm=7.353915691375732, loss=1.760711431503296
I0127 13:03:09.100063 140026067269376 logging_writer.py:48] [184100] global_step=184100, grad_norm=8.0735502243042, loss=1.6708061695098877
I0127 13:03:43.074287 140026058876672 logging_writer.py:48] [184200] global_step=184200, grad_norm=8.124724388122559, loss=1.8305649757385254
I0127 13:04:16.978666 140026067269376 logging_writer.py:48] [184300] global_step=184300, grad_norm=8.550960540771484, loss=1.8128573894500732
I0127 13:04:50.914761 140026058876672 logging_writer.py:48] [184400] global_step=184400, grad_norm=7.874197959899902, loss=1.7238080501556396
I0127 13:05:24.859983 140026067269376 logging_writer.py:48] [184500] global_step=184500, grad_norm=7.247215270996094, loss=1.7590893507003784
I0127 13:05:58.764096 140026058876672 logging_writer.py:48] [184600] global_step=184600, grad_norm=7.257191181182861, loss=1.7675745487213135
I0127 13:06:32.704082 140026067269376 logging_writer.py:48] [184700] global_step=184700, grad_norm=8.018741607666016, loss=1.7847610712051392
I0127 13:07:06.655244 140026058876672 logging_writer.py:48] [184800] global_step=184800, grad_norm=7.390565395355225, loss=1.7274070978164673
I0127 13:07:40.570662 140026067269376 logging_writer.py:48] [184900] global_step=184900, grad_norm=7.692022323608398, loss=1.781363606452942
I0127 13:07:54.282600 140187804313408 spec.py:321] Evaluating on the training split.
I0127 13:08:00.458353 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 13:08:09.495020 140187804313408 spec.py:349] Evaluating on the test split.
I0127 13:08:11.810652 140187804313408 submission_runner.py:408] Time since start: 65076.89s, 	Step: 184942, 	{'train/accuracy': 0.9215561151504517, 'train/loss': 0.3981364667415619, 'validation/accuracy': 0.7727400064468384, 'validation/loss': 0.9910697937011719, 'validation/num_examples': 50000, 'test/accuracy': 0.653700053691864, 'test/loss': 1.5995631217956543, 'test/num_examples': 10000, 'score': 62791.72662734985, 'total_duration': 65076.893508434296, 'accumulated_submission_time': 62791.72662734985, 'accumulated_eval_time': 2272.0287766456604, 'accumulated_logging_time': 6.52376127243042}
I0127 13:08:11.859599 140026058876672 logging_writer.py:48] [184942] accumulated_eval_time=2272.028777, accumulated_logging_time=6.523761, accumulated_submission_time=62791.726627, global_step=184942, preemption_count=0, score=62791.726627, test/accuracy=0.653700, test/loss=1.599563, test/num_examples=10000, total_duration=65076.893508, train/accuracy=0.921556, train/loss=0.398136, validation/accuracy=0.772740, validation/loss=0.991070, validation/num_examples=50000
I0127 13:08:31.830932 140026067269376 logging_writer.py:48] [185000] global_step=185000, grad_norm=7.733839511871338, loss=1.7691850662231445
I0127 13:09:05.728521 140026058876672 logging_writer.py:48] [185100] global_step=185100, grad_norm=8.010541915893555, loss=1.758182168006897
I0127 13:09:39.663733 140026067269376 logging_writer.py:48] [185200] global_step=185200, grad_norm=8.19497299194336, loss=1.7270019054412842
I0127 13:10:13.602873 140026058876672 logging_writer.py:48] [185300] global_step=185300, grad_norm=8.122475624084473, loss=1.8141881227493286
I0127 13:10:47.544981 140026067269376 logging_writer.py:48] [185400] global_step=185400, grad_norm=7.608285427093506, loss=1.788036823272705
I0127 13:11:21.448588 140026058876672 logging_writer.py:48] [185500] global_step=185500, grad_norm=8.674299240112305, loss=1.7706611156463623
I0127 13:11:48.392738 140026067269376 logging_writer.py:48] [185581] global_step=185581, preemption_count=0, score=63008.197351
I0127 13:11:48.829187 140187804313408 checkpoints.py:490] Saving checkpoint at step: 185581
I0127 13:11:49.906153 140187804313408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_1/checkpoint_185581
I0127 13:11:49.933585 140187804313408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_1/checkpoint_185581.
I0127 13:11:50.789788 140187804313408 submission_runner.py:583] Tuning trial 1/5
I0127 13:11:50.789996 140187804313408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0127 13:11:50.794032 140187804313408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010363520123064518, 'train/loss': 6.91261625289917, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 51.929956912994385, 'total_duration': 88.81795763969421, 'accumulated_submission_time': 51.929956912994385, 'accumulated_eval_time': 36.88790965080261, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1498, {'train/accuracy': 0.07172751426696777, 'train/loss': 5.377313137054443, 'validation/accuracy': 0.0648999959230423, 'validation/loss': 5.448593616485596, 'validation/num_examples': 50000, 'test/accuracy': 0.04650000110268593, 'test/loss': 5.657770156860352, 'test/num_examples': 10000, 'score': 562.0372688770294, 'total_duration': 617.1234951019287, 'accumulated_submission_time': 562.0372688770294, 'accumulated_eval_time': 55.00457406044006, 'accumulated_logging_time': 0.02753734588623047, 'global_step': 1498, 'preemption_count': 0}), (2994, {'train/accuracy': 0.17580117285251617, 'train/loss': 4.277118682861328, 'validation/accuracy': 0.15845999121665955, 'validation/loss': 4.395031929016113, 'validation/num_examples': 50000, 'test/accuracy': 0.11050000786781311, 'test/loss': 4.828097820281982, 'test/num_examples': 10000, 'score': 1072.2711553573608, 'total_duration': 1145.8508818149567, 'accumulated_submission_time': 1072.2711553573608, 'accumulated_eval_time': 73.41657638549805, 'accumulated_logging_time': 0.05562090873718262, 'global_step': 2994, 'preemption_count': 0}), (4489, {'train/accuracy': 0.27919721603393555, 'train/loss': 3.5287153720855713, 'validation/accuracy': 0.2509799897670746, 'validation/loss': 3.671311378479004, 'validation/num_examples': 50000, 'test/accuracy': 0.18370001018047333, 'test/loss': 4.219701766967773, 'test/num_examples': 10000, 'score': 1582.2347013950348, 'total_duration': 1674.067317724228, 'accumulated_submission_time': 1582.2347013950348, 'accumulated_eval_time': 91.58906817436218, 'accumulated_logging_time': 0.08123278617858887, 'global_step': 4489, 'preemption_count': 0}), (5985, {'train/accuracy': 0.36918047070503235, 'train/loss': 2.9538686275482178, 'validation/accuracy': 0.34627997875213623, 'validation/loss': 3.0891966819763184, 'validation/num_examples': 50000, 'test/accuracy': 0.26190000772476196, 'test/loss': 3.715914726257324, 'test/num_examples': 10000, 'score': 2092.3124701976776, 'total_duration': 2202.3470935821533, 'accumulated_submission_time': 2092.3124701976776, 'accumulated_eval_time': 109.70492148399353, 'accumulated_logging_time': 0.11393284797668457, 'global_step': 5985, 'preemption_count': 0}), (7481, {'train/accuracy': 0.4307437837123871, 'train/loss': 2.613607168197632, 'validation/accuracy': 0.3971000015735626, 'validation/loss': 2.790703296661377, 'validation/num_examples': 50000, 'test/accuracy': 0.2989000082015991, 'test/loss': 3.4827613830566406, 'test/num_examples': 10000, 'score': 2602.396719932556, 'total_duration': 2730.6603696346283, 'accumulated_submission_time': 2602.396719932556, 'accumulated_eval_time': 127.85242891311646, 'accumulated_logging_time': 0.14101171493530273, 'global_step': 7481, 'preemption_count': 0}), (8978, {'train/accuracy': 0.49228712916374207, 'train/loss': 2.275744676589966, 'validation/accuracy': 0.4618600010871887, 'validation/loss': 2.433438301086426, 'validation/num_examples': 50000, 'test/accuracy': 0.357200026512146, 'test/loss': 3.100750684738159, 'test/num_examples': 10000, 'score': 3112.5727632045746, 'total_duration': 3259.198537349701, 'accumulated_submission_time': 3112.5727632045746, 'accumulated_eval_time': 146.1283574104309, 'accumulated_logging_time': 0.17496347427368164, 'global_step': 8978, 'preemption_count': 0}), (10475, {'train/accuracy': 0.5693957209587097, 'train/loss': 1.8849722146987915, 'validation/accuracy': 0.5029999613761902, 'validation/loss': 2.2190101146698, 'validation/num_examples': 50000, 'test/accuracy': 0.39510002732276917, 'test/loss': 2.8704304695129395, 'test/num_examples': 10000, 'score': 3622.6284663677216, 'total_duration': 3787.772131204605, 'accumulated_submission_time': 3622.6284663677216, 'accumulated_eval_time': 164.56593775749207, 'accumulated_logging_time': 0.20221304893493652, 'global_step': 10475, 'preemption_count': 0}), (11974, {'train/accuracy': 0.5763313174247742, 'train/loss': 1.9071600437164307, 'validation/accuracy': 0.5275999903678894, 'validation/loss': 2.1289572715759277, 'validation/num_examples': 50000, 'test/accuracy': 0.4093000292778015, 'test/loss': 2.793846368789673, 'test/num_examples': 10000, 'score': 4132.8440663814545, 'total_duration': 4316.348064184189, 'accumulated_submission_time': 4132.8440663814545, 'accumulated_eval_time': 182.83223509788513, 'accumulated_logging_time': 0.2424328327178955, 'global_step': 11974, 'preemption_count': 0}), (13473, {'train/accuracy': 0.5816326141357422, 'train/loss': 1.8359532356262207, 'validation/accuracy': 0.535539984703064, 'validation/loss': 2.061918258666992, 'validation/num_examples': 50000, 'test/accuracy': 0.4231000244617462, 'test/loss': 2.7273426055908203, 'test/num_examples': 10000, 'score': 4643.087154865265, 'total_duration': 4845.225155115128, 'accumulated_submission_time': 4643.087154865265, 'accumulated_eval_time': 201.3808000087738, 'accumulated_logging_time': 0.27495527267456055, 'global_step': 13473, 'preemption_count': 0}), (14973, {'train/accuracy': 0.5989516973495483, 'train/loss': 1.7546666860580444, 'validation/accuracy': 0.5537399649620056, 'validation/loss': 1.9646070003509521, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.6510255336761475, 'test/num_examples': 10000, 'score': 5153.3032858371735, 'total_duration': 5374.694349765778, 'accumulated_submission_time': 5153.3032858371735, 'accumulated_eval_time': 220.55117321014404, 'accumulated_logging_time': 0.30461955070495605, 'global_step': 14973, 'preemption_count': 0}), (16473, {'train/accuracy': 0.59375, 'train/loss': 1.8033223152160645, 'validation/accuracy': 0.5557399988174438, 'validation/loss': 1.9902366399765015, 'validation/num_examples': 50000, 'test/accuracy': 0.4360000193119049, 'test/loss': 2.6539673805236816, 'test/num_examples': 10000, 'score': 5663.362612962723, 'total_duration': 5905.069729804993, 'accumulated_submission_time': 5663.362612962723, 'accumulated_eval_time': 240.77084040641785, 'accumulated_logging_time': 0.34645605087280273, 'global_step': 16473, 'preemption_count': 0}), (17974, {'train/accuracy': 0.6002271771430969, 'train/loss': 1.7657809257507324, 'validation/accuracy': 0.5660600066184998, 'validation/loss': 1.9302324056625366, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.6314282417297363, 'test/num_examples': 10000, 'score': 6173.529769182205, 'total_duration': 6436.721809148788, 'accumulated_submission_time': 6173.529769182205, 'accumulated_eval_time': 262.16706109046936, 'accumulated_logging_time': 0.3816823959350586, 'global_step': 17974, 'preemption_count': 0}), (19474, {'train/accuracy': 0.6611328125, 'train/loss': 1.4854481220245361, 'validation/accuracy': 0.5753799676895142, 'validation/loss': 1.8646032810211182, 'validation/num_examples': 50000, 'test/accuracy': 0.45090001821517944, 'test/loss': 2.5466179847717285, 'test/num_examples': 10000, 'score': 6683.501819372177, 'total_duration': 6971.627715110779, 'accumulated_submission_time': 6683.501819372177, 'accumulated_eval_time': 287.0052680969238, 'accumulated_logging_time': 0.423555850982666, 'global_step': 19474, 'preemption_count': 0}), (20975, {'train/accuracy': 0.6342673897743225, 'train/loss': 1.583891749382019, 'validation/accuracy': 0.5761199593544006, 'validation/loss': 1.8759549856185913, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.530134439468384, 'test/num_examples': 10000, 'score': 7193.579409122467, 'total_duration': 7504.646897554398, 'accumulated_submission_time': 7193.579409122467, 'accumulated_eval_time': 309.8524570465088, 'accumulated_logging_time': 0.46407437324523926, 'global_step': 20975, 'preemption_count': 0}), (22476, {'train/accuracy': 0.6265146732330322, 'train/loss': 1.6444129943847656, 'validation/accuracy': 0.5791199803352356, 'validation/loss': 1.8640754222869873, 'validation/num_examples': 50000, 'test/accuracy': 0.45260003209114075, 'test/loss': 2.587834596633911, 'test/num_examples': 10000, 'score': 7703.516690969467, 'total_duration': 8035.961810588837, 'accumulated_submission_time': 7703.516690969467, 'accumulated_eval_time': 331.14792466163635, 'accumulated_logging_time': 0.49283337593078613, 'global_step': 22476, 'preemption_count': 0}), (23978, {'train/accuracy': 0.6319355964660645, 'train/loss': 1.6008868217468262, 'validation/accuracy': 0.582260012626648, 'validation/loss': 1.830167293548584, 'validation/num_examples': 50000, 'test/accuracy': 0.4561000168323517, 'test/loss': 2.5494673252105713, 'test/num_examples': 10000, 'score': 8213.743519306183, 'total_duration': 8569.167505025864, 'accumulated_submission_time': 8213.743519306183, 'accumulated_eval_time': 354.0397229194641, 'accumulated_logging_time': 0.5262980461120605, 'global_step': 23978, 'preemption_count': 0}), (25480, {'train/accuracy': 0.6276904940605164, 'train/loss': 1.620834469795227, 'validation/accuracy': 0.586080014705658, 'validation/loss': 1.8189243078231812, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.476848602294922, 'test/num_examples': 10000, 'score': 8723.919181346893, 'total_duration': 9102.295284986496, 'accumulated_submission_time': 8723.919181346893, 'accumulated_eval_time': 376.90863513946533, 'accumulated_logging_time': 0.5557169914245605, 'global_step': 25480, 'preemption_count': 0}), (26982, {'train/accuracy': 0.6330516338348389, 'train/loss': 1.5673516988754272, 'validation/accuracy': 0.5966199636459351, 'validation/loss': 1.7551690340042114, 'validation/num_examples': 50000, 'test/accuracy': 0.469400018453598, 'test/loss': 2.4492757320404053, 'test/num_examples': 10000, 'score': 9234.135885238647, 'total_duration': 9633.838790893555, 'accumulated_submission_time': 9234.135885238647, 'accumulated_eval_time': 398.14722084999084, 'accumulated_logging_time': 0.58970046043396, 'global_step': 26982, 'preemption_count': 0}), (28481, {'train/accuracy': 0.6598572731018066, 'train/loss': 1.473987102508545, 'validation/accuracy': 0.5931400060653687, 'validation/loss': 1.779598593711853, 'validation/num_examples': 50000, 'test/accuracy': 0.4660000205039978, 'test/loss': 2.4653191566467285, 'test/num_examples': 10000, 'score': 9743.258620500565, 'total_duration': 10166.62963104248, 'accumulated_submission_time': 9743.258620500565, 'accumulated_eval_time': 420.8384771347046, 'accumulated_logging_time': 1.5133922100067139, 'global_step': 28481, 'preemption_count': 0}), (29983, {'train/accuracy': 0.6408442258834839, 'train/loss': 1.5831868648529053, 'validation/accuracy': 0.5834800004959106, 'validation/loss': 1.8493436574935913, 'validation/num_examples': 50000, 'test/accuracy': 0.4546000361442566, 'test/loss': 2.542966604232788, 'test/num_examples': 10000, 'score': 10253.200542211533, 'total_duration': 10698.343393564224, 'accumulated_submission_time': 10253.200542211533, 'accumulated_eval_time': 442.5229060649872, 'accumulated_logging_time': 1.5470540523529053, 'global_step': 29983, 'preemption_count': 0}), (31484, {'train/accuracy': 0.650390625, 'train/loss': 1.5332188606262207, 'validation/accuracy': 0.6009599566459656, 'validation/loss': 1.777511715888977, 'validation/num_examples': 50000, 'test/accuracy': 0.47510001063346863, 'test/loss': 2.450975179672241, 'test/num_examples': 10000, 'score': 10763.207682132721, 'total_duration': 11231.830409526825, 'accumulated_submission_time': 10763.207682132721, 'accumulated_eval_time': 465.9139356613159, 'accumulated_logging_time': 1.5822596549987793, 'global_step': 31484, 'preemption_count': 0}), (32987, {'train/accuracy': 0.6478196382522583, 'train/loss': 1.5455231666564941, 'validation/accuracy': 0.5983999967575073, 'validation/loss': 1.7723729610443115, 'validation/num_examples': 50000, 'test/accuracy': 0.47190001606941223, 'test/loss': 2.464462995529175, 'test/num_examples': 10000, 'score': 11273.447633743286, 'total_duration': 11764.022417783737, 'accumulated_submission_time': 11273.447633743286, 'accumulated_eval_time': 487.7795009613037, 'accumulated_logging_time': 1.6160292625427246, 'global_step': 32987, 'preemption_count': 0}), (34490, {'train/accuracy': 0.6456871628761292, 'train/loss': 1.5215743780136108, 'validation/accuracy': 0.6019399762153625, 'validation/loss': 1.7319201231002808, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.4039146900177, 'test/num_examples': 10000, 'score': 11783.682542085648, 'total_duration': 12295.01685166359, 'accumulated_submission_time': 11783.682542085648, 'accumulated_eval_time': 508.4510877132416, 'accumulated_logging_time': 1.65091872215271, 'global_step': 34490, 'preemption_count': 0}), (35993, {'train/accuracy': 0.6405652165412903, 'train/loss': 1.5603916645050049, 'validation/accuracy': 0.5936599969863892, 'validation/loss': 1.7699402570724487, 'validation/num_examples': 50000, 'test/accuracy': 0.4715000092983246, 'test/loss': 2.450953722000122, 'test/num_examples': 10000, 'score': 12293.693517684937, 'total_duration': 12827.367676734924, 'accumulated_submission_time': 12293.693517684937, 'accumulated_eval_time': 530.7041218280792, 'accumulated_logging_time': 1.6843512058258057, 'global_step': 35993, 'preemption_count': 0}), (37495, {'train/accuracy': 0.6504703164100647, 'train/loss': 1.510545015335083, 'validation/accuracy': 0.608460009098053, 'validation/loss': 1.7156388759613037, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.3600785732269287, 'test/num_examples': 10000, 'score': 12803.700054168701, 'total_duration': 13358.992667198181, 'accumulated_submission_time': 12803.700054168701, 'accumulated_eval_time': 552.2334206104279, 'accumulated_logging_time': 1.7202045917510986, 'global_step': 37495, 'preemption_count': 0}), (38998, {'train/accuracy': 0.6715760231018066, 'train/loss': 1.4409266710281372, 'validation/accuracy': 0.6063599586486816, 'validation/loss': 1.7327011823654175, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.3913426399230957, 'test/num_examples': 10000, 'score': 13313.690217733383, 'total_duration': 13889.194969892502, 'accumulated_submission_time': 13313.690217733383, 'accumulated_eval_time': 572.3573670387268, 'accumulated_logging_time': 1.755732774734497, 'global_step': 38998, 'preemption_count': 0}), (40500, {'train/accuracy': 0.6689851880073547, 'train/loss': 1.4142094850540161, 'validation/accuracy': 0.6127600073814392, 'validation/loss': 1.6750468015670776, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.3593156337738037, 'test/num_examples': 10000, 'score': 13823.646218061447, 'total_duration': 14417.326255083084, 'accumulated_submission_time': 13823.646218061447, 'accumulated_eval_time': 590.4436695575714, 'accumulated_logging_time': 1.7907118797302246, 'global_step': 40500, 'preemption_count': 0}), (42003, {'train/accuracy': 0.661531388759613, 'train/loss': 1.4491329193115234, 'validation/accuracy': 0.6156399846076965, 'validation/loss': 1.6673866510391235, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.3523242473602295, 'test/num_examples': 10000, 'score': 14333.642135620117, 'total_duration': 14947.89179635048, 'accumulated_submission_time': 14333.642135620117, 'accumulated_eval_time': 610.9255712032318, 'accumulated_logging_time': 1.8257253170013428, 'global_step': 42003, 'preemption_count': 0}), (43506, {'train/accuracy': 0.6548947691917419, 'train/loss': 1.4952327013015747, 'validation/accuracy': 0.6090599894523621, 'validation/loss': 1.7058863639831543, 'validation/num_examples': 50000, 'test/accuracy': 0.4839000105857849, 'test/loss': 2.3706626892089844, 'test/num_examples': 10000, 'score': 14843.627077817917, 'total_duration': 15476.748891830444, 'accumulated_submission_time': 14843.627077817917, 'accumulated_eval_time': 629.7090165615082, 'accumulated_logging_time': 1.8625688552856445, 'global_step': 43506, 'preemption_count': 0}), (45009, {'train/accuracy': 0.6541972160339355, 'train/loss': 1.492714524269104, 'validation/accuracy': 0.6135199666023254, 'validation/loss': 1.686639666557312, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.3645849227905273, 'test/num_examples': 10000, 'score': 15353.602659702301, 'total_duration': 16005.477684020996, 'accumulated_submission_time': 15353.602659702301, 'accumulated_eval_time': 648.3707220554352, 'accumulated_logging_time': 1.9011075496673584, 'global_step': 45009, 'preemption_count': 0}), (46513, {'train/accuracy': 0.6575055718421936, 'train/loss': 1.4960453510284424, 'validation/accuracy': 0.6189199686050415, 'validation/loss': 1.6851189136505127, 'validation/num_examples': 50000, 'test/accuracy': 0.4930000305175781, 'test/loss': 2.3379759788513184, 'test/num_examples': 10000, 'score': 15863.77936911583, 'total_duration': 16533.809260606766, 'accumulated_submission_time': 15863.77936911583, 'accumulated_eval_time': 666.4376637935638, 'accumulated_logging_time': 1.9357657432556152, 'global_step': 46513, 'preemption_count': 0}), (48017, {'train/accuracy': 0.6881377696990967, 'train/loss': 1.3390893936157227, 'validation/accuracy': 0.6141799688339233, 'validation/loss': 1.6819976568222046, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.3249881267547607, 'test/num_examples': 10000, 'score': 16373.80011177063, 'total_duration': 17062.553616285324, 'accumulated_submission_time': 16373.80011177063, 'accumulated_eval_time': 685.0665671825409, 'accumulated_logging_time': 1.9751687049865723, 'global_step': 48017, 'preemption_count': 0}), (49520, {'train/accuracy': 0.6678889989852905, 'train/loss': 1.4150813817977905, 'validation/accuracy': 0.6149399876594543, 'validation/loss': 1.6707707643508911, 'validation/num_examples': 50000, 'test/accuracy': 0.4930000305175781, 'test/loss': 2.3653693199157715, 'test/num_examples': 10000, 'score': 16883.71029162407, 'total_duration': 17590.99581003189, 'accumulated_submission_time': 16883.71029162407, 'accumulated_eval_time': 703.5050427913666, 'accumulated_logging_time': 2.0140111446380615, 'global_step': 49520, 'preemption_count': 0}), (51024, {'train/accuracy': 0.6670718789100647, 'train/loss': 1.4345439672470093, 'validation/accuracy': 0.6176199913024902, 'validation/loss': 1.679994821548462, 'validation/num_examples': 50000, 'test/accuracy': 0.5006999969482422, 'test/loss': 2.3009283542633057, 'test/num_examples': 10000, 'score': 17393.85679912567, 'total_duration': 18119.246876478195, 'accumulated_submission_time': 17393.85679912567, 'accumulated_eval_time': 721.513111114502, 'accumulated_logging_time': 2.0556466579437256, 'global_step': 51024, 'preemption_count': 0}), (52528, {'train/accuracy': 0.6692044138908386, 'train/loss': 1.4314494132995605, 'validation/accuracy': 0.6190999746322632, 'validation/loss': 1.6480151414871216, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.28476881980896, 'test/num_examples': 10000, 'score': 17904.098637342453, 'total_duration': 18648.099063634872, 'accumulated_submission_time': 17904.098637342453, 'accumulated_eval_time': 740.029061794281, 'accumulated_logging_time': 2.0966885089874268, 'global_step': 52528, 'preemption_count': 0}), (54032, {'train/accuracy': 0.6640425324440002, 'train/loss': 1.432912826538086, 'validation/accuracy': 0.620199978351593, 'validation/loss': 1.6460167169570923, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.3091156482696533, 'test/num_examples': 10000, 'score': 18414.036471366882, 'total_duration': 19175.45029401779, 'accumulated_submission_time': 18414.036471366882, 'accumulated_eval_time': 757.3529677391052, 'accumulated_logging_time': 2.1330432891845703, 'global_step': 54032, 'preemption_count': 0}), (55535, {'train/accuracy': 0.6664939522743225, 'train/loss': 1.427367925643921, 'validation/accuracy': 0.6240000128746033, 'validation/loss': 1.622045636177063, 'validation/num_examples': 50000, 'test/accuracy': 0.5022000074386597, 'test/loss': 2.2728426456451416, 'test/num_examples': 10000, 'score': 18923.95730996132, 'total_duration': 19702.92580795288, 'accumulated_submission_time': 18923.95730996132, 'accumulated_eval_time': 774.8156905174255, 'accumulated_logging_time': 2.171757459640503, 'global_step': 55535, 'preemption_count': 0}), (57040, {'train/accuracy': 0.7063934803009033, 'train/loss': 1.2701016664505005, 'validation/accuracy': 0.6281599998474121, 'validation/loss': 1.6143368482589722, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2856898307800293, 'test/num_examples': 10000, 'score': 19434.118763685226, 'total_duration': 20230.6025326252, 'accumulated_submission_time': 19434.118763685226, 'accumulated_eval_time': 792.2390511035919, 'accumulated_logging_time': 2.209519863128662, 'global_step': 57040, 'preemption_count': 0}), (58544, {'train/accuracy': 0.6819196343421936, 'train/loss': 1.3621110916137695, 'validation/accuracy': 0.6212799549102783, 'validation/loss': 1.638480544090271, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.3111507892608643, 'test/num_examples': 10000, 'score': 19944.30988574028, 'total_duration': 20758.254354953766, 'accumulated_submission_time': 19944.30988574028, 'accumulated_eval_time': 809.6045508384705, 'accumulated_logging_time': 2.2512757778167725, 'global_step': 58544, 'preemption_count': 0}), (60048, {'train/accuracy': 0.6807836294174194, 'train/loss': 1.359082579612732, 'validation/accuracy': 0.6322000026702881, 'validation/loss': 1.601298213005066, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.2946648597717285, 'test/num_examples': 10000, 'score': 20454.509740829468, 'total_duration': 21285.80756020546, 'accumulated_submission_time': 20454.509740829468, 'accumulated_eval_time': 826.8658409118652, 'accumulated_logging_time': 2.2905819416046143, 'global_step': 60048, 'preemption_count': 0}), (61552, {'train/accuracy': 0.6801857352256775, 'train/loss': 1.3753252029418945, 'validation/accuracy': 0.6317799687385559, 'validation/loss': 1.5977727174758911, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2585179805755615, 'test/num_examples': 10000, 'score': 20964.469042301178, 'total_duration': 21813.258487939835, 'accumulated_submission_time': 20964.469042301178, 'accumulated_eval_time': 844.2586009502411, 'accumulated_logging_time': 2.3358445167541504, 'global_step': 61552, 'preemption_count': 0}), (63057, {'train/accuracy': 0.6864038705825806, 'train/loss': 1.3631954193115234, 'validation/accuracy': 0.6362400054931641, 'validation/loss': 1.5933818817138672, 'validation/num_examples': 50000, 'test/accuracy': 0.5072000026702881, 'test/loss': 2.271667242050171, 'test/num_examples': 10000, 'score': 21474.617376327515, 'total_duration': 22340.84404706955, 'accumulated_submission_time': 21474.617376327515, 'accumulated_eval_time': 861.6014168262482, 'accumulated_logging_time': 2.3771815299987793, 'global_step': 63057, 'preemption_count': 0}), (64562, {'train/accuracy': 0.6725525856018066, 'train/loss': 1.4098135232925415, 'validation/accuracy': 0.6275399923324585, 'validation/loss': 1.6258821487426758, 'validation/num_examples': 50000, 'test/accuracy': 0.4985000193119049, 'test/loss': 2.332926034927368, 'test/num_examples': 10000, 'score': 21984.827194929123, 'total_duration': 22868.620908498764, 'accumulated_submission_time': 21984.827194929123, 'accumulated_eval_time': 879.075288772583, 'accumulated_logging_time': 2.4166557788848877, 'global_step': 64562, 'preemption_count': 0}), (66067, {'train/accuracy': 0.7122927308082581, 'train/loss': 1.2576779127120972, 'validation/accuracy': 0.624459981918335, 'validation/loss': 1.6419285535812378, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.2767369747161865, 'test/num_examples': 10000, 'score': 22494.960673332214, 'total_duration': 23396.362580537796, 'accumulated_submission_time': 22494.960673332214, 'accumulated_eval_time': 896.5920617580414, 'accumulated_logging_time': 2.4555139541625977, 'global_step': 66067, 'preemption_count': 0}), (67572, {'train/accuracy': 0.6955117583274841, 'train/loss': 1.2980284690856934, 'validation/accuracy': 0.6328999996185303, 'validation/loss': 1.5780168771743774, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.217724323272705, 'test/num_examples': 10000, 'score': 23005.19539809227, 'total_duration': 23924.20377588272, 'accumulated_submission_time': 23005.19539809227, 'accumulated_eval_time': 914.1040511131287, 'accumulated_logging_time': 2.495955228805542, 'global_step': 67572, 'preemption_count': 0}), (69077, {'train/accuracy': 0.6938177347183228, 'train/loss': 1.3356457948684692, 'validation/accuracy': 0.634719967842102, 'validation/loss': 1.5932949781417847, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.25681209564209, 'test/num_examples': 10000, 'score': 23515.14908361435, 'total_duration': 24453.224115133286, 'accumulated_submission_time': 23515.14908361435, 'accumulated_eval_time': 933.078031539917, 'accumulated_logging_time': 2.535719633102417, 'global_step': 69077, 'preemption_count': 0}), (70582, {'train/accuracy': 0.6916653513908386, 'train/loss': 1.3394980430603027, 'validation/accuracy': 0.6408199667930603, 'validation/loss': 1.581676959991455, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2319271564483643, 'test/num_examples': 10000, 'score': 24025.312272787094, 'total_duration': 24981.911822795868, 'accumulated_submission_time': 24025.312272787094, 'accumulated_eval_time': 951.5091438293457, 'accumulated_logging_time': 2.5747079849243164, 'global_step': 70582, 'preemption_count': 0}), (72087, {'train/accuracy': 0.6868821382522583, 'train/loss': 1.351656198501587, 'validation/accuracy': 0.6393600106239319, 'validation/loss': 1.5750479698181152, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.226827621459961, 'test/num_examples': 10000, 'score': 24535.51449584961, 'total_duration': 25509.434837818146, 'accumulated_submission_time': 24535.51449584961, 'accumulated_eval_time': 968.7333037853241, 'accumulated_logging_time': 2.617192506790161, 'global_step': 72087, 'preemption_count': 0}), (73592, {'train/accuracy': 0.6874202489852905, 'train/loss': 1.3545960187911987, 'validation/accuracy': 0.6374799609184265, 'validation/loss': 1.5858798027038574, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.2407429218292236, 'test/num_examples': 10000, 'score': 25045.693425178528, 'total_duration': 26037.469779729843, 'accumulated_submission_time': 25045.693425178528, 'accumulated_eval_time': 986.4949653148651, 'accumulated_logging_time': 2.6583826541900635, 'global_step': 73592, 'preemption_count': 0}), (75098, {'train/accuracy': 0.7245495915412903, 'train/loss': 1.1946289539337158, 'validation/accuracy': 0.6363799571990967, 'validation/loss': 1.5921239852905273, 'validation/num_examples': 50000, 'test/accuracy': 0.5095000267028809, 'test/loss': 2.269789695739746, 'test/num_examples': 10000, 'score': 25555.87436771393, 'total_duration': 26566.167140960693, 'accumulated_submission_time': 25555.87436771393, 'accumulated_eval_time': 1004.9125220775604, 'accumulated_logging_time': 2.7030186653137207, 'global_step': 75098, 'preemption_count': 0}), (76603, {'train/accuracy': 0.7009127736091614, 'train/loss': 1.2936850786209106, 'validation/accuracy': 0.6375199556350708, 'validation/loss': 1.5906641483306885, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.208439588546753, 'test/num_examples': 10000, 'score': 26066.066779613495, 'total_duration': 27094.418971776962, 'accumulated_submission_time': 26066.066779613495, 'accumulated_eval_time': 1022.8778517246246, 'accumulated_logging_time': 2.7443735599517822, 'global_step': 76603, 'preemption_count': 0}), (78107, {'train/accuracy': 0.7107182741165161, 'train/loss': 1.2404513359069824, 'validation/accuracy': 0.6543399691581726, 'validation/loss': 1.5085474252700806, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.185645580291748, 'test/num_examples': 10000, 'score': 26576.229032993317, 'total_duration': 27621.786662578583, 'accumulated_submission_time': 26576.229032993317, 'accumulated_eval_time': 1039.984982252121, 'accumulated_logging_time': 2.788038969039917, 'global_step': 78107, 'preemption_count': 0}), (79612, {'train/accuracy': 0.7019491195678711, 'train/loss': 1.2742984294891357, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.5245815515518188, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.1862270832061768, 'test/num_examples': 10000, 'score': 27086.374658584595, 'total_duration': 28149.237203598022, 'accumulated_submission_time': 27086.374658584595, 'accumulated_eval_time': 1057.1927177906036, 'accumulated_logging_time': 2.82971453666687, 'global_step': 79612, 'preemption_count': 0}), (81117, {'train/accuracy': 0.6994778513908386, 'train/loss': 1.2843109369277954, 'validation/accuracy': 0.6515199542045593, 'validation/loss': 1.5109398365020752, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.175584554672241, 'test/num_examples': 10000, 'score': 27596.460722208023, 'total_duration': 28676.73327088356, 'accumulated_submission_time': 27596.460722208023, 'accumulated_eval_time': 1074.5033564567566, 'accumulated_logging_time': 2.8747684955596924, 'global_step': 81117, 'preemption_count': 0}), (82622, {'train/accuracy': 0.6990393400192261, 'train/loss': 1.291176676750183, 'validation/accuracy': 0.6479799747467041, 'validation/loss': 1.5192651748657227, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1800341606140137, 'test/num_examples': 10000, 'score': 28106.558556318283, 'total_duration': 29204.21426296234, 'accumulated_submission_time': 28106.558556318283, 'accumulated_eval_time': 1091.787621974945, 'accumulated_logging_time': 2.9192492961883545, 'global_step': 82622, 'preemption_count': 0}), (84127, {'train/accuracy': 0.7421077489852905, 'train/loss': 1.1094766855239868, 'validation/accuracy': 0.6539199948310852, 'validation/loss': 1.498048186302185, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.172151565551758, 'test/num_examples': 10000, 'score': 28616.616693496704, 'total_duration': 29731.992182970047, 'accumulated_submission_time': 28616.616693496704, 'accumulated_eval_time': 1109.4084930419922, 'accumulated_logging_time': 2.964075803756714, 'global_step': 84127, 'preemption_count': 0}), (85632, {'train/accuracy': 0.72562575340271, 'train/loss': 1.1676472425460815, 'validation/accuracy': 0.6571399569511414, 'validation/loss': 1.469819188117981, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.140961170196533, 'test/num_examples': 10000, 'score': 29126.722969293594, 'total_duration': 30259.5151386261, 'accumulated_submission_time': 29126.722969293594, 'accumulated_eval_time': 1126.7367713451385, 'accumulated_logging_time': 2.999528408050537, 'global_step': 85632, 'preemption_count': 0}), (87137, {'train/accuracy': 0.7182118892669678, 'train/loss': 1.213655948638916, 'validation/accuracy': 0.6566799879074097, 'validation/loss': 1.4903264045715332, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.1381936073303223, 'test/num_examples': 10000, 'score': 29636.656057357788, 'total_duration': 30786.70604276657, 'accumulated_submission_time': 29636.656057357788, 'accumulated_eval_time': 1143.8874711990356, 'accumulated_logging_time': 3.0535805225372314, 'global_step': 87137, 'preemption_count': 0}), (88642, {'train/accuracy': 0.7137675285339355, 'train/loss': 1.2036211490631104, 'validation/accuracy': 0.6636999845504761, 'validation/loss': 1.4569283723831177, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.1352744102478027, 'test/num_examples': 10000, 'score': 30146.884006261826, 'total_duration': 31314.36168217659, 'accumulated_submission_time': 30146.884006261826, 'accumulated_eval_time': 1161.2099361419678, 'accumulated_logging_time': 3.103337049484253, 'global_step': 88642, 'preemption_count': 0}), (90147, {'train/accuracy': 0.7099409699440002, 'train/loss': 1.2612212896347046, 'validation/accuracy': 0.6620799899101257, 'validation/loss': 1.5005483627319336, 'validation/num_examples': 50000, 'test/accuracy': 0.5302000045776367, 'test/loss': 2.188394784927368, 'test/num_examples': 10000, 'score': 30656.89363193512, 'total_duration': 31842.028629779816, 'accumulated_submission_time': 30656.89363193512, 'accumulated_eval_time': 1178.7714076042175, 'accumulated_logging_time': 3.1447083950042725, 'global_step': 90147, 'preemption_count': 0}), (91652, {'train/accuracy': 0.7141063213348389, 'train/loss': 1.2344672679901123, 'validation/accuracy': 0.6605799794197083, 'validation/loss': 1.4747819900512695, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.1402695178985596, 'test/num_examples': 10000, 'score': 31167.033656597137, 'total_duration': 32369.68866419792, 'accumulated_submission_time': 31167.033656597137, 'accumulated_eval_time': 1196.192389011383, 'accumulated_logging_time': 3.1903398036956787, 'global_step': 91652, 'preemption_count': 0}), (93157, {'train/accuracy': 0.7379822731018066, 'train/loss': 1.1373311281204224, 'validation/accuracy': 0.6595999598503113, 'validation/loss': 1.4904597997665405, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.143916606903076, 'test/num_examples': 10000, 'score': 31677.014585733414, 'total_duration': 32897.162084817886, 'accumulated_submission_time': 31677.014585733414, 'accumulated_eval_time': 1213.5849130153656, 'accumulated_logging_time': 3.2358620166778564, 'global_step': 93157, 'preemption_count': 0}), (94662, {'train/accuracy': 0.7356903553009033, 'train/loss': 1.137479305267334, 'validation/accuracy': 0.6634199619293213, 'validation/loss': 1.4608635902404785, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.139446258544922, 'test/num_examples': 10000, 'score': 32187.246658563614, 'total_duration': 33424.74766254425, 'accumulated_submission_time': 32187.246658563614, 'accumulated_eval_time': 1230.840161561966, 'accumulated_logging_time': 3.2798256874084473, 'global_step': 94662, 'preemption_count': 0}), (96167, {'train/accuracy': 0.7338966727256775, 'train/loss': 1.1363016366958618, 'validation/accuracy': 0.6714999675750732, 'validation/loss': 1.4211866855621338, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.080425262451172, 'test/num_examples': 10000, 'score': 32697.393936157227, 'total_duration': 33952.223504543304, 'accumulated_submission_time': 32697.393936157227, 'accumulated_eval_time': 1248.0705358982086, 'accumulated_logging_time': 3.32486629486084, 'global_step': 96167, 'preemption_count': 0}), (97672, {'train/accuracy': 0.7274991869926453, 'train/loss': 1.1664454936981201, 'validation/accuracy': 0.6618399620056152, 'validation/loss': 1.453052043914795, 'validation/num_examples': 50000, 'test/accuracy': 0.5360000133514404, 'test/loss': 2.12292218208313, 'test/num_examples': 10000, 'score': 33207.3950073719, 'total_duration': 34479.80842757225, 'accumulated_submission_time': 33207.3950073719, 'accumulated_eval_time': 1265.554355621338, 'accumulated_logging_time': 3.372291326522827, 'global_step': 97672, 'preemption_count': 0}), (99177, {'train/accuracy': 0.7198660373687744, 'train/loss': 1.1820178031921387, 'validation/accuracy': 0.6629999876022339, 'validation/loss': 1.4383418560028076, 'validation/num_examples': 50000, 'test/accuracy': 0.538100004196167, 'test/loss': 2.1251816749572754, 'test/num_examples': 10000, 'score': 33717.37340140343, 'total_duration': 35007.27703619003, 'accumulated_submission_time': 33717.37340140343, 'accumulated_eval_time': 1282.9458377361298, 'accumulated_logging_time': 3.418320894241333, 'global_step': 99177, 'preemption_count': 0}), (100682, {'train/accuracy': 0.7286949753761292, 'train/loss': 1.1676808595657349, 'validation/accuracy': 0.6708599925041199, 'validation/loss': 1.4283438920974731, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.0684101581573486, 'test/num_examples': 10000, 'score': 34227.44268536568, 'total_duration': 35534.58135795593, 'accumulated_submission_time': 34227.44268536568, 'accumulated_eval_time': 1300.0815467834473, 'accumulated_logging_time': 3.4637675285339355, 'global_step': 100682, 'preemption_count': 0}), (102187, {'train/accuracy': 0.748046875, 'train/loss': 1.0852841138839722, 'validation/accuracy': 0.6756599545478821, 'validation/loss': 1.4007805585861206, 'validation/num_examples': 50000, 'test/accuracy': 0.5542000532150269, 'test/loss': 2.0193703174591064, 'test/num_examples': 10000, 'score': 34737.54347777367, 'total_duration': 36061.81385445595, 'accumulated_submission_time': 34737.54347777367, 'accumulated_eval_time': 1317.113971233368, 'accumulated_logging_time': 3.5098297595977783, 'global_step': 102187, 'preemption_count': 0}), (103692, {'train/accuracy': 0.7497408986091614, 'train/loss': 1.068345069885254, 'validation/accuracy': 0.6728799939155579, 'validation/loss': 1.414284348487854, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.0770750045776367, 'test/num_examples': 10000, 'score': 35247.70157814026, 'total_duration': 36589.16969251633, 'accumulated_submission_time': 35247.70157814026, 'accumulated_eval_time': 1334.212421655655, 'accumulated_logging_time': 3.555453062057495, 'global_step': 103692, 'preemption_count': 0}), (105198, {'train/accuracy': 0.7359095811843872, 'train/loss': 1.1194480657577515, 'validation/accuracy': 0.6708999872207642, 'validation/loss': 1.4225313663482666, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.1047375202178955, 'test/num_examples': 10000, 'score': 35757.91668009758, 'total_duration': 37117.02583503723, 'accumulated_submission_time': 35757.91668009758, 'accumulated_eval_time': 1351.7521600723267, 'accumulated_logging_time': 3.603590250015259, 'global_step': 105198, 'preemption_count': 0}), (106703, {'train/accuracy': 0.7438616156578064, 'train/loss': 1.0795286893844604, 'validation/accuracy': 0.6801799535751343, 'validation/loss': 1.3656352758407593, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.037686824798584, 'test/num_examples': 10000, 'score': 36268.02729392052, 'total_duration': 37644.65886926651, 'accumulated_submission_time': 36268.02729392052, 'accumulated_eval_time': 1369.1742932796478, 'accumulated_logging_time': 3.651136636734009, 'global_step': 106703, 'preemption_count': 0}), (108205, {'train/accuracy': 0.7301897406578064, 'train/loss': 1.1509002447128296, 'validation/accuracy': 0.6698799729347229, 'validation/loss': 1.42633056640625, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.072263479232788, 'test/num_examples': 10000, 'score': 36778.0198366642, 'total_duration': 38171.862953186035, 'accumulated_submission_time': 36778.0198366642, 'accumulated_eval_time': 1386.2812361717224, 'accumulated_logging_time': 3.69752836227417, 'global_step': 108205, 'preemption_count': 0}), (109710, {'train/accuracy': 0.7444196343421936, 'train/loss': 1.0855257511138916, 'validation/accuracy': 0.6843999624252319, 'validation/loss': 1.3646774291992188, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.0237014293670654, 'test/num_examples': 10000, 'score': 37288.15925168991, 'total_duration': 38699.43433403969, 'accumulated_submission_time': 37288.15925168991, 'accumulated_eval_time': 1403.6111969947815, 'accumulated_logging_time': 3.7452445030212402, 'global_step': 109710, 'preemption_count': 0}), (111215, {'train/accuracy': 0.7463129758834839, 'train/loss': 1.0860766172409058, 'validation/accuracy': 0.6835599541664124, 'validation/loss': 1.3672128915786743, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.0336456298828125, 'test/num_examples': 10000, 'score': 37798.17662835121, 'total_duration': 39227.00764942169, 'accumulated_submission_time': 37798.17662835121, 'accumulated_eval_time': 1421.063180923462, 'accumulated_logging_time': 3.7959115505218506, 'global_step': 111215, 'preemption_count': 0}), (112720, {'train/accuracy': 0.7732580900192261, 'train/loss': 0.9794655442237854, 'validation/accuracy': 0.6888999938964844, 'validation/loss': 1.3610857725143433, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 2.027822256088257, 'test/num_examples': 10000, 'score': 38308.20759654045, 'total_duration': 39754.66309762001, 'accumulated_submission_time': 38308.20759654045, 'accumulated_eval_time': 1438.5874259471893, 'accumulated_logging_time': 3.8423855304718018, 'global_step': 112720, 'preemption_count': 0}), (114225, {'train/accuracy': 0.7552614808082581, 'train/loss': 1.0510189533233643, 'validation/accuracy': 0.6833999752998352, 'validation/loss': 1.376708984375, 'validation/num_examples': 50000, 'test/accuracy': 0.557200014591217, 'test/loss': 2.0630359649658203, 'test/num_examples': 10000, 'score': 38818.21781897545, 'total_duration': 40282.05206513405, 'accumulated_submission_time': 38818.21781897545, 'accumulated_eval_time': 1455.8572933673859, 'accumulated_logging_time': 3.897749662399292, 'global_step': 114225, 'preemption_count': 0}), (115730, {'train/accuracy': 0.7583107352256775, 'train/loss': 1.042015790939331, 'validation/accuracy': 0.6886799931526184, 'validation/loss': 1.344862461090088, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.0068650245666504, 'test/num_examples': 10000, 'score': 39328.428878068924, 'total_duration': 40809.460448265076, 'accumulated_submission_time': 39328.428878068924, 'accumulated_eval_time': 1472.9484844207764, 'accumulated_logging_time': 3.9506664276123047, 'global_step': 115730, 'preemption_count': 0}), (117235, {'train/accuracy': 0.7524114847183228, 'train/loss': 1.0644334554672241, 'validation/accuracy': 0.6860799789428711, 'validation/loss': 1.3524702787399292, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.0201923847198486, 'test/num_examples': 10000, 'score': 39838.38658428192, 'total_duration': 41336.782682180405, 'accumulated_submission_time': 39838.38658428192, 'accumulated_eval_time': 1490.2100987434387, 'accumulated_logging_time': 4.000177621841431, 'global_step': 117235, 'preemption_count': 0}), (118739, {'train/accuracy': 0.7526506781578064, 'train/loss': 1.0996969938278198, 'validation/accuracy': 0.6875199675559998, 'validation/loss': 1.3770368099212646, 'validation/num_examples': 50000, 'test/accuracy': 0.565500020980835, 'test/loss': 2.018036365509033, 'test/num_examples': 10000, 'score': 40348.35553407669, 'total_duration': 41864.03025341034, 'accumulated_submission_time': 40348.35553407669, 'accumulated_eval_time': 1507.3889908790588, 'accumulated_logging_time': 4.0468316078186035, 'global_step': 118739, 'preemption_count': 0}), (120244, {'train/accuracy': 0.7641701102256775, 'train/loss': 1.0034244060516357, 'validation/accuracy': 0.699999988079071, 'validation/loss': 1.2891603708267212, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.9515352249145508, 'test/num_examples': 10000, 'score': 40858.48894238472, 'total_duration': 42391.6834628582, 'accumulated_submission_time': 40858.48894238472, 'accumulated_eval_time': 1524.8025405406952, 'accumulated_logging_time': 4.097309589385986, 'global_step': 120244, 'preemption_count': 0}), (121749, {'train/accuracy': 0.7843789458274841, 'train/loss': 0.9068344831466675, 'validation/accuracy': 0.6967200040817261, 'validation/loss': 1.3004614114761353, 'validation/num_examples': 50000, 'test/accuracy': 0.5667999982833862, 'test/loss': 1.9649931192398071, 'test/num_examples': 10000, 'score': 41368.59728837013, 'total_duration': 42919.342970609665, 'accumulated_submission_time': 41368.59728837013, 'accumulated_eval_time': 1542.2404384613037, 'accumulated_logging_time': 4.155265808105469, 'global_step': 121749, 'preemption_count': 0}), (123254, {'train/accuracy': 0.78324294090271, 'train/loss': 0.9465728998184204, 'validation/accuracy': 0.7019199728965759, 'validation/loss': 1.2940689325332642, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9503682851791382, 'test/num_examples': 10000, 'score': 41878.64205765724, 'total_duration': 43446.8511133194, 'accumulated_submission_time': 41878.64205765724, 'accumulated_eval_time': 1559.6006872653961, 'accumulated_logging_time': 4.203623294830322, 'global_step': 123254, 'preemption_count': 0}), (124759, {'train/accuracy': 0.7840202450752258, 'train/loss': 0.9154542088508606, 'validation/accuracy': 0.7075799703598022, 'validation/loss': 1.2590969800949097, 'validation/num_examples': 50000, 'test/accuracy': 0.578000009059906, 'test/loss': 1.9135336875915527, 'test/num_examples': 10000, 'score': 42388.77073264122, 'total_duration': 43974.48748540878, 'accumulated_submission_time': 42388.77073264122, 'accumulated_eval_time': 1577.0038568973541, 'accumulated_logging_time': 4.252768278121948, 'global_step': 124759, 'preemption_count': 0}), (126264, {'train/accuracy': 0.7784199714660645, 'train/loss': 0.9533615708351135, 'validation/accuracy': 0.7020599842071533, 'validation/loss': 1.2897897958755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5743000507354736, 'test/loss': 1.9358104467391968, 'test/num_examples': 10000, 'score': 42898.89861416817, 'total_duration': 44502.21145987511, 'accumulated_submission_time': 42898.89861416817, 'accumulated_eval_time': 1594.5007934570312, 'accumulated_logging_time': 4.299294471740723, 'global_step': 126264, 'preemption_count': 0}), (127768, {'train/accuracy': 0.7771444320678711, 'train/loss': 0.9476613402366638, 'validation/accuracy': 0.7056399583816528, 'validation/loss': 1.2707682847976685, 'validation/num_examples': 50000, 'test/accuracy': 0.5785000324249268, 'test/loss': 1.9376139640808105, 'test/num_examples': 10000, 'score': 43408.88532400131, 'total_duration': 45029.88151431084, 'accumulated_submission_time': 43408.88532400131, 'accumulated_eval_time': 1612.0683364868164, 'accumulated_logging_time': 4.361209392547607, 'global_step': 127768, 'preemption_count': 0}), (129273, {'train/accuracy': 0.7789580225944519, 'train/loss': 0.9515725374221802, 'validation/accuracy': 0.7039799690246582, 'validation/loss': 1.283861756324768, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9563974142074585, 'test/num_examples': 10000, 'score': 43919.058978796005, 'total_duration': 45557.30363321304, 'accumulated_submission_time': 43919.058978796005, 'accumulated_eval_time': 1629.2153718471527, 'accumulated_logging_time': 4.409427642822266, 'global_step': 129273, 'preemption_count': 0}), (130777, {'train/accuracy': 0.8083147406578064, 'train/loss': 0.815697968006134, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.2330472469329834, 'validation/num_examples': 50000, 'test/accuracy': 0.5872000455856323, 'test/loss': 1.8761094808578491, 'test/num_examples': 10000, 'score': 44429.077756643295, 'total_duration': 46084.88196539879, 'accumulated_submission_time': 44429.077756643295, 'accumulated_eval_time': 1646.6670017242432, 'accumulated_logging_time': 4.463230848312378, 'global_step': 130777, 'preemption_count': 0}), (132282, {'train/accuracy': 0.8037906289100647, 'train/loss': 0.8384227156639099, 'validation/accuracy': 0.7140399813652039, 'validation/loss': 1.224155306816101, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8553305864334106, 'test/num_examples': 10000, 'score': 44939.26374220848, 'total_duration': 46612.582179546356, 'accumulated_submission_time': 44939.26374220848, 'accumulated_eval_time': 1664.0718927383423, 'accumulated_logging_time': 4.518594264984131, 'global_step': 132282, 'preemption_count': 0}), (133787, {'train/accuracy': 0.8019172549247742, 'train/loss': 0.8607712388038635, 'validation/accuracy': 0.715499997138977, 'validation/loss': 1.2324215173721313, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.8591670989990234, 'test/num_examples': 10000, 'score': 45449.338752985, 'total_duration': 47140.231604099274, 'accumulated_submission_time': 45449.338752985, 'accumulated_eval_time': 1681.5357718467712, 'accumulated_logging_time': 4.574284315109253, 'global_step': 133787, 'preemption_count': 0}), (135292, {'train/accuracy': 0.8028938174247742, 'train/loss': 0.844454824924469, 'validation/accuracy': 0.7201399803161621, 'validation/loss': 1.20967698097229, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8514662981033325, 'test/num_examples': 10000, 'score': 45959.35618376732, 'total_duration': 47667.49844503403, 'accumulated_submission_time': 45959.35618376732, 'accumulated_eval_time': 1698.6805226802826, 'accumulated_logging_time': 4.625326871871948, 'global_step': 135292, 'preemption_count': 0}), (136797, {'train/accuracy': 0.8018972873687744, 'train/loss': 0.8656412959098816, 'validation/accuracy': 0.7197399735450745, 'validation/loss': 1.2183958292007446, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.8558731079101562, 'test/num_examples': 10000, 'score': 46469.46252202988, 'total_duration': 48194.805307626724, 'accumulated_submission_time': 46469.46252202988, 'accumulated_eval_time': 1715.7776033878326, 'accumulated_logging_time': 4.675408601760864, 'global_step': 136797, 'preemption_count': 0}), (138303, {'train/accuracy': 0.8061822056770325, 'train/loss': 0.8341156244277954, 'validation/accuracy': 0.7231799960136414, 'validation/loss': 1.196844220161438, 'validation/num_examples': 50000, 'test/accuracy': 0.6022000312805176, 'test/loss': 1.8155568838119507, 'test/num_examples': 10000, 'score': 46979.67847561836, 'total_duration': 48722.46280956268, 'accumulated_submission_time': 46979.67847561836, 'accumulated_eval_time': 1733.1091315746307, 'accumulated_logging_time': 4.73111629486084, 'global_step': 138303, 'preemption_count': 0}), (139808, {'train/accuracy': 0.8362762928009033, 'train/loss': 0.7129075527191162, 'validation/accuracy': 0.7239599823951721, 'validation/loss': 1.1824004650115967, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.808569073677063, 'test/num_examples': 10000, 'score': 47489.7852473259, 'total_duration': 49249.88934969902, 'accumulated_submission_time': 47489.7852473259, 'accumulated_eval_time': 1750.324533700943, 'accumulated_logging_time': 4.7820470333099365, 'global_step': 139808, 'preemption_count': 0}), (141313, {'train/accuracy': 0.8279455900192261, 'train/loss': 0.7402110695838928, 'validation/accuracy': 0.727679967880249, 'validation/loss': 1.1678305864334106, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.8135945796966553, 'test/num_examples': 10000, 'score': 47999.80777215958, 'total_duration': 49777.51341342926, 'accumulated_submission_time': 47999.80777215958, 'accumulated_eval_time': 1767.8215091228485, 'accumulated_logging_time': 4.8334338665008545, 'global_step': 141313, 'preemption_count': 0}), (142818, {'train/accuracy': 0.8234215378761292, 'train/loss': 0.757717490196228, 'validation/accuracy': 0.729479968547821, 'validation/loss': 1.166062593460083, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.7834153175354004, 'test/num_examples': 10000, 'score': 48510.02785205841, 'total_duration': 50304.98026776314, 'accumulated_submission_time': 48510.02785205841, 'accumulated_eval_time': 1784.9574799537659, 'accumulated_logging_time': 4.888729572296143, 'global_step': 142818, 'preemption_count': 0}), (144323, {'train/accuracy': 0.832429826259613, 'train/loss': 0.7265508770942688, 'validation/accuracy': 0.7337599992752075, 'validation/loss': 1.1390953063964844, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.7489664554595947, 'test/num_examples': 10000, 'score': 49020.1178920269, 'total_duration': 50832.58455181122, 'accumulated_submission_time': 49020.1178920269, 'accumulated_eval_time': 1802.3611364364624, 'accumulated_logging_time': 4.945554733276367, 'global_step': 144323, 'preemption_count': 0}), (145828, {'train/accuracy': 0.8336455225944519, 'train/loss': 0.722388744354248, 'validation/accuracy': 0.7360000014305115, 'validation/loss': 1.13362717628479, 'validation/num_examples': 50000, 'test/accuracy': 0.6071000099182129, 'test/loss': 1.7726635932922363, 'test/num_examples': 10000, 'score': 49530.3389377594, 'total_duration': 51360.16606426239, 'accumulated_submission_time': 49530.3389377594, 'accumulated_eval_time': 1819.614022731781, 'accumulated_logging_time': 4.998095750808716, 'global_step': 145828, 'preemption_count': 0}), (147333, {'train/accuracy': 0.8375119566917419, 'train/loss': 0.7116343975067139, 'validation/accuracy': 0.7391600012779236, 'validation/loss': 1.1251534223556519, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.7439031600952148, 'test/num_examples': 10000, 'score': 50040.367628097534, 'total_duration': 51887.47683286667, 'accumulated_submission_time': 50040.367628097534, 'accumulated_eval_time': 1836.7911660671234, 'accumulated_logging_time': 5.049542188644409, 'global_step': 147333, 'preemption_count': 0}), (148837, {'train/accuracy': 0.8622449040412903, 'train/loss': 0.6133614182472229, 'validation/accuracy': 0.7362200021743774, 'validation/loss': 1.1291574239730835, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.7641743421554565, 'test/num_examples': 10000, 'score': 50550.328521966934, 'total_duration': 52415.02962350845, 'accumulated_submission_time': 50550.328521966934, 'accumulated_eval_time': 1854.272777080536, 'accumulated_logging_time': 5.106694459915161, 'global_step': 148837, 'preemption_count': 0}), (150342, {'train/accuracy': 0.8563655614852905, 'train/loss': 0.637763261795044, 'validation/accuracy': 0.7451800107955933, 'validation/loss': 1.1021682024002075, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.7354857921600342, 'test/num_examples': 10000, 'score': 51060.45466089249, 'total_duration': 52942.5445561409, 'accumulated_submission_time': 51060.45466089249, 'accumulated_eval_time': 1871.5532939434052, 'accumulated_logging_time': 5.160580635070801, 'global_step': 150342, 'preemption_count': 0}), (151846, {'train/accuracy': 0.8573222160339355, 'train/loss': 0.6447144746780396, 'validation/accuracy': 0.7456799745559692, 'validation/loss': 1.0978809595108032, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.743034839630127, 'test/num_examples': 10000, 'score': 51570.480170726776, 'total_duration': 53470.75521993637, 'accumulated_submission_time': 51570.480170726776, 'accumulated_eval_time': 1889.6330585479736, 'accumulated_logging_time': 5.211879253387451, 'global_step': 151846, 'preemption_count': 0}), (153351, {'train/accuracy': 0.8552295565605164, 'train/loss': 0.6356821060180664, 'validation/accuracy': 0.7467799782752991, 'validation/loss': 1.086501955986023, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.7114746570587158, 'test/num_examples': 10000, 'score': 52080.545378923416, 'total_duration': 53998.295038700104, 'accumulated_submission_time': 52080.545378923416, 'accumulated_eval_time': 1907.0015578269958, 'accumulated_logging_time': 5.265218734741211, 'global_step': 153351, 'preemption_count': 0}), (154855, {'train/accuracy': 0.8594945669174194, 'train/loss': 0.6123570203781128, 'validation/accuracy': 0.7492199540138245, 'validation/loss': 1.0768321752548218, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.6894590854644775, 'test/num_examples': 10000, 'score': 52590.51016449928, 'total_duration': 54525.517634391785, 'accumulated_submission_time': 52590.51016449928, 'accumulated_eval_time': 1924.1550514698029, 'accumulated_logging_time': 5.317140579223633, 'global_step': 154855, 'preemption_count': 0}), (156359, {'train/accuracy': 0.861348032951355, 'train/loss': 0.6097769141197205, 'validation/accuracy': 0.7504799962043762, 'validation/loss': 1.0702511072158813, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.6836930513381958, 'test/num_examples': 10000, 'score': 53100.41500091553, 'total_duration': 55053.556411504745, 'accumulated_submission_time': 53100.41500091553, 'accumulated_eval_time': 1942.180507183075, 'accumulated_logging_time': 5.372523307800293, 'global_step': 156359, 'preemption_count': 0}), (157863, {'train/accuracy': 0.8867984414100647, 'train/loss': 0.5364611744880676, 'validation/accuracy': 0.7528600096702576, 'validation/loss': 1.0770390033721924, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.6905221939086914, 'test/num_examples': 10000, 'score': 53610.552248716354, 'total_duration': 55581.18113279343, 'accumulated_submission_time': 53610.552248716354, 'accumulated_eval_time': 1959.5681657791138, 'accumulated_logging_time': 5.417553663253784, 'global_step': 157863, 'preemption_count': 0}), (159368, {'train/accuracy': 0.8807397484779358, 'train/loss': 0.5454962849617004, 'validation/accuracy': 0.7541199922561646, 'validation/loss': 1.0671308040618896, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.6812326908111572, 'test/num_examples': 10000, 'score': 54120.69530892372, 'total_duration': 56108.90738940239, 'accumulated_submission_time': 54120.69530892372, 'accumulated_eval_time': 1977.037132024765, 'accumulated_logging_time': 5.476731061935425, 'global_step': 159368, 'preemption_count': 0}), (160872, {'train/accuracy': 0.882254421710968, 'train/loss': 0.5351816415786743, 'validation/accuracy': 0.7583799958229065, 'validation/loss': 1.0473202466964722, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.6694732904434204, 'test/num_examples': 10000, 'score': 54630.79351758957, 'total_duration': 56636.27542257309, 'accumulated_submission_time': 54630.79351758957, 'accumulated_eval_time': 1994.1933534145355, 'accumulated_logging_time': 5.536860942840576, 'global_step': 160872, 'preemption_count': 0}), (162376, {'train/accuracy': 0.8803212642669678, 'train/loss': 0.5474730134010315, 'validation/accuracy': 0.7545599937438965, 'validation/loss': 1.0637322664260864, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.6772630214691162, 'test/num_examples': 10000, 'score': 55140.68953895569, 'total_duration': 57163.919515132904, 'accumulated_submission_time': 55140.68953895569, 'accumulated_eval_time': 2011.8303670883179, 'accumulated_logging_time': 5.594317197799683, 'global_step': 162376, 'preemption_count': 0}), (163880, {'train/accuracy': 0.8883529901504517, 'train/loss': 0.5204653143882751, 'validation/accuracy': 0.7597799897193909, 'validation/loss': 1.0434074401855469, 'validation/num_examples': 50000, 'test/accuracy': 0.6351000070571899, 'test/loss': 1.6635215282440186, 'test/num_examples': 10000, 'score': 55650.608660936356, 'total_duration': 57691.20793008804, 'accumulated_submission_time': 55650.608660936356, 'accumulated_eval_time': 2029.086805820465, 'accumulated_logging_time': 5.652773380279541, 'global_step': 163880, 'preemption_count': 0}), (165384, {'train/accuracy': 0.8879145383834839, 'train/loss': 0.5141134858131409, 'validation/accuracy': 0.7617999911308289, 'validation/loss': 1.0365240573883057, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.6468669176101685, 'test/num_examples': 10000, 'score': 56160.56981110573, 'total_duration': 58218.42898082733, 'accumulated_submission_time': 56160.56981110573, 'accumulated_eval_time': 2046.2316403388977, 'accumulated_logging_time': 5.71399450302124, 'global_step': 165384, 'preemption_count': 0}), (166888, {'train/accuracy': 0.9036790132522583, 'train/loss': 0.4611811637878418, 'validation/accuracy': 0.7634199857711792, 'validation/loss': 1.030709147453308, 'validation/num_examples': 50000, 'test/accuracy': 0.6419000029563904, 'test/loss': 1.6486574411392212, 'test/num_examples': 10000, 'score': 56670.46351933479, 'total_duration': 58745.71489930153, 'accumulated_submission_time': 56670.46351933479, 'accumulated_eval_time': 2063.513118505478, 'accumulated_logging_time': 5.7704408168792725, 'global_step': 166888, 'preemption_count': 0}), (168393, {'train/accuracy': 0.9052734375, 'train/loss': 0.4601774215698242, 'validation/accuracy': 0.7659199833869934, 'validation/loss': 1.0219361782073975, 'validation/num_examples': 50000, 'test/accuracy': 0.6416000127792358, 'test/loss': 1.6349159479141235, 'test/num_examples': 10000, 'score': 57180.640429496765, 'total_duration': 59273.3946313858, 'accumulated_submission_time': 57180.640429496765, 'accumulated_eval_time': 2080.8864829540253, 'accumulated_logging_time': 5.8460328578948975, 'global_step': 168393, 'preemption_count': 0}), (169898, {'train/accuracy': 0.9046555757522583, 'train/loss': 0.45881563425064087, 'validation/accuracy': 0.7649399638175964, 'validation/loss': 1.0206623077392578, 'validation/num_examples': 50000, 'test/accuracy': 0.6459000110626221, 'test/loss': 1.622611403465271, 'test/num_examples': 10000, 'score': 57690.83558368683, 'total_duration': 59801.218878507614, 'accumulated_submission_time': 57690.83558368683, 'accumulated_eval_time': 2098.3980734348297, 'accumulated_logging_time': 5.909879684448242, 'global_step': 169898, 'preemption_count': 0}), (171403, {'train/accuracy': 0.9075254797935486, 'train/loss': 0.4488637149333954, 'validation/accuracy': 0.7670800089836121, 'validation/loss': 1.009906530380249, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.6306302547454834, 'test/num_examples': 10000, 'score': 58201.03377509117, 'total_duration': 60329.157285928726, 'accumulated_submission_time': 58201.03377509117, 'accumulated_eval_time': 2116.0283353328705, 'accumulated_logging_time': 5.96552038192749, 'global_step': 171403, 'preemption_count': 0}), (172907, {'train/accuracy': 0.9103754758834839, 'train/loss': 0.4398549795150757, 'validation/accuracy': 0.7702599763870239, 'validation/loss': 1.0039763450622559, 'validation/num_examples': 50000, 'test/accuracy': 0.6478000283241272, 'test/loss': 1.6156542301177979, 'test/num_examples': 10000, 'score': 58711.011095523834, 'total_duration': 60856.62333703041, 'accumulated_submission_time': 58711.011095523834, 'accumulated_eval_time': 2133.4003245830536, 'accumulated_logging_time': 6.02742600440979, 'global_step': 172907, 'preemption_count': 0}), (174412, {'train/accuracy': 0.9120894074440002, 'train/loss': 0.4342247247695923, 'validation/accuracy': 0.7703799605369568, 'validation/loss': 1.0012692213058472, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6078853607177734, 'test/num_examples': 10000, 'score': 59221.15896511078, 'total_duration': 61384.15951418877, 'accumulated_submission_time': 59221.15896511078, 'accumulated_eval_time': 2150.6742584705353, 'accumulated_logging_time': 6.087927341461182, 'global_step': 174412, 'preemption_count': 0}), (175916, {'train/accuracy': 0.9157963991165161, 'train/loss': 0.4226926863193512, 'validation/accuracy': 0.7699999809265137, 'validation/loss': 1.0016485452651978, 'validation/num_examples': 50000, 'test/accuracy': 0.6498000025749207, 'test/loss': 1.614419937133789, 'test/num_examples': 10000, 'score': 59731.146606206894, 'total_duration': 61911.43213367462, 'accumulated_submission_time': 59731.146606206894, 'accumulated_eval_time': 2167.850204706192, 'accumulated_logging_time': 6.143632411956787, 'global_step': 175916, 'preemption_count': 0}), (177420, {'train/accuracy': 0.9207788109779358, 'train/loss': 0.40458759665489197, 'validation/accuracy': 0.7719599604606628, 'validation/loss': 0.9974290132522583, 'validation/num_examples': 50000, 'test/accuracy': 0.6514000296592712, 'test/loss': 1.6060065031051636, 'test/num_examples': 10000, 'score': 60241.354083538055, 'total_duration': 62439.29220581055, 'accumulated_submission_time': 60241.354083538055, 'accumulated_eval_time': 2185.3897194862366, 'accumulated_logging_time': 6.202255487442017, 'global_step': 177420, 'preemption_count': 0}), (178924, {'train/accuracy': 0.9169324040412903, 'train/loss': 0.40575864911079407, 'validation/accuracy': 0.7729399800300598, 'validation/loss': 0.9932107329368591, 'validation/num_examples': 50000, 'test/accuracy': 0.6512000560760498, 'test/loss': 1.60286545753479, 'test/num_examples': 10000, 'score': 60751.29431128502, 'total_duration': 62966.76009917259, 'accumulated_submission_time': 60751.29431128502, 'accumulated_eval_time': 2202.798728942871, 'accumulated_logging_time': 6.266868829727173, 'global_step': 178924, 'preemption_count': 0}), (180429, {'train/accuracy': 0.9206991195678711, 'train/loss': 0.40181708335876465, 'validation/accuracy': 0.7720800042152405, 'validation/loss': 0.9955241084098816, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.6055456399917603, 'test/num_examples': 10000, 'score': 61261.49818825722, 'total_duration': 63494.355257987976, 'accumulated_submission_time': 61261.49818825722, 'accumulated_eval_time': 2220.0592410564423, 'accumulated_logging_time': 6.343884229660034, 'global_step': 180429, 'preemption_count': 0}), (181934, {'train/accuracy': 0.91898512840271, 'train/loss': 0.40230587124824524, 'validation/accuracy': 0.7721999883651733, 'validation/loss': 0.9920402765274048, 'validation/num_examples': 50000, 'test/accuracy': 0.653700053691864, 'test/loss': 1.6013190746307373, 'test/num_examples': 10000, 'score': 61771.65120244026, 'total_duration': 64021.787281513214, 'accumulated_submission_time': 61771.65120244026, 'accumulated_eval_time': 2237.2278864383698, 'accumulated_logging_time': 6.401177883148193, 'global_step': 181934, 'preemption_count': 0}), (183438, {'train/accuracy': 0.9219945669174194, 'train/loss': 0.39710769057273865, 'validation/accuracy': 0.772819995880127, 'validation/loss': 0.9916077852249146, 'validation/num_examples': 50000, 'test/accuracy': 0.6526000499725342, 'test/loss': 1.6008130311965942, 'test/num_examples': 10000, 'score': 62281.68023562431, 'total_duration': 64549.20379114151, 'accumulated_submission_time': 62281.68023562431, 'accumulated_eval_time': 2254.5007655620575, 'accumulated_logging_time': 6.462839603424072, 'global_step': 183438, 'preemption_count': 0}), (184942, {'train/accuracy': 0.9215561151504517, 'train/loss': 0.3981364667415619, 'validation/accuracy': 0.7727400064468384, 'validation/loss': 0.9910697937011719, 'validation/num_examples': 50000, 'test/accuracy': 0.653700053691864, 'test/loss': 1.5995631217956543, 'test/num_examples': 10000, 'score': 62791.72662734985, 'total_duration': 65076.893508434296, 'accumulated_submission_time': 62791.72662734985, 'accumulated_eval_time': 2272.0287766456604, 'accumulated_logging_time': 6.52376127243042, 'global_step': 184942, 'preemption_count': 0})], 'global_step': 185581}
I0127 13:11:50.794269 140187804313408 submission_runner.py:586] Timing: 63008.197350502014
I0127 13:11:50.794340 140187804313408 submission_runner.py:588] Total number of evals: 124
I0127 13:11:50.794398 140187804313408 submission_runner.py:589] ====================
I0127 13:11:50.794442 140187804313408 submission_runner.py:542] Using RNG seed 3827130657
I0127 13:11:50.795781 140187804313408 submission_runner.py:551] --- Tuning run 2/5 ---
I0127 13:11:50.795892 140187804313408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_2.
I0127 13:11:50.801200 140187804313408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_2/hparams.json.
I0127 13:11:50.802704 140187804313408 submission_runner.py:206] Initializing dataset.
I0127 13:11:50.813787 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0127 13:11:50.823498 140187804313408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0127 13:11:51.018524 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0127 13:11:51.305555 140187804313408 submission_runner.py:213] Initializing model.
I0127 13:11:56.506302 140187804313408 submission_runner.py:255] Initializing optimizer.
I0127 13:11:56.882634 140187804313408 submission_runner.py:262] Initializing metrics bundle.
I0127 13:11:56.882783 140187804313408 submission_runner.py:280] Initializing checkpoint and logger.
I0127 13:11:56.979866 140187804313408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0127 13:11:56.979999 140187804313408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0127 13:12:08.102212 140187804313408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0127 13:12:18.693867 140187804313408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_2/flags_0.json.
I0127 13:12:18.704743 140187804313408 submission_runner.py:314] Starting training loop.
I0127 13:12:50.591364 140026042091264 logging_writer.py:48] [0] global_step=0, grad_norm=0.528319776058197, loss=6.932884693145752
I0127 13:12:50.600686 140187804313408 spec.py:321] Evaluating on the training split.
I0127 13:12:56.710736 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 13:13:05.896981 140187804313408 spec.py:349] Evaluating on the test split.
I0127 13:13:08.983491 140187804313408 submission_runner.py:408] Time since start: 50.28s, 	Step: 1, 	{'train/accuracy': 0.000717474496923387, 'train/loss': 6.912591457366943, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 31.895853757858276, 'total_duration': 50.27870035171509, 'accumulated_submission_time': 31.895853757858276, 'accumulated_eval_time': 18.382753133773804, 'accumulated_logging_time': 0}
I0127 13:13:08.991799 140026159523584 logging_writer.py:48] [1] accumulated_eval_time=18.382753, accumulated_logging_time=0, accumulated_submission_time=31.895854, global_step=1, preemption_count=0, score=31.895854, test/accuracy=0.000600, test/loss=6.912549, test/num_examples=10000, total_duration=50.278700, train/accuracy=0.000717, train/loss=6.912591, validation/accuracy=0.000760, validation/loss=6.913175, validation/num_examples=50000
I0127 13:13:42.995996 140026167916288 logging_writer.py:48] [100] global_step=100, grad_norm=0.5241795778274536, loss=6.898794174194336
I0127 13:14:17.052243 140026159523584 logging_writer.py:48] [200] global_step=200, grad_norm=0.5220614671707153, loss=6.86052131652832
I0127 13:14:51.164623 140026167916288 logging_writer.py:48] [300] global_step=300, grad_norm=0.5741219520568848, loss=6.78878116607666
I0127 13:15:25.266124 140026159523584 logging_writer.py:48] [400] global_step=400, grad_norm=0.6246461272239685, loss=6.679013252258301
I0127 13:15:59.378293 140026167916288 logging_writer.py:48] [500] global_step=500, grad_norm=0.6514214277267456, loss=6.5994415283203125
I0127 13:16:33.484545 140026159523584 logging_writer.py:48] [600] global_step=600, grad_norm=0.6975856423377991, loss=6.5123138427734375
I0127 13:17:07.631464 140026167916288 logging_writer.py:48] [700] global_step=700, grad_norm=0.75660640001297, loss=6.461298942565918
I0127 13:17:41.756687 140026159523584 logging_writer.py:48] [800] global_step=800, grad_norm=1.1357389688491821, loss=6.382108688354492
I0127 13:18:15.877370 140026167916288 logging_writer.py:48] [900] global_step=900, grad_norm=2.1312174797058105, loss=6.248532295227051
I0127 13:18:50.019848 140026159523584 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9387649297714233, loss=6.214826583862305
I0127 13:19:24.201864 140026167916288 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.9553582668304443, loss=6.104237079620361
I0127 13:19:58.331326 140026159523584 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.3821091651916504, loss=6.080210208892822
I0127 13:20:32.444193 140026167916288 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.890124797821045, loss=6.016916751861572
I0127 13:21:06.556679 140026159523584 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.6460950374603271, loss=5.922524452209473
I0127 13:21:39.113892 140187804313408 spec.py:321] Evaluating on the training split.
I0127 13:21:45.265390 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 13:21:54.390068 140187804313408 spec.py:349] Evaluating on the test split.
I0127 13:21:56.865482 140187804313408 submission_runner.py:408] Time since start: 578.16s, 	Step: 1497, 	{'train/accuracy': 0.08177216351032257, 'train/loss': 5.256331920623779, 'validation/accuracy': 0.07620000094175339, 'validation/loss': 5.3086628913879395, 'validation/num_examples': 50000, 'test/accuracy': 0.053700003772974014, 'test/loss': 5.532280921936035, 'test/num_examples': 10000, 'score': 541.9562501907349, 'total_duration': 578.1606819629669, 'accumulated_submission_time': 541.9562501907349, 'accumulated_eval_time': 36.13430953025818, 'accumulated_logging_time': 0.016445159912109375}
I0127 13:21:56.883530 140026067269376 logging_writer.py:48] [1497] accumulated_eval_time=36.134310, accumulated_logging_time=0.016445, accumulated_submission_time=541.956250, global_step=1497, preemption_count=0, score=541.956250, test/accuracy=0.053700, test/loss=5.532281, test/num_examples=10000, total_duration=578.160682, train/accuracy=0.081772, train/loss=5.256332, validation/accuracy=0.076200, validation/loss=5.308663, validation/num_examples=50000
I0127 13:21:58.245353 140026075662080 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.8909598588943481, loss=5.969140529632568
I0127 13:22:32.334532 140026067269376 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.170696973800659, loss=5.82189416885376
I0127 13:23:06.426641 140026075662080 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.818814277648926, loss=5.800589084625244
I0127 13:23:40.556180 140026067269376 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.7524683475494385, loss=5.750079154968262
I0127 13:24:14.698303 140026075662080 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.6154565811157227, loss=5.741131782531738
I0127 13:24:48.808965 140026067269376 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.764587640762329, loss=5.719236373901367
I0127 13:25:22.996722 140026075662080 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.2502007484436035, loss=5.672548294067383
I0127 13:25:57.140196 140026067269376 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.384165048599243, loss=5.608825206756592
I0127 13:26:31.278166 140026075662080 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.6330208778381348, loss=5.540916442871094
I0127 13:27:05.397218 140026067269376 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.774282693862915, loss=5.576455116271973
I0127 13:27:39.502554 140026075662080 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.6329433917999268, loss=5.463418960571289
I0127 13:28:13.648387 140026067269376 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.530649662017822, loss=5.411351203918457
I0127 13:28:47.769222 140026075662080 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.019565105438232, loss=5.420402526855469
I0127 13:29:21.900022 140026067269376 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.3124258518218994, loss=5.35751485824585
I0127 13:29:56.053865 140026075662080 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.2198221683502197, loss=5.294589042663574
I0127 13:30:26.925754 140187804313408 spec.py:321] Evaluating on the training split.
I0127 13:30:33.065849 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 13:30:42.030200 140187804313408 spec.py:349] Evaluating on the test split.
I0127 13:30:44.494709 140187804313408 submission_runner.py:408] Time since start: 1105.79s, 	Step: 2992, 	{'train/accuracy': 0.1966477930545807, 'train/loss': 4.2057881355285645, 'validation/accuracy': 0.17455999553203583, 'validation/loss': 4.332620143890381, 'validation/num_examples': 50000, 'test/accuracy': 0.1266000121831894, 'test/loss': 4.754936218261719, 'test/num_examples': 10000, 'score': 1051.935497045517, 'total_duration': 1105.7899084091187, 'accumulated_submission_time': 1051.935497045517, 'accumulated_eval_time': 53.70322799682617, 'accumulated_logging_time': 0.0438079833984375}
I0127 13:30:44.512585 140026050483968 logging_writer.py:48] [2992] accumulated_eval_time=53.703228, accumulated_logging_time=0.043808, accumulated_submission_time=1051.935497, global_step=2992, preemption_count=0, score=1051.935497, test/accuracy=0.126600, test/loss=4.754936, test/num_examples=10000, total_duration=1105.789908, train/accuracy=0.196648, train/loss=4.205788, validation/accuracy=0.174560, validation/loss=4.332620, validation/num_examples=50000
I0127 13:30:47.591142 140026058876672 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.8637616634368896, loss=5.247735023498535
I0127 13:31:21.627532 140026050483968 logging_writer.py:48] [3100] global_step=3100, grad_norm=8.55814266204834, loss=5.257143020629883
I0127 13:31:55.787591 140026058876672 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.221285343170166, loss=5.21994686126709
I0127 13:32:29.895421 140026050483968 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.067183017730713, loss=5.144191741943359
I0127 13:33:04.049047 140026058876672 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.274617671966553, loss=5.142978668212891
I0127 13:33:38.172299 140026050483968 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.661178112030029, loss=5.1173319816589355
I0127 13:34:12.269522 140026058876672 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.7771174907684326, loss=5.06865930557251
I0127 13:34:46.419179 140026050483968 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.325969696044922, loss=5.00622034072876
I0127 13:35:20.527371 140026058876672 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.326009750366211, loss=4.920019149780273
I0127 13:35:54.655021 140026050483968 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.05522346496582, loss=4.916718482971191
I0127 13:36:28.771185 140026058876672 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.604520559310913, loss=4.955380439758301
I0127 13:37:02.903748 140026050483968 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.89668345451355, loss=4.908586502075195
I0127 13:37:36.998308 140026058876672 logging_writer.py:48] [4200] global_step=4200, grad_norm=6.316125392913818, loss=4.884484767913818
I0127 13:38:11.178778 140026050483968 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.1577467918396, loss=4.752101898193359
I0127 13:38:45.296448 140026058876672 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.5730385780334473, loss=4.7495574951171875
I0127 13:39:14.796153 140187804313408 spec.py:321] Evaluating on the training split.
I0127 13:39:21.814134 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 13:39:30.966054 140187804313408 spec.py:349] Evaluating on the test split.
I0127 13:39:33.470994 140187804313408 submission_runner.py:408] Time since start: 1634.77s, 	Step: 4488, 	{'train/accuracy': 0.3034917116165161, 'train/loss': 3.4856112003326416, 'validation/accuracy': 0.2791000008583069, 'validation/loss': 3.6248672008514404, 'validation/num_examples': 50000, 'test/accuracy': 0.21010001003742218, 'test/loss': 4.122847080230713, 'test/num_examples': 10000, 'score': 1562.1542949676514, 'total_duration': 1634.7661957740784, 'accumulated_submission_time': 1562.1542949676514, 'accumulated_eval_time': 72.37804913520813, 'accumulated_logging_time': 0.0735633373260498}
I0127 13:39:33.489089 140026075662080 logging_writer.py:48] [4488] accumulated_eval_time=72.378049, accumulated_logging_time=0.073563, accumulated_submission_time=1562.154295, global_step=4488, preemption_count=0, score=1562.154295, test/accuracy=0.210100, test/loss=4.122847, test/num_examples=10000, total_duration=1634.766196, train/accuracy=0.303492, train/loss=3.485611, validation/accuracy=0.279100, validation/loss=3.624867, validation/num_examples=50000
I0127 13:39:37.912235 140026151130880 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.087322950363159, loss=4.7327961921691895
I0127 13:40:11.980356 140026075662080 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.328941822052002, loss=4.718865871429443
I0127 13:40:46.079633 140026151130880 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.607062339782715, loss=4.696554660797119
I0127 13:41:20.170419 140026075662080 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.184985399246216, loss=4.646707534790039
I0127 13:41:54.284525 140026151130880 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.347687244415283, loss=4.650572776794434
I0127 13:42:28.361457 140026075662080 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.78317928314209, loss=4.54810094833374
I0127 13:43:02.482821 140026151130880 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.2844114303588867, loss=4.576379299163818
I0127 13:43:36.556635 140026075662080 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.253878355026245, loss=4.568454742431641
I0127 13:44:10.664575 140026151130880 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.930448055267334, loss=4.633492946624756
I0127 13:44:44.841765 140026075662080 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.1911396980285645, loss=4.4748992919921875
I0127 13:45:18.919178 140026151130880 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.87868070602417, loss=4.470242500305176
I0127 13:45:53.050793 140026075662080 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.305947303771973, loss=4.50087833404541
I0127 13:46:27.170152 140026151130880 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.5864675045013428, loss=4.385486602783203
I0127 13:47:01.265042 140026075662080 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.233274221420288, loss=4.488038063049316
I0127 13:47:35.380731 140026151130880 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.5284194946289062, loss=4.463169097900391
I0127 13:48:03.506391 140187804313408 spec.py:321] Evaluating on the training split.
I0127 13:48:09.635739 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 13:48:18.777717 140187804313408 spec.py:349] Evaluating on the test split.
I0127 13:48:21.264583 140187804313408 submission_runner.py:408] Time since start: 2162.56s, 	Step: 5984, 	{'train/accuracy': 0.3869180381298065, 'train/loss': 2.9858076572418213, 'validation/accuracy': 0.35905998945236206, 'validation/loss': 3.1390581130981445, 'validation/num_examples': 50000, 'test/accuracy': 0.27320000529289246, 'test/loss': 3.6855287551879883, 'test/num_examples': 10000, 'score': 2072.106372833252, 'total_duration': 2162.559784412384, 'accumulated_submission_time': 2072.106372833252, 'accumulated_eval_time': 90.1362087726593, 'accumulated_logging_time': 0.10370731353759766}
I0127 13:48:21.283805 140026058876672 logging_writer.py:48] [5984] accumulated_eval_time=90.136209, accumulated_logging_time=0.103707, accumulated_submission_time=2072.106373, global_step=5984, preemption_count=0, score=2072.106373, test/accuracy=0.273200, test/loss=3.685529, test/num_examples=10000, total_duration=2162.559784, train/accuracy=0.386918, train/loss=2.985808, validation/accuracy=0.359060, validation/loss=3.139058, validation/num_examples=50000
I0127 13:48:27.118604 140026067269376 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.214195728302002, loss=4.391487121582031
I0127 13:49:01.180632 140026058876672 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.9301350116729736, loss=4.442463397979736
I0127 13:49:35.265401 140026067269376 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.949838638305664, loss=4.307802200317383
I0127 13:50:09.382852 140026058876672 logging_writer.py:48] [6300] global_step=6300, grad_norm=4.605006694793701, loss=4.337802886962891
I0127 13:50:43.552380 140026067269376 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.693526029586792, loss=4.308502197265625
I0127 13:51:17.664015 140026058876672 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.2950549125671387, loss=4.239625930786133
I0127 13:51:51.765375 140026067269376 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.5005908012390137, loss=4.330874443054199
I0127 13:52:25.876255 140026058876672 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.491379499435425, loss=4.20457649230957
I0127 13:52:59.988255 140026067269376 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.1095755100250244, loss=4.250725746154785
I0127 13:53:34.076398 140026058876672 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.6484313011169434, loss=4.168822288513184
I0127 13:54:08.179872 140026067269376 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.0650317668914795, loss=4.140712738037109
I0127 13:54:42.300000 140026058876672 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.5896360874176025, loss=4.1813459396362305
I0127 13:55:16.397236 140026067269376 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.9015603065490723, loss=4.231634616851807
I0127 13:55:50.504798 140026058876672 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.7121224403381348, loss=4.151163101196289
I0127 13:56:24.606541 140026067269376 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.2345614433288574, loss=4.105162620544434
I0127 13:56:51.408279 140187804313408 spec.py:321] Evaluating on the training split.
I0127 13:56:57.587075 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 13:57:06.761358 140187804313408 spec.py:349] Evaluating on the test split.
I0127 13:57:09.269702 140187804313408 submission_runner.py:408] Time since start: 2690.56s, 	Step: 7480, 	{'train/accuracy': 0.4513911008834839, 'train/loss': 2.6858718395233154, 'validation/accuracy': 0.42056000232696533, 'validation/loss': 2.84183406829834, 'validation/num_examples': 50000, 'test/accuracy': 0.3192000091075897, 'test/loss': 3.4499173164367676, 'test/num_examples': 10000, 'score': 2582.1680810451508, 'total_duration': 2690.564907312393, 'accumulated_submission_time': 2582.1680810451508, 'accumulated_eval_time': 107.99761819839478, 'accumulated_logging_time': 0.1325531005859375}
I0127 13:57:09.288428 140026167916288 logging_writer.py:48] [7480] accumulated_eval_time=107.997618, accumulated_logging_time=0.132553, accumulated_submission_time=2582.168081, global_step=7480, preemption_count=0, score=2582.168081, test/accuracy=0.319200, test/loss=3.449917, test/num_examples=10000, total_duration=2690.564907, train/accuracy=0.451391, train/loss=2.685872, validation/accuracy=0.420560, validation/loss=2.841834, validation/num_examples=50000
I0127 13:57:16.467507 140026176308992 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.1609959602355957, loss=4.120774745941162
I0127 13:57:50.490483 140026167916288 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.098470687866211, loss=4.079585552215576
I0127 13:58:24.573328 140026176308992 logging_writer.py:48] [7700] global_step=7700, grad_norm=4.570359706878662, loss=4.2041707038879395
I0127 13:58:58.685262 140026167916288 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.3721394538879395, loss=4.073256492614746
I0127 13:59:32.821228 140026176308992 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.3430888652801514, loss=4.175673007965088
I0127 14:00:06.930599 140026167916288 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.587465524673462, loss=4.057613372802734
I0127 14:00:41.026372 140026176308992 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.819238305091858, loss=4.020874500274658
I0127 14:01:15.133341 140026167916288 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.565823554992676, loss=4.021092414855957
I0127 14:01:49.198466 140026176308992 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5638773441314697, loss=4.07181453704834
I0127 14:02:23.294112 140026167916288 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.42848801612854, loss=3.9846200942993164
I0127 14:02:57.428726 140026176308992 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.856253743171692, loss=3.980449676513672
I0127 14:03:31.511803 140026167916288 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.5700817108154297, loss=4.06644868850708
I0127 14:04:05.597417 140026176308992 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.9473098516464233, loss=4.000454425811768
I0127 14:04:39.678895 140026167916288 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.8312046527862549, loss=3.9638173580169678
I0127 14:05:13.766112 140026176308992 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.812239170074463, loss=3.95377254486084
I0127 14:05:39.498543 140187804313408 spec.py:321] Evaluating on the training split.
I0127 14:05:45.603050 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 14:05:54.710382 140187804313408 spec.py:349] Evaluating on the test split.
I0127 14:05:57.187942 140187804313408 submission_runner.py:408] Time since start: 3218.48s, 	Step: 8977, 	{'train/accuracy': 0.5364516973495483, 'train/loss': 2.2150745391845703, 'validation/accuracy': 0.47185999155044556, 'validation/loss': 2.5311429500579834, 'validation/num_examples': 50000, 'test/accuracy': 0.36250001192092896, 'test/loss': 3.1810948848724365, 'test/num_examples': 10000, 'score': 3092.3153219223022, 'total_duration': 3218.4831438064575, 'accumulated_submission_time': 3092.3153219223022, 'accumulated_eval_time': 125.68698191642761, 'accumulated_logging_time': 0.1603870391845703}
I0127 14:05:57.207370 140026050483968 logging_writer.py:48] [8977] accumulated_eval_time=125.686982, accumulated_logging_time=0.160387, accumulated_submission_time=3092.315322, global_step=8977, preemption_count=0, score=3092.315322, test/accuracy=0.362500, test/loss=3.181095, test/num_examples=10000, total_duration=3218.483144, train/accuracy=0.536452, train/loss=2.215075, validation/accuracy=0.471860, validation/loss=2.531143, validation/num_examples=50000
I0127 14:06:05.379601 140026058876672 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.346616506576538, loss=3.941434383392334
I0127 14:06:39.393671 140026050483968 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.2409563064575195, loss=3.9261999130249023
I0127 14:07:13.454318 140026058876672 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.838899850845337, loss=4.000458240509033
I0127 14:07:47.548803 140026050483968 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.041715145111084, loss=3.8756844997406006
I0127 14:08:21.653484 140026058876672 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.5272674560546875, loss=3.908356189727783
I0127 14:08:55.741427 140026050483968 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.4844138622283936, loss=3.9661366939544678
I0127 14:09:30.000826 140026058876672 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.1827287673950195, loss=3.8471078872680664
I0127 14:10:04.065724 140026050483968 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.023019313812256, loss=3.976534843444824
I0127 14:10:38.164978 140026058876672 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.6997830867767334, loss=3.840210437774658
I0127 14:11:12.231167 140026050483968 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.5579710006713867, loss=3.8878448009490967
I0127 14:11:46.351301 140026058876672 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.743485689163208, loss=3.779324769973755
I0127 14:12:20.443638 140026050483968 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.6489272117614746, loss=3.968055248260498
I0127 14:12:54.549383 140026058876672 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.7710120677947998, loss=3.940633535385132
I0127 14:13:28.602267 140026050483968 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.590634822845459, loss=3.765132427215576
I0127 14:14:02.687874 140026058876672 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.9730321168899536, loss=3.8408830165863037
I0127 14:14:27.361817 140187804313408 spec.py:321] Evaluating on the training split.
I0127 14:14:33.480722 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 14:14:42.487051 140187804313408 spec.py:349] Evaluating on the test split.
I0127 14:14:44.951967 140187804313408 submission_runner.py:408] Time since start: 3746.25s, 	Step: 10474, 	{'train/accuracy': 0.5558035373687744, 'train/loss': 2.1703338623046875, 'validation/accuracy': 0.5116599798202515, 'validation/loss': 2.3818368911743164, 'validation/num_examples': 50000, 'test/accuracy': 0.39580002427101135, 'test/loss': 3.038412094116211, 'test/num_examples': 10000, 'score': 3602.40500998497, 'total_duration': 3746.2471656799316, 'accumulated_submission_time': 3602.40500998497, 'accumulated_eval_time': 143.27709293365479, 'accumulated_logging_time': 0.19082403182983398}
I0127 14:14:44.971852 140026151130880 logging_writer.py:48] [10474] accumulated_eval_time=143.277093, accumulated_logging_time=0.190824, accumulated_submission_time=3602.405010, global_step=10474, preemption_count=0, score=3602.405010, test/accuracy=0.395800, test/loss=3.038412, test/num_examples=10000, total_duration=3746.247166, train/accuracy=0.555804, train/loss=2.170334, validation/accuracy=0.511660, validation/loss=2.381837, validation/num_examples=50000
I0127 14:14:54.169975 140026159523584 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.455695629119873, loss=3.773969888687134
I0127 14:15:28.319952 140026151130880 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.978498935699463, loss=3.694809913635254
I0127 14:16:02.365036 140026159523584 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.6310514211654663, loss=3.80808162689209
I0127 14:16:36.430196 140026151130880 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.060206651687622, loss=3.794353485107422
I0127 14:17:10.499712 140026159523584 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.017721176147461, loss=3.8169033527374268
I0127 14:17:44.588340 140026151130880 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.8241372108459473, loss=3.821465492248535
I0127 14:18:18.624763 140026159523584 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.7370954751968384, loss=3.677622079849243
I0127 14:18:52.712565 140026151130880 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.6415027379989624, loss=3.7186808586120605
I0127 14:19:26.786923 140026159523584 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.3631062507629395, loss=3.752532482147217
I0127 14:20:00.827089 140026151130880 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.674670934677124, loss=3.6565704345703125
I0127 14:20:34.890637 140026159523584 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.3634188175201416, loss=3.689393997192383
I0127 14:21:08.950241 140026151130880 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.4864968061447144, loss=3.6192355155944824
I0127 14:21:43.081274 140026159523584 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.1653130054473877, loss=3.6953630447387695
I0127 14:22:17.119591 140026151130880 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.8907957077026367, loss=3.671370029449463
I0127 14:22:51.186062 140026159523584 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.6643680334091187, loss=3.6835436820983887
I0127 14:23:15.194084 140187804313408 spec.py:321] Evaluating on the training split.
I0127 14:23:21.396446 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 14:23:30.523267 140187804313408 spec.py:349] Evaluating on the test split.
I0127 14:23:32.975283 140187804313408 submission_runner.py:408] Time since start: 4274.27s, 	Step: 11972, 	{'train/accuracy': 0.5846021771430969, 'train/loss': 1.9957231283187866, 'validation/accuracy': 0.5405799746513367, 'validation/loss': 2.2037601470947266, 'validation/num_examples': 50000, 'test/accuracy': 0.42500001192092896, 'test/loss': 2.862744092941284, 'test/num_examples': 10000, 'score': 4112.561676979065, 'total_duration': 4274.270484447479, 'accumulated_submission_time': 4112.561676979065, 'accumulated_eval_time': 161.05826830863953, 'accumulated_logging_time': 0.22270441055297852}
I0127 14:23:32.996937 140026067269376 logging_writer.py:48] [11972] accumulated_eval_time=161.058268, accumulated_logging_time=0.222704, accumulated_submission_time=4112.561677, global_step=11972, preemption_count=0, score=4112.561677, test/accuracy=0.425000, test/loss=2.862744, test/num_examples=10000, total_duration=4274.270484, train/accuracy=0.584602, train/loss=1.995723, validation/accuracy=0.540580, validation/loss=2.203760, validation/num_examples=50000
I0127 14:23:42.900436 140026075662080 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.8608258962631226, loss=3.7448625564575195
I0127 14:24:16.904312 140026067269376 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.5861800909042358, loss=3.6528005599975586
I0127 14:24:50.954586 140026075662080 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.772911787033081, loss=3.6995716094970703
I0127 14:25:25.032289 140026067269376 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.8914872407913208, loss=3.678420305252075
I0127 14:25:59.094422 140026075662080 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.454040765762329, loss=3.5998716354370117
I0127 14:26:33.157065 140026067269376 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.7541232109069824, loss=3.6325669288635254
I0127 14:27:07.214892 140026075662080 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.8973942995071411, loss=3.6192586421966553
I0127 14:27:41.277729 140026067269376 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.295877695083618, loss=3.5593910217285156
I0127 14:28:15.603750 140026075662080 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.9402440786361694, loss=3.6550705432891846
I0127 14:28:49.673820 140026067269376 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.830741047859192, loss=3.694415330886841
I0127 14:29:23.760782 140026075662080 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.5203447341918945, loss=3.5880625247955322
I0127 14:29:57.839710 140026067269376 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.5295982360839844, loss=3.6502838134765625
I0127 14:30:31.907284 140026075662080 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.7963171005249023, loss=3.59414005279541
I0127 14:31:05.984197 140026067269376 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4951612949371338, loss=3.61802339553833
I0127 14:31:40.020545 140026075662080 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.6218650341033936, loss=3.6248579025268555
I0127 14:32:02.984966 140187804313408 spec.py:321] Evaluating on the training split.
I0127 14:32:09.117159 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 14:32:17.834634 140187804313408 spec.py:349] Evaluating on the test split.
I0127 14:32:20.327759 140187804313408 submission_runner.py:408] Time since start: 4801.62s, 	Step: 13469, 	{'train/accuracy': 0.6219108700752258, 'train/loss': 1.801672101020813, 'validation/accuracy': 0.5726400017738342, 'validation/loss': 2.0319325923919678, 'validation/num_examples': 50000, 'test/accuracy': 0.4506000280380249, 'test/loss': 2.675619602203369, 'test/num_examples': 10000, 'score': 4622.485732078552, 'total_duration': 4801.6229565143585, 'accumulated_submission_time': 4622.485732078552, 'accumulated_eval_time': 178.40104007720947, 'accumulated_logging_time': 0.25369954109191895}
I0127 14:32:20.347648 140026058876672 logging_writer.py:48] [13469] accumulated_eval_time=178.401040, accumulated_logging_time=0.253700, accumulated_submission_time=4622.485732, global_step=13469, preemption_count=0, score=4622.485732, test/accuracy=0.450600, test/loss=2.675620, test/num_examples=10000, total_duration=4801.622957, train/accuracy=0.621911, train/loss=1.801672, validation/accuracy=0.572640, validation/loss=2.031933, validation/num_examples=50000
I0127 14:32:31.244339 140026067269376 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.8349671363830566, loss=3.587839365005493
I0127 14:33:05.204590 140026058876672 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.8539763689041138, loss=3.543273687362671
I0127 14:33:39.216754 140026067269376 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.4802796840667725, loss=3.5894103050231934
I0127 14:34:13.250461 140026058876672 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.6011322736740112, loss=3.5369789600372314
I0127 14:34:47.414038 140026067269376 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.5519869327545166, loss=3.5588297843933105
I0127 14:35:21.432602 140026058876672 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.0103859901428223, loss=3.5767157077789307
I0127 14:35:55.464126 140026067269376 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5861363410949707, loss=3.576796531677246
I0127 14:36:29.504366 140026058876672 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.075401782989502, loss=3.6057305335998535
I0127 14:37:03.539397 140026067269376 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.6665641069412231, loss=3.540797233581543
I0127 14:37:37.564453 140026058876672 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.6555678844451904, loss=3.6090142726898193
I0127 14:38:11.612326 140026067269376 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.6084344387054443, loss=3.598261594772339
I0127 14:38:45.640825 140026058876672 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.3602919578552246, loss=3.518000841140747
I0127 14:39:19.650464 140026067269376 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.341962218284607, loss=3.4775524139404297
I0127 14:39:53.695191 140026058876672 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.549164891242981, loss=3.473191261291504
I0127 14:40:27.726031 140026067269376 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.4666903018951416, loss=3.4801673889160156
I0127 14:40:50.399776 140187804313408 spec.py:321] Evaluating on the training split.
I0127 14:40:56.508552 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 14:41:05.600960 140187804313408 spec.py:349] Evaluating on the test split.
I0127 14:41:08.330226 140187804313408 submission_runner.py:408] Time since start: 5329.63s, 	Step: 14968, 	{'train/accuracy': 0.6355029940605164, 'train/loss': 1.6827951669692993, 'validation/accuracy': 0.5859599709510803, 'validation/loss': 1.926688551902771, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.591858148574829, 'test/num_examples': 10000, 'score': 5132.471919298172, 'total_duration': 5329.625428676605, 'accumulated_submission_time': 5132.471919298172, 'accumulated_eval_time': 196.33145880699158, 'accumulated_logging_time': 0.285891056060791}
I0127 14:41:08.346734 140026058876672 logging_writer.py:48] [14968] accumulated_eval_time=196.331459, accumulated_logging_time=0.285891, accumulated_submission_time=5132.471919, global_step=14968, preemption_count=0, score=5132.471919, test/accuracy=0.461700, test/loss=2.591858, test/num_examples=10000, total_duration=5329.625429, train/accuracy=0.635503, train/loss=1.682795, validation/accuracy=0.585960, validation/loss=1.926689, validation/num_examples=50000
I0127 14:41:19.598663 140026151130880 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.789124608039856, loss=3.5604090690612793
I0127 14:41:53.591666 140026058876672 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.7020175457000732, loss=3.4943273067474365
I0127 14:42:27.609603 140026151130880 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.356909155845642, loss=3.4911391735076904
I0127 14:43:01.636010 140026058876672 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.699637532234192, loss=3.525991439819336
I0127 14:43:35.716146 140026151130880 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.444597840309143, loss=3.4429116249084473
I0127 14:44:09.759052 140026058876672 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.042900323867798, loss=3.52060866355896
I0127 14:44:43.806946 140026151130880 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.5598063468933105, loss=3.5055627822875977
I0127 14:45:17.859090 140026058876672 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.554519534111023, loss=3.5115654468536377
I0127 14:45:51.882243 140026151130880 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.4297016859054565, loss=3.4601378440856934
I0127 14:46:25.925264 140026058876672 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.38450288772583, loss=3.4971415996551514
I0127 14:47:00.164817 140026151130880 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.155673623085022, loss=3.4793643951416016
I0127 14:47:34.210105 140026058876672 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.6618459224700928, loss=3.513605833053589
I0127 14:48:08.252285 140026151130880 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.4088530540466309, loss=3.492556095123291
I0127 14:48:42.273383 140026058876672 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.5321128368377686, loss=3.487504243850708
I0127 14:49:16.324885 140026151130880 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.3149524927139282, loss=3.5460143089294434
I0127 14:49:38.572648 140187804313408 spec.py:321] Evaluating on the training split.
I0127 14:49:44.803369 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 14:49:53.761207 140187804313408 spec.py:349] Evaluating on the test split.
I0127 14:49:56.343522 140187804313408 submission_runner.py:408] Time since start: 5857.64s, 	Step: 16467, 	{'train/accuracy': 0.6412428021430969, 'train/loss': 1.7201507091522217, 'validation/accuracy': 0.5932999849319458, 'validation/loss': 1.9380810260772705, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.6094350814819336, 'test/num_examples': 10000, 'score': 5642.635751485825, 'total_duration': 5857.638697385788, 'accumulated_submission_time': 5642.635751485825, 'accumulated_eval_time': 214.10228490829468, 'accumulated_logging_time': 0.3106076717376709}
I0127 14:49:56.363712 140026058876672 logging_writer.py:48] [16467] accumulated_eval_time=214.102285, accumulated_logging_time=0.310608, accumulated_submission_time=5642.635751, global_step=16467, preemption_count=0, score=5642.635751, test/accuracy=0.469000, test/loss=2.609435, test/num_examples=10000, total_duration=5857.638697, train/accuracy=0.641243, train/loss=1.720151, validation/accuracy=0.593300, validation/loss=1.938081, validation/num_examples=50000
I0127 14:50:07.907958 140026075662080 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.8345849514007568, loss=3.482515811920166
I0127 14:50:41.891506 140026058876672 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.4616971015930176, loss=3.484729290008545
I0127 14:51:15.888511 140026075662080 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.3869348764419556, loss=3.4220998287200928
I0127 14:51:49.902502 140026058876672 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.4440432786941528, loss=3.4796462059020996
I0127 14:52:23.952666 140026075662080 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.3846650123596191, loss=3.433950185775757
I0127 14:52:58.001423 140026058876672 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.5575535297393799, loss=3.486443281173706
I0127 14:53:32.118476 140026075662080 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.902592658996582, loss=3.491698741912842
I0127 14:54:06.131162 140026058876672 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.603058099746704, loss=3.4133901596069336
I0127 14:54:40.168454 140026075662080 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.3507741689682007, loss=3.3779451847076416
I0127 14:55:14.161544 140026058876672 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.3850854635238647, loss=3.5239346027374268
I0127 14:55:48.207862 140026075662080 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.3876996040344238, loss=3.4096107482910156
I0127 14:56:22.264269 140026058876672 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.2540348768234253, loss=3.397116184234619
I0127 14:56:56.305514 140026075662080 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.407884120941162, loss=3.3839802742004395
I0127 14:57:30.327570 140026058876672 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.5934135913848877, loss=3.429413318634033
I0127 14:58:04.337896 140026075662080 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.374498724937439, loss=3.349111318588257
I0127 14:58:26.586165 140187804313408 spec.py:321] Evaluating on the training split.
I0127 14:58:32.700660 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 14:58:41.815449 140187804313408 spec.py:349] Evaluating on the test split.
I0127 14:58:44.314415 140187804313408 submission_runner.py:408] Time since start: 6385.61s, 	Step: 17967, 	{'train/accuracy': 0.7058354616165161, 'train/loss': 1.4462586641311646, 'validation/accuracy': 0.6126999855041504, 'validation/loss': 1.8431637287139893, 'validation/num_examples': 50000, 'test/accuracy': 0.4798000156879425, 'test/loss': 2.5136966705322266, 'test/num_examples': 10000, 'score': 6152.796566486359, 'total_duration': 6385.609614610672, 'accumulated_submission_time': 6152.796566486359, 'accumulated_eval_time': 231.83049607276917, 'accumulated_logging_time': 0.34010767936706543}
I0127 14:58:44.335566 140026042091264 logging_writer.py:48] [17967] accumulated_eval_time=231.830496, accumulated_logging_time=0.340108, accumulated_submission_time=6152.796566, global_step=17967, preemption_count=0, score=6152.796566, test/accuracy=0.479800, test/loss=2.513697, test/num_examples=10000, total_duration=6385.609615, train/accuracy=0.705835, train/loss=1.446259, validation/accuracy=0.612700, validation/loss=1.843164, validation/num_examples=50000
I0127 14:58:55.905883 140026050483968 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0963480472564697, loss=3.400273084640503
I0127 14:59:29.878469 140026042091264 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.552020788192749, loss=3.4869470596313477
I0127 15:00:03.950096 140026050483968 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.491617202758789, loss=3.4531168937683105
I0127 15:00:38.022317 140026042091264 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.4596202373504639, loss=3.4282026290893555
I0127 15:01:12.056387 140026050483968 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.2805925607681274, loss=3.379319190979004
I0127 15:01:46.108187 140026042091264 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.5901548862457275, loss=3.412412405014038
I0127 15:02:20.149533 140026050483968 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.6713230609893799, loss=3.341458320617676
I0127 15:02:54.182827 140026042091264 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.6947096586227417, loss=3.511620283126831
I0127 15:03:28.196932 140026050483968 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.7133088111877441, loss=3.36769700050354
I0127 15:04:02.237404 140026042091264 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.350021243095398, loss=3.398763656616211
I0127 15:04:36.283070 140026050483968 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.2275168895721436, loss=3.4076807498931885
I0127 15:05:10.307116 140026042091264 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.46439790725708, loss=3.4076545238494873
I0127 15:05:44.325487 140026050483968 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.5271050930023193, loss=3.4198665618896484
I0127 15:06:18.397620 140026042091264 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.3366374969482422, loss=3.4324934482574463
I0127 15:06:52.424657 140026050483968 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.3030791282653809, loss=3.4301862716674805
I0127 15:07:14.327816 140187804313408 spec.py:321] Evaluating on the training split.
I0127 15:07:20.547049 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 15:07:29.643571 140187804313408 spec.py:349] Evaluating on the test split.
I0127 15:07:32.113635 140187804313408 submission_runner.py:408] Time since start: 6913.41s, 	Step: 19466, 	{'train/accuracy': 0.6865234375, 'train/loss': 1.4992294311523438, 'validation/accuracy': 0.6133399605751038, 'validation/loss': 1.8168702125549316, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.5008463859558105, 'test/num_examples': 10000, 'score': 6662.721271753311, 'total_duration': 6913.4088344573975, 'accumulated_submission_time': 6662.721271753311, 'accumulated_eval_time': 249.6162793636322, 'accumulated_logging_time': 0.37390899658203125}
I0127 15:07:32.138157 140026167916288 logging_writer.py:48] [19466] accumulated_eval_time=249.616279, accumulated_logging_time=0.373909, accumulated_submission_time=6662.721272, global_step=19466, preemption_count=0, score=6662.721272, test/accuracy=0.485200, test/loss=2.500846, test/num_examples=10000, total_duration=6913.408834, train/accuracy=0.686523, train/loss=1.499229, validation/accuracy=0.613340, validation/loss=1.816870, validation/num_examples=50000
I0127 15:07:44.050081 140026176308992 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.2679945230484009, loss=3.4589180946350098
I0127 15:08:18.022361 140026167916288 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.575317621231079, loss=3.492978811264038
I0127 15:08:52.032507 140026176308992 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.4744117259979248, loss=3.3765649795532227
I0127 15:09:26.035003 140026167916288 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.3079129457473755, loss=3.3356308937072754
I0127 15:10:00.067889 140026176308992 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.941091775894165, loss=3.41990327835083
I0127 15:10:34.089275 140026167916288 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.513441801071167, loss=3.35629940032959
I0127 15:11:08.087580 140026176308992 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.4645441770553589, loss=3.3739609718322754
I0127 15:11:42.114793 140026167916288 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.3493735790252686, loss=3.43438458442688
I0127 15:12:16.172064 140026176308992 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.215582251548767, loss=3.3822319507598877
I0127 15:12:50.188502 140026167916288 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4501508474349976, loss=3.347275972366333
I0127 15:13:24.209803 140026176308992 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.2673472166061401, loss=3.397231101989746
I0127 15:13:58.241552 140026167916288 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.4706127643585205, loss=3.3020572662353516
I0127 15:14:32.253738 140026176308992 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.0927155017852783, loss=3.283327341079712
I0127 15:15:06.273859 140026167916288 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.4673010110855103, loss=3.381575107574463
I0127 15:15:40.312253 140026176308992 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1877113580703735, loss=3.3153932094573975
I0127 15:16:02.218382 140187804313408 spec.py:321] Evaluating on the training split.
I0127 15:16:08.369806 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 15:16:17.263971 140187804313408 spec.py:349] Evaluating on the test split.
I0127 15:16:19.771154 140187804313408 submission_runner.py:408] Time since start: 7441.07s, 	Step: 20966, 	{'train/accuracy': 0.6920639276504517, 'train/loss': 1.4709275960922241, 'validation/accuracy': 0.6231799721717834, 'validation/loss': 1.7682565450668335, 'validation/num_examples': 50000, 'test/accuracy': 0.501300036907196, 'test/loss': 2.41690993309021, 'test/num_examples': 10000, 'score': 7172.737153053284, 'total_duration': 7441.06632399559, 'accumulated_submission_time': 7172.737153053284, 'accumulated_eval_time': 267.1690058708191, 'accumulated_logging_time': 0.4088256359100342}
I0127 15:16:19.793274 140026050483968 logging_writer.py:48] [20966] accumulated_eval_time=267.169006, accumulated_logging_time=0.408826, accumulated_submission_time=7172.737153, global_step=20966, preemption_count=0, score=7172.737153, test/accuracy=0.501300, test/loss=2.416910, test/num_examples=10000, total_duration=7441.066324, train/accuracy=0.692064, train/loss=1.470928, validation/accuracy=0.623180, validation/loss=1.768257, validation/num_examples=50000
I0127 15:16:31.684750 140026058876672 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.2550592422485352, loss=3.410212516784668
I0127 15:17:05.654132 140026050483968 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.14014732837677, loss=3.320411443710327
I0127 15:17:39.661119 140026058876672 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.767789602279663, loss=3.4005746841430664
I0127 15:18:13.685816 140026050483968 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.8391361236572266, loss=3.3998498916625977
I0127 15:18:47.765974 140026058876672 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.667281150817871, loss=3.3768210411071777
I0127 15:19:21.788429 140026050483968 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.8906811475753784, loss=3.390904426574707
I0127 15:19:55.826324 140026058876672 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.2704033851623535, loss=3.3972606658935547
I0127 15:20:29.876960 140026050483968 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.3164011240005493, loss=3.3385493755340576
I0127 15:21:03.915544 140026058876672 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.4169477224349976, loss=3.3355703353881836
I0127 15:21:37.959465 140026050483968 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.3852291107177734, loss=3.3585147857666016
I0127 15:22:11.987064 140026058876672 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.4538007974624634, loss=3.322636604309082
I0127 15:22:46.029577 140026050483968 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.4749391078948975, loss=3.391638994216919
I0127 15:23:20.058958 140026058876672 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.3574788570404053, loss=3.3454885482788086
I0127 15:23:54.086458 140026050483968 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.332491397857666, loss=3.2957072257995605
I0127 15:24:28.272352 140026058876672 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.6014753580093384, loss=3.375631809234619
I0127 15:24:49.841550 140187804313408 spec.py:321] Evaluating on the training split.
I0127 15:24:56.020877 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 15:25:05.015157 140187804313408 spec.py:349] Evaluating on the test split.
I0127 15:25:07.566711 140187804313408 submission_runner.py:408] Time since start: 7968.86s, 	Step: 22465, 	{'train/accuracy': 0.6925621628761292, 'train/loss': 1.4761664867401123, 'validation/accuracy': 0.6293399930000305, 'validation/loss': 1.753197431564331, 'validation/num_examples': 50000, 'test/accuracy': 0.5006999969482422, 'test/loss': 2.429164409637451, 'test/num_examples': 10000, 'score': 7682.723104953766, 'total_duration': 7968.861889123917, 'accumulated_submission_time': 7682.723104953766, 'accumulated_eval_time': 284.8941237926483, 'accumulated_logging_time': 0.44005632400512695}
I0127 15:25:07.590575 140026151130880 logging_writer.py:48] [22465] accumulated_eval_time=284.894124, accumulated_logging_time=0.440056, accumulated_submission_time=7682.723105, global_step=22465, preemption_count=0, score=7682.723105, test/accuracy=0.500700, test/loss=2.429164, test/num_examples=10000, total_duration=7968.861889, train/accuracy=0.692562, train/loss=1.476166, validation/accuracy=0.629340, validation/loss=1.753197, validation/num_examples=50000
I0127 15:25:19.805678 140026159523584 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.3101505041122437, loss=3.2909927368164062
I0127 15:25:53.743864 140026151130880 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.35320246219635, loss=3.335254192352295
I0127 15:26:27.711726 140026159523584 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.446213722229004, loss=3.395293712615967
I0127 15:27:01.713654 140026151130880 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0667353868484497, loss=3.3558809757232666
I0127 15:27:35.706734 140026159523584 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.4638069868087769, loss=3.2750909328460693
I0127 15:28:09.719503 140026151130880 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.402004361152649, loss=3.36020565032959
I0127 15:28:43.736895 140026159523584 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.474854826927185, loss=3.3066914081573486
I0127 15:29:17.737697 140026151130880 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.2386610507965088, loss=3.335812568664551
I0127 15:29:51.746662 140026159523584 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5508472919464111, loss=3.318636655807495
I0127 15:30:25.748249 140026151130880 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.3094005584716797, loss=3.320185422897339
I0127 15:30:59.859822 140026159523584 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.445594072341919, loss=3.30969500541687
I0127 15:31:33.885125 140026151130880 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.3100776672363281, loss=3.3830227851867676
I0127 15:32:07.919867 140026159523584 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.2724595069885254, loss=3.3704581260681152
I0127 15:32:41.909984 140026151130880 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.307381272315979, loss=3.3859944343566895
I0127 15:33:15.930264 140026159523584 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.5743130445480347, loss=3.335691213607788
I0127 15:33:37.859419 140187804313408 spec.py:321] Evaluating on the training split.
I0127 15:33:43.978859 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 15:33:52.908736 140187804313408 spec.py:349] Evaluating on the test split.
I0127 15:33:55.434091 140187804313408 submission_runner.py:408] Time since start: 8496.73s, 	Step: 23966, 	{'train/accuracy': 0.6956911683082581, 'train/loss': 1.4922162294387817, 'validation/accuracy': 0.634719967842102, 'validation/loss': 1.7566068172454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.4119579792022705, 'test/num_examples': 10000, 'score': 8192.924956321716, 'total_duration': 8496.729290246964, 'accumulated_submission_time': 8192.924956321716, 'accumulated_eval_time': 302.46875619888306, 'accumulated_logging_time': 0.4770851135253906}
I0127 15:33:55.455894 140026050483968 logging_writer.py:48] [23966] accumulated_eval_time=302.468756, accumulated_logging_time=0.477085, accumulated_submission_time=8192.924956, global_step=23966, preemption_count=0, score=8192.924956, test/accuracy=0.509200, test/loss=2.411958, test/num_examples=10000, total_duration=8496.729290, train/accuracy=0.695691, train/loss=1.492216, validation/accuracy=0.634720, validation/loss=1.756607, validation/num_examples=50000
I0127 15:34:07.328706 140026058876672 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.5733495950698853, loss=3.356435537338257
I0127 15:34:41.349604 140026050483968 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.2983399629592896, loss=3.34423565864563
I0127 15:35:15.392792 140026058876672 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.233030915260315, loss=3.312643527984619
I0127 15:35:49.409286 140026050483968 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.6697015762329102, loss=3.3666915893554688
I0127 15:36:23.410784 140026058876672 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.3457882404327393, loss=3.362419843673706
I0127 15:36:57.387509 140026050483968 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3601652383804321, loss=3.354916572570801
I0127 15:37:31.471250 140026058876672 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.4499871730804443, loss=3.366664171218872
I0127 15:38:05.506114 140026050483968 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.2885109186172485, loss=3.2739851474761963
I0127 15:38:39.547996 140026058876672 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.5096677541732788, loss=3.281651258468628
I0127 15:39:13.563009 140026050483968 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.4204267263412476, loss=3.3348701000213623
I0127 15:39:47.582740 140026058876672 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.3219983577728271, loss=3.350557327270508
I0127 15:40:21.577266 140026050483968 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.3165310621261597, loss=3.3698244094848633
I0127 15:40:55.592881 140026058876672 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.3596787452697754, loss=3.3685786724090576
I0127 15:41:29.592551 140026050483968 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.2689062356948853, loss=3.3767123222351074
I0127 15:42:03.615206 140026058876672 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.2681560516357422, loss=3.3565473556518555
I0127 15:42:25.540507 140187804313408 spec.py:321] Evaluating on the training split.
I0127 15:42:31.695769 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 15:42:40.611821 140187804313408 spec.py:349] Evaluating on the test split.
I0127 15:42:43.124915 140187804313408 submission_runner.py:408] Time since start: 9024.42s, 	Step: 25466, 	{'train/accuracy': 0.7027263641357422, 'train/loss': 1.419603705406189, 'validation/accuracy': 0.6396999955177307, 'validation/loss': 1.6906665563583374, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.3508965969085693, 'test/num_examples': 10000, 'score': 8702.947353839874, 'total_duration': 9024.420098781586, 'accumulated_submission_time': 8702.947353839874, 'accumulated_eval_time': 320.05310821533203, 'accumulated_logging_time': 0.508368968963623}
I0127 15:42:43.146581 140026042091264 logging_writer.py:48] [25466] accumulated_eval_time=320.053108, accumulated_logging_time=0.508369, accumulated_submission_time=8702.947354, global_step=25466, preemption_count=0, score=8702.947354, test/accuracy=0.516200, test/loss=2.350897, test/num_examples=10000, total_duration=9024.420099, train/accuracy=0.702726, train/loss=1.419604, validation/accuracy=0.639700, validation/loss=1.690667, validation/num_examples=50000
I0127 15:42:55.039699 140026050483968 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.3156026601791382, loss=3.351417064666748
I0127 15:43:29.041242 140026042091264 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.356850028038025, loss=3.2782680988311768
I0127 15:44:03.012883 140026050483968 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.3333979845046997, loss=3.3173089027404785
I0127 15:44:37.014062 140026042091264 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.8105711936950684, loss=3.2889506816864014
I0127 15:45:11.024201 140026050483968 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.354495644569397, loss=3.272177219390869
I0127 15:45:45.033210 140026042091264 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.5119681358337402, loss=3.378070831298828
I0127 15:46:19.041388 140026050483968 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.7702590227127075, loss=3.2327308654785156
I0127 15:46:53.060272 140026042091264 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.373502254486084, loss=3.2976269721984863
I0127 15:47:27.070785 140026050483968 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.237117052078247, loss=3.311947822570801
I0127 15:48:01.077148 140026042091264 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.6758509874343872, loss=3.2888376712799072
I0127 15:48:35.080343 140026050483968 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.4254295825958252, loss=3.341327428817749
I0127 15:49:09.110887 140026042091264 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.3300226926803589, loss=3.292577028274536
I0127 15:49:43.201067 140026050483968 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.694156289100647, loss=3.3227248191833496
I0127 15:50:17.208156 140026042091264 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.2421154975891113, loss=3.295206069946289
I0127 15:50:51.203808 140026050483968 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.378318190574646, loss=3.183661699295044
I0127 15:51:13.125692 140187804313408 spec.py:321] Evaluating on the training split.
I0127 15:51:19.265848 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 15:51:28.210613 140187804313408 spec.py:349] Evaluating on the test split.
I0127 15:51:30.680068 140187804313408 submission_runner.py:408] Time since start: 9551.98s, 	Step: 26966, 	{'train/accuracy': 0.6951530575752258, 'train/loss': 1.484920859336853, 'validation/accuracy': 0.6298199892044067, 'validation/loss': 1.7593168020248413, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.4571080207824707, 'test/num_examples': 10000, 'score': 9212.861717700958, 'total_duration': 9551.975267887115, 'accumulated_submission_time': 9212.861717700958, 'accumulated_eval_time': 337.60744285583496, 'accumulated_logging_time': 0.5415892601013184}
I0127 15:51:30.702279 140026050483968 logging_writer.py:48] [26966] accumulated_eval_time=337.607443, accumulated_logging_time=0.541589, accumulated_submission_time=9212.861718, global_step=26966, preemption_count=0, score=9212.861718, test/accuracy=0.495100, test/loss=2.457108, test/num_examples=10000, total_duration=9551.975268, train/accuracy=0.695153, train/loss=1.484921, validation/accuracy=0.629820, validation/loss=1.759317, validation/num_examples=50000
I0127 15:51:42.578438 140026075662080 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.3287218809127808, loss=3.3246426582336426
I0127 15:52:16.572879 140026050483968 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2010544538497925, loss=3.2559714317321777
I0127 15:52:50.603958 140026075662080 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.423136830329895, loss=3.3425252437591553
I0127 15:53:24.628459 140026050483968 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.2691577672958374, loss=3.331547498703003
I0127 15:53:58.641297 140026075662080 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.3431518077850342, loss=3.2579939365386963
I0127 15:54:32.679043 140026050483968 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.3683140277862549, loss=3.259387493133545
I0127 15:55:06.702344 140026075662080 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.5389137268066406, loss=3.2935681343078613
I0127 15:55:40.696350 140026050483968 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.2156264781951904, loss=3.259538173675537
I0127 15:56:14.772575 140026075662080 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.4973458051681519, loss=3.334226369857788
I0127 15:56:48.813040 140026050483968 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.291071891784668, loss=3.3078126907348633
I0127 15:57:22.835112 140026075662080 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.3665305376052856, loss=3.2878806591033936
I0127 15:57:56.854669 140026050483968 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.5032713413238525, loss=3.290177583694458
I0127 15:58:30.875923 140026075662080 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.6126230955123901, loss=3.3071982860565186
I0127 15:59:04.894338 140026050483968 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.288684368133545, loss=3.2427334785461426
I0127 15:59:38.926329 140026075662080 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.3746569156646729, loss=3.2290608882904053
I0127 16:00:00.834764 140187804313408 spec.py:321] Evaluating on the training split.
I0127 16:00:07.100001 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 16:00:16.177299 140187804313408 spec.py:349] Evaluating on the test split.
I0127 16:00:18.668925 140187804313408 submission_runner.py:408] Time since start: 10079.96s, 	Step: 28466, 	{'train/accuracy': 0.7315250039100647, 'train/loss': 1.2711821794509888, 'validation/accuracy': 0.6417799592018127, 'validation/loss': 1.6622222661972046, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.301248073577881, 'test/num_examples': 10000, 'score': 9722.92922616005, 'total_duration': 10079.964128255844, 'accumulated_submission_time': 9722.92922616005, 'accumulated_eval_time': 355.4415764808655, 'accumulated_logging_time': 0.5747489929199219}
I0127 16:00:18.690795 140026159523584 logging_writer.py:48] [28466] accumulated_eval_time=355.441576, accumulated_logging_time=0.574749, accumulated_submission_time=9722.929226, global_step=28466, preemption_count=0, score=9722.929226, test/accuracy=0.519000, test/loss=2.301248, test/num_examples=10000, total_duration=10079.964128, train/accuracy=0.731525, train/loss=1.271182, validation/accuracy=0.641780, validation/loss=1.662222, validation/num_examples=50000
I0127 16:00:30.585420 140026167916288 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.170072317123413, loss=3.1905996799468994
I0127 16:01:04.531724 140026159523584 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.320548415184021, loss=3.3437418937683105
I0127 16:01:38.515135 140026167916288 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.3205229043960571, loss=3.325941324234009
I0127 16:02:12.522156 140026159523584 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.277660608291626, loss=3.2407872676849365
I0127 16:02:46.583147 140026167916288 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.2898447513580322, loss=3.305288314819336
I0127 16:03:20.569237 140026159523584 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.3570479154586792, loss=3.232235908508301
I0127 16:03:54.566438 140026167916288 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.4693669080734253, loss=3.2483208179473877
I0127 16:04:28.591909 140026159523584 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.3753163814544678, loss=3.234224796295166
I0127 16:05:02.587826 140026167916288 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.4855055809020996, loss=3.3348841667175293
I0127 16:05:36.619864 140026159523584 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.7182354927062988, loss=3.318282127380371
I0127 16:06:10.615727 140026167916288 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.3515348434448242, loss=3.3209099769592285
I0127 16:06:44.644906 140026159523584 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.3127820491790771, loss=3.2552363872528076
I0127 16:07:18.647064 140026167916288 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.3738242387771606, loss=3.220360517501831
I0127 16:07:52.657779 140026159523584 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.3700947761535645, loss=3.236867666244507
I0127 16:08:26.675414 140026167916288 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.444437026977539, loss=3.2761197090148926
I0127 16:08:49.006363 140187804313408 spec.py:321] Evaluating on the training split.
I0127 16:08:55.116035 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 16:09:04.421320 140187804313408 spec.py:349] Evaluating on the test split.
I0127 16:09:06.911324 140187804313408 submission_runner.py:408] Time since start: 10608.21s, 	Step: 29967, 	{'train/accuracy': 0.7155213356018066, 'train/loss': 1.3536566495895386, 'validation/accuracy': 0.6436799764633179, 'validation/loss': 1.6779758930206299, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.333815813064575, 'test/num_examples': 10000, 'score': 10233.182272434235, 'total_duration': 10608.206499814987, 'accumulated_submission_time': 10233.182272434235, 'accumulated_eval_time': 373.3464798927307, 'accumulated_logging_time': 0.6060409545898438}
I0127 16:09:06.941797 140026050483968 logging_writer.py:48] [29967] accumulated_eval_time=373.346480, accumulated_logging_time=0.606041, accumulated_submission_time=10233.182272, global_step=29967, preemption_count=0, score=10233.182272, test/accuracy=0.519600, test/loss=2.333816, test/num_examples=10000, total_duration=10608.206500, train/accuracy=0.715521, train/loss=1.353657, validation/accuracy=0.643680, validation/loss=1.677976, validation/num_examples=50000
I0127 16:09:18.514865 140026058876672 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.544245719909668, loss=3.301943778991699
I0127 16:09:52.494405 140026050483968 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.4453877210617065, loss=3.247598886489868
I0127 16:10:26.483415 140026058876672 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.3216798305511475, loss=3.3049874305725098
I0127 16:11:00.491035 140026050483968 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.3419735431671143, loss=3.2778897285461426
I0127 16:11:34.503525 140026058876672 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.507702350616455, loss=3.217177391052246
I0127 16:12:08.535199 140026050483968 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.4382606744766235, loss=3.259303331375122
I0127 16:12:42.502618 140026058876672 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.34064781665802, loss=3.2751266956329346
I0127 16:13:16.526692 140026050483968 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.3536103963851929, loss=3.2265167236328125
I0127 16:13:50.496388 140026058876672 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.3627405166625977, loss=3.2793943881988525
I0127 16:14:24.508173 140026050483968 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.460652232170105, loss=3.332059860229492
I0127 16:14:58.541678 140026058876672 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.547413945198059, loss=3.275710344314575
I0127 16:15:32.512746 140026050483968 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.4651533365249634, loss=3.262547016143799
I0127 16:16:06.526181 140026058876672 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.3662855625152588, loss=3.2923495769500732
I0127 16:16:40.514173 140026050483968 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.4754635095596313, loss=3.247283935546875
I0127 16:17:14.556488 140026058876672 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.3262418508529663, loss=3.3081932067871094
I0127 16:17:37.172315 140187804313408 spec.py:321] Evaluating on the training split.
I0127 16:17:43.271378 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 16:17:52.415827 140187804313408 spec.py:349] Evaluating on the test split.
I0127 16:17:54.933348 140187804313408 submission_runner.py:408] Time since start: 11136.23s, 	Step: 31468, 	{'train/accuracy': 0.7057158946990967, 'train/loss': 1.420884132385254, 'validation/accuracy': 0.6421200037002563, 'validation/loss': 1.7156926393508911, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.4058032035827637, 'test/num_examples': 10000, 'score': 10743.343856096268, 'total_duration': 11136.228532791138, 'accumulated_submission_time': 10743.343856096268, 'accumulated_eval_time': 391.10746002197266, 'accumulated_logging_time': 0.6514892578125}
I0127 16:17:54.958605 140026151130880 logging_writer.py:48] [31468] accumulated_eval_time=391.107460, accumulated_logging_time=0.651489, accumulated_submission_time=10743.343856, global_step=31468, preemption_count=0, score=10743.343856, test/accuracy=0.507400, test/loss=2.405803, test/num_examples=10000, total_duration=11136.228533, train/accuracy=0.705716, train/loss=1.420884, validation/accuracy=0.642120, validation/loss=1.715693, validation/num_examples=50000
I0127 16:18:06.175309 140026159523584 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.421159029006958, loss=3.286381244659424
I0127 16:18:40.104018 140026151130880 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.5831879377365112, loss=3.1420722007751465
I0127 16:19:14.083173 140026159523584 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1381956338882446, loss=3.195861577987671
I0127 16:19:48.068438 140026151130880 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.3522683382034302, loss=3.267274856567383
I0127 16:20:22.063471 140026159523584 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.4894880056381226, loss=3.2253658771514893
I0127 16:20:56.467043 140026151130880 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.3197563886642456, loss=3.2031993865966797
I0127 16:21:30.518537 140026159523584 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.5152637958526611, loss=3.2962827682495117
I0127 16:22:04.517764 140026151130880 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.4031518697738647, loss=3.251539945602417
I0127 16:22:38.501324 140026159523584 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.201490879058838, loss=3.138995885848999
I0127 16:23:12.512666 140026151130880 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.3877122402191162, loss=3.3004772663116455
I0127 16:23:46.529792 140026159523584 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.4006224870681763, loss=3.2171075344085693
I0127 16:24:20.544825 140026151130880 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.4169318675994873, loss=3.2604990005493164
I0127 16:24:54.560002 140026159523584 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.5269970893859863, loss=3.2859482765197754
I0127 16:25:28.533941 140026151130880 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.2984364032745361, loss=3.239107370376587
I0127 16:26:02.530490 140026159523584 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.3668566942214966, loss=3.1769556999206543
I0127 16:26:25.103865 140187804313408 spec.py:321] Evaluating on the training split.
I0127 16:26:31.321589 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 16:26:40.399166 140187804313408 spec.py:349] Evaluating on the test split.
I0127 16:26:42.854937 140187804313408 submission_runner.py:408] Time since start: 11664.15s, 	Step: 32968, 	{'train/accuracy': 0.7166374325752258, 'train/loss': 1.3599779605865479, 'validation/accuracy': 0.6487799882888794, 'validation/loss': 1.6539162397384644, 'validation/num_examples': 50000, 'test/accuracy': 0.5246000289916992, 'test/loss': 2.2995214462280273, 'test/num_examples': 10000, 'score': 11253.426638364792, 'total_duration': 11664.150138139725, 'accumulated_submission_time': 11253.426638364792, 'accumulated_eval_time': 408.85849595069885, 'accumulated_logging_time': 0.6858878135681152}
I0127 16:26:42.878304 140026050483968 logging_writer.py:48] [32968] accumulated_eval_time=408.858496, accumulated_logging_time=0.685888, accumulated_submission_time=11253.426638, global_step=32968, preemption_count=0, score=11253.426638, test/accuracy=0.524600, test/loss=2.299521, test/num_examples=10000, total_duration=11664.150138, train/accuracy=0.716637, train/loss=1.359978, validation/accuracy=0.648780, validation/loss=1.653916, validation/num_examples=50000
I0127 16:26:54.085696 140026067269376 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.412558674812317, loss=3.208134412765503
I0127 16:27:28.059722 140026050483968 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.4317306280136108, loss=3.2566001415252686
I0127 16:28:02.043817 140026067269376 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.380763292312622, loss=3.238032341003418
I0127 16:28:36.072355 140026050483968 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.3792028427124023, loss=3.216510772705078
I0127 16:29:10.099495 140026067269376 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.7005691528320312, loss=3.2615604400634766
I0127 16:29:44.076805 140026050483968 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.3802717924118042, loss=3.2585525512695312
I0127 16:30:18.111551 140026067269376 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.4115452766418457, loss=3.350581169128418
I0127 16:30:52.115899 140026050483968 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.654878854751587, loss=3.1270172595977783
I0127 16:31:26.118726 140026067269376 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.4211758375167847, loss=3.267777919769287
I0127 16:32:00.130279 140026050483968 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.356743335723877, loss=3.1633200645446777
I0127 16:32:34.140551 140026067269376 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.2678769826889038, loss=3.2568979263305664
I0127 16:33:08.173269 140026050483968 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.7512294054031372, loss=3.252889394760132
I0127 16:33:42.252963 140026067269376 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.3435295820236206, loss=3.1347739696502686
I0127 16:34:16.236123 140026050483968 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.3475505113601685, loss=3.2728607654571533
I0127 16:34:50.259992 140026067269376 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.5285720825195312, loss=3.1892099380493164
I0127 16:35:12.861402 140187804313408 spec.py:321] Evaluating on the training split.
I0127 16:35:19.097045 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 16:35:27.974020 140187804313408 spec.py:349] Evaluating on the test split.
I0127 16:35:30.476674 140187804313408 submission_runner.py:408] Time since start: 12191.77s, 	Step: 34468, 	{'train/accuracy': 0.70703125, 'train/loss': 1.3962434530258179, 'validation/accuracy': 0.6412400007247925, 'validation/loss': 1.682578444480896, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.3533051013946533, 'test/num_examples': 10000, 'score': 11763.347280740738, 'total_duration': 12191.771873950958, 'accumulated_submission_time': 11763.347280740738, 'accumulated_eval_time': 426.4737284183502, 'accumulated_logging_time': 0.7184295654296875}
I0127 16:35:30.504699 140026058876672 logging_writer.py:48] [34468] accumulated_eval_time=426.473728, accumulated_logging_time=0.718430, accumulated_submission_time=11763.347281, global_step=34468, preemption_count=0, score=11763.347281, test/accuracy=0.515100, test/loss=2.353305, test/num_examples=10000, total_duration=12191.771874, train/accuracy=0.707031, train/loss=1.396243, validation/accuracy=0.641240, validation/loss=1.682578, validation/num_examples=50000
I0127 16:35:41.695900 140026151130880 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.4018073081970215, loss=3.182575225830078
I0127 16:36:15.657111 140026058876672 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.3567548990249634, loss=3.201723575592041
I0127 16:36:49.638175 140026151130880 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.3059042692184448, loss=3.192760705947876
I0127 16:37:23.640804 140026058876672 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.5453038215637207, loss=3.281506061553955
I0127 16:37:57.638115 140026151130880 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.4428718090057373, loss=3.2405619621276855
I0127 16:38:31.650787 140026058876672 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.438532829284668, loss=3.233403205871582
I0127 16:39:05.668048 140026151130880 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.4978835582733154, loss=3.2069993019104004
I0127 16:39:39.654364 140026058876672 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.4566384553909302, loss=3.204801082611084
I0127 16:40:13.711961 140026151130880 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.5192434787750244, loss=3.241464376449585
I0127 16:40:47.694716 140026058876672 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.4238965511322021, loss=3.1950089931488037
I0127 16:41:21.698242 140026151130880 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.3367873430252075, loss=3.20371150970459
I0127 16:41:55.660197 140026058876672 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.6225900650024414, loss=3.2790257930755615
I0127 16:42:29.684797 140026151130880 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.2654194831848145, loss=3.205130100250244
I0127 16:43:03.695553 140026058876672 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.4959688186645508, loss=3.207791328430176
I0127 16:43:37.709315 140026151130880 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.7012150287628174, loss=3.2019598484039307
I0127 16:44:00.633940 140187804313408 spec.py:321] Evaluating on the training split.
I0127 16:44:06.969714 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 16:44:15.945490 140187804313408 spec.py:349] Evaluating on the test split.
I0127 16:44:18.402924 140187804313408 submission_runner.py:408] Time since start: 12719.70s, 	Step: 35969, 	{'train/accuracy': 0.7151227593421936, 'train/loss': 1.3773218393325806, 'validation/accuracy': 0.6521199941635132, 'validation/loss': 1.6565839052200317, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.307307243347168, 'test/num_examples': 10000, 'score': 12273.410103797913, 'total_duration': 12719.69810295105, 'accumulated_submission_time': 12273.410103797913, 'accumulated_eval_time': 444.24265217781067, 'accumulated_logging_time': 0.7576742172241211}
I0127 16:44:18.426492 140026050483968 logging_writer.py:48] [35969] accumulated_eval_time=444.242652, accumulated_logging_time=0.757674, accumulated_submission_time=12273.410104, global_step=35969, preemption_count=0, score=12273.410104, test/accuracy=0.522600, test/loss=2.307307, test/num_examples=10000, total_duration=12719.698103, train/accuracy=0.715123, train/loss=1.377322, validation/accuracy=0.652120, validation/loss=1.656584, validation/num_examples=50000
I0127 16:44:29.303294 140026058876672 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6025652885437012, loss=3.190304756164551
I0127 16:45:03.270611 140026050483968 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.5841511487960815, loss=3.2825589179992676
I0127 16:45:37.278183 140026058876672 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.4882603883743286, loss=3.187657356262207
I0127 16:46:11.357060 140026050483968 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.5511749982833862, loss=3.2622005939483643
I0127 16:46:45.360399 140026058876672 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.4573440551757812, loss=3.2723870277404785
I0127 16:47:19.360749 140026050483968 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.403850793838501, loss=3.228062629699707
I0127 16:47:53.368898 140026058876672 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.4671478271484375, loss=3.2246146202087402
I0127 16:48:27.362995 140026050483968 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.4980707168579102, loss=3.179183006286621
I0127 16:49:01.390330 140026058876672 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.3430901765823364, loss=3.1998538970947266
I0127 16:49:35.427244 140026050483968 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.4981223344802856, loss=3.2122421264648438
I0127 16:50:09.451718 140026058876672 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.3830747604370117, loss=3.286107063293457
I0127 16:50:43.457515 140026050483968 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.5758682489395142, loss=3.2356534004211426
I0127 16:51:17.451847 140026058876672 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.4801411628723145, loss=3.190946102142334
I0127 16:51:51.455234 140026050483968 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.6767535209655762, loss=3.1962838172912598
I0127 16:52:25.558785 140026058876672 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.4530421495437622, loss=3.139951705932617
I0127 16:52:48.503378 140187804313408 spec.py:321] Evaluating on the training split.
I0127 16:52:54.668749 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 16:53:03.803688 140187804313408 spec.py:349] Evaluating on the test split.
I0127 16:53:06.278873 140187804313408 submission_runner.py:408] Time since start: 13247.57s, 	Step: 37469, 	{'train/accuracy': 0.7365872263908386, 'train/loss': 1.2994897365570068, 'validation/accuracy': 0.6460199952125549, 'validation/loss': 1.7007862329483032, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.3833701610565186, 'test/num_examples': 10000, 'score': 12783.423173427582, 'total_duration': 13247.574071884155, 'accumulated_submission_time': 12783.423173427582, 'accumulated_eval_time': 462.0181083679199, 'accumulated_logging_time': 0.7907888889312744}
I0127 16:53:06.303342 140026042091264 logging_writer.py:48] [37469] accumulated_eval_time=462.018108, accumulated_logging_time=0.790789, accumulated_submission_time=12783.423173, global_step=37469, preemption_count=0, score=12783.423173, test/accuracy=0.507800, test/loss=2.383370, test/num_examples=10000, total_duration=13247.574072, train/accuracy=0.736587, train/loss=1.299490, validation/accuracy=0.646020, validation/loss=1.700786, validation/num_examples=50000
I0127 16:53:17.192454 140026050483968 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.5310118198394775, loss=3.2788095474243164
I0127 16:53:51.115317 140026042091264 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.4445587396621704, loss=3.167776584625244
I0127 16:54:25.104447 140026050483968 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.4160094261169434, loss=3.2088303565979004
I0127 16:54:59.106152 140026042091264 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.3030544519424438, loss=3.1894476413726807
I0127 16:55:33.099117 140026050483968 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.5891311168670654, loss=3.299250602722168
I0127 16:56:07.107542 140026042091264 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.5156656503677368, loss=3.25754451751709
I0127 16:56:41.104886 140026050483968 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.3953688144683838, loss=3.2758214473724365
I0127 16:57:15.111468 140026042091264 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.5577465295791626, loss=3.194206714630127
I0127 16:57:49.135414 140026050483968 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.7100282907485962, loss=3.312675952911377
I0127 16:58:23.147311 140026042091264 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7151834964752197, loss=3.1905124187469482
I0127 16:58:57.195296 140026050483968 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.7906806468963623, loss=3.201303005218506
I0127 16:59:31.215984 140026042091264 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.4178487062454224, loss=3.2203712463378906
I0127 17:00:05.224472 140026050483968 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.5861655473709106, loss=3.146536111831665
I0127 17:00:39.206990 140026042091264 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.4957926273345947, loss=3.2289745807647705
I0127 17:01:13.183623 140026050483968 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.4492806196212769, loss=3.1954643726348877
I0127 17:01:36.452546 140187804313408 spec.py:321] Evaluating on the training split.
I0127 17:01:42.591839 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 17:01:51.673697 140187804313408 spec.py:349] Evaluating on the test split.
I0127 17:01:54.212077 140187804313408 submission_runner.py:408] Time since start: 13775.51s, 	Step: 38970, 	{'train/accuracy': 0.7347337007522583, 'train/loss': 1.2530560493469238, 'validation/accuracy': 0.6542199850082397, 'validation/loss': 1.6197998523712158, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.2923173904418945, 'test/num_examples': 10000, 'score': 13293.508579730988, 'total_duration': 13775.507248878479, 'accumulated_submission_time': 13293.508579730988, 'accumulated_eval_time': 479.77757358551025, 'accumulated_logging_time': 0.8264029026031494}
I0127 17:01:54.235962 140026151130880 logging_writer.py:48] [38970] accumulated_eval_time=479.777574, accumulated_logging_time=0.826403, accumulated_submission_time=13293.508580, global_step=38970, preemption_count=0, score=13293.508580, test/accuracy=0.521900, test/loss=2.292317, test/num_examples=10000, total_duration=13775.507249, train/accuracy=0.734734, train/loss=1.253056, validation/accuracy=0.654220, validation/loss=1.619800, validation/num_examples=50000
I0127 17:02:04.774541 140026159523584 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.441681146621704, loss=3.137479782104492
I0127 17:02:38.740568 140026151130880 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.604345679283142, loss=3.148379325866699
I0127 17:03:12.703655 140026159523584 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.6753586530685425, loss=3.173917293548584
I0127 17:03:46.716793 140026151130880 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.5565561056137085, loss=3.209428310394287
I0127 17:04:20.702272 140026159523584 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.5243418216705322, loss=3.277982234954834
I0127 17:04:54.723273 140026151130880 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.6442514657974243, loss=3.207787275314331
I0127 17:05:28.763956 140026159523584 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.47329580783844, loss=3.164965867996216
I0127 17:06:02.751495 140026151130880 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.6657567024230957, loss=3.221432685852051
I0127 17:06:36.741423 140026159523584 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.6745705604553223, loss=3.2380926609039307
I0127 17:07:10.731390 140026151130880 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.518730878829956, loss=3.1889684200286865
I0127 17:07:44.730156 140026159523584 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.9357719421386719, loss=3.231266975402832
I0127 17:08:18.736010 140026151130880 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.6122868061065674, loss=3.1989314556121826
I0127 17:08:52.735895 140026159523584 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.5379949808120728, loss=3.1707022190093994
I0127 17:09:26.742785 140026151130880 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.7056444883346558, loss=3.2360315322875977
I0127 17:10:00.768310 140026159523584 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.573631763458252, loss=3.2293312549591064
I0127 17:10:24.379094 140187804313408 spec.py:321] Evaluating on the training split.
I0127 17:10:30.531880 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 17:10:39.299792 140187804313408 spec.py:349] Evaluating on the test split.
I0127 17:10:41.803885 140187804313408 submission_runner.py:408] Time since start: 14303.10s, 	Step: 40471, 	{'train/accuracy': 0.7262037396430969, 'train/loss': 1.325469732284546, 'validation/accuracy': 0.6515399813652039, 'validation/loss': 1.6518381834030151, 'validation/num_examples': 50000, 'test/accuracy': 0.5252000093460083, 'test/loss': 2.321791410446167, 'test/num_examples': 10000, 'score': 13803.586438894272, 'total_duration': 14303.099082946777, 'accumulated_submission_time': 13803.586438894272, 'accumulated_eval_time': 497.2023296356201, 'accumulated_logging_time': 0.8594932556152344}
I0127 17:10:41.829622 140026042091264 logging_writer.py:48] [40471] accumulated_eval_time=497.202330, accumulated_logging_time=0.859493, accumulated_submission_time=13803.586439, global_step=40471, preemption_count=0, score=13803.586439, test/accuracy=0.525200, test/loss=2.321791, test/num_examples=10000, total_duration=14303.099083, train/accuracy=0.726204, train/loss=1.325470, validation/accuracy=0.651540, validation/loss=1.651838, validation/num_examples=50000
I0127 17:10:52.018800 140026050483968 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.5502142906188965, loss=3.192441940307617
I0127 17:11:26.043679 140026042091264 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.654341220855713, loss=3.1441750526428223
I0127 17:12:00.039610 140026050483968 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.6540871858596802, loss=3.187568187713623
I0127 17:12:34.026431 140026042091264 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.574836015701294, loss=3.2562716007232666
I0127 17:13:08.006036 140026050483968 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.4658161401748657, loss=3.197470188140869
I0127 17:13:41.980990 140026042091264 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.4802054166793823, loss=3.1527862548828125
I0127 17:14:15.963424 140026050483968 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.444546103477478, loss=3.203742742538452
I0127 17:14:49.953366 140026042091264 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.6051958799362183, loss=3.1292977333068848
I0127 17:15:23.941431 140026050483968 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.2939655780792236, loss=3.2146339416503906
I0127 17:15:57.946766 140026042091264 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.5706313848495483, loss=3.1475114822387695
I0127 17:16:31.950547 140026050483968 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.7140840291976929, loss=3.2317471504211426
I0127 17:17:05.942110 140026042091264 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.5608681440353394, loss=3.1673574447631836
I0127 17:17:40.122543 140026050483968 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.4777787923812866, loss=3.1280150413513184
I0127 17:18:14.107784 140026042091264 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.5261963605880737, loss=3.1704626083374023
I0127 17:18:48.101581 140026050483968 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8470858335494995, loss=3.2079927921295166
I0127 17:19:12.045110 140187804313408 spec.py:321] Evaluating on the training split.
I0127 17:19:18.863434 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 17:19:27.916604 140187804313408 spec.py:349] Evaluating on the test split.
I0127 17:19:30.439572 140187804313408 submission_runner.py:408] Time since start: 14831.73s, 	Step: 41972, 	{'train/accuracy': 0.7281568646430969, 'train/loss': 1.2736696004867554, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.5901299715042114, 'validation/num_examples': 50000, 'test/accuracy': 0.5282000303268433, 'test/loss': 2.2479236125946045, 'test/num_examples': 10000, 'score': 14313.738857030869, 'total_duration': 14831.734774827957, 'accumulated_submission_time': 14313.738857030869, 'accumulated_eval_time': 515.5967583656311, 'accumulated_logging_time': 0.8947463035583496}
I0127 17:19:30.464737 140026159523584 logging_writer.py:48] [41972] accumulated_eval_time=515.596758, accumulated_logging_time=0.894746, accumulated_submission_time=14313.738857, global_step=41972, preemption_count=0, score=14313.738857, test/accuracy=0.528200, test/loss=2.247924, test/num_examples=10000, total_duration=14831.734775, train/accuracy=0.728157, train/loss=1.273670, validation/accuracy=0.658100, validation/loss=1.590130, validation/num_examples=50000
I0127 17:19:40.303245 140026167916288 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.5567330121994019, loss=3.1423418521881104
I0127 17:20:14.247900 140026159523584 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.7072948217391968, loss=3.226346015930176
I0127 17:20:48.208870 140026167916288 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.7581368684768677, loss=3.2200286388397217
I0127 17:21:22.206541 140026159523584 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6242780685424805, loss=3.1947455406188965
I0127 17:21:56.205088 140026167916288 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.8747979402542114, loss=3.1452090740203857
I0127 17:22:30.208409 140026159523584 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.5478788614273071, loss=3.1939733028411865
I0127 17:23:04.206592 140026167916288 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.7657968997955322, loss=3.1440372467041016
I0127 17:23:38.234046 140026159523584 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.5421255826950073, loss=3.199542284011841
I0127 17:24:12.333314 140026167916288 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.592409372329712, loss=3.116020441055298
I0127 17:24:46.344114 140026159523584 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.6369340419769287, loss=3.143193006515503
I0127 17:25:20.342246 140026167916288 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.5862762928009033, loss=3.1426186561584473
I0127 17:25:54.323361 140026159523584 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.5482839345932007, loss=3.1427299976348877
I0127 17:26:28.320626 140026167916288 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.778499960899353, loss=3.127922296524048
I0127 17:27:02.356845 140026159523584 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.6563559770584106, loss=3.1610260009765625
I0127 17:27:36.339830 140026167916288 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.7340786457061768, loss=3.259136915206909
I0127 17:28:00.625920 140187804313408 spec.py:321] Evaluating on the training split.
I0127 17:28:07.001683 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 17:28:15.912510 140187804313408 spec.py:349] Evaluating on the test split.
I0127 17:28:18.440973 140187804313408 submission_runner.py:408] Time since start: 15359.74s, 	Step: 43473, 	{'train/accuracy': 0.7159597873687744, 'train/loss': 1.3737213611602783, 'validation/accuracy': 0.6536200046539307, 'validation/loss': 1.6578227281570435, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.3332509994506836, 'test/num_examples': 10000, 'score': 14823.836078882217, 'total_duration': 15359.736160516739, 'accumulated_submission_time': 14823.836078882217, 'accumulated_eval_time': 533.4117612838745, 'accumulated_logging_time': 0.9289801120758057}
I0127 17:28:18.465465 140026167916288 logging_writer.py:48] [43473] accumulated_eval_time=533.411761, accumulated_logging_time=0.928980, accumulated_submission_time=14823.836079, global_step=43473, preemption_count=0, score=14823.836079, test/accuracy=0.523200, test/loss=2.333251, test/num_examples=10000, total_duration=15359.736161, train/accuracy=0.715960, train/loss=1.373721, validation/accuracy=0.653620, validation/loss=1.657823, validation/num_examples=50000
I0127 17:28:28.006755 140026176308992 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.5859780311584473, loss=3.248936891555786
I0127 17:29:01.939920 140026167916288 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.7585334777832031, loss=3.1713976860046387
I0127 17:29:35.896632 140026176308992 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.5874991416931152, loss=3.1531150341033936
I0127 17:30:09.932584 140026167916288 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.715644359588623, loss=3.1505696773529053
I0127 17:30:43.917065 140026176308992 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.6450268030166626, loss=3.235623836517334
I0127 17:31:17.900585 140026167916288 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.6429544687271118, loss=3.1990602016448975
I0127 17:31:51.869952 140026176308992 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6023428440093994, loss=3.2041170597076416
I0127 17:32:25.866843 140026167916288 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.5068423748016357, loss=3.1209421157836914
I0127 17:32:59.854546 140026176308992 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.887528896331787, loss=3.1205220222473145
I0127 17:33:33.856184 140026167916288 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.7036585807800293, loss=3.1835319995880127
I0127 17:34:07.813977 140026176308992 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.6871836185455322, loss=3.1618494987487793
I0127 17:34:41.822591 140026167916288 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6044071912765503, loss=3.1287553310394287
I0127 17:35:15.815273 140026176308992 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.5665048360824585, loss=3.149991989135742
I0127 17:35:49.853789 140026167916288 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.5310628414154053, loss=3.1285738945007324
I0127 17:36:23.912016 140026176308992 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.687397837638855, loss=3.1973185539245605
I0127 17:36:48.490801 140187804313408 spec.py:321] Evaluating on the training split.
I0127 17:36:54.619692 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 17:37:03.873748 140187804313408 spec.py:349] Evaluating on the test split.
I0127 17:37:06.351417 140187804313408 submission_runner.py:408] Time since start: 15887.65s, 	Step: 44974, 	{'train/accuracy': 0.7290138602256775, 'train/loss': 1.3344924449920654, 'validation/accuracy': 0.6631999611854553, 'validation/loss': 1.6209385395050049, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.273008108139038, 'test/num_examples': 10000, 'score': 15333.796644449234, 'total_duration': 15887.646605014801, 'accumulated_submission_time': 15333.796644449234, 'accumulated_eval_time': 551.2723331451416, 'accumulated_logging_time': 0.9648346900939941}
I0127 17:37:06.379247 140026067269376 logging_writer.py:48] [44974] accumulated_eval_time=551.272333, accumulated_logging_time=0.964835, accumulated_submission_time=15333.796644, global_step=44974, preemption_count=0, score=15333.796644, test/accuracy=0.532800, test/loss=2.273008, test/num_examples=10000, total_duration=15887.646605, train/accuracy=0.729014, train/loss=1.334492, validation/accuracy=0.663200, validation/loss=1.620939, validation/num_examples=50000
I0127 17:37:15.538555 140026075662080 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.804351568222046, loss=3.160827875137329
I0127 17:37:49.487823 140026067269376 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.6187529563903809, loss=3.2646141052246094
I0127 17:38:23.475010 140026075662080 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.5112519264221191, loss=3.24040150642395
I0127 17:38:57.491175 140026067269376 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.6579331159591675, loss=3.1898891925811768
I0127 17:39:31.473997 140026075662080 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.7538261413574219, loss=3.17875337600708
I0127 17:40:05.471300 140026067269376 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7020539045333862, loss=3.1609933376312256
I0127 17:40:39.459593 140026075662080 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.6798158884048462, loss=3.1778101921081543
I0127 17:41:13.439185 140026067269376 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.6056838035583496, loss=3.1517388820648193
I0127 17:41:47.429571 140026075662080 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.7400660514831543, loss=3.2213268280029297
I0127 17:42:21.425057 140026067269376 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.839743733406067, loss=3.234926700592041
I0127 17:42:55.457438 140026075662080 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.776295781135559, loss=3.1985933780670166
I0127 17:43:29.442977 140026067269376 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.7609087228775024, loss=3.1450395584106445
I0127 17:44:03.459377 140026075662080 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.6500325202941895, loss=3.203779935836792
I0127 17:44:37.456717 140026067269376 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6748071908950806, loss=3.125009536743164
I0127 17:45:11.437747 140026075662080 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.7026530504226685, loss=3.1453192234039307
I0127 17:45:36.409935 140187804313408 spec.py:321] Evaluating on the training split.
I0127 17:45:42.535215 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 17:45:51.315145 140187804313408 spec.py:349] Evaluating on the test split.
I0127 17:45:53.843014 140187804313408 submission_runner.py:408] Time since start: 16415.14s, 	Step: 46475, 	{'train/accuracy': 0.75882887840271, 'train/loss': 1.2321513891220093, 'validation/accuracy': 0.6564799547195435, 'validation/loss': 1.6605032682418823, 'validation/num_examples': 50000, 'test/accuracy': 0.527999997138977, 'test/loss': 2.3292038440704346, 'test/num_examples': 10000, 'score': 15843.76311159134, 'total_duration': 16415.138216257095, 'accumulated_submission_time': 15843.76311159134, 'accumulated_eval_time': 568.7053790092468, 'accumulated_logging_time': 1.002387285232544}
I0127 17:45:53.872310 140026151130880 logging_writer.py:48] [46475] accumulated_eval_time=568.705379, accumulated_logging_time=1.002387, accumulated_submission_time=15843.763112, global_step=46475, preemption_count=0, score=15843.763112, test/accuracy=0.528000, test/loss=2.329204, test/num_examples=10000, total_duration=16415.138216, train/accuracy=0.758829, train/loss=1.232151, validation/accuracy=0.656480, validation/loss=1.660503, validation/num_examples=50000
I0127 17:46:02.707968 140026159523584 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.92859947681427, loss=3.1460201740264893
I0127 17:46:36.619949 140026151130880 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.5719335079193115, loss=3.1389923095703125
I0127 17:47:10.601600 140026159523584 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.6132198572158813, loss=3.1940503120422363
I0127 17:47:44.570043 140026151130880 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.6139541864395142, loss=3.1215057373046875
I0127 17:48:18.579768 140026159523584 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.7982215881347656, loss=3.1408658027648926
I0127 17:48:52.611922 140026151130880 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.798356294631958, loss=3.1738011837005615
I0127 17:49:26.615951 140026159523584 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.6326361894607544, loss=3.1440181732177734
I0127 17:50:00.670334 140026151130880 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.7400621175765991, loss=3.184293270111084
I0127 17:50:34.683330 140026159523584 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8370981216430664, loss=3.1101479530334473
I0127 17:51:08.656481 140026151130880 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.5742355585098267, loss=3.0784595012664795
I0127 17:51:42.660701 140026159523584 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.943495750427246, loss=3.167156934738159
I0127 17:52:16.644585 140026151130880 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.042832136154175, loss=3.1490166187286377
I0127 17:52:50.647612 140026159523584 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.6416940689086914, loss=3.101628065109253
I0127 17:53:24.624226 140026151130880 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.583074688911438, loss=3.096980571746826
I0127 17:53:58.619462 140026159523584 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.8050010204315186, loss=3.175089120864868
I0127 17:54:23.916453 140187804313408 spec.py:321] Evaluating on the training split.
I0127 17:54:30.076614 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 17:54:39.083493 140187804313408 spec.py:349] Evaluating on the test split.
I0127 17:54:41.610088 140187804313408 submission_runner.py:408] Time since start: 16942.91s, 	Step: 47976, 	{'train/accuracy': 0.7443000674247742, 'train/loss': 1.2563570737838745, 'validation/accuracy': 0.6606599688529968, 'validation/loss': 1.619337797164917, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.2724545001983643, 'test/num_examples': 10000, 'score': 16353.741973161697, 'total_duration': 16942.905286073685, 'accumulated_submission_time': 16353.741973161697, 'accumulated_eval_time': 586.3989787101746, 'accumulated_logging_time': 1.043410301208496}
I0127 17:54:41.638659 140026050483968 logging_writer.py:48] [47976] accumulated_eval_time=586.398979, accumulated_logging_time=1.043410, accumulated_submission_time=16353.741973, global_step=47976, preemption_count=0, score=16353.741973, test/accuracy=0.536100, test/loss=2.272455, test/num_examples=10000, total_duration=16942.905286, train/accuracy=0.744300, train/loss=1.256357, validation/accuracy=0.660660, validation/loss=1.619338, validation/num_examples=50000
I0127 17:54:50.146257 140026058876672 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.9615919589996338, loss=3.1910457611083984
I0127 17:55:24.153201 140026050483968 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.7565665245056152, loss=3.1467397212982178
I0127 17:55:58.148023 140026058876672 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.744499921798706, loss=3.115983009338379
I0127 17:56:32.151987 140026050483968 logging_writer.py:48] [48300] global_step=48300, grad_norm=2.017266273498535, loss=3.2141005992889404
I0127 17:57:06.133022 140026058876672 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.7154371738433838, loss=3.1604161262512207
I0127 17:57:40.130200 140026050483968 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.7332146167755127, loss=3.122483968734741
I0127 17:58:14.146381 140026058876672 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.7371622323989868, loss=3.201979637145996
I0127 17:58:48.148698 140026050483968 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.8193531036376953, loss=3.153931140899658
I0127 17:59:22.139013 140026058876672 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.5951085090637207, loss=3.1828627586364746
I0127 17:59:56.168592 140026050483968 logging_writer.py:48] [48900] global_step=48900, grad_norm=2.0144550800323486, loss=3.1329076290130615
I0127 18:00:30.175622 140026058876672 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.5968353748321533, loss=3.114173650741577
I0127 18:01:04.154591 140026050483968 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.8301548957824707, loss=3.151620388031006
I0127 18:01:38.265162 140026058876672 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.717926263809204, loss=3.1247243881225586
I0127 18:02:12.284812 140026050483968 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.7723917961120605, loss=3.100048780441284
I0127 18:02:46.253358 140026058876672 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.610575556755066, loss=3.0889358520507812
I0127 18:03:11.898328 140187804313408 spec.py:321] Evaluating on the training split.
I0127 18:03:18.183795 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 18:03:27.240792 140187804313408 spec.py:349] Evaluating on the test split.
I0127 18:03:29.717573 140187804313408 submission_runner.py:408] Time since start: 17471.01s, 	Step: 49477, 	{'train/accuracy': 0.73539137840271, 'train/loss': 1.2704975605010986, 'validation/accuracy': 0.6642000079154968, 'validation/loss': 1.5893797874450684, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.2808008193969727, 'test/num_examples': 10000, 'score': 16863.93771147728, 'total_duration': 17471.012765169144, 'accumulated_submission_time': 16863.93771147728, 'accumulated_eval_time': 604.2181794643402, 'accumulated_logging_time': 1.0814259052276611}
I0127 18:03:29.742412 140026042091264 logging_writer.py:48] [49477] accumulated_eval_time=604.218179, accumulated_logging_time=1.081426, accumulated_submission_time=16863.937711, global_step=49477, preemption_count=0, score=16863.937711, test/accuracy=0.527200, test/loss=2.280801, test/num_examples=10000, total_duration=17471.012765, train/accuracy=0.735391, train/loss=1.270498, validation/accuracy=0.664200, validation/loss=1.589380, validation/num_examples=50000
I0127 18:03:37.897134 140026050483968 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7334933280944824, loss=3.0808284282684326
I0127 18:04:11.833837 140026042091264 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.8205772638320923, loss=3.165821075439453
I0127 18:04:45.796874 140026050483968 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.6921740770339966, loss=3.1288537979125977
I0127 18:05:19.796839 140026042091264 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.7683771848678589, loss=3.128671169281006
I0127 18:05:53.789026 140026050483968 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.7257282733917236, loss=3.2016022205352783
I0127 18:06:27.783328 140026042091264 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7067395448684692, loss=3.212737798690796
I0127 18:07:01.788419 140026050483968 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.7719823122024536, loss=3.1509430408477783
I0127 18:07:35.851680 140026042091264 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.8344058990478516, loss=3.163848400115967
I0127 18:08:09.850955 140026050483968 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7465214729309082, loss=3.0881729125976562
I0127 18:08:43.865923 140026042091264 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.861098289489746, loss=3.0813822746276855
I0127 18:09:17.844168 140026050483968 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.7313302755355835, loss=3.17447566986084
I0127 18:09:51.841118 140026042091264 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.775403618812561, loss=3.1712279319763184
I0127 18:10:25.849403 140026050483968 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.8238180875778198, loss=3.120689868927002
I0127 18:10:59.828886 140026042091264 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.121403932571411, loss=3.254110813140869
I0127 18:11:33.815620 140026050483968 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.872868537902832, loss=3.1720101833343506
I0127 18:11:59.800528 140187804313408 spec.py:321] Evaluating on the training split.
I0127 18:12:06.167137 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 18:12:15.147627 140187804313408 spec.py:349] Evaluating on the test split.
I0127 18:12:17.630367 140187804313408 submission_runner.py:408] Time since start: 17998.93s, 	Step: 50978, 	{'train/accuracy': 0.7208425998687744, 'train/loss': 1.3237276077270508, 'validation/accuracy': 0.6489999890327454, 'validation/loss': 1.642844319343567, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.2644474506378174, 'test/num_examples': 10000, 'score': 17373.93242096901, 'total_duration': 17998.925570249557, 'accumulated_submission_time': 17373.93242096901, 'accumulated_eval_time': 622.0479846000671, 'accumulated_logging_time': 1.1158974170684814}
I0127 18:12:17.658007 140026050483968 logging_writer.py:48] [50978] accumulated_eval_time=622.047985, accumulated_logging_time=1.115897, accumulated_submission_time=17373.932421, global_step=50978, preemption_count=0, score=17373.932421, test/accuracy=0.526100, test/loss=2.264447, test/num_examples=10000, total_duration=17998.925570, train/accuracy=0.720843, train/loss=1.323728, validation/accuracy=0.649000, validation/loss=1.642844, validation/num_examples=50000
I0127 18:12:25.448337 140026159523584 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.789365291595459, loss=3.158123731613159
I0127 18:12:59.401381 140026050483968 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7119756937026978, loss=3.19541597366333
I0127 18:13:33.359216 140026159523584 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.8089427947998047, loss=3.1280367374420166
I0127 18:14:07.530330 140026050483968 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9566407203674316, loss=3.2087807655334473
I0127 18:14:41.505050 140026159523584 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.5969334840774536, loss=3.0966546535491943
I0127 18:15:15.498366 140026050483968 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.81077241897583, loss=3.1244728565216064
I0127 18:15:49.483516 140026159523584 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.983533501625061, loss=3.0992674827575684
I0127 18:16:23.484476 140026050483968 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.857952356338501, loss=3.158266067504883
I0127 18:16:57.516306 140026159523584 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.7603538036346436, loss=3.1724753379821777
I0127 18:17:31.517219 140026050483968 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.9901268482208252, loss=3.2226510047912598
I0127 18:18:05.510639 140026159523584 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.803946852684021, loss=3.092810869216919
I0127 18:18:39.491227 140026050483968 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.706115961074829, loss=3.1364855766296387
I0127 18:19:13.501117 140026159523584 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7303274869918823, loss=3.1192526817321777
I0127 18:19:47.473529 140026050483968 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.7403430938720703, loss=3.1340579986572266
I0127 18:20:21.595117 140026159523584 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.8568028211593628, loss=3.088214635848999
I0127 18:20:47.910759 140187804313408 spec.py:321] Evaluating on the training split.
I0127 18:20:54.096225 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 18:21:03.202350 140187804313408 spec.py:349] Evaluating on the test split.
I0127 18:21:05.713423 140187804313408 submission_runner.py:408] Time since start: 18527.01s, 	Step: 52479, 	{'train/accuracy': 0.7428650856018066, 'train/loss': 1.2034056186676025, 'validation/accuracy': 0.6698399782180786, 'validation/loss': 1.527472972869873, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 2.1574642658233643, 'test/num_examples': 10000, 'score': 17884.122307777405, 'total_duration': 18527.0086209774, 'accumulated_submission_time': 17884.122307777405, 'accumulated_eval_time': 639.8506090641022, 'accumulated_logging_time': 1.1524429321289062}
I0127 18:21:05.743260 140026075662080 logging_writer.py:48] [52479] accumulated_eval_time=639.850609, accumulated_logging_time=1.152443, accumulated_submission_time=17884.122308, global_step=52479, preemption_count=0, score=17884.122308, test/accuracy=0.547100, test/loss=2.157464, test/num_examples=10000, total_duration=18527.008621, train/accuracy=0.742865, train/loss=1.203406, validation/accuracy=0.669840, validation/loss=1.527473, validation/num_examples=50000
I0127 18:21:13.229708 140026151130880 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8357183933258057, loss=3.130025863647461
I0127 18:21:47.120815 140026075662080 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.7885655164718628, loss=3.1510589122772217
I0127 18:22:21.088333 140026151130880 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.7626694440841675, loss=3.143893241882324
I0127 18:22:55.086422 140026075662080 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.802156686782837, loss=3.122389793395996
I0127 18:23:29.084522 140026151130880 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.8763526678085327, loss=3.222507953643799
I0127 18:24:03.060312 140026075662080 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.7558256387710571, loss=3.1805052757263184
I0127 18:24:37.067844 140026151130880 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.6957736015319824, loss=3.1486189365386963
I0127 18:25:11.061818 140026075662080 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.7336899042129517, loss=3.181659460067749
I0127 18:25:45.058843 140026151130880 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.259659767150879, loss=3.1423115730285645
I0127 18:26:19.033173 140026075662080 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.7763689756393433, loss=3.157033920288086
I0127 18:26:53.072809 140026151130880 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.831782579421997, loss=3.117936849594116
I0127 18:27:27.079708 140026075662080 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.884730339050293, loss=3.171692132949829
I0127 18:28:01.055478 140026151130880 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.8085728883743286, loss=3.174391984939575
I0127 18:28:35.052465 140026075662080 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.9010889530181885, loss=3.1280274391174316
I0127 18:29:09.028446 140026151130880 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.875915288925171, loss=3.128946542739868
I0127 18:29:36.032836 140187804313408 spec.py:321] Evaluating on the training split.
I0127 18:29:42.134831 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 18:29:51.320796 140187804313408 spec.py:349] Evaluating on the test split.
I0127 18:29:53.721980 140187804313408 submission_runner.py:408] Time since start: 19055.02s, 	Step: 53981, 	{'train/accuracy': 0.7333585619926453, 'train/loss': 1.3001765012741089, 'validation/accuracy': 0.6660999655723572, 'validation/loss': 1.604732871055603, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2656149864196777, 'test/num_examples': 10000, 'score': 18394.34732890129, 'total_duration': 19055.01717185974, 'accumulated_submission_time': 18394.34732890129, 'accumulated_eval_time': 657.5397083759308, 'accumulated_logging_time': 1.1924428939819336}
I0127 18:29:53.752778 140026067269376 logging_writer.py:48] [53981] accumulated_eval_time=657.539708, accumulated_logging_time=1.192443, accumulated_submission_time=18394.347329, global_step=53981, preemption_count=0, score=18394.347329, test/accuracy=0.538600, test/loss=2.265615, test/num_examples=10000, total_duration=19055.017172, train/accuracy=0.733359, train/loss=1.300177, validation/accuracy=0.666100, validation/loss=1.604733, validation/num_examples=50000
I0127 18:30:00.544863 140026075662080 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8711098432540894, loss=3.100419044494629
I0127 18:30:34.490054 140026067269376 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.853398323059082, loss=3.206144332885742
I0127 18:31:08.448707 140026075662080 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8635278940200806, loss=3.1070966720581055
I0127 18:31:42.447982 140026067269376 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.8938195705413818, loss=3.113293170928955
I0127 18:32:16.442362 140026075662080 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.7796827554702759, loss=3.1828155517578125
I0127 18:32:50.595995 140026067269376 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.880606770515442, loss=3.091367483139038
I0127 18:33:24.582949 140026075662080 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.7647812366485596, loss=3.1191728115081787
I0127 18:33:58.562248 140026067269376 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.7050435543060303, loss=3.153075695037842
I0127 18:34:32.546967 140026075662080 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.858980655670166, loss=3.1233415603637695
I0127 18:35:06.537010 140026067269376 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.867461919784546, loss=3.1910195350646973
I0127 18:35:40.521320 140026075662080 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.7238608598709106, loss=3.1099672317504883
I0127 18:36:14.512896 140026067269376 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.9098321199417114, loss=3.146190643310547
I0127 18:36:48.493240 140026075662080 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.854539394378662, loss=3.1150174140930176
I0127 18:37:22.492733 140026067269376 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.9622927904129028, loss=3.0854291915893555
I0127 18:37:56.500218 140026075662080 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.9093722105026245, loss=3.0819737911224365
I0127 18:38:23.855003 140187804313408 spec.py:321] Evaluating on the training split.
I0127 18:38:30.110693 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 18:38:38.916286 140187804313408 spec.py:349] Evaluating on the test split.
I0127 18:38:41.435689 140187804313408 submission_runner.py:408] Time since start: 19582.73s, 	Step: 55482, 	{'train/accuracy': 0.7835220098495483, 'train/loss': 1.07129967212677, 'validation/accuracy': 0.6714000105857849, 'validation/loss': 1.5408074855804443, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.194451332092285, 'test/num_examples': 10000, 'score': 18904.384672641754, 'total_duration': 19582.730887413025, 'accumulated_submission_time': 18904.384672641754, 'accumulated_eval_time': 675.1203720569611, 'accumulated_logging_time': 1.2325246334075928}
I0127 18:38:41.465955 140026058876672 logging_writer.py:48] [55482] accumulated_eval_time=675.120372, accumulated_logging_time=1.232525, accumulated_submission_time=18904.384673, global_step=55482, preemption_count=0, score=18904.384673, test/accuracy=0.545100, test/loss=2.194451, test/num_examples=10000, total_duration=19582.730887, train/accuracy=0.783522, train/loss=1.071300, validation/accuracy=0.671400, validation/loss=1.540807, validation/num_examples=50000
I0127 18:38:47.911538 140026067269376 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.824581265449524, loss=3.1477537155151367
I0127 18:39:21.997698 140026058876672 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.964556336402893, loss=3.1386332511901855
I0127 18:39:55.933305 140026067269376 logging_writer.py:48] [55700] global_step=55700, grad_norm=2.050184488296509, loss=3.170898199081421
I0127 18:40:29.904048 140026058876672 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.8358237743377686, loss=3.1110880374908447
I0127 18:41:03.884107 140026067269376 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.948964238166809, loss=3.160125970840454
I0127 18:41:37.896447 140026058876672 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.9102680683135986, loss=3.1287574768066406
I0127 18:42:11.867849 140026067269376 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.8487348556518555, loss=3.183178424835205
I0127 18:42:45.866369 140026058876672 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.9024001359939575, loss=3.1757614612579346
I0127 18:43:19.835565 140026067269376 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8546205759048462, loss=3.1124796867370605
I0127 18:43:53.853533 140026058876672 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.846732497215271, loss=3.1166574954986572
I0127 18:44:27.802889 140026067269376 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.251235246658325, loss=3.1686818599700928
I0127 18:45:01.780008 140026058876672 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.091813564300537, loss=3.0951955318450928
I0127 18:45:35.872705 140026067269376 logging_writer.py:48] [56700] global_step=56700, grad_norm=2.0049328804016113, loss=3.0879626274108887
I0127 18:46:09.869013 140026058876672 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.973060965538025, loss=3.177381992340088
I0127 18:46:43.853917 140026067269376 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.9175912141799927, loss=3.2090184688568115
I0127 18:47:11.553495 140187804313408 spec.py:321] Evaluating on the training split.
I0127 18:47:17.784370 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 18:47:26.856677 140187804313408 spec.py:349] Evaluating on the test split.
I0127 18:47:29.353075 140187804313408 submission_runner.py:408] Time since start: 20110.65s, 	Step: 56983, 	{'train/accuracy': 0.753926157951355, 'train/loss': 1.2177770137786865, 'validation/accuracy': 0.6661799550056458, 'validation/loss': 1.6002877950668335, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.25341796875, 'test/num_examples': 10000, 'score': 19414.409068584442, 'total_duration': 20110.64827489853, 'accumulated_submission_time': 19414.409068584442, 'accumulated_eval_time': 692.9199199676514, 'accumulated_logging_time': 1.2718331813812256}
I0127 18:47:29.381123 140026050483968 logging_writer.py:48] [56983] accumulated_eval_time=692.919920, accumulated_logging_time=1.271833, accumulated_submission_time=19414.409069, global_step=56983, preemption_count=0, score=19414.409069, test/accuracy=0.539200, test/loss=2.253418, test/num_examples=10000, total_duration=20110.648275, train/accuracy=0.753926, train/loss=1.217777, validation/accuracy=0.666180, validation/loss=1.600288, validation/num_examples=50000
I0127 18:47:35.476399 140026058876672 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9598909616470337, loss=3.1901180744171143
I0127 18:48:09.423406 140026050483968 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8629674911499023, loss=3.1172213554382324
I0127 18:48:43.383031 140026058876672 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.764168620109558, loss=3.0432944297790527
I0127 18:49:17.414533 140026050483968 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.4272983074188232, loss=3.152977466583252
I0127 18:49:51.398940 140026058876672 logging_writer.py:48] [57400] global_step=57400, grad_norm=2.028379440307617, loss=3.1973319053649902
I0127 18:50:25.388777 140026050483968 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.953826904296875, loss=3.1822686195373535
I0127 18:50:59.372381 140026058876672 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.9621412754058838, loss=3.114441156387329
I0127 18:51:33.540749 140026050483968 logging_writer.py:48] [57700] global_step=57700, grad_norm=2.1603803634643555, loss=3.1269068717956543
I0127 18:52:07.534733 140026058876672 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.9378701448440552, loss=3.0895802974700928
I0127 18:52:41.548991 140026050483968 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8894613981246948, loss=3.1490073204040527
I0127 18:53:15.539806 140026058876672 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.9142467975616455, loss=3.142648935317993
I0127 18:53:49.541208 140026050483968 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.1052982807159424, loss=3.1934354305267334
I0127 18:54:23.533733 140026058876672 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.8690649271011353, loss=3.206198215484619
I0127 18:54:57.524628 140026050483968 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.9238907098770142, loss=3.1263315677642822
I0127 18:55:31.530577 140026058876672 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.909470796585083, loss=3.090297222137451
I0127 18:55:59.571798 140187804313408 spec.py:321] Evaluating on the training split.
I0127 18:56:05.917659 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 18:56:14.927312 140187804313408 spec.py:349] Evaluating on the test split.
I0127 18:56:17.393915 140187804313408 submission_runner.py:408] Time since start: 20638.69s, 	Step: 58484, 	{'train/accuracy': 0.7466916441917419, 'train/loss': 1.2169824838638306, 'validation/accuracy': 0.6627799868583679, 'validation/loss': 1.5723552703857422, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.232109785079956, 'test/num_examples': 10000, 'score': 19924.53598690033, 'total_duration': 20638.689115047455, 'accumulated_submission_time': 19924.53598690033, 'accumulated_eval_time': 710.7420144081116, 'accumulated_logging_time': 1.3092410564422607}
I0127 18:56:17.421810 140026058876672 logging_writer.py:48] [58484] accumulated_eval_time=710.742014, accumulated_logging_time=1.309241, accumulated_submission_time=19924.535987, global_step=58484, preemption_count=0, score=19924.535987, test/accuracy=0.541200, test/loss=2.232110, test/num_examples=10000, total_duration=20638.689115, train/accuracy=0.746692, train/loss=1.216982, validation/accuracy=0.662780, validation/loss=1.572355, validation/num_examples=50000
I0127 18:56:23.178042 140026067269376 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8610135316848755, loss=3.1018097400665283
I0127 18:56:57.090400 140026058876672 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.7791473865509033, loss=3.0792577266693115
I0127 18:57:31.038739 140026067269376 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.7305657863616943, loss=3.0885398387908936
I0127 18:58:05.186587 140026058876672 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.9158825874328613, loss=3.0709784030914307
I0127 18:58:39.211577 140026067269376 logging_writer.py:48] [58900] global_step=58900, grad_norm=2.0229973793029785, loss=3.1338348388671875
I0127 18:59:13.206446 140026058876672 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9435352087020874, loss=3.1089577674865723
I0127 18:59:47.183742 140026067269376 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.9345461130142212, loss=3.0736236572265625
I0127 19:00:21.180210 140026058876672 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9353550672531128, loss=3.199183940887451
I0127 19:00:55.185401 140026067269376 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.7732019424438477, loss=3.087646245956421
I0127 19:01:29.189192 140026058876672 logging_writer.py:48] [59400] global_step=59400, grad_norm=2.0494256019592285, loss=3.173452615737915
I0127 19:02:03.141402 140026067269376 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.0308098793029785, loss=3.1646268367767334
I0127 19:02:37.138477 140026058876672 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.8717750310897827, loss=3.121828317642212
I0127 19:03:11.145715 140026067269376 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.156515121459961, loss=3.0707340240478516
I0127 19:03:45.143208 140026058876672 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.144657850265503, loss=3.078556537628174
I0127 19:04:19.237916 140026067269376 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.9681708812713623, loss=3.1236724853515625
I0127 19:04:47.570474 140187804313408 spec.py:321] Evaluating on the training split.
I0127 19:04:53.805654 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 19:05:02.840912 140187804313408 spec.py:349] Evaluating on the test split.
I0127 19:05:05.336643 140187804313408 submission_runner.py:408] Time since start: 21166.63s, 	Step: 59985, 	{'train/accuracy': 0.7434031963348389, 'train/loss': 1.2602286338806152, 'validation/accuracy': 0.6678599715232849, 'validation/loss': 1.5897306203842163, 'validation/num_examples': 50000, 'test/accuracy': 0.5350000262260437, 'test/loss': 2.256978988647461, 'test/num_examples': 10000, 'score': 20434.61958694458, 'total_duration': 21166.6318423748, 'accumulated_submission_time': 20434.61958694458, 'accumulated_eval_time': 728.5081448554993, 'accumulated_logging_time': 1.348177433013916}
I0127 19:05:05.366493 140026151130880 logging_writer.py:48] [59985] accumulated_eval_time=728.508145, accumulated_logging_time=1.348177, accumulated_submission_time=20434.619587, global_step=59985, preemption_count=0, score=20434.619587, test/accuracy=0.535000, test/loss=2.256979, test/num_examples=10000, total_duration=21166.631842, train/accuracy=0.743403, train/loss=1.260229, validation/accuracy=0.667860, validation/loss=1.589731, validation/num_examples=50000
I0127 19:05:10.789658 140026167916288 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.0527610778808594, loss=3.065629720687866
I0127 19:05:44.734069 140026151130880 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.9730297327041626, loss=3.1793830394744873
I0127 19:06:18.727267 140026167916288 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.079575538635254, loss=3.090517044067383
I0127 19:06:52.729116 140026151130880 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.9434151649475098, loss=3.119905471801758
I0127 19:07:26.725579 140026167916288 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9630324840545654, loss=3.1182732582092285
I0127 19:08:00.734736 140026151130880 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8488597869873047, loss=3.0228562355041504
I0127 19:08:34.712347 140026167916288 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.9987906217575073, loss=3.108747720718384
I0127 19:09:08.706934 140026151130880 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.9804433584213257, loss=3.056509494781494
I0127 19:09:42.686347 140026167916288 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.8558109998703003, loss=3.131298303604126
I0127 19:10:16.716809 140026151130880 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9061574935913086, loss=3.1494672298431396
I0127 19:10:50.698791 140026167916288 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.004560947418213, loss=3.095979690551758
I0127 19:11:24.691091 140026151130880 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.9180395603179932, loss=3.0445406436920166
I0127 19:11:58.709970 140026167916288 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.281775951385498, loss=3.149409294128418
I0127 19:12:32.725470 140026151130880 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.8788061141967773, loss=3.09737491607666
I0127 19:13:06.694930 140026167916288 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.2056727409362793, loss=3.1174182891845703
I0127 19:13:35.413141 140187804313408 spec.py:321] Evaluating on the training split.
I0127 19:13:41.552154 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 19:13:50.371685 140187804313408 spec.py:349] Evaluating on the test split.
I0127 19:13:52.983436 140187804313408 submission_runner.py:408] Time since start: 21694.28s, 	Step: 61486, 	{'train/accuracy': 0.7493622303009033, 'train/loss': 1.2253031730651855, 'validation/accuracy': 0.6746999621391296, 'validation/loss': 1.5467970371246338, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.180800676345825, 'test/num_examples': 10000, 'score': 20944.602358579636, 'total_duration': 21694.278636217117, 'accumulated_submission_time': 20944.602358579636, 'accumulated_eval_time': 746.0784072875977, 'accumulated_logging_time': 1.3884241580963135}
I0127 19:13:53.013525 140026067269376 logging_writer.py:48] [61486] accumulated_eval_time=746.078407, accumulated_logging_time=1.388424, accumulated_submission_time=20944.602359, global_step=61486, preemption_count=0, score=20944.602359, test/accuracy=0.554100, test/loss=2.180801, test/num_examples=10000, total_duration=21694.278636, train/accuracy=0.749362, train/loss=1.225303, validation/accuracy=0.674700, validation/loss=1.546797, validation/num_examples=50000
I0127 19:13:58.103043 140026075662080 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.9713047742843628, loss=3.160658359527588
I0127 19:14:32.033623 140026067269376 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.0918807983398438, loss=3.1258792877197266
I0127 19:15:05.983822 140026075662080 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.8932583332061768, loss=3.0783212184906006
I0127 19:15:39.972875 140026067269376 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.922595739364624, loss=3.093714952468872
I0127 19:16:13.971216 140026075662080 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.9428199529647827, loss=3.0652050971984863
I0127 19:16:48.048362 140026067269376 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.9252963066101074, loss=3.1292617321014404
I0127 19:17:22.004497 140026075662080 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.0644121170043945, loss=3.1293201446533203
I0127 19:17:56.009842 140026067269376 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.1295506954193115, loss=3.127152681350708
I0127 19:18:30.001819 140026075662080 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.9254897832870483, loss=3.0880126953125
I0127 19:19:04.008475 140026067269376 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.1771702766418457, loss=3.1261215209960938
I0127 19:19:37.988249 140026075662080 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.0812289714813232, loss=3.088167905807495
I0127 19:20:11.985634 140026067269376 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.009976625442505, loss=3.0190348625183105
I0127 19:20:45.967757 140026075662080 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.1696507930755615, loss=3.2603631019592285
I0127 19:21:19.978412 140026067269376 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.006718397140503, loss=3.0227978229522705
I0127 19:21:53.977462 140026075662080 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.9904816150665283, loss=3.0504331588745117
I0127 19:22:22.997300 140187804313408 spec.py:321] Evaluating on the training split.
I0127 19:22:29.310232 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 19:22:38.318520 140187804313408 spec.py:349] Evaluating on the test split.
I0127 19:22:40.971553 140187804313408 submission_runner.py:408] Time since start: 22222.27s, 	Step: 62987, 	{'train/accuracy': 0.7569953799247742, 'train/loss': 1.1516437530517578, 'validation/accuracy': 0.6823599934577942, 'validation/loss': 1.4813121557235718, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.1404056549072266, 'test/num_examples': 10000, 'score': 21454.522994041443, 'total_duration': 22222.266753673553, 'accumulated_submission_time': 21454.522994041443, 'accumulated_eval_time': 764.052640914917, 'accumulated_logging_time': 1.428706407546997}
I0127 19:22:41.000105 140026151130880 logging_writer.py:48] [62987] accumulated_eval_time=764.052641, accumulated_logging_time=1.428706, accumulated_submission_time=21454.522994, global_step=62987, preemption_count=0, score=21454.522994, test/accuracy=0.551500, test/loss=2.140406, test/num_examples=10000, total_duration=22222.266754, train/accuracy=0.756995, train/loss=1.151644, validation/accuracy=0.682360, validation/loss=1.481312, validation/num_examples=50000
I0127 19:22:45.755668 140026167916288 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.9537124633789062, loss=3.15950345993042
I0127 19:23:19.763273 140026151130880 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.010725736618042, loss=3.0985324382781982
I0127 19:23:53.744519 140026167916288 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.9637545347213745, loss=3.093590021133423
I0127 19:24:27.729656 140026151130880 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.9754568338394165, loss=3.0517733097076416
I0127 19:25:01.726554 140026167916288 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.9631760120391846, loss=3.1290900707244873
I0127 19:25:35.723323 140026151130880 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.076549768447876, loss=3.0505244731903076
I0127 19:26:09.723371 140026167916288 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8131400346755981, loss=2.9900219440460205
I0127 19:26:43.711740 140026151130880 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.029247283935547, loss=3.056811571121216
I0127 19:27:17.709165 140026167916288 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.0213396549224854, loss=3.142582893371582
I0127 19:27:51.709502 140026151130880 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.9876915216445923, loss=3.045743703842163
I0127 19:28:25.718667 140026167916288 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.9345879554748535, loss=3.0741312503814697
I0127 19:28:59.723325 140026151130880 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.269101142883301, loss=3.1538219451904297
I0127 19:29:33.835260 140026167916288 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.012148141860962, loss=3.106562376022339
I0127 19:30:07.808243 140026151130880 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.0735862255096436, loss=3.0548722743988037
I0127 19:30:41.803948 140026167916288 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.944732904434204, loss=3.072601318359375
I0127 19:31:11.182967 140187804313408 spec.py:321] Evaluating on the training split.
I0127 19:31:17.361622 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 19:31:26.492624 140187804313408 spec.py:349] Evaluating on the test split.
I0127 19:31:29.137186 140187804313408 submission_runner.py:408] Time since start: 22750.43s, 	Step: 64488, 	{'train/accuracy': 0.7630141973495483, 'train/loss': 1.167246699333191, 'validation/accuracy': 0.6773399710655212, 'validation/loss': 1.535491704940796, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.1842844486236572, 'test/num_examples': 10000, 'score': 21964.64013814926, 'total_duration': 22750.431900024414, 'accumulated_submission_time': 21964.64013814926, 'accumulated_eval_time': 782.0063388347626, 'accumulated_logging_time': 1.467320203781128}
I0127 19:31:29.169693 140026058876672 logging_writer.py:48] [64488] accumulated_eval_time=782.006339, accumulated_logging_time=1.467320, accumulated_submission_time=21964.640138, global_step=64488, preemption_count=0, score=21964.640138, test/accuracy=0.546700, test/loss=2.184284, test/num_examples=10000, total_duration=22750.431900, train/accuracy=0.763014, train/loss=1.167247, validation/accuracy=0.677340, validation/loss=1.535492, validation/num_examples=50000
I0127 19:31:33.580225 140026067269376 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.108506441116333, loss=3.2028260231018066
I0127 19:32:07.536069 140026058876672 logging_writer.py:48] [64600] global_step=64600, grad_norm=2.062983274459839, loss=3.0104761123657227
I0127 19:32:41.490873 140026067269376 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.0652835369110107, loss=3.1086909770965576
I0127 19:33:15.436463 140026058876672 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.2344768047332764, loss=3.057262659072876
I0127 19:33:49.432380 140026067269376 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9607049226760864, loss=3.111830234527588
I0127 19:34:23.421429 140026058876672 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.9551869630813599, loss=3.07301664352417
I0127 19:34:57.427769 140026067269376 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.116861581802368, loss=3.153974771499634
I0127 19:35:31.515619 140026058876672 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.9581631422042847, loss=3.0554986000061035
I0127 19:36:05.504238 140026067269376 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.0156664848327637, loss=3.1620798110961914
I0127 19:36:39.464960 140026058876672 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.033751964569092, loss=3.0310802459716797
I0127 19:37:17.257387 140026067269376 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.929806113243103, loss=3.0145363807678223
I0127 19:38:08.449406 140026058876672 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.0102741718292236, loss=3.14867901802063
I0127 19:38:42.388292 140026067269376 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.079573154449463, loss=3.147430896759033
I0127 19:39:16.384986 140026058876672 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.9223257303237915, loss=3.055220365524292
I0127 19:39:50.356577 140026067269376 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.0778326988220215, loss=3.0663981437683105
I0127 19:39:59.324253 140187804313408 spec.py:321] Evaluating on the training split.
I0127 19:40:05.648247 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 19:40:14.511625 140187804313408 spec.py:349] Evaluating on the test split.
I0127 19:40:16.959507 140187804313408 submission_runner.py:408] Time since start: 23278.25s, 	Step: 65928, 	{'train/accuracy': 0.7694514989852905, 'train/loss': 1.1408820152282715, 'validation/accuracy': 0.6707199811935425, 'validation/loss': 1.5621381998062134, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.232006549835205, 'test/num_examples': 10000, 'score': 22474.734596014023, 'total_duration': 23278.254689455032, 'accumulated_submission_time': 22474.734596014023, 'accumulated_eval_time': 799.6415371894836, 'accumulated_logging_time': 1.5086112022399902}
I0127 19:40:16.989478 140026159523584 logging_writer.py:48] [65928] accumulated_eval_time=799.641537, accumulated_logging_time=1.508611, accumulated_submission_time=22474.734596, global_step=65928, preemption_count=0, score=22474.734596, test/accuracy=0.543900, test/loss=2.232007, test/num_examples=10000, total_duration=23278.254689, train/accuracy=0.769451, train/loss=1.140882, validation/accuracy=0.670720, validation/loss=1.562138, validation/num_examples=50000
I0127 19:40:41.771123 140026167916288 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.010446310043335, loss=3.068152666091919
I0127 19:41:15.740598 140026159523584 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9638645648956299, loss=3.087805986404419
I0127 19:41:49.686853 140026167916288 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.985982894897461, loss=3.0933737754821777
I0127 19:42:23.740319 140026159523584 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.0315229892730713, loss=3.096311569213867
I0127 19:42:57.700627 140026167916288 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.080548048019409, loss=3.163968801498413
I0127 19:43:31.719635 140026159523584 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.0245361328125, loss=3.0669941902160645
I0127 19:44:05.679630 140026167916288 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.885616660118103, loss=3.0834100246429443
I0127 19:44:39.698029 140026159523584 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.0193307399749756, loss=3.031528949737549
I0127 19:45:13.648212 140026167916288 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.0605037212371826, loss=3.032499313354492
I0127 19:45:47.655635 140026159523584 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.1449365615844727, loss=3.1054773330688477
I0127 19:46:21.632968 140026167916288 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.8957679271697998, loss=3.0258750915527344
I0127 19:46:55.643373 140026159523584 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.0660922527313232, loss=3.097137928009033
I0127 19:47:29.562543 140026167916288 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.22505259513855, loss=3.0794613361358643
I0127 19:48:03.570134 140026159523584 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.0339465141296387, loss=3.1112208366394043
I0127 19:48:37.735347 140026167916288 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.921061396598816, loss=3.023679733276367
I0127 19:48:47.043186 140187804313408 spec.py:321] Evaluating on the training split.
I0127 19:48:53.182008 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 19:49:01.959347 140187804313408 spec.py:349] Evaluating on the test split.
I0127 19:49:04.566708 140187804313408 submission_runner.py:408] Time since start: 23805.86s, 	Step: 67429, 	{'train/accuracy': 0.7701291441917419, 'train/loss': 1.1213020086288452, 'validation/accuracy': 0.6814000010490417, 'validation/loss': 1.508509635925293, 'validation/num_examples': 50000, 'test/accuracy': 0.551800012588501, 'test/loss': 2.163882255554199, 'test/num_examples': 10000, 'score': 22984.724188804626, 'total_duration': 23805.86190366745, 'accumulated_submission_time': 22984.724188804626, 'accumulated_eval_time': 817.1650323867798, 'accumulated_logging_time': 1.5480930805206299}
I0127 19:49:04.599433 140026067269376 logging_writer.py:48] [67429] accumulated_eval_time=817.165032, accumulated_logging_time=1.548093, accumulated_submission_time=22984.724189, global_step=67429, preemption_count=0, score=22984.724189, test/accuracy=0.551800, test/loss=2.163882, test/num_examples=10000, total_duration=23805.861904, train/accuracy=0.770129, train/loss=1.121302, validation/accuracy=0.681400, validation/loss=1.508510, validation/num_examples=50000
I0127 19:49:29.029283 140026075662080 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.9500460624694824, loss=3.0474750995635986
I0127 19:50:02.945687 140026067269376 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.04742431640625, loss=3.0985310077667236
I0127 19:50:36.909605 140026075662080 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.1221072673797607, loss=3.0709710121154785
I0127 19:51:10.893472 140026067269376 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.0507500171661377, loss=3.1493663787841797
I0127 19:51:44.880504 140026075662080 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.076075315475464, loss=3.124978542327881
I0127 19:52:18.882352 140026067269376 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.0684335231781006, loss=3.039198398590088
I0127 19:52:52.865365 140026075662080 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.0973403453826904, loss=3.11865234375
I0127 19:53:26.841325 140026067269376 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.106550693511963, loss=2.9825427532196045
I0127 19:54:00.831472 140026075662080 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.069013833999634, loss=3.0749154090881348
I0127 19:54:34.921151 140026067269376 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.0193240642547607, loss=3.0952816009521484
I0127 19:55:08.885452 140026075662080 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.0830307006835938, loss=3.0467443466186523
I0127 19:55:42.868323 140026067269376 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.9699366092681885, loss=3.040304183959961
I0127 19:56:16.838920 140026075662080 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.196928024291992, loss=3.0755903720855713
I0127 19:56:50.781363 140026067269376 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0770838260650635, loss=3.0375452041625977
I0127 19:57:24.743377 140026075662080 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.1208207607269287, loss=3.0870933532714844
I0127 19:57:34.738265 140187804313408 spec.py:321] Evaluating on the training split.
I0127 19:57:40.865081 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 19:57:49.774127 140187804313408 spec.py:349] Evaluating on the test split.
I0127 19:57:52.259851 140187804313408 submission_runner.py:408] Time since start: 24333.56s, 	Step: 68931, 	{'train/accuracy': 0.7631736397743225, 'train/loss': 1.1109957695007324, 'validation/accuracy': 0.6834200024604797, 'validation/loss': 1.4736833572387695, 'validation/num_examples': 50000, 'test/accuracy': 0.5585000514984131, 'test/loss': 2.107130527496338, 'test/num_examples': 10000, 'score': 23494.799648284912, 'total_duration': 24333.555045366287, 'accumulated_submission_time': 23494.799648284912, 'accumulated_eval_time': 834.686586856842, 'accumulated_logging_time': 1.5904114246368408}
I0127 19:57:52.292238 140026167916288 logging_writer.py:48] [68931] accumulated_eval_time=834.686587, accumulated_logging_time=1.590411, accumulated_submission_time=23494.799648, global_step=68931, preemption_count=0, score=23494.799648, test/accuracy=0.558500, test/loss=2.107131, test/num_examples=10000, total_duration=24333.555045, train/accuracy=0.763174, train/loss=1.110996, validation/accuracy=0.683420, validation/loss=1.473683, validation/num_examples=50000
I0127 19:58:16.079097 140026176308992 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.233860969543457, loss=3.115246534347534
I0127 19:58:50.023599 140026167916288 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.1049857139587402, loss=3.031073808670044
I0127 19:59:24.018003 140026176308992 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.1839981079101562, loss=2.9798641204833984
I0127 19:59:58.006946 140026167916288 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.0450942516326904, loss=3.0823514461517334
I0127 20:00:32.013594 140026176308992 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.077300786972046, loss=3.1019949913024902
I0127 20:01:06.043711 140026167916288 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.2029595375061035, loss=3.0966014862060547
I0127 20:01:40.039621 140026176308992 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.2726261615753174, loss=3.079190492630005
I0127 20:02:14.030874 140026167916288 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.0435237884521484, loss=3.0333192348480225
I0127 20:02:48.024488 140026176308992 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.2002627849578857, loss=3.0825302600860596
I0127 20:03:22.017135 140026167916288 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.056833505630493, loss=3.0413241386413574
I0127 20:03:56.007500 140026176308992 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.9988020658493042, loss=3.0742347240448
I0127 20:04:30.008589 140026167916288 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.9327774047851562, loss=3.0745294094085693
I0127 20:05:03.983370 140026176308992 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.049234390258789, loss=2.971653938293457
I0127 20:05:37.993112 140026167916288 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.1006758213043213, loss=3.0520853996276855
I0127 20:06:11.971492 140026176308992 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.2194178104400635, loss=3.1553192138671875
I0127 20:06:22.319678 140187804313408 spec.py:321] Evaluating on the training split.
I0127 20:06:28.463590 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 20:06:37.295620 140187804313408 spec.py:349] Evaluating on the test split.
I0127 20:06:39.802156 140187804313408 submission_runner.py:408] Time since start: 24861.10s, 	Step: 70432, 	{'train/accuracy': 0.7593470811843872, 'train/loss': 1.125109314918518, 'validation/accuracy': 0.6784999966621399, 'validation/loss': 1.4743475914001465, 'validation/num_examples': 50000, 'test/accuracy': 0.554900050163269, 'test/loss': 2.1304423809051514, 'test/num_examples': 10000, 'score': 24004.760673046112, 'total_duration': 24861.097346305847, 'accumulated_submission_time': 24004.760673046112, 'accumulated_eval_time': 852.1690158843994, 'accumulated_logging_time': 1.633352518081665}
I0127 20:06:39.831909 140026075662080 logging_writer.py:48] [70432] accumulated_eval_time=852.169016, accumulated_logging_time=1.633353, accumulated_submission_time=24004.760673, global_step=70432, preemption_count=0, score=24004.760673, test/accuracy=0.554900, test/loss=2.130442, test/num_examples=10000, total_duration=24861.097346, train/accuracy=0.759347, train/loss=1.125109, validation/accuracy=0.678500, validation/loss=1.474348, validation/num_examples=50000
I0127 20:07:03.234285 140026151130880 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.145850896835327, loss=3.0380606651306152
I0127 20:07:37.260322 140026075662080 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.107161521911621, loss=3.035395622253418
I0127 20:08:11.219912 140026151130880 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.15321683883667, loss=3.1309146881103516
I0127 20:08:45.211608 140026075662080 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.1549837589263916, loss=3.1069228649139404
I0127 20:09:19.207776 140026151130880 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.0808706283569336, loss=3.155519962310791
I0127 20:09:53.176768 140026075662080 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.1416428089141846, loss=3.0361149311065674
I0127 20:10:27.152940 140026151130880 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.9338206052780151, loss=2.998757839202881
I0127 20:11:01.087238 140026075662080 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.077678918838501, loss=3.081092596054077
I0127 20:11:35.059860 140026151130880 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.9077917337417603, loss=3.086591958999634
I0127 20:12:09.063382 140026075662080 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.1868233680725098, loss=3.047116756439209
I0127 20:12:43.053687 140026151130880 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.110024929046631, loss=3.1123390197753906
I0127 20:13:17.097657 140026075662080 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.137073040008545, loss=3.1280620098114014
I0127 20:13:51.081484 140026151130880 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.9922363758087158, loss=3.0808911323547363
I0127 20:14:25.049097 140026075662080 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.0198898315429688, loss=2.9978601932525635
I0127 20:14:59.039493 140026151130880 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.530313014984131, loss=3.124471664428711
I0127 20:15:10.062567 140187804313408 spec.py:321] Evaluating on the training split.
I0127 20:15:16.230976 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 20:15:25.136155 140187804313408 spec.py:349] Evaluating on the test split.
I0127 20:15:27.703826 140187804313408 submission_runner.py:408] Time since start: 25389.00s, 	Step: 71934, 	{'train/accuracy': 0.7483657598495483, 'train/loss': 1.220489263534546, 'validation/accuracy': 0.6733199954032898, 'validation/loss': 1.5536441802978516, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.1928555965423584, 'test/num_examples': 10000, 'score': 24514.92568397522, 'total_duration': 25388.99902510643, 'accumulated_submission_time': 24514.92568397522, 'accumulated_eval_time': 869.810240983963, 'accumulated_logging_time': 1.6746103763580322}
I0127 20:15:27.733569 140026058876672 logging_writer.py:48] [71934] accumulated_eval_time=869.810241, accumulated_logging_time=1.674610, accumulated_submission_time=24514.925684, global_step=71934, preemption_count=0, score=24514.925684, test/accuracy=0.549600, test/loss=2.192856, test/num_examples=10000, total_duration=25388.999025, train/accuracy=0.748366, train/loss=1.220489, validation/accuracy=0.673320, validation/loss=1.553644, validation/num_examples=50000
I0127 20:15:50.461998 140026067269376 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.1752047538757324, loss=3.035628080368042
I0127 20:16:24.424908 140026058876672 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.2709288597106934, loss=3.11160945892334
I0127 20:16:58.424346 140026067269376 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.1228203773498535, loss=3.069908380508423
I0127 20:17:32.414721 140026058876672 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.125175952911377, loss=3.086155652999878
I0127 20:18:06.419916 140026067269376 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.117220878601074, loss=3.124580144882202
I0127 20:18:40.390544 140026058876672 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1642653942108154, loss=3.0789756774902344
I0127 20:19:14.366379 140026067269376 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.302574634552002, loss=2.98335862159729
I0127 20:19:48.402486 140026058876672 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.180316209793091, loss=2.9665937423706055
I0127 20:20:22.382948 140026067269376 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.1256418228149414, loss=3.043463945388794
I0127 20:20:56.366044 140026058876672 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.0142009258270264, loss=3.0369627475738525
I0127 20:21:30.359567 140026067269376 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.2249183654785156, loss=3.0827558040618896
I0127 20:22:04.340075 140026058876672 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.1190366744995117, loss=3.096930503845215
I0127 20:22:38.341477 140026067269376 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.0336267948150635, loss=3.063838005065918
I0127 20:23:12.325083 140026058876672 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.186278820037842, loss=2.978977680206299
I0127 20:23:46.312581 140026067269376 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.092130422592163, loss=3.0192415714263916
I0127 20:23:58.013396 140187804313408 spec.py:321] Evaluating on the training split.
I0127 20:24:04.568816 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 20:24:13.268956 140187804313408 spec.py:349] Evaluating on the test split.
I0127 20:24:15.773042 140187804313408 submission_runner.py:408] Time since start: 25917.07s, 	Step: 73436, 	{'train/accuracy': 0.7437220811843872, 'train/loss': 1.2418133020401, 'validation/accuracy': 0.6713399887084961, 'validation/loss': 1.5635813474655151, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.236577272415161, 'test/num_examples': 10000, 'score': 25025.141628980637, 'total_duration': 25917.068249225616, 'accumulated_submission_time': 25025.141628980637, 'accumulated_eval_time': 887.5698609352112, 'accumulated_logging_time': 1.713430643081665}
I0127 20:24:15.803742 140026058876672 logging_writer.py:48] [73436] accumulated_eval_time=887.569861, accumulated_logging_time=1.713431, accumulated_submission_time=25025.141629, global_step=73436, preemption_count=0, score=25025.141629, test/accuracy=0.541500, test/loss=2.236577, test/num_examples=10000, total_duration=25917.068249, train/accuracy=0.743722, train/loss=1.241813, validation/accuracy=0.671340, validation/loss=1.563581, validation/num_examples=50000
I0127 20:24:37.820592 140026151130880 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.087993860244751, loss=3.054129123687744
I0127 20:25:11.715980 140026058876672 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.19271183013916, loss=2.968395233154297
I0127 20:25:45.676113 140026151130880 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.099945545196533, loss=2.9946608543395996
I0127 20:26:19.804779 140026058876672 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.2253754138946533, loss=3.0258169174194336
I0127 20:26:53.812101 140026151130880 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.1375153064727783, loss=3.0707848072052
I0127 20:27:27.790981 140026058876672 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9826626777648926, loss=2.9913339614868164
I0127 20:28:01.729769 140026151130880 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.036161422729492, loss=2.9952826499938965
I0127 20:28:35.738119 140026058876672 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.2936973571777344, loss=3.1267716884613037
I0127 20:29:09.696363 140026151130880 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.030294179916382, loss=3.0806241035461426
I0127 20:29:43.683291 140026058876672 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.4101805686950684, loss=3.0236165523529053
I0127 20:30:17.621199 140026151130880 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.099025249481201, loss=3.041649341583252
I0127 20:30:51.598577 140026058876672 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.1113290786743164, loss=3.159083127975464
I0127 20:31:25.599736 140026151130880 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.0526909828186035, loss=3.074625253677368
I0127 20:31:59.693110 140026058876672 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.2326643466949463, loss=3.1275906562805176
I0127 20:32:33.632483 140026151130880 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.180032253265381, loss=3.0372848510742188
I0127 20:32:45.999783 140187804313408 spec.py:321] Evaluating on the training split.
I0127 20:32:52.162430 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 20:33:01.170725 140187804313408 spec.py:349] Evaluating on the test split.
I0127 20:33:03.821694 140187804313408 submission_runner.py:408] Time since start: 26445.12s, 	Step: 74938, 	{'train/accuracy': 0.7940250039100647, 'train/loss': 1.0290786027908325, 'validation/accuracy': 0.6819599866867065, 'validation/loss': 1.4988394975662231, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.1356232166290283, 'test/num_examples': 10000, 'score': 25535.273594856262, 'total_duration': 26445.11689734459, 'accumulated_submission_time': 25535.273594856262, 'accumulated_eval_time': 905.3917419910431, 'accumulated_logging_time': 1.753354549407959}
I0127 20:33:03.852092 140026159523584 logging_writer.py:48] [74938] accumulated_eval_time=905.391742, accumulated_logging_time=1.753355, accumulated_submission_time=25535.273595, global_step=74938, preemption_count=0, score=25535.273595, test/accuracy=0.559300, test/loss=2.135623, test/num_examples=10000, total_duration=26445.116897, train/accuracy=0.794025, train/loss=1.029079, validation/accuracy=0.681960, validation/loss=1.498839, validation/num_examples=50000
I0127 20:33:25.240069 140026167916288 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.17777419090271, loss=3.03070068359375
I0127 20:33:59.176737 140026159523584 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.313187837600708, loss=3.0749902725219727
I0127 20:34:33.150887 140026167916288 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.180227041244507, loss=3.070955753326416
I0127 20:35:07.151801 140026159523584 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.1073875427246094, loss=3.064938545227051
I0127 20:35:41.178682 140026167916288 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.3442437648773193, loss=3.0571415424346924
I0127 20:36:15.138112 140026159523584 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.1663875579833984, loss=3.059861421585083
I0127 20:36:49.110535 140026167916288 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.1561992168426514, loss=3.038649797439575
I0127 20:37:23.093340 140026159523584 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.3522744178771973, loss=3.062018394470215
I0127 20:37:57.073002 140026167916288 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.2159433364868164, loss=3.097182512283325
I0127 20:38:31.124022 140026159523584 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.0708255767822266, loss=3.1101980209350586
I0127 20:39:05.130117 140026167916288 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.0641682147979736, loss=3.046797275543213
I0127 20:39:39.114012 140026159523584 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.211954355239868, loss=3.0685293674468994
I0127 20:40:13.126373 140026167916288 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.2132022380828857, loss=3.024975061416626
I0127 20:40:47.105545 140026159523584 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.2525339126586914, loss=3.002258539199829
I0127 20:41:21.105937 140026167916288 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.13022518157959, loss=3.0652472972869873
I0127 20:41:33.840633 140187804313408 spec.py:321] Evaluating on the training split.
I0127 20:41:40.083609 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 20:41:49.309163 140187804313408 spec.py:349] Evaluating on the test split.
I0127 20:41:51.735651 140187804313408 submission_runner.py:408] Time since start: 26973.03s, 	Step: 76439, 	{'train/accuracy': 0.7794363498687744, 'train/loss': 1.0432428121566772, 'validation/accuracy': 0.6851999759674072, 'validation/loss': 1.4631946086883545, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 2.140300989151001, 'test/num_examples': 10000, 'score': 26045.199329137802, 'total_duration': 26973.03084754944, 'accumulated_submission_time': 26045.199329137802, 'accumulated_eval_time': 923.286732673645, 'accumulated_logging_time': 1.793353796005249}
I0127 20:41:51.766270 140026151130880 logging_writer.py:48] [76439] accumulated_eval_time=923.286733, accumulated_logging_time=1.793354, accumulated_submission_time=26045.199329, global_step=76439, preemption_count=0, score=26045.199329, test/accuracy=0.558700, test/loss=2.140301, test/num_examples=10000, total_duration=26973.030848, train/accuracy=0.779436, train/loss=1.043243, validation/accuracy=0.685200, validation/loss=1.463195, validation/num_examples=50000
I0127 20:42:12.819944 140026176308992 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.1869444847106934, loss=3.0543274879455566
I0127 20:42:46.744172 140026151130880 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.059540271759033, loss=3.0632834434509277
I0127 20:43:20.709088 140026176308992 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.204113721847534, loss=3.057542324066162
I0127 20:43:54.691735 140026151130880 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.1558613777160645, loss=3.0207738876342773
I0127 20:44:28.661793 140026176308992 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.0750722885131836, loss=3.009333848953247
I0127 20:45:02.708104 140026151130880 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1086978912353516, loss=3.105400323867798
I0127 20:45:36.662572 140026176308992 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.031191349029541, loss=3.0165927410125732
I0127 20:46:10.628909 140026151130880 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.1979494094848633, loss=3.0328829288482666
I0127 20:46:44.617463 140026176308992 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.251849889755249, loss=3.068671226501465
I0127 20:47:18.570706 140026151130880 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.140258312225342, loss=3.0795087814331055
I0127 20:47:52.586756 140026176308992 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.2432615756988525, loss=3.010829448699951
I0127 20:48:26.546903 140026151130880 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.196657180786133, loss=3.0452237129211426
I0127 20:49:00.506216 140026176308992 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.2513318061828613, loss=2.9919791221618652
I0127 20:49:34.492456 140026151130880 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.146610975265503, loss=3.078444719314575
I0127 20:50:08.452509 140026176308992 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.263439416885376, loss=3.0827510356903076
I0127 20:50:21.863718 140187804313408 spec.py:321] Evaluating on the training split.
I0127 20:50:28.124193 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 20:50:36.964625 140187804313408 spec.py:349] Evaluating on the test split.
I0127 20:50:39.443062 140187804313408 submission_runner.py:408] Time since start: 27500.74s, 	Step: 77941, 	{'train/accuracy': 0.7739556431770325, 'train/loss': 1.1201239824295044, 'validation/accuracy': 0.686199963092804, 'validation/loss': 1.504704236984253, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 2.144953489303589, 'test/num_examples': 10000, 'score': 26555.232617139816, 'total_duration': 27500.738260746002, 'accumulated_submission_time': 26555.232617139816, 'accumulated_eval_time': 940.8660507202148, 'accumulated_logging_time': 1.8331985473632812}
I0127 20:50:39.476339 140026067269376 logging_writer.py:48] [77941] accumulated_eval_time=940.866051, accumulated_logging_time=1.833199, accumulated_submission_time=26555.232617, global_step=77941, preemption_count=0, score=26555.232617, test/accuracy=0.561300, test/loss=2.144953, test/num_examples=10000, total_duration=27500.738261, train/accuracy=0.773956, train/loss=1.120124, validation/accuracy=0.686200, validation/loss=1.504704, validation/num_examples=50000
I0127 20:50:59.913998 140026075662080 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.121811628341675, loss=3.0016632080078125
I0127 20:51:33.866726 140026067269376 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.248008966445923, loss=2.9619264602661133
I0127 20:52:07.847934 140026075662080 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.1760058403015137, loss=3.0825729370117188
I0127 20:52:41.848165 140026067269376 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.3327736854553223, loss=3.051704168319702
I0127 20:53:15.853558 140026075662080 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.244906425476074, loss=3.0067944526672363
I0127 20:53:49.830515 140026067269376 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.381479024887085, loss=3.109055519104004
I0127 20:54:23.836440 140026075662080 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.207057237625122, loss=3.023616313934326
I0127 20:54:57.800020 140026067269376 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.4380176067352295, loss=2.9884836673736572
I0127 20:55:31.804683 140026075662080 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.248884439468384, loss=3.0391526222229004
I0127 20:56:05.797180 140026067269376 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.1899638175964355, loss=3.0092036724090576
I0127 20:56:39.779617 140026075662080 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.1825368404388428, loss=3.072901964187622
I0127 20:57:13.884257 140026067269376 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.1147048473358154, loss=3.0565896034240723
I0127 20:57:47.870529 140026075662080 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.2408413887023926, loss=3.037503719329834
I0127 20:58:21.821866 140026067269376 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.051974296569824, loss=2.9851996898651123
I0127 20:58:55.821151 140026075662080 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.1977972984313965, loss=3.024420976638794
I0127 20:59:09.568440 140187804313408 spec.py:321] Evaluating on the training split.
I0127 20:59:15.709286 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 20:59:24.457602 140187804313408 spec.py:349] Evaluating on the test split.
I0127 20:59:26.908875 140187804313408 submission_runner.py:408] Time since start: 28028.20s, 	Step: 79442, 	{'train/accuracy': 0.7662029266357422, 'train/loss': 1.158005952835083, 'validation/accuracy': 0.6850000023841858, 'validation/loss': 1.5323312282562256, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 2.1702988147735596, 'test/num_examples': 10000, 'score': 27065.260931015015, 'total_duration': 28028.20407128334, 'accumulated_submission_time': 27065.260931015015, 'accumulated_eval_time': 958.2064433097839, 'accumulated_logging_time': 1.8757927417755127}
I0127 20:59:26.940215 140026058876672 logging_writer.py:48] [79442] accumulated_eval_time=958.206443, accumulated_logging_time=1.875793, accumulated_submission_time=27065.260931, global_step=79442, preemption_count=0, score=27065.260931, test/accuracy=0.556700, test/loss=2.170299, test/num_examples=10000, total_duration=28028.204071, train/accuracy=0.766203, train/loss=1.158006, validation/accuracy=0.685000, validation/loss=1.532331, validation/num_examples=50000
I0127 20:59:46.964874 140026151130880 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.2337262630462646, loss=3.0517759323120117
I0127 21:00:20.909899 140026058876672 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.444094657897949, loss=3.0594964027404785
I0127 21:00:54.866996 140026151130880 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.3720738887786865, loss=3.0589113235473633
I0127 21:01:28.785600 140026058876672 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.2653424739837646, loss=2.984529495239258
I0127 21:02:02.763327 140026151130880 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.3718442916870117, loss=2.9361987113952637
I0127 21:02:36.755756 140026058876672 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.161430835723877, loss=2.9372358322143555
I0127 21:03:10.739755 140026151130880 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.301046848297119, loss=3.0490329265594482
I0127 21:03:44.773087 140026058876672 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.20943021774292, loss=3.034489393234253
I0127 21:04:18.754764 140026151130880 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.1741788387298584, loss=3.0252645015716553
I0127 21:04:52.729056 140026058876672 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.078643321990967, loss=2.989393472671509
I0127 21:05:26.706493 140026151130880 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.4183738231658936, loss=3.0622735023498535
I0127 21:06:00.712511 140026058876672 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.100649118423462, loss=3.078303337097168
I0127 21:06:34.670934 140026151130880 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.2004270553588867, loss=3.040802240371704
I0127 21:07:08.645114 140026058876672 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.1692187786102295, loss=3.037363052368164
I0127 21:07:42.618108 140026151130880 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.20801043510437, loss=2.952354907989502
I0127 21:07:57.031695 140187804313408 spec.py:321] Evaluating on the training split.
I0127 21:08:04.192153 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 21:08:13.240365 140187804313408 spec.py:349] Evaluating on the test split.
I0127 21:08:15.751533 140187804313408 submission_runner.py:408] Time since start: 28557.05s, 	Step: 80944, 	{'train/accuracy': 0.7602837681770325, 'train/loss': 1.1421968936920166, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.5045496225357056, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1207895278930664, 'test/num_examples': 10000, 'score': 27575.289494991302, 'total_duration': 28557.046733379364, 'accumulated_submission_time': 27575.289494991302, 'accumulated_eval_time': 976.9262478351593, 'accumulated_logging_time': 1.9166314601898193}
I0127 21:08:15.784747 140026075662080 logging_writer.py:48] [80944] accumulated_eval_time=976.926248, accumulated_logging_time=1.916631, accumulated_submission_time=27575.289495, global_step=80944, preemption_count=0, score=27575.289495, test/accuracy=0.554100, test/loss=2.120790, test/num_examples=10000, total_duration=28557.046733, train/accuracy=0.760284, train/loss=1.142197, validation/accuracy=0.679880, validation/loss=1.504550, validation/num_examples=50000
I0127 21:08:35.142503 140026159523584 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.3200676441192627, loss=3.0152382850646973
I0127 21:09:09.086750 140026075662080 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.29888916015625, loss=2.9576265811920166
I0127 21:09:43.086046 140026159523584 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.089934825897217, loss=2.9264235496520996
I0127 21:10:17.074180 140026075662080 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.342575788497925, loss=2.98333740234375
I0127 21:10:51.052469 140026159523584 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.136265277862549, loss=2.9928932189941406
I0127 21:11:25.040494 140026075662080 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.3135664463043213, loss=3.004956007003784
I0127 21:11:59.034806 140026159523584 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.31607723236084, loss=3.00046443939209
I0127 21:12:33.032993 140026075662080 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.2188172340393066, loss=3.040743350982666
I0127 21:13:07.006725 140026159523584 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.2153327465057373, loss=2.9782514572143555
I0127 21:13:40.995396 140026075662080 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.1740376949310303, loss=3.0422377586364746
I0127 21:14:14.953223 140026159523584 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.1993956565856934, loss=2.9809460639953613
I0127 21:14:48.932607 140026075662080 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.2054848670959473, loss=2.9794013500213623
I0127 21:15:23.309209 140026159523584 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.2822515964508057, loss=3.0506038665771484
I0127 21:15:57.317195 140026075662080 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.366703510284424, loss=3.0298211574554443
I0127 21:16:31.298345 140026159523584 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.370351791381836, loss=3.0198209285736084
I0127 21:16:46.071331 140187804313408 spec.py:321] Evaluating on the training split.
I0127 21:16:52.247012 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 21:17:01.320944 140187804313408 spec.py:349] Evaluating on the test split.
I0127 21:17:04.033790 140187804313408 submission_runner.py:408] Time since start: 29085.33s, 	Step: 82445, 	{'train/accuracy': 0.7724011540412903, 'train/loss': 1.0933239459991455, 'validation/accuracy': 0.6927399635314941, 'validation/loss': 1.4421255588531494, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 2.080371379852295, 'test/num_examples': 10000, 'score': 28085.512244701385, 'total_duration': 29085.328985452652, 'accumulated_submission_time': 28085.512244701385, 'accumulated_eval_time': 994.8886668682098, 'accumulated_logging_time': 1.9587581157684326}
I0127 21:17:04.067499 140026067269376 logging_writer.py:48] [82445] accumulated_eval_time=994.888667, accumulated_logging_time=1.958758, accumulated_submission_time=28085.512245, global_step=82445, preemption_count=0, score=28085.512245, test/accuracy=0.569700, test/loss=2.080371, test/num_examples=10000, total_duration=29085.328985, train/accuracy=0.772401, train/loss=1.093324, validation/accuracy=0.692740, validation/loss=1.442126, validation/num_examples=50000
I0127 21:17:23.070011 140026151130880 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.1932804584503174, loss=3.01226544380188
I0127 21:17:57.011109 140026067269376 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.2178561687469482, loss=2.9770472049713135
I0127 21:18:30.913983 140026151130880 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.3273732662200928, loss=3.0676498413085938
I0127 21:19:04.889590 140026067269376 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.352945566177368, loss=3.0112152099609375
I0127 21:19:38.869646 140026151130880 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.1145992279052734, loss=3.0244603157043457
I0127 21:20:12.831798 140026067269376 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.1569535732269287, loss=2.9844577312469482
I0127 21:20:46.820857 140026151130880 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1581079959869385, loss=2.9723029136657715
I0127 21:21:20.796467 140026067269376 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.372783660888672, loss=3.009308338165283
I0127 21:21:54.758660 140026151130880 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.217662811279297, loss=2.995924711227417
I0127 21:22:28.777316 140026067269376 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.3270719051361084, loss=3.0431418418884277
I0127 21:23:02.728187 140026151130880 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.2396867275238037, loss=3.046814203262329
I0127 21:23:36.718613 140026067269376 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.288525342941284, loss=3.0551345348358154
I0127 21:24:10.645487 140026151130880 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.2726478576660156, loss=3.0017752647399902
I0127 21:24:44.615752 140026067269376 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.369791030883789, loss=3.005732774734497
I0127 21:25:18.581619 140026151130880 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.235358238220215, loss=2.9492626190185547
I0127 21:25:34.359084 140187804313408 spec.py:321] Evaluating on the training split.
I0127 21:25:40.551053 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 21:25:49.323000 140187804313408 spec.py:349] Evaluating on the test split.
I0127 21:25:51.840943 140187804313408 submission_runner.py:408] Time since start: 29613.14s, 	Step: 83948, 	{'train/accuracy': 0.8116828799247742, 'train/loss': 0.9399776458740234, 'validation/accuracy': 0.6905199885368347, 'validation/loss': 1.4603952169418335, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.10579776763916, 'test/num_examples': 10000, 'score': 28595.73851132393, 'total_duration': 29613.136142492294, 'accumulated_submission_time': 28595.73851132393, 'accumulated_eval_time': 1012.3704881668091, 'accumulated_logging_time': 2.003067970275879}
I0127 21:25:51.873703 140026075662080 logging_writer.py:48] [83948] accumulated_eval_time=1012.370488, accumulated_logging_time=2.003068, accumulated_submission_time=28595.738511, global_step=83948, preemption_count=0, score=28595.738511, test/accuracy=0.561800, test/loss=2.105798, test/num_examples=10000, total_duration=29613.136142, train/accuracy=0.811683, train/loss=0.939978, validation/accuracy=0.690520, validation/loss=1.460395, validation/num_examples=50000
I0127 21:26:09.892869 140026159523584 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.3192784786224365, loss=3.0009279251098633
I0127 21:26:43.809960 140026075662080 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.5275213718414307, loss=3.0471930503845215
I0127 21:27:17.783716 140026159523584 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.2293264865875244, loss=2.9285173416137695
I0127 21:27:51.786269 140026075662080 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.432072162628174, loss=3.0303287506103516
I0127 21:28:25.775264 140026159523584 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.285660982131958, loss=2.9702844619750977
I0127 21:28:59.826546 140026075662080 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.1518890857696533, loss=2.9943528175354004
I0127 21:29:33.774249 140026159523584 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.2838194370269775, loss=3.058044195175171
I0127 21:30:07.767939 140026075662080 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.3623645305633545, loss=3.048041582107544
I0127 21:30:41.747245 140026159523584 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.36800217628479, loss=2.9882421493530273
I0127 21:31:15.722091 140026075662080 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.359870672225952, loss=2.9475479125976562
I0127 21:31:49.724309 140026159523584 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.2803919315338135, loss=3.029604434967041
I0127 21:32:23.715116 140026075662080 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.3064844608306885, loss=2.996367931365967
I0127 21:32:57.696757 140026159523584 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.222820281982422, loss=2.9804933071136475
I0127 21:33:31.670146 140026075662080 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.2703800201416016, loss=3.0201241970062256
I0127 21:34:05.639556 140026159523584 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.2458174228668213, loss=2.9698874950408936
I0127 21:34:22.106252 140187804313408 spec.py:321] Evaluating on the training split.
I0127 21:34:28.297092 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 21:34:37.104004 140187804313408 spec.py:349] Evaluating on the test split.
I0127 21:34:39.637775 140187804313408 submission_runner.py:408] Time since start: 30140.93s, 	Step: 85450, 	{'train/accuracy': 0.7909757494926453, 'train/loss': 1.0620595216751099, 'validation/accuracy': 0.6925599575042725, 'validation/loss': 1.4879108667373657, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 2.1164779663085938, 'test/num_examples': 10000, 'score': 29105.90775370598, 'total_duration': 30140.932975292206, 'accumulated_submission_time': 29105.90775370598, 'accumulated_eval_time': 1029.901986837387, 'accumulated_logging_time': 2.045266628265381}
I0127 21:34:39.670791 140026050483968 logging_writer.py:48] [85450] accumulated_eval_time=1029.901987, accumulated_logging_time=2.045267, accumulated_submission_time=29105.907754, global_step=85450, preemption_count=0, score=29105.907754, test/accuracy=0.567300, test/loss=2.116478, test/num_examples=10000, total_duration=30140.932975, train/accuracy=0.790976, train/loss=1.062060, validation/accuracy=0.692560, validation/loss=1.487911, validation/num_examples=50000
I0127 21:34:57.117327 140026058876672 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.338855266571045, loss=3.0361244678497314
I0127 21:35:31.020364 140026050483968 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.1687963008880615, loss=2.957603931427002
I0127 21:36:04.978103 140026058876672 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.3685152530670166, loss=3.0383613109588623
I0127 21:36:38.954723 140026050483968 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.3178513050079346, loss=3.003937005996704
I0127 21:37:12.909152 140026058876672 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.300044536590576, loss=3.009237051010132
I0127 21:37:46.866667 140026050483968 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.237173080444336, loss=3.0098109245300293
I0127 21:38:20.851069 140026058876672 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.232616424560547, loss=2.952883243560791
I0127 21:38:54.821283 140026050483968 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.259999990463257, loss=3.010995388031006
I0127 21:39:28.782302 140026058876672 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.3949875831604004, loss=3.007852077484131
I0127 21:40:02.732994 140026050483968 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.4254097938537598, loss=3.0437211990356445
I0127 21:40:36.697872 140026058876672 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.2745444774627686, loss=3.0282347202301025
I0127 21:41:10.761928 140026050483968 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.3701627254486084, loss=3.0381996631622314
I0127 21:41:44.730238 140026058876672 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.275179147720337, loss=3.0933594703674316
I0127 21:42:18.701381 140026050483968 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.4576632976531982, loss=3.0501115322113037
I0127 21:42:52.634815 140026058876672 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.2858896255493164, loss=2.9549248218536377
I0127 21:43:09.775328 140187804313408 spec.py:321] Evaluating on the training split.
I0127 21:43:15.889552 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 21:43:24.875295 140187804313408 spec.py:349] Evaluating on the test split.
I0127 21:43:27.373581 140187804313408 submission_runner.py:408] Time since start: 30668.67s, 	Step: 86952, 	{'train/accuracy': 0.7831034660339355, 'train/loss': 1.0348970890045166, 'validation/accuracy': 0.6964600086212158, 'validation/loss': 1.4234379529953003, 'validation/num_examples': 50000, 'test/accuracy': 0.5677000284194946, 'test/loss': 2.063734769821167, 'test/num_examples': 10000, 'score': 29615.94760608673, 'total_duration': 30668.66878247261, 'accumulated_submission_time': 29615.94760608673, 'accumulated_eval_time': 1047.5002024173737, 'accumulated_logging_time': 2.0875396728515625}
I0127 21:43:27.413851 140026151130880 logging_writer.py:48] [86952] accumulated_eval_time=1047.500202, accumulated_logging_time=2.087540, accumulated_submission_time=29615.947606, global_step=86952, preemption_count=0, score=29615.947606, test/accuracy=0.567700, test/loss=2.063735, test/num_examples=10000, total_duration=30668.668782, train/accuracy=0.783103, train/loss=1.034897, validation/accuracy=0.696460, validation/loss=1.423438, validation/num_examples=50000
I0127 21:43:44.055041 140026159523584 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.2196195125579834, loss=2.9868006706237793
I0127 21:44:18.018917 140026151130880 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.3697659969329834, loss=3.02704119682312
I0127 21:44:51.981752 140026159523584 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.211618185043335, loss=2.9973812103271484
I0127 21:45:25.987084 140026151130880 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.4204277992248535, loss=2.959153175354004
I0127 21:45:59.967062 140026159523584 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.357095241546631, loss=3.030219554901123
I0127 21:46:33.950535 140026151130880 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.303377151489258, loss=3.0208587646484375
I0127 21:47:07.914068 140026159523584 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.2976977825164795, loss=3.019138813018799
I0127 21:47:41.999502 140026151130880 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.364396333694458, loss=2.99397611618042
I0127 21:48:15.982007 140026159523584 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.0730504989624023, loss=2.996018171310425
I0127 21:48:49.973217 140026151130880 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.185199022293091, loss=3.0172269344329834
I0127 21:49:23.955124 140026159523584 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.252061128616333, loss=3.03696346282959
I0127 21:49:57.940487 140026151130880 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3314032554626465, loss=3.005913734436035
I0127 21:50:31.925953 140026159523584 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3567488193511963, loss=3.0738987922668457
I0127 21:51:05.925075 140026151130880 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.386505365371704, loss=2.9418413639068604
I0127 21:51:39.914670 140026159523584 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.517759084701538, loss=3.0453999042510986
I0127 21:51:57.406603 140187804313408 spec.py:321] Evaluating on the training split.
I0127 21:52:03.666459 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 21:52:12.538840 140187804313408 spec.py:349] Evaluating on the test split.
I0127 21:52:15.024058 140187804313408 submission_runner.py:408] Time since start: 31196.32s, 	Step: 88453, 	{'train/accuracy': 0.7817681431770325, 'train/loss': 1.088742971420288, 'validation/accuracy': 0.6913599967956543, 'validation/loss': 1.4801713228225708, 'validation/num_examples': 50000, 'test/accuracy': 0.5667000412940979, 'test/loss': 2.1170437335968018, 'test/num_examples': 10000, 'score': 30125.87562441826, 'total_duration': 31196.319259643555, 'accumulated_submission_time': 30125.87562441826, 'accumulated_eval_time': 1065.1176307201385, 'accumulated_logging_time': 2.137878894805908}
I0127 21:52:15.056999 140026042091264 logging_writer.py:48] [88453] accumulated_eval_time=1065.117631, accumulated_logging_time=2.137879, accumulated_submission_time=30125.875624, global_step=88453, preemption_count=0, score=30125.875624, test/accuracy=0.566700, test/loss=2.117044, test/num_examples=10000, total_duration=31196.319260, train/accuracy=0.781768, train/loss=1.088743, validation/accuracy=0.691360, validation/loss=1.480171, validation/num_examples=50000
I0127 21:52:31.336837 140026050483968 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.475926399230957, loss=3.050342082977295
I0127 21:53:05.208611 140026042091264 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.2219066619873047, loss=2.964144229888916
I0127 21:53:39.230983 140026050483968 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.4633543491363525, loss=2.951209306716919
I0127 21:54:13.182027 140026042091264 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.4006078243255615, loss=3.043382167816162
I0127 21:54:47.154347 140026050483968 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.416053295135498, loss=2.9875528812408447
I0127 21:55:21.101289 140026042091264 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.287675619125366, loss=2.981807231903076
I0127 21:55:55.079150 140026050483968 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.5074543952941895, loss=2.9525506496429443
I0127 21:56:29.038962 140026042091264 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.2775204181671143, loss=3.0049538612365723
I0127 21:57:02.982268 140026050483968 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.367361545562744, loss=2.96871280670166
I0127 21:57:36.929355 140026042091264 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.300621747970581, loss=3.021953821182251
I0127 21:58:10.892552 140026050483968 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.577009677886963, loss=3.006446599960327
I0127 21:58:44.860850 140026042091264 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.4437801837921143, loss=3.0764822959899902
I0127 21:59:18.857836 140026050483968 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.230530261993408, loss=2.9339840412139893
I0127 21:59:53.007890 140026042091264 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.5404887199401855, loss=3.032089948654175
I0127 22:00:27.010816 140026050483968 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.3634114265441895, loss=3.037964105606079
I0127 22:00:45.175865 140187804313408 spec.py:321] Evaluating on the training split.
I0127 22:00:51.348841 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 22:01:00.364170 140187804313408 spec.py:349] Evaluating on the test split.
I0127 22:01:02.843634 140187804313408 submission_runner.py:408] Time since start: 31724.14s, 	Step: 89955, 	{'train/accuracy': 0.7678770422935486, 'train/loss': 1.1227507591247559, 'validation/accuracy': 0.6862999796867371, 'validation/loss': 1.481406569480896, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.1493637561798096, 'test/num_examples': 10000, 'score': 30635.928003787994, 'total_duration': 31724.138827323914, 'accumulated_submission_time': 30635.928003787994, 'accumulated_eval_time': 1082.7853560447693, 'accumulated_logging_time': 2.1824607849121094}
I0127 22:01:02.876984 140026050483968 logging_writer.py:48] [89955] accumulated_eval_time=1082.785356, accumulated_logging_time=2.182461, accumulated_submission_time=30635.928004, global_step=89955, preemption_count=0, score=30635.928004, test/accuracy=0.560100, test/loss=2.149364, test/num_examples=10000, total_duration=31724.138827, train/accuracy=0.767877, train/loss=1.122751, validation/accuracy=0.686300, validation/loss=1.481407, validation/num_examples=50000
I0127 22:01:18.497461 140026159523584 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.351208448410034, loss=3.0363311767578125
I0127 22:01:52.381811 140026050483968 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.399503231048584, loss=3.018442153930664
I0127 22:02:26.349765 140026159523584 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.3566501140594482, loss=2.9988114833831787
I0127 22:03:00.330803 140026050483968 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.380401611328125, loss=3.0184974670410156
I0127 22:03:34.298486 140026159523584 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.141167402267456, loss=2.9538822174072266
I0127 22:04:08.297173 140026050483968 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.357974052429199, loss=2.9904611110687256
I0127 22:04:42.271863 140026159523584 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.3820438385009766, loss=3.0100743770599365
I0127 22:05:16.268132 140026050483968 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.248197555541992, loss=3.0211715698242188
I0127 22:05:50.256150 140026159523584 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.3411641120910645, loss=2.924191474914551
I0127 22:06:24.271532 140026050483968 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.3596909046173096, loss=2.960540533065796
I0127 22:06:58.245400 140026159523584 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.4494433403015137, loss=2.9358537197113037
I0127 22:07:32.246930 140026050483968 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.207040548324585, loss=2.976830244064331
I0127 22:08:06.231408 140026159523584 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.439059019088745, loss=2.967353105545044
I0127 22:08:40.214349 140026050483968 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.3537850379943848, loss=2.994349956512451
I0127 22:09:14.181567 140026159523584 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.382356882095337, loss=2.940793037414551
I0127 22:09:33.018273 140187804313408 spec.py:321] Evaluating on the training split.
I0127 22:09:39.111858 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 22:09:48.164503 140187804313408 spec.py:349] Evaluating on the test split.
I0127 22:09:50.657135 140187804313408 submission_runner.py:408] Time since start: 32251.95s, 	Step: 91457, 	{'train/accuracy': 0.7881656289100647, 'train/loss': 1.0311802625656128, 'validation/accuracy': 0.7014600038528442, 'validation/loss': 1.415004014968872, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 2.05700421333313, 'test/num_examples': 10000, 'score': 31146.005562067032, 'total_duration': 32251.952335357666, 'accumulated_submission_time': 31146.005562067032, 'accumulated_eval_time': 1100.4241781234741, 'accumulated_logging_time': 2.2250864505767822}
I0127 22:09:50.692930 140026075662080 logging_writer.py:48] [91457] accumulated_eval_time=1100.424178, accumulated_logging_time=2.225086, accumulated_submission_time=31146.005562, global_step=91457, preemption_count=0, score=31146.005562, test/accuracy=0.572600, test/loss=2.057004, test/num_examples=10000, total_duration=32251.952335, train/accuracy=0.788166, train/loss=1.031180, validation/accuracy=0.701460, validation/loss=1.415004, validation/num_examples=50000
I0127 22:10:05.643555 140026151130880 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.442563533782959, loss=2.948368787765503
I0127 22:10:39.549842 140026075662080 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.2430124282836914, loss=3.028259038925171
I0127 22:11:13.462415 140026151130880 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.456620216369629, loss=3.0075290203094482
I0127 22:11:47.446726 140026075662080 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.4683167934417725, loss=3.0228700637817383
I0127 22:12:21.463317 140026151130880 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3384134769439697, loss=2.9949142932891846
I0127 22:12:55.432075 140026075662080 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.5308961868286133, loss=2.9883251190185547
I0127 22:13:29.386673 140026151130880 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.515596389770508, loss=2.9932339191436768
I0127 22:14:03.378591 140026075662080 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.27093243598938, loss=2.9307289123535156
I0127 22:14:37.318539 140026151130880 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.3216137886047363, loss=2.974231719970703
I0127 22:15:11.302767 140026075662080 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.3993008136749268, loss=2.9824726581573486
I0127 22:15:45.280128 140026151130880 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.4307563304901123, loss=2.97786021232605
I0127 22:16:19.226984 140026075662080 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.322723627090454, loss=2.880610942840576
I0127 22:16:53.215770 140026151130880 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.392786979675293, loss=2.9320945739746094
I0127 22:17:27.183269 140026075662080 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.4345920085906982, loss=2.900346517562866
I0127 22:18:01.152645 140026151130880 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.3881168365478516, loss=3.0200912952423096
I0127 22:18:20.659712 140187804313408 spec.py:321] Evaluating on the training split.
I0127 22:18:27.081336 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 22:18:36.110984 140187804313408 spec.py:349] Evaluating on the test split.
I0127 22:18:38.612607 140187804313408 submission_runner.py:408] Time since start: 32779.91s, 	Step: 92959, 	{'train/accuracy': 0.7984693646430969, 'train/loss': 1.0095969438552856, 'validation/accuracy': 0.6953799724578857, 'validation/loss': 1.4463399648666382, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 2.128091812133789, 'test/num_examples': 10000, 'score': 31655.910717964172, 'total_duration': 32779.907801151276, 'accumulated_submission_time': 31655.910717964172, 'accumulated_eval_time': 1118.377030134201, 'accumulated_logging_time': 2.269920825958252}
I0127 22:18:38.645760 140026042091264 logging_writer.py:48] [92959] accumulated_eval_time=1118.377030, accumulated_logging_time=2.269921, accumulated_submission_time=31655.910718, global_step=92959, preemption_count=0, score=31655.910718, test/accuracy=0.563800, test/loss=2.128092, test/num_examples=10000, total_duration=32779.907801, train/accuracy=0.798469, train/loss=1.009597, validation/accuracy=0.695380, validation/loss=1.446340, validation/num_examples=50000
I0127 22:18:52.885146 140026050483968 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.5761139392852783, loss=3.0181570053100586
I0127 22:19:26.782487 140026042091264 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.3409600257873535, loss=2.9030590057373047
I0127 22:20:00.702031 140026050483968 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.382754325866699, loss=2.955388307571411
I0127 22:20:34.665863 140026042091264 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.1810314655303955, loss=2.888223171234131
I0127 22:21:08.660131 140026050483968 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.5597710609436035, loss=3.0642154216766357
I0127 22:21:42.650546 140026042091264 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.300882339477539, loss=2.9284026622772217
I0127 22:22:16.586758 140026050483968 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.425701856613159, loss=2.977552652359009
I0127 22:22:50.565906 140026042091264 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.3281466960906982, loss=2.975616455078125
I0127 22:23:24.540110 140026050483968 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.544361114501953, loss=2.9867000579833984
I0127 22:23:58.515982 140026042091264 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.276540994644165, loss=2.8811020851135254
I0127 22:24:32.493876 140026050483968 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.61486554145813, loss=3.030470609664917
I0127 22:25:06.494407 140026042091264 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.551826238632202, loss=2.984440803527832
I0127 22:25:40.488609 140026050483968 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.2804415225982666, loss=2.9703097343444824
I0127 22:26:14.455825 140026042091264 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.494641065597534, loss=2.904994487762451
I0127 22:26:48.416473 140026050483968 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.4791016578674316, loss=2.897148847579956
I0127 22:27:08.948880 140187804313408 spec.py:321] Evaluating on the training split.
I0127 22:27:15.036206 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 22:27:24.129464 140187804313408 spec.py:349] Evaluating on the test split.
I0127 22:27:26.556405 140187804313408 submission_runner.py:408] Time since start: 33307.85s, 	Step: 94462, 	{'train/accuracy': 0.8036909699440002, 'train/loss': 1.0080145597457886, 'validation/accuracy': 0.6985599994659424, 'validation/loss': 1.4534142017364502, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 2.11687970161438, 'test/num_examples': 10000, 'score': 32166.15032839775, 'total_duration': 33307.851593732834, 'accumulated_submission_time': 32166.15032839775, 'accumulated_eval_time': 1135.9845206737518, 'accumulated_logging_time': 2.3124051094055176}
I0127 22:27:26.590559 140026050483968 logging_writer.py:48] [94462] accumulated_eval_time=1135.984521, accumulated_logging_time=2.312405, accumulated_submission_time=32166.150328, global_step=94462, preemption_count=0, score=32166.150328, test/accuracy=0.569700, test/loss=2.116880, test/num_examples=10000, total_duration=33307.851594, train/accuracy=0.803691, train/loss=1.008015, validation/accuracy=0.698560, validation/loss=1.453414, validation/num_examples=50000
I0127 22:27:39.828800 140026159523584 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.2819604873657227, loss=2.9876086711883545
I0127 22:28:13.749247 140026050483968 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.367227792739868, loss=2.9671390056610107
I0127 22:28:47.670554 140026159523584 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.2932565212249756, loss=2.9204723834991455
I0127 22:29:21.617225 140026050483968 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.6254255771636963, loss=3.039236545562744
I0127 22:29:55.599530 140026159523584 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.340301752090454, loss=2.924243450164795
I0127 22:30:29.540890 140026050483968 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.3635456562042236, loss=2.9551279544830322
I0127 22:31:03.692651 140026159523584 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.5020554065704346, loss=3.0155117511749268
I0127 22:31:37.642984 140026050483968 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.353408098220825, loss=2.8867404460906982
I0127 22:32:11.587892 140026159523584 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.5653035640716553, loss=2.9574697017669678
I0127 22:32:45.551313 140026050483968 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.541775703430176, loss=2.9010980129241943
I0127 22:33:19.525721 140026159523584 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.5190675258636475, loss=2.921982526779175
I0127 22:33:53.502120 140026050483968 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.922924518585205, loss=2.9430246353149414
I0127 22:34:27.433507 140026159523584 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.429865598678589, loss=2.945692539215088
I0127 22:35:01.414288 140026050483968 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.58786678314209, loss=2.9691009521484375
I0127 22:35:35.373790 140026159523584 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.5405519008636475, loss=2.928478956222534
I0127 22:35:56.884658 140187804313408 spec.py:321] Evaluating on the training split.
I0127 22:36:03.218420 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 22:36:12.004128 140187804313408 spec.py:349] Evaluating on the test split.
I0127 22:36:14.490975 140187804313408 submission_runner.py:408] Time since start: 33835.79s, 	Step: 95965, 	{'train/accuracy': 0.7919324040412903, 'train/loss': 1.0704740285873413, 'validation/accuracy': 0.6911799907684326, 'validation/loss': 1.506101369857788, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 2.1151468753814697, 'test/num_examples': 10000, 'score': 32676.37836766243, 'total_duration': 33835.78617501259, 'accumulated_submission_time': 32676.37836766243, 'accumulated_eval_time': 1153.5908043384552, 'accumulated_logging_time': 2.356222152709961}
I0127 22:36:14.528980 140026067269376 logging_writer.py:48] [95965] accumulated_eval_time=1153.590804, accumulated_logging_time=2.356222, accumulated_submission_time=32676.378368, global_step=95965, preemption_count=0, score=32676.378368, test/accuracy=0.571700, test/loss=2.115147, test/num_examples=10000, total_duration=33835.786175, train/accuracy=0.791932, train/loss=1.070474, validation/accuracy=0.691180, validation/loss=1.506101, validation/num_examples=50000
I0127 22:36:26.769694 140026075662080 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.311797857284546, loss=3.0490801334381104
I0127 22:37:00.657629 140026067269376 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.6170315742492676, loss=3.0721535682678223
I0127 22:37:34.743848 140026075662080 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.525345802307129, loss=2.982741594314575
I0127 22:38:08.698470 140026067269376 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.383228302001953, loss=2.990875482559204
I0127 22:38:42.700772 140026075662080 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.4014148712158203, loss=2.9344747066497803
I0127 22:39:16.663969 140026067269376 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.6605358123779297, loss=2.980172872543335
I0127 22:39:50.634832 140026075662080 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.4915127754211426, loss=3.00049090385437
I0127 22:40:24.624525 140026067269376 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.4007067680358887, loss=3.0101680755615234
I0127 22:40:58.622067 140026075662080 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.383463144302368, loss=2.919351816177368
I0127 22:41:32.606710 140026067269376 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.5456578731536865, loss=2.9557044506073
I0127 22:42:06.589352 140026075662080 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.448115825653076, loss=2.9481709003448486
I0127 22:42:40.568535 140026067269376 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.3290603160858154, loss=2.9109230041503906
I0127 22:43:14.556306 140026075662080 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.451791524887085, loss=2.9321956634521484
I0127 22:43:48.619015 140026067269376 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.4459023475646973, loss=2.9279489517211914
I0127 22:44:22.581201 140026075662080 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.444002866744995, loss=2.96647310256958
I0127 22:44:44.823717 140187804313408 spec.py:321] Evaluating on the training split.
I0127 22:44:51.052670 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 22:45:00.158985 140187804313408 spec.py:349] Evaluating on the test split.
I0127 22:45:02.706654 140187804313408 submission_runner.py:408] Time since start: 34364.00s, 	Step: 97467, 	{'train/accuracy': 0.7996053695678711, 'train/loss': 0.9840648174285889, 'validation/accuracy': 0.70551997423172, 'validation/loss': 1.3981292247772217, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 2.038626194000244, 'test/num_examples': 10000, 'score': 33186.609236717224, 'total_duration': 34364.00185251236, 'accumulated_submission_time': 33186.609236717224, 'accumulated_eval_time': 1171.4737193584442, 'accumulated_logging_time': 2.403014659881592}
I0127 22:45:02.742036 140026159523584 logging_writer.py:48] [97467] accumulated_eval_time=1171.473719, accumulated_logging_time=2.403015, accumulated_submission_time=33186.609237, global_step=97467, preemption_count=0, score=33186.609237, test/accuracy=0.579500, test/loss=2.038626, test/num_examples=10000, total_duration=34364.001853, train/accuracy=0.799605, train/loss=0.984065, validation/accuracy=0.705520, validation/loss=1.398129, validation/num_examples=50000
I0127 22:45:14.284654 140026167916288 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.343000888824463, loss=2.8841121196746826
I0127 22:45:48.158264 140026159523584 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.5086843967437744, loss=3.008816719055176
I0127 22:46:22.120906 140026167916288 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.411115884780884, loss=2.9315943717956543
I0127 22:46:56.103915 140026159523584 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.4770572185516357, loss=2.96054744720459
I0127 22:47:30.094798 140026167916288 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.442389965057373, loss=2.9319205284118652
I0127 22:48:04.063703 140026159523584 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.595097303390503, loss=2.9929683208465576
I0127 22:48:38.060521 140026167916288 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.5936286449432373, loss=2.9438865184783936
I0127 22:49:12.013673 140026159523584 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.558654308319092, loss=2.9820241928100586
I0127 22:49:46.016049 140026167916288 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.5384814739227295, loss=2.9249932765960693
I0127 22:50:20.060311 140026159523584 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.409764528274536, loss=2.869154930114746
I0127 22:50:54.045204 140026167916288 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.624088764190674, loss=2.9725959300994873
I0127 22:51:28.020574 140026159523584 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.533414363861084, loss=3.0085878372192383
I0127 22:52:01.977888 140026167916288 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.473313808441162, loss=2.9226438999176025
I0127 22:52:35.959836 140026159523584 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.430475950241089, loss=2.954862356185913
I0127 22:53:09.902837 140026167916288 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.4962666034698486, loss=2.9657557010650635
I0127 22:53:32.789938 140187804313408 spec.py:321] Evaluating on the training split.
I0127 22:53:38.986923 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 22:53:47.990855 140187804313408 spec.py:349] Evaluating on the test split.
I0127 22:53:50.485415 140187804313408 submission_runner.py:408] Time since start: 34891.78s, 	Step: 98969, 	{'train/accuracy': 0.7989476919174194, 'train/loss': 0.9818856716156006, 'validation/accuracy': 0.7056799530982971, 'validation/loss': 1.3842735290527344, 'validation/num_examples': 50000, 'test/accuracy': 0.5839000344276428, 'test/loss': 2.0269968509674072, 'test/num_examples': 10000, 'score': 33696.592266082764, 'total_duration': 34891.780616760254, 'accumulated_submission_time': 33696.592266082764, 'accumulated_eval_time': 1189.1691591739655, 'accumulated_logging_time': 2.448249578475952}
I0127 22:53:50.523769 140026050483968 logging_writer.py:48] [98969] accumulated_eval_time=1189.169159, accumulated_logging_time=2.448250, accumulated_submission_time=33696.592266, global_step=98969, preemption_count=0, score=33696.592266, test/accuracy=0.583900, test/loss=2.026997, test/num_examples=10000, total_duration=34891.780617, train/accuracy=0.798948, train/loss=0.981886, validation/accuracy=0.705680, validation/loss=1.384274, validation/num_examples=50000
I0127 22:54:01.374770 140026058876672 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.52878737449646, loss=2.884596586227417
I0127 22:54:35.314602 140026050483968 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.2719924449920654, loss=2.8448562622070312
I0127 22:55:09.279539 140026058876672 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.5880303382873535, loss=2.943112850189209
I0127 22:55:43.261156 140026050483968 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.728928804397583, loss=3.015428066253662
I0127 22:56:17.425182 140026058876672 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.7211568355560303, loss=2.956023693084717
I0127 22:56:51.370721 140026050483968 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.701321601867676, loss=2.985513925552368
I0127 22:57:25.357380 140026058876672 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.4494516849517822, loss=2.953634023666382
I0127 22:57:59.315096 140026050483968 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.5791656970977783, loss=2.9726643562316895
I0127 22:58:33.304408 140026058876672 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.486123561859131, loss=2.9152889251708984
I0127 22:59:07.267797 140026050483968 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.599093198776245, loss=2.884864568710327
I0127 22:59:41.251379 140026058876672 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.430074453353882, loss=2.9423766136169434
I0127 23:00:15.235916 140026050483968 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.4400906562805176, loss=2.870462417602539
I0127 23:00:49.198765 140026058876672 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.3702518939971924, loss=2.886605978012085
I0127 23:01:23.197565 140026050483968 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.540113687515259, loss=2.9154622554779053
I0127 23:01:57.181975 140026058876672 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.508099317550659, loss=2.9110381603240967
I0127 23:02:20.520541 140187804313408 spec.py:321] Evaluating on the training split.
I0127 23:02:26.676651 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 23:02:35.646548 140187804313408 spec.py:349] Evaluating on the test split.
I0127 23:02:38.108658 140187804313408 submission_runner.py:408] Time since start: 35419.40s, 	Step: 100470, 	{'train/accuracy': 0.8025948405265808, 'train/loss': 0.9827648401260376, 'validation/accuracy': 0.707539975643158, 'validation/loss': 1.3925204277038574, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 2.023866653442383, 'test/num_examples': 10000, 'score': 34206.52460384369, 'total_duration': 35419.403853178024, 'accumulated_submission_time': 34206.52460384369, 'accumulated_eval_time': 1206.7572317123413, 'accumulated_logging_time': 2.495995283126831}
I0127 23:02:38.144158 140026167916288 logging_writer.py:48] [100470] accumulated_eval_time=1206.757232, accumulated_logging_time=2.495995, accumulated_submission_time=34206.524604, global_step=100470, preemption_count=0, score=34206.524604, test/accuracy=0.584200, test/loss=2.023867, test/num_examples=10000, total_duration=35419.403853, train/accuracy=0.802595, train/loss=0.982765, validation/accuracy=0.707540, validation/loss=1.392520, validation/num_examples=50000
I0127 23:02:48.672977 140026176308992 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.5551939010620117, loss=2.9520328044891357
I0127 23:03:22.619386 140026167916288 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.568852663040161, loss=2.880208969116211
I0127 23:03:56.551377 140026176308992 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.84287428855896, loss=2.9241104125976562
I0127 23:04:30.488547 140026167916288 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.6172049045562744, loss=2.9234936237335205
I0127 23:05:04.464250 140026176308992 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.531557083129883, loss=2.967524766921997
I0127 23:05:38.426312 140026167916288 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.5564751625061035, loss=2.9319815635681152
I0127 23:06:12.393546 140026176308992 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.5586555004119873, loss=2.9494259357452393
I0127 23:06:46.360359 140026167916288 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.8714609146118164, loss=2.8866565227508545
I0127 23:07:20.302629 140026176308992 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.5622479915618896, loss=2.9298033714294434
I0127 23:07:54.288027 140026167916288 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.587129831314087, loss=2.9330286979675293
I0127 23:08:28.340992 140026176308992 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.6392202377319336, loss=2.886052131652832
I0127 23:09:02.317850 140026167916288 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.6739485263824463, loss=2.904231071472168
I0127 23:09:36.284937 140026176308992 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.7652664184570312, loss=3.010152816772461
I0127 23:10:10.236055 140026167916288 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.5937726497650146, loss=2.929497480392456
I0127 23:10:44.198845 140026176308992 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.5300981998443604, loss=3.005974054336548
I0127 23:11:08.138533 140187804313408 spec.py:321] Evaluating on the training split.
I0127 23:11:14.317529 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 23:11:23.356492 140187804313408 spec.py:349] Evaluating on the test split.
I0127 23:11:25.826363 140187804313408 submission_runner.py:408] Time since start: 35947.12s, 	Step: 101972, 	{'train/accuracy': 0.8014987111091614, 'train/loss': 1.0160855054855347, 'validation/accuracy': 0.707040011882782, 'validation/loss': 1.427125334739685, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 2.0674097537994385, 'test/num_examples': 10000, 'score': 34716.45545458794, 'total_duration': 35947.121560812, 'accumulated_submission_time': 34716.45545458794, 'accumulated_eval_time': 1224.4450266361237, 'accumulated_logging_time': 2.540762186050415}
I0127 23:11:25.861786 140026050483968 logging_writer.py:48] [101972] accumulated_eval_time=1224.445027, accumulated_logging_time=2.540762, accumulated_submission_time=34716.455455, global_step=101972, preemption_count=0, score=34716.455455, test/accuracy=0.578400, test/loss=2.067410, test/num_examples=10000, total_duration=35947.121561, train/accuracy=0.801499, train/loss=1.016086, validation/accuracy=0.707040, validation/loss=1.427125, validation/num_examples=50000
I0127 23:11:35.702914 140026058876672 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.4822115898132324, loss=2.9315176010131836
I0127 23:12:09.641819 140026050483968 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.4947476387023926, loss=2.8481838703155518
I0127 23:12:43.598926 140026058876672 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.66586971282959, loss=2.9896399974823
I0127 23:13:17.594314 140026050483968 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.5621771812438965, loss=2.926480293273926
I0127 23:13:51.537590 140026058876672 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.5091805458068848, loss=2.8136258125305176
I0127 23:14:25.511988 140026050483968 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.7328948974609375, loss=2.897911548614502
I0127 23:14:59.545707 140026058876672 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.662647247314453, loss=2.8767385482788086
I0127 23:15:33.520328 140026050483968 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.5159995555877686, loss=2.8664438724517822
I0127 23:16:07.517167 140026058876672 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.6744961738586426, loss=2.9735758304595947
I0127 23:16:41.475787 140026050483968 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.6069376468658447, loss=2.9635026454925537
I0127 23:17:15.442196 140026058876672 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.7110695838928223, loss=2.8741025924682617
I0127 23:17:49.431874 140026050483968 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.7534146308898926, loss=2.9225831031799316
I0127 23:18:23.383501 140026058876672 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.58471941947937, loss=2.887260913848877
I0127 23:18:57.349622 140026050483968 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.5888333320617676, loss=2.8584721088409424
I0127 23:19:31.310520 140026058876672 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.6201038360595703, loss=2.8622629642486572
I0127 23:19:55.924179 140187804313408 spec.py:321] Evaluating on the training split.
I0127 23:20:02.165496 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 23:20:11.333683 140187804313408 spec.py:349] Evaluating on the test split.
I0127 23:20:13.777622 140187804313408 submission_runner.py:408] Time since start: 36475.07s, 	Step: 103474, 	{'train/accuracy': 0.8282246589660645, 'train/loss': 0.8729314804077148, 'validation/accuracy': 0.7114599943161011, 'validation/loss': 1.368599534034729, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 2.0045225620269775, 'test/num_examples': 10000, 'score': 35226.452053546906, 'total_duration': 36475.07280921936, 'accumulated_submission_time': 35226.452053546906, 'accumulated_eval_time': 1242.2984237670898, 'accumulated_logging_time': 2.5861055850982666}
I0127 23:20:13.813653 140026159523584 logging_writer.py:48] [103474] accumulated_eval_time=1242.298424, accumulated_logging_time=2.586106, accumulated_submission_time=35226.452054, global_step=103474, preemption_count=0, score=35226.452054, test/accuracy=0.585100, test/loss=2.004523, test/num_examples=10000, total_duration=36475.072809, train/accuracy=0.828225, train/loss=0.872931, validation/accuracy=0.711460, validation/loss=1.368600, validation/num_examples=50000
I0127 23:20:22.961174 140026167916288 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.726917028427124, loss=2.96856689453125
I0127 23:20:56.880015 140026159523584 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.6473159790039062, loss=2.9445228576660156
I0127 23:21:30.871606 140026167916288 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.679003953933716, loss=2.959625482559204
I0127 23:22:04.834820 140026159523584 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.4760396480560303, loss=2.8710975646972656
I0127 23:22:38.778657 140026167916288 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.5756077766418457, loss=2.921384811401367
I0127 23:23:12.762097 140026159523584 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.667052984237671, loss=2.9322657585144043
I0127 23:23:46.721805 140026167916288 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.7138378620147705, loss=2.9163050651550293
I0127 23:24:20.686716 140026159523584 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.60433292388916, loss=2.863452196121216
I0127 23:24:54.656228 140026167916288 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.643078327178955, loss=2.8811588287353516
I0127 23:25:28.629041 140026159523584 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.5879898071289062, loss=2.952942132949829
I0127 23:26:02.588865 140026167916288 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.692880392074585, loss=2.947409152984619
I0127 23:26:36.553080 140026159523584 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.6263303756713867, loss=2.903865337371826
I0127 23:27:10.518002 140026167916288 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.5940473079681396, loss=2.9370808601379395
I0127 23:27:44.551346 140026159523584 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.704926013946533, loss=2.9070801734924316
I0127 23:28:18.513156 140026167916288 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.639690637588501, loss=2.8888254165649414
I0127 23:28:43.796697 140187804313408 spec.py:321] Evaluating on the training split.
I0127 23:28:49.901428 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 23:28:58.935619 140187804313408 spec.py:349] Evaluating on the test split.
I0127 23:29:01.410180 140187804313408 submission_runner.py:408] Time since start: 37002.71s, 	Step: 104976, 	{'train/accuracy': 0.8075972199440002, 'train/loss': 0.9604279398918152, 'validation/accuracy': 0.7060999870300293, 'validation/loss': 1.4053521156311035, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 2.0361974239349365, 'test/num_examples': 10000, 'score': 35736.37076330185, 'total_duration': 37002.70536971092, 'accumulated_submission_time': 35736.37076330185, 'accumulated_eval_time': 1259.911861896515, 'accumulated_logging_time': 2.632046937942505}
I0127 23:29:01.446098 140026050483968 logging_writer.py:48] [104976] accumulated_eval_time=1259.911862, accumulated_logging_time=2.632047, accumulated_submission_time=35736.370763, global_step=104976, preemption_count=0, score=35736.370763, test/accuracy=0.578900, test/loss=2.036197, test/num_examples=10000, total_duration=37002.705370, train/accuracy=0.807597, train/loss=0.960428, validation/accuracy=0.706100, validation/loss=1.405352, validation/num_examples=50000
I0127 23:29:09.931108 140026058876672 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.5190269947052, loss=2.8792710304260254
I0127 23:29:43.864959 140026050483968 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.5958619117736816, loss=2.934535503387451
I0127 23:30:17.831067 140026058876672 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.5280752182006836, loss=2.885211944580078
I0127 23:30:51.814443 140026050483968 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.7814135551452637, loss=2.981259346008301
I0127 23:31:25.783685 140026058876672 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.7748754024505615, loss=2.913233757019043
I0127 23:31:59.762722 140026050483968 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.692958116531372, loss=2.8303451538085938
I0127 23:32:33.729868 140026058876672 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.689526319503784, loss=2.909519910812378
I0127 23:33:07.679033 140026050483968 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.6134467124938965, loss=2.9121880531311035
I0127 23:33:41.703356 140026058876672 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.599271535873413, loss=2.9139182567596436
I0127 23:34:15.656421 140026050483968 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.6031434535980225, loss=2.8958425521850586
I0127 23:34:49.652234 140026058876672 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.7492823600769043, loss=2.8819150924682617
I0127 23:35:23.627975 140026050483968 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.790252685546875, loss=2.8873517513275146
I0127 23:35:57.605575 140026058876672 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.608511447906494, loss=2.9184622764587402
I0127 23:36:31.604942 140026050483968 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.6802611351013184, loss=2.9863598346710205
I0127 23:37:05.613190 140026058876672 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.6959424018859863, loss=2.8924989700317383
I0127 23:37:31.584526 140187804313408 spec.py:321] Evaluating on the training split.
I0127 23:37:37.728621 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 23:37:46.664282 140187804313408 spec.py:349] Evaluating on the test split.
I0127 23:37:49.158836 140187804313408 submission_runner.py:408] Time since start: 37530.45s, 	Step: 106478, 	{'train/accuracy': 0.8165656924247742, 'train/loss': 0.916153609752655, 'validation/accuracy': 0.7117599844932556, 'validation/loss': 1.3760380744934082, 'validation/num_examples': 50000, 'test/accuracy': 0.5820000171661377, 'test/loss': 2.037717342376709, 'test/num_examples': 10000, 'score': 36246.44602751732, 'total_duration': 37530.45402097702, 'accumulated_submission_time': 36246.44602751732, 'accumulated_eval_time': 1277.4861352443695, 'accumulated_logging_time': 2.677516460418701}
I0127 23:37:49.199113 140026167916288 logging_writer.py:48] [106478] accumulated_eval_time=1277.486135, accumulated_logging_time=2.677516, accumulated_submission_time=36246.446028, global_step=106478, preemption_count=0, score=36246.446028, test/accuracy=0.582000, test/loss=2.037717, test/num_examples=10000, total_duration=37530.454021, train/accuracy=0.816566, train/loss=0.916154, validation/accuracy=0.711760, validation/loss=1.376038, validation/num_examples=50000
I0127 23:37:57.015373 140026176308992 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.7602827548980713, loss=2.869229793548584
I0127 23:38:30.907023 140026167916288 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.6679813861846924, loss=2.9397292137145996
I0127 23:39:04.820125 140026176308992 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.7074837684631348, loss=2.8577747344970703
I0127 23:39:38.772599 140026167916288 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.692098617553711, loss=2.9191999435424805
I0127 23:40:12.825542 140026176308992 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.716123104095459, loss=2.904545307159424
I0127 23:40:46.762295 140026167916288 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.4740824699401855, loss=2.9076478481292725
I0127 23:41:20.714135 140026176308992 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.774287223815918, loss=2.924468994140625
I0127 23:41:54.676327 140026167916288 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.5536558628082275, loss=2.925093412399292
I0127 23:42:28.636695 140026176308992 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.68375825881958, loss=2.8694913387298584
I0127 23:43:02.624662 140026167916288 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.731130361557007, loss=2.8323981761932373
I0127 23:43:36.588428 140026176308992 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.571939706802368, loss=2.889538288116455
I0127 23:44:10.516599 140026167916288 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.714529514312744, loss=2.912787437438965
I0127 23:44:44.476935 140026176308992 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.763216018676758, loss=2.900688886642456
I0127 23:45:18.451880 140026167916288 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.845597982406616, loss=2.9925875663757324
I0127 23:45:52.403132 140026176308992 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.6225247383117676, loss=2.808361530303955
I0127 23:46:19.405490 140187804313408 spec.py:321] Evaluating on the training split.
I0127 23:46:25.601676 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 23:46:34.682736 140187804313408 spec.py:349] Evaluating on the test split.
I0127 23:46:37.193536 140187804313408 submission_runner.py:408] Time since start: 38058.49s, 	Step: 107981, 	{'train/accuracy': 0.8157086968421936, 'train/loss': 0.9369272589683533, 'validation/accuracy': 0.7161999940872192, 'validation/loss': 1.3731088638305664, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 2.0006630420684814, 'test/num_examples': 10000, 'score': 36756.588076114655, 'total_duration': 38058.4887046814, 'accumulated_submission_time': 36756.588076114655, 'accumulated_eval_time': 1295.2741153240204, 'accumulated_logging_time': 2.727273941040039}
I0127 23:46:37.232995 140026050483968 logging_writer.py:48] [107981] accumulated_eval_time=1295.274115, accumulated_logging_time=2.727274, accumulated_submission_time=36756.588076, global_step=107981, preemption_count=0, score=36756.588076, test/accuracy=0.590100, test/loss=2.000663, test/num_examples=10000, total_duration=38058.488705, train/accuracy=0.815709, train/loss=0.936927, validation/accuracy=0.716200, validation/loss=1.373109, validation/num_examples=50000
I0127 23:46:44.043531 140026075662080 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.7952940464019775, loss=2.9609456062316895
I0127 23:47:17.963967 140026050483968 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.668806791305542, loss=2.872982978820801
I0127 23:47:51.913879 140026075662080 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.6295723915100098, loss=2.9058690071105957
I0127 23:48:25.894832 140026050483968 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.694279670715332, loss=2.9418106079101562
I0127 23:48:59.871705 140026075662080 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.656757354736328, loss=2.9143428802490234
I0127 23:49:33.856992 140026050483968 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.7050058841705322, loss=2.862485408782959
I0127 23:50:07.836724 140026075662080 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.6282119750976562, loss=2.8281352519989014
I0127 23:50:41.827172 140026050483968 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.6125991344451904, loss=2.8787479400634766
I0127 23:51:15.798391 140026075662080 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.7269556522369385, loss=2.9094197750091553
I0127 23:51:49.762537 140026050483968 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.947136402130127, loss=2.9225456714630127
I0127 23:52:23.790835 140026075662080 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.51826810836792, loss=2.8624579906463623
I0127 23:52:57.770222 140026050483968 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.8486948013305664, loss=2.9253482818603516
I0127 23:53:31.771281 140026075662080 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.723104238510132, loss=2.9105000495910645
I0127 23:54:05.736613 140026050483968 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.5515425205230713, loss=2.8335583209991455
I0127 23:54:39.727010 140026075662080 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.8396055698394775, loss=2.9110000133514404
I0127 23:55:07.388274 140187804313408 spec.py:321] Evaluating on the training split.
I0127 23:55:13.685297 140187804313408 spec.py:333] Evaluating on the validation split.
I0127 23:55:22.585822 140187804313408 spec.py:349] Evaluating on the test split.
I0127 23:55:24.980535 140187804313408 submission_runner.py:408] Time since start: 38586.28s, 	Step: 109483, 	{'train/accuracy': 0.8148317933082581, 'train/loss': 0.9315711855888367, 'validation/accuracy': 0.7142199873924255, 'validation/loss': 1.3580296039581299, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0119309425354004, 'test/num_examples': 10000, 'score': 37266.67953324318, 'total_duration': 38586.27570772171, 'accumulated_submission_time': 37266.67953324318, 'accumulated_eval_time': 1312.8663160800934, 'accumulated_logging_time': 2.7767326831817627}
I0127 23:55:25.024138 140026042091264 logging_writer.py:48] [109483] accumulated_eval_time=1312.866316, accumulated_logging_time=2.776733, accumulated_submission_time=37266.679533, global_step=109483, preemption_count=0, score=37266.679533, test/accuracy=0.583500, test/loss=2.011931, test/num_examples=10000, total_duration=38586.275708, train/accuracy=0.814832, train/loss=0.931571, validation/accuracy=0.714220, validation/loss=1.358030, validation/num_examples=50000
I0127 23:55:31.135386 140026050483968 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.8156557083129883, loss=2.8932056427001953
I0127 23:56:05.052435 140026042091264 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.532451868057251, loss=2.845992088317871
I0127 23:56:38.999704 140026050483968 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.6583032608032227, loss=2.902235507965088
I0127 23:57:12.931793 140026042091264 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.7871007919311523, loss=2.92177677154541
I0127 23:57:46.893445 140026050483968 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.6060047149658203, loss=2.852144956588745
I0127 23:58:20.857642 140026042091264 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.6786935329437256, loss=2.9155380725860596
I0127 23:58:54.890667 140026050483968 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.804654598236084, loss=2.8612406253814697
I0127 23:59:28.901626 140026042091264 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.7016279697418213, loss=2.865870237350464
I0128 00:00:02.886631 140026050483968 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.851569414138794, loss=2.870673179626465
I0128 00:00:36.863435 140026042091264 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.769430160522461, loss=2.908705949783325
I0128 00:01:10.820190 140026050483968 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.7361791133880615, loss=2.890434741973877
I0128 00:01:44.785812 140026042091264 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.957655906677246, loss=2.9219095706939697
I0128 00:02:18.750001 140026050483968 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.849616527557373, loss=2.9102561473846436
I0128 00:02:52.741736 140026042091264 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.8516435623168945, loss=2.8860104084014893
I0128 00:03:26.695604 140026050483968 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.7722556591033936, loss=2.8970954418182373
I0128 00:03:54.995716 140187804313408 spec.py:321] Evaluating on the training split.
I0128 00:04:01.187679 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 00:04:10.391384 140187804313408 spec.py:349] Evaluating on the test split.
I0128 00:04:12.899396 140187804313408 submission_runner.py:408] Time since start: 39114.19s, 	Step: 110985, 	{'train/accuracy': 0.8172432780265808, 'train/loss': 0.890960693359375, 'validation/accuracy': 0.7128599882125854, 'validation/loss': 1.340518832206726, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 1.9949579238891602, 'test/num_examples': 10000, 'score': 37776.58733320236, 'total_duration': 39114.19459462166, 'accumulated_submission_time': 37776.58733320236, 'accumulated_eval_time': 1330.7699587345123, 'accumulated_logging_time': 2.829568386077881}
I0128 00:04:12.935699 140026050483968 logging_writer.py:48] [110985] accumulated_eval_time=1330.769959, accumulated_logging_time=2.829568, accumulated_submission_time=37776.587333, global_step=110985, preemption_count=0, score=37776.587333, test/accuracy=0.581100, test/loss=1.994958, test/num_examples=10000, total_duration=39114.194595, train/accuracy=0.817243, train/loss=0.890961, validation/accuracy=0.712860, validation/loss=1.340519, validation/num_examples=50000
I0128 00:04:18.347020 140026159523584 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.665255069732666, loss=2.8589022159576416
I0128 00:04:52.272380 140026050483968 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.8465046882629395, loss=2.871119499206543
I0128 00:05:26.305307 140026159523584 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.960188388824463, loss=2.9415619373321533
I0128 00:06:00.264585 140026050483968 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.7638790607452393, loss=2.8791050910949707
I0128 00:06:34.218897 140026159523584 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.734750270843506, loss=2.8013827800750732
I0128 00:07:08.166187 140026050483968 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.746861696243286, loss=2.815394878387451
I0128 00:07:42.157017 140026159523584 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.7334485054016113, loss=2.8847172260284424
I0128 00:08:16.126235 140026050483968 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.7001476287841797, loss=2.8539211750030518
I0128 00:08:50.129710 140026159523584 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.593003273010254, loss=2.8296258449554443
I0128 00:09:24.102488 140026050483968 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.8297781944274902, loss=2.895306348800659
I0128 00:09:58.097068 140026159523584 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.786538600921631, loss=2.930922031402588
I0128 00:10:32.065243 140026050483968 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.917471170425415, loss=2.911489963531494
I0128 00:11:06.059061 140026159523584 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.6109609603881836, loss=2.790691614151001
I0128 00:11:40.095286 140026050483968 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.722511053085327, loss=2.831341028213501
I0128 00:12:14.070764 140026159523584 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.7200474739074707, loss=2.916822910308838
I0128 00:12:43.067710 140187804313408 spec.py:321] Evaluating on the training split.
I0128 00:12:49.552213 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 00:12:58.482562 140187804313408 spec.py:349] Evaluating on the test split.
I0128 00:13:00.871415 140187804313408 submission_runner.py:408] Time since start: 39642.17s, 	Step: 112487, 	{'train/accuracy': 0.8461814522743225, 'train/loss': 0.7963310480117798, 'validation/accuracy': 0.7188400030136108, 'validation/loss': 1.3273828029632568, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9578930139541626, 'test/num_examples': 10000, 'score': 38286.656383514404, 'total_duration': 39642.166607141495, 'accumulated_submission_time': 38286.656383514404, 'accumulated_eval_time': 1348.5736198425293, 'accumulated_logging_time': 2.8754875659942627}
I0128 00:13:00.909687 140026075662080 logging_writer.py:48] [112487] accumulated_eval_time=1348.573620, accumulated_logging_time=2.875488, accumulated_submission_time=38286.656384, global_step=112487, preemption_count=0, score=38286.656384, test/accuracy=0.596300, test/loss=1.957893, test/num_examples=10000, total_duration=39642.166607, train/accuracy=0.846181, train/loss=0.796331, validation/accuracy=0.718840, validation/loss=1.327383, validation/num_examples=50000
I0128 00:13:05.673680 140026151130880 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.7729830741882324, loss=2.8967037200927734
I0128 00:13:39.585009 140026075662080 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.6670281887054443, loss=2.8471784591674805
I0128 00:14:13.513569 140026151130880 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.7063872814178467, loss=2.8202431201934814
I0128 00:14:47.497340 140026075662080 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.7675373554229736, loss=2.887403964996338
I0128 00:15:21.445986 140026151130880 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.8666818141937256, loss=2.9187493324279785
I0128 00:15:55.420172 140026075662080 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.6659374237060547, loss=2.872438430786133
I0128 00:16:29.361702 140026151130880 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.8797683715820312, loss=2.9054043292999268
I0128 00:17:03.315096 140026075662080 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.848128318786621, loss=2.8133552074432373
I0128 00:17:37.352912 140026151130880 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.8120970726013184, loss=2.8329482078552246
I0128 00:18:11.288816 140026075662080 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.9503655433654785, loss=2.8729257583618164
I0128 00:18:45.240620 140026151130880 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.906224489212036, loss=2.924506664276123
I0128 00:19:19.187932 140026075662080 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.845508337020874, loss=2.801244020462036
I0128 00:19:53.156478 140026151130880 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.862386465072632, loss=2.851914167404175
I0128 00:20:27.133534 140026075662080 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.846486806869507, loss=2.8127284049987793
I0128 00:21:01.075700 140026151130880 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.8498010635375977, loss=2.828272581100464
I0128 00:21:31.109088 140187804313408 spec.py:321] Evaluating on the training split.
I0128 00:21:37.340157 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 00:21:46.114730 140187804313408 spec.py:349] Evaluating on the test split.
I0128 00:21:48.606489 140187804313408 submission_runner.py:408] Time since start: 40169.90s, 	Step: 113990, 	{'train/accuracy': 0.8255141973495483, 'train/loss': 0.8634473085403442, 'validation/accuracy': 0.7139399647712708, 'validation/loss': 1.3523683547973633, 'validation/num_examples': 50000, 'test/accuracy': 0.5891000032424927, 'test/loss': 2.0083277225494385, 'test/num_examples': 10000, 'score': 38796.7903342247, 'total_duration': 40169.901686668396, 'accumulated_submission_time': 38796.7903342247, 'accumulated_eval_time': 1366.0709924697876, 'accumulated_logging_time': 2.9247477054595947}
I0128 00:21:48.644229 140026050483968 logging_writer.py:48] [113990] accumulated_eval_time=1366.070992, accumulated_logging_time=2.924748, accumulated_submission_time=38796.790334, global_step=113990, preemption_count=0, score=38796.790334, test/accuracy=0.589100, test/loss=2.008328, test/num_examples=10000, total_duration=40169.901687, train/accuracy=0.825514, train/loss=0.863447, validation/accuracy=0.713940, validation/loss=1.352368, validation/num_examples=50000
I0128 00:21:52.372328 140026058876672 logging_writer.py:48] [114000] global_step=114000, grad_norm=3.009093761444092, loss=2.840832233428955
I0128 00:22:26.316642 140026050483968 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.883549690246582, loss=2.8335108757019043
I0128 00:23:00.282960 140026058876672 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.8218629360198975, loss=2.9546518325805664
I0128 00:23:34.265339 140026050483968 logging_writer.py:48] [114300] global_step=114300, grad_norm=3.035723924636841, loss=2.854229688644409
I0128 00:24:08.287006 140026058876672 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.9274473190307617, loss=2.8643276691436768
I0128 00:24:42.260014 140026050483968 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.685839891433716, loss=2.8366546630859375
I0128 00:25:16.259124 140026058876672 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.788036346435547, loss=2.865427017211914
I0128 00:25:50.248183 140026050483968 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.789684534072876, loss=2.8792948722839355
I0128 00:26:24.239086 140026058876672 logging_writer.py:48] [114800] global_step=114800, grad_norm=3.1574928760528564, loss=2.8854422569274902
I0128 00:26:58.210977 140026050483968 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.8777003288269043, loss=2.816990852355957
I0128 00:27:32.196468 140026058876672 logging_writer.py:48] [115000] global_step=115000, grad_norm=3.1392502784729004, loss=2.8656437397003174
I0128 00:28:06.163081 140026050483968 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.855304718017578, loss=2.801596164703369
I0128 00:28:40.166275 140026058876672 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.9733974933624268, loss=2.830045700073242
I0128 00:29:14.130739 140026050483968 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.8216397762298584, loss=2.8570239543914795
I0128 00:29:48.112503 140026058876672 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.946807861328125, loss=2.8450162410736084
I0128 00:30:18.860082 140187804313408 spec.py:321] Evaluating on the training split.
I0128 00:30:25.084515 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 00:30:34.156299 140187804313408 spec.py:349] Evaluating on the test split.
I0128 00:30:36.624721 140187804313408 submission_runner.py:408] Time since start: 40697.92s, 	Step: 115492, 	{'train/accuracy': 0.8307756781578064, 'train/loss': 0.861309826374054, 'validation/accuracy': 0.7196199893951416, 'validation/loss': 1.3414368629455566, 'validation/num_examples': 50000, 'test/accuracy': 0.5921000242233276, 'test/loss': 1.9662647247314453, 'test/num_examples': 10000, 'score': 39306.942506313324, 'total_duration': 40697.91963505745, 'accumulated_submission_time': 39306.942506313324, 'accumulated_eval_time': 1383.8353281021118, 'accumulated_logging_time': 2.9718353748321533}
I0128 00:30:36.664916 140026151130880 logging_writer.py:48] [115492] accumulated_eval_time=1383.835328, accumulated_logging_time=2.971835, accumulated_submission_time=39306.942506, global_step=115492, preemption_count=0, score=39306.942506, test/accuracy=0.592100, test/loss=1.966265, test/num_examples=10000, total_duration=40697.919635, train/accuracy=0.830776, train/loss=0.861310, validation/accuracy=0.719620, validation/loss=1.341437, validation/num_examples=50000
I0128 00:30:39.736127 140026167916288 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.690145492553711, loss=2.8355393409729004
I0128 00:31:13.677266 140026151130880 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.8488287925720215, loss=2.8211240768432617
I0128 00:31:47.616662 140026167916288 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.959235906600952, loss=2.8418829441070557
I0128 00:32:21.553109 140026151130880 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.7173712253570557, loss=2.8136725425720215
I0128 00:32:55.494708 140026167916288 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.7290701866149902, loss=2.819672107696533
I0128 00:33:29.482519 140026151130880 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.843524217605591, loss=2.8455564975738525
I0128 00:34:03.447193 140026167916288 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.8990795612335205, loss=2.9054298400878906
I0128 00:34:37.435636 140026151130880 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.9468131065368652, loss=2.850273847579956
I0128 00:35:11.403214 140026167916288 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.955632448196411, loss=2.897014617919922
I0128 00:35:45.392529 140026151130880 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.9955060482025146, loss=2.803830146789551
I0128 00:36:19.400901 140026167916288 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.9560906887054443, loss=2.948885917663574
I0128 00:36:53.380552 140026151130880 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.792813539505005, loss=2.933373212814331
I0128 00:37:27.330772 140026167916288 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.8184194564819336, loss=2.836785078048706
I0128 00:38:01.292761 140026151130880 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.801668882369995, loss=2.824902057647705
I0128 00:38:35.235424 140026167916288 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.0160722732543945, loss=2.836369276046753
I0128 00:39:06.957211 140187804313408 spec.py:321] Evaluating on the training split.
I0128 00:39:13.159849 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 00:39:22.046212 140187804313408 spec.py:349] Evaluating on the test split.
I0128 00:39:24.544423 140187804313408 submission_runner.py:408] Time since start: 41225.84s, 	Step: 116995, 	{'train/accuracy': 0.8356983065605164, 'train/loss': 0.8533991575241089, 'validation/accuracy': 0.7249400019645691, 'validation/loss': 1.3167498111724854, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.9473471641540527, 'test/num_examples': 10000, 'score': 39817.17044043541, 'total_duration': 41225.83962345123, 'accumulated_submission_time': 39817.17044043541, 'accumulated_eval_time': 1401.4225118160248, 'accumulated_logging_time': 3.0217180252075195}
I0128 00:39:24.582235 140026067269376 logging_writer.py:48] [116995] accumulated_eval_time=1401.422512, accumulated_logging_time=3.021718, accumulated_submission_time=39817.170440, global_step=116995, preemption_count=0, score=39817.170440, test/accuracy=0.595500, test/loss=1.947347, test/num_examples=10000, total_duration=41225.839623, train/accuracy=0.835698, train/loss=0.853399, validation/accuracy=0.724940, validation/loss=1.316750, validation/num_examples=50000
I0128 00:39:26.611366 140026075662080 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.9051191806793213, loss=2.8671517372131348
I0128 00:40:00.500640 140026067269376 logging_writer.py:48] [117100] global_step=117100, grad_norm=3.1569716930389404, loss=2.874819278717041
I0128 00:40:34.429499 140026075662080 logging_writer.py:48] [117200] global_step=117200, grad_norm=3.0029914379119873, loss=2.8634350299835205
I0128 00:41:08.395219 140026067269376 logging_writer.py:48] [117300] global_step=117300, grad_norm=3.014216899871826, loss=2.871962785720825
I0128 00:41:42.382651 140026075662080 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.8759095668792725, loss=2.8615975379943848
I0128 00:42:16.343714 140026067269376 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.8866727352142334, loss=2.8601040840148926
I0128 00:42:50.385516 140026075662080 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.911979913711548, loss=2.872814655303955
I0128 00:43:24.384486 140026067269376 logging_writer.py:48] [117700] global_step=117700, grad_norm=3.009342908859253, loss=2.783926248550415
I0128 00:43:58.379825 140026075662080 logging_writer.py:48] [117800] global_step=117800, grad_norm=3.0060791969299316, loss=2.853271245956421
I0128 00:44:32.354221 140026067269376 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.999638080596924, loss=2.832077741622925
I0128 00:45:06.333504 140026075662080 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.9277162551879883, loss=2.8120651245117188
I0128 00:45:40.275438 140026067269376 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.9088191986083984, loss=2.807133197784424
I0128 00:46:14.260408 140026075662080 logging_writer.py:48] [118200] global_step=118200, grad_norm=3.055793523788452, loss=2.8656678199768066
I0128 00:46:48.209438 140026067269376 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.7309651374816895, loss=2.7769839763641357
I0128 00:47:22.194944 140026075662080 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.753816604614258, loss=2.8198986053466797
I0128 00:47:54.605470 140187804313408 spec.py:321] Evaluating on the training split.
I0128 00:48:01.371370 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 00:48:10.518213 140187804313408 spec.py:349] Evaluating on the test split.
I0128 00:48:12.988625 140187804313408 submission_runner.py:408] Time since start: 41754.28s, 	Step: 118497, 	{'train/accuracy': 0.8339644074440002, 'train/loss': 0.8613070249557495, 'validation/accuracy': 0.7239999771118164, 'validation/loss': 1.3303016424179077, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.9697046279907227, 'test/num_examples': 10000, 'score': 40327.12989664078, 'total_duration': 41754.2838177681, 'accumulated_submission_time': 40327.12989664078, 'accumulated_eval_time': 1419.805627822876, 'accumulated_logging_time': 3.0692250728607178}
I0128 00:48:13.028883 140026151130880 logging_writer.py:48] [118497] accumulated_eval_time=1419.805628, accumulated_logging_time=3.069225, accumulated_submission_time=40327.129897, global_step=118497, preemption_count=0, score=40327.129897, test/accuracy=0.594100, test/loss=1.969705, test/num_examples=10000, total_duration=41754.283818, train/accuracy=0.833964, train/loss=0.861307, validation/accuracy=0.724000, validation/loss=1.330302, validation/num_examples=50000
I0128 00:48:14.405392 140026167916288 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.898468255996704, loss=2.9196486473083496
I0128 00:48:48.289458 140026151130880 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.0662713050842285, loss=2.804246664047241
I0128 00:49:22.387367 140026167916288 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.746582269668579, loss=2.773981809616089
I0128 00:49:56.335096 140026151130880 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.069991111755371, loss=2.8711235523223877
I0128 00:50:30.300109 140026167916288 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.9066383838653564, loss=2.8022007942199707
I0128 00:51:04.230991 140026151130880 logging_writer.py:48] [119000] global_step=119000, grad_norm=3.107464075088501, loss=2.8152196407318115
I0128 00:51:38.190222 140026167916288 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.015566349029541, loss=2.8321821689605713
I0128 00:52:12.187453 140026151130880 logging_writer.py:48] [119200] global_step=119200, grad_norm=3.0216476917266846, loss=2.805039882659912
I0128 00:52:46.152225 140026167916288 logging_writer.py:48] [119300] global_step=119300, grad_norm=3.0270893573760986, loss=2.823230266571045
I0128 00:53:20.083338 140026151130880 logging_writer.py:48] [119400] global_step=119400, grad_norm=3.090576410293579, loss=2.8451313972473145
I0128 00:53:54.084910 140026167916288 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.974118232727051, loss=2.783576488494873
I0128 00:54:28.023653 140026151130880 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.0990591049194336, loss=2.88962984085083
I0128 00:55:02.006332 140026167916288 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.9151182174682617, loss=2.833702564239502
I0128 00:55:36.099987 140026151130880 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.8962934017181396, loss=2.8118157386779785
I0128 00:56:10.082387 140026167916288 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.069059371948242, loss=2.8083932399749756
I0128 00:56:43.201186 140187804313408 spec.py:321] Evaluating on the training split.
I0128 00:56:49.410376 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 00:56:58.507068 140187804313408 spec.py:349] Evaluating on the test split.
I0128 00:57:01.047351 140187804313408 submission_runner.py:408] Time since start: 42282.34s, 	Step: 119999, 	{'train/accuracy': 0.8390266299247742, 'train/loss': 0.8340352177619934, 'validation/accuracy': 0.7273600101470947, 'validation/loss': 1.301812767982483, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.9222824573516846, 'test/num_examples': 10000, 'score': 40837.23765873909, 'total_duration': 42282.342547655106, 'accumulated_submission_time': 40837.23765873909, 'accumulated_eval_time': 1437.6517629623413, 'accumulated_logging_time': 3.1198184490203857}
I0128 00:57:01.089761 140026058876672 logging_writer.py:48] [119999] accumulated_eval_time=1437.651763, accumulated_logging_time=3.119818, accumulated_submission_time=40837.237659, global_step=119999, preemption_count=0, score=40837.237659, test/accuracy=0.605600, test/loss=1.922282, test/num_examples=10000, total_duration=42282.342548, train/accuracy=0.839027, train/loss=0.834035, validation/accuracy=0.727360, validation/loss=1.301813, validation/num_examples=50000
I0128 00:57:01.772056 140026067269376 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.05961275100708, loss=2.8160653114318848
I0128 00:57:35.684449 140026058876672 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.9980227947235107, loss=2.8341434001922607
I0128 00:58:09.588497 140026067269376 logging_writer.py:48] [120200] global_step=120200, grad_norm=3.1705784797668457, loss=2.8249850273132324
I0128 00:58:43.563902 140026058876672 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.917708158493042, loss=2.7975106239318848
I0128 00:59:17.547347 140026067269376 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.1004250049591064, loss=2.8897147178649902
I0128 00:59:51.512071 140026058876672 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.141850471496582, loss=2.8257384300231934
I0128 01:00:25.497385 140026067269376 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.9472813606262207, loss=2.74381685256958
I0128 01:00:59.480241 140026058876672 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.967177391052246, loss=2.9151155948638916
I0128 01:01:33.503257 140026067269376 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.0596561431884766, loss=2.78373122215271
I0128 01:02:07.508756 140026058876672 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.9757676124572754, loss=2.820103645324707
I0128 01:02:41.461641 140026067269376 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.96478271484375, loss=2.7908453941345215
I0128 01:03:15.431303 140026058876672 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.8703904151916504, loss=2.780658721923828
I0128 01:03:49.407727 140026067269376 logging_writer.py:48] [121200] global_step=121200, grad_norm=3.2167303562164307, loss=2.8304569721221924
I0128 01:04:23.403546 140026058876672 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.2658138275146484, loss=2.810525894165039
I0128 01:04:57.340974 140026067269376 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.017094373703003, loss=2.799564838409424
I0128 01:05:31.313425 140026058876672 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.937772274017334, loss=2.734335422515869
I0128 01:05:31.320571 140187804313408 spec.py:321] Evaluating on the training split.
I0128 01:05:37.438761 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 01:05:46.174057 140187804313408 spec.py:349] Evaluating on the test split.
I0128 01:05:48.757093 140187804313408 submission_runner.py:408] Time since start: 42810.05s, 	Step: 121501, 	{'train/accuracy': 0.8613081574440002, 'train/loss': 0.7704369425773621, 'validation/accuracy': 0.7250399589538574, 'validation/loss': 1.3356198072433472, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.9819520711898804, 'test/num_examples': 10000, 'score': 41347.405833005905, 'total_duration': 42810.052292108536, 'accumulated_submission_time': 41347.405833005905, 'accumulated_eval_time': 1455.0882284641266, 'accumulated_logging_time': 3.1719682216644287}
I0128 01:05:48.798050 140026159523584 logging_writer.py:48] [121501] accumulated_eval_time=1455.088228, accumulated_logging_time=3.171968, accumulated_submission_time=41347.405833, global_step=121501, preemption_count=0, score=41347.405833, test/accuracy=0.601000, test/loss=1.981952, test/num_examples=10000, total_duration=42810.052292, train/accuracy=0.861308, train/loss=0.770437, validation/accuracy=0.725040, validation/loss=1.335620, validation/num_examples=50000
I0128 01:06:22.682892 140026167916288 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.05472731590271, loss=2.8377327919006348
I0128 01:06:56.606091 140026159523584 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.142030954360962, loss=2.834986925125122
I0128 01:07:30.573580 140026167916288 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.9797604084014893, loss=2.77026629447937
I0128 01:08:04.579343 140026159523584 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.770937442779541, loss=2.8199820518493652
I0128 01:08:38.532119 140026167916288 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.2016100883483887, loss=2.8445584774017334
I0128 01:09:12.451203 140026159523584 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.961486577987671, loss=2.794572114944458
I0128 01:09:46.422791 140026167916288 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.1299173831939697, loss=2.886439323425293
I0128 01:10:20.380351 140026159523584 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.1054840087890625, loss=2.798642635345459
I0128 01:10:54.335967 140026167916288 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.8649871349334717, loss=2.8163344860076904
I0128 01:11:28.270450 140026159523584 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.9942359924316406, loss=2.7897191047668457
I0128 01:12:02.230899 140026167916288 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.0659067630767822, loss=2.8123228549957275
I0128 01:12:36.218615 140026159523584 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.030219793319702, loss=2.797377586364746
I0128 01:13:10.177363 140026167916288 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.8580873012542725, loss=2.774390935897827
I0128 01:13:44.134098 140026159523584 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.1994645595550537, loss=2.8312628269195557
I0128 01:14:18.156886 140026167916288 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.8267738819122314, loss=2.7580108642578125
I0128 01:14:18.979036 140187804313408 spec.py:321] Evaluating on the training split.
I0128 01:14:25.089468 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 01:14:33.819586 140187804313408 spec.py:349] Evaluating on the test split.
I0128 01:14:36.290196 140187804313408 submission_runner.py:408] Time since start: 43337.59s, 	Step: 123004, 	{'train/accuracy': 0.8552694320678711, 'train/loss': 0.7926141023635864, 'validation/accuracy': 0.7265399694442749, 'validation/loss': 1.3171532154083252, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.9438683986663818, 'test/num_examples': 10000, 'score': 41857.52238154411, 'total_duration': 43337.58539104462, 'accumulated_submission_time': 41857.52238154411, 'accumulated_eval_time': 1472.3993520736694, 'accumulated_logging_time': 3.2225170135498047}
I0128 01:14:36.328546 140026050483968 logging_writer.py:48] [123004] accumulated_eval_time=1472.399352, accumulated_logging_time=3.222517, accumulated_submission_time=41857.522382, global_step=123004, preemption_count=0, score=41857.522382, test/accuracy=0.605500, test/loss=1.943868, test/num_examples=10000, total_duration=43337.585391, train/accuracy=0.855269, train/loss=0.792614, validation/accuracy=0.726540, validation/loss=1.317153, validation/num_examples=50000
I0128 01:15:09.234843 140026058876672 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.1109566688537598, loss=2.8243136405944824
I0128 01:15:43.167523 140026050483968 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.1104683876037598, loss=2.8059592247009277
I0128 01:16:17.130365 140026058876672 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.1356921195983887, loss=2.8609328269958496
I0128 01:16:51.147278 140026050483968 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.017728328704834, loss=2.80704665184021
I0128 01:17:25.118706 140026058876672 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.042725086212158, loss=2.81730055809021
I0128 01:17:59.122214 140026050483968 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.014786720275879, loss=2.837620735168457
I0128 01:18:33.099739 140026058876672 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.293869733810425, loss=2.8650283813476562
I0128 01:19:07.082822 140026050483968 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.1887333393096924, loss=2.751554012298584
I0128 01:19:41.063429 140026058876672 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.0422511100769043, loss=2.8455147743225098
I0128 01:20:15.134918 140026050483968 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.0113437175750732, loss=2.7792398929595947
I0128 01:20:49.105859 140026058876672 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.214754104614258, loss=2.831425189971924
I0128 01:21:23.086836 140026050483968 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.007388114929199, loss=2.8092525005340576
I0128 01:21:57.074573 140026058876672 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.1390514373779297, loss=2.811156749725342
I0128 01:22:31.057488 140026050483968 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.8928418159484863, loss=2.763383150100708
I0128 01:23:05.050590 140026058876672 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.1622872352600098, loss=2.7526583671569824
I0128 01:23:06.560258 140187804313408 spec.py:321] Evaluating on the training split.
I0128 01:23:12.775669 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 01:23:21.597275 140187804313408 spec.py:349] Evaluating on the test split.
I0128 01:23:24.011076 140187804313408 submission_runner.py:408] Time since start: 43865.31s, 	Step: 124506, 	{'train/accuracy': 0.8481743931770325, 'train/loss': 0.8100681304931641, 'validation/accuracy': 0.7289199829101562, 'validation/loss': 1.3144792318344116, 'validation/num_examples': 50000, 'test/accuracy': 0.6068000197410583, 'test/loss': 1.9477180242538452, 'test/num_examples': 10000, 'score': 42367.69151568413, 'total_duration': 43865.30625462532, 'accumulated_submission_time': 42367.69151568413, 'accumulated_eval_time': 1489.850107908249, 'accumulated_logging_time': 3.269981622695923}
I0128 01:23:24.051605 140026151130880 logging_writer.py:48] [124506] accumulated_eval_time=1489.850108, accumulated_logging_time=3.269982, accumulated_submission_time=42367.691516, global_step=124506, preemption_count=0, score=42367.691516, test/accuracy=0.606800, test/loss=1.947718, test/num_examples=10000, total_duration=43865.306255, train/accuracy=0.848174, train/loss=0.810068, validation/accuracy=0.728920, validation/loss=1.314479, validation/num_examples=50000
I0128 01:23:56.266095 140026159523584 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.141690969467163, loss=2.782904624938965
I0128 01:24:30.229537 140026151130880 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.3443500995635986, loss=2.8051557540893555
I0128 01:25:04.170412 140026159523584 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0673530101776123, loss=2.7483067512512207
I0128 01:25:38.122415 140026151130880 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.2047221660614014, loss=2.835683584213257
I0128 01:26:12.066248 140026159523584 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.9675815105438232, loss=2.76176381111145
I0128 01:26:46.081756 140026151130880 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.1154251098632812, loss=2.763190269470215
I0128 01:27:20.037629 140026159523584 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.0798051357269287, loss=2.745502233505249
I0128 01:27:54.014963 140026151130880 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.04522705078125, loss=2.787388801574707
I0128 01:28:27.963687 140026159523584 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.1321661472320557, loss=2.8079488277435303
I0128 01:29:01.890281 140026151130880 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.228938579559326, loss=2.7835707664489746
I0128 01:29:35.879894 140026159523584 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.1332128047943115, loss=2.787764310836792
I0128 01:30:09.849470 140026151130880 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.0799763202667236, loss=2.7885546684265137
I0128 01:30:43.803275 140026159523584 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.303370237350464, loss=2.806696653366089
I0128 01:31:17.771714 140026151130880 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.0914244651794434, loss=2.7967162132263184
I0128 01:31:51.720496 140026159523584 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.200385808944702, loss=2.816734790802002
I0128 01:31:54.248211 140187804313408 spec.py:321] Evaluating on the training split.
I0128 01:32:00.375774 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 01:32:09.648853 140187804313408 spec.py:349] Evaluating on the test split.
I0128 01:32:12.110038 140187804313408 submission_runner.py:408] Time since start: 44393.41s, 	Step: 126009, 	{'train/accuracy': 0.845723032951355, 'train/loss': 0.7846372127532959, 'validation/accuracy': 0.7291799783706665, 'validation/loss': 1.2905118465423584, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.9218626022338867, 'test/num_examples': 10000, 'score': 42877.82626962662, 'total_duration': 44393.40523290634, 'accumulated_submission_time': 42877.82626962662, 'accumulated_eval_time': 1507.7119023799896, 'accumulated_logging_time': 3.3195204734802246}
I0128 01:32:12.149098 140026042091264 logging_writer.py:48] [126009] accumulated_eval_time=1507.711902, accumulated_logging_time=3.319520, accumulated_submission_time=42877.826270, global_step=126009, preemption_count=0, score=42877.826270, test/accuracy=0.603400, test/loss=1.921863, test/num_examples=10000, total_duration=44393.405233, train/accuracy=0.845723, train/loss=0.784637, validation/accuracy=0.729180, validation/loss=1.290512, validation/num_examples=50000
I0128 01:32:43.318886 140026058876672 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.0250842571258545, loss=2.7561943531036377
I0128 01:33:17.383359 140026042091264 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.963162899017334, loss=2.7561051845550537
I0128 01:33:51.291826 140026058876672 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.008145332336426, loss=2.7575323581695557
I0128 01:34:25.240458 140026042091264 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.249520778656006, loss=2.7694051265716553
I0128 01:34:59.219404 140026058876672 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.9630420207977295, loss=2.710710048675537
I0128 01:35:33.190894 140026042091264 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.255924701690674, loss=2.8048906326293945
I0128 01:36:07.164338 140026058876672 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.193300485610962, loss=2.838625431060791
I0128 01:36:41.140247 140026042091264 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.168353319168091, loss=2.800375461578369
I0128 01:37:15.088473 140026058876672 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.1398837566375732, loss=2.7691547870635986
I0128 01:37:49.077070 140026042091264 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.201082706451416, loss=2.7271246910095215
I0128 01:38:23.022222 140026058876672 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.1489837169647217, loss=2.7659640312194824
I0128 01:38:56.982879 140026042091264 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.3181183338165283, loss=2.7555036544799805
I0128 01:39:31.161955 140026058876672 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.0915729999542236, loss=2.8144643306732178
I0128 01:40:05.137846 140026042091264 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.0517008304595947, loss=2.7556164264678955
I0128 01:40:39.156712 140026058876672 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.0321242809295654, loss=2.739208221435547
I0128 01:40:42.365467 140187804313408 spec.py:321] Evaluating on the training split.
I0128 01:40:48.488767 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 01:40:57.480665 140187804313408 spec.py:349] Evaluating on the test split.
I0128 01:41:00.027683 140187804313408 submission_runner.py:408] Time since start: 44921.32s, 	Step: 127511, 	{'train/accuracy': 0.8580795526504517, 'train/loss': 0.7403988838195801, 'validation/accuracy': 0.7360000014305115, 'validation/loss': 1.253514289855957, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.869235634803772, 'test/num_examples': 10000, 'score': 43387.9768986702, 'total_duration': 44921.322884082794, 'accumulated_submission_time': 43387.9768986702, 'accumulated_eval_time': 1525.3740797042847, 'accumulated_logging_time': 3.3690085411071777}
I0128 01:41:00.066281 140026050483968 logging_writer.py:48] [127511] accumulated_eval_time=1525.374080, accumulated_logging_time=3.369009, accumulated_submission_time=43387.976899, global_step=127511, preemption_count=0, score=43387.976899, test/accuracy=0.615600, test/loss=1.869236, test/num_examples=10000, total_duration=44921.322884, train/accuracy=0.858080, train/loss=0.740399, validation/accuracy=0.736000, validation/loss=1.253514, validation/num_examples=50000
I0128 01:41:30.538402 140026159523584 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.387742280960083, loss=2.764462471008301
I0128 01:42:04.462317 140026050483968 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.207319736480713, loss=2.730710983276367
I0128 01:42:38.423413 140026159523584 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.2388439178466797, loss=2.8346457481384277
I0128 01:43:12.390017 140026050483968 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.986938238143921, loss=2.7250704765319824
I0128 01:43:46.340735 140026159523584 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.359560012817383, loss=2.8236050605773926
I0128 01:44:20.268601 140026050483968 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.015437126159668, loss=2.7744126319885254
I0128 01:44:54.251397 140026159523584 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.3751425743103027, loss=2.7655062675476074
I0128 01:45:28.248518 140026050483968 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.4192564487457275, loss=2.763334035873413
I0128 01:46:02.219622 140026159523584 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.996824264526367, loss=2.702793836593628
I0128 01:46:36.158722 140026050483968 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.2728545665740967, loss=2.8101038932800293
I0128 01:47:10.108014 140026159523584 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.8856096267700195, loss=2.7156078815460205
I0128 01:47:44.088071 140026050483968 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.1619372367858887, loss=2.7298285961151123
I0128 01:48:18.046376 140026159523584 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.2116992473602295, loss=2.8841307163238525
I0128 01:48:52.003044 140026050483968 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.1218600273132324, loss=2.7952826023101807
I0128 01:49:25.985698 140026159523584 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.4521100521087646, loss=2.8026227951049805
I0128 01:49:30.200816 140187804313408 spec.py:321] Evaluating on the training split.
I0128 01:49:36.328158 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 01:49:45.187810 140187804313408 spec.py:349] Evaluating on the test split.
I0128 01:49:47.694790 140187804313408 submission_runner.py:408] Time since start: 45448.99s, 	Step: 129014, 	{'train/accuracy': 0.85550856590271, 'train/loss': 0.7635818123817444, 'validation/accuracy': 0.7354599833488464, 'validation/loss': 1.2662724256515503, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.904442310333252, 'test/num_examples': 10000, 'score': 43898.0478746891, 'total_duration': 45448.98998808861, 'accumulated_submission_time': 43898.0478746891, 'accumulated_eval_time': 1542.8680157661438, 'accumulated_logging_time': 3.4177677631378174}
I0128 01:49:47.739100 140026075662080 logging_writer.py:48] [129014] accumulated_eval_time=1542.868016, accumulated_logging_time=3.417768, accumulated_submission_time=43898.047875, global_step=129014, preemption_count=0, score=43898.047875, test/accuracy=0.605600, test/loss=1.904442, test/num_examples=10000, total_duration=45448.989988, train/accuracy=0.855509, train/loss=0.763582, validation/accuracy=0.735460, validation/loss=1.266272, validation/num_examples=50000
I0128 01:50:17.248350 140026151130880 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.3427937030792236, loss=2.8001596927642822
I0128 01:50:51.183768 140026075662080 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.2067947387695312, loss=2.742924690246582
I0128 01:51:25.157657 140026151130880 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.4414584636688232, loss=2.837203025817871
I0128 01:51:59.344429 140026075662080 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.1558918952941895, loss=2.766526222229004
I0128 01:52:33.288789 140026151130880 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.3149611949920654, loss=2.8391528129577637
I0128 01:53:07.261382 140026075662080 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.158327102661133, loss=2.7747342586517334
I0128 01:53:41.226736 140026151130880 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.0128934383392334, loss=2.706228733062744
I0128 01:54:15.188965 140026075662080 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.2247087955474854, loss=2.785186529159546
I0128 01:54:49.166726 140026151130880 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.502044200897217, loss=2.727715015411377
I0128 01:55:23.136619 140026075662080 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.514524459838867, loss=2.778369188308716
I0128 01:55:57.074273 140026151130880 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.3277535438537598, loss=2.7308833599090576
I0128 01:56:31.045953 140026075662080 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.306299924850464, loss=2.7246274948120117
I0128 01:57:05.024896 140026151130880 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.0848655700683594, loss=2.7794785499572754
I0128 01:57:38.994848 140026075662080 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.571288585662842, loss=2.7714614868164062
I0128 01:58:13.155180 140026151130880 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.2604665756225586, loss=2.802804470062256
I0128 01:58:17.709810 140187804313408 spec.py:321] Evaluating on the training split.
I0128 01:58:23.836536 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 01:58:32.613206 140187804313408 spec.py:349] Evaluating on the test split.
I0128 01:58:35.103834 140187804313408 submission_runner.py:408] Time since start: 45976.40s, 	Step: 130515, 	{'train/accuracy': 0.8826330900192261, 'train/loss': 0.6655307412147522, 'validation/accuracy': 0.7340999841690063, 'validation/loss': 1.2825183868408203, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.9217170476913452, 'test/num_examples': 10000, 'score': 44407.95130634308, 'total_duration': 45976.39903450012, 'accumulated_submission_time': 44407.95130634308, 'accumulated_eval_time': 1560.2620012760162, 'accumulated_logging_time': 3.4746148586273193}
I0128 01:58:35.147196 140026058876672 logging_writer.py:48] [130515] accumulated_eval_time=1560.262001, accumulated_logging_time=3.474615, accumulated_submission_time=44407.951306, global_step=130515, preemption_count=0, score=44407.951306, test/accuracy=0.610500, test/loss=1.921717, test/num_examples=10000, total_duration=45976.399035, train/accuracy=0.882633, train/loss=0.665531, validation/accuracy=0.734100, validation/loss=1.282518, validation/num_examples=50000
I0128 01:59:04.340484 140026067269376 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.3120882511138916, loss=2.799062967300415
I0128 01:59:38.224112 140026058876672 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.367194175720215, loss=2.7131030559539795
I0128 02:00:12.144496 140026067269376 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.264735221862793, loss=2.7231030464172363
I0128 02:00:46.083931 140026058876672 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.200251817703247, loss=2.7523903846740723
I0128 02:01:20.029119 140026067269376 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.1949574947357178, loss=2.725581169128418
I0128 02:01:53.967899 140026058876672 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.097560167312622, loss=2.7562618255615234
I0128 02:02:27.933533 140026067269376 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.1082656383514404, loss=2.724534034729004
I0128 02:03:01.892796 140026058876672 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.1780591011047363, loss=2.735287666320801
I0128 02:03:35.874056 140026067269376 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.525512933731079, loss=2.768630027770996
I0128 02:04:09.850895 140026058876672 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.1669905185699463, loss=2.708364963531494
I0128 02:04:43.930280 140026067269376 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.384201765060425, loss=2.776719808578491
I0128 02:05:17.865373 140026058876672 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.1097378730773926, loss=2.6904165744781494
I0128 02:05:51.851179 140026067269376 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.2794229984283447, loss=2.664477825164795
I0128 02:06:25.812067 140026058876672 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.3264760971069336, loss=2.7463021278381348
I0128 02:06:59.786442 140026067269376 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.293302297592163, loss=2.7419447898864746
I0128 02:07:05.367808 140187804313408 spec.py:321] Evaluating on the training split.
I0128 02:07:11.522800 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 02:07:20.674485 140187804313408 spec.py:349] Evaluating on the test split.
I0128 02:07:23.182600 140187804313408 submission_runner.py:408] Time since start: 46504.48s, 	Step: 132018, 	{'train/accuracy': 0.8704758882522583, 'train/loss': 0.7248345613479614, 'validation/accuracy': 0.731499969959259, 'validation/loss': 1.2995328903198242, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.936708688735962, 'test/num_examples': 10000, 'score': 44918.10623574257, 'total_duration': 46504.47780227661, 'accumulated_submission_time': 44918.10623574257, 'accumulated_eval_time': 1578.0767569541931, 'accumulated_logging_time': 3.5267868041992188}
I0128 02:07:23.221852 140026167916288 logging_writer.py:48] [132018] accumulated_eval_time=1578.076757, accumulated_logging_time=3.526787, accumulated_submission_time=44918.106236, global_step=132018, preemption_count=0, score=44918.106236, test/accuracy=0.607700, test/loss=1.936709, test/num_examples=10000, total_duration=46504.477802, train/accuracy=0.870476, train/loss=0.724835, validation/accuracy=0.731500, validation/loss=1.299533, validation/num_examples=50000
I0128 02:07:52.420614 140026176308992 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.3032519817352295, loss=2.813704013824463
I0128 02:08:26.320789 140026167916288 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.1842453479766846, loss=2.758868932723999
I0128 02:09:00.343000 140026176308992 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.329329252243042, loss=2.784618377685547
I0128 02:09:34.312371 140026167916288 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.2225117683410645, loss=2.7325563430786133
I0128 02:10:08.285390 140026176308992 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.439558744430542, loss=2.751302719116211
I0128 02:10:42.278919 140026167916288 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.3239943981170654, loss=2.7081212997436523
I0128 02:11:16.274394 140026176308992 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.20992374420166, loss=2.7500948905944824
I0128 02:11:50.271319 140026167916288 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.412754535675049, loss=2.7450153827667236
I0128 02:12:24.244640 140026176308992 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.2676947116851807, loss=2.754094123840332
I0128 02:12:58.211045 140026167916288 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.1707382202148438, loss=2.7799110412597656
I0128 02:13:32.200733 140026176308992 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.3518221378326416, loss=2.7080435752868652
I0128 02:14:06.146821 140026167916288 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.2852747440338135, loss=2.765481948852539
I0128 02:14:40.103958 140026176308992 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.2791759967803955, loss=2.6981968879699707
I0128 02:15:14.060478 140026167916288 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.223128080368042, loss=2.7703919410705566
I0128 02:15:48.011680 140026176308992 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.5258097648620605, loss=2.721637487411499
I0128 02:15:53.243615 140187804313408 spec.py:321] Evaluating on the training split.
I0128 02:15:59.521812 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 02:16:08.515084 140187804313408 spec.py:349] Evaluating on the test split.
I0128 02:16:11.017712 140187804313408 submission_runner.py:408] Time since start: 47032.31s, 	Step: 133517, 	{'train/accuracy': 0.8717314600944519, 'train/loss': 0.7035813331604004, 'validation/accuracy': 0.7369599938392639, 'validation/loss': 1.2577601671218872, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.885425329208374, 'test/num_examples': 10000, 'score': 45426.990837574005, 'total_duration': 47032.312911748886, 'accumulated_submission_time': 45426.990837574005, 'accumulated_eval_time': 1595.8508143424988, 'accumulated_logging_time': 4.649079084396362}
I0128 02:16:11.061635 140026058876672 logging_writer.py:48] [133517] accumulated_eval_time=1595.850814, accumulated_logging_time=4.649079, accumulated_submission_time=45426.990838, global_step=133517, preemption_count=0, score=45426.990838, test/accuracy=0.608600, test/loss=1.885425, test/num_examples=10000, total_duration=47032.312912, train/accuracy=0.871731, train/loss=0.703581, validation/accuracy=0.736960, validation/loss=1.257760, validation/num_examples=50000
I0128 02:16:39.549316 140026067269376 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.2416985034942627, loss=2.689589262008667
I0128 02:17:13.553666 140026058876672 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.2975730895996094, loss=2.6905531883239746
I0128 02:17:47.480724 140026067269376 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.356009006500244, loss=2.804060935974121
I0128 02:18:21.437285 140026058876672 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.33552622795105, loss=2.7339060306549072
I0128 02:18:55.390896 140026067269376 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.4886059761047363, loss=2.8155934810638428
I0128 02:19:29.366221 140026058876672 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.5305209159851074, loss=2.8192355632781982
I0128 02:20:03.332148 140026067269376 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.475680351257324, loss=2.7506797313690186
I0128 02:20:37.293624 140026058876672 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.193779230117798, loss=2.7218899726867676
I0128 02:21:11.245290 140026067269376 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.219463586807251, loss=2.6909122467041016
I0128 02:21:45.211662 140026058876672 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.2776784896850586, loss=2.7063941955566406
I0128 02:22:19.151182 140026067269376 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.3002426624298096, loss=2.6969823837280273
I0128 02:22:53.168532 140026058876672 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.3794803619384766, loss=2.7378640174865723
I0128 02:23:27.125911 140026067269376 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.296175718307495, loss=2.741856098175049
I0128 02:24:01.089740 140026058876672 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.372650384902954, loss=2.7513344287872314
I0128 02:24:35.045390 140026067269376 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.2389137744903564, loss=2.6670775413513184
I0128 02:24:41.300746 140187804313408 spec.py:321] Evaluating on the training split.
I0128 02:24:47.587743 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 02:24:56.623109 140187804313408 spec.py:349] Evaluating on the test split.
I0128 02:24:59.157632 140187804313408 submission_runner.py:408] Time since start: 47560.45s, 	Step: 135020, 	{'train/accuracy': 0.868582546710968, 'train/loss': 0.7156649827957153, 'validation/accuracy': 0.7356799840927124, 'validation/loss': 1.2662253379821777, 'validation/num_examples': 50000, 'test/accuracy': 0.6134000420570374, 'test/loss': 1.8910014629364014, 'test/num_examples': 10000, 'score': 45937.16322660446, 'total_duration': 47560.45282816887, 'accumulated_submission_time': 45937.16322660446, 'accumulated_eval_time': 1613.7076733112335, 'accumulated_logging_time': 4.703073501586914}
I0128 02:24:59.200143 140026159523584 logging_writer.py:48] [135020] accumulated_eval_time=1613.707673, accumulated_logging_time=4.703074, accumulated_submission_time=45937.163227, global_step=135020, preemption_count=0, score=45937.163227, test/accuracy=0.613400, test/loss=1.891001, test/num_examples=10000, total_duration=47560.452828, train/accuracy=0.868583, train/loss=0.715665, validation/accuracy=0.735680, validation/loss=1.266225, validation/num_examples=50000
I0128 02:25:26.701468 140026167916288 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.389052391052246, loss=2.781442642211914
I0128 02:26:00.612882 140026159523584 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.4889469146728516, loss=2.7766878604888916
I0128 02:26:34.603549 140026167916288 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.5391268730163574, loss=2.752103805541992
I0128 02:27:08.564732 140026159523584 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.2225534915924072, loss=2.718494415283203
I0128 02:27:42.548042 140026167916288 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.2321579456329346, loss=2.6963131427764893
I0128 02:28:16.498244 140026159523584 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.5977866649627686, loss=2.761533260345459
I0128 02:28:50.418962 140026167916288 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.2103192806243896, loss=2.734769821166992
I0128 02:29:24.449636 140026159523584 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.399892568588257, loss=2.7347543239593506
I0128 02:29:58.387804 140026167916288 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.291771650314331, loss=2.6720945835113525
I0128 02:30:32.375963 140026159523584 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.307448625564575, loss=2.7177205085754395
I0128 02:31:06.338072 140026167916288 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.389573574066162, loss=2.7185921669006348
I0128 02:31:40.286219 140026159523584 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.3395285606384277, loss=2.6548962593078613
I0128 02:32:14.270234 140026167916288 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.430432081222534, loss=2.7116096019744873
I0128 02:32:48.247078 140026159523584 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.306746244430542, loss=2.714020013809204
I0128 02:33:22.218246 140026167916288 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.532083749771118, loss=2.7643401622772217
I0128 02:33:29.494943 140187804313408 spec.py:321] Evaluating on the training split.
I0128 02:33:35.634411 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 02:33:44.431707 140187804313408 spec.py:349] Evaluating on the test split.
I0128 02:33:46.940329 140187804313408 submission_runner.py:408] Time since start: 48088.24s, 	Step: 136523, 	{'train/accuracy': 0.8720503449440002, 'train/loss': 0.7015858888626099, 'validation/accuracy': 0.7404599785804749, 'validation/loss': 1.2451156377792358, 'validation/num_examples': 50000, 'test/accuracy': 0.6157000064849854, 'test/loss': 1.8840872049331665, 'test/num_examples': 10000, 'score': 46447.39303159714, 'total_duration': 48088.235530138016, 'accumulated_submission_time': 46447.39303159714, 'accumulated_eval_time': 1631.1530323028564, 'accumulated_logging_time': 4.756369113922119}
I0128 02:33:46.980924 140026075662080 logging_writer.py:48] [136523] accumulated_eval_time=1631.153032, accumulated_logging_time=4.756369, accumulated_submission_time=46447.393032, global_step=136523, preemption_count=0, score=46447.393032, test/accuracy=0.615700, test/loss=1.884087, test/num_examples=10000, total_duration=48088.235530, train/accuracy=0.872050, train/loss=0.701586, validation/accuracy=0.740460, validation/loss=1.245116, validation/num_examples=50000
I0128 02:34:13.419027 140026151130880 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.5559473037719727, loss=2.756556987762451
I0128 02:34:47.337015 140026075662080 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.358954668045044, loss=2.7111287117004395
I0128 02:35:21.265355 140026151130880 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.2214627265930176, loss=2.6716148853302
I0128 02:35:55.306300 140026075662080 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.063812494277954, loss=2.6255388259887695
I0128 02:36:29.285423 140026151130880 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.301511764526367, loss=2.774631977081299
I0128 02:37:03.243503 140026075662080 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.6575682163238525, loss=2.73336124420166
I0128 02:37:37.187858 140026151130880 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.490879535675049, loss=2.7466988563537598
I0128 02:38:11.153409 140026075662080 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.523265838623047, loss=2.6530263423919678
I0128 02:38:45.128091 140026151130880 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.5713860988616943, loss=2.7280192375183105
I0128 02:39:19.089688 140026075662080 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.521789073944092, loss=2.6546859741210938
I0128 02:39:53.042178 140026151130880 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2800726890563965, loss=2.733245372772217
I0128 02:40:27.013830 140026075662080 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.375030755996704, loss=2.722902774810791
I0128 02:41:00.974910 140026151130880 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.492873430252075, loss=2.705141305923462
I0128 02:41:34.941593 140026075662080 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.447444200515747, loss=2.7350897789001465
I0128 02:42:08.986407 140026151130880 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.5064470767974854, loss=2.6719112396240234
I0128 02:42:16.946287 140187804313408 spec.py:321] Evaluating on the training split.
I0128 02:42:23.154213 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 02:42:32.221978 140187804313408 spec.py:349] Evaluating on the test split.
I0128 02:42:34.760243 140187804313408 submission_runner.py:408] Time since start: 48616.06s, 	Step: 138025, 	{'train/accuracy': 0.8668486475944519, 'train/loss': 0.7391188144683838, 'validation/accuracy': 0.7368199825286865, 'validation/loss': 1.2867945432662964, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.9359248876571655, 'test/num_examples': 10000, 'score': 46957.294365644455, 'total_duration': 48616.05543756485, 'accumulated_submission_time': 46957.294365644455, 'accumulated_eval_time': 1648.9669427871704, 'accumulated_logging_time': 4.806999683380127}
I0128 02:42:34.801565 140026050483968 logging_writer.py:48] [138025] accumulated_eval_time=1648.966943, accumulated_logging_time=4.807000, accumulated_submission_time=46957.294366, global_step=138025, preemption_count=0, score=46957.294366, test/accuracy=0.607400, test/loss=1.935925, test/num_examples=10000, total_duration=48616.055438, train/accuracy=0.866849, train/loss=0.739119, validation/accuracy=0.736820, validation/loss=1.286795, validation/num_examples=50000
I0128 02:43:00.546509 140026058876672 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.5209312438964844, loss=2.731947898864746
I0128 02:43:34.474308 140026050483968 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.71907639503479, loss=2.720163345336914
I0128 02:44:08.456566 140026058876672 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.3874988555908203, loss=2.747615098953247
I0128 02:44:42.415962 140026050483968 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.6012675762176514, loss=2.682063102722168
I0128 02:45:16.378098 140026058876672 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.2788894176483154, loss=2.6765706539154053
I0128 02:45:50.393767 140026050483968 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.5182297229766846, loss=2.712792158126831
I0128 02:46:24.313025 140026058876672 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.664822578430176, loss=2.7211756706237793
I0128 02:46:58.268445 140026050483968 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.586280345916748, loss=2.7157278060913086
I0128 02:47:32.259090 140026058876672 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.504330635070801, loss=2.720595121383667
I0128 02:48:06.264512 140026050483968 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.47817325592041, loss=2.7387237548828125
I0128 02:48:40.257279 140026058876672 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.673936128616333, loss=2.6942667961120605
I0128 02:49:14.242993 140026050483968 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.2278032302856445, loss=2.671236038208008
I0128 02:49:48.231576 140026058876672 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.516505479812622, loss=2.694814682006836
I0128 02:50:22.197422 140026050483968 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.4900119304656982, loss=2.6921565532684326
I0128 02:50:56.207728 140026058876672 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.553422451019287, loss=2.6674203872680664
I0128 02:51:04.865067 140187804313408 spec.py:321] Evaluating on the training split.
I0128 02:51:11.000397 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 02:51:19.909908 140187804313408 spec.py:349] Evaluating on the test split.
I0128 02:51:22.411834 140187804313408 submission_runner.py:408] Time since start: 49143.71s, 	Step: 139527, 	{'train/accuracy': 0.8956273794174194, 'train/loss': 0.627384603023529, 'validation/accuracy': 0.7456600069999695, 'validation/loss': 1.240861415863037, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.87476646900177, 'test/num_examples': 10000, 'score': 47467.2943482399, 'total_duration': 49143.7070350647, 'accumulated_submission_time': 47467.2943482399, 'accumulated_eval_time': 1666.5136742591858, 'accumulated_logging_time': 4.857511758804321}
I0128 02:51:22.457496 140026159523584 logging_writer.py:48] [139527] accumulated_eval_time=1666.513674, accumulated_logging_time=4.857512, accumulated_submission_time=47467.294348, global_step=139527, preemption_count=0, score=47467.294348, test/accuracy=0.618300, test/loss=1.874766, test/num_examples=10000, total_duration=49143.707035, train/accuracy=0.895627, train/loss=0.627385, validation/accuracy=0.745660, validation/loss=1.240861, validation/num_examples=50000
I0128 02:51:47.575471 140026167916288 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.5512046813964844, loss=2.740318536758423
I0128 02:52:21.482576 140026159523584 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.5040855407714844, loss=2.710501194000244
I0128 02:52:55.444057 140026167916288 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.4819371700286865, loss=2.712096929550171
I0128 02:53:29.416852 140026159523584 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.329461097717285, loss=2.6808395385742188
I0128 02:54:03.373185 140026167916288 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.519411087036133, loss=2.715824604034424
I0128 02:54:37.369503 140026159523584 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.46569561958313, loss=2.6773715019226074
I0128 02:55:11.322821 140026167916288 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.5036752223968506, loss=2.6997880935668945
I0128 02:55:45.283884 140026159523584 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.527474880218506, loss=2.691200017929077
I0128 02:56:19.249178 140026167916288 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.3788084983825684, loss=2.7060580253601074
I0128 02:56:53.243383 140026159523584 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.425363302230835, loss=2.6982436180114746
I0128 02:57:27.171556 140026167916288 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.436082601547241, loss=2.6587717533111572
I0128 02:58:01.127204 140026159523584 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.5436079502105713, loss=2.6709299087524414
I0128 02:58:35.092568 140026167916288 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.577274799346924, loss=2.6666197776794434
I0128 02:59:09.072603 140026159523584 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.7375926971435547, loss=2.73994779586792
I0128 02:59:43.043259 140026167916288 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.66902232170105, loss=2.71702241897583
I0128 02:59:52.679499 140187804313408 spec.py:321] Evaluating on the training split.
I0128 02:59:58.830563 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 03:00:07.828553 140187804313408 spec.py:349] Evaluating on the test split.
I0128 03:00:10.349786 140187804313408 submission_runner.py:408] Time since start: 49671.64s, 	Step: 141030, 	{'train/accuracy': 0.893973171710968, 'train/loss': 0.6151627898216248, 'validation/accuracy': 0.7440399527549744, 'validation/loss': 1.2287087440490723, 'validation/num_examples': 50000, 'test/accuracy': 0.619700014591217, 'test/loss': 1.8548864126205444, 'test/num_examples': 10000, 'score': 47977.450786590576, 'total_duration': 49671.644984960556, 'accumulated_submission_time': 47977.450786590576, 'accumulated_eval_time': 1684.1839241981506, 'accumulated_logging_time': 4.9148759841918945}
I0128 03:00:10.394831 140026067269376 logging_writer.py:48] [141030] accumulated_eval_time=1684.183924, accumulated_logging_time=4.914876, accumulated_submission_time=47977.450787, global_step=141030, preemption_count=0, score=47977.450787, test/accuracy=0.619700, test/loss=1.854886, test/num_examples=10000, total_duration=49671.644985, train/accuracy=0.893973, train/loss=0.615163, validation/accuracy=0.744040, validation/loss=1.228709, validation/num_examples=50000
I0128 03:00:34.544242 140026075662080 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.4359936714172363, loss=2.6792984008789062
I0128 03:01:08.440507 140026067269376 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.4817943572998047, loss=2.6830310821533203
I0128 03:01:42.408394 140026075662080 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.5788323879241943, loss=2.6546711921691895
I0128 03:02:16.376399 140026067269376 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.2060022354125977, loss=2.682654619216919
I0128 03:02:50.373311 140026075662080 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.5298712253570557, loss=2.6807453632354736
I0128 03:03:24.339339 140026067269376 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.4351418018341064, loss=2.655151605606079
I0128 03:03:58.325401 140026075662080 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.6812474727630615, loss=2.7441859245300293
I0128 03:04:32.276804 140026067269376 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.4328696727752686, loss=2.683389186859131
I0128 03:05:06.250764 140026075662080 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.6117935180664062, loss=2.6292097568511963
I0128 03:05:40.234376 140026067269376 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.696704387664795, loss=2.6362390518188477
I0128 03:06:14.222912 140026075662080 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.5611252784729004, loss=2.658245325088501
I0128 03:06:48.250288 140026067269376 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.553194046020508, loss=2.6843154430389404
I0128 03:07:22.247156 140026075662080 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.6182472705841064, loss=2.7425758838653564
I0128 03:07:56.196973 140026067269376 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.544996738433838, loss=2.652223825454712
I0128 03:08:30.159529 140026075662080 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.337980270385742, loss=2.6697239875793457
I0128 03:08:40.513708 140187804313408 spec.py:321] Evaluating on the training split.
I0128 03:08:46.728484 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 03:08:55.724402 140187804313408 spec.py:349] Evaluating on the test split.
I0128 03:08:58.203885 140187804313408 submission_runner.py:408] Time since start: 50199.50s, 	Step: 142532, 	{'train/accuracy': 0.8904854655265808, 'train/loss': 0.6626537442207336, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.2583589553833008, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.8964543342590332, 'test/num_examples': 10000, 'score': 48487.50527572632, 'total_duration': 50199.49908399582, 'accumulated_submission_time': 48487.50527572632, 'accumulated_eval_time': 1701.8740639686584, 'accumulated_logging_time': 4.969464063644409}
I0128 03:08:58.245514 140026067269376 logging_writer.py:48] [142532] accumulated_eval_time=1701.874064, accumulated_logging_time=4.969464, accumulated_submission_time=48487.505276, global_step=142532, preemption_count=0, score=48487.505276, test/accuracy=0.616000, test/loss=1.896454, test/num_examples=10000, total_duration=50199.499084, train/accuracy=0.890485, train/loss=0.662654, validation/accuracy=0.744800, validation/loss=1.258359, validation/num_examples=50000
I0128 03:09:21.621608 140026159523584 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.850905418395996, loss=2.641052007675171
I0128 03:09:55.563982 140026067269376 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.6236801147460938, loss=2.691190719604492
I0128 03:10:29.502774 140026159523584 logging_writer.py:48] [142800] global_step=142800, grad_norm=4.019134521484375, loss=2.6815974712371826
I0128 03:11:03.461609 140026067269376 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.6824698448181152, loss=2.7707571983337402
I0128 03:11:37.418998 140026159523584 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.4525935649871826, loss=2.6720683574676514
I0128 03:12:11.427211 140026067269376 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.557450771331787, loss=2.6579904556274414
I0128 03:12:45.368010 140026159523584 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.6014184951782227, loss=2.6426303386688232
I0128 03:13:19.412231 140026067269376 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.4259846210479736, loss=2.7132484912872314
I0128 03:13:53.356642 140026159523584 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.5041704177856445, loss=2.67130970954895
I0128 03:14:27.260106 140026067269376 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.634974241256714, loss=2.665431261062622
I0128 03:15:01.223693 140026159523584 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.762977123260498, loss=2.7157411575317383
I0128 03:15:35.216368 140026067269376 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.6130330562591553, loss=2.692206621170044
I0128 03:16:09.178503 140026159523584 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.5358989238739014, loss=2.662325382232666
I0128 03:16:43.094986 140026067269376 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.4625744819641113, loss=2.669544219970703
I0128 03:17:17.057805 140026159523584 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.4847605228424072, loss=2.697171211242676
I0128 03:17:28.403978 140187804313408 spec.py:321] Evaluating on the training split.
I0128 03:17:34.555814 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 03:17:43.382121 140187804313408 spec.py:349] Evaluating on the test split.
I0128 03:17:45.887503 140187804313408 submission_runner.py:408] Time since start: 50727.18s, 	Step: 144035, 	{'train/accuracy': 0.892598032951355, 'train/loss': 0.6262313723564148, 'validation/accuracy': 0.7450399994850159, 'validation/loss': 1.2290778160095215, 'validation/num_examples': 50000, 'test/accuracy': 0.6198000311851501, 'test/loss': 1.864946722984314, 'test/num_examples': 10000, 'score': 48997.59909081459, 'total_duration': 50727.18270134926, 'accumulated_submission_time': 48997.59909081459, 'accumulated_eval_time': 1719.357551574707, 'accumulated_logging_time': 5.022829294204712}
I0128 03:17:45.933930 140026075662080 logging_writer.py:48] [144035] accumulated_eval_time=1719.357552, accumulated_logging_time=5.022829, accumulated_submission_time=48997.599091, global_step=144035, preemption_count=0, score=48997.599091, test/accuracy=0.619800, test/loss=1.864947, test/num_examples=10000, total_duration=50727.182701, train/accuracy=0.892598, train/loss=0.626231, validation/accuracy=0.745040, validation/loss=1.229078, validation/num_examples=50000
I0128 03:18:08.305903 140026151130880 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.5584545135498047, loss=2.7344517707824707
I0128 03:18:42.237304 140026075662080 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.717149257659912, loss=2.683192729949951
I0128 03:19:16.205617 140026151130880 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.3649165630340576, loss=2.6737260818481445
I0128 03:19:50.241352 140026075662080 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.88227915763855, loss=2.6752991676330566
I0128 03:20:24.218090 140026151130880 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.4273641109466553, loss=2.6678905487060547
I0128 03:20:58.187470 140026075662080 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.512207508087158, loss=2.6661815643310547
I0128 03:21:32.111567 140026151130880 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.8242762088775635, loss=2.71941876411438
I0128 03:22:06.090739 140026075662080 logging_writer.py:48] [144800] global_step=144800, grad_norm=4.085008144378662, loss=2.7127819061279297
I0128 03:22:40.054583 140026151130880 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.876250982284546, loss=2.689692974090576
I0128 03:23:14.029070 140026075662080 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.747238874435425, loss=2.6947731971740723
I0128 03:23:48.007601 140026151130880 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.472637176513672, loss=2.6584043502807617
I0128 03:24:21.976030 140026075662080 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.5660948753356934, loss=2.673100233078003
I0128 03:24:55.918026 140026151130880 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.5877180099487305, loss=2.666783094406128
I0128 03:25:29.896444 140026075662080 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.451206922531128, loss=2.579409122467041
I0128 03:26:03.943007 140026151130880 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.7834136486053467, loss=2.6291370391845703
I0128 03:26:15.975575 140187804313408 spec.py:321] Evaluating on the training split.
I0128 03:26:22.098827 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 03:26:31.234817 140187804313408 spec.py:349] Evaluating on the test split.
I0128 03:26:33.737962 140187804313408 submission_runner.py:408] Time since start: 51255.03s, 	Step: 145537, 	{'train/accuracy': 0.8907046914100647, 'train/loss': 0.6271235346794128, 'validation/accuracy': 0.7487599849700928, 'validation/loss': 1.223987102508545, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.8546254634857178, 'test/num_examples': 10000, 'score': 49507.578125715256, 'total_duration': 51255.033161878586, 'accumulated_submission_time': 49507.578125715256, 'accumulated_eval_time': 1737.1198983192444, 'accumulated_logging_time': 5.078593730926514}
I0128 03:26:33.780911 140026058876672 logging_writer.py:48] [145537] accumulated_eval_time=1737.119898, accumulated_logging_time=5.078594, accumulated_submission_time=49507.578126, global_step=145537, preemption_count=0, score=49507.578126, test/accuracy=0.622400, test/loss=1.854625, test/num_examples=10000, total_duration=51255.033162, train/accuracy=0.890705, train/loss=0.627124, validation/accuracy=0.748760, validation/loss=1.223987, validation/num_examples=50000
I0128 03:26:55.480170 140026067269376 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.6373815536499023, loss=2.5881545543670654
I0128 03:27:29.389240 140026058876672 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.92425799369812, loss=2.6610283851623535
I0128 03:28:03.337787 140026067269376 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.6371240615844727, loss=2.712143898010254
I0128 03:28:37.328191 140026058876672 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.4006171226501465, loss=2.625649929046631
I0128 03:29:11.295436 140026067269376 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.7152185440063477, loss=2.6692612171173096
I0128 03:29:45.280006 140026058876672 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.603472948074341, loss=2.6270601749420166
I0128 03:30:19.224676 140026067269376 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.627311944961548, loss=2.6626951694488525
I0128 03:30:53.178275 140026058876672 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.6558735370635986, loss=2.634159564971924
I0128 03:31:27.142821 140026067269376 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.629403829574585, loss=2.6770505905151367
I0128 03:32:01.274529 140026058876672 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.688678741455078, loss=2.6488420963287354
I0128 03:32:35.218672 140026067269376 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.5035152435302734, loss=2.635737895965576
I0128 03:33:09.176836 140026058876672 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.6159989833831787, loss=2.6324715614318848
I0128 03:33:43.124330 140026067269376 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.7149085998535156, loss=2.6370692253112793
I0128 03:34:17.088042 140026058876672 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.6323201656341553, loss=2.660079002380371
I0128 03:34:51.071167 140026067269376 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.8211936950683594, loss=2.698399066925049
I0128 03:35:03.786018 140187804313408 spec.py:321] Evaluating on the training split.
I0128 03:35:10.032743 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 03:35:18.997787 140187804313408 spec.py:349] Evaluating on the test split.
I0128 03:35:21.486110 140187804313408 submission_runner.py:408] Time since start: 51782.78s, 	Step: 147039, 	{'train/accuracy': 0.8966238498687744, 'train/loss': 0.6165596842765808, 'validation/accuracy': 0.7476199865341187, 'validation/loss': 1.2288175821304321, 'validation/num_examples': 50000, 'test/accuracy': 0.6242000460624695, 'test/loss': 1.8519423007965088, 'test/num_examples': 10000, 'score': 50017.51508355141, 'total_duration': 51782.78130912781, 'accumulated_submission_time': 50017.51508355141, 'accumulated_eval_time': 1754.819967508316, 'accumulated_logging_time': 5.133638858795166}
I0128 03:35:21.532161 140026042091264 logging_writer.py:48] [147039] accumulated_eval_time=1754.819968, accumulated_logging_time=5.133639, accumulated_submission_time=50017.515084, global_step=147039, preemption_count=0, score=50017.515084, test/accuracy=0.624200, test/loss=1.851942, test/num_examples=10000, total_duration=51782.781309, train/accuracy=0.896624, train/loss=0.616560, validation/accuracy=0.747620, validation/loss=1.228818, validation/num_examples=50000
I0128 03:35:42.607346 140026050483968 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.693655252456665, loss=2.6593422889709473
I0128 03:36:16.529365 140026042091264 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.626020669937134, loss=2.644444704055786
I0128 03:36:50.450880 140026050483968 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.8156402111053467, loss=2.610102653503418
I0128 03:37:24.397724 140026042091264 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.5394351482391357, loss=2.6079201698303223
I0128 03:37:58.352532 140026050483968 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.452024221420288, loss=2.6492063999176025
I0128 03:38:32.401899 140026042091264 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.604810953140259, loss=2.6604936122894287
I0128 03:39:06.348756 140026050483968 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.767072916030884, loss=2.678401231765747
I0128 03:39:40.333407 140026042091264 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.0999345779418945, loss=2.6637468338012695
I0128 03:40:14.291405 140026050483968 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.4679999351501465, loss=2.6999752521514893
I0128 03:40:48.245745 140026042091264 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.454455852508545, loss=2.6373724937438965
I0128 03:41:22.256599 140026050483968 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.623582124710083, loss=2.6062655448913574
I0128 03:41:56.227770 140026042091264 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.485374927520752, loss=2.591252565383911
I0128 03:42:30.227943 140026050483968 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.3569416999816895, loss=2.5757434368133545
I0128 03:43:04.201506 140026042091264 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.5231282711029053, loss=2.618812084197998
I0128 03:43:38.198373 140026050483968 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.56679368019104, loss=2.6522789001464844
I0128 03:43:51.584874 140187804313408 spec.py:321] Evaluating on the training split.
I0128 03:43:57.730773 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 03:44:06.824419 140187804313408 spec.py:349] Evaluating on the test split.
I0128 03:44:09.756824 140187804313408 submission_runner.py:408] Time since start: 52311.05s, 	Step: 148541, 	{'train/accuracy': 0.9024234414100647, 'train/loss': 0.5903157591819763, 'validation/accuracy': 0.7495200037956238, 'validation/loss': 1.215433120727539, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8344119787216187, 'test/num_examples': 10000, 'score': 50527.503370285034, 'total_duration': 52311.052006959915, 'accumulated_submission_time': 50527.503370285034, 'accumulated_eval_time': 1772.9918661117554, 'accumulated_logging_time': 5.189709186553955}
I0128 03:44:09.793138 140026151130880 logging_writer.py:48] [148541] accumulated_eval_time=1772.991866, accumulated_logging_time=5.189709, accumulated_submission_time=50527.503370, global_step=148541, preemption_count=0, score=50527.503370, test/accuracy=0.627200, test/loss=1.834412, test/num_examples=10000, total_duration=52311.052007, train/accuracy=0.902423, train/loss=0.590316, validation/accuracy=0.749520, validation/loss=1.215433, validation/num_examples=50000
I0128 03:44:30.192005 140026159523584 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.6642229557037354, loss=2.63193416595459
I0128 03:45:04.137457 140026151130880 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.8115034103393555, loss=2.6475236415863037
I0128 03:45:38.069984 140026159523584 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.7881481647491455, loss=2.5947749614715576
I0128 03:46:12.000456 140026151130880 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.7670176029205322, loss=2.6214892864227295
I0128 03:46:45.953796 140026159523584 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.785724639892578, loss=2.6926095485687256
I0128 03:47:19.920663 140026151130880 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.902332067489624, loss=2.665983200073242
I0128 03:47:53.897260 140026159523584 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.699711799621582, loss=2.6230554580688477
I0128 03:48:27.840679 140026151130880 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.5109541416168213, loss=2.6452975273132324
I0128 03:49:01.778116 140026159523584 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.6382036209106445, loss=2.629725456237793
I0128 03:49:35.733045 140026151130880 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.734346628189087, loss=2.6298959255218506
I0128 03:50:09.684865 140026159523584 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.848630666732788, loss=2.6479134559631348
I0128 03:50:43.779438 140026151130880 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.683603525161743, loss=2.640514850616455
I0128 03:51:17.696601 140026159523584 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.717259645462036, loss=2.637938976287842
I0128 03:51:51.658229 140026151130880 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.883676528930664, loss=2.6285393238067627
I0128 03:52:25.636010 140026159523584 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.7673401832580566, loss=2.6122565269470215
I0128 03:52:40.029963 140187804313408 spec.py:321] Evaluating on the training split.
I0128 03:52:46.477925 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 03:52:55.187736 140187804313408 spec.py:349] Evaluating on the test split.
I0128 03:52:57.644457 140187804313408 submission_runner.py:408] Time since start: 52838.94s, 	Step: 150044, 	{'train/accuracy': 0.910574734210968, 'train/loss': 0.5532589554786682, 'validation/accuracy': 0.75, 'validation/loss': 1.207600712776184, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.8446438312530518, 'test/num_examples': 10000, 'score': 51037.676836013794, 'total_duration': 52838.9393966198, 'accumulated_submission_time': 51037.676836013794, 'accumulated_eval_time': 1790.6060602664948, 'accumulated_logging_time': 5.234796047210693}
I0128 03:52:57.687504 140026042091264 logging_writer.py:48] [150044] accumulated_eval_time=1790.606060, accumulated_logging_time=5.234796, accumulated_submission_time=51037.676836, global_step=150044, preemption_count=0, score=51037.676836, test/accuracy=0.622800, test/loss=1.844644, test/num_examples=10000, total_duration=52838.939397, train/accuracy=0.910575, train/loss=0.553259, validation/accuracy=0.750000, validation/loss=1.207601, validation/num_examples=50000
I0128 03:53:17.070950 140026050483968 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.7533316612243652, loss=2.6243784427642822
I0128 03:53:51.000138 140026042091264 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.765803813934326, loss=2.563328742980957
I0128 03:54:24.983500 140026050483968 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.6309866905212402, loss=2.6464266777038574
I0128 03:54:58.976648 140026042091264 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.971428394317627, loss=2.577012062072754
I0128 03:55:32.963503 140026050483968 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.6261837482452393, loss=2.6508255004882812
I0128 03:56:06.943258 140026042091264 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.8840291500091553, loss=2.680525064468384
I0128 03:56:40.939632 140026050483968 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.776256561279297, loss=2.5946969985961914
I0128 03:57:15.079306 140026042091264 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.6086843013763428, loss=2.584134340286255
I0128 03:57:49.060708 140026050483968 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.9340837001800537, loss=2.713513135910034
I0128 03:58:23.041406 140026042091264 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.918473243713379, loss=2.572908401489258
I0128 03:58:57.025145 140026050483968 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.530012369155884, loss=2.679992437362671
I0128 03:59:30.947726 140026042091264 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.7327957153320312, loss=2.6073412895202637
I0128 04:00:04.930456 140026050483968 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.7398996353149414, loss=2.631208658218384
I0128 04:00:38.922629 140026042091264 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.647681951522827, loss=2.641310214996338
I0128 04:01:12.888701 140026050483968 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.74111008644104, loss=2.611692428588867
I0128 04:01:27.647950 140187804313408 spec.py:321] Evaluating on the training split.
I0128 04:01:33.862191 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 04:01:42.695848 140187804313408 spec.py:349] Evaluating on the test split.
I0128 04:01:45.216955 140187804313408 submission_runner.py:408] Time since start: 53366.51s, 	Step: 151545, 	{'train/accuracy': 0.9098173975944519, 'train/loss': 0.566749632358551, 'validation/accuracy': 0.7523599863052368, 'validation/loss': 1.2168418169021606, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.8432186841964722, 'test/num_examples': 10000, 'score': 51547.57273697853, 'total_duration': 53366.512150764465, 'accumulated_submission_time': 51547.57273697853, 'accumulated_eval_time': 1808.1750228404999, 'accumulated_logging_time': 5.288285970687866}
I0128 04:01:45.261298 140026058876672 logging_writer.py:48] [151545] accumulated_eval_time=1808.175023, accumulated_logging_time=5.288286, accumulated_submission_time=51547.572737, global_step=151545, preemption_count=0, score=51547.572737, test/accuracy=0.624300, test/loss=1.843219, test/num_examples=10000, total_duration=53366.512151, train/accuracy=0.909817, train/loss=0.566750, validation/accuracy=0.752360, validation/loss=1.216842, validation/num_examples=50000
I0128 04:02:04.255394 140026151130880 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.942959785461426, loss=2.6533169746398926
I0128 04:02:38.206590 140026058876672 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.5904183387756348, loss=2.6022562980651855
I0128 04:03:12.150757 140026151130880 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.688404083251953, loss=2.612241744995117
I0128 04:03:46.203098 140026058876672 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.6932809352874756, loss=2.647799015045166
I0128 04:04:20.163383 140026151130880 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.7653818130493164, loss=2.6564385890960693
I0128 04:04:54.128005 140026058876672 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.680981159210205, loss=2.6585586071014404
I0128 04:05:28.086959 140026151130880 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.946195602416992, loss=2.6142399311065674
I0128 04:06:02.022913 140026058876672 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.8543689250946045, loss=2.658172130584717
I0128 04:06:35.986312 140026151130880 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.993413209915161, loss=2.596518039703369
I0128 04:07:09.946019 140026058876672 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.731875419616699, loss=2.5749850273132324
I0128 04:07:43.893490 140026151130880 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.57020902633667, loss=2.6694273948669434
I0128 04:08:17.821436 140026058876672 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.6058292388916016, loss=2.558709144592285
I0128 04:08:51.776602 140026151130880 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.61435604095459, loss=2.6047863960266113
I0128 04:09:25.753401 140026058876672 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.9626758098602295, loss=2.6812338829040527
I0128 04:09:59.758902 140026151130880 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.857630968093872, loss=2.6064298152923584
I0128 04:10:15.524292 140187804313408 spec.py:321] Evaluating on the training split.
I0128 04:10:21.713380 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 04:10:30.893642 140187804313408 spec.py:349] Evaluating on the test split.
I0128 04:10:33.390104 140187804313408 submission_runner.py:408] Time since start: 53894.69s, 	Step: 153048, 	{'train/accuracy': 0.915058970451355, 'train/loss': 0.5383015871047974, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.1951937675476074, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.818287968635559, 'test/num_examples': 10000, 'score': 52057.77176237106, 'total_duration': 53894.68529486656, 'accumulated_submission_time': 52057.77176237106, 'accumulated_eval_time': 1826.0408027172089, 'accumulated_logging_time': 5.3418190479278564}
I0128 04:10:33.439350 140026067269376 logging_writer.py:48] [153048] accumulated_eval_time=1826.040803, accumulated_logging_time=5.341819, accumulated_submission_time=52057.771762, global_step=153048, preemption_count=0, score=52057.771762, test/accuracy=0.631300, test/loss=1.818288, test/num_examples=10000, total_duration=53894.685295, train/accuracy=0.915059, train/loss=0.538302, validation/accuracy=0.754640, validation/loss=1.195194, validation/num_examples=50000
I0128 04:10:51.415686 140026075662080 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.965388774871826, loss=2.6275529861450195
I0128 04:11:25.339094 140026067269376 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.6289775371551514, loss=2.5809168815612793
I0128 04:11:59.287588 140026075662080 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.190335750579834, loss=2.6563689708709717
I0128 04:12:33.226814 140026067269376 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.9120731353759766, loss=2.6730258464813232
I0128 04:13:07.215989 140026075662080 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.8709754943847656, loss=2.6728734970092773
I0128 04:13:41.175773 140026067269376 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.6630234718322754, loss=2.6251368522644043
I0128 04:14:15.144016 140026075662080 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.749141216278076, loss=2.5782389640808105
I0128 04:14:49.124058 140026067269376 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.467766523361206, loss=2.5714006423950195
I0128 04:15:23.084306 140026075662080 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.6307506561279297, loss=2.569118022918701
I0128 04:15:57.237504 140026067269376 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.7667157649993896, loss=2.6275317668914795
I0128 04:16:31.206918 140026075662080 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.9449737071990967, loss=2.6868343353271484
I0128 04:17:05.189788 140026067269376 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.7627060413360596, loss=2.5416414737701416
I0128 04:17:39.186461 140026075662080 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.214510917663574, loss=2.64285945892334
I0128 04:18:13.184326 140026067269376 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.5666253566741943, loss=2.525965690612793
I0128 04:18:47.178764 140026075662080 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.9577724933624268, loss=2.6263434886932373
I0128 04:19:03.631849 140187804313408 spec.py:321] Evaluating on the training split.
I0128 04:19:09.833477 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 04:19:18.606507 140187804313408 spec.py:349] Evaluating on the test split.
I0128 04:19:21.105551 140187804313408 submission_runner.py:408] Time since start: 54422.40s, 	Step: 154550, 	{'train/accuracy': 0.9122289419174194, 'train/loss': 0.5614323616027832, 'validation/accuracy': 0.75382000207901, 'validation/loss': 1.2097517251968384, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8361276388168335, 'test/num_examples': 10000, 'score': 52567.898394823074, 'total_duration': 54422.400752067566, 'accumulated_submission_time': 52567.898394823074, 'accumulated_eval_time': 1843.5144710540771, 'accumulated_logging_time': 5.40025782585144}
I0128 04:19:21.151591 140026058876672 logging_writer.py:48] [154550] accumulated_eval_time=1843.514471, accumulated_logging_time=5.400258, accumulated_submission_time=52567.898395, global_step=154550, preemption_count=0, score=52567.898395, test/accuracy=0.627800, test/loss=1.836128, test/num_examples=10000, total_duration=54422.400752, train/accuracy=0.912229, train/loss=0.561432, validation/accuracy=0.753820, validation/loss=1.209752, validation/num_examples=50000
I0128 04:19:38.449389 140026151130880 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.545994281768799, loss=2.5381577014923096
I0128 04:20:12.337970 140026058876672 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.526278495788574, loss=2.5711917877197266
I0128 04:20:46.278975 140026151130880 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.7462213039398193, loss=2.670053243637085
I0128 04:21:20.213105 140026058876672 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.587390899658203, loss=2.5278122425079346
I0128 04:21:54.111982 140026151130880 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.889918565750122, loss=2.6137659549713135
I0128 04:22:28.190217 140026058876672 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.8641560077667236, loss=2.5877556800842285
I0128 04:23:02.123758 140026151130880 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.7540595531463623, loss=2.639092206954956
I0128 04:23:36.083301 140026058876672 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.0717315673828125, loss=2.647813320159912
I0128 04:24:10.067035 140026151130880 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.771632671356201, loss=2.598029613494873
I0128 04:24:44.040517 140026058876672 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.674529790878296, loss=2.572721004486084
I0128 04:25:18.013231 140026151130880 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.8443245887756348, loss=2.5842747688293457
I0128 04:25:51.988230 140026058876672 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.9310200214385986, loss=2.639625310897827
I0128 04:26:25.913680 140026151130880 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.8537235260009766, loss=2.6176488399505615
I0128 04:26:59.885844 140026058876672 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.9270918369293213, loss=2.6531693935394287
I0128 04:27:33.851704 140026151130880 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.6890580654144287, loss=2.6028618812561035
I0128 04:27:51.306378 140187804313408 spec.py:321] Evaluating on the training split.
I0128 04:27:57.480630 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 04:28:06.569903 140187804313408 spec.py:349] Evaluating on the test split.
I0128 04:28:09.161479 140187804313408 submission_runner.py:408] Time since start: 54950.46s, 	Step: 156053, 	{'train/accuracy': 0.9175502061843872, 'train/loss': 0.5501120090484619, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.2017172574996948, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.8328757286071777, 'test/num_examples': 10000, 'score': 53077.98852467537, 'total_duration': 54950.4566886425, 'accumulated_submission_time': 53077.98852467537, 'accumulated_eval_time': 1861.3695440292358, 'accumulated_logging_time': 5.455489873886108}
I0128 04:28:09.200949 140026058876672 logging_writer.py:48] [156053] accumulated_eval_time=1861.369544, accumulated_logging_time=5.455490, accumulated_submission_time=53077.988525, global_step=156053, preemption_count=0, score=53077.988525, test/accuracy=0.629100, test/loss=1.832876, test/num_examples=10000, total_duration=54950.456689, train/accuracy=0.917550, train/loss=0.550112, validation/accuracy=0.755600, validation/loss=1.201717, validation/num_examples=50000
I0128 04:28:25.543452 140026067269376 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.6776318550109863, loss=2.5600619316101074
I0128 04:28:59.445959 140026058876672 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.868725538253784, loss=2.6221835613250732
I0128 04:29:33.414262 140026067269376 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.9370551109313965, loss=2.5980710983276367
I0128 04:30:07.354517 140026058876672 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.7394518852233887, loss=2.5825870037078857
I0128 04:30:41.277950 140026067269376 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.824960708618164, loss=2.5922114849090576
I0128 04:31:15.274891 140026058876672 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.649925947189331, loss=2.625868320465088
I0128 04:31:49.236148 140026067269376 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.23687219619751, loss=2.606854200363159
I0128 04:32:23.213066 140026058876672 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.728548049926758, loss=2.5728940963745117
I0128 04:32:57.208843 140026067269376 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.950077772140503, loss=2.5512008666992188
I0128 04:33:31.186417 140026058876672 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.794971466064453, loss=2.5978472232818604
I0128 04:34:05.163212 140026067269376 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.09181022644043, loss=2.612560510635376
I0128 04:34:39.210422 140026058876672 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.9551947116851807, loss=2.5554006099700928
I0128 04:35:13.200685 140026067269376 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.8491811752319336, loss=2.585171937942505
I0128 04:35:47.215115 140026058876672 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.1045379638671875, loss=2.5997519493103027
I0128 04:36:21.195344 140026067269376 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.688183546066284, loss=2.52351713180542
I0128 04:36:39.336215 140187804313408 spec.py:321] Evaluating on the training split.
I0128 04:36:46.198704 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 04:36:55.262233 140187804313408 spec.py:349] Evaluating on the test split.
I0128 04:36:57.768107 140187804313408 submission_runner.py:408] Time since start: 55479.06s, 	Step: 157555, 	{'train/accuracy': 0.9153977632522583, 'train/loss': 0.5394268035888672, 'validation/accuracy': 0.7566399574279785, 'validation/loss': 1.1991684436798096, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8347876071929932, 'test/num_examples': 10000, 'score': 53588.060198783875, 'total_duration': 55479.06328034401, 'accumulated_submission_time': 53588.060198783875, 'accumulated_eval_time': 1879.8014032840729, 'accumulated_logging_time': 5.5035552978515625}
I0128 04:36:57.812664 140026042091264 logging_writer.py:48] [157555] accumulated_eval_time=1879.801403, accumulated_logging_time=5.503555, accumulated_submission_time=53588.060199, global_step=157555, preemption_count=0, score=53588.060199, test/accuracy=0.629600, test/loss=1.834788, test/num_examples=10000, total_duration=55479.063280, train/accuracy=0.915398, train/loss=0.539427, validation/accuracy=0.756640, validation/loss=1.199168, validation/num_examples=50000
I0128 04:37:13.419692 140026050483968 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.6868741512298584, loss=2.5911054611206055
I0128 04:37:47.299628 140026042091264 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.9318344593048096, loss=2.6050643920898438
I0128 04:38:21.264828 140026050483968 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.873969793319702, loss=2.568446636199951
I0128 04:38:55.220975 140026042091264 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.917882204055786, loss=2.673845052719116
I0128 04:39:29.182281 140026050483968 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.7718148231506348, loss=2.52890682220459
I0128 04:40:03.177001 140026042091264 logging_writer.py:48] [158100] global_step=158100, grad_norm=3.789477825164795, loss=2.5755574703216553
I0128 04:40:37.136158 140026050483968 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.087480545043945, loss=2.6315853595733643
I0128 04:41:11.183279 140026042091264 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.790200710296631, loss=2.620675563812256
I0128 04:41:45.168652 140026050483968 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.7717831134796143, loss=2.631024122238159
I0128 04:42:19.112906 140026042091264 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.380970478057861, loss=2.6709532737731934
I0128 04:42:53.030319 140026050483968 logging_writer.py:48] [158600] global_step=158600, grad_norm=3.9654760360717773, loss=2.5917112827301025
I0128 04:43:26.990504 140026042091264 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.053145885467529, loss=2.610140562057495
I0128 04:44:00.925538 140026050483968 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.9724013805389404, loss=2.647825002670288
I0128 04:44:34.907943 140026042091264 logging_writer.py:48] [158900] global_step=158900, grad_norm=3.9377188682556152, loss=2.6103157997131348
I0128 04:45:08.841329 140026050483968 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.9732930660247803, loss=2.5896379947662354
I0128 04:45:27.986670 140187804313408 spec.py:321] Evaluating on the training split.
I0128 04:45:34.119805 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 04:45:43.190866 140187804313408 spec.py:349] Evaluating on the test split.
I0128 04:45:45.771649 140187804313408 submission_runner.py:408] Time since start: 56007.07s, 	Step: 159058, 	{'train/accuracy': 0.9265186190605164, 'train/loss': 0.5162858366966248, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.2046244144439697, 'validation/num_examples': 50000, 'test/accuracy': 0.6288000345230103, 'test/loss': 1.8388917446136475, 'test/num_examples': 10000, 'score': 54098.17010354996, 'total_duration': 56007.06683254242, 'accumulated_submission_time': 54098.17010354996, 'accumulated_eval_time': 1897.5863370895386, 'accumulated_logging_time': 5.557616472244263}
I0128 04:45:45.816712 140026042091264 logging_writer.py:48] [159058] accumulated_eval_time=1897.586337, accumulated_logging_time=5.557616, accumulated_submission_time=54098.170104, global_step=159058, preemption_count=0, score=54098.170104, test/accuracy=0.628800, test/loss=1.838892, test/num_examples=10000, total_duration=56007.066833, train/accuracy=0.926519, train/loss=0.516286, validation/accuracy=0.754940, validation/loss=1.204624, validation/num_examples=50000
I0128 04:46:00.409117 140026151130880 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.261439800262451, loss=2.6019389629364014
I0128 04:46:34.326784 140026042091264 logging_writer.py:48] [159200] global_step=159200, grad_norm=3.9584949016571045, loss=2.5866684913635254
I0128 04:47:08.273222 140026151130880 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.7039177417755127, loss=2.5923912525177
I0128 04:47:42.292114 140026042091264 logging_writer.py:48] [159400] global_step=159400, grad_norm=3.716599464416504, loss=2.5953330993652344
I0128 04:48:16.253726 140026151130880 logging_writer.py:48] [159500] global_step=159500, grad_norm=3.7850148677825928, loss=2.6369669437408447
I0128 04:48:50.228038 140026042091264 logging_writer.py:48] [159600] global_step=159600, grad_norm=3.716632604598999, loss=2.560317277908325
I0128 04:49:24.188821 140026151130880 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.03263521194458, loss=2.557887554168701
I0128 04:49:58.168513 140026042091264 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.008382797241211, loss=2.5773303508758545
I0128 04:50:32.122502 140026151130880 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.913022518157959, loss=2.609142780303955
I0128 04:51:06.049005 140026042091264 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.922451972961426, loss=2.6528594493865967
I0128 04:51:40.014223 140026151130880 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.94986891746521, loss=2.582958221435547
I0128 04:52:13.988402 140026042091264 logging_writer.py:48] [160200] global_step=160200, grad_norm=3.847804069519043, loss=2.5716214179992676
I0128 04:52:47.953367 140026151130880 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.9517788887023926, loss=2.6392033100128174
I0128 04:53:21.908724 140026042091264 logging_writer.py:48] [160400] global_step=160400, grad_norm=3.907123327255249, loss=2.585101842880249
I0128 04:53:55.943924 140026151130880 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.061559200286865, loss=2.6175782680511475
I0128 04:54:15.793708 140187804313408 spec.py:321] Evaluating on the training split.
I0128 04:54:21.940594 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 04:54:30.667422 140187804313408 spec.py:349] Evaluating on the test split.
I0128 04:54:33.186611 140187804313408 submission_runner.py:408] Time since start: 56534.48s, 	Step: 160560, 	{'train/accuracy': 0.924824595451355, 'train/loss': 0.5198838114738464, 'validation/accuracy': 0.7564599514007568, 'validation/loss': 1.200865626335144, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8306161165237427, 'test/num_examples': 10000, 'score': 54608.08381175995, 'total_duration': 56534.481812000275, 'accumulated_submission_time': 54608.08381175995, 'accumulated_eval_time': 1914.9792048931122, 'accumulated_logging_time': 5.611308336257935}
I0128 04:54:33.236199 140026067269376 logging_writer.py:48] [160560] accumulated_eval_time=1914.979205, accumulated_logging_time=5.611308, accumulated_submission_time=54608.083812, global_step=160560, preemption_count=0, score=54608.083812, test/accuracy=0.630700, test/loss=1.830616, test/num_examples=10000, total_duration=56534.481812, train/accuracy=0.924825, train/loss=0.519884, validation/accuracy=0.756460, validation/loss=1.200866, validation/num_examples=50000
I0128 04:54:47.130553 140026075662080 logging_writer.py:48] [160600] global_step=160600, grad_norm=3.7791266441345215, loss=2.519369602203369
I0128 04:55:21.042058 140026067269376 logging_writer.py:48] [160700] global_step=160700, grad_norm=3.729581117630005, loss=2.529522180557251
I0128 04:55:54.997730 140026075662080 logging_writer.py:48] [160800] global_step=160800, grad_norm=3.880023241043091, loss=2.5526232719421387
I0128 04:56:28.966917 140026067269376 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.054608345031738, loss=2.552708625793457
I0128 04:57:02.940080 140026075662080 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.007047653198242, loss=2.5855507850646973
I0128 04:57:36.926760 140026067269376 logging_writer.py:48] [161100] global_step=161100, grad_norm=3.6088008880615234, loss=2.521364450454712
I0128 04:58:10.908822 140026075662080 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.391916751861572, loss=2.6092300415039062
I0128 04:58:44.876449 140026067269376 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.893381118774414, loss=2.6121742725372314
I0128 04:59:18.852554 140026075662080 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.464066982269287, loss=2.592862367630005
I0128 04:59:52.892843 140026067269376 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.1072869300842285, loss=2.55289363861084
I0128 05:00:26.873977 140026075662080 logging_writer.py:48] [161600] global_step=161600, grad_norm=3.9799206256866455, loss=2.557727336883545
I0128 05:01:00.818294 140026067269376 logging_writer.py:48] [161700] global_step=161700, grad_norm=3.982652425765991, loss=2.569042205810547
I0128 05:01:34.798128 140026075662080 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.7475030422210693, loss=2.5488040447235107
I0128 05:02:08.752123 140026067269376 logging_writer.py:48] [161900] global_step=161900, grad_norm=3.7950315475463867, loss=2.5517358779907227
I0128 05:02:42.739064 140026075662080 logging_writer.py:48] [162000] global_step=162000, grad_norm=3.9507739543914795, loss=2.556523561477661
I0128 05:03:03.287394 140187804313408 spec.py:321] Evaluating on the training split.
I0128 05:03:09.485839 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 05:03:18.496549 140187804313408 spec.py:349] Evaluating on the test split.
I0128 05:03:20.950171 140187804313408 submission_runner.py:408] Time since start: 57062.25s, 	Step: 162062, 	{'train/accuracy': 0.9266381859779358, 'train/loss': 0.4990904629230499, 'validation/accuracy': 0.7584199905395508, 'validation/loss': 1.1812278032302856, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.8089417219161987, 'test/num_examples': 10000, 'score': 55118.07119345665, 'total_duration': 57062.24536585808, 'accumulated_submission_time': 55118.07119345665, 'accumulated_eval_time': 1932.6419672966003, 'accumulated_logging_time': 5.669963121414185}
I0128 05:03:21.001398 140026050483968 logging_writer.py:48] [162062] accumulated_eval_time=1932.641967, accumulated_logging_time=5.669963, accumulated_submission_time=55118.071193, global_step=162062, preemption_count=0, score=55118.071193, test/accuracy=0.633800, test/loss=1.808942, test/num_examples=10000, total_duration=57062.245366, train/accuracy=0.926638, train/loss=0.499090, validation/accuracy=0.758420, validation/loss=1.181228, validation/num_examples=50000
I0128 05:03:34.231811 140026058876672 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.013930320739746, loss=2.6274518966674805
I0128 05:04:08.135353 140026050483968 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.955397129058838, loss=2.5903635025024414
I0128 05:04:42.112101 140026058876672 logging_writer.py:48] [162300] global_step=162300, grad_norm=3.9361579418182373, loss=2.5451693534851074
I0128 05:05:16.090020 140026050483968 logging_writer.py:48] [162400] global_step=162400, grad_norm=3.785499095916748, loss=2.5741989612579346
I0128 05:05:50.054103 140026058876672 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.049149513244629, loss=2.555185317993164
I0128 05:06:24.071678 140026050483968 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.038547992706299, loss=2.545816421508789
I0128 05:06:58.067587 140026058876672 logging_writer.py:48] [162700] global_step=162700, grad_norm=3.8135085105895996, loss=2.56927490234375
I0128 05:07:32.026081 140026050483968 logging_writer.py:48] [162800] global_step=162800, grad_norm=3.9855010509490967, loss=2.5515570640563965
I0128 05:08:05.997011 140026058876672 logging_writer.py:48] [162900] global_step=162900, grad_norm=3.8661608695983887, loss=2.5484390258789062
I0128 05:08:39.977678 140026050483968 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.101586818695068, loss=2.555305242538452
I0128 05:09:13.962676 140026058876672 logging_writer.py:48] [163100] global_step=163100, grad_norm=3.9605493545532227, loss=2.572978973388672
I0128 05:09:47.946046 140026050483968 logging_writer.py:48] [163200] global_step=163200, grad_norm=3.9987244606018066, loss=2.535355806350708
I0128 05:10:21.911798 140026058876672 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.041165351867676, loss=2.614194869995117
I0128 05:10:55.931545 140026050483968 logging_writer.py:48] [163400] global_step=163400, grad_norm=3.908153772354126, loss=2.566648483276367
I0128 05:11:29.885262 140026058876672 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.205226421356201, loss=2.5579168796539307
I0128 05:11:51.096460 140187804313408 spec.py:321] Evaluating on the training split.
I0128 05:11:57.213798 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 05:12:06.295400 140187804313408 spec.py:349] Evaluating on the test split.
I0128 05:12:08.846280 140187804313408 submission_runner.py:408] Time since start: 57590.14s, 	Step: 163564, 	{'train/accuracy': 0.9269371628761292, 'train/loss': 0.5071867108345032, 'validation/accuracy': 0.7584199905395508, 'validation/loss': 1.188880205154419, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8226237297058105, 'test/num_examples': 10000, 'score': 55628.10046887398, 'total_duration': 57590.141486644745, 'accumulated_submission_time': 55628.10046887398, 'accumulated_eval_time': 1950.3917593955994, 'accumulated_logging_time': 5.7316248416900635}
I0128 05:12:08.885369 140026058876672 logging_writer.py:48] [163564] accumulated_eval_time=1950.391759, accumulated_logging_time=5.731625, accumulated_submission_time=55628.100469, global_step=163564, preemption_count=0, score=55628.100469, test/accuracy=0.632500, test/loss=1.822624, test/num_examples=10000, total_duration=57590.141487, train/accuracy=0.926937, train/loss=0.507187, validation/accuracy=0.758420, validation/loss=1.188880, validation/num_examples=50000
I0128 05:12:21.495409 140026067269376 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.278318405151367, loss=2.573719024658203
I0128 05:12:55.411521 140026058876672 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.848180055618286, loss=2.5496344566345215
I0128 05:13:29.361924 140026067269376 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.848024368286133, loss=2.55285906791687
I0128 05:14:03.323910 140026058876672 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.211067199707031, loss=2.569505214691162
I0128 05:14:37.294661 140026067269376 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.8905277252197266, loss=2.509059429168701
I0128 05:15:11.274407 140026058876672 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.0719404220581055, loss=2.5459835529327393
I0128 05:15:45.190706 140026067269376 logging_writer.py:48] [164200] global_step=164200, grad_norm=3.699342966079712, loss=2.5694570541381836
I0128 05:16:19.195908 140026058876672 logging_writer.py:48] [164300] global_step=164300, grad_norm=3.927957057952881, loss=2.5880088806152344
I0128 05:16:53.160391 140026067269376 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.162879467010498, loss=2.5725231170654297
I0128 05:17:27.164434 140026058876672 logging_writer.py:48] [164500] global_step=164500, grad_norm=3.6943304538726807, loss=2.5608551502227783
I0128 05:18:01.113710 140026067269376 logging_writer.py:48] [164600] global_step=164600, grad_norm=3.78082275390625, loss=2.5346152782440186
I0128 05:18:35.125929 140026058876672 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.113251209259033, loss=2.537360668182373
I0128 05:19:09.142111 140026067269376 logging_writer.py:48] [164800] global_step=164800, grad_norm=3.874276638031006, loss=2.548994541168213
I0128 05:19:43.090294 140026058876672 logging_writer.py:48] [164900] global_step=164900, grad_norm=3.9156084060668945, loss=2.6176602840423584
I0128 05:20:17.082049 140026067269376 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.276144504547119, loss=2.555271625518799
I0128 05:20:38.984717 140187804313408 spec.py:321] Evaluating on the training split.
I0128 05:20:45.125238 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 05:20:53.969104 140187804313408 spec.py:349] Evaluating on the test split.
I0128 05:20:56.504822 140187804313408 submission_runner.py:408] Time since start: 58117.80s, 	Step: 165066, 	{'train/accuracy': 0.9267378449440002, 'train/loss': 0.4970170259475708, 'validation/accuracy': 0.7583799958229065, 'validation/loss': 1.1788029670715332, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.815388798713684, 'test/num_examples': 10000, 'score': 56138.13663291931, 'total_duration': 58117.80002164841, 'accumulated_submission_time': 56138.13663291931, 'accumulated_eval_time': 1967.9118270874023, 'accumulated_logging_time': 5.7794764041900635}
I0128 05:20:56.550689 140026058876672 logging_writer.py:48] [165066] accumulated_eval_time=1967.911827, accumulated_logging_time=5.779476, accumulated_submission_time=56138.136633, global_step=165066, preemption_count=0, score=56138.136633, test/accuracy=0.634500, test/loss=1.815389, test/num_examples=10000, total_duration=58117.800022, train/accuracy=0.926738, train/loss=0.497017, validation/accuracy=0.758380, validation/loss=1.178803, validation/num_examples=50000
I0128 05:21:08.424065 140026151130880 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.09547233581543, loss=2.5782949924468994
I0128 05:21:42.331448 140026058876672 logging_writer.py:48] [165200] global_step=165200, grad_norm=3.663637638092041, loss=2.477959632873535
I0128 05:22:16.311815 140026151130880 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.085381984710693, loss=2.6368322372436523
I0128 05:22:50.275845 140026058876672 logging_writer.py:48] [165400] global_step=165400, grad_norm=3.9224367141723633, loss=2.535043239593506
I0128 05:23:24.261737 140026151130880 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.1096320152282715, loss=2.5387895107269287
I0128 05:23:58.249235 140026058876672 logging_writer.py:48] [165600] global_step=165600, grad_norm=3.8809762001037598, loss=2.5959417819976807
I0128 05:24:32.234726 140026151130880 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.7826263904571533, loss=2.537043809890747
I0128 05:25:06.263936 140026058876672 logging_writer.py:48] [165800] global_step=165800, grad_norm=3.667631149291992, loss=2.5166327953338623
I0128 05:25:40.245978 140026151130880 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.101570129394531, loss=2.577540159225464
I0128 05:26:14.233211 140026058876672 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.143860816955566, loss=2.5657050609588623
I0128 05:26:48.230328 140026151130880 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.7810513973236084, loss=2.483597993850708
I0128 05:27:22.244721 140026058876672 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.386977672576904, loss=2.5518832206726074
I0128 05:27:56.199747 140026151130880 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.0754828453063965, loss=2.6156604290008545
I0128 05:28:30.212875 140026058876672 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.226985454559326, loss=2.5522778034210205
I0128 05:29:04.198496 140026151130880 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.041031360626221, loss=2.5656752586364746
I0128 05:29:26.785551 140187804313408 spec.py:321] Evaluating on the training split.
I0128 05:29:33.014647 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 05:29:41.723383 140187804313408 spec.py:349] Evaluating on the test split.
I0128 05:29:44.335998 140187804313408 submission_runner.py:408] Time since start: 58645.63s, 	Step: 166568, 	{'train/accuracy': 0.9300262928009033, 'train/loss': 0.5010972619056702, 'validation/accuracy': 0.7594999670982361, 'validation/loss': 1.1896491050720215, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.816957950592041, 'test/num_examples': 10000, 'score': 56648.308772563934, 'total_duration': 58645.63115429878, 'accumulated_submission_time': 56648.308772563934, 'accumulated_eval_time': 1985.4621953964233, 'accumulated_logging_time': 5.834491014480591}
I0128 05:29:44.391426 140026075662080 logging_writer.py:48] [166568] accumulated_eval_time=1985.462195, accumulated_logging_time=5.834491, accumulated_submission_time=56648.308773, global_step=166568, preemption_count=0, score=56648.308773, test/accuracy=0.634500, test/loss=1.816958, test/num_examples=10000, total_duration=58645.631154, train/accuracy=0.930026, train/loss=0.501097, validation/accuracy=0.759500, validation/loss=1.189649, validation/num_examples=50000
I0128 05:29:55.585390 140026159523584 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.246737003326416, loss=2.604281425476074
I0128 05:30:29.485515 140026075662080 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.502079010009766, loss=2.5421249866485596
I0128 05:31:03.422893 140026159523584 logging_writer.py:48] [166800] global_step=166800, grad_norm=3.8700904846191406, loss=2.5671072006225586
I0128 05:31:37.466346 140026075662080 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.040892124176025, loss=2.5648093223571777
I0128 05:32:11.388919 140026159523584 logging_writer.py:48] [167000] global_step=167000, grad_norm=3.8218226432800293, loss=2.5627870559692383
I0128 05:32:45.381328 140026075662080 logging_writer.py:48] [167100] global_step=167100, grad_norm=3.8672564029693604, loss=2.505793333053589
I0128 05:33:19.360137 140026159523584 logging_writer.py:48] [167200] global_step=167200, grad_norm=3.9682531356811523, loss=2.506561040878296
I0128 05:33:53.334977 140026075662080 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.9242377281188965, loss=2.533567428588867
I0128 05:34:27.298124 140026159523584 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.0199713706970215, loss=2.549199342727661
I0128 05:35:01.267211 140026075662080 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.093803405761719, loss=2.581202507019043
I0128 05:35:35.215425 140026159523584 logging_writer.py:48] [167600] global_step=167600, grad_norm=3.934459686279297, loss=2.507310628890991
I0128 05:36:09.180984 140026075662080 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.275846004486084, loss=2.595939874649048
I0128 05:36:43.182361 140026159523584 logging_writer.py:48] [167800] global_step=167800, grad_norm=3.874709129333496, loss=2.5371241569519043
I0128 05:37:17.150953 140026075662080 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.312586307525635, loss=2.5869877338409424
I0128 05:37:51.203719 140026159523584 logging_writer.py:48] [168000] global_step=168000, grad_norm=3.977837562561035, loss=2.5125889778137207
I0128 05:38:14.462995 140187804313408 spec.py:321] Evaluating on the training split.
I0128 05:38:20.599655 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 05:38:29.657614 140187804313408 spec.py:349] Evaluating on the test split.
I0128 05:38:32.160600 140187804313408 submission_runner.py:408] Time since start: 59173.46s, 	Step: 168070, 	{'train/accuracy': 0.9396922588348389, 'train/loss': 0.45635488629341125, 'validation/accuracy': 0.7609599828720093, 'validation/loss': 1.1790025234222412, 'validation/num_examples': 50000, 'test/accuracy': 0.6356000304222107, 'test/loss': 1.80722975730896, 'test/num_examples': 10000, 'score': 57158.316095113754, 'total_duration': 59173.45578980446, 'accumulated_submission_time': 57158.316095113754, 'accumulated_eval_time': 2003.1597566604614, 'accumulated_logging_time': 5.8996758460998535}
I0128 05:38:32.207232 140026067269376 logging_writer.py:48] [168070] accumulated_eval_time=2003.159757, accumulated_logging_time=5.899676, accumulated_submission_time=57158.316095, global_step=168070, preemption_count=0, score=57158.316095, test/accuracy=0.635600, test/loss=1.807230, test/num_examples=10000, total_duration=59173.455790, train/accuracy=0.939692, train/loss=0.456355, validation/accuracy=0.760960, validation/loss=1.179003, validation/num_examples=50000
I0128 05:38:42.739726 140026151130880 logging_writer.py:48] [168100] global_step=168100, grad_norm=3.9902868270874023, loss=2.63610577583313
I0128 05:39:16.634989 140026067269376 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.162829399108887, loss=2.5466768741607666
I0128 05:39:50.619440 140026151130880 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.568666458129883, loss=2.6230316162109375
I0128 05:40:24.597948 140026067269376 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.0622782707214355, loss=2.5548486709594727
I0128 05:40:58.559721 140026151130880 logging_writer.py:48] [168500] global_step=168500, grad_norm=3.927532196044922, loss=2.5318682193756104
I0128 05:41:32.554526 140026067269376 logging_writer.py:48] [168600] global_step=168600, grad_norm=3.9608263969421387, loss=2.517725944519043
I0128 05:42:06.516441 140026151130880 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.9672045707702637, loss=2.57698655128479
I0128 05:42:40.503503 140026067269376 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.006991863250732, loss=2.5643575191497803
I0128 05:43:14.481133 140026151130880 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.1895575523376465, loss=2.548224925994873
I0128 05:43:48.521044 140026067269376 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.111760139465332, loss=2.5729284286499023
I0128 05:44:22.497464 140026151130880 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.060250759124756, loss=2.5702524185180664
I0128 05:44:56.509883 140026067269376 logging_writer.py:48] [169200] global_step=169200, grad_norm=3.8768324851989746, loss=2.4895899295806885
I0128 05:45:30.489279 140026151130880 logging_writer.py:48] [169300] global_step=169300, grad_norm=3.890798568725586, loss=2.5406365394592285
I0128 05:46:04.460567 140026067269376 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.092389106750488, loss=2.54654860496521
I0128 05:46:38.432772 140026151130880 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.063126564025879, loss=2.559446096420288
I0128 05:47:02.384608 140187804313408 spec.py:321] Evaluating on the training split.
I0128 05:47:08.553998 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 05:47:17.325452 140187804313408 spec.py:349] Evaluating on the test split.
I0128 05:47:19.870639 140187804313408 submission_runner.py:408] Time since start: 59701.17s, 	Step: 169572, 	{'train/accuracy': 0.9342115521430969, 'train/loss': 0.4681693911552429, 'validation/accuracy': 0.7597799897193909, 'validation/loss': 1.1787148714065552, 'validation/num_examples': 50000, 'test/accuracy': 0.6360000371932983, 'test/loss': 1.8080520629882812, 'test/num_examples': 10000, 'score': 57668.43019104004, 'total_duration': 59701.16584134102, 'accumulated_submission_time': 57668.43019104004, 'accumulated_eval_time': 2020.6457545757294, 'accumulated_logging_time': 5.955905914306641}
I0128 05:47:19.920486 140026159523584 logging_writer.py:48] [169572] accumulated_eval_time=2020.645755, accumulated_logging_time=5.955906, accumulated_submission_time=57668.430191, global_step=169572, preemption_count=0, score=57668.430191, test/accuracy=0.636000, test/loss=1.808052, test/num_examples=10000, total_duration=59701.165841, train/accuracy=0.934212, train/loss=0.468169, validation/accuracy=0.759780, validation/loss=1.178715, validation/num_examples=50000
I0128 05:47:29.758697 140026176308992 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.0051045417785645, loss=2.51349139213562
I0128 05:48:03.614126 140026159523584 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.141866683959961, loss=2.5665602684020996
I0128 05:48:37.555927 140026176308992 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.073312282562256, loss=2.5510447025299072
I0128 05:49:11.508871 140026159523584 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.0628204345703125, loss=2.5383458137512207
I0128 05:49:45.481797 140026176308992 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.019577503204346, loss=2.539952516555786
I0128 05:50:19.659121 140026159523584 logging_writer.py:48] [170100] global_step=170100, grad_norm=3.7753849029541016, loss=2.5012738704681396
I0128 05:50:53.609958 140026176308992 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.289259433746338, loss=2.507681369781494
I0128 05:51:27.579284 140026159523584 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.082683563232422, loss=2.559704542160034
I0128 05:52:01.538519 140026176308992 logging_writer.py:48] [170400] global_step=170400, grad_norm=3.8965413570404053, loss=2.4982590675354004
I0128 05:52:35.526605 140026159523584 logging_writer.py:48] [170500] global_step=170500, grad_norm=3.8294830322265625, loss=2.5725526809692383
I0128 05:53:09.524845 140026176308992 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.406989574432373, loss=2.5050721168518066
I0128 05:53:43.473864 140026159523584 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.23246431350708, loss=2.540315628051758
I0128 05:54:17.448434 140026176308992 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.009615898132324, loss=2.4845051765441895
I0128 05:54:51.440712 140026159523584 logging_writer.py:48] [170900] global_step=170900, grad_norm=3.984520196914673, loss=2.5368382930755615
I0128 05:55:25.415927 140026176308992 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.217357635498047, loss=2.5380008220672607
I0128 05:55:50.053205 140187804313408 spec.py:321] Evaluating on the training split.
I0128 05:55:56.281990 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 05:56:05.483780 140187804313408 spec.py:349] Evaluating on the test split.
I0128 05:56:07.988685 140187804313408 submission_runner.py:408] Time since start: 60229.28s, 	Step: 171074, 	{'train/accuracy': 0.9379583597183228, 'train/loss': 0.4724557101726532, 'validation/accuracy': 0.7612800002098083, 'validation/loss': 1.180039882659912, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.8073753118515015, 'test/num_examples': 10000, 'score': 58178.49935603142, 'total_duration': 60229.283868551254, 'accumulated_submission_time': 58178.49935603142, 'accumulated_eval_time': 2038.581184387207, 'accumulated_logging_time': 6.014849662780762}
I0128 05:56:08.038783 140026058876672 logging_writer.py:48] [171074] accumulated_eval_time=2038.581184, accumulated_logging_time=6.014850, accumulated_submission_time=58178.499356, global_step=171074, preemption_count=0, score=58178.499356, test/accuracy=0.636700, test/loss=1.807375, test/num_examples=10000, total_duration=60229.283869, train/accuracy=0.937958, train/loss=0.472456, validation/accuracy=0.761280, validation/loss=1.180040, validation/num_examples=50000
I0128 05:56:17.233559 140026067269376 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.132734298706055, loss=2.5349488258361816
I0128 05:56:51.236886 140026058876672 logging_writer.py:48] [171200] global_step=171200, grad_norm=3.7994160652160645, loss=2.526602268218994
I0128 05:57:25.191352 140026067269376 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.074404716491699, loss=2.581632614135742
I0128 05:57:59.131051 140026058876672 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.00091028213501, loss=2.534614324569702
I0128 05:58:33.106686 140026067269376 logging_writer.py:48] [171500] global_step=171500, grad_norm=3.9990181922912598, loss=2.505524158477783
I0128 05:59:07.107842 140026058876672 logging_writer.py:48] [171600] global_step=171600, grad_norm=3.939448356628418, loss=2.5027148723602295
I0128 05:59:41.117220 140026067269376 logging_writer.py:48] [171700] global_step=171700, grad_norm=3.9846060276031494, loss=2.5216729640960693
I0128 06:00:15.109436 140026058876672 logging_writer.py:48] [171800] global_step=171800, grad_norm=3.798546075820923, loss=2.4364562034606934
I0128 06:00:49.092405 140026067269376 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.215198516845703, loss=2.51515531539917
I0128 06:01:23.090710 140026058876672 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.266359329223633, loss=2.610576629638672
I0128 06:01:57.068053 140026067269376 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.158248424530029, loss=2.5193936824798584
I0128 06:02:31.048157 140026058876672 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.073494911193848, loss=2.5237159729003906
I0128 06:03:05.109081 140026067269376 logging_writer.py:48] [172300] global_step=172300, grad_norm=3.8862242698669434, loss=2.5274271965026855
I0128 06:03:39.072374 140026058876672 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.002134799957275, loss=2.5205743312835693
I0128 06:04:13.067543 140026067269376 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.026666641235352, loss=2.512335777282715
I0128 06:04:38.048398 140187804313408 spec.py:321] Evaluating on the training split.
I0128 06:04:44.187615 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 06:04:53.020983 140187804313408 spec.py:349] Evaluating on the test split.
I0128 06:04:55.546233 140187804313408 submission_runner.py:408] Time since start: 60756.84s, 	Step: 172575, 	{'train/accuracy': 0.9350087642669678, 'train/loss': 0.46790677309036255, 'validation/accuracy': 0.761680006980896, 'validation/loss': 1.1774041652679443, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.7978694438934326, 'test/num_examples': 10000, 'score': 58688.43921303749, 'total_duration': 60756.84143066406, 'accumulated_submission_time': 58688.43921303749, 'accumulated_eval_time': 2056.078993320465, 'accumulated_logging_time': 6.078775405883789}
I0128 06:04:55.595375 140026050483968 logging_writer.py:48] [172575] accumulated_eval_time=2056.078993, accumulated_logging_time=6.078775, accumulated_submission_time=58688.439213, global_step=172575, preemption_count=0, score=58688.439213, test/accuracy=0.637300, test/loss=1.797869, test/num_examples=10000, total_duration=60756.841431, train/accuracy=0.935009, train/loss=0.467907, validation/accuracy=0.761680, validation/loss=1.177404, validation/num_examples=50000
I0128 06:05:04.431596 140026151130880 logging_writer.py:48] [172600] global_step=172600, grad_norm=3.9574036598205566, loss=2.5256171226501465
I0128 06:05:38.326162 140026050483968 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.195852756500244, loss=2.5013606548309326
I0128 06:06:12.285311 140026151130880 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.46245002746582, loss=2.5637357234954834
I0128 06:06:46.261142 140026050483968 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.31246280670166, loss=2.548034429550171
I0128 06:07:20.257125 140026151130880 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.283846378326416, loss=2.5121524333953857
I0128 06:07:54.216004 140026050483968 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.162703514099121, loss=2.5430684089660645
I0128 06:08:28.193724 140026151130880 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.089081287384033, loss=2.526259183883667
I0128 06:09:02.171089 140026050483968 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.189907073974609, loss=2.5341057777404785
I0128 06:09:36.221436 140026151130880 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.026055812835693, loss=2.510218381881714
I0128 06:10:10.208096 140026050483968 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.337297439575195, loss=2.575688600540161
I0128 06:10:44.182470 140026151130880 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.456142902374268, loss=2.541269540786743
I0128 06:11:18.152266 140026050483968 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.5788044929504395, loss=2.5638060569763184
I0128 06:11:52.102208 140026151130880 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.296475410461426, loss=2.492326021194458
I0128 06:12:26.041041 140026050483968 logging_writer.py:48] [173900] global_step=173900, grad_norm=3.9989662170410156, loss=2.551957607269287
I0128 06:13:00.000676 140026151130880 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.103416919708252, loss=2.490938425064087
I0128 06:13:25.598412 140187804313408 spec.py:321] Evaluating on the training split.
I0128 06:13:31.711897 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 06:13:40.795679 140187804313408 spec.py:349] Evaluating on the test split.
I0128 06:13:43.304219 140187804313408 submission_runner.py:408] Time since start: 61284.60s, 	Step: 174077, 	{'train/accuracy': 0.9363440275192261, 'train/loss': 0.4684905409812927, 'validation/accuracy': 0.7619799971580505, 'validation/loss': 1.1775026321411133, 'validation/num_examples': 50000, 'test/accuracy': 0.6359000205993652, 'test/loss': 1.8029717206954956, 'test/num_examples': 10000, 'score': 59198.377883434296, 'total_duration': 61284.59939098358, 'accumulated_submission_time': 59198.377883434296, 'accumulated_eval_time': 2073.7847397327423, 'accumulated_logging_time': 6.137209415435791}
I0128 06:13:43.351728 140026058876672 logging_writer.py:48] [174077] accumulated_eval_time=2073.784740, accumulated_logging_time=6.137209, accumulated_submission_time=59198.377883, global_step=174077, preemption_count=0, score=59198.377883, test/accuracy=0.635900, test/loss=1.802972, test/num_examples=10000, total_duration=61284.599391, train/accuracy=0.936344, train/loss=0.468491, validation/accuracy=0.761980, validation/loss=1.177503, validation/num_examples=50000
I0128 06:13:51.480630 140026067269376 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.060059547424316, loss=2.56986927986145
I0128 06:14:25.406486 140026058876672 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.095357894897461, loss=2.5533032417297363
I0128 06:14:59.395465 140026067269376 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.092763900756836, loss=2.546126365661621
I0128 06:15:33.422132 140026058876672 logging_writer.py:48] [174400] global_step=174400, grad_norm=3.664323329925537, loss=2.4616684913635254
I0128 06:16:07.416008 140026067269376 logging_writer.py:48] [174500] global_step=174500, grad_norm=3.8974969387054443, loss=2.4815919399261475
I0128 06:16:41.394973 140026058876672 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.093086242675781, loss=2.4784083366394043
I0128 06:17:15.392554 140026067269376 logging_writer.py:48] [174700] global_step=174700, grad_norm=3.7993407249450684, loss=2.4765050411224365
I0128 06:17:49.386195 140026058876672 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.052762985229492, loss=2.4878122806549072
I0128 06:18:23.361341 140026067269376 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.084226608276367, loss=2.5304136276245117
I0128 06:18:57.368976 140026058876672 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.020645618438721, loss=2.527966260910034
I0128 06:19:31.334234 140026067269376 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.2577996253967285, loss=2.562804937362671
I0128 06:20:05.325392 140026058876672 logging_writer.py:48] [175200] global_step=175200, grad_norm=3.89589524269104, loss=2.4831302165985107
I0128 06:20:39.308662 140026067269376 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.0328369140625, loss=2.4786837100982666
I0128 06:21:13.281399 140026058876672 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.226089954376221, loss=2.5209553241729736
I0128 06:21:47.367110 140026067269376 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.067336559295654, loss=2.501347541809082
I0128 06:22:13.355299 140187804313408 spec.py:321] Evaluating on the training split.
I0128 06:22:19.510495 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 06:22:28.214855 140187804313408 spec.py:349] Evaluating on the test split.
I0128 06:22:30.705087 140187804313408 submission_runner.py:408] Time since start: 61812.00s, 	Step: 175578, 	{'train/accuracy': 0.9384765625, 'train/loss': 0.46365800499916077, 'validation/accuracy': 0.7625199556350708, 'validation/loss': 1.177310824394226, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.8003582954406738, 'test/num_examples': 10000, 'score': 59708.31727671623, 'total_duration': 61812.000276088715, 'accumulated_submission_time': 59708.31727671623, 'accumulated_eval_time': 2091.134479045868, 'accumulated_logging_time': 6.194151878356934}
I0128 06:22:30.752801 140026050483968 logging_writer.py:48] [175578] accumulated_eval_time=2091.134479, accumulated_logging_time=6.194152, accumulated_submission_time=59708.317277, global_step=175578, preemption_count=0, score=59708.317277, test/accuracy=0.639900, test/loss=1.800358, test/num_examples=10000, total_duration=61812.000276, train/accuracy=0.938477, train/loss=0.463658, validation/accuracy=0.762520, validation/loss=1.177311, validation/num_examples=50000
I0128 06:22:38.571871 140026159523584 logging_writer.py:48] [175600] global_step=175600, grad_norm=3.887667417526245, loss=2.545309066772461
I0128 06:23:12.499878 140026050483968 logging_writer.py:48] [175700] global_step=175700, grad_norm=3.853963851928711, loss=2.5020554065704346
I0128 06:23:46.461047 140026159523584 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.112690448760986, loss=2.514028787612915
I0128 06:24:20.436912 140026050483968 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.008379936218262, loss=2.4978625774383545
I0128 06:24:54.401734 140026159523584 logging_writer.py:48] [176000] global_step=176000, grad_norm=3.9758105278015137, loss=2.5498411655426025
I0128 06:25:28.372743 140026050483968 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.074686050415039, loss=2.4859461784362793
I0128 06:26:02.342661 140026159523584 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.026954174041748, loss=2.531214952468872
I0128 06:26:36.345217 140026050483968 logging_writer.py:48] [176300] global_step=176300, grad_norm=3.9087233543395996, loss=2.4977755546569824
I0128 06:27:10.272380 140026159523584 logging_writer.py:48] [176400] global_step=176400, grad_norm=3.9691593647003174, loss=2.458796977996826
I0128 06:27:44.225828 140026050483968 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.5436201095581055, loss=2.5099143981933594
I0128 06:28:18.248824 140026159523584 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.369246006011963, loss=2.519711971282959
I0128 06:28:52.213617 140026050483968 logging_writer.py:48] [176700] global_step=176700, grad_norm=3.967982769012451, loss=2.488959550857544
I0128 06:29:26.224267 140026159523584 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.0673089027404785, loss=2.559054136276245
I0128 06:30:00.219682 140026050483968 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.244577407836914, loss=2.4769222736358643
I0128 06:30:34.191565 140026159523584 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.216066360473633, loss=2.50144100189209
I0128 06:31:00.846802 140187804313408 spec.py:321] Evaluating on the training split.
I0128 06:31:07.200888 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 06:31:16.273870 140187804313408 spec.py:349] Evaluating on the test split.
I0128 06:31:18.794355 140187804313408 submission_runner.py:408] Time since start: 62340.09s, 	Step: 177080, 	{'train/accuracy': 0.94046950340271, 'train/loss': 0.45434367656707764, 'validation/accuracy': 0.762779951095581, 'validation/loss': 1.1745065450668335, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.799221396446228, 'test/num_examples': 10000, 'score': 60218.344750881195, 'total_duration': 62340.08955526352, 'accumulated_submission_time': 60218.344750881195, 'accumulated_eval_time': 2109.08202624321, 'accumulated_logging_time': 6.251053094863892}
I0128 06:31:18.841825 140026058876672 logging_writer.py:48] [177080] accumulated_eval_time=2109.082026, accumulated_logging_time=6.251053, accumulated_submission_time=60218.344751, global_step=177080, preemption_count=0, score=60218.344751, test/accuracy=0.640300, test/loss=1.799221, test/num_examples=10000, total_duration=62340.089555, train/accuracy=0.940470, train/loss=0.454344, validation/accuracy=0.762780, validation/loss=1.174507, validation/num_examples=50000
I0128 06:31:25.957858 140026067269376 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.189221382141113, loss=2.5011754035949707
I0128 06:31:59.882052 140026058876672 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.139247894287109, loss=2.5656771659851074
I0128 06:32:33.823168 140026067269376 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.0714874267578125, loss=2.5108842849731445
I0128 06:33:07.801801 140026058876672 logging_writer.py:48] [177400] global_step=177400, grad_norm=3.961714267730713, loss=2.5416831970214844
I0128 06:33:41.772494 140026067269376 logging_writer.py:48] [177500] global_step=177500, grad_norm=3.9200072288513184, loss=2.5056557655334473
I0128 06:34:15.962440 140026058876672 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.03134822845459, loss=2.5109434127807617
I0128 06:34:49.944891 140026067269376 logging_writer.py:48] [177700] global_step=177700, grad_norm=3.689822196960449, loss=2.4848546981811523
I0128 06:35:23.916922 140026058876672 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.034526348114014, loss=2.492091178894043
I0128 06:35:57.896214 140026067269376 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.476320266723633, loss=2.5584592819213867
I0128 06:36:31.882905 140026058876672 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.252725601196289, loss=2.5490126609802246
I0128 06:37:05.862946 140026067269376 logging_writer.py:48] [178100] global_step=178100, grad_norm=3.8824827671051025, loss=2.4966025352478027
I0128 06:37:39.840904 140026058876672 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.1374359130859375, loss=2.550597667694092
I0128 06:38:13.811483 140026067269376 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.493828296661377, loss=2.5793704986572266
I0128 06:38:47.775857 140026058876672 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.005722522735596, loss=2.500049114227295
I0128 06:39:21.785737 140026067269376 logging_writer.py:48] [178500] global_step=178500, grad_norm=3.8906941413879395, loss=2.531755208969116
I0128 06:39:49.090701 140187804313408 spec.py:321] Evaluating on the training split.
I0128 06:39:55.213581 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 06:40:04.200614 140187804313408 spec.py:349] Evaluating on the test split.
I0128 06:40:06.748195 140187804313408 submission_runner.py:408] Time since start: 62868.04s, 	Step: 178582, 	{'train/accuracy': 0.9402901530265808, 'train/loss': 0.45552629232406616, 'validation/accuracy': 0.7623199820518494, 'validation/loss': 1.173393964767456, 'validation/num_examples': 50000, 'test/accuracy': 0.6392000317573547, 'test/loss': 1.7995479106903076, 'test/num_examples': 10000, 'score': 60728.52793264389, 'total_duration': 62868.04338693619, 'accumulated_submission_time': 60728.52793264389, 'accumulated_eval_time': 2126.7394778728485, 'accumulated_logging_time': 6.30780291557312}
I0128 06:40:06.796936 140026167916288 logging_writer.py:48] [178582] accumulated_eval_time=2126.739478, accumulated_logging_time=6.307803, accumulated_submission_time=60728.527933, global_step=178582, preemption_count=0, score=60728.527933, test/accuracy=0.639200, test/loss=1.799548, test/num_examples=10000, total_duration=62868.043387, train/accuracy=0.940290, train/loss=0.455526, validation/accuracy=0.762320, validation/loss=1.173394, validation/num_examples=50000
I0128 06:40:13.231356 140026176308992 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.239715576171875, loss=2.5602447986602783
I0128 06:40:47.266000 140026167916288 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.059220314025879, loss=2.490097999572754
I0128 06:41:21.215241 140026176308992 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.41862678527832, loss=2.522782802581787
I0128 06:41:55.227608 140026167916288 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.066218852996826, loss=2.574246644973755
I0128 06:42:29.190742 140026176308992 logging_writer.py:48] [179000] global_step=179000, grad_norm=3.9838545322418213, loss=2.603313446044922
I0128 06:43:03.181901 140026167916288 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.101783275604248, loss=2.5369184017181396
I0128 06:43:37.140543 140026176308992 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.028582572937012, loss=2.5335190296173096
I0128 06:44:11.067481 140026167916288 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.288787841796875, loss=2.515308380126953
I0128 06:44:45.035259 140026176308992 logging_writer.py:48] [179400] global_step=179400, grad_norm=3.9374492168426514, loss=2.526190757751465
I0128 06:45:19.037203 140026167916288 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.3198018074035645, loss=2.566812515258789
I0128 06:45:53.000837 140026176308992 logging_writer.py:48] [179600] global_step=179600, grad_norm=3.8574936389923096, loss=2.516226053237915
I0128 06:46:26.986757 140026167916288 logging_writer.py:48] [179700] global_step=179700, grad_norm=3.8928864002227783, loss=2.51334810256958
I0128 06:47:01.006060 140026176308992 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.17997407913208, loss=2.524113416671753
I0128 06:47:35.021916 140026167916288 logging_writer.py:48] [179900] global_step=179900, grad_norm=3.9345691204071045, loss=2.5256497859954834
I0128 06:48:08.983781 140026176308992 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.468620300292969, loss=2.4995527267456055
I0128 06:48:37.008081 140187804313408 spec.py:321] Evaluating on the training split.
I0128 06:48:43.112973 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 06:48:51.933099 140187804313408 spec.py:349] Evaluating on the test split.
I0128 06:48:54.465584 140187804313408 submission_runner.py:408] Time since start: 63395.76s, 	Step: 180084, 	{'train/accuracy': 0.9409279227256775, 'train/loss': 0.45500898361206055, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 1.172922968864441, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.7973567247390747, 'test/num_examples': 10000, 'score': 61238.67451667786, 'total_duration': 63395.76078367233, 'accumulated_submission_time': 61238.67451667786, 'accumulated_eval_time': 2144.196943998337, 'accumulated_logging_time': 6.366096258163452}
I0128 06:48:54.513826 140026067269376 logging_writer.py:48] [180084] accumulated_eval_time=2144.196944, accumulated_logging_time=6.366096, accumulated_submission_time=61238.674517, global_step=180084, preemption_count=0, score=61238.674517, test/accuracy=0.639100, test/loss=1.797357, test/num_examples=10000, total_duration=63395.760784, train/accuracy=0.940928, train/loss=0.455009, validation/accuracy=0.762580, validation/loss=1.172923, validation/num_examples=50000
I0128 06:49:00.305474 140026075662080 logging_writer.py:48] [180100] global_step=180100, grad_norm=3.71309494972229, loss=2.501760959625244
I0128 06:49:34.228669 140026067269376 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.132430076599121, loss=2.4994378089904785
I0128 06:50:08.220801 140026075662080 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.437305927276611, loss=2.5035555362701416
I0128 06:50:42.175970 140026067269376 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.01617431640625, loss=2.515047073364258
I0128 06:51:16.170975 140026075662080 logging_writer.py:48] [180500] global_step=180500, grad_norm=3.9319241046905518, loss=2.552133321762085
I0128 06:51:50.111763 140026067269376 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.091017246246338, loss=2.533942222595215
I0128 06:52:24.080100 140026075662080 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.075098991394043, loss=2.5456202030181885
I0128 06:52:58.077342 140026067269376 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.035965442657471, loss=2.501124143600464
I0128 06:53:32.148923 140026075662080 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.040900707244873, loss=2.5354466438293457
I0128 06:54:06.125112 140026067269376 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.133274078369141, loss=2.464301586151123
I0128 06:54:40.103566 140026075662080 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.123627662658691, loss=2.5716428756713867
I0128 06:55:14.069598 140026067269376 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.155701160430908, loss=2.5370421409606934
I0128 06:55:48.093952 140026075662080 logging_writer.py:48] [181300] global_step=181300, grad_norm=3.9060635566711426, loss=2.509889602661133
I0128 06:56:22.073608 140026067269376 logging_writer.py:48] [181400] global_step=181400, grad_norm=3.8575046062469482, loss=2.499373435974121
I0128 06:56:56.061228 140026075662080 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.218663215637207, loss=2.5365614891052246
I0128 06:57:24.746576 140187804313408 spec.py:321] Evaluating on the training split.
I0128 06:57:30.986748 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 06:57:39.777419 140187804313408 spec.py:349] Evaluating on the test split.
I0128 06:57:42.221623 140187804313408 submission_runner.py:408] Time since start: 63923.52s, 	Step: 181586, 	{'train/accuracy': 0.941824734210968, 'train/loss': 0.455964058637619, 'validation/accuracy': 0.7629599571228027, 'validation/loss': 1.1759765148162842, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.7996957302093506, 'test/num_examples': 10000, 'score': 61748.84334039688, 'total_duration': 63923.51679396629, 'accumulated_submission_time': 61748.84334039688, 'accumulated_eval_time': 2161.671940803528, 'accumulated_logging_time': 6.423700332641602}
I0128 06:57:42.273954 140026159523584 logging_writer.py:48] [181586] accumulated_eval_time=2161.671941, accumulated_logging_time=6.423700, accumulated_submission_time=61748.843340, global_step=181586, preemption_count=0, score=61748.843340, test/accuracy=0.638900, test/loss=1.799696, test/num_examples=10000, total_duration=63923.516794, train/accuracy=0.941825, train/loss=0.455964, validation/accuracy=0.762960, validation/loss=1.175977, validation/num_examples=50000
I0128 06:57:47.372455 140026167916288 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.2505784034729, loss=2.574638843536377
I0128 06:58:21.259988 140026159523584 logging_writer.py:48] [181700] global_step=181700, grad_norm=3.9505224227905273, loss=2.4753236770629883
I0128 06:58:55.219687 140026167916288 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.252543926239014, loss=2.471850872039795
I0128 06:59:29.265525 140026159523584 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.0621490478515625, loss=2.500527858734131
I0128 07:00:03.632782 140026167916288 logging_writer.py:48] [182000] global_step=182000, grad_norm=3.7696707248687744, loss=2.5098657608032227
I0128 07:00:37.598443 140026159523584 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.1907758712768555, loss=2.5306267738342285
I0128 07:01:11.587770 140026167916288 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.153747081756592, loss=2.545987367630005
I0128 07:01:45.595573 140026159523584 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.070295333862305, loss=2.52163028717041
I0128 07:02:19.576243 140026167916288 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.3497843742370605, loss=2.5416667461395264
I0128 07:02:53.540427 140026159523584 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.059830188751221, loss=2.47469162940979
I0128 07:03:27.543013 140026167916288 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.358338356018066, loss=2.5567407608032227
I0128 07:04:01.483448 140026159523584 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.091492176055908, loss=2.518746852874756
I0128 07:04:35.446409 140026167916288 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.317921161651611, loss=2.5880279541015625
I0128 07:05:09.390021 140026159523584 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.2722859382629395, loss=2.5282962322235107
I0128 07:05:43.445340 140026167916288 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.148874282836914, loss=2.529071807861328
I0128 07:06:12.466222 140187804313408 spec.py:321] Evaluating on the training split.
I0128 07:06:18.640662 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 07:06:27.647346 140187804313408 spec.py:349] Evaluating on the test split.
I0128 07:06:30.161645 140187804313408 submission_runner.py:408] Time since start: 64451.46s, 	Step: 183087, 	{'train/accuracy': 0.9397321343421936, 'train/loss': 0.4528777301311493, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.174703598022461, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.797763705253601, 'test/num_examples': 10000, 'score': 62258.972554922104, 'total_duration': 64451.45683383942, 'accumulated_submission_time': 62258.972554922104, 'accumulated_eval_time': 2179.367330789566, 'accumulated_logging_time': 6.485302448272705}
I0128 07:06:30.219605 140026058876672 logging_writer.py:48] [183087] accumulated_eval_time=2179.367331, accumulated_logging_time=6.485302, accumulated_submission_time=62258.972555, global_step=183087, preemption_count=0, score=62258.972555, test/accuracy=0.639400, test/loss=1.797764, test/num_examples=10000, total_duration=64451.456834, train/accuracy=0.939732, train/loss=0.452878, validation/accuracy=0.762860, validation/loss=1.174704, validation/num_examples=50000
I0128 07:06:34.982545 140026067269376 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.1718668937683105, loss=2.4969725608825684
I0128 07:07:08.934111 140026058876672 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.116329193115234, loss=2.5369513034820557
I0128 07:07:42.896991 140026067269376 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.3502888679504395, loss=2.5121524333953857
I0128 07:08:16.883680 140026058876672 logging_writer.py:48] [183400] global_step=183400, grad_norm=3.730358600616455, loss=2.4668092727661133
I0128 07:08:50.866253 140026067269376 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.417237281799316, loss=2.565646171569824
I0128 07:09:24.849879 140026058876672 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.393055438995361, loss=2.555845260620117
I0128 07:09:58.848194 140026067269376 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.1034321784973145, loss=2.562166929244995
I0128 07:10:32.824567 140026058876672 logging_writer.py:48] [183800] global_step=183800, grad_norm=3.765911817550659, loss=2.5053110122680664
I0128 07:11:06.812910 140026067269376 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.15303897857666, loss=2.5928657054901123
I0128 07:11:40.817001 140026058876672 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.094936370849609, loss=2.5458033084869385
I0128 07:12:14.981477 140026067269376 logging_writer.py:48] [184100] global_step=184100, grad_norm=3.9638571739196777, loss=2.412182331085205
I0128 07:12:48.991492 140026058876672 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.25078821182251, loss=2.5304248332977295
I0128 07:13:22.960894 140026067269376 logging_writer.py:48] [184300] global_step=184300, grad_norm=3.9388504028320312, loss=2.565394878387451
I0128 07:13:56.938872 140026058876672 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.206243991851807, loss=2.490187883377075
I0128 07:14:30.930769 140026067269376 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.3383612632751465, loss=2.512113332748413
I0128 07:15:00.307554 140187804313408 spec.py:321] Evaluating on the training split.
I0128 07:15:06.645676 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 07:15:15.544460 140187804313408 spec.py:349] Evaluating on the test split.
I0128 07:15:18.063645 140187804313408 submission_runner.py:408] Time since start: 64979.36s, 	Step: 184588, 	{'train/accuracy': 0.939473032951355, 'train/loss': 0.4559187591075897, 'validation/accuracy': 0.7632799744606018, 'validation/loss': 1.1745176315307617, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.797345757484436, 'test/num_examples': 10000, 'score': 62768.99567198753, 'total_duration': 64979.35884261131, 'accumulated_submission_time': 62768.99567198753, 'accumulated_eval_time': 2197.1233875751495, 'accumulated_logging_time': 6.553771495819092}
I0128 07:15:18.114635 140026159523584 logging_writer.py:48] [184588] accumulated_eval_time=2197.123388, accumulated_logging_time=6.553771, accumulated_submission_time=62768.995672, global_step=184588, preemption_count=0, score=62768.995672, test/accuracy=0.639400, test/loss=1.797346, test/num_examples=10000, total_duration=64979.358843, train/accuracy=0.939473, train/loss=0.455919, validation/accuracy=0.763280, validation/loss=1.174518, validation/num_examples=50000
I0128 07:15:22.518106 140026167916288 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.069330215454102, loss=2.491936206817627
I0128 07:15:56.435629 140026159523584 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.00106954574585, loss=2.5146045684814453
I0128 07:16:30.382465 140026167916288 logging_writer.py:48] [184800] global_step=184800, grad_norm=3.991652011871338, loss=2.4978718757629395
I0128 07:17:04.336422 140026159523584 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.195258617401123, loss=2.5373778343200684
I0128 07:17:38.276785 140026167916288 logging_writer.py:48] [185000] global_step=185000, grad_norm=3.9919981956481934, loss=2.5210797786712646
I0128 07:18:12.234375 140026159523584 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.24522590637207, loss=2.5227108001708984
I0128 07:18:46.378445 140026167916288 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.175022125244141, loss=2.4668333530426025
I0128 07:19:17.489655 140026159523584 logging_writer.py:48] [185293] global_step=185293, preemption_count=0, score=63008.305857
I0128 07:19:17.914226 140187804313408 checkpoints.py:490] Saving checkpoint at step: 185293
I0128 07:19:19.022243 140187804313408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_2/checkpoint_185293
I0128 07:19:19.047399 140187804313408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_2/checkpoint_185293.
I0128 07:19:19.782071 140187804313408 submission_runner.py:583] Tuning trial 2/5
I0128 07:19:19.782299 140187804313408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0128 07:19:19.788348 140187804313408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000717474496923387, 'train/loss': 6.912591457366943, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 31.895853757858276, 'total_duration': 50.27870035171509, 'accumulated_submission_time': 31.895853757858276, 'accumulated_eval_time': 18.382753133773804, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1497, {'train/accuracy': 0.08177216351032257, 'train/loss': 5.256331920623779, 'validation/accuracy': 0.07620000094175339, 'validation/loss': 5.3086628913879395, 'validation/num_examples': 50000, 'test/accuracy': 0.053700003772974014, 'test/loss': 5.532280921936035, 'test/num_examples': 10000, 'score': 541.9562501907349, 'total_duration': 578.1606819629669, 'accumulated_submission_time': 541.9562501907349, 'accumulated_eval_time': 36.13430953025818, 'accumulated_logging_time': 0.016445159912109375, 'global_step': 1497, 'preemption_count': 0}), (2992, {'train/accuracy': 0.1966477930545807, 'train/loss': 4.2057881355285645, 'validation/accuracy': 0.17455999553203583, 'validation/loss': 4.332620143890381, 'validation/num_examples': 50000, 'test/accuracy': 0.1266000121831894, 'test/loss': 4.754936218261719, 'test/num_examples': 10000, 'score': 1051.935497045517, 'total_duration': 1105.7899084091187, 'accumulated_submission_time': 1051.935497045517, 'accumulated_eval_time': 53.70322799682617, 'accumulated_logging_time': 0.0438079833984375, 'global_step': 2992, 'preemption_count': 0}), (4488, {'train/accuracy': 0.3034917116165161, 'train/loss': 3.4856112003326416, 'validation/accuracy': 0.2791000008583069, 'validation/loss': 3.6248672008514404, 'validation/num_examples': 50000, 'test/accuracy': 0.21010001003742218, 'test/loss': 4.122847080230713, 'test/num_examples': 10000, 'score': 1562.1542949676514, 'total_duration': 1634.7661957740784, 'accumulated_submission_time': 1562.1542949676514, 'accumulated_eval_time': 72.37804913520813, 'accumulated_logging_time': 0.0735633373260498, 'global_step': 4488, 'preemption_count': 0}), (5984, {'train/accuracy': 0.3869180381298065, 'train/loss': 2.9858076572418213, 'validation/accuracy': 0.35905998945236206, 'validation/loss': 3.1390581130981445, 'validation/num_examples': 50000, 'test/accuracy': 0.27320000529289246, 'test/loss': 3.6855287551879883, 'test/num_examples': 10000, 'score': 2072.106372833252, 'total_duration': 2162.559784412384, 'accumulated_submission_time': 2072.106372833252, 'accumulated_eval_time': 90.1362087726593, 'accumulated_logging_time': 0.10370731353759766, 'global_step': 5984, 'preemption_count': 0}), (7480, {'train/accuracy': 0.4513911008834839, 'train/loss': 2.6858718395233154, 'validation/accuracy': 0.42056000232696533, 'validation/loss': 2.84183406829834, 'validation/num_examples': 50000, 'test/accuracy': 0.3192000091075897, 'test/loss': 3.4499173164367676, 'test/num_examples': 10000, 'score': 2582.1680810451508, 'total_duration': 2690.564907312393, 'accumulated_submission_time': 2582.1680810451508, 'accumulated_eval_time': 107.99761819839478, 'accumulated_logging_time': 0.1325531005859375, 'global_step': 7480, 'preemption_count': 0}), (8977, {'train/accuracy': 0.5364516973495483, 'train/loss': 2.2150745391845703, 'validation/accuracy': 0.47185999155044556, 'validation/loss': 2.5311429500579834, 'validation/num_examples': 50000, 'test/accuracy': 0.36250001192092896, 'test/loss': 3.1810948848724365, 'test/num_examples': 10000, 'score': 3092.3153219223022, 'total_duration': 3218.4831438064575, 'accumulated_submission_time': 3092.3153219223022, 'accumulated_eval_time': 125.68698191642761, 'accumulated_logging_time': 0.1603870391845703, 'global_step': 8977, 'preemption_count': 0}), (10474, {'train/accuracy': 0.5558035373687744, 'train/loss': 2.1703338623046875, 'validation/accuracy': 0.5116599798202515, 'validation/loss': 2.3818368911743164, 'validation/num_examples': 50000, 'test/accuracy': 0.39580002427101135, 'test/loss': 3.038412094116211, 'test/num_examples': 10000, 'score': 3602.40500998497, 'total_duration': 3746.2471656799316, 'accumulated_submission_time': 3602.40500998497, 'accumulated_eval_time': 143.27709293365479, 'accumulated_logging_time': 0.19082403182983398, 'global_step': 10474, 'preemption_count': 0}), (11972, {'train/accuracy': 0.5846021771430969, 'train/loss': 1.9957231283187866, 'validation/accuracy': 0.5405799746513367, 'validation/loss': 2.2037601470947266, 'validation/num_examples': 50000, 'test/accuracy': 0.42500001192092896, 'test/loss': 2.862744092941284, 'test/num_examples': 10000, 'score': 4112.561676979065, 'total_duration': 4274.270484447479, 'accumulated_submission_time': 4112.561676979065, 'accumulated_eval_time': 161.05826830863953, 'accumulated_logging_time': 0.22270441055297852, 'global_step': 11972, 'preemption_count': 0}), (13469, {'train/accuracy': 0.6219108700752258, 'train/loss': 1.801672101020813, 'validation/accuracy': 0.5726400017738342, 'validation/loss': 2.0319325923919678, 'validation/num_examples': 50000, 'test/accuracy': 0.4506000280380249, 'test/loss': 2.675619602203369, 'test/num_examples': 10000, 'score': 4622.485732078552, 'total_duration': 4801.6229565143585, 'accumulated_submission_time': 4622.485732078552, 'accumulated_eval_time': 178.40104007720947, 'accumulated_logging_time': 0.25369954109191895, 'global_step': 13469, 'preemption_count': 0}), (14968, {'train/accuracy': 0.6355029940605164, 'train/loss': 1.6827951669692993, 'validation/accuracy': 0.5859599709510803, 'validation/loss': 1.926688551902771, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.591858148574829, 'test/num_examples': 10000, 'score': 5132.471919298172, 'total_duration': 5329.625428676605, 'accumulated_submission_time': 5132.471919298172, 'accumulated_eval_time': 196.33145880699158, 'accumulated_logging_time': 0.285891056060791, 'global_step': 14968, 'preemption_count': 0}), (16467, {'train/accuracy': 0.6412428021430969, 'train/loss': 1.7201507091522217, 'validation/accuracy': 0.5932999849319458, 'validation/loss': 1.9380810260772705, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.6094350814819336, 'test/num_examples': 10000, 'score': 5642.635751485825, 'total_duration': 5857.638697385788, 'accumulated_submission_time': 5642.635751485825, 'accumulated_eval_time': 214.10228490829468, 'accumulated_logging_time': 0.3106076717376709, 'global_step': 16467, 'preemption_count': 0}), (17967, {'train/accuracy': 0.7058354616165161, 'train/loss': 1.4462586641311646, 'validation/accuracy': 0.6126999855041504, 'validation/loss': 1.8431637287139893, 'validation/num_examples': 50000, 'test/accuracy': 0.4798000156879425, 'test/loss': 2.5136966705322266, 'test/num_examples': 10000, 'score': 6152.796566486359, 'total_duration': 6385.609614610672, 'accumulated_submission_time': 6152.796566486359, 'accumulated_eval_time': 231.83049607276917, 'accumulated_logging_time': 0.34010767936706543, 'global_step': 17967, 'preemption_count': 0}), (19466, {'train/accuracy': 0.6865234375, 'train/loss': 1.4992294311523438, 'validation/accuracy': 0.6133399605751038, 'validation/loss': 1.8168702125549316, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.5008463859558105, 'test/num_examples': 10000, 'score': 6662.721271753311, 'total_duration': 6913.4088344573975, 'accumulated_submission_time': 6662.721271753311, 'accumulated_eval_time': 249.6162793636322, 'accumulated_logging_time': 0.37390899658203125, 'global_step': 19466, 'preemption_count': 0}), (20966, {'train/accuracy': 0.6920639276504517, 'train/loss': 1.4709275960922241, 'validation/accuracy': 0.6231799721717834, 'validation/loss': 1.7682565450668335, 'validation/num_examples': 50000, 'test/accuracy': 0.501300036907196, 'test/loss': 2.41690993309021, 'test/num_examples': 10000, 'score': 7172.737153053284, 'total_duration': 7441.06632399559, 'accumulated_submission_time': 7172.737153053284, 'accumulated_eval_time': 267.1690058708191, 'accumulated_logging_time': 0.4088256359100342, 'global_step': 20966, 'preemption_count': 0}), (22465, {'train/accuracy': 0.6925621628761292, 'train/loss': 1.4761664867401123, 'validation/accuracy': 0.6293399930000305, 'validation/loss': 1.753197431564331, 'validation/num_examples': 50000, 'test/accuracy': 0.5006999969482422, 'test/loss': 2.429164409637451, 'test/num_examples': 10000, 'score': 7682.723104953766, 'total_duration': 7968.861889123917, 'accumulated_submission_time': 7682.723104953766, 'accumulated_eval_time': 284.8941237926483, 'accumulated_logging_time': 0.44005632400512695, 'global_step': 22465, 'preemption_count': 0}), (23966, {'train/accuracy': 0.6956911683082581, 'train/loss': 1.4922162294387817, 'validation/accuracy': 0.634719967842102, 'validation/loss': 1.7566068172454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.4119579792022705, 'test/num_examples': 10000, 'score': 8192.924956321716, 'total_duration': 8496.729290246964, 'accumulated_submission_time': 8192.924956321716, 'accumulated_eval_time': 302.46875619888306, 'accumulated_logging_time': 0.4770851135253906, 'global_step': 23966, 'preemption_count': 0}), (25466, {'train/accuracy': 0.7027263641357422, 'train/loss': 1.419603705406189, 'validation/accuracy': 0.6396999955177307, 'validation/loss': 1.6906665563583374, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.3508965969085693, 'test/num_examples': 10000, 'score': 8702.947353839874, 'total_duration': 9024.420098781586, 'accumulated_submission_time': 8702.947353839874, 'accumulated_eval_time': 320.05310821533203, 'accumulated_logging_time': 0.508368968963623, 'global_step': 25466, 'preemption_count': 0}), (26966, {'train/accuracy': 0.6951530575752258, 'train/loss': 1.484920859336853, 'validation/accuracy': 0.6298199892044067, 'validation/loss': 1.7593168020248413, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.4571080207824707, 'test/num_examples': 10000, 'score': 9212.861717700958, 'total_duration': 9551.975267887115, 'accumulated_submission_time': 9212.861717700958, 'accumulated_eval_time': 337.60744285583496, 'accumulated_logging_time': 0.5415892601013184, 'global_step': 26966, 'preemption_count': 0}), (28466, {'train/accuracy': 0.7315250039100647, 'train/loss': 1.2711821794509888, 'validation/accuracy': 0.6417799592018127, 'validation/loss': 1.6622222661972046, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.301248073577881, 'test/num_examples': 10000, 'score': 9722.92922616005, 'total_duration': 10079.964128255844, 'accumulated_submission_time': 9722.92922616005, 'accumulated_eval_time': 355.4415764808655, 'accumulated_logging_time': 0.5747489929199219, 'global_step': 28466, 'preemption_count': 0}), (29967, {'train/accuracy': 0.7155213356018066, 'train/loss': 1.3536566495895386, 'validation/accuracy': 0.6436799764633179, 'validation/loss': 1.6779758930206299, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.333815813064575, 'test/num_examples': 10000, 'score': 10233.182272434235, 'total_duration': 10608.206499814987, 'accumulated_submission_time': 10233.182272434235, 'accumulated_eval_time': 373.3464798927307, 'accumulated_logging_time': 0.6060409545898438, 'global_step': 29967, 'preemption_count': 0}), (31468, {'train/accuracy': 0.7057158946990967, 'train/loss': 1.420884132385254, 'validation/accuracy': 0.6421200037002563, 'validation/loss': 1.7156926393508911, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.4058032035827637, 'test/num_examples': 10000, 'score': 10743.343856096268, 'total_duration': 11136.228532791138, 'accumulated_submission_time': 10743.343856096268, 'accumulated_eval_time': 391.10746002197266, 'accumulated_logging_time': 0.6514892578125, 'global_step': 31468, 'preemption_count': 0}), (32968, {'train/accuracy': 0.7166374325752258, 'train/loss': 1.3599779605865479, 'validation/accuracy': 0.6487799882888794, 'validation/loss': 1.6539162397384644, 'validation/num_examples': 50000, 'test/accuracy': 0.5246000289916992, 'test/loss': 2.2995214462280273, 'test/num_examples': 10000, 'score': 11253.426638364792, 'total_duration': 11664.150138139725, 'accumulated_submission_time': 11253.426638364792, 'accumulated_eval_time': 408.85849595069885, 'accumulated_logging_time': 0.6858878135681152, 'global_step': 32968, 'preemption_count': 0}), (34468, {'train/accuracy': 0.70703125, 'train/loss': 1.3962434530258179, 'validation/accuracy': 0.6412400007247925, 'validation/loss': 1.682578444480896, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.3533051013946533, 'test/num_examples': 10000, 'score': 11763.347280740738, 'total_duration': 12191.771873950958, 'accumulated_submission_time': 11763.347280740738, 'accumulated_eval_time': 426.4737284183502, 'accumulated_logging_time': 0.7184295654296875, 'global_step': 34468, 'preemption_count': 0}), (35969, {'train/accuracy': 0.7151227593421936, 'train/loss': 1.3773218393325806, 'validation/accuracy': 0.6521199941635132, 'validation/loss': 1.6565839052200317, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.307307243347168, 'test/num_examples': 10000, 'score': 12273.410103797913, 'total_duration': 12719.69810295105, 'accumulated_submission_time': 12273.410103797913, 'accumulated_eval_time': 444.24265217781067, 'accumulated_logging_time': 0.7576742172241211, 'global_step': 35969, 'preemption_count': 0}), (37469, {'train/accuracy': 0.7365872263908386, 'train/loss': 1.2994897365570068, 'validation/accuracy': 0.6460199952125549, 'validation/loss': 1.7007862329483032, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.3833701610565186, 'test/num_examples': 10000, 'score': 12783.423173427582, 'total_duration': 13247.574071884155, 'accumulated_submission_time': 12783.423173427582, 'accumulated_eval_time': 462.0181083679199, 'accumulated_logging_time': 0.7907888889312744, 'global_step': 37469, 'preemption_count': 0}), (38970, {'train/accuracy': 0.7347337007522583, 'train/loss': 1.2530560493469238, 'validation/accuracy': 0.6542199850082397, 'validation/loss': 1.6197998523712158, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.2923173904418945, 'test/num_examples': 10000, 'score': 13293.508579730988, 'total_duration': 13775.507248878479, 'accumulated_submission_time': 13293.508579730988, 'accumulated_eval_time': 479.77757358551025, 'accumulated_logging_time': 0.8264029026031494, 'global_step': 38970, 'preemption_count': 0}), (40471, {'train/accuracy': 0.7262037396430969, 'train/loss': 1.325469732284546, 'validation/accuracy': 0.6515399813652039, 'validation/loss': 1.6518381834030151, 'validation/num_examples': 50000, 'test/accuracy': 0.5252000093460083, 'test/loss': 2.321791410446167, 'test/num_examples': 10000, 'score': 13803.586438894272, 'total_duration': 14303.099082946777, 'accumulated_submission_time': 13803.586438894272, 'accumulated_eval_time': 497.2023296356201, 'accumulated_logging_time': 0.8594932556152344, 'global_step': 40471, 'preemption_count': 0}), (41972, {'train/accuracy': 0.7281568646430969, 'train/loss': 1.2736696004867554, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.5901299715042114, 'validation/num_examples': 50000, 'test/accuracy': 0.5282000303268433, 'test/loss': 2.2479236125946045, 'test/num_examples': 10000, 'score': 14313.738857030869, 'total_duration': 14831.734774827957, 'accumulated_submission_time': 14313.738857030869, 'accumulated_eval_time': 515.5967583656311, 'accumulated_logging_time': 0.8947463035583496, 'global_step': 41972, 'preemption_count': 0}), (43473, {'train/accuracy': 0.7159597873687744, 'train/loss': 1.3737213611602783, 'validation/accuracy': 0.6536200046539307, 'validation/loss': 1.6578227281570435, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.3332509994506836, 'test/num_examples': 10000, 'score': 14823.836078882217, 'total_duration': 15359.736160516739, 'accumulated_submission_time': 14823.836078882217, 'accumulated_eval_time': 533.4117612838745, 'accumulated_logging_time': 0.9289801120758057, 'global_step': 43473, 'preemption_count': 0}), (44974, {'train/accuracy': 0.7290138602256775, 'train/loss': 1.3344924449920654, 'validation/accuracy': 0.6631999611854553, 'validation/loss': 1.6209385395050049, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.273008108139038, 'test/num_examples': 10000, 'score': 15333.796644449234, 'total_duration': 15887.646605014801, 'accumulated_submission_time': 15333.796644449234, 'accumulated_eval_time': 551.2723331451416, 'accumulated_logging_time': 0.9648346900939941, 'global_step': 44974, 'preemption_count': 0}), (46475, {'train/accuracy': 0.75882887840271, 'train/loss': 1.2321513891220093, 'validation/accuracy': 0.6564799547195435, 'validation/loss': 1.6605032682418823, 'validation/num_examples': 50000, 'test/accuracy': 0.527999997138977, 'test/loss': 2.3292038440704346, 'test/num_examples': 10000, 'score': 15843.76311159134, 'total_duration': 16415.138216257095, 'accumulated_submission_time': 15843.76311159134, 'accumulated_eval_time': 568.7053790092468, 'accumulated_logging_time': 1.002387285232544, 'global_step': 46475, 'preemption_count': 0}), (47976, {'train/accuracy': 0.7443000674247742, 'train/loss': 1.2563570737838745, 'validation/accuracy': 0.6606599688529968, 'validation/loss': 1.619337797164917, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.2724545001983643, 'test/num_examples': 10000, 'score': 16353.741973161697, 'total_duration': 16942.905286073685, 'accumulated_submission_time': 16353.741973161697, 'accumulated_eval_time': 586.3989787101746, 'accumulated_logging_time': 1.043410301208496, 'global_step': 47976, 'preemption_count': 0}), (49477, {'train/accuracy': 0.73539137840271, 'train/loss': 1.2704975605010986, 'validation/accuracy': 0.6642000079154968, 'validation/loss': 1.5893797874450684, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.2808008193969727, 'test/num_examples': 10000, 'score': 16863.93771147728, 'total_duration': 17471.012765169144, 'accumulated_submission_time': 16863.93771147728, 'accumulated_eval_time': 604.2181794643402, 'accumulated_logging_time': 1.0814259052276611, 'global_step': 49477, 'preemption_count': 0}), (50978, {'train/accuracy': 0.7208425998687744, 'train/loss': 1.3237276077270508, 'validation/accuracy': 0.6489999890327454, 'validation/loss': 1.642844319343567, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.2644474506378174, 'test/num_examples': 10000, 'score': 17373.93242096901, 'total_duration': 17998.925570249557, 'accumulated_submission_time': 17373.93242096901, 'accumulated_eval_time': 622.0479846000671, 'accumulated_logging_time': 1.1158974170684814, 'global_step': 50978, 'preemption_count': 0}), (52479, {'train/accuracy': 0.7428650856018066, 'train/loss': 1.2034056186676025, 'validation/accuracy': 0.6698399782180786, 'validation/loss': 1.527472972869873, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 2.1574642658233643, 'test/num_examples': 10000, 'score': 17884.122307777405, 'total_duration': 18527.0086209774, 'accumulated_submission_time': 17884.122307777405, 'accumulated_eval_time': 639.8506090641022, 'accumulated_logging_time': 1.1524429321289062, 'global_step': 52479, 'preemption_count': 0}), (53981, {'train/accuracy': 0.7333585619926453, 'train/loss': 1.3001765012741089, 'validation/accuracy': 0.6660999655723572, 'validation/loss': 1.604732871055603, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2656149864196777, 'test/num_examples': 10000, 'score': 18394.34732890129, 'total_duration': 19055.01717185974, 'accumulated_submission_time': 18394.34732890129, 'accumulated_eval_time': 657.5397083759308, 'accumulated_logging_time': 1.1924428939819336, 'global_step': 53981, 'preemption_count': 0}), (55482, {'train/accuracy': 0.7835220098495483, 'train/loss': 1.07129967212677, 'validation/accuracy': 0.6714000105857849, 'validation/loss': 1.5408074855804443, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.194451332092285, 'test/num_examples': 10000, 'score': 18904.384672641754, 'total_duration': 19582.730887413025, 'accumulated_submission_time': 18904.384672641754, 'accumulated_eval_time': 675.1203720569611, 'accumulated_logging_time': 1.2325246334075928, 'global_step': 55482, 'preemption_count': 0}), (56983, {'train/accuracy': 0.753926157951355, 'train/loss': 1.2177770137786865, 'validation/accuracy': 0.6661799550056458, 'validation/loss': 1.6002877950668335, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.25341796875, 'test/num_examples': 10000, 'score': 19414.409068584442, 'total_duration': 20110.64827489853, 'accumulated_submission_time': 19414.409068584442, 'accumulated_eval_time': 692.9199199676514, 'accumulated_logging_time': 1.2718331813812256, 'global_step': 56983, 'preemption_count': 0}), (58484, {'train/accuracy': 0.7466916441917419, 'train/loss': 1.2169824838638306, 'validation/accuracy': 0.6627799868583679, 'validation/loss': 1.5723552703857422, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.232109785079956, 'test/num_examples': 10000, 'score': 19924.53598690033, 'total_duration': 20638.689115047455, 'accumulated_submission_time': 19924.53598690033, 'accumulated_eval_time': 710.7420144081116, 'accumulated_logging_time': 1.3092410564422607, 'global_step': 58484, 'preemption_count': 0}), (59985, {'train/accuracy': 0.7434031963348389, 'train/loss': 1.2602286338806152, 'validation/accuracy': 0.6678599715232849, 'validation/loss': 1.5897306203842163, 'validation/num_examples': 50000, 'test/accuracy': 0.5350000262260437, 'test/loss': 2.256978988647461, 'test/num_examples': 10000, 'score': 20434.61958694458, 'total_duration': 21166.6318423748, 'accumulated_submission_time': 20434.61958694458, 'accumulated_eval_time': 728.5081448554993, 'accumulated_logging_time': 1.348177433013916, 'global_step': 59985, 'preemption_count': 0}), (61486, {'train/accuracy': 0.7493622303009033, 'train/loss': 1.2253031730651855, 'validation/accuracy': 0.6746999621391296, 'validation/loss': 1.5467970371246338, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.180800676345825, 'test/num_examples': 10000, 'score': 20944.602358579636, 'total_duration': 21694.278636217117, 'accumulated_submission_time': 20944.602358579636, 'accumulated_eval_time': 746.0784072875977, 'accumulated_logging_time': 1.3884241580963135, 'global_step': 61486, 'preemption_count': 0}), (62987, {'train/accuracy': 0.7569953799247742, 'train/loss': 1.1516437530517578, 'validation/accuracy': 0.6823599934577942, 'validation/loss': 1.4813121557235718, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.1404056549072266, 'test/num_examples': 10000, 'score': 21454.522994041443, 'total_duration': 22222.266753673553, 'accumulated_submission_time': 21454.522994041443, 'accumulated_eval_time': 764.052640914917, 'accumulated_logging_time': 1.428706407546997, 'global_step': 62987, 'preemption_count': 0}), (64488, {'train/accuracy': 0.7630141973495483, 'train/loss': 1.167246699333191, 'validation/accuracy': 0.6773399710655212, 'validation/loss': 1.535491704940796, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.1842844486236572, 'test/num_examples': 10000, 'score': 21964.64013814926, 'total_duration': 22750.431900024414, 'accumulated_submission_time': 21964.64013814926, 'accumulated_eval_time': 782.0063388347626, 'accumulated_logging_time': 1.467320203781128, 'global_step': 64488, 'preemption_count': 0}), (65928, {'train/accuracy': 0.7694514989852905, 'train/loss': 1.1408820152282715, 'validation/accuracy': 0.6707199811935425, 'validation/loss': 1.5621381998062134, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.232006549835205, 'test/num_examples': 10000, 'score': 22474.734596014023, 'total_duration': 23278.254689455032, 'accumulated_submission_time': 22474.734596014023, 'accumulated_eval_time': 799.6415371894836, 'accumulated_logging_time': 1.5086112022399902, 'global_step': 65928, 'preemption_count': 0}), (67429, {'train/accuracy': 0.7701291441917419, 'train/loss': 1.1213020086288452, 'validation/accuracy': 0.6814000010490417, 'validation/loss': 1.508509635925293, 'validation/num_examples': 50000, 'test/accuracy': 0.551800012588501, 'test/loss': 2.163882255554199, 'test/num_examples': 10000, 'score': 22984.724188804626, 'total_duration': 23805.86190366745, 'accumulated_submission_time': 22984.724188804626, 'accumulated_eval_time': 817.1650323867798, 'accumulated_logging_time': 1.5480930805206299, 'global_step': 67429, 'preemption_count': 0}), (68931, {'train/accuracy': 0.7631736397743225, 'train/loss': 1.1109957695007324, 'validation/accuracy': 0.6834200024604797, 'validation/loss': 1.4736833572387695, 'validation/num_examples': 50000, 'test/accuracy': 0.5585000514984131, 'test/loss': 2.107130527496338, 'test/num_examples': 10000, 'score': 23494.799648284912, 'total_duration': 24333.555045366287, 'accumulated_submission_time': 23494.799648284912, 'accumulated_eval_time': 834.686586856842, 'accumulated_logging_time': 1.5904114246368408, 'global_step': 68931, 'preemption_count': 0}), (70432, {'train/accuracy': 0.7593470811843872, 'train/loss': 1.125109314918518, 'validation/accuracy': 0.6784999966621399, 'validation/loss': 1.4743475914001465, 'validation/num_examples': 50000, 'test/accuracy': 0.554900050163269, 'test/loss': 2.1304423809051514, 'test/num_examples': 10000, 'score': 24004.760673046112, 'total_duration': 24861.097346305847, 'accumulated_submission_time': 24004.760673046112, 'accumulated_eval_time': 852.1690158843994, 'accumulated_logging_time': 1.633352518081665, 'global_step': 70432, 'preemption_count': 0}), (71934, {'train/accuracy': 0.7483657598495483, 'train/loss': 1.220489263534546, 'validation/accuracy': 0.6733199954032898, 'validation/loss': 1.5536441802978516, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.1928555965423584, 'test/num_examples': 10000, 'score': 24514.92568397522, 'total_duration': 25388.99902510643, 'accumulated_submission_time': 24514.92568397522, 'accumulated_eval_time': 869.810240983963, 'accumulated_logging_time': 1.6746103763580322, 'global_step': 71934, 'preemption_count': 0}), (73436, {'train/accuracy': 0.7437220811843872, 'train/loss': 1.2418133020401, 'validation/accuracy': 0.6713399887084961, 'validation/loss': 1.5635813474655151, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.236577272415161, 'test/num_examples': 10000, 'score': 25025.141628980637, 'total_duration': 25917.068249225616, 'accumulated_submission_time': 25025.141628980637, 'accumulated_eval_time': 887.5698609352112, 'accumulated_logging_time': 1.713430643081665, 'global_step': 73436, 'preemption_count': 0}), (74938, {'train/accuracy': 0.7940250039100647, 'train/loss': 1.0290786027908325, 'validation/accuracy': 0.6819599866867065, 'validation/loss': 1.4988394975662231, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.1356232166290283, 'test/num_examples': 10000, 'score': 25535.273594856262, 'total_duration': 26445.11689734459, 'accumulated_submission_time': 25535.273594856262, 'accumulated_eval_time': 905.3917419910431, 'accumulated_logging_time': 1.753354549407959, 'global_step': 74938, 'preemption_count': 0}), (76439, {'train/accuracy': 0.7794363498687744, 'train/loss': 1.0432428121566772, 'validation/accuracy': 0.6851999759674072, 'validation/loss': 1.4631946086883545, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 2.140300989151001, 'test/num_examples': 10000, 'score': 26045.199329137802, 'total_duration': 26973.03084754944, 'accumulated_submission_time': 26045.199329137802, 'accumulated_eval_time': 923.286732673645, 'accumulated_logging_time': 1.793353796005249, 'global_step': 76439, 'preemption_count': 0}), (77941, {'train/accuracy': 0.7739556431770325, 'train/loss': 1.1201239824295044, 'validation/accuracy': 0.686199963092804, 'validation/loss': 1.504704236984253, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 2.144953489303589, 'test/num_examples': 10000, 'score': 26555.232617139816, 'total_duration': 27500.738260746002, 'accumulated_submission_time': 26555.232617139816, 'accumulated_eval_time': 940.8660507202148, 'accumulated_logging_time': 1.8331985473632812, 'global_step': 77941, 'preemption_count': 0}), (79442, {'train/accuracy': 0.7662029266357422, 'train/loss': 1.158005952835083, 'validation/accuracy': 0.6850000023841858, 'validation/loss': 1.5323312282562256, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 2.1702988147735596, 'test/num_examples': 10000, 'score': 27065.260931015015, 'total_duration': 28028.20407128334, 'accumulated_submission_time': 27065.260931015015, 'accumulated_eval_time': 958.2064433097839, 'accumulated_logging_time': 1.8757927417755127, 'global_step': 79442, 'preemption_count': 0}), (80944, {'train/accuracy': 0.7602837681770325, 'train/loss': 1.1421968936920166, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.5045496225357056, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1207895278930664, 'test/num_examples': 10000, 'score': 27575.289494991302, 'total_duration': 28557.046733379364, 'accumulated_submission_time': 27575.289494991302, 'accumulated_eval_time': 976.9262478351593, 'accumulated_logging_time': 1.9166314601898193, 'global_step': 80944, 'preemption_count': 0}), (82445, {'train/accuracy': 0.7724011540412903, 'train/loss': 1.0933239459991455, 'validation/accuracy': 0.6927399635314941, 'validation/loss': 1.4421255588531494, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 2.080371379852295, 'test/num_examples': 10000, 'score': 28085.512244701385, 'total_duration': 29085.328985452652, 'accumulated_submission_time': 28085.512244701385, 'accumulated_eval_time': 994.8886668682098, 'accumulated_logging_time': 1.9587581157684326, 'global_step': 82445, 'preemption_count': 0}), (83948, {'train/accuracy': 0.8116828799247742, 'train/loss': 0.9399776458740234, 'validation/accuracy': 0.6905199885368347, 'validation/loss': 1.4603952169418335, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.10579776763916, 'test/num_examples': 10000, 'score': 28595.73851132393, 'total_duration': 29613.136142492294, 'accumulated_submission_time': 28595.73851132393, 'accumulated_eval_time': 1012.3704881668091, 'accumulated_logging_time': 2.003067970275879, 'global_step': 83948, 'preemption_count': 0}), (85450, {'train/accuracy': 0.7909757494926453, 'train/loss': 1.0620595216751099, 'validation/accuracy': 0.6925599575042725, 'validation/loss': 1.4879108667373657, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 2.1164779663085938, 'test/num_examples': 10000, 'score': 29105.90775370598, 'total_duration': 30140.932975292206, 'accumulated_submission_time': 29105.90775370598, 'accumulated_eval_time': 1029.901986837387, 'accumulated_logging_time': 2.045266628265381, 'global_step': 85450, 'preemption_count': 0}), (86952, {'train/accuracy': 0.7831034660339355, 'train/loss': 1.0348970890045166, 'validation/accuracy': 0.6964600086212158, 'validation/loss': 1.4234379529953003, 'validation/num_examples': 50000, 'test/accuracy': 0.5677000284194946, 'test/loss': 2.063734769821167, 'test/num_examples': 10000, 'score': 29615.94760608673, 'total_duration': 30668.66878247261, 'accumulated_submission_time': 29615.94760608673, 'accumulated_eval_time': 1047.5002024173737, 'accumulated_logging_time': 2.0875396728515625, 'global_step': 86952, 'preemption_count': 0}), (88453, {'train/accuracy': 0.7817681431770325, 'train/loss': 1.088742971420288, 'validation/accuracy': 0.6913599967956543, 'validation/loss': 1.4801713228225708, 'validation/num_examples': 50000, 'test/accuracy': 0.5667000412940979, 'test/loss': 2.1170437335968018, 'test/num_examples': 10000, 'score': 30125.87562441826, 'total_duration': 31196.319259643555, 'accumulated_submission_time': 30125.87562441826, 'accumulated_eval_time': 1065.1176307201385, 'accumulated_logging_time': 2.137878894805908, 'global_step': 88453, 'preemption_count': 0}), (89955, {'train/accuracy': 0.7678770422935486, 'train/loss': 1.1227507591247559, 'validation/accuracy': 0.6862999796867371, 'validation/loss': 1.481406569480896, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.1493637561798096, 'test/num_examples': 10000, 'score': 30635.928003787994, 'total_duration': 31724.138827323914, 'accumulated_submission_time': 30635.928003787994, 'accumulated_eval_time': 1082.7853560447693, 'accumulated_logging_time': 2.1824607849121094, 'global_step': 89955, 'preemption_count': 0}), (91457, {'train/accuracy': 0.7881656289100647, 'train/loss': 1.0311802625656128, 'validation/accuracy': 0.7014600038528442, 'validation/loss': 1.415004014968872, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 2.05700421333313, 'test/num_examples': 10000, 'score': 31146.005562067032, 'total_duration': 32251.952335357666, 'accumulated_submission_time': 31146.005562067032, 'accumulated_eval_time': 1100.4241781234741, 'accumulated_logging_time': 2.2250864505767822, 'global_step': 91457, 'preemption_count': 0}), (92959, {'train/accuracy': 0.7984693646430969, 'train/loss': 1.0095969438552856, 'validation/accuracy': 0.6953799724578857, 'validation/loss': 1.4463399648666382, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 2.128091812133789, 'test/num_examples': 10000, 'score': 31655.910717964172, 'total_duration': 32779.907801151276, 'accumulated_submission_time': 31655.910717964172, 'accumulated_eval_time': 1118.377030134201, 'accumulated_logging_time': 2.269920825958252, 'global_step': 92959, 'preemption_count': 0}), (94462, {'train/accuracy': 0.8036909699440002, 'train/loss': 1.0080145597457886, 'validation/accuracy': 0.6985599994659424, 'validation/loss': 1.4534142017364502, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 2.11687970161438, 'test/num_examples': 10000, 'score': 32166.15032839775, 'total_duration': 33307.851593732834, 'accumulated_submission_time': 32166.15032839775, 'accumulated_eval_time': 1135.9845206737518, 'accumulated_logging_time': 2.3124051094055176, 'global_step': 94462, 'preemption_count': 0}), (95965, {'train/accuracy': 0.7919324040412903, 'train/loss': 1.0704740285873413, 'validation/accuracy': 0.6911799907684326, 'validation/loss': 1.506101369857788, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 2.1151468753814697, 'test/num_examples': 10000, 'score': 32676.37836766243, 'total_duration': 33835.78617501259, 'accumulated_submission_time': 32676.37836766243, 'accumulated_eval_time': 1153.5908043384552, 'accumulated_logging_time': 2.356222152709961, 'global_step': 95965, 'preemption_count': 0}), (97467, {'train/accuracy': 0.7996053695678711, 'train/loss': 0.9840648174285889, 'validation/accuracy': 0.70551997423172, 'validation/loss': 1.3981292247772217, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 2.038626194000244, 'test/num_examples': 10000, 'score': 33186.609236717224, 'total_duration': 34364.00185251236, 'accumulated_submission_time': 33186.609236717224, 'accumulated_eval_time': 1171.4737193584442, 'accumulated_logging_time': 2.403014659881592, 'global_step': 97467, 'preemption_count': 0}), (98969, {'train/accuracy': 0.7989476919174194, 'train/loss': 0.9818856716156006, 'validation/accuracy': 0.7056799530982971, 'validation/loss': 1.3842735290527344, 'validation/num_examples': 50000, 'test/accuracy': 0.5839000344276428, 'test/loss': 2.0269968509674072, 'test/num_examples': 10000, 'score': 33696.592266082764, 'total_duration': 34891.780616760254, 'accumulated_submission_time': 33696.592266082764, 'accumulated_eval_time': 1189.1691591739655, 'accumulated_logging_time': 2.448249578475952, 'global_step': 98969, 'preemption_count': 0}), (100470, {'train/accuracy': 0.8025948405265808, 'train/loss': 0.9827648401260376, 'validation/accuracy': 0.707539975643158, 'validation/loss': 1.3925204277038574, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 2.023866653442383, 'test/num_examples': 10000, 'score': 34206.52460384369, 'total_duration': 35419.403853178024, 'accumulated_submission_time': 34206.52460384369, 'accumulated_eval_time': 1206.7572317123413, 'accumulated_logging_time': 2.495995283126831, 'global_step': 100470, 'preemption_count': 0}), (101972, {'train/accuracy': 0.8014987111091614, 'train/loss': 1.0160855054855347, 'validation/accuracy': 0.707040011882782, 'validation/loss': 1.427125334739685, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 2.0674097537994385, 'test/num_examples': 10000, 'score': 34716.45545458794, 'total_duration': 35947.121560812, 'accumulated_submission_time': 34716.45545458794, 'accumulated_eval_time': 1224.4450266361237, 'accumulated_logging_time': 2.540762186050415, 'global_step': 101972, 'preemption_count': 0}), (103474, {'train/accuracy': 0.8282246589660645, 'train/loss': 0.8729314804077148, 'validation/accuracy': 0.7114599943161011, 'validation/loss': 1.368599534034729, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 2.0045225620269775, 'test/num_examples': 10000, 'score': 35226.452053546906, 'total_duration': 36475.07280921936, 'accumulated_submission_time': 35226.452053546906, 'accumulated_eval_time': 1242.2984237670898, 'accumulated_logging_time': 2.5861055850982666, 'global_step': 103474, 'preemption_count': 0}), (104976, {'train/accuracy': 0.8075972199440002, 'train/loss': 0.9604279398918152, 'validation/accuracy': 0.7060999870300293, 'validation/loss': 1.4053521156311035, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 2.0361974239349365, 'test/num_examples': 10000, 'score': 35736.37076330185, 'total_duration': 37002.70536971092, 'accumulated_submission_time': 35736.37076330185, 'accumulated_eval_time': 1259.911861896515, 'accumulated_logging_time': 2.632046937942505, 'global_step': 104976, 'preemption_count': 0}), (106478, {'train/accuracy': 0.8165656924247742, 'train/loss': 0.916153609752655, 'validation/accuracy': 0.7117599844932556, 'validation/loss': 1.3760380744934082, 'validation/num_examples': 50000, 'test/accuracy': 0.5820000171661377, 'test/loss': 2.037717342376709, 'test/num_examples': 10000, 'score': 36246.44602751732, 'total_duration': 37530.45402097702, 'accumulated_submission_time': 36246.44602751732, 'accumulated_eval_time': 1277.4861352443695, 'accumulated_logging_time': 2.677516460418701, 'global_step': 106478, 'preemption_count': 0}), (107981, {'train/accuracy': 0.8157086968421936, 'train/loss': 0.9369272589683533, 'validation/accuracy': 0.7161999940872192, 'validation/loss': 1.3731088638305664, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 2.0006630420684814, 'test/num_examples': 10000, 'score': 36756.588076114655, 'total_duration': 38058.4887046814, 'accumulated_submission_time': 36756.588076114655, 'accumulated_eval_time': 1295.2741153240204, 'accumulated_logging_time': 2.727273941040039, 'global_step': 107981, 'preemption_count': 0}), (109483, {'train/accuracy': 0.8148317933082581, 'train/loss': 0.9315711855888367, 'validation/accuracy': 0.7142199873924255, 'validation/loss': 1.3580296039581299, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0119309425354004, 'test/num_examples': 10000, 'score': 37266.67953324318, 'total_duration': 38586.27570772171, 'accumulated_submission_time': 37266.67953324318, 'accumulated_eval_time': 1312.8663160800934, 'accumulated_logging_time': 2.7767326831817627, 'global_step': 109483, 'preemption_count': 0}), (110985, {'train/accuracy': 0.8172432780265808, 'train/loss': 0.890960693359375, 'validation/accuracy': 0.7128599882125854, 'validation/loss': 1.340518832206726, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 1.9949579238891602, 'test/num_examples': 10000, 'score': 37776.58733320236, 'total_duration': 39114.19459462166, 'accumulated_submission_time': 37776.58733320236, 'accumulated_eval_time': 1330.7699587345123, 'accumulated_logging_time': 2.829568386077881, 'global_step': 110985, 'preemption_count': 0}), (112487, {'train/accuracy': 0.8461814522743225, 'train/loss': 0.7963310480117798, 'validation/accuracy': 0.7188400030136108, 'validation/loss': 1.3273828029632568, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9578930139541626, 'test/num_examples': 10000, 'score': 38286.656383514404, 'total_duration': 39642.166607141495, 'accumulated_submission_time': 38286.656383514404, 'accumulated_eval_time': 1348.5736198425293, 'accumulated_logging_time': 2.8754875659942627, 'global_step': 112487, 'preemption_count': 0}), (113990, {'train/accuracy': 0.8255141973495483, 'train/loss': 0.8634473085403442, 'validation/accuracy': 0.7139399647712708, 'validation/loss': 1.3523683547973633, 'validation/num_examples': 50000, 'test/accuracy': 0.5891000032424927, 'test/loss': 2.0083277225494385, 'test/num_examples': 10000, 'score': 38796.7903342247, 'total_duration': 40169.901686668396, 'accumulated_submission_time': 38796.7903342247, 'accumulated_eval_time': 1366.0709924697876, 'accumulated_logging_time': 2.9247477054595947, 'global_step': 113990, 'preemption_count': 0}), (115492, {'train/accuracy': 0.8307756781578064, 'train/loss': 0.861309826374054, 'validation/accuracy': 0.7196199893951416, 'validation/loss': 1.3414368629455566, 'validation/num_examples': 50000, 'test/accuracy': 0.5921000242233276, 'test/loss': 1.9662647247314453, 'test/num_examples': 10000, 'score': 39306.942506313324, 'total_duration': 40697.91963505745, 'accumulated_submission_time': 39306.942506313324, 'accumulated_eval_time': 1383.8353281021118, 'accumulated_logging_time': 2.9718353748321533, 'global_step': 115492, 'preemption_count': 0}), (116995, {'train/accuracy': 0.8356983065605164, 'train/loss': 0.8533991575241089, 'validation/accuracy': 0.7249400019645691, 'validation/loss': 1.3167498111724854, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.9473471641540527, 'test/num_examples': 10000, 'score': 39817.17044043541, 'total_duration': 41225.83962345123, 'accumulated_submission_time': 39817.17044043541, 'accumulated_eval_time': 1401.4225118160248, 'accumulated_logging_time': 3.0217180252075195, 'global_step': 116995, 'preemption_count': 0}), (118497, {'train/accuracy': 0.8339644074440002, 'train/loss': 0.8613070249557495, 'validation/accuracy': 0.7239999771118164, 'validation/loss': 1.3303016424179077, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.9697046279907227, 'test/num_examples': 10000, 'score': 40327.12989664078, 'total_duration': 41754.2838177681, 'accumulated_submission_time': 40327.12989664078, 'accumulated_eval_time': 1419.805627822876, 'accumulated_logging_time': 3.0692250728607178, 'global_step': 118497, 'preemption_count': 0}), (119999, {'train/accuracy': 0.8390266299247742, 'train/loss': 0.8340352177619934, 'validation/accuracy': 0.7273600101470947, 'validation/loss': 1.301812767982483, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.9222824573516846, 'test/num_examples': 10000, 'score': 40837.23765873909, 'total_duration': 42282.342547655106, 'accumulated_submission_time': 40837.23765873909, 'accumulated_eval_time': 1437.6517629623413, 'accumulated_logging_time': 3.1198184490203857, 'global_step': 119999, 'preemption_count': 0}), (121501, {'train/accuracy': 0.8613081574440002, 'train/loss': 0.7704369425773621, 'validation/accuracy': 0.7250399589538574, 'validation/loss': 1.3356198072433472, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.9819520711898804, 'test/num_examples': 10000, 'score': 41347.405833005905, 'total_duration': 42810.052292108536, 'accumulated_submission_time': 41347.405833005905, 'accumulated_eval_time': 1455.0882284641266, 'accumulated_logging_time': 3.1719682216644287, 'global_step': 121501, 'preemption_count': 0}), (123004, {'train/accuracy': 0.8552694320678711, 'train/loss': 0.7926141023635864, 'validation/accuracy': 0.7265399694442749, 'validation/loss': 1.3171532154083252, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.9438683986663818, 'test/num_examples': 10000, 'score': 41857.52238154411, 'total_duration': 43337.58539104462, 'accumulated_submission_time': 41857.52238154411, 'accumulated_eval_time': 1472.3993520736694, 'accumulated_logging_time': 3.2225170135498047, 'global_step': 123004, 'preemption_count': 0}), (124506, {'train/accuracy': 0.8481743931770325, 'train/loss': 0.8100681304931641, 'validation/accuracy': 0.7289199829101562, 'validation/loss': 1.3144792318344116, 'validation/num_examples': 50000, 'test/accuracy': 0.6068000197410583, 'test/loss': 1.9477180242538452, 'test/num_examples': 10000, 'score': 42367.69151568413, 'total_duration': 43865.30625462532, 'accumulated_submission_time': 42367.69151568413, 'accumulated_eval_time': 1489.850107908249, 'accumulated_logging_time': 3.269981622695923, 'global_step': 124506, 'preemption_count': 0}), (126009, {'train/accuracy': 0.845723032951355, 'train/loss': 0.7846372127532959, 'validation/accuracy': 0.7291799783706665, 'validation/loss': 1.2905118465423584, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.9218626022338867, 'test/num_examples': 10000, 'score': 42877.82626962662, 'total_duration': 44393.40523290634, 'accumulated_submission_time': 42877.82626962662, 'accumulated_eval_time': 1507.7119023799896, 'accumulated_logging_time': 3.3195204734802246, 'global_step': 126009, 'preemption_count': 0}), (127511, {'train/accuracy': 0.8580795526504517, 'train/loss': 0.7403988838195801, 'validation/accuracy': 0.7360000014305115, 'validation/loss': 1.253514289855957, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.869235634803772, 'test/num_examples': 10000, 'score': 43387.9768986702, 'total_duration': 44921.322884082794, 'accumulated_submission_time': 43387.9768986702, 'accumulated_eval_time': 1525.3740797042847, 'accumulated_logging_time': 3.3690085411071777, 'global_step': 127511, 'preemption_count': 0}), (129014, {'train/accuracy': 0.85550856590271, 'train/loss': 0.7635818123817444, 'validation/accuracy': 0.7354599833488464, 'validation/loss': 1.2662724256515503, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.904442310333252, 'test/num_examples': 10000, 'score': 43898.0478746891, 'total_duration': 45448.98998808861, 'accumulated_submission_time': 43898.0478746891, 'accumulated_eval_time': 1542.8680157661438, 'accumulated_logging_time': 3.4177677631378174, 'global_step': 129014, 'preemption_count': 0}), (130515, {'train/accuracy': 0.8826330900192261, 'train/loss': 0.6655307412147522, 'validation/accuracy': 0.7340999841690063, 'validation/loss': 1.2825183868408203, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.9217170476913452, 'test/num_examples': 10000, 'score': 44407.95130634308, 'total_duration': 45976.39903450012, 'accumulated_submission_time': 44407.95130634308, 'accumulated_eval_time': 1560.2620012760162, 'accumulated_logging_time': 3.4746148586273193, 'global_step': 130515, 'preemption_count': 0}), (132018, {'train/accuracy': 0.8704758882522583, 'train/loss': 0.7248345613479614, 'validation/accuracy': 0.731499969959259, 'validation/loss': 1.2995328903198242, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.936708688735962, 'test/num_examples': 10000, 'score': 44918.10623574257, 'total_duration': 46504.47780227661, 'accumulated_submission_time': 44918.10623574257, 'accumulated_eval_time': 1578.0767569541931, 'accumulated_logging_time': 3.5267868041992188, 'global_step': 132018, 'preemption_count': 0}), (133517, {'train/accuracy': 0.8717314600944519, 'train/loss': 0.7035813331604004, 'validation/accuracy': 0.7369599938392639, 'validation/loss': 1.2577601671218872, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.885425329208374, 'test/num_examples': 10000, 'score': 45426.990837574005, 'total_duration': 47032.312911748886, 'accumulated_submission_time': 45426.990837574005, 'accumulated_eval_time': 1595.8508143424988, 'accumulated_logging_time': 4.649079084396362, 'global_step': 133517, 'preemption_count': 0}), (135020, {'train/accuracy': 0.868582546710968, 'train/loss': 0.7156649827957153, 'validation/accuracy': 0.7356799840927124, 'validation/loss': 1.2662253379821777, 'validation/num_examples': 50000, 'test/accuracy': 0.6134000420570374, 'test/loss': 1.8910014629364014, 'test/num_examples': 10000, 'score': 45937.16322660446, 'total_duration': 47560.45282816887, 'accumulated_submission_time': 45937.16322660446, 'accumulated_eval_time': 1613.7076733112335, 'accumulated_logging_time': 4.703073501586914, 'global_step': 135020, 'preemption_count': 0}), (136523, {'train/accuracy': 0.8720503449440002, 'train/loss': 0.7015858888626099, 'validation/accuracy': 0.7404599785804749, 'validation/loss': 1.2451156377792358, 'validation/num_examples': 50000, 'test/accuracy': 0.6157000064849854, 'test/loss': 1.8840872049331665, 'test/num_examples': 10000, 'score': 46447.39303159714, 'total_duration': 48088.235530138016, 'accumulated_submission_time': 46447.39303159714, 'accumulated_eval_time': 1631.1530323028564, 'accumulated_logging_time': 4.756369113922119, 'global_step': 136523, 'preemption_count': 0}), (138025, {'train/accuracy': 0.8668486475944519, 'train/loss': 0.7391188144683838, 'validation/accuracy': 0.7368199825286865, 'validation/loss': 1.2867945432662964, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.9359248876571655, 'test/num_examples': 10000, 'score': 46957.294365644455, 'total_duration': 48616.05543756485, 'accumulated_submission_time': 46957.294365644455, 'accumulated_eval_time': 1648.9669427871704, 'accumulated_logging_time': 4.806999683380127, 'global_step': 138025, 'preemption_count': 0}), (139527, {'train/accuracy': 0.8956273794174194, 'train/loss': 0.627384603023529, 'validation/accuracy': 0.7456600069999695, 'validation/loss': 1.240861415863037, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.87476646900177, 'test/num_examples': 10000, 'score': 47467.2943482399, 'total_duration': 49143.7070350647, 'accumulated_submission_time': 47467.2943482399, 'accumulated_eval_time': 1666.5136742591858, 'accumulated_logging_time': 4.857511758804321, 'global_step': 139527, 'preemption_count': 0}), (141030, {'train/accuracy': 0.893973171710968, 'train/loss': 0.6151627898216248, 'validation/accuracy': 0.7440399527549744, 'validation/loss': 1.2287087440490723, 'validation/num_examples': 50000, 'test/accuracy': 0.619700014591217, 'test/loss': 1.8548864126205444, 'test/num_examples': 10000, 'score': 47977.450786590576, 'total_duration': 49671.644984960556, 'accumulated_submission_time': 47977.450786590576, 'accumulated_eval_time': 1684.1839241981506, 'accumulated_logging_time': 4.9148759841918945, 'global_step': 141030, 'preemption_count': 0}), (142532, {'train/accuracy': 0.8904854655265808, 'train/loss': 0.6626537442207336, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.2583589553833008, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.8964543342590332, 'test/num_examples': 10000, 'score': 48487.50527572632, 'total_duration': 50199.49908399582, 'accumulated_submission_time': 48487.50527572632, 'accumulated_eval_time': 1701.8740639686584, 'accumulated_logging_time': 4.969464063644409, 'global_step': 142532, 'preemption_count': 0}), (144035, {'train/accuracy': 0.892598032951355, 'train/loss': 0.6262313723564148, 'validation/accuracy': 0.7450399994850159, 'validation/loss': 1.2290778160095215, 'validation/num_examples': 50000, 'test/accuracy': 0.6198000311851501, 'test/loss': 1.864946722984314, 'test/num_examples': 10000, 'score': 48997.59909081459, 'total_duration': 50727.18270134926, 'accumulated_submission_time': 48997.59909081459, 'accumulated_eval_time': 1719.357551574707, 'accumulated_logging_time': 5.022829294204712, 'global_step': 144035, 'preemption_count': 0}), (145537, {'train/accuracy': 0.8907046914100647, 'train/loss': 0.6271235346794128, 'validation/accuracy': 0.7487599849700928, 'validation/loss': 1.223987102508545, 'validation/num_examples': 50000, 'test/accuracy': 0.6224000453948975, 'test/loss': 1.8546254634857178, 'test/num_examples': 10000, 'score': 49507.578125715256, 'total_duration': 51255.033161878586, 'accumulated_submission_time': 49507.578125715256, 'accumulated_eval_time': 1737.1198983192444, 'accumulated_logging_time': 5.078593730926514, 'global_step': 145537, 'preemption_count': 0}), (147039, {'train/accuracy': 0.8966238498687744, 'train/loss': 0.6165596842765808, 'validation/accuracy': 0.7476199865341187, 'validation/loss': 1.2288175821304321, 'validation/num_examples': 50000, 'test/accuracy': 0.6242000460624695, 'test/loss': 1.8519423007965088, 'test/num_examples': 10000, 'score': 50017.51508355141, 'total_duration': 51782.78130912781, 'accumulated_submission_time': 50017.51508355141, 'accumulated_eval_time': 1754.819967508316, 'accumulated_logging_time': 5.133638858795166, 'global_step': 147039, 'preemption_count': 0}), (148541, {'train/accuracy': 0.9024234414100647, 'train/loss': 0.5903157591819763, 'validation/accuracy': 0.7495200037956238, 'validation/loss': 1.215433120727539, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.8344119787216187, 'test/num_examples': 10000, 'score': 50527.503370285034, 'total_duration': 52311.052006959915, 'accumulated_submission_time': 50527.503370285034, 'accumulated_eval_time': 1772.9918661117554, 'accumulated_logging_time': 5.189709186553955, 'global_step': 148541, 'preemption_count': 0}), (150044, {'train/accuracy': 0.910574734210968, 'train/loss': 0.5532589554786682, 'validation/accuracy': 0.75, 'validation/loss': 1.207600712776184, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.8446438312530518, 'test/num_examples': 10000, 'score': 51037.676836013794, 'total_duration': 52838.9393966198, 'accumulated_submission_time': 51037.676836013794, 'accumulated_eval_time': 1790.6060602664948, 'accumulated_logging_time': 5.234796047210693, 'global_step': 150044, 'preemption_count': 0}), (151545, {'train/accuracy': 0.9098173975944519, 'train/loss': 0.566749632358551, 'validation/accuracy': 0.7523599863052368, 'validation/loss': 1.2168418169021606, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.8432186841964722, 'test/num_examples': 10000, 'score': 51547.57273697853, 'total_duration': 53366.512150764465, 'accumulated_submission_time': 51547.57273697853, 'accumulated_eval_time': 1808.1750228404999, 'accumulated_logging_time': 5.288285970687866, 'global_step': 151545, 'preemption_count': 0}), (153048, {'train/accuracy': 0.915058970451355, 'train/loss': 0.5383015871047974, 'validation/accuracy': 0.7546399831771851, 'validation/loss': 1.1951937675476074, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.818287968635559, 'test/num_examples': 10000, 'score': 52057.77176237106, 'total_duration': 53894.68529486656, 'accumulated_submission_time': 52057.77176237106, 'accumulated_eval_time': 1826.0408027172089, 'accumulated_logging_time': 5.3418190479278564, 'global_step': 153048, 'preemption_count': 0}), (154550, {'train/accuracy': 0.9122289419174194, 'train/loss': 0.5614323616027832, 'validation/accuracy': 0.75382000207901, 'validation/loss': 1.2097517251968384, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8361276388168335, 'test/num_examples': 10000, 'score': 52567.898394823074, 'total_duration': 54422.400752067566, 'accumulated_submission_time': 52567.898394823074, 'accumulated_eval_time': 1843.5144710540771, 'accumulated_logging_time': 5.40025782585144, 'global_step': 154550, 'preemption_count': 0}), (156053, {'train/accuracy': 0.9175502061843872, 'train/loss': 0.5501120090484619, 'validation/accuracy': 0.7555999755859375, 'validation/loss': 1.2017172574996948, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.8328757286071777, 'test/num_examples': 10000, 'score': 53077.98852467537, 'total_duration': 54950.4566886425, 'accumulated_submission_time': 53077.98852467537, 'accumulated_eval_time': 1861.3695440292358, 'accumulated_logging_time': 5.455489873886108, 'global_step': 156053, 'preemption_count': 0}), (157555, {'train/accuracy': 0.9153977632522583, 'train/loss': 0.5394268035888672, 'validation/accuracy': 0.7566399574279785, 'validation/loss': 1.1991684436798096, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.8347876071929932, 'test/num_examples': 10000, 'score': 53588.060198783875, 'total_duration': 55479.06328034401, 'accumulated_submission_time': 53588.060198783875, 'accumulated_eval_time': 1879.8014032840729, 'accumulated_logging_time': 5.5035552978515625, 'global_step': 157555, 'preemption_count': 0}), (159058, {'train/accuracy': 0.9265186190605164, 'train/loss': 0.5162858366966248, 'validation/accuracy': 0.7549399733543396, 'validation/loss': 1.2046244144439697, 'validation/num_examples': 50000, 'test/accuracy': 0.6288000345230103, 'test/loss': 1.8388917446136475, 'test/num_examples': 10000, 'score': 54098.17010354996, 'total_duration': 56007.06683254242, 'accumulated_submission_time': 54098.17010354996, 'accumulated_eval_time': 1897.5863370895386, 'accumulated_logging_time': 5.557616472244263, 'global_step': 159058, 'preemption_count': 0}), (160560, {'train/accuracy': 0.924824595451355, 'train/loss': 0.5198838114738464, 'validation/accuracy': 0.7564599514007568, 'validation/loss': 1.200865626335144, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.8306161165237427, 'test/num_examples': 10000, 'score': 54608.08381175995, 'total_duration': 56534.481812000275, 'accumulated_submission_time': 54608.08381175995, 'accumulated_eval_time': 1914.9792048931122, 'accumulated_logging_time': 5.611308336257935, 'global_step': 160560, 'preemption_count': 0}), (162062, {'train/accuracy': 0.9266381859779358, 'train/loss': 0.4990904629230499, 'validation/accuracy': 0.7584199905395508, 'validation/loss': 1.1812278032302856, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.8089417219161987, 'test/num_examples': 10000, 'score': 55118.07119345665, 'total_duration': 57062.24536585808, 'accumulated_submission_time': 55118.07119345665, 'accumulated_eval_time': 1932.6419672966003, 'accumulated_logging_time': 5.669963121414185, 'global_step': 162062, 'preemption_count': 0}), (163564, {'train/accuracy': 0.9269371628761292, 'train/loss': 0.5071867108345032, 'validation/accuracy': 0.7584199905395508, 'validation/loss': 1.188880205154419, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8226237297058105, 'test/num_examples': 10000, 'score': 55628.10046887398, 'total_duration': 57590.141486644745, 'accumulated_submission_time': 55628.10046887398, 'accumulated_eval_time': 1950.3917593955994, 'accumulated_logging_time': 5.7316248416900635, 'global_step': 163564, 'preemption_count': 0}), (165066, {'train/accuracy': 0.9267378449440002, 'train/loss': 0.4970170259475708, 'validation/accuracy': 0.7583799958229065, 'validation/loss': 1.1788029670715332, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.815388798713684, 'test/num_examples': 10000, 'score': 56138.13663291931, 'total_duration': 58117.80002164841, 'accumulated_submission_time': 56138.13663291931, 'accumulated_eval_time': 1967.9118270874023, 'accumulated_logging_time': 5.7794764041900635, 'global_step': 165066, 'preemption_count': 0}), (166568, {'train/accuracy': 0.9300262928009033, 'train/loss': 0.5010972619056702, 'validation/accuracy': 0.7594999670982361, 'validation/loss': 1.1896491050720215, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.816957950592041, 'test/num_examples': 10000, 'score': 56648.308772563934, 'total_duration': 58645.63115429878, 'accumulated_submission_time': 56648.308772563934, 'accumulated_eval_time': 1985.4621953964233, 'accumulated_logging_time': 5.834491014480591, 'global_step': 166568, 'preemption_count': 0}), (168070, {'train/accuracy': 0.9396922588348389, 'train/loss': 0.45635488629341125, 'validation/accuracy': 0.7609599828720093, 'validation/loss': 1.1790025234222412, 'validation/num_examples': 50000, 'test/accuracy': 0.6356000304222107, 'test/loss': 1.80722975730896, 'test/num_examples': 10000, 'score': 57158.316095113754, 'total_duration': 59173.45578980446, 'accumulated_submission_time': 57158.316095113754, 'accumulated_eval_time': 2003.1597566604614, 'accumulated_logging_time': 5.8996758460998535, 'global_step': 168070, 'preemption_count': 0}), (169572, {'train/accuracy': 0.9342115521430969, 'train/loss': 0.4681693911552429, 'validation/accuracy': 0.7597799897193909, 'validation/loss': 1.1787148714065552, 'validation/num_examples': 50000, 'test/accuracy': 0.6360000371932983, 'test/loss': 1.8080520629882812, 'test/num_examples': 10000, 'score': 57668.43019104004, 'total_duration': 59701.16584134102, 'accumulated_submission_time': 57668.43019104004, 'accumulated_eval_time': 2020.6457545757294, 'accumulated_logging_time': 5.955905914306641, 'global_step': 169572, 'preemption_count': 0}), (171074, {'train/accuracy': 0.9379583597183228, 'train/loss': 0.4724557101726532, 'validation/accuracy': 0.7612800002098083, 'validation/loss': 1.180039882659912, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.8073753118515015, 'test/num_examples': 10000, 'score': 58178.49935603142, 'total_duration': 60229.283868551254, 'accumulated_submission_time': 58178.49935603142, 'accumulated_eval_time': 2038.581184387207, 'accumulated_logging_time': 6.014849662780762, 'global_step': 171074, 'preemption_count': 0}), (172575, {'train/accuracy': 0.9350087642669678, 'train/loss': 0.46790677309036255, 'validation/accuracy': 0.761680006980896, 'validation/loss': 1.1774041652679443, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.7978694438934326, 'test/num_examples': 10000, 'score': 58688.43921303749, 'total_duration': 60756.84143066406, 'accumulated_submission_time': 58688.43921303749, 'accumulated_eval_time': 2056.078993320465, 'accumulated_logging_time': 6.078775405883789, 'global_step': 172575, 'preemption_count': 0}), (174077, {'train/accuracy': 0.9363440275192261, 'train/loss': 0.4684905409812927, 'validation/accuracy': 0.7619799971580505, 'validation/loss': 1.1775026321411133, 'validation/num_examples': 50000, 'test/accuracy': 0.6359000205993652, 'test/loss': 1.8029717206954956, 'test/num_examples': 10000, 'score': 59198.377883434296, 'total_duration': 61284.59939098358, 'accumulated_submission_time': 59198.377883434296, 'accumulated_eval_time': 2073.7847397327423, 'accumulated_logging_time': 6.137209415435791, 'global_step': 174077, 'preemption_count': 0}), (175578, {'train/accuracy': 0.9384765625, 'train/loss': 0.46365800499916077, 'validation/accuracy': 0.7625199556350708, 'validation/loss': 1.177310824394226, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.8003582954406738, 'test/num_examples': 10000, 'score': 59708.31727671623, 'total_duration': 61812.000276088715, 'accumulated_submission_time': 59708.31727671623, 'accumulated_eval_time': 2091.134479045868, 'accumulated_logging_time': 6.194151878356934, 'global_step': 175578, 'preemption_count': 0}), (177080, {'train/accuracy': 0.94046950340271, 'train/loss': 0.45434367656707764, 'validation/accuracy': 0.762779951095581, 'validation/loss': 1.1745065450668335, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.799221396446228, 'test/num_examples': 10000, 'score': 60218.344750881195, 'total_duration': 62340.08955526352, 'accumulated_submission_time': 60218.344750881195, 'accumulated_eval_time': 2109.08202624321, 'accumulated_logging_time': 6.251053094863892, 'global_step': 177080, 'preemption_count': 0}), (178582, {'train/accuracy': 0.9402901530265808, 'train/loss': 0.45552629232406616, 'validation/accuracy': 0.7623199820518494, 'validation/loss': 1.173393964767456, 'validation/num_examples': 50000, 'test/accuracy': 0.6392000317573547, 'test/loss': 1.7995479106903076, 'test/num_examples': 10000, 'score': 60728.52793264389, 'total_duration': 62868.04338693619, 'accumulated_submission_time': 60728.52793264389, 'accumulated_eval_time': 2126.7394778728485, 'accumulated_logging_time': 6.30780291557312, 'global_step': 178582, 'preemption_count': 0}), (180084, {'train/accuracy': 0.9409279227256775, 'train/loss': 0.45500898361206055, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 1.172922968864441, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.7973567247390747, 'test/num_examples': 10000, 'score': 61238.67451667786, 'total_duration': 63395.76078367233, 'accumulated_submission_time': 61238.67451667786, 'accumulated_eval_time': 2144.196943998337, 'accumulated_logging_time': 6.366096258163452, 'global_step': 180084, 'preemption_count': 0}), (181586, {'train/accuracy': 0.941824734210968, 'train/loss': 0.455964058637619, 'validation/accuracy': 0.7629599571228027, 'validation/loss': 1.1759765148162842, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.7996957302093506, 'test/num_examples': 10000, 'score': 61748.84334039688, 'total_duration': 63923.51679396629, 'accumulated_submission_time': 61748.84334039688, 'accumulated_eval_time': 2161.671940803528, 'accumulated_logging_time': 6.423700332641602, 'global_step': 181586, 'preemption_count': 0}), (183087, {'train/accuracy': 0.9397321343421936, 'train/loss': 0.4528777301311493, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.174703598022461, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.797763705253601, 'test/num_examples': 10000, 'score': 62258.972554922104, 'total_duration': 64451.45683383942, 'accumulated_submission_time': 62258.972554922104, 'accumulated_eval_time': 2179.367330789566, 'accumulated_logging_time': 6.485302448272705, 'global_step': 183087, 'preemption_count': 0}), (184588, {'train/accuracy': 0.939473032951355, 'train/loss': 0.4559187591075897, 'validation/accuracy': 0.7632799744606018, 'validation/loss': 1.1745176315307617, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.797345757484436, 'test/num_examples': 10000, 'score': 62768.99567198753, 'total_duration': 64979.35884261131, 'accumulated_submission_time': 62768.99567198753, 'accumulated_eval_time': 2197.1233875751495, 'accumulated_logging_time': 6.553771495819092, 'global_step': 184588, 'preemption_count': 0})], 'global_step': 185293}
I0128 07:19:19.788689 140187804313408 submission_runner.py:586] Timing: 63008.30585741997
I0128 07:19:19.788756 140187804313408 submission_runner.py:588] Total number of evals: 124
I0128 07:19:19.788799 140187804313408 submission_runner.py:589] ====================
I0128 07:19:19.788842 140187804313408 submission_runner.py:542] Using RNG seed 3827130657
I0128 07:19:19.790159 140187804313408 submission_runner.py:551] --- Tuning run 3/5 ---
I0128 07:19:19.790273 140187804313408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_3.
I0128 07:19:19.793592 140187804313408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_3/hparams.json.
I0128 07:19:19.794335 140187804313408 submission_runner.py:206] Initializing dataset.
I0128 07:19:19.803374 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0128 07:19:19.813476 140187804313408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0128 07:19:19.993873 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0128 07:19:20.203340 140187804313408 submission_runner.py:213] Initializing model.
I0128 07:19:25.629102 140187804313408 submission_runner.py:255] Initializing optimizer.
I0128 07:19:26.033808 140187804313408 submission_runner.py:262] Initializing metrics bundle.
I0128 07:19:26.033970 140187804313408 submission_runner.py:280] Initializing checkpoint and logger.
I0128 07:19:26.049227 140187804313408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0128 07:19:26.049347 140187804313408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0128 07:19:37.457328 140187804313408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0128 07:19:48.795783 140187804313408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_3/flags_0.json.
I0128 07:19:48.799999 140187804313408 submission_runner.py:314] Starting training loop.
I0128 07:20:19.295343 140026058876672 logging_writer.py:48] [0] global_step=0, grad_norm=0.6558058857917786, loss=6.934148788452148
I0128 07:20:19.308452 140187804313408 spec.py:321] Evaluating on the training split.
I0128 07:20:25.524604 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 07:20:34.537137 140187804313408 spec.py:349] Evaluating on the test split.
I0128 07:20:37.160116 140187804313408 submission_runner.py:408] Time since start: 48.36s, 	Step: 1, 	{'train/accuracy': 0.000996492337435484, 'train/loss': 6.91312313079834, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 30.50831913948059, 'total_duration': 48.360055446624756, 'accumulated_submission_time': 30.50831913948059, 'accumulated_eval_time': 17.85161852836609, 'accumulated_logging_time': 0}
I0128 07:20:37.169992 140026159523584 logging_writer.py:48] [1] accumulated_eval_time=17.851619, accumulated_logging_time=0, accumulated_submission_time=30.508319, global_step=1, preemption_count=0, score=30.508319, test/accuracy=0.000600, test/loss=6.912549, test/num_examples=10000, total_duration=48.360055, train/accuracy=0.000996, train/loss=6.913123, validation/accuracy=0.000760, validation/loss=6.913175, validation/num_examples=50000
I0128 07:21:11.261309 140026167916288 logging_writer.py:48] [100] global_step=100, grad_norm=0.6533726453781128, loss=6.897608280181885
I0128 07:21:45.337742 140026159523584 logging_writer.py:48] [200] global_step=200, grad_norm=0.6475525498390198, loss=6.861892223358154
I0128 07:22:19.408312 140026167916288 logging_writer.py:48] [300] global_step=300, grad_norm=0.6928495764732361, loss=6.796716690063477
I0128 07:22:53.516632 140026159523584 logging_writer.py:48] [400] global_step=400, grad_norm=0.7538101673126221, loss=6.683480739593506
I0128 07:23:27.602201 140026167916288 logging_writer.py:48] [500] global_step=500, grad_norm=0.7955703735351562, loss=6.547853469848633
I0128 07:24:01.710446 140026159523584 logging_writer.py:48] [600] global_step=600, grad_norm=0.8473390340805054, loss=6.440446376800537
I0128 07:24:35.838154 140026167916288 logging_writer.py:48] [700] global_step=700, grad_norm=0.8668419718742371, loss=6.369297504425049
I0128 07:25:09.952290 140026159523584 logging_writer.py:48] [800] global_step=800, grad_norm=1.0322355031967163, loss=6.273805141448975
I0128 07:25:44.071870 140026167916288 logging_writer.py:48] [900] global_step=900, grad_norm=2.347334146499634, loss=6.067586898803711
I0128 07:26:18.196642 140026159523584 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.173936128616333, loss=5.989800930023193
I0128 07:26:52.274661 140026167916288 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.8400930166244507, loss=5.812633991241455
I0128 07:27:26.426628 140026159523584 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.0297398567199707, loss=5.772386074066162
I0128 07:28:00.558083 140026167916288 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.2532763481140137, loss=5.697347640991211
I0128 07:28:34.678353 140026159523584 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.166278600692749, loss=5.566329002380371
I0128 07:29:07.235452 140187804313408 spec.py:321] Evaluating on the training split.
I0128 07:29:13.487898 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 07:29:22.717183 140187804313408 spec.py:349] Evaluating on the test split.
I0128 07:29:25.303151 140187804313408 submission_runner.py:408] Time since start: 576.50s, 	Step: 1497, 	{'train/accuracy': 0.06869818270206451, 'train/loss': 5.348140716552734, 'validation/accuracy': 0.06425999850034714, 'validation/loss': 5.3968071937561035, 'validation/num_examples': 50000, 'test/accuracy': 0.04620000347495079, 'test/loss': 5.633206367492676, 'test/num_examples': 10000, 'score': 540.5099921226501, 'total_duration': 576.5030663013458, 'accumulated_submission_time': 540.5099921226501, 'accumulated_eval_time': 35.9192590713501, 'accumulated_logging_time': 0.019089937210083008}
I0128 07:29:25.327283 140026067269376 logging_writer.py:48] [1497] accumulated_eval_time=35.919259, accumulated_logging_time=0.019090, accumulated_submission_time=540.509992, global_step=1497, preemption_count=0, score=540.509992, test/accuracy=0.046200, test/loss=5.633206, test/num_examples=10000, total_duration=576.503066, train/accuracy=0.068698, train/loss=5.348141, validation/accuracy=0.064260, validation/loss=5.396807, validation/num_examples=50000
I0128 07:29:26.703447 140026075662080 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.621307134628296, loss=5.581053256988525
I0128 07:30:00.730126 140026067269376 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.7740299701690674, loss=5.382948875427246
I0128 07:30:34.780085 140026075662080 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.314894437789917, loss=5.325825214385986
I0128 07:31:08.886653 140026067269376 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.0707647800445557, loss=5.248387336730957
I0128 07:31:42.964148 140026075662080 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.3767056465148926, loss=5.260183811187744
I0128 07:32:17.051204 140026067269376 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.175426483154297, loss=5.213905334472656
I0128 07:32:51.125529 140026075662080 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.4612183570861816, loss=5.131316661834717
I0128 07:33:25.221856 140026067269376 logging_writer.py:48] [2200] global_step=2200, grad_norm=5.164046287536621, loss=5.056492328643799
I0128 07:33:59.499174 140026075662080 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.192972183227539, loss=4.941689968109131
I0128 07:34:33.604531 140026067269376 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.057859420776367, loss=5.044637680053711
I0128 07:35:07.688602 140026075662080 logging_writer.py:48] [2500] global_step=2500, grad_norm=7.263390064239502, loss=4.912806034088135
I0128 07:35:41.805980 140026067269376 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.7798471450805664, loss=4.800103187561035
I0128 07:36:15.935594 140026075662080 logging_writer.py:48] [2700] global_step=2700, grad_norm=8.334019660949707, loss=4.8424272537231445
I0128 07:36:50.034475 140026067269376 logging_writer.py:48] [2800] global_step=2800, grad_norm=6.545033931732178, loss=4.708309173583984
I0128 07:37:24.141036 140026075662080 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.680351734161377, loss=4.656520843505859
I0128 07:37:55.307959 140187804313408 spec.py:321] Evaluating on the training split.
I0128 07:38:01.555425 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 07:38:10.587923 140187804313408 spec.py:349] Evaluating on the test split.
I0128 07:38:13.239602 140187804313408 submission_runner.py:408] Time since start: 1104.44s, 	Step: 2993, 	{'train/accuracy': 0.16974250972270966, 'train/loss': 4.304430961608887, 'validation/accuracy': 0.15215998888015747, 'validation/loss': 4.434348106384277, 'validation/num_examples': 50000, 'test/accuracy': 0.10750000178813934, 'test/loss': 4.922767639160156, 'test/num_examples': 10000, 'score': 1050.4253158569336, 'total_duration': 1104.4395382404327, 'accumulated_submission_time': 1050.4253158569336, 'accumulated_eval_time': 53.85086178779602, 'accumulated_logging_time': 0.05376577377319336}
I0128 07:38:13.256860 140026050483968 logging_writer.py:48] [2993] accumulated_eval_time=53.850862, accumulated_logging_time=0.053766, accumulated_submission_time=1050.425316, global_step=2993, preemption_count=0, score=1050.425316, test/accuracy=0.107500, test/loss=4.922768, test/num_examples=10000, total_duration=1104.439538, train/accuracy=0.169743, train/loss=4.304431, validation/accuracy=0.152160, validation/loss=4.434348, validation/num_examples=50000
I0128 07:38:15.974591 140026058876672 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.664111614227295, loss=4.567070007324219
I0128 07:38:50.028263 140026050483968 logging_writer.py:48] [3100] global_step=3100, grad_norm=7.431914329528809, loss=4.579644203186035
I0128 07:39:24.139539 140026058876672 logging_writer.py:48] [3200] global_step=3200, grad_norm=9.659852981567383, loss=4.610473155975342
I0128 07:39:58.213384 140026050483968 logging_writer.py:48] [3300] global_step=3300, grad_norm=6.804960250854492, loss=4.465188026428223
I0128 07:40:32.438609 140026058876672 logging_writer.py:48] [3400] global_step=3400, grad_norm=6.3642778396606445, loss=4.458704471588135
I0128 07:41:06.558699 140026050483968 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.937528133392334, loss=4.385241985321045
I0128 07:41:40.665443 140026058876672 logging_writer.py:48] [3600] global_step=3600, grad_norm=7.0836873054504395, loss=4.376157283782959
I0128 07:42:14.769991 140026050483968 logging_writer.py:48] [3700] global_step=3700, grad_norm=5.892492294311523, loss=4.243226051330566
I0128 07:42:48.930754 140026058876672 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.078632354736328, loss=4.150761127471924
I0128 07:43:23.029355 140026050483968 logging_writer.py:48] [3900] global_step=3900, grad_norm=6.49774169921875, loss=4.123162269592285
I0128 07:43:57.167537 140026058876672 logging_writer.py:48] [4000] global_step=4000, grad_norm=9.134160995483398, loss=4.175893783569336
I0128 07:44:31.274317 140026050483968 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.750135898590088, loss=4.153453826904297
I0128 07:45:05.396460 140026058876672 logging_writer.py:48] [4200] global_step=4200, grad_norm=7.360040187835693, loss=4.09222936630249
I0128 07:45:39.524585 140026050483968 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.256209373474121, loss=3.953782796859741
I0128 07:46:13.645356 140026058876672 logging_writer.py:48] [4400] global_step=4400, grad_norm=7.74697208404541, loss=3.924367904663086
I0128 07:46:43.548848 140187804313408 spec.py:321] Evaluating on the training split.
I0128 07:46:49.813608 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 07:46:58.632524 140187804313408 spec.py:349] Evaluating on the test split.
I0128 07:47:01.265965 140187804313408 submission_runner.py:408] Time since start: 1632.47s, 	Step: 4489, 	{'train/accuracy': 0.2671595811843872, 'train/loss': 3.5422446727752686, 'validation/accuracy': 0.245619997382164, 'validation/loss': 3.673764944076538, 'validation/num_examples': 50000, 'test/accuracy': 0.1746000051498413, 'test/loss': 4.269079208374023, 'test/num_examples': 10000, 'score': 1560.6546158790588, 'total_duration': 1632.4659051895142, 'accumulated_submission_time': 1560.6546158790588, 'accumulated_eval_time': 71.5679407119751, 'accumulated_logging_time': 0.08060026168823242}
I0128 07:47:01.286159 140026167916288 logging_writer.py:48] [4489] accumulated_eval_time=71.567941, accumulated_logging_time=0.080600, accumulated_submission_time=1560.654616, global_step=4489, preemption_count=0, score=1560.654616, test/accuracy=0.174600, test/loss=4.269079, test/num_examples=10000, total_duration=1632.465905, train/accuracy=0.267160, train/loss=3.542245, validation/accuracy=0.245620, validation/loss=3.673765, validation/num_examples=50000
I0128 07:47:05.377261 140026176308992 logging_writer.py:48] [4500] global_step=4500, grad_norm=8.334443092346191, loss=3.9139902591705322
I0128 07:47:39.446753 140026167916288 logging_writer.py:48] [4600] global_step=4600, grad_norm=8.465130805969238, loss=3.8219237327575684
I0128 07:48:13.506432 140026176308992 logging_writer.py:48] [4700] global_step=4700, grad_norm=6.504533767700195, loss=3.85176420211792
I0128 07:48:47.616194 140026167916288 logging_writer.py:48] [4800] global_step=4800, grad_norm=6.096896171569824, loss=3.8029916286468506
I0128 07:49:21.719973 140026176308992 logging_writer.py:48] [4900] global_step=4900, grad_norm=6.467418193817139, loss=3.712113857269287
I0128 07:49:55.844061 140026167916288 logging_writer.py:48] [5000] global_step=5000, grad_norm=7.743505954742432, loss=3.5807127952575684
I0128 07:50:29.941429 140026176308992 logging_writer.py:48] [5100] global_step=5100, grad_norm=7.840372562408447, loss=3.6361241340637207
I0128 07:51:04.039553 140026167916288 logging_writer.py:48] [5200] global_step=5200, grad_norm=6.9555840492248535, loss=3.5896899700164795
I0128 07:51:38.138763 140026176308992 logging_writer.py:48] [5300] global_step=5300, grad_norm=10.001384735107422, loss=3.7592995166778564
I0128 07:52:12.242362 140026167916288 logging_writer.py:48] [5400] global_step=5400, grad_norm=7.597901821136475, loss=3.4627938270568848
I0128 07:52:46.410187 140026176308992 logging_writer.py:48] [5500] global_step=5500, grad_norm=6.771588325500488, loss=3.5814368724823
I0128 07:53:20.527172 140026167916288 logging_writer.py:48] [5600] global_step=5600, grad_norm=8.848642349243164, loss=3.50028133392334
I0128 07:53:54.650094 140026176308992 logging_writer.py:48] [5700] global_step=5700, grad_norm=5.333061218261719, loss=3.3834781646728516
I0128 07:54:28.761022 140026167916288 logging_writer.py:48] [5800] global_step=5800, grad_norm=8.220878601074219, loss=3.550938606262207
I0128 07:55:02.863576 140026176308992 logging_writer.py:48] [5900] global_step=5900, grad_norm=7.163137435913086, loss=3.5751781463623047
I0128 07:55:31.315855 140187804313408 spec.py:321] Evaluating on the training split.
I0128 07:55:37.540745 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 07:55:46.452952 140187804313408 spec.py:349] Evaluating on the test split.
I0128 07:55:49.080264 140187804313408 submission_runner.py:408] Time since start: 2160.28s, 	Step: 5985, 	{'train/accuracy': 0.3645368218421936, 'train/loss': 2.9136829376220703, 'validation/accuracy': 0.340719997882843, 'validation/loss': 3.063777446746826, 'validation/num_examples': 50000, 'test/accuracy': 0.2565000057220459, 'test/loss': 3.7345449924468994, 'test/num_examples': 10000, 'score': 2070.620194196701, 'total_duration': 2160.280205488205, 'accumulated_submission_time': 2070.620194196701, 'accumulated_eval_time': 89.33231997489929, 'accumulated_logging_time': 0.11035013198852539}
I0128 07:55:49.098773 140026050483968 logging_writer.py:48] [5985] accumulated_eval_time=89.332320, accumulated_logging_time=0.110350, accumulated_submission_time=2070.620194, global_step=5985, preemption_count=0, score=2070.620194, test/accuracy=0.256500, test/loss=3.734545, test/num_examples=10000, total_duration=2160.280205, train/accuracy=0.364537, train/loss=2.913683, validation/accuracy=0.340720, validation/loss=3.063777, validation/num_examples=50000
I0128 07:55:54.575045 140026058876672 logging_writer.py:48] [6000] global_step=6000, grad_norm=7.051062107086182, loss=3.394634246826172
I0128 07:56:28.626462 140026050483968 logging_writer.py:48] [6100] global_step=6100, grad_norm=6.418942451477051, loss=3.473090171813965
I0128 07:57:02.725352 140026058876672 logging_writer.py:48] [6200] global_step=6200, grad_norm=7.660884380340576, loss=3.2417585849761963
I0128 07:57:36.873590 140026050483968 logging_writer.py:48] [6300] global_step=6300, grad_norm=11.595942497253418, loss=3.309541940689087
I0128 07:58:10.988488 140026058876672 logging_writer.py:48] [6400] global_step=6400, grad_norm=12.078389167785645, loss=3.2727670669555664
I0128 07:58:45.141408 140026050483968 logging_writer.py:48] [6500] global_step=6500, grad_norm=5.586670875549316, loss=3.161177158355713
I0128 07:59:19.324414 140026058876672 logging_writer.py:48] [6600] global_step=6600, grad_norm=12.769291877746582, loss=3.3508245944976807
I0128 07:59:53.426823 140026050483968 logging_writer.py:48] [6700] global_step=6700, grad_norm=7.069140911102295, loss=3.106792449951172
I0128 08:00:27.501113 140026058876672 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.777975082397461, loss=3.1911251544952393
I0128 08:01:01.605386 140026050483968 logging_writer.py:48] [6900] global_step=6900, grad_norm=5.741639614105225, loss=3.037890672683716
I0128 08:01:35.722194 140026058876672 logging_writer.py:48] [7000] global_step=7000, grad_norm=7.300779819488525, loss=3.0059118270874023
I0128 08:02:09.821895 140026050483968 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.7983784675598145, loss=3.0770328044891357
I0128 08:02:43.966819 140026058876672 logging_writer.py:48] [7200] global_step=7200, grad_norm=7.413153648376465, loss=3.1569252014160156
I0128 08:03:18.064887 140026050483968 logging_writer.py:48] [7300] global_step=7300, grad_norm=4.04030704498291, loss=3.0209310054779053
I0128 08:03:52.176155 140026058876672 logging_writer.py:48] [7400] global_step=7400, grad_norm=5.329312324523926, loss=2.9952898025512695
I0128 08:04:19.268111 140187804313408 spec.py:321] Evaluating on the training split.
I0128 08:04:25.679477 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 08:04:34.694104 140187804313408 spec.py:349] Evaluating on the test split.
I0128 08:04:37.332356 140187804313408 submission_runner.py:408] Time since start: 2688.53s, 	Step: 7481, 	{'train/accuracy': 0.442402720451355, 'train/loss': 2.474280834197998, 'validation/accuracy': 0.38402000069618225, 'validation/loss': 2.817934513092041, 'validation/num_examples': 50000, 'test/accuracy': 0.28950002789497375, 'test/loss': 3.5531110763549805, 'test/num_examples': 10000, 'score': 2580.7226872444153, 'total_duration': 2688.5322892665863, 'accumulated_submission_time': 2580.7226872444153, 'accumulated_eval_time': 107.3965380191803, 'accumulated_logging_time': 0.142303466796875}
I0128 08:04:37.352030 140026151130880 logging_writer.py:48] [7481] accumulated_eval_time=107.396538, accumulated_logging_time=0.142303, accumulated_submission_time=2580.722687, global_step=7481, preemption_count=0, score=2580.722687, test/accuracy=0.289500, test/loss=3.553111, test/num_examples=10000, total_duration=2688.532289, train/accuracy=0.442403, train/loss=2.474281, validation/accuracy=0.384020, validation/loss=2.817935, validation/num_examples=50000
I0128 08:04:44.156606 140026159523584 logging_writer.py:48] [7500] global_step=7500, grad_norm=5.936028957366943, loss=2.938969850540161
I0128 08:05:18.160679 140026151130880 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.962125301361084, loss=2.973555564880371
I0128 08:05:52.271951 140026159523584 logging_writer.py:48] [7700] global_step=7700, grad_norm=7.310459136962891, loss=3.1221399307250977
I0128 08:06:26.336149 140026151130880 logging_writer.py:48] [7800] global_step=7800, grad_norm=8.509533882141113, loss=2.900062322616577
I0128 08:07:00.444279 140026159523584 logging_writer.py:48] [7900] global_step=7900, grad_norm=6.414226055145264, loss=3.0475196838378906
I0128 08:07:34.531665 140026151130880 logging_writer.py:48] [8000] global_step=8000, grad_norm=6.125955104827881, loss=2.8778295516967773
I0128 08:08:08.618458 140026159523584 logging_writer.py:48] [8100] global_step=8100, grad_norm=6.3183913230896, loss=2.8578875064849854
I0128 08:08:42.703724 140026151130880 logging_writer.py:48] [8200] global_step=8200, grad_norm=6.15688419342041, loss=2.8085882663726807
I0128 08:09:16.822846 140026159523584 logging_writer.py:48] [8300] global_step=8300, grad_norm=4.259186744689941, loss=2.898787260055542
I0128 08:09:50.882142 140026151130880 logging_writer.py:48] [8400] global_step=8400, grad_norm=5.201014518737793, loss=2.753026008605957
I0128 08:10:24.948607 140026159523584 logging_writer.py:48] [8500] global_step=8500, grad_norm=5.171924591064453, loss=2.721041679382324
I0128 08:10:59.018810 140026151130880 logging_writer.py:48] [8600] global_step=8600, grad_norm=6.954333782196045, loss=2.8823513984680176
I0128 08:11:33.090001 140026159523584 logging_writer.py:48] [8700] global_step=8700, grad_norm=7.49188756942749, loss=2.8046488761901855
I0128 08:12:07.212705 140026151130880 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.469359874725342, loss=2.747847080230713
I0128 08:12:41.291074 140026159523584 logging_writer.py:48] [8900] global_step=8900, grad_norm=6.9674787521362305, loss=2.7343757152557373
I0128 08:13:07.349564 140187804313408 spec.py:321] Evaluating on the training split.
I0128 08:13:14.217910 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 08:13:23.329571 140187804313408 spec.py:349] Evaluating on the test split.
I0128 08:13:25.979317 140187804313408 submission_runner.py:408] Time since start: 3217.18s, 	Step: 8978, 	{'train/accuracy': 0.4957549273967743, 'train/loss': 2.1577329635620117, 'validation/accuracy': 0.4500799775123596, 'validation/loss': 2.436403274536133, 'validation/num_examples': 50000, 'test/accuracy': 0.34530001878738403, 'test/loss': 3.1740477085113525, 'test/num_examples': 10000, 'score': 3090.6557648181915, 'total_duration': 3217.1792571544647, 'accumulated_submission_time': 3090.6557648181915, 'accumulated_eval_time': 126.0262610912323, 'accumulated_logging_time': 0.1721491813659668}
I0128 08:13:25.999327 140026050483968 logging_writer.py:48] [8978] accumulated_eval_time=126.026261, accumulated_logging_time=0.172149, accumulated_submission_time=3090.655765, global_step=8978, preemption_count=0, score=3090.655765, test/accuracy=0.345300, test/loss=3.174048, test/num_examples=10000, total_duration=3217.179257, train/accuracy=0.495755, train/loss=2.157733, validation/accuracy=0.450080, validation/loss=2.436403, validation/num_examples=50000
I0128 08:13:33.824832 140026058876672 logging_writer.py:48] [9000] global_step=9000, grad_norm=5.175506114959717, loss=2.7260518074035645
I0128 08:14:07.861410 140026050483968 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.5533576011657715, loss=2.7288055419921875
I0128 08:14:41.933674 140026058876672 logging_writer.py:48] [9200] global_step=9200, grad_norm=7.4081645011901855, loss=2.805272102355957
I0128 08:15:16.038221 140026050483968 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.501076698303223, loss=2.6459240913391113
I0128 08:15:50.116363 140026058876672 logging_writer.py:48] [9400] global_step=9400, grad_norm=8.155899047851562, loss=2.7011055946350098
I0128 08:16:24.224493 140026050483968 logging_writer.py:48] [9500] global_step=9500, grad_norm=8.532636642456055, loss=2.7040367126464844
I0128 08:16:58.302426 140026058876672 logging_writer.py:48] [9600] global_step=9600, grad_norm=6.2343058586120605, loss=2.5529541969299316
I0128 08:17:32.394747 140026050483968 logging_writer.py:48] [9700] global_step=9700, grad_norm=8.199095726013184, loss=2.7445578575134277
I0128 08:18:06.485529 140026058876672 logging_writer.py:48] [9800] global_step=9800, grad_norm=6.804346561431885, loss=2.6067070960998535
I0128 08:18:40.742470 140026050483968 logging_writer.py:48] [9900] global_step=9900, grad_norm=7.688096523284912, loss=2.621864080429077
I0128 08:19:14.826005 140026058876672 logging_writer.py:48] [10000] global_step=10000, grad_norm=6.110201835632324, loss=2.5242395401000977
I0128 08:19:48.914814 140026050483968 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.5680437088012695, loss=2.793682813644409
I0128 08:20:22.999393 140026058876672 logging_writer.py:48] [10200] global_step=10200, grad_norm=5.830497741699219, loss=2.7423133850097656
I0128 08:20:57.060329 140026050483968 logging_writer.py:48] [10300] global_step=10300, grad_norm=6.312552452087402, loss=2.5341103076934814
I0128 08:21:31.153860 140026058876672 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.344857692718506, loss=2.566464900970459
I0128 08:21:56.184841 140187804313408 spec.py:321] Evaluating on the training split.
I0128 08:22:02.593710 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 08:22:11.672597 140187804313408 spec.py:349] Evaluating on the test split.
I0128 08:22:14.257068 140187804313408 submission_runner.py:408] Time since start: 3745.46s, 	Step: 10475, 	{'train/accuracy': 0.521882951259613, 'train/loss': 2.0246825218200684, 'validation/accuracy': 0.4817799925804138, 'validation/loss': 2.2595558166503906, 'validation/num_examples': 50000, 'test/accuracy': 0.3767000138759613, 'test/loss': 2.959792375564575, 'test/num_examples': 10000, 'score': 3600.778788328171, 'total_duration': 3745.4569323062897, 'accumulated_submission_time': 3600.778788328171, 'accumulated_eval_time': 144.09838795661926, 'accumulated_logging_time': 0.2016129493713379}
I0128 08:22:14.277149 140026159523584 logging_writer.py:48] [10475] accumulated_eval_time=144.098388, accumulated_logging_time=0.201613, accumulated_submission_time=3600.778788, global_step=10475, preemption_count=0, score=3600.778788, test/accuracy=0.376700, test/loss=2.959792, test/num_examples=10000, total_duration=3745.456932, train/accuracy=0.521883, train/loss=2.024683, validation/accuracy=0.481780, validation/loss=2.259556, validation/num_examples=50000
I0128 08:22:23.141769 140026167916288 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.96560525894165, loss=2.4681177139282227
I0128 08:22:57.154619 140026159523584 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.096773147583008, loss=2.3682661056518555
I0128 08:23:31.206877 140026167916288 logging_writer.py:48] [10700] global_step=10700, grad_norm=9.073949813842773, loss=2.6026315689086914
I0128 08:24:05.278343 140026159523584 logging_writer.py:48] [10800] global_step=10800, grad_norm=5.375461578369141, loss=2.525475025177002
I0128 08:24:39.351162 140026167916288 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.663887977600098, loss=2.5704407691955566
I0128 08:25:13.525442 140026159523584 logging_writer.py:48] [11000] global_step=11000, grad_norm=7.725703716278076, loss=2.5553414821624756
I0128 08:25:47.569028 140026167916288 logging_writer.py:48] [11100] global_step=11100, grad_norm=6.76125431060791, loss=2.34035062789917
I0128 08:26:21.622825 140026159523584 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.976670742034912, loss=2.430759906768799
I0128 08:26:55.713926 140026167916288 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.651116847991943, loss=2.4471664428710938
I0128 08:27:29.753690 140026159523584 logging_writer.py:48] [11400] global_step=11400, grad_norm=7.217275142669678, loss=2.388450860977173
I0128 08:28:03.803286 140026167916288 logging_writer.py:48] [11500] global_step=11500, grad_norm=9.025782585144043, loss=2.4393725395202637
I0128 08:28:37.864222 140026159523584 logging_writer.py:48] [11600] global_step=11600, grad_norm=7.179778099060059, loss=2.312143325805664
I0128 08:29:11.921257 140026167916288 logging_writer.py:48] [11700] global_step=11700, grad_norm=6.22321891784668, loss=2.4208405017852783
I0128 08:29:45.954210 140026159523584 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.590777397155762, loss=2.4030094146728516
I0128 08:30:20.023562 140026167916288 logging_writer.py:48] [11900] global_step=11900, grad_norm=7.047780513763428, loss=2.405235528945923
I0128 08:30:44.328016 140187804313408 spec.py:321] Evaluating on the training split.
I0128 08:30:50.562917 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 08:30:59.439358 140187804313408 spec.py:349] Evaluating on the test split.
I0128 08:31:02.077078 140187804313408 submission_runner.py:408] Time since start: 4273.28s, 	Step: 11973, 	{'train/accuracy': 0.5642538070678711, 'train/loss': 1.8322232961654663, 'validation/accuracy': 0.5165799856185913, 'validation/loss': 2.0682852268218994, 'validation/num_examples': 50000, 'test/accuracy': 0.397100031375885, 'test/loss': 2.806823492050171, 'test/num_examples': 10000, 'score': 4110.76530623436, 'total_duration': 4273.276990890503, 'accumulated_submission_time': 4110.76530623436, 'accumulated_eval_time': 161.8473880290985, 'accumulated_logging_time': 0.2317366600036621}
I0128 08:31:02.108489 140026058876672 logging_writer.py:48] [11973] accumulated_eval_time=161.847388, accumulated_logging_time=0.231737, accumulated_submission_time=4110.765306, global_step=11973, preemption_count=0, score=4110.765306, test/accuracy=0.397100, test/loss=2.806823, test/num_examples=10000, total_duration=4273.276991, train/accuracy=0.564254, train/loss=1.832223, validation/accuracy=0.516580, validation/loss=2.068285, validation/num_examples=50000
I0128 08:31:11.645718 140026067269376 logging_writer.py:48] [12000] global_step=12000, grad_norm=6.424738883972168, loss=2.5139527320861816
I0128 08:31:45.696960 140026058876672 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.993558883666992, loss=2.356700897216797
I0128 08:32:19.760639 140026067269376 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.804718017578125, loss=2.467057704925537
I0128 08:32:53.808344 140026058876672 logging_writer.py:48] [12300] global_step=12300, grad_norm=6.355085849761963, loss=2.3867149353027344
I0128 08:33:27.884357 140026067269376 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.642718315124512, loss=2.2904229164123535
I0128 08:34:01.901957 140026058876672 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.729435443878174, loss=2.366339921951294
I0128 08:34:35.955309 140026067269376 logging_writer.py:48] [12600] global_step=12600, grad_norm=6.042009353637695, loss=2.372649669647217
I0128 08:35:09.998533 140026058876672 logging_writer.py:48] [12700] global_step=12700, grad_norm=5.217106819152832, loss=2.2500720024108887
I0128 08:35:44.076735 140026067269376 logging_writer.py:48] [12800] global_step=12800, grad_norm=8.593525886535645, loss=2.380089282989502
I0128 08:36:18.123443 140026058876672 logging_writer.py:48] [12900] global_step=12900, grad_norm=5.374526023864746, loss=2.4180469512939453
I0128 08:36:52.164558 140026067269376 logging_writer.py:48] [13000] global_step=13000, grad_norm=8.500883102416992, loss=2.2915759086608887
I0128 08:37:26.200487 140026058876672 logging_writer.py:48] [13100] global_step=13100, grad_norm=7.871399402618408, loss=2.3420848846435547
I0128 08:38:00.310784 140026067269376 logging_writer.py:48] [13200] global_step=13200, grad_norm=8.220526695251465, loss=2.2667863368988037
I0128 08:38:34.363452 140026058876672 logging_writer.py:48] [13300] global_step=13300, grad_norm=8.197153091430664, loss=2.3241653442382812
I0128 08:39:08.448887 140026067269376 logging_writer.py:48] [13400] global_step=13400, grad_norm=8.651589393615723, loss=2.325654983520508
I0128 08:39:32.401053 140187804313408 spec.py:321] Evaluating on the training split.
I0128 08:39:38.785422 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 08:39:47.911239 140187804313408 spec.py:349] Evaluating on the test split.
I0128 08:39:50.507221 140187804313408 submission_runner.py:408] Time since start: 4801.71s, 	Step: 13472, 	{'train/accuracy': 0.5740792155265808, 'train/loss': 1.7880722284317017, 'validation/accuracy': 0.5296199917793274, 'validation/loss': 2.0117011070251465, 'validation/num_examples': 50000, 'test/accuracy': 0.4182000160217285, 'test/loss': 2.717848777770996, 'test/num_examples': 10000, 'score': 4620.987542629242, 'total_duration': 4801.707133054733, 'accumulated_submission_time': 4620.987542629242, 'accumulated_eval_time': 179.95350456237793, 'accumulated_logging_time': 0.2778005599975586}
I0128 08:39:50.526890 140026159523584 logging_writer.py:48] [13472] accumulated_eval_time=179.953505, accumulated_logging_time=0.277801, accumulated_submission_time=4620.987543, global_step=13472, preemption_count=0, score=4620.987543, test/accuracy=0.418200, test/loss=2.717849, test/num_examples=10000, total_duration=4801.707133, train/accuracy=0.574079, train/loss=1.788072, validation/accuracy=0.529620, validation/loss=2.011701, validation/num_examples=50000
I0128 08:40:00.368546 140026167916288 logging_writer.py:48] [13500] global_step=13500, grad_norm=9.177803039550781, loss=2.331403970718384
I0128 08:40:34.358840 140026159523584 logging_writer.py:48] [13600] global_step=13600, grad_norm=6.514845848083496, loss=2.230861186981201
I0128 08:41:08.367991 140026167916288 logging_writer.py:48] [13700] global_step=13700, grad_norm=6.376920700073242, loss=2.305222511291504
I0128 08:41:42.393020 140026159523584 logging_writer.py:48] [13800] global_step=13800, grad_norm=7.6573567390441895, loss=2.25284481048584
I0128 08:42:16.429843 140026167916288 logging_writer.py:48] [13900] global_step=13900, grad_norm=8.924978256225586, loss=2.247570037841797
I0128 08:42:50.453548 140026159523584 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.925011157989502, loss=2.2850348949432373
I0128 08:43:24.476768 140026167916288 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.882617950439453, loss=2.2205119132995605
I0128 08:43:58.500565 140026159523584 logging_writer.py:48] [14200] global_step=14200, grad_norm=8.469834327697754, loss=2.297269582748413
I0128 08:44:32.593707 140026167916288 logging_writer.py:48] [14300] global_step=14300, grad_norm=6.9152092933654785, loss=2.282292366027832
I0128 08:45:06.609914 140026159523584 logging_writer.py:48] [14400] global_step=14400, grad_norm=6.575280666351318, loss=2.356588363647461
I0128 08:45:40.636068 140026167916288 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.578108787536621, loss=2.382075548171997
I0128 08:46:14.672843 140026159523584 logging_writer.py:48] [14600] global_step=14600, grad_norm=8.179266929626465, loss=2.281917095184326
I0128 08:46:48.694888 140026167916288 logging_writer.py:48] [14700] global_step=14700, grad_norm=7.697649002075195, loss=2.1697323322296143
I0128 08:47:22.690614 140026159523584 logging_writer.py:48] [14800] global_step=14800, grad_norm=5.4180707931518555, loss=2.1751623153686523
I0128 08:47:56.748509 140026167916288 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.991453647613525, loss=2.181124687194824
I0128 08:48:20.715563 140187804313408 spec.py:321] Evaluating on the training split.
I0128 08:48:26.997083 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 08:48:35.979317 140187804313408 spec.py:349] Evaluating on the test split.
I0128 08:48:38.601165 140187804313408 submission_runner.py:408] Time since start: 5329.80s, 	Step: 14972, 	{'train/accuracy': 0.5755141973495483, 'train/loss': 1.759779930114746, 'validation/accuracy': 0.540399968624115, 'validation/loss': 1.9560120105743408, 'validation/num_examples': 50000, 'test/accuracy': 0.4134000241756439, 'test/loss': 2.7285807132720947, 'test/num_examples': 10000, 'score': 5131.111432313919, 'total_duration': 5329.801098108292, 'accumulated_submission_time': 5131.111432313919, 'accumulated_eval_time': 197.83906412124634, 'accumulated_logging_time': 0.3077218532562256}
I0128 08:48:38.620990 140026050483968 logging_writer.py:48] [14972] accumulated_eval_time=197.839064, accumulated_logging_time=0.307722, accumulated_submission_time=5131.111432, global_step=14972, preemption_count=0, score=5131.111432, test/accuracy=0.413400, test/loss=2.728581, test/num_examples=10000, total_duration=5329.801098, train/accuracy=0.575514, train/loss=1.759780, validation/accuracy=0.540400, validation/loss=1.956012, validation/num_examples=50000
I0128 08:48:48.490590 140026058876672 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.376901626586914, loss=2.3003203868865967
I0128 08:49:22.483159 140026050483968 logging_writer.py:48] [15100] global_step=15100, grad_norm=5.010117053985596, loss=2.207159996032715
I0128 08:49:56.499215 140026058876672 logging_writer.py:48] [15200] global_step=15200, grad_norm=7.093149185180664, loss=2.2105259895324707
I0128 08:50:30.697189 140026050483968 logging_writer.py:48] [15300] global_step=15300, grad_norm=9.479135513305664, loss=2.259049892425537
I0128 08:51:04.712718 140026058876672 logging_writer.py:48] [15400] global_step=15400, grad_norm=7.547457218170166, loss=2.133974552154541
I0128 08:51:38.768692 140026050483968 logging_writer.py:48] [15500] global_step=15500, grad_norm=7.471222400665283, loss=2.2966349124908447
I0128 08:52:12.787113 140026058876672 logging_writer.py:48] [15600] global_step=15600, grad_norm=6.185327529907227, loss=2.2147669792175293
I0128 08:52:46.796066 140026050483968 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.515449047088623, loss=2.2095346450805664
I0128 08:53:20.802162 140026058876672 logging_writer.py:48] [15800] global_step=15800, grad_norm=5.359513282775879, loss=2.2291507720947266
I0128 08:53:54.828182 140026050483968 logging_writer.py:48] [15900] global_step=15900, grad_norm=7.4386396408081055, loss=2.2435595989227295
I0128 08:54:28.876281 140026058876672 logging_writer.py:48] [16000] global_step=16000, grad_norm=8.248577117919922, loss=2.2724833488464355
I0128 08:55:02.915689 140026050483968 logging_writer.py:48] [16100] global_step=16100, grad_norm=6.3345160484313965, loss=2.230736255645752
I0128 08:55:36.936728 140026058876672 logging_writer.py:48] [16200] global_step=16200, grad_norm=8.125645637512207, loss=2.2784810066223145
I0128 08:56:10.956933 140026050483968 logging_writer.py:48] [16300] global_step=16300, grad_norm=5.49894905090332, loss=2.231389284133911
I0128 08:56:44.995958 140026058876672 logging_writer.py:48] [16400] global_step=16400, grad_norm=4.569432258605957, loss=2.3194146156311035
I0128 08:57:08.723045 140187804313408 spec.py:321] Evaluating on the training split.
I0128 08:57:14.959372 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 08:57:23.763668 140187804313408 spec.py:349] Evaluating on the test split.
I0128 08:57:26.410621 140187804313408 submission_runner.py:408] Time since start: 5857.61s, 	Step: 16471, 	{'train/accuracy': 0.6073421239852905, 'train/loss': 1.608778953552246, 'validation/accuracy': 0.5467000007629395, 'validation/loss': 1.9259779453277588, 'validation/num_examples': 50000, 'test/accuracy': 0.43570002913475037, 'test/loss': 2.652667284011841, 'test/num_examples': 10000, 'score': 5641.149353981018, 'total_duration': 5857.610563755035, 'accumulated_submission_time': 5641.149353981018, 'accumulated_eval_time': 215.52660512924194, 'accumulated_logging_time': 0.33754658699035645}
I0128 08:57:26.434840 140026159523584 logging_writer.py:48] [16471] accumulated_eval_time=215.526605, accumulated_logging_time=0.337547, accumulated_submission_time=5641.149354, global_step=16471, preemption_count=0, score=5641.149354, test/accuracy=0.435700, test/loss=2.652667, test/num_examples=10000, total_duration=5857.610564, train/accuracy=0.607342, train/loss=1.608779, validation/accuracy=0.546700, validation/loss=1.925978, validation/num_examples=50000
I0128 08:57:36.636663 140026167916288 logging_writer.py:48] [16500] global_step=16500, grad_norm=5.332332611083984, loss=2.1251814365386963
I0128 08:58:10.597007 140026159523584 logging_writer.py:48] [16600] global_step=16600, grad_norm=6.47105073928833, loss=2.2504963874816895
I0128 08:58:44.566712 140026167916288 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.244833469390869, loss=2.1293509006500244
I0128 08:59:18.582619 140026159523584 logging_writer.py:48] [16800] global_step=16800, grad_norm=5.517605304718018, loss=2.194113254547119
I0128 08:59:52.573997 140026167916288 logging_writer.py:48] [16900] global_step=16900, grad_norm=8.50900936126709, loss=2.2088425159454346
I0128 09:00:26.582646 140026159523584 logging_writer.py:48] [17000] global_step=17000, grad_norm=6.204242706298828, loss=2.272500514984131
I0128 09:01:00.565294 140026167916288 logging_writer.py:48] [17100] global_step=17100, grad_norm=5.643758773803711, loss=2.226642370223999
I0128 09:01:34.567793 140026159523584 logging_writer.py:48] [17200] global_step=17200, grad_norm=4.566801071166992, loss=2.126309871673584
I0128 09:02:08.575460 140026167916288 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.06111478805542, loss=2.0977089405059814
I0128 09:02:42.587298 140026159523584 logging_writer.py:48] [17400] global_step=17400, grad_norm=5.189391136169434, loss=2.2831289768218994
I0128 09:03:16.646079 140026167916288 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.2713189125061035, loss=2.0683767795562744
I0128 09:03:50.641216 140026159523584 logging_writer.py:48] [17600] global_step=17600, grad_norm=5.593729496002197, loss=2.0637924671173096
I0128 09:04:24.624499 140026167916288 logging_writer.py:48] [17700] global_step=17700, grad_norm=4.9163312911987305, loss=2.1443915367126465
I0128 09:04:58.624533 140026159523584 logging_writer.py:48] [17800] global_step=17800, grad_norm=6.44918155670166, loss=2.1574292182922363
I0128 09:05:32.601208 140026167916288 logging_writer.py:48] [17900] global_step=17900, grad_norm=5.854573726654053, loss=2.078040361404419
I0128 09:05:56.533022 140187804313408 spec.py:321] Evaluating on the training split.
I0128 09:06:02.912438 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 09:06:11.793728 140187804313408 spec.py:349] Evaluating on the test split.
I0128 09:06:14.464572 140187804313408 submission_runner.py:408] Time since start: 6385.66s, 	Step: 17972, 	{'train/accuracy': 0.6112284660339355, 'train/loss': 1.6073660850524902, 'validation/accuracy': 0.5543400049209595, 'validation/loss': 1.8950927257537842, 'validation/num_examples': 50000, 'test/accuracy': 0.43220001459121704, 'test/loss': 2.6180338859558105, 'test/num_examples': 10000, 'score': 6151.183410406113, 'total_duration': 6385.664488315582, 'accumulated_submission_time': 6151.183410406113, 'accumulated_eval_time': 233.45811223983765, 'accumulated_logging_time': 0.37097597122192383}
I0128 09:06:14.496031 140026058876672 logging_writer.py:48] [17972] accumulated_eval_time=233.458112, accumulated_logging_time=0.370976, accumulated_submission_time=6151.183410, global_step=17972, preemption_count=0, score=6151.183410, test/accuracy=0.432200, test/loss=2.618034, test/num_examples=10000, total_duration=6385.664488, train/accuracy=0.611228, train/loss=1.607366, validation/accuracy=0.554340, validation/loss=1.895093, validation/num_examples=50000
I0128 09:06:24.350181 140026067269376 logging_writer.py:48] [18000] global_step=18000, grad_norm=8.330802917480469, loss=2.164374589920044
I0128 09:06:58.277171 140026058876672 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.9394023418426514, loss=2.2602062225341797
I0128 09:07:32.262839 140026067269376 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.962900638580322, loss=2.1751132011413574
I0128 09:08:06.239343 140026058876672 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.118767261505127, loss=2.1921324729919434
I0128 09:08:40.248643 140026067269376 logging_writer.py:48] [18400] global_step=18400, grad_norm=4.796450614929199, loss=2.1183454990386963
I0128 09:09:14.217453 140026058876672 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.809639930725098, loss=2.1934547424316406
I0128 09:09:48.275363 140026067269376 logging_writer.py:48] [18600] global_step=18600, grad_norm=7.439981460571289, loss=2.11834716796875
I0128 09:10:22.285631 140026058876672 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.3445024490356445, loss=2.325054407119751
I0128 09:10:56.271940 140026067269376 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.312057971954346, loss=2.084153652191162
I0128 09:11:30.284227 140026058876672 logging_writer.py:48] [18900] global_step=18900, grad_norm=5.789975166320801, loss=2.163820743560791
I0128 09:12:04.260562 140026067269376 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.28083610534668, loss=2.143436908721924
I0128 09:12:38.286158 140026058876672 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.9556288719177246, loss=2.1435706615448
I0128 09:13:12.293025 140026067269376 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.8497931957244873, loss=2.181881904602051
I0128 09:13:46.279349 140026058876672 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.8054075241088867, loss=2.228595495223999
I0128 09:14:20.305220 140026067269376 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.0574045181274414, loss=2.1608004570007324
I0128 09:14:44.577116 140187804313408 spec.py:321] Evaluating on the training split.
I0128 09:14:50.856220 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 09:14:59.743362 140187804313408 spec.py:349] Evaluating on the test split.
I0128 09:15:02.375599 140187804313408 submission_runner.py:408] Time since start: 6913.58s, 	Step: 19473, 	{'train/accuracy': 0.6114277839660645, 'train/loss': 1.5878651142120361, 'validation/accuracy': 0.5617799758911133, 'validation/loss': 1.8461424112319946, 'validation/num_examples': 50000, 'test/accuracy': 0.4406000077724457, 'test/loss': 2.583203077316284, 'test/num_examples': 10000, 'score': 6661.199192047119, 'total_duration': 6913.575413227081, 'accumulated_submission_time': 6661.199192047119, 'accumulated_eval_time': 251.256432056427, 'accumulated_logging_time': 0.4122631549835205}
I0128 09:15:02.405599 140026058876672 logging_writer.py:48] [19473] accumulated_eval_time=251.256432, accumulated_logging_time=0.412263, accumulated_submission_time=6661.199192, global_step=19473, preemption_count=0, score=6661.199192, test/accuracy=0.440600, test/loss=2.583203, test/num_examples=10000, total_duration=6913.575413, train/accuracy=0.611428, train/loss=1.587865, validation/accuracy=0.561780, validation/loss=1.846142, validation/num_examples=50000
I0128 09:15:11.942227 140026159523584 logging_writer.py:48] [19500] global_step=19500, grad_norm=6.180380344390869, loss=2.265342950820923
I0128 09:15:45.852808 140026058876672 logging_writer.py:48] [19600] global_step=19600, grad_norm=4.488199234008789, loss=2.2451415061950684
I0128 09:16:19.984878 140026159523584 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.484426498413086, loss=2.077299118041992
I0128 09:16:53.991182 140026058876672 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.9265646934509277, loss=2.0499684810638428
I0128 09:17:27.971314 140026159523584 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.093631744384766, loss=2.168506622314453
I0128 09:18:01.967984 140026058876672 logging_writer.py:48] [20000] global_step=20000, grad_norm=4.839858055114746, loss=2.0561859607696533
I0128 09:18:35.991804 140026159523584 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.442945957183838, loss=2.074253559112549
I0128 09:19:09.971274 140026058876672 logging_writer.py:48] [20200] global_step=20200, grad_norm=5.004359722137451, loss=2.2609987258911133
I0128 09:19:43.948350 140026159523584 logging_writer.py:48] [20300] global_step=20300, grad_norm=4.527859210968018, loss=2.1658716201782227
I0128 09:20:17.932621 140026058876672 logging_writer.py:48] [20400] global_step=20400, grad_norm=4.0474419593811035, loss=2.052719831466675
I0128 09:20:51.940514 140026159523584 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.141545295715332, loss=2.1847493648529053
I0128 09:21:25.915527 140026058876672 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.2201173305511475, loss=2.0522689819335938
I0128 09:21:59.881264 140026159523584 logging_writer.py:48] [20700] global_step=20700, grad_norm=8.45151138305664, loss=2.0242910385131836
I0128 09:22:33.931876 140026058876672 logging_writer.py:48] [20800] global_step=20800, grad_norm=6.373571872711182, loss=2.096163034439087
I0128 09:23:07.921305 140026159523584 logging_writer.py:48] [20900] global_step=20900, grad_norm=4.279597759246826, loss=2.001854181289673
I0128 09:23:32.533682 140187804313408 spec.py:321] Evaluating on the training split.
I0128 09:23:38.766640 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 09:23:47.570249 140187804313408 spec.py:349] Evaluating on the test split.
I0128 09:23:50.200094 140187804313408 submission_runner.py:408] Time since start: 7441.40s, 	Step: 20974, 	{'train/accuracy': 0.6094148755073547, 'train/loss': 1.5837292671203613, 'validation/accuracy': 0.5648800134658813, 'validation/loss': 1.8347382545471191, 'validation/num_examples': 50000, 'test/accuracy': 0.4406000077724457, 'test/loss': 2.576707363128662, 'test/num_examples': 10000, 'score': 7171.261469125748, 'total_duration': 7441.400032997131, 'accumulated_submission_time': 7171.261469125748, 'accumulated_eval_time': 268.92280554771423, 'accumulated_logging_time': 0.4528038501739502}
I0128 09:23:50.221736 140026058876672 logging_writer.py:48] [20974] accumulated_eval_time=268.922806, accumulated_logging_time=0.452804, accumulated_submission_time=7171.261469, global_step=20974, preemption_count=0, score=7171.261469, test/accuracy=0.440600, test/loss=2.576707, test/num_examples=10000, total_duration=7441.400033, train/accuracy=0.609415, train/loss=1.583729, validation/accuracy=0.564880, validation/loss=1.834738, validation/num_examples=50000
I0128 09:23:59.357085 140026067269376 logging_writer.py:48] [21000] global_step=21000, grad_norm=4.606375694274902, loss=2.197077751159668
I0128 09:24:33.272011 140026058876672 logging_writer.py:48] [21100] global_step=21100, grad_norm=4.2660231590271, loss=2.1063625812530518
I0128 09:25:07.255733 140026067269376 logging_writer.py:48] [21200] global_step=21200, grad_norm=4.186800479888916, loss=2.1457858085632324
I0128 09:25:41.262576 140026058876672 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.519404411315918, loss=2.104973316192627
I0128 09:26:15.233250 140026067269376 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.0554065704345703, loss=2.0972347259521484
I0128 09:26:49.219482 140026058876672 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.985653877258301, loss=2.1468865871429443
I0128 09:27:23.211051 140026067269376 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.5068135261535645, loss=2.086974859237671
I0128 09:27:57.219559 140026058876672 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.254471778869629, loss=2.090744972229004
I0128 09:28:31.181076 140026067269376 logging_writer.py:48] [21800] global_step=21800, grad_norm=4.039340019226074, loss=2.0689213275909424
I0128 09:29:05.178540 140026058876672 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.4275314807891846, loss=2.1184744834899902
I0128 09:29:39.161959 140026067269376 logging_writer.py:48] [22000] global_step=22000, grad_norm=5.029726028442383, loss=2.0265371799468994
I0128 09:30:13.151953 140026058876672 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.6849851608276367, loss=2.0892128944396973
I0128 09:30:47.128369 140026067269376 logging_writer.py:48] [22200] global_step=22200, grad_norm=4.245305061340332, loss=2.0999538898468018
I0128 09:31:21.121255 140026058876672 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.5621414184570312, loss=2.0143346786499023
I0128 09:31:55.116833 140026067269376 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.5653553009033203, loss=2.1071956157684326
I0128 09:32:20.397650 140187804313408 spec.py:321] Evaluating on the training split.
I0128 09:32:26.639098 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 09:32:35.640727 140187804313408 spec.py:349] Evaluating on the test split.
I0128 09:32:38.303471 140187804313408 submission_runner.py:408] Time since start: 7969.50s, 	Step: 22476, 	{'train/accuracy': 0.6075215339660645, 'train/loss': 1.6099942922592163, 'validation/accuracy': 0.5667600035667419, 'validation/loss': 1.828776240348816, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.5712270736694336, 'test/num_examples': 10000, 'score': 7681.3733949661255, 'total_duration': 7969.5034058094025, 'accumulated_submission_time': 7681.3733949661255, 'accumulated_eval_time': 286.82858443260193, 'accumulated_logging_time': 0.48407411575317383}
I0128 09:32:38.326865 140026151130880 logging_writer.py:48] [22476] accumulated_eval_time=286.828584, accumulated_logging_time=0.484074, accumulated_submission_time=7681.373395, global_step=22476, preemption_count=0, score=7681.373395, test/accuracy=0.443200, test/loss=2.571227, test/num_examples=10000, total_duration=7969.503406, train/accuracy=0.607522, train/loss=1.609994, validation/accuracy=0.566760, validation/loss=1.828776, validation/num_examples=50000
I0128 09:32:46.830555 140026159523584 logging_writer.py:48] [22500] global_step=22500, grad_norm=4.022567272186279, loss=1.9183253049850464
I0128 09:33:20.744137 140026151130880 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.0581488609313965, loss=2.0738935470581055
I0128 09:33:54.706514 140026159523584 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.253565549850464, loss=2.1640634536743164
I0128 09:34:28.689831 140026151130880 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.7346527576446533, loss=2.125576972961426
I0128 09:35:02.684111 140026159523584 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.415893316268921, loss=1.974725365638733
I0128 09:35:36.754150 140026151130880 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.454010486602783, loss=2.1173808574676514
I0128 09:36:10.724078 140026159523584 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.0372135639190674, loss=2.009854793548584
I0128 09:36:44.713314 140026151130880 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.394456624984741, loss=2.0240681171417236
I0128 09:37:18.683000 140026159523584 logging_writer.py:48] [23300] global_step=23300, grad_norm=5.231933116912842, loss=1.9887521266937256
I0128 09:37:52.689075 140026151130880 logging_writer.py:48] [23400] global_step=23400, grad_norm=4.421061038970947, loss=2.043733835220337
I0128 09:38:26.673310 140026159523584 logging_writer.py:48] [23500] global_step=23500, grad_norm=4.532223701477051, loss=2.0697739124298096
I0128 09:39:00.677094 140026151130880 logging_writer.py:48] [23600] global_step=23600, grad_norm=4.777667045593262, loss=2.127824306488037
I0128 09:39:34.656295 140026159523584 logging_writer.py:48] [23700] global_step=23700, grad_norm=4.102951526641846, loss=2.1057517528533936
I0128 09:40:08.646222 140026151130880 logging_writer.py:48] [23800] global_step=23800, grad_norm=4.317110538482666, loss=2.145416736602783
I0128 09:40:42.605518 140026159523584 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.118386745452881, loss=2.0387260913848877
I0128 09:41:08.552429 140187804313408 spec.py:321] Evaluating on the training split.
I0128 09:41:14.773941 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 09:41:23.550881 140187804313408 spec.py:349] Evaluating on the test split.
I0128 09:41:26.171044 140187804313408 submission_runner.py:408] Time since start: 8497.37s, 	Step: 23978, 	{'train/accuracy': 0.6109893321990967, 'train/loss': 1.584595799446106, 'validation/accuracy': 0.5736599564552307, 'validation/loss': 1.797381043434143, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.56406831741333, 'test/num_examples': 10000, 'score': 8191.5354125499725, 'total_duration': 8497.370985507965, 'accumulated_submission_time': 8191.5354125499725, 'accumulated_eval_time': 304.44716572761536, 'accumulated_logging_time': 0.5166065692901611}
I0128 09:41:26.193450 140026058876672 logging_writer.py:48] [23978] accumulated_eval_time=304.447166, accumulated_logging_time=0.516607, accumulated_submission_time=8191.535413, global_step=23978, preemption_count=0, score=8191.535413, test/accuracy=0.443500, test/loss=2.564068, test/num_examples=10000, total_duration=8497.370986, train/accuracy=0.610989, train/loss=1.584596, validation/accuracy=0.573660, validation/loss=1.797381, validation/num_examples=50000
I0128 09:41:34.081422 140026067269376 logging_writer.py:48] [24000] global_step=24000, grad_norm=4.15254545211792, loss=2.0810353755950928
I0128 09:42:08.011452 140026058876672 logging_writer.py:48] [24100] global_step=24100, grad_norm=4.329690456390381, loss=2.013256549835205
I0128 09:42:41.971801 140026067269376 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.3741402626037598, loss=2.0407934188842773
I0128 09:43:15.936590 140026058876672 logging_writer.py:48] [24300] global_step=24300, grad_norm=4.078346252441406, loss=2.1071724891662598
I0128 09:43:49.914085 140026067269376 logging_writer.py:48] [24400] global_step=24400, grad_norm=4.951961040496826, loss=2.1313586235046387
I0128 09:44:23.889829 140026058876672 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.173229455947876, loss=2.141244888305664
I0128 09:44:57.896100 140026067269376 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.553365707397461, loss=2.040578603744507
I0128 09:45:31.874810 140026058876672 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.1932432651519775, loss=1.9662909507751465
I0128 09:46:05.877361 140026067269376 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.471855401992798, loss=2.0250372886657715
I0128 09:46:39.880050 140026058876672 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.3981478214263916, loss=2.039612293243408
I0128 09:47:13.867712 140026067269376 logging_writer.py:48] [25000] global_step=25000, grad_norm=4.502204418182373, loss=2.079477548599243
I0128 09:47:47.870719 140026058876672 logging_writer.py:48] [25100] global_step=25100, grad_norm=4.016444683074951, loss=2.1291074752807617
I0128 09:48:21.924552 140026067269376 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.813826560974121, loss=2.1876220703125
I0128 09:48:55.899313 140026058876672 logging_writer.py:48] [25300] global_step=25300, grad_norm=4.303244113922119, loss=2.1586532592773438
I0128 09:49:29.872222 140026067269376 logging_writer.py:48] [25400] global_step=25400, grad_norm=4.202768325805664, loss=2.049661636352539
I0128 09:49:56.174152 140187804313408 spec.py:321] Evaluating on the training split.
I0128 09:50:02.591087 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 09:50:11.825749 140187804313408 spec.py:349] Evaluating on the test split.
I0128 09:50:14.365618 140187804313408 submission_runner.py:408] Time since start: 9025.57s, 	Step: 25479, 	{'train/accuracy': 0.6316764950752258, 'train/loss': 1.4977443218231201, 'validation/accuracy': 0.5838800072669983, 'validation/loss': 1.7310395240783691, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.4388608932495117, 'test/num_examples': 10000, 'score': 8701.451757669449, 'total_duration': 9025.565544128418, 'accumulated_submission_time': 8701.451757669449, 'accumulated_eval_time': 322.6385922431946, 'accumulated_logging_time': 0.5493104457855225}
I0128 09:50:14.387702 140026058876672 logging_writer.py:48] [25479] accumulated_eval_time=322.638592, accumulated_logging_time=0.549310, accumulated_submission_time=8701.451758, global_step=25479, preemption_count=0, score=8701.451758, test/accuracy=0.468900, test/loss=2.438861, test/num_examples=10000, total_duration=9025.565544, train/accuracy=0.631676, train/loss=1.497744, validation/accuracy=0.583880, validation/loss=1.731040, validation/num_examples=50000
I0128 09:50:21.857152 140026067269376 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.8162753582000732, loss=2.0801236629486084
I0128 09:50:55.770020 140026058876672 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.6592557430267334, loss=1.9956260919570923
I0128 09:51:29.708982 140026067269376 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.8203823566436768, loss=2.0642409324645996
I0128 09:52:03.687831 140026058876672 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.1969380378723145, loss=1.9530971050262451
I0128 09:52:37.647036 140026067269376 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.6944491863250732, loss=1.9882370233535767
I0128 09:53:11.576915 140026058876672 logging_writer.py:48] [26000] global_step=26000, grad_norm=4.3919878005981445, loss=2.199685573577881
I0128 09:53:45.543787 140026067269376 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.3263301849365234, loss=1.9582362174987793
I0128 09:54:19.544915 140026058876672 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.8677570819854736, loss=2.012251853942871
I0128 09:54:53.524490 140026067269376 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.69193172454834, loss=2.1383888721466064
I0128 09:55:27.516243 140026058876672 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.869532585144043, loss=1.9807443618774414
I0128 09:56:01.431802 140026067269376 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.7735393047332764, loss=2.09092116355896
I0128 09:56:35.382951 140026058876672 logging_writer.py:48] [26600] global_step=26600, grad_norm=4.031736850738525, loss=2.0229198932647705
I0128 09:57:09.350818 140026067269376 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.246037721633911, loss=2.055201292037964
I0128 09:57:43.318900 140026058876672 logging_writer.py:48] [26800] global_step=26800, grad_norm=4.465449333190918, loss=1.9902726411819458
I0128 09:58:17.303969 140026067269376 logging_writer.py:48] [26900] global_step=26900, grad_norm=4.150440692901611, loss=1.8229526281356812
I0128 09:58:44.582547 140187804313408 spec.py:321] Evaluating on the training split.
I0128 09:58:50.899327 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 09:58:59.710152 140187804313408 spec.py:349] Evaluating on the test split.
I0128 09:59:02.336823 140187804313408 submission_runner.py:408] Time since start: 9553.54s, 	Step: 26982, 	{'train/accuracy': 0.6469228267669678, 'train/loss': 1.421025037765503, 'validation/accuracy': 0.5805799961090088, 'validation/loss': 1.7613977193832397, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.5006914138793945, 'test/num_examples': 10000, 'score': 9211.581802606583, 'total_duration': 9553.536763191223, 'accumulated_submission_time': 9211.581802606583, 'accumulated_eval_time': 340.3928325176239, 'accumulated_logging_time': 0.5819401741027832}
I0128 09:59:02.359446 140026042091264 logging_writer.py:48] [26982] accumulated_eval_time=340.392833, accumulated_logging_time=0.581940, accumulated_submission_time=9211.581803, global_step=26982, preemption_count=0, score=9211.581803, test/accuracy=0.459300, test/loss=2.500691, test/num_examples=10000, total_duration=9553.536763, train/accuracy=0.646923, train/loss=1.421025, validation/accuracy=0.580580, validation/loss=1.761398, validation/num_examples=50000
I0128 09:59:08.807957 140026050483968 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.5755839347839355, loss=2.071122646331787
I0128 09:59:42.725932 140026042091264 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.8618266582489014, loss=2.010373592376709
I0128 10:00:16.641400 140026050483968 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.814807415008545, loss=2.0707688331604004
I0128 10:00:50.717342 140026042091264 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.8700976371765137, loss=2.030190944671631
I0128 10:01:24.701656 140026050483968 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.4157590866088867, loss=1.9861661195755005
I0128 10:01:58.688603 140026042091264 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.830514907836914, loss=1.969637393951416
I0128 10:02:32.635254 140026050483968 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.6749002933502197, loss=2.010528326034546
I0128 10:03:06.602888 140026042091264 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.243936061859131, loss=1.921858310699463
I0128 10:03:40.591514 140026050483968 logging_writer.py:48] [27800] global_step=27800, grad_norm=4.530088424682617, loss=2.056584119796753
I0128 10:04:14.581225 140026042091264 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.4991071224212646, loss=2.1007156372070312
I0128 10:04:48.577753 140026050483968 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.84722638130188, loss=1.9717493057250977
I0128 10:05:22.553941 140026042091264 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.070619106292725, loss=1.9723484516143799
I0128 10:05:56.529810 140026050483968 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.4722678661346436, loss=2.0829262733459473
I0128 10:06:30.535689 140026042091264 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.3774495124816895, loss=1.933002233505249
I0128 10:07:04.616515 140026050483968 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.661775588989258, loss=1.9325001239776611
I0128 10:07:32.618246 140187804313408 spec.py:321] Evaluating on the training split.
I0128 10:07:38.833086 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 10:07:47.929516 140187804313408 spec.py:349] Evaluating on the test split.
I0128 10:07:50.506494 140187804313408 submission_runner.py:408] Time since start: 10081.71s, 	Step: 28484, 	{'train/accuracy': 0.6459661722183228, 'train/loss': 1.4371527433395386, 'validation/accuracy': 0.5914199948310852, 'validation/loss': 1.6988749504089355, 'validation/num_examples': 50000, 'test/accuracy': 0.46550002694129944, 'test/loss': 2.420314311981201, 'test/num_examples': 10000, 'score': 9721.777070045471, 'total_duration': 10081.70643401146, 'accumulated_submission_time': 9721.777070045471, 'accumulated_eval_time': 358.2810490131378, 'accumulated_logging_time': 0.6139397621154785}
I0128 10:07:50.530211 140026159523584 logging_writer.py:48] [28484] accumulated_eval_time=358.281049, accumulated_logging_time=0.613940, accumulated_submission_time=9721.777070, global_step=28484, preemption_count=0, score=9721.777070, test/accuracy=0.465500, test/loss=2.420314, test/num_examples=10000, total_duration=10081.706434, train/accuracy=0.645966, train/loss=1.437153, validation/accuracy=0.591420, validation/loss=1.698875, validation/num_examples=50000
I0128 10:07:56.295529 140026167916288 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.8321497440338135, loss=1.8385576009750366
I0128 10:08:30.171648 140026159523584 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.707582712173462, loss=2.0707905292510986
I0128 10:09:04.066008 140026167916288 logging_writer.py:48] [28700] global_step=28700, grad_norm=4.191792964935303, loss=2.0442991256713867
I0128 10:09:38.012623 140026159523584 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.582609176635742, loss=1.9284937381744385
I0128 10:10:11.985633 140026167916288 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.3245980739593506, loss=2.0168604850769043
I0128 10:10:45.930300 140026159523584 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.3512911796569824, loss=1.9009394645690918
I0128 10:11:19.871018 140026167916288 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.976372480392456, loss=1.9510352611541748
I0128 10:11:53.825444 140026159523584 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.0638084411621094, loss=1.9514362812042236
I0128 10:12:27.765868 140026167916288 logging_writer.py:48] [29300] global_step=29300, grad_norm=4.1130170822143555, loss=2.0844273567199707
I0128 10:13:01.718161 140026159523584 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.904409170150757, loss=2.015136480331421
I0128 10:13:35.761530 140026167916288 logging_writer.py:48] [29500] global_step=29500, grad_norm=4.209317207336426, loss=2.0119128227233887
I0128 10:14:09.709361 140026159523584 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.4653682708740234, loss=2.013021230697632
I0128 10:14:43.658490 140026167916288 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.5196008682250977, loss=1.8804906606674194
I0128 10:15:17.598819 140026159523584 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.766718626022339, loss=1.9660542011260986
I0128 10:15:51.535266 140026167916288 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.923736333847046, loss=2.0794553756713867
I0128 10:16:20.848110 140187804313408 spec.py:321] Evaluating on the training split.
I0128 10:16:27.084256 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 10:16:35.914216 140187804313408 spec.py:349] Evaluating on the test split.
I0128 10:16:38.587904 140187804313408 submission_runner.py:408] Time since start: 10609.79s, 	Step: 29988, 	{'train/accuracy': 0.6504305005073547, 'train/loss': 1.4086905717849731, 'validation/accuracy': 0.5985400080680847, 'validation/loss': 1.6694458723068237, 'validation/num_examples': 50000, 'test/accuracy': 0.4718000292778015, 'test/loss': 2.4125912189483643, 'test/num_examples': 10000, 'score': 10232.031190395355, 'total_duration': 10609.787842988968, 'accumulated_submission_time': 10232.031190395355, 'accumulated_eval_time': 376.0208065509796, 'accumulated_logging_time': 0.6470503807067871}
I0128 10:16:38.611465 140026058876672 logging_writer.py:48] [29988] accumulated_eval_time=376.020807, accumulated_logging_time=0.647050, accumulated_submission_time=10232.031190, global_step=29988, preemption_count=0, score=10232.031190, test/accuracy=0.471800, test/loss=2.412591, test/num_examples=10000, total_duration=10609.787843, train/accuracy=0.650431, train/loss=1.408691, validation/accuracy=0.598540, validation/loss=1.669446, validation/num_examples=50000
I0128 10:16:43.006835 140026067269376 logging_writer.py:48] [30000] global_step=30000, grad_norm=4.095484733581543, loss=2.0941286087036133
I0128 10:17:16.907777 140026058876672 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.2283377647399902, loss=1.9587340354919434
I0128 10:17:50.818034 140026067269376 logging_writer.py:48] [30200] global_step=30200, grad_norm=4.371960639953613, loss=2.0445306301116943
I0128 10:18:24.787004 140026058876672 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.186182975769043, loss=2.0017387866973877
I0128 10:18:58.751543 140026067269376 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.363173246383667, loss=1.9193329811096191
I0128 10:19:32.724651 140026058876672 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.201350688934326, loss=1.9270169734954834
I0128 10:20:06.761113 140026067269376 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.2790300846099854, loss=1.9893567562103271
I0128 10:20:40.717851 140026058876672 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.4623196125030518, loss=1.9521516561508179
I0128 10:21:14.707783 140026067269376 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.8318068981170654, loss=1.9753414392471313
I0128 10:21:48.663367 140026058876672 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.777696132659912, loss=2.0615620613098145
I0128 10:22:22.589039 140026067269376 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.817704200744629, loss=1.956371545791626
I0128 10:22:56.548080 140026058876672 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.6613059043884277, loss=1.935919165611267
I0128 10:23:30.539255 140026067269376 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.724095582962036, loss=1.9594955444335938
I0128 10:24:04.506628 140026058876672 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.652099132537842, loss=1.927498459815979
I0128 10:24:38.402879 140026067269376 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.7090258598327637, loss=2.020261764526367
I0128 10:25:08.789049 140187804313408 spec.py:321] Evaluating on the training split.
I0128 10:25:15.036223 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 10:25:23.837082 140187804313408 spec.py:349] Evaluating on the test split.
I0128 10:25:26.451865 140187804313408 submission_runner.py:408] Time since start: 11137.65s, 	Step: 31491, 	{'train/accuracy': 0.6327327489852905, 'train/loss': 1.4778612852096558, 'validation/accuracy': 0.5869199633598328, 'validation/loss': 1.7286511659622192, 'validation/num_examples': 50000, 'test/accuracy': 0.4706000089645386, 'test/loss': 2.442723035812378, 'test/num_examples': 10000, 'score': 10742.144251823425, 'total_duration': 11137.651804208755, 'accumulated_submission_time': 10742.144251823425, 'accumulated_eval_time': 393.6835870742798, 'accumulated_logging_time': 0.680239200592041}
I0128 10:25:26.475559 140026151130880 logging_writer.py:48] [31491] accumulated_eval_time=393.683587, accumulated_logging_time=0.680239, accumulated_submission_time=10742.144252, global_step=31491, preemption_count=0, score=10742.144252, test/accuracy=0.470600, test/loss=2.442723, test/num_examples=10000, total_duration=11137.651804, train/accuracy=0.632733, train/loss=1.477861, validation/accuracy=0.586920, validation/loss=1.728651, validation/num_examples=50000
I0128 10:25:29.894480 140026159523584 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.852543830871582, loss=1.9941742420196533
I0128 10:26:03.788184 140026151130880 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.6527621746063232, loss=1.8519093990325928
I0128 10:26:37.755833 140026159523584 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.334530830383301, loss=1.9254095554351807
I0128 10:27:11.661676 140026151130880 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.65181303024292, loss=2.002206563949585
I0128 10:27:45.623740 140026159523584 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.3118669986724854, loss=1.9548542499542236
I0128 10:28:19.539844 140026151130880 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.9247612953186035, loss=1.8567395210266113
I0128 10:28:53.508360 140026159523584 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.222787618637085, loss=2.0132954120635986
I0128 10:29:27.439711 140026151130880 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.8497610092163086, loss=2.042239189147949
I0128 10:30:01.412935 140026159523584 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.316527843475342, loss=1.8070480823516846
I0128 10:30:35.369085 140026151130880 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.9825737476348877, loss=2.042867422103882
I0128 10:31:09.337393 140026159523584 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.9937281608581543, loss=1.9376206398010254
I0128 10:31:43.235526 140026151130880 logging_writer.py:48] [32600] global_step=32600, grad_norm=4.131945610046387, loss=1.9896612167358398
I0128 10:32:17.170951 140026159523584 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.2351653575897217, loss=1.997847557067871
I0128 10:32:51.327593 140026151130880 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.3225648403167725, loss=1.9452883005142212
I0128 10:33:25.262773 140026159523584 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.982177734375, loss=1.9056434631347656
I0128 10:33:56.631868 140187804313408 spec.py:321] Evaluating on the training split.
I0128 10:34:03.016219 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 10:34:12.127096 140187804313408 spec.py:349] Evaluating on the test split.
I0128 10:34:14.773007 140187804313408 submission_runner.py:408] Time since start: 11665.97s, 	Step: 32994, 	{'train/accuracy': 0.6381337642669678, 'train/loss': 1.4649090766906738, 'validation/accuracy': 0.5950799584388733, 'validation/loss': 1.6732388734817505, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.3845551013946533, 'test/num_examples': 10000, 'score': 11252.236163139343, 'total_duration': 11665.972929954529, 'accumulated_submission_time': 11252.236163139343, 'accumulated_eval_time': 411.8246719837189, 'accumulated_logging_time': 0.7132534980773926}
I0128 10:34:14.806484 140026058876672 logging_writer.py:48] [32994] accumulated_eval_time=411.824672, accumulated_logging_time=0.713253, accumulated_submission_time=11252.236163, global_step=32994, preemption_count=0, score=11252.236163, test/accuracy=0.477800, test/loss=2.384555, test/num_examples=10000, total_duration=11665.972930, train/accuracy=0.638134, train/loss=1.464909, validation/accuracy=0.595080, validation/loss=1.673239, validation/num_examples=50000
I0128 10:34:17.179202 140026067269376 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.2447659969329834, loss=1.8939002752304077
I0128 10:34:51.033627 140026058876672 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.647296905517578, loss=1.985508680343628
I0128 10:35:24.953679 140026067269376 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.283189535140991, loss=2.008086681365967
I0128 10:35:58.909863 140026058876672 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.744004964828491, loss=1.9088765382766724
I0128 10:36:32.844438 140026067269376 logging_writer.py:48] [33400] global_step=33400, grad_norm=4.266300678253174, loss=1.9978480339050293
I0128 10:37:06.799235 140026058876672 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.781299352645874, loss=1.9889500141143799
I0128 10:37:40.762804 140026067269376 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.5141499042510986, loss=2.156789779663086
I0128 10:38:14.706310 140026058876672 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.4006612300872803, loss=1.7898292541503906
I0128 10:38:48.655811 140026067269376 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.178851366043091, loss=2.0141680240631104
I0128 10:39:22.728247 140026058876672 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.00592041015625, loss=1.8528075218200684
I0128 10:39:56.687863 140026067269376 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.3896822929382324, loss=2.021329164505005
I0128 10:40:30.637118 140026058876672 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.21865177154541, loss=1.9507896900177002
I0128 10:41:04.594985 140026067269376 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.919926881790161, loss=1.7633984088897705
I0128 10:41:38.567191 140026058876672 logging_writer.py:48] [34300] global_step=34300, grad_norm=4.190971851348877, loss=2.0438694953918457
I0128 10:42:12.537838 140026067269376 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.605304002761841, loss=1.9020445346832275
I0128 10:42:44.951526 140187804313408 spec.py:321] Evaluating on the training split.
I0128 10:42:51.162375 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 10:42:59.993506 140187804313408 spec.py:349] Evaluating on the test split.
I0128 10:43:02.609700 140187804313408 submission_runner.py:408] Time since start: 12193.81s, 	Step: 34497, 	{'train/accuracy': 0.647480845451355, 'train/loss': 1.4073861837387085, 'validation/accuracy': 0.6032800078392029, 'validation/loss': 1.6388838291168213, 'validation/num_examples': 50000, 'test/accuracy': 0.4724000096321106, 'test/loss': 2.404728889465332, 'test/num_examples': 10000, 'score': 11762.316961288452, 'total_duration': 12193.809639453888, 'accumulated_submission_time': 11762.316961288452, 'accumulated_eval_time': 429.4828112125397, 'accumulated_logging_time': 0.7562506198883057}
I0128 10:43:02.635305 140026159523584 logging_writer.py:48] [34497] accumulated_eval_time=429.482811, accumulated_logging_time=0.756251, accumulated_submission_time=11762.316961, global_step=34497, preemption_count=0, score=11762.316961, test/accuracy=0.472400, test/loss=2.404729, test/num_examples=10000, total_duration=12193.809639, train/accuracy=0.647481, train/loss=1.407386, validation/accuracy=0.603280, validation/loss=1.638884, validation/num_examples=50000
I0128 10:43:03.998428 140026167916288 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.671555757522583, loss=1.8973041772842407
I0128 10:43:37.883381 140026159523584 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.007490873336792, loss=1.870532512664795
I0128 10:44:11.825497 140026167916288 logging_writer.py:48] [34700] global_step=34700, grad_norm=5.5765461921691895, loss=1.9894020557403564
I0128 10:44:45.822125 140026159523584 logging_writer.py:48] [34800] global_step=34800, grad_norm=4.363908290863037, loss=2.08996844291687
I0128 10:45:19.776632 140026167916288 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.9134597778320312, loss=1.9753607511520386
I0128 10:45:53.786166 140026159523584 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.984081506729126, loss=1.9281096458435059
I0128 10:46:27.730323 140026167916288 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.4915194511413574, loss=1.9167382717132568
I0128 10:47:01.700934 140026159523584 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.2263283729553223, loss=1.974743127822876
I0128 10:47:35.636337 140026167916288 logging_writer.py:48] [35300] global_step=35300, grad_norm=4.190989971160889, loss=1.961518406867981
I0128 10:48:09.585800 140026159523584 logging_writer.py:48] [35400] global_step=35400, grad_norm=4.875026226043701, loss=1.9279850721359253
I0128 10:48:43.553245 140026167916288 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.796597719192505, loss=1.9257564544677734
I0128 10:49:17.500695 140026159523584 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.249988317489624, loss=1.968775987625122
I0128 10:49:51.440160 140026167916288 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.221054792404175, loss=1.823984980583191
I0128 10:50:25.397401 140026159523584 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.2981977462768555, loss=1.9104969501495361
I0128 10:50:59.348570 140026167916288 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.7600178718566895, loss=1.92061448097229
I0128 10:51:32.776499 140187804313408 spec.py:321] Evaluating on the training split.
I0128 10:51:39.166361 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 10:51:48.139412 140187804313408 spec.py:349] Evaluating on the test split.
I0128 10:51:50.619093 140187804313408 submission_runner.py:408] Time since start: 12721.82s, 	Step: 36000, 	{'train/accuracy': 0.6731704473495483, 'train/loss': 1.2901920080184937, 'validation/accuracy': 0.5949400067329407, 'validation/loss': 1.6796997785568237, 'validation/num_examples': 50000, 'test/accuracy': 0.4748000204563141, 'test/loss': 2.377642869949341, 'test/num_examples': 10000, 'score': 12272.395174503326, 'total_duration': 12721.819028377533, 'accumulated_submission_time': 12272.395174503326, 'accumulated_eval_time': 447.32536363601685, 'accumulated_logging_time': 0.7911787033081055}
I0128 10:51:50.643026 140026042091264 logging_writer.py:48] [36000] accumulated_eval_time=447.325364, accumulated_logging_time=0.791179, accumulated_submission_time=12272.395175, global_step=36000, preemption_count=0, score=12272.395175, test/accuracy=0.474800, test/loss=2.377643, test/num_examples=10000, total_duration=12721.819028, train/accuracy=0.673170, train/loss=1.290192, validation/accuracy=0.594940, validation/loss=1.679700, validation/num_examples=50000
I0128 10:51:50.995301 140026050483968 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.573047161102295, loss=1.8525036573410034
I0128 10:52:24.955859 140026042091264 logging_writer.py:48] [36100] global_step=36100, grad_norm=4.143061637878418, loss=1.9977439641952515
I0128 10:52:58.845790 140026050483968 logging_writer.py:48] [36200] global_step=36200, grad_norm=4.218591213226318, loss=1.9629135131835938
I0128 10:53:32.772719 140026042091264 logging_writer.py:48] [36300] global_step=36300, grad_norm=4.634027481079102, loss=1.9938256740570068
I0128 10:54:06.722517 140026050483968 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.4412777423858643, loss=1.9455238580703735
I0128 10:54:40.683445 140026042091264 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.5443434715270996, loss=1.9334962368011475
I0128 10:55:14.602988 140026050483968 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.2580811977386475, loss=2.0058701038360596
I0128 10:55:48.542254 140026042091264 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.671562910079956, loss=1.8526321649551392
I0128 10:56:22.527049 140026050483968 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.4651312828063965, loss=1.8770089149475098
I0128 10:56:56.472629 140026042091264 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.323157548904419, loss=1.9377707242965698
I0128 10:57:30.414060 140026050483968 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.515136480331421, loss=2.005499839782715
I0128 10:58:04.379045 140026042091264 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.01115345954895, loss=2.0454118251800537
I0128 10:58:38.440750 140026050483968 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.603257417678833, loss=1.9264392852783203
I0128 10:59:12.385815 140026042091264 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.8288211822509766, loss=1.9573185443878174
I0128 10:59:46.350022 140026050483968 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.3507795333862305, loss=1.8890211582183838
I0128 11:00:20.788029 140026042091264 logging_writer.py:48] [37500] global_step=37500, grad_norm=4.029605865478516, loss=2.0634238719940186
I0128 11:00:20.796029 140187804313408 spec.py:321] Evaluating on the training split.
I0128 11:00:27.067696 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 11:00:35.969062 140187804313408 spec.py:349] Evaluating on the test split.
I0128 11:00:38.567787 140187804313408 submission_runner.py:408] Time since start: 13249.77s, 	Step: 37501, 	{'train/accuracy': 0.6614716053009033, 'train/loss': 1.3486958742141724, 'validation/accuracy': 0.602679967880249, 'validation/loss': 1.6470184326171875, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.3493621349334717, 'test/num_examples': 10000, 'score': 12782.483393192291, 'total_duration': 13249.76772403717, 'accumulated_submission_time': 12782.483393192291, 'accumulated_eval_time': 465.09705877304077, 'accumulated_logging_time': 0.8253750801086426}
I0128 11:00:38.595717 140026151130880 logging_writer.py:48] [37501] accumulated_eval_time=465.097059, accumulated_logging_time=0.825375, accumulated_submission_time=12782.483393, global_step=37501, preemption_count=0, score=12782.483393, test/accuracy=0.479400, test/loss=2.349362, test/num_examples=10000, total_duration=13249.767724, train/accuracy=0.661472, train/loss=1.348696, validation/accuracy=0.602680, validation/loss=1.647018, validation/num_examples=50000
I0128 11:01:12.434383 140026159523584 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.7267534732818604, loss=1.88173246383667
I0128 11:01:46.333705 140026151130880 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.963963031768799, loss=1.9312180280685425
I0128 11:02:20.268040 140026159523584 logging_writer.py:48] [37800] global_step=37800, grad_norm=4.307525157928467, loss=1.9567958116531372
I0128 11:02:54.211299 140026151130880 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.941563367843628, loss=2.0468013286590576
I0128 11:03:28.162320 140026159523584 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.6577794551849365, loss=2.0305614471435547
I0128 11:04:02.076887 140026151130880 logging_writer.py:48] [38100] global_step=38100, grad_norm=4.189481258392334, loss=2.0215840339660645
I0128 11:04:36.131773 140026159523584 logging_writer.py:48] [38200] global_step=38200, grad_norm=4.490294456481934, loss=1.925058364868164
I0128 11:05:10.035519 140026151130880 logging_writer.py:48] [38300] global_step=38300, grad_norm=4.130841255187988, loss=2.072298526763916
I0128 11:05:43.996788 140026159523584 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.560577869415283, loss=1.9469672441482544
I0128 11:06:17.929046 140026151130880 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.7524619102478027, loss=1.98301362991333
I0128 11:06:51.883384 140026159523584 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.2674450874328613, loss=1.8979816436767578
I0128 11:07:25.803753 140026151130880 logging_writer.py:48] [38700] global_step=38700, grad_norm=4.163270950317383, loss=1.8993674516677856
I0128 11:07:59.723340 140026159523584 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.312253475189209, loss=1.9270219802856445
I0128 11:08:33.631202 140026151130880 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.1114282608032227, loss=1.9079632759094238
I0128 11:09:07.583239 140026159523584 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.8338072299957275, loss=1.8901287317276
I0128 11:09:08.754840 140187804313408 spec.py:321] Evaluating on the training split.
I0128 11:09:14.997895 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 11:09:24.130487 140187804313408 spec.py:349] Evaluating on the test split.
I0128 11:09:26.783303 140187804313408 submission_runner.py:408] Time since start: 13777.98s, 	Step: 39005, 	{'train/accuracy': 0.6461654901504517, 'train/loss': 1.4173647165298462, 'validation/accuracy': 0.5987799763679504, 'validation/loss': 1.6708097457885742, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.3809192180633545, 'test/num_examples': 10000, 'score': 13292.577105998993, 'total_duration': 13777.983241558075, 'accumulated_submission_time': 13292.577105998993, 'accumulated_eval_time': 483.1254951953888, 'accumulated_logging_time': 0.8638536930084229}
I0128 11:09:26.807827 140026058876672 logging_writer.py:48] [39005] accumulated_eval_time=483.125495, accumulated_logging_time=0.863854, accumulated_submission_time=13292.577106, global_step=39005, preemption_count=0, score=13292.577106, test/accuracy=0.479400, test/loss=2.380919, test/num_examples=10000, total_duration=13777.983242, train/accuracy=0.646165, train/loss=1.417365, validation/accuracy=0.598780, validation/loss=1.670810, validation/num_examples=50000
I0128 11:09:59.367619 140026067269376 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.512652635574341, loss=1.9494519233703613
I0128 11:10:33.272621 140026058876672 logging_writer.py:48] [39200] global_step=39200, grad_norm=4.121213436126709, loss=1.8877692222595215
I0128 11:11:07.267604 140026067269376 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.7709765434265137, loss=1.9678102731704712
I0128 11:11:41.239053 140026058876672 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.2364444732666016, loss=2.0040793418884277
I0128 11:12:15.186568 140026067269376 logging_writer.py:48] [39500] global_step=39500, grad_norm=4.005429267883301, loss=1.8984522819519043
I0128 11:12:49.137303 140026058876672 logging_writer.py:48] [39600] global_step=39600, grad_norm=4.239577770233154, loss=1.8979947566986084
I0128 11:13:23.077165 140026067269376 logging_writer.py:48] [39700] global_step=39700, grad_norm=4.2662787437438965, loss=1.9520583152770996
I0128 11:13:57.011628 140026058876672 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.7256221771240234, loss=1.9408267736434937
I0128 11:14:30.962358 140026067269376 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.620234966278076, loss=1.9308733940124512
I0128 11:15:04.918912 140026058876672 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.3339645862579346, loss=1.966009497642517
I0128 11:15:38.869107 140026067269376 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.305065870285034, loss=1.8882813453674316
I0128 11:16:12.796048 140026058876672 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.798158645629883, loss=1.857680082321167
I0128 11:16:46.745050 140026067269376 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.801093578338623, loss=1.9991036653518677
I0128 11:17:20.755388 140026058876672 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.9121856689453125, loss=1.9777172803878784
I0128 11:17:54.709005 140026067269376 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.4700915813446045, loss=1.924822449684143
I0128 11:17:56.894802 140187804313408 spec.py:321] Evaluating on the training split.
I0128 11:18:03.364344 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 11:18:12.228743 140187804313408 spec.py:349] Evaluating on the test split.
I0128 11:18:14.894709 140187804313408 submission_runner.py:408] Time since start: 14306.09s, 	Step: 40508, 	{'train/accuracy': 0.6537986397743225, 'train/loss': 1.3841086626052856, 'validation/accuracy': 0.6029999852180481, 'validation/loss': 1.6390022039413452, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.363870143890381, 'test/num_examples': 10000, 'score': 13802.59973692894, 'total_duration': 14306.094644546509, 'accumulated_submission_time': 13802.59973692894, 'accumulated_eval_time': 501.1253571510315, 'accumulated_logging_time': 0.8978090286254883}
I0128 11:18:14.926896 140026159523584 logging_writer.py:48] [40508] accumulated_eval_time=501.125357, accumulated_logging_time=0.897809, accumulated_submission_time=13802.599737, global_step=40508, preemption_count=0, score=13802.599737, test/accuracy=0.488500, test/loss=2.363870, test/num_examples=10000, total_duration=14306.094645, train/accuracy=0.653799, train/loss=1.384109, validation/accuracy=0.603000, validation/loss=1.639002, validation/num_examples=50000
I0128 11:18:46.457162 140026167916288 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.653541326522827, loss=1.828814148902893
I0128 11:19:20.344242 140026159523584 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.4232451915740967, loss=1.9696650505065918
I0128 11:19:54.293158 140026167916288 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.590388774871826, loss=1.9887230396270752
I0128 11:20:28.261934 140026159523584 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.6192307472229004, loss=1.9248671531677246
I0128 11:21:02.165359 140026167916288 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.5960705280303955, loss=1.9190860986709595
I0128 11:21:36.092332 140026159523584 logging_writer.py:48] [41100] global_step=41100, grad_norm=4.3833794593811035, loss=1.975471019744873
I0128 11:22:10.037066 140026167916288 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.750308036804199, loss=1.797400712966919
I0128 11:22:44.004146 140026159523584 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.6449782848358154, loss=1.9254956245422363
I0128 11:23:17.915888 140026167916288 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.868255376815796, loss=1.9314392805099487
I0128 11:23:51.897131 140026159523584 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.460447311401367, loss=1.9421766996383667
I0128 11:24:25.812122 140026167916288 logging_writer.py:48] [41600] global_step=41600, grad_norm=4.028606414794922, loss=1.8983222246170044
I0128 11:24:59.768109 140026159523584 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.5459938049316406, loss=1.860791802406311
I0128 11:25:33.699088 140026167916288 logging_writer.py:48] [41800] global_step=41800, grad_norm=4.376284122467041, loss=1.9506500959396362
I0128 11:26:07.670392 140026159523584 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.291356086730957, loss=1.9182474613189697
I0128 11:26:41.542892 140026167916288 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.9771838188171387, loss=1.7619620561599731
I0128 11:26:45.088279 140187804313408 spec.py:321] Evaluating on the training split.
I0128 11:26:51.329573 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 11:27:00.147047 140187804313408 spec.py:349] Evaluating on the test split.
I0128 11:27:02.794494 140187804313408 submission_runner.py:408] Time since start: 14833.99s, 	Step: 42012, 	{'train/accuracy': 0.644949734210968, 'train/loss': 1.4297263622283936, 'validation/accuracy': 0.5964999794960022, 'validation/loss': 1.6860605478286743, 'validation/num_examples': 50000, 'test/accuracy': 0.4715000092983246, 'test/loss': 2.4117226600646973, 'test/num_examples': 10000, 'score': 14312.696018218994, 'total_duration': 14833.994425058365, 'accumulated_submission_time': 14312.696018218994, 'accumulated_eval_time': 518.831524848938, 'accumulated_logging_time': 0.941624641418457}
I0128 11:27:02.819186 140026067269376 logging_writer.py:48] [42012] accumulated_eval_time=518.831525, accumulated_logging_time=0.941625, accumulated_submission_time=14312.696018, global_step=42012, preemption_count=0, score=14312.696018, test/accuracy=0.471500, test/loss=2.411723, test/num_examples=10000, total_duration=14833.994425, train/accuracy=0.644950, train/loss=1.429726, validation/accuracy=0.596500, validation/loss=1.686061, validation/num_examples=50000
I0128 11:27:32.985367 140026075662080 logging_writer.py:48] [42100] global_step=42100, grad_norm=4.557736873626709, loss=1.9427998065948486
I0128 11:28:06.901154 140026067269376 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.4375007152557373, loss=1.9297871589660645
I0128 11:28:40.800277 140026075662080 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.193629026412964, loss=1.9092280864715576
I0128 11:29:14.740792 140026067269376 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.957780599594116, loss=1.8531348705291748
I0128 11:29:48.716996 140026075662080 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.5986998081207275, loss=1.9492464065551758
I0128 11:30:22.756278 140026067269376 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.9956440925598145, loss=1.8654899597167969
I0128 11:30:56.713655 140026075662080 logging_writer.py:48] [42700] global_step=42700, grad_norm=5.532180309295654, loss=1.9740630388259888
I0128 11:31:30.639298 140026067269376 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.8674254417419434, loss=1.8342056274414062
I0128 11:32:04.569861 140026075662080 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.5563509464263916, loss=1.7949116230010986
I0128 11:32:38.519516 140026067269376 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.510775089263916, loss=1.84817373752594
I0128 11:33:12.476876 140026075662080 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.6651463508605957, loss=1.900169014930725
I0128 11:33:46.432020 140026067269376 logging_writer.py:48] [43200] global_step=43200, grad_norm=4.074429988861084, loss=1.8420857191085815
I0128 11:34:20.360883 140026075662080 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.6616737842559814, loss=1.850389838218689
I0128 11:34:54.306729 140026067269376 logging_writer.py:48] [43400] global_step=43400, grad_norm=4.238622188568115, loss=1.9723386764526367
I0128 11:35:28.268029 140026075662080 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.530216693878174, loss=1.9729583263397217
I0128 11:35:32.832080 140187804313408 spec.py:321] Evaluating on the training split.
I0128 11:35:39.035301 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 11:35:48.117879 140187804313408 spec.py:349] Evaluating on the test split.
I0128 11:35:50.769043 140187804313408 submission_runner.py:408] Time since start: 15361.97s, 	Step: 43515, 	{'train/accuracy': 0.6421595811843872, 'train/loss': 1.4424054622650146, 'validation/accuracy': 0.5974400043487549, 'validation/loss': 1.672808289527893, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.345468759536743, 'test/num_examples': 10000, 'score': 14822.645218849182, 'total_duration': 15361.96886754036, 'accumulated_submission_time': 14822.645218849182, 'accumulated_eval_time': 536.7683305740356, 'accumulated_logging_time': 0.976036548614502}
I0128 11:35:50.798046 140026058876672 logging_writer.py:48] [43515] accumulated_eval_time=536.768331, accumulated_logging_time=0.976037, accumulated_submission_time=14822.645219, global_step=43515, preemption_count=0, score=14822.645219, test/accuracy=0.484400, test/loss=2.345469, test/num_examples=10000, total_duration=15361.968868, train/accuracy=0.642160, train/loss=1.442405, validation/accuracy=0.597440, validation/loss=1.672808, validation/num_examples=50000
I0128 11:36:19.889032 140026067269376 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.4145889282226562, loss=1.913954257965088
I0128 11:36:53.984309 140026058876672 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.5170180797576904, loss=1.8896291255950928
I0128 11:37:27.878245 140026067269376 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.3497893810272217, loss=1.8842295408248901
I0128 11:38:01.829571 140026058876672 logging_writer.py:48] [43900] global_step=43900, grad_norm=4.044569492340088, loss=1.9634969234466553
I0128 11:38:35.731717 140026067269376 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.6996891498565674, loss=1.9554989337921143
I0128 11:39:09.666905 140026058876672 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.1630992889404297, loss=1.9352765083312988
I0128 11:39:43.627013 140026067269376 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.801814556121826, loss=1.8237206935882568
I0128 11:40:17.513828 140026058876672 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.8940627574920654, loss=1.8307338953018188
I0128 11:40:51.479739 140026067269376 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.745555877685547, loss=1.9603207111358643
I0128 11:41:25.430400 140026058876672 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.7410409450531006, loss=1.9007537364959717
I0128 11:41:59.344396 140026067269376 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.609529733657837, loss=1.907457709312439
I0128 11:42:33.279224 140026058876672 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.2332303524017334, loss=1.8222745656967163
I0128 11:43:07.292613 140026067269376 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.396425247192383, loss=1.8156410455703735
I0128 11:43:41.238820 140026058876672 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.6851723194122314, loss=1.9219661951065063
I0128 11:44:15.133505 140026067269376 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.4985153675079346, loss=1.9332188367843628
I0128 11:44:21.067402 140187804313408 spec.py:321] Evaluating on the training split.
I0128 11:44:27.456336 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 11:44:36.133530 140187804313408 spec.py:349] Evaluating on the test split.
I0128 11:44:38.759706 140187804313408 submission_runner.py:408] Time since start: 15889.96s, 	Step: 45019, 	{'train/accuracy': 0.6957908272743225, 'train/loss': 1.203609585762024, 'validation/accuracy': 0.6162199974060059, 'validation/loss': 1.5929490327835083, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.3410987854003906, 'test/num_examples': 10000, 'score': 15332.848045110703, 'total_duration': 15889.959641456604, 'accumulated_submission_time': 15332.848045110703, 'accumulated_eval_time': 554.4605889320374, 'accumulated_logging_time': 1.0155045986175537}
I0128 11:44:38.785825 140026058876672 logging_writer.py:48] [45019] accumulated_eval_time=554.460589, accumulated_logging_time=1.015505, accumulated_submission_time=15332.848045, global_step=45019, preemption_count=0, score=15332.848045, test/accuracy=0.486900, test/loss=2.341099, test/num_examples=10000, total_duration=15889.959641, train/accuracy=0.695791, train/loss=1.203610, validation/accuracy=0.616220, validation/loss=1.592949, validation/num_examples=50000
I0128 11:45:06.537343 140026067269376 logging_writer.py:48] [45100] global_step=45100, grad_norm=4.0756096839904785, loss=2.052619457244873
I0128 11:45:40.465765 140026058876672 logging_writer.py:48] [45200] global_step=45200, grad_norm=4.099411964416504, loss=1.9327783584594727
I0128 11:46:14.379418 140026067269376 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.8343169689178467, loss=1.9341785907745361
I0128 11:46:48.326484 140026058876672 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.461296558380127, loss=1.8821500539779663
I0128 11:47:22.270106 140026067269376 logging_writer.py:48] [45500] global_step=45500, grad_norm=4.4759416580200195, loss=1.9120948314666748
I0128 11:47:56.216754 140026058876672 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.8404533863067627, loss=1.8603185415267944
I0128 11:48:30.126742 140026067269376 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.6163017749786377, loss=1.901209831237793
I0128 11:49:04.059115 140026058876672 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.6661784648895264, loss=1.9680877923965454
I0128 11:49:38.053844 140026067269376 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.02572774887085, loss=1.9953877925872803
I0128 11:50:11.973318 140026058876672 logging_writer.py:48] [46000] global_step=46000, grad_norm=4.122665882110596, loss=1.8974223136901855
I0128 11:50:45.921933 140026067269376 logging_writer.py:48] [46100] global_step=46100, grad_norm=4.378318786621094, loss=1.8588107824325562
I0128 11:51:19.853959 140026058876672 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.5702807903289795, loss=1.9593900442123413
I0128 11:51:53.764147 140026067269376 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.976083278656006, loss=1.8214592933654785
I0128 11:52:27.681399 140026058876672 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.7874703407287598, loss=1.885123610496521
I0128 11:53:01.615556 140026067269376 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.861569404602051, loss=1.8693814277648926
I0128 11:53:08.935390 140187804313408 spec.py:321] Evaluating on the training split.
I0128 11:53:15.219429 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 11:53:24.316407 140187804313408 spec.py:349] Evaluating on the test split.
I0128 11:53:26.939358 140187804313408 submission_runner.py:408] Time since start: 16418.14s, 	Step: 46523, 	{'train/accuracy': 0.6674505472183228, 'train/loss': 1.308665156364441, 'validation/accuracy': 0.6080999970436096, 'validation/loss': 1.629995584487915, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.37629771232605, 'test/num_examples': 10000, 'score': 15842.934381008148, 'total_duration': 16418.139196634293, 'accumulated_submission_time': 15842.934381008148, 'accumulated_eval_time': 572.4644169807434, 'accumulated_logging_time': 1.0506083965301514}
I0128 11:53:26.968154 140026058876672 logging_writer.py:48] [46523] accumulated_eval_time=572.464417, accumulated_logging_time=1.050608, accumulated_submission_time=15842.934381, global_step=46523, preemption_count=0, score=15842.934381, test/accuracy=0.481200, test/loss=2.376298, test/num_examples=10000, total_duration=16418.139197, train/accuracy=0.667451, train/loss=1.308665, validation/accuracy=0.608100, validation/loss=1.629996, validation/num_examples=50000
I0128 11:53:53.368456 140026151130880 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.4523444175720215, loss=1.869558572769165
I0128 11:54:27.262112 140026058876672 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.440486431121826, loss=1.900114893913269
I0128 11:55:01.184122 140026151130880 logging_writer.py:48] [46800] global_step=46800, grad_norm=4.754735469818115, loss=1.9028208255767822
I0128 11:55:35.156827 140026058876672 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.342907428741455, loss=1.8877164125442505
I0128 11:56:09.045130 140026151130880 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.7671709060668945, loss=1.9208062887191772
I0128 11:56:42.968427 140026058876672 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.891786813735962, loss=1.8306665420532227
I0128 11:57:16.861010 140026151130880 logging_writer.py:48] [47200] global_step=47200, grad_norm=4.667357921600342, loss=1.9517179727554321
I0128 11:57:50.800967 140026058876672 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.8196403980255127, loss=1.8195793628692627
I0128 11:58:24.725105 140026151130880 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.4609789848327637, loss=1.7517927885055542
I0128 11:58:58.624827 140026058876672 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.064937591552734, loss=1.9159319400787354
I0128 11:59:32.577814 140026151130880 logging_writer.py:48] [47600] global_step=47600, grad_norm=4.154214382171631, loss=1.8817745447158813
I0128 12:00:06.485599 140026058876672 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.39790415763855, loss=1.8684872388839722
I0128 12:00:40.423825 140026151130880 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.1273415088653564, loss=1.8696277141571045
I0128 12:01:14.365935 140026058876672 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.5353598594665527, loss=1.861178994178772
I0128 12:01:48.293820 140026151130880 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.4911725521087646, loss=1.912782073020935
I0128 12:01:56.982652 140187804313408 spec.py:321] Evaluating on the training split.
I0128 12:02:04.012712 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 12:02:13.008113 140187804313408 spec.py:349] Evaluating on the test split.
I0128 12:02:15.597530 140187804313408 submission_runner.py:408] Time since start: 16946.80s, 	Step: 48027, 	{'train/accuracy': 0.6508689522743225, 'train/loss': 1.3953670263290405, 'validation/accuracy': 0.5989199876785278, 'validation/loss': 1.662032961845398, 'validation/num_examples': 50000, 'test/accuracy': 0.47510001063346863, 'test/loss': 2.403264045715332, 'test/num_examples': 10000, 'score': 16352.884350776672, 'total_duration': 16946.79745745659, 'accumulated_submission_time': 16352.884350776672, 'accumulated_eval_time': 591.0792412757874, 'accumulated_logging_time': 1.088939905166626}
I0128 12:02:15.629420 140026050483968 logging_writer.py:48] [48027] accumulated_eval_time=591.079241, accumulated_logging_time=1.088940, accumulated_submission_time=16352.884351, global_step=48027, preemption_count=0, score=16352.884351, test/accuracy=0.475100, test/loss=2.403264, test/num_examples=10000, total_duration=16946.797457, train/accuracy=0.650869, train/loss=1.395367, validation/accuracy=0.598920, validation/loss=1.662033, validation/num_examples=50000
I0128 12:02:40.704998 140026067269376 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.643817186355591, loss=1.905380129814148
I0128 12:03:14.600935 140026050483968 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.643496036529541, loss=1.784705638885498
I0128 12:03:48.522316 140026067269376 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.909024238586426, loss=2.023566722869873
I0128 12:04:22.486256 140026050483968 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.538074493408203, loss=1.8402434587478638
I0128 12:04:56.417587 140026067269376 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.685600996017456, loss=1.8937859535217285
I0128 12:05:30.321962 140026050483968 logging_writer.py:48] [48600] global_step=48600, grad_norm=4.076932430267334, loss=1.9096925258636475
I0128 12:06:04.251404 140026067269376 logging_writer.py:48] [48700] global_step=48700, grad_norm=4.247189998626709, loss=1.8848949670791626
I0128 12:06:38.181196 140026050483968 logging_writer.py:48] [48800] global_step=48800, grad_norm=4.51002836227417, loss=1.8751064538955688
I0128 12:07:12.148692 140026067269376 logging_writer.py:48] [48900] global_step=48900, grad_norm=4.282466411590576, loss=1.8762378692626953
I0128 12:07:46.089724 140026050483968 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.342417001724243, loss=1.8694839477539062
I0128 12:08:20.105576 140026067269376 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.568794012069702, loss=1.9114872217178345
I0128 12:08:54.053270 140026050483968 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.57780385017395, loss=1.8611059188842773
I0128 12:09:27.990463 140026067269376 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.9431021213531494, loss=1.8927066326141357
I0128 12:10:01.926917 140026050483968 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.5436336994171143, loss=1.781423568725586
I0128 12:10:35.882229 140026067269376 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.4460608959198, loss=1.7304385900497437
I0128 12:10:45.858312 140187804313408 spec.py:321] Evaluating on the training split.
I0128 12:10:52.034361 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 12:11:00.869775 140187804313408 spec.py:349] Evaluating on the test split.
I0128 12:11:03.607217 140187804313408 submission_runner.py:408] Time since start: 17474.81s, 	Step: 49531, 	{'train/accuracy': 0.6604352593421936, 'train/loss': 1.361952304840088, 'validation/accuracy': 0.6140999794006348, 'validation/loss': 1.5894731283187866, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.311565637588501, 'test/num_examples': 10000, 'score': 16863.04797935486, 'total_duration': 17474.806631326675, 'accumulated_submission_time': 16863.04797935486, 'accumulated_eval_time': 608.8275811672211, 'accumulated_logging_time': 1.131298542022705}
I0128 12:11:03.635441 140026050483968 logging_writer.py:48] [49531] accumulated_eval_time=608.827581, accumulated_logging_time=1.131299, accumulated_submission_time=16863.047979, global_step=49531, preemption_count=0, score=16863.047979, test/accuracy=0.496300, test/loss=2.311566, test/num_examples=10000, total_duration=17474.806631, train/accuracy=0.660435, train/loss=1.361952, validation/accuracy=0.614100, validation/loss=1.589473, validation/num_examples=50000
I0128 12:11:27.349543 140026058876672 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.4737861156463623, loss=1.8932106494903564
I0128 12:12:01.227704 140026050483968 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.3424859046936035, loss=1.8447091579437256
I0128 12:12:35.091374 140026058876672 logging_writer.py:48] [49800] global_step=49800, grad_norm=4.047915458679199, loss=1.8445954322814941
I0128 12:13:09.031154 140026050483968 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.77970027923584, loss=1.9614421129226685
I0128 12:13:42.941110 140026058876672 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.754055976867676, loss=1.932813048362732
I0128 12:14:16.861477 140026050483968 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.818326234817505, loss=1.8927944898605347
I0128 12:14:50.864512 140026058876672 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.521437406539917, loss=1.8744590282440186
I0128 12:15:24.781767 140026050483968 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.507497787475586, loss=1.7821462154388428
I0128 12:15:58.715726 140026058876672 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.7678427696228027, loss=1.8046518564224243
I0128 12:16:32.630611 140026050483968 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.3780517578125, loss=1.823940634727478
I0128 12:17:06.521083 140026058876672 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.7659478187561035, loss=1.8610183000564575
I0128 12:17:40.449060 140026050483968 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.766873359680176, loss=1.793370008468628
I0128 12:18:14.367290 140026058876672 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.860150098800659, loss=1.9992046356201172
I0128 12:18:48.276996 140026050483968 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.2486672401428223, loss=1.855903148651123
I0128 12:19:22.197650 140026058876672 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.0635182857513428, loss=1.8765411376953125
I0128 12:19:33.898506 140187804313408 spec.py:321] Evaluating on the training split.
I0128 12:19:40.139891 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 12:19:48.971984 140187804313408 spec.py:349] Evaluating on the test split.
I0128 12:19:51.581005 140187804313408 submission_runner.py:408] Time since start: 18002.78s, 	Step: 51036, 	{'train/accuracy': 0.6528220772743225, 'train/loss': 1.377036690711975, 'validation/accuracy': 0.6045599579811096, 'validation/loss': 1.6280007362365723, 'validation/num_examples': 50000, 'test/accuracy': 0.4839000105857849, 'test/loss': 2.3610148429870605, 'test/num_examples': 10000, 'score': 17373.247692346573, 'total_duration': 18002.78094768524, 'accumulated_submission_time': 17373.247692346573, 'accumulated_eval_time': 626.5100448131561, 'accumulated_logging_time': 1.1692650318145752}
I0128 12:19:51.608283 140026075662080 logging_writer.py:48] [51036] accumulated_eval_time=626.510045, accumulated_logging_time=1.169265, accumulated_submission_time=17373.247692, global_step=51036, preemption_count=0, score=17373.247692, test/accuracy=0.483900, test/loss=2.361015, test/num_examples=10000, total_duration=18002.780948, train/accuracy=0.652822, train/loss=1.377037, validation/accuracy=0.604560, validation/loss=1.628001, validation/num_examples=50000
I0128 12:20:13.597742 140026167916288 logging_writer.py:48] [51100] global_step=51100, grad_norm=4.287459850311279, loss=1.962906837463379
I0128 12:20:47.482954 140026075662080 logging_writer.py:48] [51200] global_step=51200, grad_norm=4.573910236358643, loss=1.823428750038147
I0128 12:21:21.492956 140026167916288 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.6661739349365234, loss=1.9644548892974854
I0128 12:21:55.414529 140026075662080 logging_writer.py:48] [51400] global_step=51400, grad_norm=4.3712615966796875, loss=1.8166354894638062
I0128 12:22:29.325497 140026167916288 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.347783327102661, loss=1.7825660705566406
I0128 12:23:03.274273 140026075662080 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.9331629276275635, loss=1.810200810432434
I0128 12:23:37.205075 140026167916288 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.050567626953125, loss=1.9078848361968994
I0128 12:24:11.141979 140026075662080 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.875582456588745, loss=1.9191054105758667
I0128 12:24:45.050616 140026167916288 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.3892171382904053, loss=1.9392257928848267
I0128 12:25:18.984938 140026075662080 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.565305471420288, loss=1.762392282485962
I0128 12:25:52.887734 140026167916288 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.612471580505371, loss=1.830758810043335
I0128 12:26:26.827175 140026075662080 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.991774559020996, loss=1.881086826324463
I0128 12:27:00.805699 140026167916288 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.224191427230835, loss=1.8924100399017334
I0128 12:27:34.795990 140026075662080 logging_writer.py:48] [52400] global_step=52400, grad_norm=4.633963584899902, loss=1.8424129486083984
I0128 12:28:08.726671 140026167916288 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.591987371444702, loss=1.803000569343567
I0128 12:28:21.751457 140187804313408 spec.py:321] Evaluating on the training split.
I0128 12:28:27.948245 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 12:28:37.008046 140187804313408 spec.py:349] Evaluating on the test split.
I0128 12:28:39.635490 140187804313408 submission_runner.py:408] Time since start: 18530.84s, 	Step: 52540, 	{'train/accuracy': 0.6638432741165161, 'train/loss': 1.3441935777664185, 'validation/accuracy': 0.6182799935340881, 'validation/loss': 1.5748586654663086, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.280700206756592, 'test/num_examples': 10000, 'score': 17883.32636666298, 'total_duration': 18530.83543086052, 'accumulated_submission_time': 17883.32636666298, 'accumulated_eval_time': 644.3940415382385, 'accumulated_logging_time': 1.2060277462005615}
I0128 12:28:39.663033 140026067269376 logging_writer.py:48] [52540] accumulated_eval_time=644.394042, accumulated_logging_time=1.206028, accumulated_submission_time=17883.326367, global_step=52540, preemption_count=0, score=17883.326367, test/accuracy=0.494400, test/loss=2.280700, test/num_examples=10000, total_duration=18530.835431, train/accuracy=0.663843, train/loss=1.344194, validation/accuracy=0.618280, validation/loss=1.574859, validation/num_examples=50000
I0128 12:29:00.308112 140026151130880 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.766279458999634, loss=1.939416527748108
I0128 12:29:34.172838 140026067269376 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.5352725982666016, loss=1.9139469861984253
I0128 12:30:08.093982 140026151130880 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.3637261390686035, loss=1.816449761390686
I0128 12:30:42.006754 140026067269376 logging_writer.py:48] [52900] global_step=52900, grad_norm=4.120621681213379, loss=1.9505754709243774
I0128 12:31:15.942876 140026151130880 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.669762372970581, loss=1.9051707983016968
I0128 12:31:49.867170 140026067269376 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.946089267730713, loss=1.8600600957870483
I0128 12:32:23.782052 140026151130880 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.3534064292907715, loss=1.95645010471344
I0128 12:32:57.707693 140026067269376 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.725571870803833, loss=1.845950961112976
I0128 12:33:31.607641 140026151130880 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.8961575031280518, loss=1.8984415531158447
I0128 12:34:05.588906 140026067269376 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.5608644485473633, loss=1.8628281354904175
I0128 12:34:39.525515 140026151130880 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.7987639904022217, loss=1.9416744709014893
I0128 12:35:13.447471 140026067269376 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.9492156505584717, loss=1.9374150037765503
I0128 12:35:47.364824 140026151130880 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.847902297973633, loss=1.8425990343093872
I0128 12:36:21.296051 140026067269376 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.4195923805236816, loss=1.9047648906707764
I0128 12:36:55.197691 140026151130880 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.888551950454712, loss=1.823324203491211
I0128 12:37:09.939366 140187804313408 spec.py:321] Evaluating on the training split.
I0128 12:37:16.308916 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 12:37:25.119354 140187804313408 spec.py:349] Evaluating on the test split.
I0128 12:37:27.714084 140187804313408 submission_runner.py:408] Time since start: 19058.91s, 	Step: 54045, 	{'train/accuracy': 0.6905492544174194, 'train/loss': 1.2071837186813354, 'validation/accuracy': 0.604699969291687, 'validation/loss': 1.6392205953598022, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.356093168258667, 'test/num_examples': 10000, 'score': 18393.53794503212, 'total_duration': 19058.91395521164, 'accumulated_submission_time': 18393.53794503212, 'accumulated_eval_time': 662.1686675548553, 'accumulated_logging_time': 1.2445552349090576}
I0128 12:37:27.745128 140026050483968 logging_writer.py:48] [54045] accumulated_eval_time=662.168668, accumulated_logging_time=1.244555, accumulated_submission_time=18393.537945, global_step=54045, preemption_count=0, score=18393.537945, test/accuracy=0.482000, test/loss=2.356093, test/num_examples=10000, total_duration=19058.913955, train/accuracy=0.690549, train/loss=1.207184, validation/accuracy=0.604700, validation/loss=1.639221, validation/num_examples=50000
I0128 12:37:46.727235 140026058876672 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.855135679244995, loss=1.9089998006820679
I0128 12:38:20.593878 140026050483968 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.9386157989501953, loss=1.809350848197937
I0128 12:38:54.487055 140026058876672 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.536942481994629, loss=1.8301148414611816
I0128 12:39:28.428919 140026050483968 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.654327392578125, loss=1.9792671203613281
I0128 12:40:02.363551 140026058876672 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.3611855506896973, loss=1.8425612449645996
I0128 12:40:36.365300 140026050483968 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.791386842727661, loss=1.856635570526123
I0128 12:41:10.251809 140026058876672 logging_writer.py:48] [54700] global_step=54700, grad_norm=4.04341983795166, loss=1.9165858030319214
I0128 12:41:44.155676 140026050483968 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.4444525241851807, loss=1.840260624885559
I0128 12:42:18.073976 140026058876672 logging_writer.py:48] [54900] global_step=54900, grad_norm=4.443265438079834, loss=1.9129371643066406
I0128 12:42:52.005995 140026050483968 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.543897867202759, loss=1.8437612056732178
I0128 12:43:25.969294 140026058876672 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.5272319316864014, loss=1.9029101133346558
I0128 12:43:59.915519 140026050483968 logging_writer.py:48] [55200] global_step=55200, grad_norm=4.038537502288818, loss=1.7739146947860718
I0128 12:44:33.812360 140026058876672 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.9546959400177, loss=1.7751898765563965
I0128 12:45:07.729683 140026050483968 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.9432969093322754, loss=1.820068597793579
I0128 12:45:41.648425 140026058876672 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.8412392139434814, loss=1.9255412817001343
I0128 12:45:57.750407 140187804313408 spec.py:321] Evaluating on the training split.
I0128 12:46:04.134891 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 12:46:12.943315 140187804313408 spec.py:349] Evaluating on the test split.
I0128 12:46:15.515668 140187804313408 submission_runner.py:408] Time since start: 19586.72s, 	Step: 55549, 	{'train/accuracy': 0.6698620915412903, 'train/loss': 1.3108255863189697, 'validation/accuracy': 0.608959972858429, 'validation/loss': 1.6123535633087158, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.349951982498169, 'test/num_examples': 10000, 'score': 18903.47699022293, 'total_duration': 19586.715607881546, 'accumulated_submission_time': 18903.47699022293, 'accumulated_eval_time': 679.9338908195496, 'accumulated_logging_time': 1.2859973907470703}
I0128 12:46:15.548062 140026058876672 logging_writer.py:48] [55549] accumulated_eval_time=679.933891, accumulated_logging_time=1.285997, accumulated_submission_time=18903.476990, global_step=55549, preemption_count=0, score=18903.476990, test/accuracy=0.485800, test/loss=2.349952, test/num_examples=10000, total_duration=19586.715608, train/accuracy=0.669862, train/loss=1.310826, validation/accuracy=0.608960, validation/loss=1.612354, validation/num_examples=50000
I0128 12:46:33.139576 140026067269376 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.7474637031555176, loss=1.8268189430236816
I0128 12:47:07.039594 140026058876672 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.934607744216919, loss=1.9126298427581787
I0128 12:47:40.929890 140026067269376 logging_writer.py:48] [55800] global_step=55800, grad_norm=4.119208335876465, loss=1.8658679723739624
I0128 12:48:14.854704 140026058876672 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.9985196590423584, loss=1.8717916011810303
I0128 12:48:48.757436 140026067269376 logging_writer.py:48] [56000] global_step=56000, grad_norm=4.038184642791748, loss=1.8663585186004639
I0128 12:49:22.690639 140026058876672 logging_writer.py:48] [56100] global_step=56100, grad_norm=4.021421432495117, loss=1.9347577095031738
I0128 12:49:56.561665 140026067269376 logging_writer.py:48] [56200] global_step=56200, grad_norm=4.40057373046875, loss=1.8994684219360352
I0128 12:50:30.492277 140026058876672 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.596444845199585, loss=1.8010525703430176
I0128 12:51:04.392703 140026067269376 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.7623276710510254, loss=1.8198754787445068
I0128 12:51:38.329080 140026058876672 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.7967898845672607, loss=1.8524199724197388
I0128 12:52:12.232198 140026067269376 logging_writer.py:48] [56600] global_step=56600, grad_norm=4.656919956207275, loss=1.8488880395889282
I0128 12:52:46.189847 140026058876672 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.461618423461914, loss=1.77855384349823
I0128 12:53:20.130504 140026067269376 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.9035165309906006, loss=1.8409440517425537
I0128 12:53:54.041215 140026058876672 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.2626843452453613, loss=1.9230164289474487
I0128 12:54:27.930052 140026067269376 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.724144220352173, loss=1.9336885213851929
I0128 12:54:45.738237 140187804313408 spec.py:321] Evaluating on the training split.
I0128 12:54:51.978702 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 12:55:00.825374 140187804313408 spec.py:349] Evaluating on the test split.
I0128 12:55:03.541710 140187804313408 submission_runner.py:408] Time since start: 20114.74s, 	Step: 57054, 	{'train/accuracy': 0.6702606678009033, 'train/loss': 1.301419734954834, 'validation/accuracy': 0.6184399724006653, 'validation/loss': 1.5728846788406372, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.252488136291504, 'test/num_examples': 10000, 'score': 19413.604824781418, 'total_duration': 20114.741649627686, 'accumulated_submission_time': 19413.604824781418, 'accumulated_eval_time': 697.7373259067535, 'accumulated_logging_time': 1.3273625373840332}
I0128 12:55:03.572825 140026050483968 logging_writer.py:48] [57054] accumulated_eval_time=697.737326, accumulated_logging_time=1.327363, accumulated_submission_time=19413.604825, global_step=57054, preemption_count=0, score=19413.604825, test/accuracy=0.497700, test/loss=2.252488, test/num_examples=10000, total_duration=20114.741650, train/accuracy=0.670261, train/loss=1.301420, validation/accuracy=0.618440, validation/loss=1.572885, validation/num_examples=50000
I0128 12:55:19.520430 140026159523584 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.556323766708374, loss=1.7991538047790527
I0128 12:55:53.393700 140026050483968 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.866631507873535, loss=1.76316237449646
I0128 12:56:27.271538 140026159523584 logging_writer.py:48] [57300] global_step=57300, grad_norm=4.684195041656494, loss=1.8898828029632568
I0128 12:57:01.228522 140026050483968 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.9498040676116943, loss=1.9221662282943726
I0128 12:57:35.171058 140026159523584 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.83064603805542, loss=1.9238024950027466
I0128 12:58:09.063560 140026050483968 logging_writer.py:48] [57600] global_step=57600, grad_norm=4.469978332519531, loss=1.8125256299972534
I0128 12:58:43.034525 140026159523584 logging_writer.py:48] [57700] global_step=57700, grad_norm=4.163242816925049, loss=1.9049816131591797
I0128 12:59:17.019502 140026050483968 logging_writer.py:48] [57800] global_step=57800, grad_norm=4.523094654083252, loss=1.7371721267700195
I0128 12:59:50.959778 140026159523584 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.807879686355591, loss=1.904741644859314
I0128 13:00:24.885600 140026050483968 logging_writer.py:48] [58000] global_step=58000, grad_norm=4.360791206359863, loss=1.8686919212341309
I0128 13:00:58.803044 140026159523584 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.6378097534179688, loss=1.9283589124679565
I0128 13:01:32.742117 140026050483968 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.2839863300323486, loss=1.9680874347686768
I0128 13:02:06.678755 140026159523584 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.2557766437530518, loss=1.850128173828125
I0128 13:02:40.602100 140026050483968 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.4618752002716064, loss=1.806801438331604
I0128 13:03:14.539364 140026159523584 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.736344337463379, loss=1.85550856590271
I0128 13:03:33.668846 140187804313408 spec.py:321] Evaluating on the training split.
I0128 13:03:39.914314 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 13:03:48.909513 140187804313408 spec.py:349] Evaluating on the test split.
I0128 13:03:51.528513 140187804313408 submission_runner.py:408] Time since start: 20642.73s, 	Step: 58558, 	{'train/accuracy': 0.6655771732330322, 'train/loss': 1.3216028213500977, 'validation/accuracy': 0.6157599687576294, 'validation/loss': 1.5845470428466797, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.278593063354492, 'test/num_examples': 10000, 'score': 19923.637528181076, 'total_duration': 20642.72845196724, 'accumulated_submission_time': 19923.637528181076, 'accumulated_eval_time': 715.5969526767731, 'accumulated_logging_time': 1.3676202297210693}
I0128 13:03:51.559108 140026067269376 logging_writer.py:48] [58558] accumulated_eval_time=715.596953, accumulated_logging_time=1.367620, accumulated_submission_time=19923.637528, global_step=58558, preemption_count=0, score=19923.637528, test/accuracy=0.494600, test/loss=2.278593, test/num_examples=10000, total_duration=20642.728452, train/accuracy=0.665577, train/loss=1.321603, validation/accuracy=0.615760, validation/loss=1.584547, validation/num_examples=50000
I0128 13:04:06.130364 140026075662080 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.354970932006836, loss=1.8165515661239624
I0128 13:04:39.952608 140026067269376 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.7714250087738037, loss=1.7896747589111328
I0128 13:05:13.817847 140026075662080 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.847174882888794, loss=1.7341550588607788
I0128 13:05:47.782066 140026067269376 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.6223251819610596, loss=1.8686832189559937
I0128 13:06:21.659014 140026075662080 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.7179031372070312, loss=1.7766532897949219
I0128 13:06:55.578956 140026067269376 logging_writer.py:48] [59100] global_step=59100, grad_norm=4.057159900665283, loss=1.7375353574752808
I0128 13:07:29.465262 140026075662080 logging_writer.py:48] [59200] global_step=59200, grad_norm=4.00681734085083, loss=2.0111472606658936
I0128 13:08:03.394484 140026067269376 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.399686098098755, loss=1.765615463256836
I0128 13:08:37.303667 140026075662080 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.644207000732422, loss=1.9153099060058594
I0128 13:09:11.243565 140026067269376 logging_writer.py:48] [59500] global_step=59500, grad_norm=4.227797985076904, loss=1.8499999046325684
I0128 13:09:45.123175 140026075662080 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.938883066177368, loss=1.8929578065872192
I0128 13:10:19.066332 140026067269376 logging_writer.py:48] [59700] global_step=59700, grad_norm=4.168441295623779, loss=1.7668170928955078
I0128 13:10:52.950847 140026075662080 logging_writer.py:48] [59800] global_step=59800, grad_norm=4.046335697174072, loss=1.8071688413619995
I0128 13:11:26.874628 140026067269376 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.9715890884399414, loss=1.7980968952178955
I0128 13:12:00.838443 140026075662080 logging_writer.py:48] [60000] global_step=60000, grad_norm=5.82963752746582, loss=1.8104501962661743
I0128 13:12:21.660994 140187804313408 spec.py:321] Evaluating on the training split.
I0128 13:12:27.940428 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 13:12:36.683547 140187804313408 spec.py:349] Evaluating on the test split.
I0128 13:12:39.335426 140187804313408 submission_runner.py:408] Time since start: 21170.54s, 	Step: 60063, 	{'train/accuracy': 0.6641222834587097, 'train/loss': 1.3311790227890015, 'validation/accuracy': 0.6173999905586243, 'validation/loss': 1.5765674114227295, 'validation/num_examples': 50000, 'test/accuracy': 0.4969000220298767, 'test/loss': 2.2979726791381836, 'test/num_examples': 10000, 'score': 20433.673320770264, 'total_duration': 21170.53536248207, 'accumulated_submission_time': 20433.673320770264, 'accumulated_eval_time': 733.2713446617126, 'accumulated_logging_time': 1.408951997756958}
I0128 13:12:39.364855 140026050483968 logging_writer.py:48] [60063] accumulated_eval_time=733.271345, accumulated_logging_time=1.408952, accumulated_submission_time=20433.673321, global_step=60063, preemption_count=0, score=20433.673321, test/accuracy=0.496900, test/loss=2.297973, test/num_examples=10000, total_duration=21170.535362, train/accuracy=0.664122, train/loss=1.331179, validation/accuracy=0.617400, validation/loss=1.576567, validation/num_examples=50000
I0128 13:12:52.213390 140026058876672 logging_writer.py:48] [60100] global_step=60100, grad_norm=4.139057159423828, loss=1.9813854694366455
I0128 13:13:26.108141 140026050483968 logging_writer.py:48] [60200] global_step=60200, grad_norm=4.036856651306152, loss=1.829698085784912
I0128 13:14:00.003236 140026058876672 logging_writer.py:48] [60300] global_step=60300, grad_norm=5.0108866691589355, loss=1.8396755456924438
I0128 13:14:33.936216 140026050483968 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.8659870624542236, loss=1.7869477272033691
I0128 13:15:07.873093 140026058876672 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.6185522079467773, loss=1.703120231628418
I0128 13:15:41.813135 140026050483968 logging_writer.py:48] [60600] global_step=60600, grad_norm=5.944305896759033, loss=1.864546298980713
I0128 13:16:15.726336 140026058876672 logging_writer.py:48] [60700] global_step=60700, grad_norm=4.499605178833008, loss=1.784775972366333
I0128 13:16:49.655922 140026050483968 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.469433307647705, loss=1.9008865356445312
I0128 13:17:23.541280 140026058876672 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.800295829772949, loss=1.8414992094039917
I0128 13:17:57.449153 140026050483968 logging_writer.py:48] [61000] global_step=61000, grad_norm=4.626662254333496, loss=1.8078641891479492
I0128 13:18:31.389816 140026058876672 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.6482417583465576, loss=1.7226934432983398
I0128 13:19:05.313005 140026050483968 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.816561698913574, loss=1.853859782218933
I0128 13:19:39.187300 140026058876672 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.9647116661071777, loss=1.8494460582733154
I0128 13:20:13.094962 140026050483968 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.7312777042388916, loss=1.8272266387939453
I0128 13:20:46.985347 140026058876672 logging_writer.py:48] [61500] global_step=61500, grad_norm=4.027243614196777, loss=1.9112786054611206
I0128 13:21:09.512862 140187804313408 spec.py:321] Evaluating on the training split.
I0128 13:21:15.831023 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 13:21:24.623306 140187804313408 spec.py:349] Evaluating on the test split.
I0128 13:21:27.226513 140187804313408 submission_runner.py:408] Time since start: 21698.43s, 	Step: 61568, 	{'train/accuracy': 0.6678690910339355, 'train/loss': 1.3077174425125122, 'validation/accuracy': 0.622439980506897, 'validation/loss': 1.5476828813552856, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.2889938354492188, 'test/num_examples': 10000, 'score': 20943.756311655045, 'total_duration': 21698.42645382881, 'accumulated_submission_time': 20943.756311655045, 'accumulated_eval_time': 750.9849836826324, 'accumulated_logging_time': 1.4479811191558838}
I0128 13:21:27.254817 140026067269376 logging_writer.py:48] [61568] accumulated_eval_time=750.984984, accumulated_logging_time=1.447981, accumulated_submission_time=20943.756312, global_step=61568, preemption_count=0, score=20943.756312, test/accuracy=0.497700, test/loss=2.288994, test/num_examples=10000, total_duration=21698.426454, train/accuracy=0.667869, train/loss=1.307717, validation/accuracy=0.622440, validation/loss=1.547683, validation/num_examples=50000
I0128 13:21:38.458549 140026075662080 logging_writer.py:48] [61600] global_step=61600, grad_norm=4.049765110015869, loss=1.821432113647461
I0128 13:22:12.269899 140026067269376 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.872596263885498, loss=1.760300874710083
I0128 13:22:46.137533 140026075662080 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.8344082832336426, loss=1.843949794769287
I0128 13:23:20.061094 140026067269376 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.5396523475646973, loss=1.7713136672973633
I0128 13:23:53.969848 140026075662080 logging_writer.py:48] [62000] global_step=62000, grad_norm=4.017492771148682, loss=1.8470702171325684
I0128 13:24:27.875242 140026067269376 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.706875801086426, loss=1.850675106048584
I0128 13:25:01.838114 140026075662080 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.639880418777466, loss=1.814611792564392
I0128 13:25:35.720403 140026067269376 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.621323585510254, loss=1.7816802263259888
I0128 13:26:09.574436 140026075662080 logging_writer.py:48] [62400] global_step=62400, grad_norm=4.569748401641846, loss=1.8873100280761719
I0128 13:26:43.487233 140026067269376 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.7639400959014893, loss=1.8732815980911255
I0128 13:27:17.374678 140026075662080 logging_writer.py:48] [62600] global_step=62600, grad_norm=4.052281856536865, loss=1.7111914157867432
I0128 13:27:51.292685 140026067269376 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.7210702896118164, loss=2.0844614505767822
I0128 13:28:25.189607 140026075662080 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.818941116333008, loss=1.771322250366211
I0128 13:28:59.120717 140026067269376 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.504931688308716, loss=1.7512269020080566
I0128 13:29:33.018027 140026075662080 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.638709783554077, loss=1.898127794265747
I0128 13:29:57.566727 140187804313408 spec.py:321] Evaluating on the training split.
I0128 13:30:04.108998 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 13:30:12.929569 140187804313408 spec.py:349] Evaluating on the test split.
I0128 13:30:15.584896 140187804313408 submission_runner.py:408] Time since start: 22226.78s, 	Step: 63074, 	{'train/accuracy': 0.7181122303009033, 'train/loss': 1.0941146612167358, 'validation/accuracy': 0.6288599967956543, 'validation/loss': 1.528099536895752, 'validation/num_examples': 50000, 'test/accuracy': 0.5130000114440918, 'test/loss': 2.2014949321746826, 'test/num_examples': 10000, 'score': 21454.002192497253, 'total_duration': 22226.784834861755, 'accumulated_submission_time': 21454.002192497253, 'accumulated_eval_time': 769.0031280517578, 'accumulated_logging_time': 1.4872050285339355}
I0128 13:30:15.613083 140026159523584 logging_writer.py:48] [63074] accumulated_eval_time=769.003128, accumulated_logging_time=1.487205, accumulated_submission_time=21454.002192, global_step=63074, preemption_count=0, score=21454.002192, test/accuracy=0.513000, test/loss=2.201495, test/num_examples=10000, total_duration=22226.784835, train/accuracy=0.718112, train/loss=1.094115, validation/accuracy=0.628860, validation/loss=1.528100, validation/num_examples=50000
I0128 13:30:24.751019 140026167916288 logging_writer.py:48] [63100] global_step=63100, grad_norm=4.202753067016602, loss=1.8145066499710083
I0128 13:30:58.679669 140026159523584 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.7506942749023438, loss=1.8025503158569336
I0128 13:31:32.559434 140026167916288 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.329622268676758, loss=1.7838786840438843
I0128 13:32:06.457530 140026159523584 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.488680601119995, loss=1.8739738464355469
I0128 13:32:40.374340 140026167916288 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.550846576690674, loss=1.708768606185913
I0128 13:33:14.304907 140026159523584 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.920044183731079, loss=1.686690330505371
I0128 13:33:48.239978 140026167916288 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.757760524749756, loss=1.7925875186920166
I0128 13:34:22.158871 140026159523584 logging_writer.py:48] [63800] global_step=63800, grad_norm=4.10354471206665, loss=1.8773727416992188
I0128 13:34:56.079184 140026167916288 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.4389350414276123, loss=1.7044713497161865
I0128 13:35:29.966585 140026159523584 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.922360420227051, loss=1.832585096359253
I0128 13:36:03.891378 140026167916288 logging_writer.py:48] [64100] global_step=64100, grad_norm=4.038609504699707, loss=1.924928903579712
I0128 13:36:37.804587 140026159523584 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.4237093925476074, loss=1.7603213787078857
I0128 13:37:11.712842 140026167916288 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.815568447113037, loss=1.7542095184326172
I0128 13:37:45.701435 140026159523584 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.797675371170044, loss=1.73390531539917
I0128 13:38:19.658218 140026167916288 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.923082113265991, loss=1.9420987367630005
I0128 13:38:45.903154 140187804313408 spec.py:321] Evaluating on the training split.
I0128 13:38:52.188536 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 13:39:00.977685 140187804313408 spec.py:349] Evaluating on the test split.
I0128 13:39:03.796179 140187804313408 submission_runner.py:408] Time since start: 22755.00s, 	Step: 64579, 	{'train/accuracy': 0.6921635866165161, 'train/loss': 1.2201071977615356, 'validation/accuracy': 0.6279599666595459, 'validation/loss': 1.5359902381896973, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.242102861404419, 'test/num_examples': 10000, 'score': 21964.22785615921, 'total_duration': 22754.996037244797, 'accumulated_submission_time': 21964.22785615921, 'accumulated_eval_time': 786.8960626125336, 'accumulated_logging_time': 1.5244412422180176}
I0128 13:39:03.827081 140026067269376 logging_writer.py:48] [64579] accumulated_eval_time=786.896063, accumulated_logging_time=1.524441, accumulated_submission_time=21964.227856, global_step=64579, preemption_count=0, score=21964.227856, test/accuracy=0.502900, test/loss=2.242103, test/num_examples=10000, total_duration=22754.996037, train/accuracy=0.692164, train/loss=1.220107, validation/accuracy=0.627960, validation/loss=1.535990, validation/num_examples=50000
I0128 13:39:11.290286 140026075662080 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.607938528060913, loss=1.742791771888733
I0128 13:39:45.082007 140026067269376 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.7259390354156494, loss=1.8310174942016602
I0128 13:40:18.941405 140026075662080 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.9244349002838135, loss=1.771713137626648
I0128 13:40:52.842792 140026067269376 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.5873546600341797, loss=1.892051100730896
I0128 13:41:26.755741 140026075662080 logging_writer.py:48] [65000] global_step=65000, grad_norm=4.201821804046631, loss=1.7153278589248657
I0128 13:42:00.678875 140026067269376 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.909050941467285, loss=1.9366590976715088
I0128 13:42:34.537430 140026075662080 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.7643320560455322, loss=1.7911086082458496
I0128 13:43:08.431072 140026067269376 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.7244601249694824, loss=1.9252238273620605
I0128 13:43:42.413055 140026075662080 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.8476521968841553, loss=1.761540412902832
I0128 13:44:16.291985 140026067269376 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.4882357120513916, loss=1.7051470279693604
I0128 13:44:50.178178 140026075662080 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.8922548294067383, loss=1.8940809965133667
I0128 13:45:24.094148 140026067269376 logging_writer.py:48] [65700] global_step=65700, grad_norm=4.178609371185303, loss=1.8529424667358398
I0128 13:45:57.977704 140026075662080 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.8624677658081055, loss=1.713291883468628
I0128 13:46:31.914373 140026067269376 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.904097557067871, loss=1.7884396314620972
I0128 13:47:05.814450 140026075662080 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.961730480194092, loss=1.7673628330230713
I0128 13:47:34.088320 140187804313408 spec.py:321] Evaluating on the training split.
I0128 13:47:40.295885 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 13:47:49.368656 140187804313408 spec.py:349] Evaluating on the test split.
I0128 13:47:51.988021 140187804313408 submission_runner.py:408] Time since start: 23283.19s, 	Step: 66085, 	{'train/accuracy': 0.6760801672935486, 'train/loss': 1.270575761795044, 'validation/accuracy': 0.623199999332428, 'validation/loss': 1.5508557558059692, 'validation/num_examples': 50000, 'test/accuracy': 0.4992000162601471, 'test/loss': 2.274237632751465, 'test/num_examples': 10000, 'score': 22474.41934657097, 'total_duration': 23283.187956809998, 'accumulated_submission_time': 22474.41934657097, 'accumulated_eval_time': 804.7957236766815, 'accumulated_logging_time': 1.5703482627868652}
I0128 13:47:52.017155 140026058876672 logging_writer.py:48] [66085] accumulated_eval_time=804.795724, accumulated_logging_time=1.570348, accumulated_submission_time=22474.419347, global_step=66085, preemption_count=0, score=22474.419347, test/accuracy=0.499200, test/loss=2.274238, test/num_examples=10000, total_duration=23283.187957, train/accuracy=0.676080, train/loss=1.270576, validation/accuracy=0.623200, validation/loss=1.550856, validation/num_examples=50000
I0128 13:47:57.457988 140026067269376 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.7812271118164062, loss=1.8036205768585205
I0128 13:48:31.313278 140026058876672 logging_writer.py:48] [66200] global_step=66200, grad_norm=4.269579887390137, loss=1.7958898544311523
I0128 13:49:05.177700 140026067269376 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.943311929702759, loss=1.7613884210586548
I0128 13:49:39.059490 140026058876672 logging_writer.py:48] [66400] global_step=66400, grad_norm=4.326488494873047, loss=1.8886879682540894
I0128 13:50:13.120434 140026067269376 logging_writer.py:48] [66500] global_step=66500, grad_norm=4.833040237426758, loss=1.7246698141098022
I0128 13:50:47.072775 140026058876672 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.7485413551330566, loss=1.816716194152832
I0128 13:51:20.990360 140026067269376 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.780377149581909, loss=1.7747893333435059
I0128 13:51:54.876881 140026058876672 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.5546183586120605, loss=1.7038627862930298
I0128 13:52:28.802770 140026067269376 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.575218915939331, loss=1.8765220642089844
I0128 13:53:02.684989 140026058876672 logging_writer.py:48] [67000] global_step=67000, grad_norm=4.103999137878418, loss=1.7354191541671753
I0128 13:53:36.618159 140026067269376 logging_writer.py:48] [67100] global_step=67100, grad_norm=4.073138236999512, loss=1.8372493982315063
I0128 13:54:10.516970 140026058876672 logging_writer.py:48] [67200] global_step=67200, grad_norm=4.104560375213623, loss=1.7865279912948608
I0128 13:54:44.414795 140026067269376 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.919502019882202, loss=1.7950321435928345
I0128 13:55:18.301585 140026058876672 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.6290457248687744, loss=1.6850638389587402
I0128 13:55:52.217529 140026067269376 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.6682958602905273, loss=1.7282110452651978
I0128 13:56:22.193120 140187804313408 spec.py:321] Evaluating on the training split.
I0128 13:56:28.422168 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 13:56:37.417745 140187804313408 spec.py:349] Evaluating on the test split.
I0128 13:56:40.020783 140187804313408 submission_runner.py:408] Time since start: 23811.22s, 	Step: 67590, 	{'train/accuracy': 0.675203263759613, 'train/loss': 1.2853842973709106, 'validation/accuracy': 0.6229400038719177, 'validation/loss': 1.5472275018692017, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.2707667350769043, 'test/num_examples': 10000, 'score': 22984.531358480453, 'total_duration': 23811.220719575882, 'accumulated_submission_time': 22984.531358480453, 'accumulated_eval_time': 822.6233458518982, 'accumulated_logging_time': 1.609081745147705}
I0128 13:56:40.055730 140026058876672 logging_writer.py:48] [67590] accumulated_eval_time=822.623346, accumulated_logging_time=1.609082, accumulated_submission_time=22984.531358, global_step=67590, preemption_count=0, score=22984.531358, test/accuracy=0.504500, test/loss=2.270767, test/num_examples=10000, total_duration=23811.220720, train/accuracy=0.675203, train/loss=1.285384, validation/accuracy=0.622940, validation/loss=1.547228, validation/num_examples=50000
I0128 13:56:43.788247 140026075662080 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.7807884216308594, loss=1.8120331764221191
I0128 13:57:17.639080 140026058876672 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.9344048500061035, loss=1.7747420072555542
I0128 13:57:51.489346 140026075662080 logging_writer.py:48] [67800] global_step=67800, grad_norm=4.517091751098633, loss=1.9440536499023438
I0128 13:58:25.388635 140026058876672 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.865255832672119, loss=1.8931798934936523
I0128 13:58:59.313343 140026075662080 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.660191774368286, loss=1.7268946170806885
I0128 13:59:33.188170 140026058876672 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.8152456283569336, loss=1.832764744758606
I0128 14:00:07.054960 140026075662080 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.660971164703369, loss=1.6444244384765625
I0128 14:00:40.940911 140026058876672 logging_writer.py:48] [68300] global_step=68300, grad_norm=4.159511566162109, loss=1.7842199802398682
I0128 14:01:14.814180 140026075662080 logging_writer.py:48] [68400] global_step=68400, grad_norm=4.138529300689697, loss=1.8351025581359863
I0128 14:01:48.728447 140026058876672 logging_writer.py:48] [68500] global_step=68500, grad_norm=4.31978702545166, loss=1.7094573974609375
I0128 14:02:22.627129 140026075662080 logging_writer.py:48] [68600] global_step=68600, grad_norm=4.235642433166504, loss=1.7452850341796875
I0128 14:02:56.725629 140026058876672 logging_writer.py:48] [68700] global_step=68700, grad_norm=4.24850606918335, loss=1.8244292736053467
I0128 14:03:30.636628 140026075662080 logging_writer.py:48] [68800] global_step=68800, grad_norm=4.368505001068115, loss=1.7616996765136719
I0128 14:04:04.514114 140026058876672 logging_writer.py:48] [68900] global_step=68900, grad_norm=4.819145202636719, loss=1.8016315698623657
I0128 14:04:38.442982 140026075662080 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.8084964752197266, loss=1.8243376016616821
I0128 14:05:10.094409 140187804313408 spec.py:321] Evaluating on the training split.
I0128 14:05:16.319537 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 14:05:25.166075 140187804313408 spec.py:349] Evaluating on the test split.
I0128 14:05:27.768389 140187804313408 submission_runner.py:408] Time since start: 24338.97s, 	Step: 69095, 	{'train/accuracy': 0.682039201259613, 'train/loss': 1.2529497146606445, 'validation/accuracy': 0.6260799765586853, 'validation/loss': 1.544729471206665, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2389981746673584, 'test/num_examples': 10000, 'score': 23494.504409313202, 'total_duration': 24338.96833062172, 'accumulated_submission_time': 23494.504409313202, 'accumulated_eval_time': 840.2972972393036, 'accumulated_logging_time': 1.6531808376312256}
I0128 14:05:27.799080 140026167916288 logging_writer.py:48] [69095] accumulated_eval_time=840.297297, accumulated_logging_time=1.653181, accumulated_submission_time=23494.504409, global_step=69095, preemption_count=0, score=23494.504409, test/accuracy=0.506300, test/loss=2.238998, test/num_examples=10000, total_duration=24338.968331, train/accuracy=0.682039, train/loss=1.252950, validation/accuracy=0.626080, validation/loss=1.544729, validation/num_examples=50000
I0128 14:05:29.834323 140026176308992 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.6576905250549316, loss=1.69680655002594
I0128 14:06:03.654792 140026167916288 logging_writer.py:48] [69200] global_step=69200, grad_norm=4.170243740081787, loss=1.6393309831619263
I0128 14:06:37.555502 140026176308992 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.986534357070923, loss=1.81572687625885
I0128 14:07:11.455958 140026167916288 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.773681163787842, loss=1.8399813175201416
I0128 14:07:45.396280 140026176308992 logging_writer.py:48] [69500] global_step=69500, grad_norm=4.633560657501221, loss=1.8243757486343384
I0128 14:08:19.292125 140026167916288 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.894589900970459, loss=1.7728548049926758
I0128 14:08:53.190957 140026176308992 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.7816896438598633, loss=1.7152212858200073
I0128 14:09:27.308400 140026167916288 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.8044025897979736, loss=1.7939473390579224
I0128 14:10:01.187935 140026176308992 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.658766746520996, loss=1.758591890335083
I0128 14:10:35.072749 140026167916288 logging_writer.py:48] [70000] global_step=70000, grad_norm=4.169788360595703, loss=1.7616580724716187
I0128 14:11:08.962304 140026176308992 logging_writer.py:48] [70100] global_step=70100, grad_norm=4.120089054107666, loss=1.786496877670288
I0128 14:11:42.871441 140026167916288 logging_writer.py:48] [70200] global_step=70200, grad_norm=5.173989772796631, loss=1.7142579555511475
I0128 14:12:16.827208 140026176308992 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.4494550228118896, loss=1.7428267002105713
I0128 14:12:50.707699 140026167916288 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.596118688583374, loss=1.8359216451644897
I0128 14:13:24.616066 140026176308992 logging_writer.py:48] [70500] global_step=70500, grad_norm=4.103145122528076, loss=1.7250761985778809
I0128 14:13:58.001292 140187804313408 spec.py:321] Evaluating on the training split.
I0128 14:14:04.420467 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 14:14:13.167500 140187804313408 spec.py:349] Evaluating on the test split.
I0128 14:14:15.795355 140187804313408 submission_runner.py:408] Time since start: 24867.00s, 	Step: 70600, 	{'train/accuracy': 0.6823381781578064, 'train/loss': 1.261600375175476, 'validation/accuracy': 0.6331599950790405, 'validation/loss': 1.5052260160446167, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.193894147872925, 'test/num_examples': 10000, 'score': 24004.64289021492, 'total_duration': 24866.995292663574, 'accumulated_submission_time': 24004.64289021492, 'accumulated_eval_time': 858.0913376808167, 'accumulated_logging_time': 1.6934871673583984}
I0128 14:14:15.829043 140026058876672 logging_writer.py:48] [70600] accumulated_eval_time=858.091338, accumulated_logging_time=1.693487, accumulated_submission_time=24004.642890, global_step=70600, preemption_count=0, score=24004.642890, test/accuracy=0.512500, test/loss=2.193894, test/num_examples=10000, total_duration=24866.995293, train/accuracy=0.682338, train/loss=1.261600, validation/accuracy=0.633160, validation/loss=1.505226, validation/num_examples=50000
I0128 14:14:16.187388 140026067269376 logging_writer.py:48] [70600] global_step=70600, grad_norm=4.304253578186035, loss=1.8036551475524902
I0128 14:14:50.011386 140026058876672 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.597376823425293, loss=1.8194584846496582
I0128 14:15:23.858235 140026067269376 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.5210225582122803, loss=1.7820439338684082
I0128 14:15:57.885832 140026058876672 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.7496581077575684, loss=1.87394380569458
I0128 14:16:31.810664 140026067269376 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.8857991695404053, loss=1.7113548517227173
I0128 14:17:05.676702 140026058876672 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.6234068870544434, loss=1.6254098415374756
I0128 14:17:39.555423 140026067269376 logging_writer.py:48] [71200] global_step=71200, grad_norm=4.469391345977783, loss=1.7980490922927856
I0128 14:18:13.448677 140026058876672 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.525900363922119, loss=1.8628178834915161
I0128 14:18:47.331511 140026067269376 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.911579132080078, loss=1.791433334350586
I0128 14:19:21.224035 140026058876672 logging_writer.py:48] [71500] global_step=71500, grad_norm=4.5442795753479, loss=1.873917818069458
I0128 14:19:55.116350 140026067269376 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.984130382537842, loss=1.90321683883667
I0128 14:20:29.048788 140026058876672 logging_writer.py:48] [71700] global_step=71700, grad_norm=4.387892246246338, loss=1.836299180984497
I0128 14:21:02.911190 140026067269376 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.905769109725952, loss=1.665575385093689
I0128 14:21:36.839963 140026058876672 logging_writer.py:48] [71900] global_step=71900, grad_norm=4.531507968902588, loss=1.8531619310379028
I0128 14:22:10.917276 140026067269376 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.924412488937378, loss=1.6752281188964844
I0128 14:22:44.816637 140026058876672 logging_writer.py:48] [72100] global_step=72100, grad_norm=4.526089191436768, loss=1.8127460479736328
I0128 14:22:45.985438 140187804313408 spec.py:321] Evaluating on the training split.
I0128 14:22:52.198267 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 14:23:00.994941 140187804313408 spec.py:349] Evaluating on the test split.
I0128 14:23:03.786532 140187804313408 submission_runner.py:408] Time since start: 25394.99s, 	Step: 72105, 	{'train/accuracy': 0.7115353941917419, 'train/loss': 1.123003363609314, 'validation/accuracy': 0.6309399604797363, 'validation/loss': 1.522621989250183, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2285428047180176, 'test/num_examples': 10000, 'score': 24514.733004808426, 'total_duration': 25394.98640203476, 'accumulated_submission_time': 24514.733004808426, 'accumulated_eval_time': 875.8923208713531, 'accumulated_logging_time': 1.7388732433319092}
I0128 14:23:03.820339 140026151130880 logging_writer.py:48] [72105] accumulated_eval_time=875.892321, accumulated_logging_time=1.738873, accumulated_submission_time=24514.733005, global_step=72105, preemption_count=0, score=24514.733005, test/accuracy=0.505400, test/loss=2.228543, test/num_examples=10000, total_duration=25394.986402, train/accuracy=0.711535, train/loss=1.123003, validation/accuracy=0.630940, validation/loss=1.522622, validation/num_examples=50000
I0128 14:23:36.312785 140026159523584 logging_writer.py:48] [72200] global_step=72200, grad_norm=4.594644546508789, loss=1.7102861404418945
I0128 14:24:10.169330 140026151130880 logging_writer.py:48] [72300] global_step=72300, grad_norm=4.114622116088867, loss=1.8102903366088867
I0128 14:24:44.086043 140026159523584 logging_writer.py:48] [72400] global_step=72400, grad_norm=4.385051727294922, loss=1.8385908603668213
I0128 14:25:17.933506 140026151130880 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.6645843982696533, loss=1.7900458574295044
I0128 14:25:51.880431 140026159523584 logging_writer.py:48] [72600] global_step=72600, grad_norm=4.215743064880371, loss=1.6369423866271973
I0128 14:26:25.753783 140026151130880 logging_writer.py:48] [72700] global_step=72700, grad_norm=4.898825645446777, loss=1.6664711236953735
I0128 14:26:59.686195 140026159523584 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.9215803146362305, loss=1.7569818496704102
I0128 14:27:33.560386 140026151130880 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.5088906288146973, loss=1.7113282680511475
I0128 14:28:07.482545 140026159523584 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.5423107147216797, loss=1.8053042888641357
I0128 14:28:41.487773 140026151130880 logging_writer.py:48] [73100] global_step=73100, grad_norm=4.334481716156006, loss=1.8797885179519653
I0128 14:29:15.392558 140026159523584 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.5643887519836426, loss=1.7755318880081177
I0128 14:29:49.290118 140026151130880 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.909219980239868, loss=1.603081464767456
I0128 14:30:23.165812 140026159523584 logging_writer.py:48] [73400] global_step=73400, grad_norm=4.188561916351318, loss=1.7365772724151611
I0128 14:30:57.080993 140026151130880 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.634681463241577, loss=1.7668163776397705
I0128 14:31:30.957778 140026159523584 logging_writer.py:48] [73600] global_step=73600, grad_norm=4.3406782150268555, loss=1.6567418575286865
I0128 14:31:33.811406 140187804313408 spec.py:321] Evaluating on the training split.
I0128 14:31:40.152332 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 14:31:48.839722 140187804313408 spec.py:349] Evaluating on the test split.
I0128 14:31:51.427541 140187804313408 submission_runner.py:408] Time since start: 25922.63s, 	Step: 73610, 	{'train/accuracy': 0.7038823366165161, 'train/loss': 1.1389704942703247, 'validation/accuracy': 0.6362000107765198, 'validation/loss': 1.4967260360717773, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.229922294616699, 'test/num_examples': 10000, 'score': 25024.659342050552, 'total_duration': 25922.627481222153, 'accumulated_submission_time': 25024.659342050552, 'accumulated_eval_time': 893.5084192752838, 'accumulated_logging_time': 1.7819523811340332}
I0128 14:31:51.459364 140026067269376 logging_writer.py:48] [73610] accumulated_eval_time=893.508419, accumulated_logging_time=1.781952, accumulated_submission_time=25024.659342, global_step=73610, preemption_count=0, score=25024.659342, test/accuracy=0.511800, test/loss=2.229922, test/num_examples=10000, total_duration=25922.627481, train/accuracy=0.703882, train/loss=1.138970, validation/accuracy=0.636200, validation/loss=1.496726, validation/num_examples=50000
I0128 14:32:22.265772 140026075662080 logging_writer.py:48] [73700] global_step=73700, grad_norm=4.021202564239502, loss=1.66750967502594
I0128 14:32:56.098637 140026067269376 logging_writer.py:48] [73800] global_step=73800, grad_norm=4.045755386352539, loss=1.6707994937896729
I0128 14:33:29.997341 140026075662080 logging_writer.py:48] [73900] global_step=73900, grad_norm=4.502214431762695, loss=1.8853446245193481
I0128 14:34:03.880447 140026067269376 logging_writer.py:48] [74000] global_step=74000, grad_norm=4.275156021118164, loss=1.6484029293060303
I0128 14:34:37.800531 140026075662080 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.678863525390625, loss=1.6025385856628418
I0128 14:35:11.740088 140026067269376 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.813760280609131, loss=1.7908151149749756
I0128 14:35:45.618928 140026075662080 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.7723941802978516, loss=1.8302987813949585
I0128 14:36:19.486743 140026067269376 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.821423053741455, loss=1.737375020980835
I0128 14:36:53.405347 140026075662080 logging_writer.py:48] [74500] global_step=74500, grad_norm=4.044419765472412, loss=1.7946341037750244
I0128 14:37:27.305675 140026067269376 logging_writer.py:48] [74600] global_step=74600, grad_norm=4.415853977203369, loss=1.9245532751083374
I0128 14:38:01.174690 140026075662080 logging_writer.py:48] [74700] global_step=74700, grad_norm=4.506913661956787, loss=1.8055390119552612
I0128 14:38:35.078697 140026067269376 logging_writer.py:48] [74800] global_step=74800, grad_norm=4.148097038269043, loss=1.8885691165924072
I0128 14:39:08.948232 140026075662080 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.8634393215179443, loss=1.7580375671386719
I0128 14:39:42.857021 140026067269376 logging_writer.py:48] [75000] global_step=75000, grad_norm=4.247513294219971, loss=1.777597188949585
I0128 14:40:16.710924 140026075662080 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.7620208263397217, loss=1.7272231578826904
I0128 14:40:21.600644 140187804313408 spec.py:321] Evaluating on the training split.
I0128 14:40:27.831064 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 14:40:36.862922 140187804313408 spec.py:349] Evaluating on the test split.
I0128 14:40:39.462671 140187804313408 submission_runner.py:408] Time since start: 26450.66s, 	Step: 75116, 	{'train/accuracy': 0.6950334906578064, 'train/loss': 1.1846526861190796, 'validation/accuracy': 0.6358999609947205, 'validation/loss': 1.490337610244751, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.216155767440796, 'test/num_examples': 10000, 'score': 25534.734695911407, 'total_duration': 26450.662058591843, 'accumulated_submission_time': 25534.734695911407, 'accumulated_eval_time': 911.3698537349701, 'accumulated_logging_time': 1.823401689529419}
I0128 14:40:39.493482 140026042091264 logging_writer.py:48] [75116] accumulated_eval_time=911.369854, accumulated_logging_time=1.823402, accumulated_submission_time=25534.734696, global_step=75116, preemption_count=0, score=25534.734696, test/accuracy=0.509100, test/loss=2.216156, test/num_examples=10000, total_duration=26450.662059, train/accuracy=0.695033, train/loss=1.184653, validation/accuracy=0.635900, validation/loss=1.490338, validation/num_examples=50000
I0128 14:41:08.254997 140026050483968 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.929922103881836, loss=1.7814784049987793
I0128 14:41:42.201458 140026042091264 logging_writer.py:48] [75300] global_step=75300, grad_norm=4.00656270980835, loss=1.8002371788024902
I0128 14:42:16.123066 140026050483968 logging_writer.py:48] [75400] global_step=75400, grad_norm=4.229353904724121, loss=1.8080536127090454
I0128 14:42:50.000908 140026042091264 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.8980624675750732, loss=1.7430065870285034
I0128 14:43:23.926748 140026050483968 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.7809107303619385, loss=1.7190593481063843
I0128 14:43:57.823374 140026042091264 logging_writer.py:48] [75700] global_step=75700, grad_norm=4.172610759735107, loss=1.7450518608093262
I0128 14:44:31.722115 140026050483968 logging_writer.py:48] [75800] global_step=75800, grad_norm=4.221934795379639, loss=1.7909631729125977
I0128 14:45:05.626393 140026042091264 logging_writer.py:48] [75900] global_step=75900, grad_norm=4.123488426208496, loss=1.8532254695892334
I0128 14:45:39.541532 140026050483968 logging_writer.py:48] [76000] global_step=76000, grad_norm=4.1263604164123535, loss=1.7297461032867432
I0128 14:46:13.433387 140026042091264 logging_writer.py:48] [76100] global_step=76100, grad_norm=4.234016418457031, loss=1.8255113363265991
I0128 14:46:47.332414 140026050483968 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.9277267456054688, loss=1.6806330680847168
I0128 14:47:21.208509 140026042091264 logging_writer.py:48] [76300] global_step=76300, grad_norm=4.251890182495117, loss=1.6548523902893066
I0128 14:47:55.220656 140026050483968 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.6963417530059814, loss=1.7647912502288818
I0128 14:48:29.128517 140026042091264 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.515310287475586, loss=1.694496750831604
I0128 14:49:03.036627 140026050483968 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.775362014770508, loss=1.7674235105514526
I0128 14:49:09.645613 140187804313408 spec.py:321] Evaluating on the training split.
I0128 14:49:15.902423 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 14:49:24.751805 140187804313408 spec.py:349] Evaluating on the test split.
I0128 14:49:27.380605 140187804313408 submission_runner.py:408] Time since start: 26978.58s, 	Step: 76621, 	{'train/accuracy': 0.6759606003761292, 'train/loss': 1.2714606523513794, 'validation/accuracy': 0.6222000122070312, 'validation/loss': 1.5547786951065063, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.2797510623931885, 'test/num_examples': 10000, 'score': 26044.821749687195, 'total_duration': 26978.580530405045, 'accumulated_submission_time': 26044.821749687195, 'accumulated_eval_time': 929.1047916412354, 'accumulated_logging_time': 1.8654460906982422}
I0128 14:49:27.416161 140026050483968 logging_writer.py:48] [76621] accumulated_eval_time=929.104792, accumulated_logging_time=1.865446, accumulated_submission_time=26044.821750, global_step=76621, preemption_count=0, score=26044.821750, test/accuracy=0.490300, test/loss=2.279751, test/num_examples=10000, total_duration=26978.580530, train/accuracy=0.675961, train/loss=1.271461, validation/accuracy=0.622200, validation/loss=1.554779, validation/num_examples=50000
I0128 14:49:54.479288 140026159523584 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.8852245807647705, loss=1.7802810668945312
I0128 14:50:28.315732 140026050483968 logging_writer.py:48] [76800] global_step=76800, grad_norm=4.413227081298828, loss=1.7252157926559448
I0128 14:51:02.214058 140026159523584 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.6581990718841553, loss=1.6823680400848389
I0128 14:51:36.116668 140026050483968 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.6346542835235596, loss=1.7682439088821411
I0128 14:52:10.005586 140026159523584 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.8796865940093994, loss=1.763261318206787
I0128 14:52:43.854991 140026050483968 logging_writer.py:48] [77200] global_step=77200, grad_norm=4.641308784484863, loss=1.723480224609375
I0128 14:53:17.763564 140026159523584 logging_writer.py:48] [77300] global_step=77300, grad_norm=4.169009208679199, loss=1.7924630641937256
I0128 14:53:51.662142 140026050483968 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.8836722373962402, loss=1.7994115352630615
I0128 14:54:25.765526 140026159523584 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.732943534851074, loss=1.64670729637146
I0128 14:54:59.649787 140026050483968 logging_writer.py:48] [77600] global_step=77600, grad_norm=4.420046806335449, loss=1.7483136653900146
I0128 14:55:33.536156 140026159523584 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.547219753265381, loss=1.6312676668167114
I0128 14:56:07.428088 140026050483968 logging_writer.py:48] [77800] global_step=77800, grad_norm=4.038580894470215, loss=1.792229175567627
I0128 14:56:41.315380 140026159523584 logging_writer.py:48] [77900] global_step=77900, grad_norm=4.265921592712402, loss=1.8295838832855225
I0128 14:57:15.213755 140026050483968 logging_writer.py:48] [78000] global_step=78000, grad_norm=4.040095329284668, loss=1.601996660232544
I0128 14:57:49.096140 140026159523584 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.9319241046905518, loss=1.5870391130447388
I0128 14:57:57.702964 140187804313408 spec.py:321] Evaluating on the training split.
I0128 14:58:04.239948 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 14:58:12.873601 140187804313408 spec.py:349] Evaluating on the test split.
I0128 14:58:15.523139 140187804313408 submission_runner.py:408] Time since start: 27506.72s, 	Step: 78127, 	{'train/accuracy': 0.696687638759613, 'train/loss': 1.187483787536621, 'validation/accuracy': 0.6402400135993958, 'validation/loss': 1.469867467880249, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.1450984477996826, 'test/num_examples': 10000, 'score': 26555.042127132416, 'total_duration': 27506.72307229042, 'accumulated_submission_time': 26555.042127132416, 'accumulated_eval_time': 946.9249217510223, 'accumulated_logging_time': 1.913517951965332}
I0128 14:58:15.557237 140026075662080 logging_writer.py:48] [78127] accumulated_eval_time=946.924922, accumulated_logging_time=1.913518, accumulated_submission_time=26555.042127, global_step=78127, preemption_count=0, score=26555.042127, test/accuracy=0.520100, test/loss=2.145098, test/num_examples=10000, total_duration=27506.723072, train/accuracy=0.696688, train/loss=1.187484, validation/accuracy=0.640240, validation/loss=1.469867, validation/num_examples=50000
I0128 14:58:40.636109 140026151130880 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.8724441528320312, loss=1.8206787109375
I0128 14:59:14.477945 140026075662080 logging_writer.py:48] [78300] global_step=78300, grad_norm=4.12011194229126, loss=1.6486746072769165
I0128 14:59:48.385414 140026151130880 logging_writer.py:48] [78400] global_step=78400, grad_norm=4.301407814025879, loss=1.7005281448364258
I0128 15:00:22.278463 140026075662080 logging_writer.py:48] [78500] global_step=78500, grad_norm=4.401670932769775, loss=1.785828709602356
I0128 15:00:56.265403 140026151130880 logging_writer.py:48] [78600] global_step=78600, grad_norm=4.312145233154297, loss=1.7672781944274902
I0128 15:01:30.166634 140026075662080 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.895151376724243, loss=1.623321533203125
I0128 15:02:04.068405 140026151130880 logging_writer.py:48] [78800] global_step=78800, grad_norm=4.579043865203857, loss=1.7022318840026855
I0128 15:02:37.967686 140026075662080 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.5350186824798584, loss=1.6834698915481567
I0128 15:03:11.871758 140026151130880 logging_writer.py:48] [79000] global_step=79000, grad_norm=4.193443775177002, loss=1.845474362373352
I0128 15:03:45.798586 140026075662080 logging_writer.py:48] [79100] global_step=79100, grad_norm=4.267939567565918, loss=1.7279821634292603
I0128 15:04:19.694007 140026151130880 logging_writer.py:48] [79200] global_step=79200, grad_norm=4.1013689041137695, loss=1.6969993114471436
I0128 15:04:53.606366 140026075662080 logging_writer.py:48] [79300] global_step=79300, grad_norm=4.371408939361572, loss=1.6503252983093262
I0128 15:05:27.505027 140026151130880 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.732602834701538, loss=1.692999243736267
I0128 15:06:01.363565 140026075662080 logging_writer.py:48] [79500] global_step=79500, grad_norm=4.02788782119751, loss=1.7597306966781616
I0128 15:06:35.269105 140026151130880 logging_writer.py:48] [79600] global_step=79600, grad_norm=4.080629825592041, loss=1.7323887348175049
I0128 15:06:45.630197 140187804313408 spec.py:321] Evaluating on the training split.
I0128 15:06:51.878260 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 15:07:00.800185 140187804313408 spec.py:349] Evaluating on the test split.
I0128 15:07:03.604384 140187804313408 submission_runner.py:408] Time since start: 28034.80s, 	Step: 79632, 	{'train/accuracy': 0.6890544891357422, 'train/loss': 1.2304526567459106, 'validation/accuracy': 0.635919988155365, 'validation/loss': 1.4974093437194824, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.213355302810669, 'test/num_examples': 10000, 'score': 27065.05168557167, 'total_duration': 28034.80432486534, 'accumulated_submission_time': 27065.05168557167, 'accumulated_eval_time': 964.8990716934204, 'accumulated_logging_time': 1.9566450119018555}
I0128 15:07:03.636019 140026058876672 logging_writer.py:48] [79632] accumulated_eval_time=964.899072, accumulated_logging_time=1.956645, accumulated_submission_time=27065.051686, global_step=79632, preemption_count=0, score=27065.051686, test/accuracy=0.503300, test/loss=2.213355, test/num_examples=10000, total_duration=28034.804325, train/accuracy=0.689054, train/loss=1.230453, validation/accuracy=0.635920, validation/loss=1.497409, validation/num_examples=50000
I0128 15:07:27.002040 140026067269376 logging_writer.py:48] [79700] global_step=79700, grad_norm=4.794126987457275, loss=1.804247498512268
I0128 15:08:00.853093 140026058876672 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.9548561573028564, loss=1.673551321029663
I0128 15:08:34.692929 140026067269376 logging_writer.py:48] [79900] global_step=79900, grad_norm=4.284284591674805, loss=1.6391570568084717
I0128 15:09:08.533971 140026058876672 logging_writer.py:48] [80000] global_step=80000, grad_norm=4.359969615936279, loss=1.5723767280578613
I0128 15:09:42.405924 140026067269376 logging_writer.py:48] [80100] global_step=80100, grad_norm=4.610962390899658, loss=1.775010347366333
I0128 15:10:16.314002 140026058876672 logging_writer.py:48] [80200] global_step=80200, grad_norm=4.23935079574585, loss=1.7436939477920532
I0128 15:10:50.244591 140026067269376 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.007251262664795, loss=1.6655287742614746
I0128 15:11:24.122172 140026058876672 logging_writer.py:48] [80400] global_step=80400, grad_norm=4.018438816070557, loss=1.7028135061264038
I0128 15:11:57.999805 140026067269376 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.7550063133239746, loss=1.7345737218856812
I0128 15:12:31.926363 140026058876672 logging_writer.py:48] [80600] global_step=80600, grad_norm=4.102779388427734, loss=1.8192572593688965
I0128 15:13:05.791057 140026067269376 logging_writer.py:48] [80700] global_step=80700, grad_norm=4.070517539978027, loss=1.7785745859146118
I0128 15:13:39.856506 140026058876672 logging_writer.py:48] [80800] global_step=80800, grad_norm=4.148474216461182, loss=1.685859203338623
I0128 15:14:13.739629 140026067269376 logging_writer.py:48] [80900] global_step=80900, grad_norm=4.546385765075684, loss=1.642747402191162
I0128 15:14:47.594192 140026058876672 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.7916016578674316, loss=1.6743367910385132
I0128 15:15:21.505385 140026067269376 logging_writer.py:48] [81100] global_step=81100, grad_norm=4.870405673980713, loss=1.6092002391815186
I0128 15:15:33.837220 140187804313408 spec.py:321] Evaluating on the training split.
I0128 15:15:40.013283 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 15:15:49.122038 140187804313408 spec.py:349] Evaluating on the test split.
I0128 15:15:51.695861 140187804313408 submission_runner.py:408] Time since start: 28562.90s, 	Step: 81138, 	{'train/accuracy': 0.7074896097183228, 'train/loss': 1.138108491897583, 'validation/accuracy': 0.6323999762535095, 'validation/loss': 1.5167434215545654, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.267728090286255, 'test/num_examples': 10000, 'score': 27575.18598818779, 'total_duration': 28562.895799398422, 'accumulated_submission_time': 27575.18598818779, 'accumulated_eval_time': 982.7576727867126, 'accumulated_logging_time': 2.000518560409546}
I0128 15:15:51.730231 140026075662080 logging_writer.py:48] [81138] accumulated_eval_time=982.757673, accumulated_logging_time=2.000519, accumulated_submission_time=27575.185988, global_step=81138, preemption_count=0, score=27575.185988, test/accuracy=0.502800, test/loss=2.267728, test/num_examples=10000, total_duration=28562.895799, train/accuracy=0.707490, train/loss=1.138108, validation/accuracy=0.632400, validation/loss=1.516743, validation/num_examples=50000
I0128 15:16:13.031369 140026151130880 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.379052639007568, loss=1.553445816040039
I0128 15:16:46.862397 140026075662080 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.618180990219116, loss=1.5968595743179321
I0128 15:17:20.790660 140026151130880 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.9580812454223633, loss=1.7263941764831543
I0128 15:17:54.666013 140026075662080 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.327249526977539, loss=1.7332793474197388
I0128 15:18:28.631248 140026151130880 logging_writer.py:48] [81600] global_step=81600, grad_norm=4.1143412590026855, loss=1.7022769451141357
I0128 15:19:02.524199 140026075662080 logging_writer.py:48] [81700] global_step=81700, grad_norm=4.423194885253906, loss=1.7533643245697021
I0128 15:19:36.437495 140026151130880 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.8295936584472656, loss=1.713411569595337
I0128 15:20:10.441231 140026075662080 logging_writer.py:48] [81900] global_step=81900, grad_norm=4.621795654296875, loss=1.6921441555023193
I0128 15:20:44.334628 140026151130880 logging_writer.py:48] [82000] global_step=82000, grad_norm=4.288304805755615, loss=1.6861525774002075
I0128 15:21:18.250386 140026075662080 logging_writer.py:48] [82100] global_step=82100, grad_norm=4.943206310272217, loss=1.6830401420593262
I0128 15:21:52.177237 140026151130880 logging_writer.py:48] [82200] global_step=82200, grad_norm=4.543696403503418, loss=1.721505045890808
I0128 15:22:26.104635 140026075662080 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.6691348552703857, loss=1.6991853713989258
I0128 15:23:00.005085 140026151130880 logging_writer.py:48] [82400] global_step=82400, grad_norm=4.399271488189697, loss=1.6868455410003662
I0128 15:23:33.934700 140026075662080 logging_writer.py:48] [82500] global_step=82500, grad_norm=4.6599297523498535, loss=1.6778016090393066
I0128 15:24:07.810844 140026151130880 logging_writer.py:48] [82600] global_step=82600, grad_norm=4.812132358551025, loss=1.6422104835510254
I0128 15:24:21.866366 140187804313408 spec.py:321] Evaluating on the training split.
I0128 15:24:28.068588 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 15:24:36.859260 140187804313408 spec.py:349] Evaluating on the test split.
I0128 15:24:39.534988 140187804313408 submission_runner.py:408] Time since start: 29090.73s, 	Step: 82643, 	{'train/accuracy': 0.7142258882522583, 'train/loss': 1.0972977876663208, 'validation/accuracy': 0.6411199569702148, 'validation/loss': 1.474290132522583, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.1949095726013184, 'test/num_examples': 10000, 'score': 28085.256512880325, 'total_duration': 29090.73493361473, 'accumulated_submission_time': 28085.256512880325, 'accumulated_eval_time': 1000.4262602329254, 'accumulated_logging_time': 2.045931100845337}
I0128 15:24:39.567395 140026058876672 logging_writer.py:48] [82643] accumulated_eval_time=1000.426260, accumulated_logging_time=2.045931, accumulated_submission_time=28085.256513, global_step=82643, preemption_count=0, score=28085.256513, test/accuracy=0.516200, test/loss=2.194910, test/num_examples=10000, total_duration=29090.734934, train/accuracy=0.714226, train/loss=1.097298, validation/accuracy=0.641120, validation/loss=1.474290, validation/num_examples=50000
I0128 15:24:59.184117 140026067269376 logging_writer.py:48] [82700] global_step=82700, grad_norm=4.300812244415283, loss=1.7884438037872314
I0128 15:25:33.013864 140026058876672 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.7822182178497314, loss=1.6585724353790283
I0128 15:26:07.003696 140026067269376 logging_writer.py:48] [82900] global_step=82900, grad_norm=4.227808475494385, loss=1.74861478805542
I0128 15:26:40.889734 140026058876672 logging_writer.py:48] [83000] global_step=83000, grad_norm=5.036970138549805, loss=1.7053302526474
I0128 15:27:14.774436 140026067269376 logging_writer.py:48] [83100] global_step=83100, grad_norm=4.351808547973633, loss=1.6624091863632202
I0128 15:27:48.683262 140026058876672 logging_writer.py:48] [83200] global_step=83200, grad_norm=4.911066055297852, loss=1.6997640132904053
I0128 15:28:22.568288 140026067269376 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.7002880573272705, loss=1.660658836364746
I0128 15:28:56.482807 140026058876672 logging_writer.py:48] [83400] global_step=83400, grad_norm=4.811062335968018, loss=1.7634109258651733
I0128 15:29:30.361468 140026067269376 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.370794773101807, loss=1.7030987739562988
I0128 15:30:04.268738 140026058876672 logging_writer.py:48] [83600] global_step=83600, grad_norm=4.018487453460693, loss=1.7664706707000732
I0128 15:30:38.148639 140026067269376 logging_writer.py:48] [83700] global_step=83700, grad_norm=4.2858357429504395, loss=1.7036173343658447
I0128 15:31:12.079088 140026058876672 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.973019599914551, loss=1.6929590702056885
I0128 15:31:45.937151 140026067269376 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.094430446624756, loss=1.719112515449524
I0128 15:32:19.846485 140026058876672 logging_writer.py:48] [84000] global_step=84000, grad_norm=4.340036392211914, loss=1.7297393083572388
I0128 15:32:53.851964 140026067269376 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.5150341987609863, loss=1.7400213479995728
I0128 15:33:09.560237 140187804313408 spec.py:321] Evaluating on the training split.
I0128 15:33:15.814923 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 15:33:24.916126 140187804313408 spec.py:349] Evaluating on the test split.
I0128 15:33:27.481316 140187804313408 submission_runner.py:408] Time since start: 29618.68s, 	Step: 84148, 	{'train/accuracy': 0.7117147445678711, 'train/loss': 1.1199214458465576, 'validation/accuracy': 0.6462799906730652, 'validation/loss': 1.4425675868988037, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.1688649654388428, 'test/num_examples': 10000, 'score': 28595.185331583023, 'total_duration': 29618.681260347366, 'accumulated_submission_time': 28595.185331583023, 'accumulated_eval_time': 1018.3473055362701, 'accumulated_logging_time': 2.0879697799682617}
I0128 15:33:27.517653 140026058876672 logging_writer.py:48] [84148] accumulated_eval_time=1018.347306, accumulated_logging_time=2.087970, accumulated_submission_time=28595.185332, global_step=84148, preemption_count=0, score=28595.185332, test/accuracy=0.517600, test/loss=2.168865, test/num_examples=10000, total_duration=29618.681260, train/accuracy=0.711715, train/loss=1.119921, validation/accuracy=0.646280, validation/loss=1.442568, validation/num_examples=50000
I0128 15:33:45.503044 140026075662080 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.8548638820648193, loss=1.6310354471206665
I0128 15:34:19.342662 140026058876672 logging_writer.py:48] [84300] global_step=84300, grad_norm=4.450850009918213, loss=1.6826931238174438
I0128 15:34:53.228550 140026075662080 logging_writer.py:48] [84400] global_step=84400, grad_norm=5.039997577667236, loss=1.6793944835662842
I0128 15:35:27.119470 140026058876672 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.968205690383911, loss=1.7005062103271484
I0128 15:36:01.020056 140026075662080 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.892609119415283, loss=1.7873848676681519
I0128 15:36:34.930166 140026058876672 logging_writer.py:48] [84700] global_step=84700, grad_norm=4.1763224601745605, loss=1.6873779296875
I0128 15:37:08.794748 140026075662080 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.6275200843811035, loss=1.6531330347061157
I0128 15:37:42.711197 140026058876672 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.122195243835449, loss=1.636898159980774
I0128 15:38:16.599693 140026075662080 logging_writer.py:48] [85000] global_step=85000, grad_norm=4.197248935699463, loss=1.6744587421417236
I0128 15:38:50.551713 140026058876672 logging_writer.py:48] [85100] global_step=85100, grad_norm=4.311008930206299, loss=1.6677085161209106
I0128 15:39:24.457763 140026075662080 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.955552101135254, loss=1.5796905755996704
I0128 15:39:58.351652 140026058876672 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.539929151535034, loss=1.754758596420288
I0128 15:40:32.241807 140026075662080 logging_writer.py:48] [85400] global_step=85400, grad_norm=4.4036478996276855, loss=1.6277244091033936
I0128 15:41:06.116005 140026058876672 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.388246059417725, loss=1.6997867822647095
I0128 15:41:40.008998 140026075662080 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.7527787685394287, loss=1.6691405773162842
I0128 15:41:57.771036 140187804313408 spec.py:321] Evaluating on the training split.
I0128 15:42:04.162549 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 15:42:13.402102 140187804313408 spec.py:349] Evaluating on the test split.
I0128 15:42:16.005814 140187804313408 submission_runner.py:408] Time since start: 30147.21s, 	Step: 85654, 	{'train/accuracy': 0.7042809128761292, 'train/loss': 1.1611301898956299, 'validation/accuracy': 0.6452400088310242, 'validation/loss': 1.4567824602127075, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.153033971786499, 'test/num_examples': 10000, 'score': 29105.374361276627, 'total_duration': 30147.205749988556, 'accumulated_submission_time': 29105.374361276627, 'accumulated_eval_time': 1036.582043170929, 'accumulated_logging_time': 2.1334755420684814}
I0128 15:42:16.046445 140026067269376 logging_writer.py:48] [85654] accumulated_eval_time=1036.582043, accumulated_logging_time=2.133476, accumulated_submission_time=29105.374361, global_step=85654, preemption_count=0, score=29105.374361, test/accuracy=0.521600, test/loss=2.153034, test/num_examples=10000, total_duration=30147.205750, train/accuracy=0.704281, train/loss=1.161130, validation/accuracy=0.645240, validation/loss=1.456782, validation/num_examples=50000
I0128 15:42:31.939751 140026159523584 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.459325313568115, loss=1.6991366147994995
I0128 15:43:05.775027 140026067269376 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.680769681930542, loss=1.6930742263793945
I0128 15:43:39.640291 140026159523584 logging_writer.py:48] [85900] global_step=85900, grad_norm=4.13370418548584, loss=1.7300426959991455
I0128 15:44:13.493322 140026067269376 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.7873806953430176, loss=1.6679810285568237
I0128 15:44:47.379826 140026159523584 logging_writer.py:48] [86100] global_step=86100, grad_norm=4.007974147796631, loss=1.6733953952789307
I0128 15:45:21.319200 140026067269376 logging_writer.py:48] [86200] global_step=86200, grad_norm=5.2018513679504395, loss=1.8252311944961548
I0128 15:45:55.221834 140026159523584 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.427921772003174, loss=1.6624443531036377
I0128 15:46:29.086137 140026067269376 logging_writer.py:48] [86400] global_step=86400, grad_norm=4.388919353485107, loss=1.8192014694213867
I0128 15:47:02.918762 140026159523584 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.777964115142822, loss=1.726589560508728
I0128 15:47:36.828374 140026067269376 logging_writer.py:48] [86600] global_step=86600, grad_norm=4.256990909576416, loss=1.7426153421401978
I0128 15:48:10.707586 140026159523584 logging_writer.py:48] [86700] global_step=86700, grad_norm=4.190359115600586, loss=1.7885065078735352
I0128 15:48:44.602424 140026067269376 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.260820388793945, loss=1.7540223598480225
I0128 15:49:18.486701 140026159523584 logging_writer.py:48] [86900] global_step=86900, grad_norm=4.766360282897949, loss=1.6066875457763672
I0128 15:49:52.373561 140026067269376 logging_writer.py:48] [87000] global_step=87000, grad_norm=4.739272117614746, loss=1.6602468490600586
I0128 15:50:26.255000 140026159523584 logging_writer.py:48] [87100] global_step=87100, grad_norm=5.076336860656738, loss=1.7263050079345703
I0128 15:50:46.042050 140187804313408 spec.py:321] Evaluating on the training split.
I0128 15:50:52.299703 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 15:51:01.348094 140187804313408 spec.py:349] Evaluating on the test split.
I0128 15:51:04.176695 140187804313408 submission_runner.py:408] Time since start: 30675.38s, 	Step: 87160, 	{'train/accuracy': 0.7081273794174194, 'train/loss': 1.1381235122680664, 'validation/accuracy': 0.6510800123214722, 'validation/loss': 1.4280773401260376, 'validation/num_examples': 50000, 'test/accuracy': 0.5284000039100647, 'test/loss': 2.1416590213775635, 'test/num_examples': 10000, 'score': 29615.305659532547, 'total_duration': 30675.376630306244, 'accumulated_submission_time': 29615.305659532547, 'accumulated_eval_time': 1054.7166481018066, 'accumulated_logging_time': 2.183864116668701}
I0128 15:51:04.209413 140026151130880 logging_writer.py:48] [87160] accumulated_eval_time=1054.716648, accumulated_logging_time=2.183864, accumulated_submission_time=29615.305660, global_step=87160, preemption_count=0, score=29615.305660, test/accuracy=0.528400, test/loss=2.141659, test/num_examples=10000, total_duration=30675.376630, train/accuracy=0.708127, train/loss=1.138124, validation/accuracy=0.651080, validation/loss=1.428077, validation/num_examples=50000
I0128 15:51:19.172383 140026167916288 logging_writer.py:48] [87200] global_step=87200, grad_norm=4.011753559112549, loss=1.6539782285690308
I0128 15:51:53.108422 140026151130880 logging_writer.py:48] [87300] global_step=87300, grad_norm=4.42290735244751, loss=1.6281416416168213
I0128 15:52:26.964515 140026167916288 logging_writer.py:48] [87400] global_step=87400, grad_norm=4.061121940612793, loss=1.7024143934249878
I0128 15:53:00.828998 140026151130880 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.273984909057617, loss=1.7252839803695679
I0128 15:53:34.746088 140026167916288 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.184498310089111, loss=1.6671459674835205
I0128 15:54:08.650403 140026151130880 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.158951282501221, loss=1.6855087280273438
I0128 15:54:42.559412 140026167916288 logging_writer.py:48] [87800] global_step=87800, grad_norm=4.566750526428223, loss=1.698362946510315
I0128 15:55:16.470329 140026151130880 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.649047374725342, loss=1.7059545516967773
I0128 15:55:50.350171 140026167916288 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.077484130859375, loss=1.7759417295455933
I0128 15:56:24.238975 140026151130880 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.9314565658569336, loss=1.6918110847473145
I0128 15:56:58.105530 140026167916288 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.943399667739868, loss=1.7557194232940674
I0128 15:57:32.037490 140026151130880 logging_writer.py:48] [88300] global_step=88300, grad_norm=5.064034461975098, loss=1.5801477432250977
I0128 15:58:06.054217 140026167916288 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.4051079750061035, loss=1.7150670289993286
I0128 15:58:39.941751 140026151130880 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.55189847946167, loss=1.7866365909576416
I0128 15:59:13.832725 140026167916288 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.096729278564453, loss=1.5969009399414062
I0128 15:59:34.297951 140187804313408 spec.py:321] Evaluating on the training split.
I0128 15:59:40.518640 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 15:59:49.262273 140187804313408 spec.py:349] Evaluating on the test split.
I0128 15:59:51.948967 140187804313408 submission_runner.py:408] Time since start: 31203.15s, 	Step: 88662, 	{'train/accuracy': 0.706074595451355, 'train/loss': 1.1489758491516113, 'validation/accuracy': 0.6509799957275391, 'validation/loss': 1.4250391721725464, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.128875970840454, 'test/num_examples': 10000, 'score': 30124.241693258286, 'total_duration': 31203.1488199234, 'accumulated_submission_time': 30124.241693258286, 'accumulated_eval_time': 1072.3675389289856, 'accumulated_logging_time': 3.314368963241577}
I0128 15:59:51.986624 140026058876672 logging_writer.py:48] [88662] accumulated_eval_time=1072.367539, accumulated_logging_time=3.314369, accumulated_submission_time=30124.241693, global_step=88662, preemption_count=0, score=30124.241693, test/accuracy=0.520300, test/loss=2.128876, test/num_examples=10000, total_duration=31203.148820, train/accuracy=0.706075, train/loss=1.148976, validation/accuracy=0.650980, validation/loss=1.425039, validation/num_examples=50000
I0128 16:00:05.205565 140026067269376 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.824557304382324, loss=1.5854933261871338
I0128 16:00:39.036452 140026058876672 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.26518440246582, loss=1.763152003288269
I0128 16:01:12.873725 140026067269376 logging_writer.py:48] [88900] global_step=88900, grad_norm=4.7011590003967285, loss=1.6432292461395264
I0128 16:01:46.748245 140026058876672 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.64503812789917, loss=1.676710844039917
I0128 16:02:20.639232 140026067269376 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.513239860534668, loss=1.607679009437561
I0128 16:02:54.506576 140026058876672 logging_writer.py:48] [89200] global_step=89200, grad_norm=4.323447227478027, loss=1.6925326585769653
I0128 16:03:28.383269 140026067269376 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.209944725036621, loss=1.670440435409546
I0128 16:04:02.308548 140026058876672 logging_writer.py:48] [89400] global_step=89400, grad_norm=4.379099369049072, loss=1.780109167098999
I0128 16:04:36.297386 140026067269376 logging_writer.py:48] [89500] global_step=89500, grad_norm=4.073634147644043, loss=1.7249195575714111
I0128 16:05:10.146849 140026058876672 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.265573978424072, loss=1.771134853363037
I0128 16:05:44.076883 140026067269376 logging_writer.py:48] [89700] global_step=89700, grad_norm=4.091358661651611, loss=1.5833266973495483
I0128 16:06:17.970985 140026058876672 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.954083442687988, loss=1.7027699947357178
I0128 16:06:51.883864 140026067269376 logging_writer.py:48] [89900] global_step=89900, grad_norm=4.983920097351074, loss=1.719498872756958
I0128 16:07:25.787271 140026058876672 logging_writer.py:48] [90000] global_step=90000, grad_norm=4.2945709228515625, loss=1.7766531705856323
I0128 16:07:59.640403 140026067269376 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.27055549621582, loss=1.6644309759140015
I0128 16:08:22.137581 140187804313408 spec.py:321] Evaluating on the training split.
I0128 16:08:28.451704 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 16:08:37.263221 140187804313408 spec.py:349] Evaluating on the test split.
I0128 16:08:39.847614 140187804313408 submission_runner.py:408] Time since start: 31731.05s, 	Step: 90168, 	{'train/accuracy': 0.7113161683082581, 'train/loss': 1.1303361654281616, 'validation/accuracy': 0.6467599868774414, 'validation/loss': 1.444667100906372, 'validation/num_examples': 50000, 'test/accuracy': 0.5240000486373901, 'test/loss': 2.1700925827026367, 'test/num_examples': 10000, 'score': 30634.328468084335, 'total_duration': 31731.047548294067, 'accumulated_submission_time': 30634.328468084335, 'accumulated_eval_time': 1090.077528476715, 'accumulated_logging_time': 3.361715793609619}
I0128 16:08:39.883853 140026151130880 logging_writer.py:48] [90168] accumulated_eval_time=1090.077528, accumulated_logging_time=3.361716, accumulated_submission_time=30634.328468, global_step=90168, preemption_count=0, score=30634.328468, test/accuracy=0.524000, test/loss=2.170093, test/num_examples=10000, total_duration=31731.047548, train/accuracy=0.711316, train/loss=1.130336, validation/accuracy=0.646760, validation/loss=1.444667, validation/num_examples=50000
I0128 16:08:51.068557 140026167916288 logging_writer.py:48] [90200] global_step=90200, grad_norm=4.0412116050720215, loss=1.6614749431610107
I0128 16:09:24.886663 140026151130880 logging_writer.py:48] [90300] global_step=90300, grad_norm=4.207719326019287, loss=1.7249547243118286
I0128 16:09:58.757767 140026167916288 logging_writer.py:48] [90400] global_step=90400, grad_norm=4.284867763519287, loss=1.6134021282196045
I0128 16:10:32.621560 140026151130880 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.874224662780762, loss=1.6360132694244385
I0128 16:11:06.629998 140026167916288 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.263751029968262, loss=1.6349025964736938
I0128 16:11:40.548623 140026151130880 logging_writer.py:48] [90700] global_step=90700, grad_norm=4.372861862182617, loss=1.7267956733703613
I0128 16:12:14.412093 140026167916288 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.304137706756592, loss=1.596562385559082
I0128 16:12:48.335829 140026151130880 logging_writer.py:48] [90900] global_step=90900, grad_norm=4.860527515411377, loss=1.6202812194824219
I0128 16:13:22.208353 140026167916288 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.414419174194336, loss=1.6243270635604858
I0128 16:13:56.118824 140026151130880 logging_writer.py:48] [91100] global_step=91100, grad_norm=5.455418586730957, loss=1.648200273513794
I0128 16:14:30.008473 140026167916288 logging_writer.py:48] [91200] global_step=91200, grad_norm=4.892167091369629, loss=1.6397197246551514
I0128 16:15:03.912960 140026151130880 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.91566801071167, loss=1.6750972270965576
I0128 16:15:37.810944 140026167916288 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.20549201965332, loss=1.6611295938491821
I0128 16:16:11.714062 140026151130880 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.115342617034912, loss=1.5831587314605713
I0128 16:16:45.610937 140026167916288 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.084805488586426, loss=1.6959550380706787
I0128 16:17:10.025806 140187804313408 spec.py:321] Evaluating on the training split.
I0128 16:17:16.224620 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 16:17:25.014649 140187804313408 spec.py:349] Evaluating on the test split.
I0128 16:17:27.641177 140187804313408 submission_runner.py:408] Time since start: 32258.84s, 	Step: 91673, 	{'train/accuracy': 0.7281768321990967, 'train/loss': 1.0502692461013794, 'validation/accuracy': 0.6510199904441833, 'validation/loss': 1.4127343893051147, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.0981850624084473, 'test/num_examples': 10000, 'score': 31144.406147003174, 'total_duration': 32258.84111380577, 'accumulated_submission_time': 31144.406147003174, 'accumulated_eval_time': 1107.6928596496582, 'accumulated_logging_time': 3.4071123600006104}
I0128 16:17:27.675292 140026058876672 logging_writer.py:48] [91673] accumulated_eval_time=1107.692860, accumulated_logging_time=3.407112, accumulated_submission_time=31144.406147, global_step=91673, preemption_count=0, score=31144.406147, test/accuracy=0.531100, test/loss=2.098185, test/num_examples=10000, total_duration=32258.841114, train/accuracy=0.728177, train/loss=1.050269, validation/accuracy=0.651020, validation/loss=1.412734, validation/num_examples=50000
I0128 16:17:37.156474 140026067269376 logging_writer.py:48] [91700] global_step=91700, grad_norm=5.133229732513428, loss=1.750438928604126
I0128 16:18:10.993775 140026058876672 logging_writer.py:48] [91800] global_step=91800, grad_norm=6.042623519897461, loss=1.7529807090759277
I0128 16:18:44.885262 140026067269376 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.963944911956787, loss=1.6810472011566162
I0128 16:19:18.750209 140026058876672 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.8795876502990723, loss=1.6852151155471802
I0128 16:19:52.639127 140026067269376 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.244184494018555, loss=1.6584382057189941
I0128 16:20:26.536370 140026058876672 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.009883403778076, loss=1.578698754310608
I0128 16:21:00.428246 140026067269376 logging_writer.py:48] [92300] global_step=92300, grad_norm=4.7230000495910645, loss=1.6247693300247192
I0128 16:21:34.295923 140026058876672 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.990480661392212, loss=1.6470054388046265
I0128 16:22:08.164039 140026067269376 logging_writer.py:48] [92500] global_step=92500, grad_norm=4.705142974853516, loss=1.633827805519104
I0128 16:22:42.040565 140026058876672 logging_writer.py:48] [92600] global_step=92600, grad_norm=4.421270847320557, loss=1.5329509973526
I0128 16:23:15.943299 140026067269376 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.624884128570557, loss=1.6285322904586792
I0128 16:23:49.973573 140026058876672 logging_writer.py:48] [92800] global_step=92800, grad_norm=5.411212921142578, loss=1.5740363597869873
I0128 16:24:23.857930 140026067269376 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.462726593017578, loss=1.734712839126587
I0128 16:24:57.775164 140026058876672 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.305532455444336, loss=1.7060661315917969
I0128 16:25:31.626420 140026067269376 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.1534833908081055, loss=1.5638693571090698
I0128 16:25:57.871819 140187804313408 spec.py:321] Evaluating on the training split.
I0128 16:26:04.270140 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 16:26:13.107528 140187804313408 spec.py:349] Evaluating on the test split.
I0128 16:26:15.696734 140187804313408 submission_runner.py:408] Time since start: 32786.90s, 	Step: 93179, 	{'train/accuracy': 0.7147042155265808, 'train/loss': 1.1151316165924072, 'validation/accuracy': 0.6510599851608276, 'validation/loss': 1.43448007106781, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1194303035736084, 'test/num_examples': 10000, 'score': 31654.537391901016, 'total_duration': 32786.896673202515, 'accumulated_submission_time': 31654.537391901016, 'accumulated_eval_time': 1125.5177392959595, 'accumulated_logging_time': 3.4525933265686035}
I0128 16:26:15.734447 140026159523584 logging_writer.py:48] [93179] accumulated_eval_time=1125.517739, accumulated_logging_time=3.452593, accumulated_submission_time=31654.537392, global_step=93179, preemption_count=0, score=31654.537392, test/accuracy=0.520800, test/loss=2.119430, test/num_examples=10000, total_duration=32786.896673, train/accuracy=0.714704, train/loss=1.115132, validation/accuracy=0.651060, validation/loss=1.434480, validation/num_examples=50000
I0128 16:26:23.188051 140026167916288 logging_writer.py:48] [93200] global_step=93200, grad_norm=4.314663410186768, loss=1.6739840507507324
I0128 16:26:57.059175 140026159523584 logging_writer.py:48] [93300] global_step=93300, grad_norm=5.096919059753418, loss=1.5589072704315186
I0128 16:27:30.939777 140026167916288 logging_writer.py:48] [93400] global_step=93400, grad_norm=4.337584495544434, loss=1.7813678979873657
I0128 16:28:04.803190 140026159523584 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.134040355682373, loss=1.5863982439041138
I0128 16:28:38.734337 140026167916288 logging_writer.py:48] [93600] global_step=93600, grad_norm=4.863105773925781, loss=1.6176351308822632
I0128 16:29:12.601307 140026159523584 logging_writer.py:48] [93700] global_step=93700, grad_norm=4.67780065536499, loss=1.6100417375564575
I0128 16:29:46.583304 140026167916288 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.558877944946289, loss=1.6383973360061646
I0128 16:30:20.456071 140026159523584 logging_writer.py:48] [93900] global_step=93900, grad_norm=5.0608811378479, loss=1.5384522676467896
I0128 16:30:54.392541 140026167916288 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.297628879547119, loss=1.6891067028045654
I0128 16:31:28.295576 140026159523584 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.879094362258911, loss=1.6736918687820435
I0128 16:32:02.194532 140026167916288 logging_writer.py:48] [94200] global_step=94200, grad_norm=5.340499401092529, loss=1.6413774490356445
I0128 16:32:36.096159 140026159523584 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.079944610595703, loss=1.5158451795578003
I0128 16:33:10.007311 140026167916288 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.210705757141113, loss=1.5410816669464111
I0128 16:33:43.921050 140026159523584 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.664632797241211, loss=1.6162493228912354
I0128 16:34:17.804216 140026167916288 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.423232078552246, loss=1.6705626249313354
I0128 16:34:45.731813 140187804313408 spec.py:321] Evaluating on the training split.
I0128 16:34:51.962877 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 16:35:00.772343 140187804313408 spec.py:349] Evaluating on the test split.
I0128 16:35:03.429023 140187804313408 submission_runner.py:408] Time since start: 33314.63s, 	Step: 94684, 	{'train/accuracy': 0.7174146771430969, 'train/loss': 1.0910450220108032, 'validation/accuracy': 0.6616799831390381, 'validation/loss': 1.3794057369232178, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.0857291221618652, 'test/num_examples': 10000, 'score': 32164.470888614655, 'total_duration': 33314.628957271576, 'accumulated_submission_time': 32164.470888614655, 'accumulated_eval_time': 1143.2149093151093, 'accumulated_logging_time': 3.5000839233398438}
I0128 16:35:03.462471 140026058876672 logging_writer.py:48] [94684] accumulated_eval_time=1143.214909, accumulated_logging_time=3.500084, accumulated_submission_time=32164.470889, global_step=94684, preemption_count=0, score=32164.470889, test/accuracy=0.532000, test/loss=2.085729, test/num_examples=10000, total_duration=33314.628957, train/accuracy=0.717415, train/loss=1.091045, validation/accuracy=0.661680, validation/loss=1.379406, validation/num_examples=50000
I0128 16:35:09.213003 140026067269376 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.194375991821289, loss=1.5474929809570312
I0128 16:35:43.025838 140026058876672 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.536416053771973, loss=1.7151708602905273
I0128 16:36:16.928585 140026067269376 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.001585960388184, loss=1.5568231344223022
I0128 16:36:50.850539 140026058876672 logging_writer.py:48] [95000] global_step=95000, grad_norm=4.2955803871154785, loss=1.7067536115646362
I0128 16:37:24.732533 140026067269376 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.060018062591553, loss=1.6525437831878662
I0128 16:37:58.647084 140026058876672 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.7592878341674805, loss=1.5490875244140625
I0128 16:38:32.524243 140026067269376 logging_writer.py:48] [95300] global_step=95300, grad_norm=4.345345497131348, loss=1.5931545495986938
I0128 16:39:06.435584 140026058876672 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.90369176864624, loss=1.6244795322418213
I0128 16:39:40.312451 140026067269376 logging_writer.py:48] [95500] global_step=95500, grad_norm=5.021833896636963, loss=1.552018642425537
I0128 16:40:14.161981 140026058876672 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.5276923179626465, loss=1.5958824157714844
I0128 16:40:48.065583 140026067269376 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.38608980178833, loss=1.6240307092666626
I0128 16:41:21.956151 140026058876672 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.571598052978516, loss=1.6842892169952393
I0128 16:41:55.791274 140026067269376 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.8427793979644775, loss=1.5706043243408203
I0128 16:42:29.746829 140026058876672 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.591556072235107, loss=1.7687394618988037
I0128 16:43:03.605348 140026067269376 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.734516620635986, loss=1.837284803390503
I0128 16:43:33.565576 140187804313408 spec.py:321] Evaluating on the training split.
I0128 16:43:39.797981 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 16:43:48.929903 140187804313408 spec.py:349] Evaluating on the test split.
I0128 16:43:51.523673 140187804313408 submission_runner.py:408] Time since start: 33842.72s, 	Step: 96190, 	{'train/accuracy': 0.7141262888908386, 'train/loss': 1.1053730249404907, 'validation/accuracy': 0.6563599705696106, 'validation/loss': 1.3915393352508545, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.102585792541504, 'test/num_examples': 10000, 'score': 32674.50990009308, 'total_duration': 33842.723615169525, 'accumulated_submission_time': 32674.50990009308, 'accumulated_eval_time': 1161.172973394394, 'accumulated_logging_time': 3.5434374809265137}
I0128 16:43:51.561455 140026058876672 logging_writer.py:48] [96190] accumulated_eval_time=1161.172973, accumulated_logging_time=3.543437, accumulated_submission_time=32674.509900, global_step=96190, preemption_count=0, score=32674.509900, test/accuracy=0.530500, test/loss=2.102586, test/num_examples=10000, total_duration=33842.723615, train/accuracy=0.714126, train/loss=1.105373, validation/accuracy=0.656360, validation/loss=1.391539, validation/num_examples=50000
I0128 16:43:55.283431 140026151130880 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.515214443206787, loss=1.7052539587020874
I0128 16:44:29.098566 140026058876672 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.705753803253174, loss=1.688281536102295
I0128 16:45:02.955768 140026151130880 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.966718673706055, loss=1.5862438678741455
I0128 16:45:36.803000 140026058876672 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.6403326988220215, loss=1.6407517194747925
I0128 16:46:10.712502 140026151130880 logging_writer.py:48] [96600] global_step=96600, grad_norm=4.76282262802124, loss=1.752946138381958
I0128 16:46:44.592796 140026058876672 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.961684226989746, loss=1.6646263599395752
I0128 16:47:18.492850 140026151130880 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.622804641723633, loss=1.5576286315917969
I0128 16:47:52.403810 140026058876672 logging_writer.py:48] [96900] global_step=96900, grad_norm=4.756868362426758, loss=1.6201345920562744
I0128 16:48:26.289076 140026151130880 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.9667835235595703, loss=1.6154959201812744
I0128 16:49:00.294610 140026058876672 logging_writer.py:48] [97100] global_step=97100, grad_norm=4.9463701248168945, loss=1.6275724172592163
I0128 16:49:34.174570 140026151130880 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.527280807495117, loss=1.56948983669281
I0128 16:50:08.039087 140026058876672 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.6452836990356445, loss=1.6054575443267822
I0128 16:50:41.933379 140026151130880 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.73781156539917, loss=1.597280502319336
I0128 16:51:15.824630 140026058876672 logging_writer.py:48] [97500] global_step=97500, grad_norm=4.3658976554870605, loss=1.451035976409912
I0128 16:51:49.728984 140026151130880 logging_writer.py:48] [97600] global_step=97600, grad_norm=4.909249782562256, loss=1.7213478088378906
I0128 16:52:21.766770 140187804313408 spec.py:321] Evaluating on the training split.
I0128 16:52:27.985844 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 16:52:36.987684 140187804313408 spec.py:349] Evaluating on the test split.
I0128 16:52:39.585239 140187804313408 submission_runner.py:408] Time since start: 34370.79s, 	Step: 97696, 	{'train/accuracy': 0.7200254797935486, 'train/loss': 1.0727653503417969, 'validation/accuracy': 0.6601399779319763, 'validation/loss': 1.3852746486663818, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.091149091720581, 'test/num_examples': 10000, 'score': 33184.651337623596, 'total_duration': 34370.78517818451, 'accumulated_submission_time': 33184.651337623596, 'accumulated_eval_time': 1178.9914045333862, 'accumulated_logging_time': 3.5905425548553467}
I0128 16:52:39.619872 140026058876672 logging_writer.py:48] [97696] accumulated_eval_time=1178.991405, accumulated_logging_time=3.590543, accumulated_submission_time=33184.651338, global_step=97696, preemption_count=0, score=33184.651338, test/accuracy=0.534400, test/loss=2.091149, test/num_examples=10000, total_duration=34370.785178, train/accuracy=0.720025, train/loss=1.072765, validation/accuracy=0.660140, validation/loss=1.385275, validation/num_examples=50000
I0128 16:52:41.318478 140026067269376 logging_writer.py:48] [97700] global_step=97700, grad_norm=5.473799705505371, loss=1.610548496246338
I0128 16:53:15.095642 140026058876672 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.799471855163574, loss=1.68998122215271
I0128 16:53:48.961522 140026067269376 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.697571277618408, loss=1.5894474983215332
I0128 16:54:22.813395 140026058876672 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.606894493103027, loss=1.6985304355621338
I0128 16:54:56.720768 140026067269376 logging_writer.py:48] [98100] global_step=98100, grad_norm=5.109350204467773, loss=1.6273913383483887
I0128 16:55:30.671024 140026058876672 logging_writer.py:48] [98200] global_step=98200, grad_norm=4.136769771575928, loss=1.701130986213684
I0128 16:56:04.550830 140026067269376 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.700713634490967, loss=1.5807037353515625
I0128 16:56:38.448290 140026058876672 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.544118404388428, loss=1.458316445350647
I0128 16:57:12.346759 140026067269376 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.357647895812988, loss=1.7119777202606201
I0128 16:57:46.225711 140026058876672 logging_writer.py:48] [98600] global_step=98600, grad_norm=5.268181800842285, loss=1.6769579648971558
I0128 16:58:20.129155 140026067269376 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.292414665222168, loss=1.6033217906951904
I0128 16:58:54.038167 140026058876672 logging_writer.py:48] [98800] global_step=98800, grad_norm=5.073542594909668, loss=1.6852118968963623
I0128 16:59:27.898928 140026067269376 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.586258411407471, loss=1.702525019645691
I0128 17:00:01.833625 140026058876672 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.306013584136963, loss=1.449323058128357
I0128 17:00:35.672414 140026067269376 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.281605243682861, loss=1.4955796003341675
I0128 17:01:09.570318 140026058876672 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.375459671020508, loss=1.5660802125930786
I0128 17:01:09.727294 140187804313408 spec.py:321] Evaluating on the training split.
I0128 17:01:15.918035 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 17:01:25.054977 140187804313408 spec.py:349] Evaluating on the test split.
I0128 17:01:27.653669 140187804313408 submission_runner.py:408] Time since start: 34898.85s, 	Step: 99202, 	{'train/accuracy': 0.7228953838348389, 'train/loss': 1.0721486806869507, 'validation/accuracy': 0.6584399938583374, 'validation/loss': 1.3808057308197021, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.0861809253692627, 'test/num_examples': 10000, 'score': 33694.69447398186, 'total_duration': 34898.85359764099, 'accumulated_submission_time': 33694.69447398186, 'accumulated_eval_time': 1196.9177355766296, 'accumulated_logging_time': 3.6347944736480713}
I0128 17:01:27.691404 140026050483968 logging_writer.py:48] [99202] accumulated_eval_time=1196.917736, accumulated_logging_time=3.634794, accumulated_submission_time=33694.694474, global_step=99202, preemption_count=0, score=33694.694474, test/accuracy=0.530600, test/loss=2.086181, test/num_examples=10000, total_duration=34898.853598, train/accuracy=0.722895, train/loss=1.072149, validation/accuracy=0.658440, validation/loss=1.380806, validation/num_examples=50000
I0128 17:02:01.176674 140026159523584 logging_writer.py:48] [99300] global_step=99300, grad_norm=5.327343940734863, loss=1.756606101989746
I0128 17:02:35.068635 140026050483968 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.762598037719727, loss=1.661604642868042
I0128 17:03:08.980377 140026159523584 logging_writer.py:48] [99500] global_step=99500, grad_norm=4.820123672485352, loss=1.5935264825820923
I0128 17:03:42.897602 140026050483968 logging_writer.py:48] [99600] global_step=99600, grad_norm=4.331421375274658, loss=1.605786919593811
I0128 17:04:16.772548 140026159523584 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.819729328155518, loss=1.6632776260375977
I0128 17:04:50.669684 140026050483968 logging_writer.py:48] [99800] global_step=99800, grad_norm=4.034733772277832, loss=1.5679357051849365
I0128 17:05:24.573329 140026159523584 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.302572727203369, loss=1.538844347000122
I0128 17:05:58.491055 140026050483968 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.37049674987793, loss=1.6202049255371094
I0128 17:06:32.360715 140026159523584 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.81217622756958, loss=1.534902811050415
I0128 17:07:06.223767 140026050483968 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.581709384918213, loss=1.502173662185669
I0128 17:07:40.220808 140026159523584 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.661375045776367, loss=1.603001356124878
I0128 17:08:14.157104 140026050483968 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.656277656555176, loss=1.5596468448638916
I0128 17:08:48.084511 140026159523584 logging_writer.py:48] [100500] global_step=100500, grad_norm=4.369365692138672, loss=1.6153223514556885
I0128 17:09:21.938493 140026050483968 logging_writer.py:48] [100600] global_step=100600, grad_norm=5.132881164550781, loss=1.544737696647644
I0128 17:09:55.875054 140026159523584 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.914919376373291, loss=1.5579886436462402
I0128 17:09:57.715722 140187804313408 spec.py:321] Evaluating on the training split.
I0128 17:10:04.199644 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 17:10:12.881226 140187804313408 spec.py:349] Evaluating on the test split.
I0128 17:10:15.512984 140187804313408 submission_runner.py:408] Time since start: 35426.71s, 	Step: 100707, 	{'train/accuracy': 0.7236925959587097, 'train/loss': 1.0682148933410645, 'validation/accuracy': 0.6428999900817871, 'validation/loss': 1.46091890335083, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.180105209350586, 'test/num_examples': 10000, 'score': 34204.65418744087, 'total_duration': 35426.71285367012, 'accumulated_submission_time': 34204.65418744087, 'accumulated_eval_time': 1214.7148866653442, 'accumulated_logging_time': 3.682264804840088}
I0128 17:10:15.547808 140026075662080 logging_writer.py:48] [100707] accumulated_eval_time=1214.714887, accumulated_logging_time=3.682265, accumulated_submission_time=34204.654187, global_step=100707, preemption_count=0, score=34204.654187, test/accuracy=0.521300, test/loss=2.180105, test/num_examples=10000, total_duration=35426.712854, train/accuracy=0.723693, train/loss=1.068215, validation/accuracy=0.642900, validation/loss=1.460919, validation/num_examples=50000
I0128 17:10:47.349156 140026151130880 logging_writer.py:48] [100800] global_step=100800, grad_norm=5.01460075378418, loss=1.6121655702590942
I0128 17:11:21.190617 140026075662080 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.26674747467041, loss=1.6595721244812012
I0128 17:11:55.088641 140026151130880 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.619227886199951, loss=1.6144263744354248
I0128 17:12:28.930149 140026075662080 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.881141185760498, loss=1.6340302228927612
I0128 17:13:02.840225 140026151130880 logging_writer.py:48] [101200] global_step=101200, grad_norm=5.669559478759766, loss=1.5164285898208618
I0128 17:13:36.736447 140026075662080 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.885591983795166, loss=1.538552165031433
I0128 17:14:10.848942 140026151130880 logging_writer.py:48] [101400] global_step=101400, grad_norm=5.015382289886475, loss=1.6264300346374512
I0128 17:14:44.745198 140026075662080 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.320418357849121, loss=1.5446691513061523
I0128 17:15:18.650430 140026151130880 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.352453231811523, loss=1.5311388969421387
I0128 17:15:52.514387 140026075662080 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.402214527130127, loss=1.6721081733703613
I0128 17:16:26.358385 140026151130880 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.853808403015137, loss=1.5729209184646606
I0128 17:17:00.269389 140026075662080 logging_writer.py:48] [101900] global_step=101900, grad_norm=5.130868911743164, loss=1.7016810178756714
I0128 17:17:34.174800 140026151130880 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.727898120880127, loss=1.6464840173721313
I0128 17:18:08.064164 140026075662080 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.2771315574646, loss=1.4481984376907349
I0128 17:18:41.972510 140026151130880 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.7229180335998535, loss=1.6171247959136963
I0128 17:18:45.840316 140187804313408 spec.py:321] Evaluating on the training split.
I0128 17:18:52.108902 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 17:19:00.891617 140187804313408 spec.py:349] Evaluating on the test split.
I0128 17:19:03.748732 140187804313408 submission_runner.py:408] Time since start: 35954.95s, 	Step: 102213, 	{'train/accuracy': 0.7354512214660645, 'train/loss': 1.0134378671646118, 'validation/accuracy': 0.6638399958610535, 'validation/loss': 1.3594108819961548, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.092923641204834, 'test/num_examples': 10000, 'score': 34714.8821105957, 'total_duration': 35954.948600530624, 'accumulated_submission_time': 34714.8821105957, 'accumulated_eval_time': 1232.6231932640076, 'accumulated_logging_time': 3.7265255451202393}
I0128 17:19:03.783580 140026067269376 logging_writer.py:48] [102213] accumulated_eval_time=1232.623193, accumulated_logging_time=3.726526, accumulated_submission_time=34714.882111, global_step=102213, preemption_count=0, score=34714.882111, test/accuracy=0.533000, test/loss=2.092924, test/num_examples=10000, total_duration=35954.948601, train/accuracy=0.735451, train/loss=1.013438, validation/accuracy=0.663840, validation/loss=1.359411, validation/num_examples=50000
I0128 17:19:33.567611 140026159523584 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.767276763916016, loss=1.6029784679412842
I0128 17:20:07.433891 140026067269376 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.187854290008545, loss=1.4456981420516968
I0128 17:20:41.490800 140026159523584 logging_writer.py:48] [102500] global_step=102500, grad_norm=5.042388439178467, loss=1.5119837522506714
I0128 17:21:15.400059 140026067269376 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.761758327484131, loss=1.451766848564148
I0128 17:21:49.279922 140026159523584 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.281914234161377, loss=1.4484152793884277
I0128 17:22:23.211041 140026067269376 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.41969108581543, loss=1.6514809131622314
I0128 17:22:57.086168 140026159523584 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.779642105102539, loss=1.6457936763763428
I0128 17:23:31.018997 140026067269376 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.27401876449585, loss=1.4707200527191162
I0128 17:24:04.885948 140026159523584 logging_writer.py:48] [103100] global_step=103100, grad_norm=5.227410316467285, loss=1.602800726890564
I0128 17:24:38.787728 140026067269376 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.3931403160095215, loss=1.5406290292739868
I0128 17:25:12.694013 140026159523584 logging_writer.py:48] [103300] global_step=103300, grad_norm=4.79512357711792, loss=1.5278657674789429
I0128 17:25:46.601124 140026067269376 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.4577412605285645, loss=1.443532943725586
I0128 17:26:20.462493 140026159523584 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.670335292816162, loss=1.6720911264419556
I0128 17:26:54.552854 140026067269376 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.259428024291992, loss=1.5985382795333862
I0128 17:27:28.433799 140026159523584 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.701435565948486, loss=1.6504828929901123
I0128 17:27:34.020161 140187804313408 spec.py:321] Evaluating on the training split.
I0128 17:27:40.302206 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 17:27:49.066600 140187804313408 spec.py:349] Evaluating on the test split.
I0128 17:27:51.634662 140187804313408 submission_runner.py:408] Time since start: 36482.83s, 	Step: 103718, 	{'train/accuracy': 0.735371470451355, 'train/loss': 1.017613410949707, 'validation/accuracy': 0.6706799864768982, 'validation/loss': 1.3379477262496948, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.0400118827819824, 'test/num_examples': 10000, 'score': 35225.05365109444, 'total_duration': 36482.834594249725, 'accumulated_submission_time': 35225.05365109444, 'accumulated_eval_time': 1250.2376444339752, 'accumulated_logging_time': 3.770775318145752}
I0128 17:27:51.671741 140026075662080 logging_writer.py:48] [103718] accumulated_eval_time=1250.237644, accumulated_logging_time=3.770775, accumulated_submission_time=35225.053651, global_step=103718, preemption_count=0, score=35225.053651, test/accuracy=0.544500, test/loss=2.040012, test/num_examples=10000, total_duration=36482.834594, train/accuracy=0.735371, train/loss=1.017613, validation/accuracy=0.670680, validation/loss=1.337948, validation/num_examples=50000
I0128 17:28:19.755557 140026151130880 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.330952167510986, loss=1.4937083721160889
I0128 17:28:53.585852 140026075662080 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.984796524047852, loss=1.56337308883667
I0128 17:29:27.469787 140026151130880 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.523471355438232, loss=1.6182913780212402
I0128 17:30:01.322415 140026075662080 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.400301456451416, loss=1.615856647491455
I0128 17:30:35.185142 140026151130880 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.041545391082764, loss=1.517077088356018
I0128 17:31:09.044062 140026075662080 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.6171979904174805, loss=1.521385908126831
I0128 17:31:42.949815 140026151130880 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.9873948097229, loss=1.6568632125854492
I0128 17:32:16.841872 140026075662080 logging_writer.py:48] [104500] global_step=104500, grad_norm=5.088244915008545, loss=1.6368980407714844
I0128 17:32:50.726865 140026151130880 logging_writer.py:48] [104600] global_step=104600, grad_norm=5.49562931060791, loss=1.4888869524002075
I0128 17:33:24.743326 140026075662080 logging_writer.py:48] [104700] global_step=104700, grad_norm=5.549114227294922, loss=1.6385231018066406
I0128 17:33:58.638618 140026151130880 logging_writer.py:48] [104800] global_step=104800, grad_norm=5.2109479904174805, loss=1.5777804851531982
I0128 17:34:32.481211 140026075662080 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.289334297180176, loss=1.4984416961669922
I0128 17:35:06.396235 140026151130880 logging_writer.py:48] [105000] global_step=105000, grad_norm=5.114864349365234, loss=1.559117078781128
I0128 17:35:40.275538 140026075662080 logging_writer.py:48] [105100] global_step=105100, grad_norm=5.067442417144775, loss=1.5994231700897217
I0128 17:36:14.132702 140026151130880 logging_writer.py:48] [105200] global_step=105200, grad_norm=5.136852264404297, loss=1.5194565057754517
I0128 17:36:21.761384 140187804313408 spec.py:321] Evaluating on the training split.
I0128 17:36:27.998525 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 17:36:36.882064 140187804313408 spec.py:349] Evaluating on the test split.
I0128 17:36:39.430889 140187804313408 submission_runner.py:408] Time since start: 37010.63s, 	Step: 105224, 	{'train/accuracy': 0.7281967401504517, 'train/loss': 1.0495648384094238, 'validation/accuracy': 0.6665599942207336, 'validation/loss': 1.3577643632888794, 'validation/num_examples': 50000, 'test/accuracy': 0.5378000140190125, 'test/loss': 2.0792884826660156, 'test/num_examples': 10000, 'score': 35735.07813715935, 'total_duration': 37010.63081288338, 'accumulated_submission_time': 35735.07813715935, 'accumulated_eval_time': 1267.9070928096771, 'accumulated_logging_time': 3.817821502685547}
I0128 17:36:39.470380 140026058876672 logging_writer.py:48] [105224] accumulated_eval_time=1267.907093, accumulated_logging_time=3.817822, accumulated_submission_time=35735.078137, global_step=105224, preemption_count=0, score=35735.078137, test/accuracy=0.537800, test/loss=2.079288, test/num_examples=10000, total_duration=37010.630813, train/accuracy=0.728197, train/loss=1.049565, validation/accuracy=0.666560, validation/loss=1.357764, validation/num_examples=50000
I0128 17:37:05.544950 140026067269376 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.803035736083984, loss=1.6781301498413086
I0128 17:37:39.359960 140026058876672 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.967028617858887, loss=1.5434505939483643
I0128 17:38:13.212282 140026067269376 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.1735005378723145, loss=1.4358572959899902
I0128 17:38:47.103726 140026058876672 logging_writer.py:48] [105600] global_step=105600, grad_norm=4.685894966125488, loss=1.546987533569336
I0128 17:39:20.988042 140026067269376 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.924880027770996, loss=1.5368162393569946
I0128 17:39:54.935174 140026058876672 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.285521030426025, loss=1.562310814857483
I0128 17:40:28.848074 140026067269376 logging_writer.py:48] [105900] global_step=105900, grad_norm=5.069380760192871, loss=1.6111443042755127
I0128 17:41:02.735757 140026058876672 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.859341144561768, loss=1.5158774852752686
I0128 17:41:36.622167 140026067269376 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.643144607543945, loss=1.4759740829467773
I0128 17:42:10.527675 140026058876672 logging_writer.py:48] [106200] global_step=106200, grad_norm=5.096030235290527, loss=1.5523496866226196
I0128 17:42:44.435706 140026067269376 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.735017776489258, loss=1.6450080871582031
I0128 17:43:18.299043 140026058876672 logging_writer.py:48] [106400] global_step=106400, grad_norm=5.043697834014893, loss=1.5491273403167725
I0128 17:43:52.202586 140026067269376 logging_writer.py:48] [106500] global_step=106500, grad_norm=5.309418201446533, loss=1.4897615909576416
I0128 17:44:26.112297 140026058876672 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.583703994750977, loss=1.6307439804077148
I0128 17:44:59.995461 140026067269376 logging_writer.py:48] [106700] global_step=106700, grad_norm=5.449843883514404, loss=1.5323572158813477
I0128 17:45:09.631061 140187804313408 spec.py:321] Evaluating on the training split.
I0128 17:45:15.889665 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 17:45:24.787503 140187804313408 spec.py:349] Evaluating on the test split.
I0128 17:45:27.397535 140187804313408 submission_runner.py:408] Time since start: 37538.60s, 	Step: 106730, 	{'train/accuracy': 0.7329201102256775, 'train/loss': 1.0269001722335815, 'validation/accuracy': 0.6684399843215942, 'validation/loss': 1.3400194644927979, 'validation/num_examples': 50000, 'test/accuracy': 0.5428000092506409, 'test/loss': 2.031836748123169, 'test/num_examples': 10000, 'score': 36245.17560458183, 'total_duration': 37538.59745979309, 'accumulated_submission_time': 36245.17560458183, 'accumulated_eval_time': 1285.6735136508942, 'accumulated_logging_time': 3.8665313720703125}
I0128 17:45:27.432002 140026075662080 logging_writer.py:48] [106730] accumulated_eval_time=1285.673514, accumulated_logging_time=3.866531, accumulated_submission_time=36245.175605, global_step=106730, preemption_count=0, score=36245.175605, test/accuracy=0.542800, test/loss=2.031837, test/num_examples=10000, total_duration=37538.597460, train/accuracy=0.732920, train/loss=1.026900, validation/accuracy=0.668440, validation/loss=1.340019, validation/num_examples=50000
I0128 17:45:51.440432 140026151130880 logging_writer.py:48] [106800] global_step=106800, grad_norm=4.540598392486572, loss=1.5689589977264404
I0128 17:46:25.376372 140026075662080 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.892500400543213, loss=1.5356624126434326
I0128 17:46:59.267036 140026151130880 logging_writer.py:48] [107000] global_step=107000, grad_norm=5.4769158363342285, loss=1.6405131816864014
I0128 17:47:33.098368 140026075662080 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.773143768310547, loss=1.4802463054656982
I0128 17:48:07.004040 140026151130880 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.7632341384887695, loss=1.5930520296096802
I0128 17:48:40.897136 140026075662080 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.871631622314453, loss=1.468904972076416
I0128 17:49:14.752328 140026151130880 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.276195049285889, loss=1.4882304668426514
I0128 17:49:48.608433 140026075662080 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.402692794799805, loss=1.5425082445144653
I0128 17:50:22.493221 140026151130880 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.912901878356934, loss=1.5860599279403687
I0128 17:50:56.382184 140026075662080 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.759344577789307, loss=1.5616388320922852
I0128 17:51:30.254601 140026151130880 logging_writer.py:48] [107800] global_step=107800, grad_norm=5.1183648109436035, loss=1.6352849006652832
I0128 17:52:04.148380 140026075662080 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.53396463394165, loss=1.4195871353149414
I0128 17:52:38.116001 140026151130880 logging_writer.py:48] [108000] global_step=108000, grad_norm=5.08882474899292, loss=1.680633783340454
I0128 17:53:12.003794 140026075662080 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.5971479415893555, loss=1.5310043096542358
I0128 17:53:45.922091 140026151130880 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.817286014556885, loss=1.553678274154663
I0128 17:53:57.564983 140187804313408 spec.py:321] Evaluating on the training split.
I0128 17:54:04.085787 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 17:54:12.767686 140187804313408 spec.py:349] Evaluating on the test split.
I0128 17:54:15.386457 140187804313408 submission_runner.py:408] Time since start: 38066.59s, 	Step: 108236, 	{'train/accuracy': 0.7391581535339355, 'train/loss': 1.0007308721542358, 'validation/accuracy': 0.6759799718856812, 'validation/loss': 1.3103091716766357, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.014052152633667, 'test/num_examples': 10000, 'score': 36755.24262714386, 'total_duration': 38066.58639526367, 'accumulated_submission_time': 36755.24262714386, 'accumulated_eval_time': 1303.4949452877045, 'accumulated_logging_time': 3.911306619644165}
I0128 17:54:15.422206 140026067269376 logging_writer.py:48] [108236] accumulated_eval_time=1303.494945, accumulated_logging_time=3.911307, accumulated_submission_time=36755.242627, global_step=108236, preemption_count=0, score=36755.242627, test/accuracy=0.546700, test/loss=2.014052, test/num_examples=10000, total_duration=38066.586395, train/accuracy=0.739158, train/loss=1.000731, validation/accuracy=0.675980, validation/loss=1.310309, validation/num_examples=50000
I0128 17:54:37.433158 140026159523584 logging_writer.py:48] [108300] global_step=108300, grad_norm=5.226675033569336, loss=1.6011934280395508
I0128 17:55:11.322486 140026067269376 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.742300033569336, loss=1.5625200271606445
I0128 17:55:45.224648 140026159523584 logging_writer.py:48] [108500] global_step=108500, grad_norm=5.032278537750244, loss=1.5453786849975586
I0128 17:56:19.114288 140026067269376 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.7390594482421875, loss=1.4268591403961182
I0128 17:56:53.000208 140026159523584 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.819088459014893, loss=1.5631386041641235
I0128 17:57:26.882987 140026067269376 logging_writer.py:48] [108800] global_step=108800, grad_norm=5.235992431640625, loss=1.6187934875488281
I0128 17:58:00.791056 140026159523584 logging_writer.py:48] [108900] global_step=108900, grad_norm=5.267308712005615, loss=1.486107587814331
I0128 17:58:34.772712 140026067269376 logging_writer.py:48] [109000] global_step=109000, grad_norm=4.9146270751953125, loss=1.4848308563232422
I0128 17:59:08.650269 140026159523584 logging_writer.py:48] [109100] global_step=109100, grad_norm=5.109212875366211, loss=1.5758404731750488
I0128 17:59:42.557842 140026067269376 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.385088920593262, loss=1.539306640625
I0128 18:00:16.438159 140026159523584 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.973216533660889, loss=1.4757187366485596
I0128 18:00:50.361452 140026067269376 logging_writer.py:48] [109400] global_step=109400, grad_norm=6.145225524902344, loss=1.5389255285263062
I0128 18:01:24.232025 140026159523584 logging_writer.py:48] [109500] global_step=109500, grad_norm=5.236406326293945, loss=1.5299638509750366
I0128 18:01:58.148832 140026067269376 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.772270679473877, loss=1.4822475910186768
I0128 18:02:32.035773 140026159523584 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.903363227844238, loss=1.5844957828521729
I0128 18:02:45.391664 140187804313408 spec.py:321] Evaluating on the training split.
I0128 18:02:51.577543 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 18:03:00.326269 140187804313408 spec.py:349] Evaluating on the test split.
I0128 18:03:02.981135 140187804313408 submission_runner.py:408] Time since start: 38594.18s, 	Step: 109741, 	{'train/accuracy': 0.7607222199440002, 'train/loss': 0.9054629802703857, 'validation/accuracy': 0.6724399924278259, 'validation/loss': 1.3212249279022217, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.0401992797851562, 'test/num_examples': 10000, 'score': 37265.14801168442, 'total_duration': 38594.18107557297, 'accumulated_submission_time': 37265.14801168442, 'accumulated_eval_time': 1321.0843846797943, 'accumulated_logging_time': 3.95632004737854}
I0128 18:03:03.021699 140026050483968 logging_writer.py:48] [109741] accumulated_eval_time=1321.084385, accumulated_logging_time=3.956320, accumulated_submission_time=37265.148012, global_step=109741, preemption_count=0, score=37265.148012, test/accuracy=0.546100, test/loss=2.040199, test/num_examples=10000, total_duration=38594.181076, train/accuracy=0.760722, train/loss=0.905463, validation/accuracy=0.672440, validation/loss=1.321225, validation/num_examples=50000
I0128 18:03:23.323741 140026058876672 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.91078519821167, loss=1.5780531167984009
I0128 18:03:57.181150 140026050483968 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.474899768829346, loss=1.459240198135376
I0128 18:04:31.035464 140026058876672 logging_writer.py:48] [110000] global_step=110000, grad_norm=5.587214469909668, loss=1.5144000053405762
I0128 18:05:05.099415 140026050483968 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.674549579620361, loss=1.4444470405578613
I0128 18:05:38.990031 140026058876672 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.827430248260498, loss=1.4369202852249146
I0128 18:06:12.872989 140026050483968 logging_writer.py:48] [110300] global_step=110300, grad_norm=5.373266220092773, loss=1.4944339990615845
I0128 18:06:46.773093 140026058876672 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.847867965698242, loss=1.5836327075958252
I0128 18:07:20.683339 140026050483968 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.744533538818359, loss=1.5184475183486938
I0128 18:07:54.566445 140026058876672 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.829450607299805, loss=1.614710807800293
I0128 18:08:28.479488 140026050483968 logging_writer.py:48] [110700] global_step=110700, grad_norm=5.217334270477295, loss=1.5305505990982056
I0128 18:09:02.385982 140026058876672 logging_writer.py:48] [110800] global_step=110800, grad_norm=5.0372700691223145, loss=1.5405548810958862
I0128 18:09:36.285997 140026050483968 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.744042873382568, loss=1.5763659477233887
I0128 18:10:10.191650 140026058876672 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.8123674392700195, loss=1.453019142150879
I0128 18:10:44.082707 140026050483968 logging_writer.py:48] [111100] global_step=111100, grad_norm=5.016401290893555, loss=1.535760760307312
I0128 18:11:18.110862 140026058876672 logging_writer.py:48] [111200] global_step=111200, grad_norm=5.535284519195557, loss=1.6660164594650269
I0128 18:11:33.143806 140187804313408 spec.py:321] Evaluating on the training split.
I0128 18:11:39.432951 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 18:11:48.396188 140187804313408 spec.py:349] Evaluating on the test split.
I0128 18:11:51.028665 140187804313408 submission_runner.py:408] Time since start: 39122.23s, 	Step: 111246, 	{'train/accuracy': 0.7552016973495483, 'train/loss': 0.9197171330451965, 'validation/accuracy': 0.6800199747085571, 'validation/loss': 1.3009675741195679, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.99822998046875, 'test/num_examples': 10000, 'score': 37775.20650601387, 'total_duration': 39122.22860813141, 'accumulated_submission_time': 37775.20650601387, 'accumulated_eval_time': 1338.969208240509, 'accumulated_logging_time': 4.006459951400757}
I0128 18:11:51.066432 140026050483968 logging_writer.py:48] [111246] accumulated_eval_time=1338.969208, accumulated_logging_time=4.006460, accumulated_submission_time=37775.206506, global_step=111246, preemption_count=0, score=37775.206506, test/accuracy=0.558400, test/loss=1.998230, test/num_examples=10000, total_duration=39122.228608, train/accuracy=0.755202, train/loss=0.919717, validation/accuracy=0.680020, validation/loss=1.300968, validation/num_examples=50000
I0128 18:12:09.697771 140026159523584 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.847781181335449, loss=1.5152792930603027
I0128 18:12:43.518086 140026050483968 logging_writer.py:48] [111400] global_step=111400, grad_norm=5.031190395355225, loss=1.3796014785766602
I0128 18:13:17.411898 140026159523584 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.582291603088379, loss=1.4423776865005493
I0128 18:13:51.301759 140026050483968 logging_writer.py:48] [111600] global_step=111600, grad_norm=5.134859561920166, loss=1.5128281116485596
I0128 18:14:25.186074 140026159523584 logging_writer.py:48] [111700] global_step=111700, grad_norm=5.0180253982543945, loss=1.4924918413162231
I0128 18:14:59.091163 140026050483968 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.981108665466309, loss=1.463110089302063
I0128 18:15:32.979813 140026159523584 logging_writer.py:48] [111900] global_step=111900, grad_norm=5.011724948883057, loss=1.5053768157958984
I0128 18:16:06.884274 140026050483968 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.80751895904541, loss=1.5920212268829346
I0128 18:16:40.745351 140026159523584 logging_writer.py:48] [112100] global_step=112100, grad_norm=5.18731689453125, loss=1.5877519845962524
I0128 18:17:14.654901 140026050483968 logging_writer.py:48] [112200] global_step=112200, grad_norm=5.274477005004883, loss=1.3320084810256958
I0128 18:17:48.596150 140026159523584 logging_writer.py:48] [112300] global_step=112300, grad_norm=5.231820106506348, loss=1.468145489692688
I0128 18:18:22.515646 140026050483968 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.6999688148498535, loss=1.5621634721755981
I0128 18:18:56.408978 140026159523584 logging_writer.py:48] [112500] global_step=112500, grad_norm=5.458571434020996, loss=1.545501708984375
I0128 18:19:30.324355 140026050483968 logging_writer.py:48] [112600] global_step=112600, grad_norm=5.730482578277588, loss=1.4810187816619873
I0128 18:20:04.253095 140026159523584 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.585943698883057, loss=1.4330356121063232
I0128 18:20:21.354753 140187804313408 spec.py:321] Evaluating on the training split.
I0128 18:20:27.544876 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 18:20:36.526988 140187804313408 spec.py:349] Evaluating on the test split.
I0128 18:20:39.119766 140187804313408 submission_runner.py:408] Time since start: 39650.32s, 	Step: 112752, 	{'train/accuracy': 0.752949595451355, 'train/loss': 0.9380612969398499, 'validation/accuracy': 0.6771599650382996, 'validation/loss': 1.3039065599441528, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.019953727722168, 'test/num_examples': 10000, 'score': 38285.428384542465, 'total_duration': 39650.31968307495, 'accumulated_submission_time': 38285.428384542465, 'accumulated_eval_time': 1356.734162569046, 'accumulated_logging_time': 4.056137800216675}
I0128 18:20:39.170002 140026067269376 logging_writer.py:48] [112752] accumulated_eval_time=1356.734163, accumulated_logging_time=4.056138, accumulated_submission_time=38285.428385, global_step=112752, preemption_count=0, score=38285.428385, test/accuracy=0.551000, test/loss=2.019954, test/num_examples=10000, total_duration=39650.319683, train/accuracy=0.752950, train/loss=0.938061, validation/accuracy=0.677160, validation/loss=1.303907, validation/num_examples=50000
I0128 18:20:55.773295 140026075662080 logging_writer.py:48] [112800] global_step=112800, grad_norm=5.298062801361084, loss=1.5545216798782349
I0128 18:21:29.593003 140026067269376 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.971653461456299, loss=1.5719248056411743
I0128 18:22:03.458942 140026075662080 logging_writer.py:48] [113000] global_step=113000, grad_norm=5.163416385650635, loss=1.527787208557129
I0128 18:22:37.337347 140026067269376 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.849220275878906, loss=1.5613746643066406
I0128 18:23:11.216260 140026075662080 logging_writer.py:48] [113200] global_step=113200, grad_norm=5.126414775848389, loss=1.4156591892242432
I0128 18:23:45.121643 140026067269376 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.845466136932373, loss=1.4081000089645386
I0128 18:24:19.074460 140026075662080 logging_writer.py:48] [113400] global_step=113400, grad_norm=5.025465965270996, loss=1.5128259658813477
I0128 18:24:52.994409 140026067269376 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.970951080322266, loss=1.6221388578414917
I0128 18:25:26.885931 140026075662080 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.74460506439209, loss=1.3945597410202026
I0128 18:26:00.787113 140026067269376 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.926037311553955, loss=1.5154249668121338
I0128 18:26:34.711211 140026075662080 logging_writer.py:48] [113800] global_step=113800, grad_norm=5.13173246383667, loss=1.4164085388183594
I0128 18:27:08.585415 140026067269376 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.9930830001831055, loss=1.4929280281066895
I0128 18:27:42.496626 140026075662080 logging_writer.py:48] [114000] global_step=114000, grad_norm=5.076472282409668, loss=1.401296854019165
I0128 18:28:16.375802 140026067269376 logging_writer.py:48] [114100] global_step=114100, grad_norm=5.660284996032715, loss=1.4560153484344482
I0128 18:28:50.243829 140026075662080 logging_writer.py:48] [114200] global_step=114200, grad_norm=6.040534496307373, loss=1.6244274377822876
I0128 18:29:09.356020 140187804313408 spec.py:321] Evaluating on the training split.
I0128 18:29:15.544239 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 18:29:24.519146 140187804313408 spec.py:349] Evaluating on the test split.
I0128 18:29:27.011209 140187804313408 submission_runner.py:408] Time since start: 40178.21s, 	Step: 114258, 	{'train/accuracy': 0.7528300285339355, 'train/loss': 0.9420802593231201, 'validation/accuracy': 0.6818199753761292, 'validation/loss': 1.2975364923477173, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 1.993594765663147, 'test/num_examples': 10000, 'score': 38795.5488409996, 'total_duration': 40178.211097717285, 'accumulated_submission_time': 38795.5488409996, 'accumulated_eval_time': 1374.3892624378204, 'accumulated_logging_time': 4.116119623184204}
I0128 18:29:27.050993 140026159523584 logging_writer.py:48] [114258] accumulated_eval_time=1374.389262, accumulated_logging_time=4.116120, accumulated_submission_time=38795.548841, global_step=114258, preemption_count=0, score=38795.548841, test/accuracy=0.558100, test/loss=1.993595, test/num_examples=10000, total_duration=40178.211098, train/accuracy=0.752830, train/loss=0.942080, validation/accuracy=0.681820, validation/loss=1.297536, validation/num_examples=50000
I0128 18:29:41.655677 140026167916288 logging_writer.py:48] [114300] global_step=114300, grad_norm=5.5667314529418945, loss=1.431624174118042
I0128 18:30:15.494777 140026159523584 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.770901679992676, loss=1.4813652038574219
I0128 18:30:49.469684 140026167916288 logging_writer.py:48] [114500] global_step=114500, grad_norm=5.091893196105957, loss=1.4900155067443848
I0128 18:31:23.368695 140026159523584 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.7142791748046875, loss=1.5111849308013916
I0128 18:31:57.245628 140026167916288 logging_writer.py:48] [114700] global_step=114700, grad_norm=5.1710124015808105, loss=1.5689311027526855
I0128 18:32:31.121928 140026159523584 logging_writer.py:48] [114800] global_step=114800, grad_norm=5.449605464935303, loss=1.5683891773223877
I0128 18:33:05.018893 140026167916288 logging_writer.py:48] [114900] global_step=114900, grad_norm=5.22300910949707, loss=1.4475256204605103
I0128 18:33:38.896235 140026159523584 logging_writer.py:48] [115000] global_step=115000, grad_norm=5.22915506362915, loss=1.4753634929656982
I0128 18:34:12.787173 140026167916288 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.69612979888916, loss=1.4000860452651978
I0128 18:34:46.705163 140026159523584 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.826830863952637, loss=1.395956039428711
I0128 18:35:20.611114 140026167916288 logging_writer.py:48] [115300] global_step=115300, grad_norm=5.239300727844238, loss=1.4769999980926514
I0128 18:35:54.536887 140026159523584 logging_writer.py:48] [115400] global_step=115400, grad_norm=5.155168533325195, loss=1.4427506923675537
I0128 18:36:28.418110 140026167916288 logging_writer.py:48] [115500] global_step=115500, grad_norm=5.316657066345215, loss=1.4405134916305542
I0128 18:37:02.420146 140026159523584 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.995424747467041, loss=1.4089566469192505
I0128 18:37:36.313638 140026167916288 logging_writer.py:48] [115700] global_step=115700, grad_norm=5.9263105392456055, loss=1.5185596942901611
I0128 18:37:57.154886 140187804313408 spec.py:321] Evaluating on the training split.
I0128 18:38:03.614645 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 18:38:12.753647 140187804313408 spec.py:349] Evaluating on the test split.
I0128 18:38:15.517030 140187804313408 submission_runner.py:408] Time since start: 40706.72s, 	Step: 115763, 	{'train/accuracy': 0.74418044090271, 'train/loss': 0.970153272151947, 'validation/accuracy': 0.678059995174408, 'validation/loss': 1.2999392747879028, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.0128350257873535, 'test/num_examples': 10000, 'score': 39305.58757019043, 'total_duration': 40706.71689796448, 'accumulated_submission_time': 39305.58757019043, 'accumulated_eval_time': 1392.7512967586517, 'accumulated_logging_time': 4.165997743606567}
I0128 18:38:15.556298 140026067269376 logging_writer.py:48] [115763] accumulated_eval_time=1392.751297, accumulated_logging_time=4.165998, accumulated_submission_time=39305.587570, global_step=115763, preemption_count=0, score=39305.587570, test/accuracy=0.546600, test/loss=2.012835, test/num_examples=10000, total_duration=40706.716898, train/accuracy=0.744180, train/loss=0.970153, validation/accuracy=0.678060, validation/loss=1.299939, validation/num_examples=50000
I0128 18:38:28.439295 140026075662080 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.717543125152588, loss=1.3811650276184082
I0128 18:39:02.271291 140026067269376 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.9208526611328125, loss=1.421915054321289
I0128 18:39:36.120795 140026075662080 logging_writer.py:48] [116000] global_step=116000, grad_norm=5.053948879241943, loss=1.4522407054901123
I0128 18:40:09.978448 140026067269376 logging_writer.py:48] [116100] global_step=116100, grad_norm=4.817529201507568, loss=1.566162347793579
I0128 18:40:43.874257 140026075662080 logging_writer.py:48] [116200] global_step=116200, grad_norm=5.04099178314209, loss=1.462153673171997
I0128 18:41:17.747703 140026067269376 logging_writer.py:48] [116300] global_step=116300, grad_norm=5.734215259552002, loss=1.5702524185180664
I0128 18:41:51.633219 140026075662080 logging_writer.py:48] [116400] global_step=116400, grad_norm=5.322586536407471, loss=1.414122223854065
I0128 18:42:25.553622 140026067269376 logging_writer.py:48] [116500] global_step=116500, grad_norm=5.41050386428833, loss=1.6038978099822998
I0128 18:42:59.452415 140026075662080 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.990372657775879, loss=1.58087158203125
I0128 18:43:33.426124 140026067269376 logging_writer.py:48] [116700] global_step=116700, grad_norm=5.097080230712891, loss=1.496145486831665
I0128 18:44:07.319386 140026075662080 logging_writer.py:48] [116800] global_step=116800, grad_norm=5.7360029220581055, loss=1.4043289422988892
I0128 18:44:41.180251 140026067269376 logging_writer.py:48] [116900] global_step=116900, grad_norm=5.1388702392578125, loss=1.4686952829360962
I0128 18:45:15.090689 140026075662080 logging_writer.py:48] [117000] global_step=117000, grad_norm=5.657355308532715, loss=1.496612548828125
I0128 18:45:48.975472 140026067269376 logging_writer.py:48] [117100] global_step=117100, grad_norm=5.710049629211426, loss=1.4910646677017212
I0128 18:46:22.842556 140026075662080 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.6621904373168945, loss=1.4510061740875244
I0128 18:46:45.682319 140187804313408 spec.py:321] Evaluating on the training split.
I0128 18:46:51.944007 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 18:47:00.816907 140187804313408 spec.py:349] Evaluating on the test split.
I0128 18:47:03.468862 140187804313408 submission_runner.py:408] Time since start: 41234.67s, 	Step: 117269, 	{'train/accuracy': 0.7614795565605164, 'train/loss': 0.9109672904014587, 'validation/accuracy': 0.6902799606323242, 'validation/loss': 1.2496405839920044, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 1.921729564666748, 'test/num_examples': 10000, 'score': 39815.647740364075, 'total_duration': 41234.668796777725, 'accumulated_submission_time': 39815.647740364075, 'accumulated_eval_time': 1410.537811756134, 'accumulated_logging_time': 4.214937686920166}
I0128 18:47:03.507391 140026167916288 logging_writer.py:48] [117269] accumulated_eval_time=1410.537812, accumulated_logging_time=4.214938, accumulated_submission_time=39815.647740, global_step=117269, preemption_count=0, score=39815.647740, test/accuracy=0.568400, test/loss=1.921730, test/num_examples=10000, total_duration=41234.668797, train/accuracy=0.761480, train/loss=0.910967, validation/accuracy=0.690280, validation/loss=1.249641, validation/num_examples=50000
I0128 18:47:14.352573 140026176308992 logging_writer.py:48] [117300] global_step=117300, grad_norm=5.190940856933594, loss=1.4402793645858765
I0128 18:47:48.174367 140026167916288 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.898634910583496, loss=1.4348604679107666
I0128 18:48:22.050608 140026176308992 logging_writer.py:48] [117500] global_step=117500, grad_norm=5.666644096374512, loss=1.526819109916687
I0128 18:48:55.979083 140026167916288 logging_writer.py:48] [117600] global_step=117600, grad_norm=5.508422374725342, loss=1.504537582397461
I0128 18:49:29.928009 140026176308992 logging_writer.py:48] [117700] global_step=117700, grad_norm=6.017580032348633, loss=1.3852617740631104
I0128 18:50:03.766417 140026167916288 logging_writer.py:48] [117800] global_step=117800, grad_norm=5.705491065979004, loss=1.4380170106887817
I0128 18:50:37.630687 140026176308992 logging_writer.py:48] [117900] global_step=117900, grad_norm=5.481523513793945, loss=1.447790265083313
I0128 18:51:11.560598 140026167916288 logging_writer.py:48] [118000] global_step=118000, grad_norm=5.1008710861206055, loss=1.4542152881622314
I0128 18:51:45.444156 140026176308992 logging_writer.py:48] [118100] global_step=118100, grad_norm=5.637913227081299, loss=1.429274082183838
I0128 18:52:19.385304 140026167916288 logging_writer.py:48] [118200] global_step=118200, grad_norm=5.224606990814209, loss=1.471380591392517
I0128 18:52:53.253541 140026176308992 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.752706050872803, loss=1.3484368324279785
I0128 18:53:27.155167 140026167916288 logging_writer.py:48] [118400] global_step=118400, grad_norm=5.134251117706299, loss=1.4460159540176392
I0128 18:54:01.064949 140026176308992 logging_writer.py:48] [118500] global_step=118500, grad_norm=5.008506774902344, loss=1.5637513399124146
I0128 18:54:34.961115 140026167916288 logging_writer.py:48] [118600] global_step=118600, grad_norm=5.557259559631348, loss=1.4450880289077759
I0128 18:55:08.858759 140026176308992 logging_writer.py:48] [118700] global_step=118700, grad_norm=5.353276252746582, loss=1.3634611368179321
I0128 18:55:33.767013 140187804313408 spec.py:321] Evaluating on the training split.
I0128 18:55:40.045526 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 18:55:48.821010 140187804313408 spec.py:349] Evaluating on the test split.
I0128 18:55:51.735955 140187804313408 submission_runner.py:408] Time since start: 41762.94s, 	Step: 118775, 	{'train/accuracy': 0.7867307066917419, 'train/loss': 0.7906128764152527, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.260579228401184, 'validation/num_examples': 50000, 'test/accuracy': 0.5646000504493713, 'test/loss': 1.9624043703079224, 'test/num_examples': 10000, 'score': 40325.84529519081, 'total_duration': 41762.935829401016, 'accumulated_submission_time': 40325.84529519081, 'accumulated_eval_time': 1428.5066511631012, 'accumulated_logging_time': 4.262691259384155}
I0128 18:55:51.775827 140026042091264 logging_writer.py:48] [118775] accumulated_eval_time=1428.506651, accumulated_logging_time=4.262691, accumulated_submission_time=40325.845295, global_step=118775, preemption_count=0, score=40325.845295, test/accuracy=0.564600, test/loss=1.962404, test/num_examples=10000, total_duration=41762.935829, train/accuracy=0.786731, train/loss=0.790613, validation/accuracy=0.689120, validation/loss=1.260579, validation/num_examples=50000
I0128 18:56:00.597453 140026050483968 logging_writer.py:48] [118800] global_step=118800, grad_norm=6.476125240325928, loss=1.5322062969207764
I0128 18:56:34.418063 140026042091264 logging_writer.py:48] [118900] global_step=118900, grad_norm=6.091903209686279, loss=1.4367231130599976
I0128 18:57:08.280930 140026050483968 logging_writer.py:48] [119000] global_step=119000, grad_norm=5.615604877471924, loss=1.4574368000030518
I0128 18:57:42.151829 140026042091264 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.731327056884766, loss=1.5115667581558228
I0128 18:58:16.047271 140026050483968 logging_writer.py:48] [119200] global_step=119200, grad_norm=5.068005084991455, loss=1.4310204982757568
I0128 18:58:49.938197 140026042091264 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.227488040924072, loss=1.3833608627319336
I0128 18:59:23.816704 140026050483968 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.946012020111084, loss=1.4098824262619019
I0128 18:59:57.710844 140026042091264 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.918573379516602, loss=1.3884469270706177
I0128 19:00:31.567144 140026050483968 logging_writer.py:48] [119600] global_step=119600, grad_norm=5.4256134033203125, loss=1.5436921119689941
I0128 19:01:05.448258 140026042091264 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.399592399597168, loss=1.4811840057373047
I0128 19:01:39.341021 140026050483968 logging_writer.py:48] [119800] global_step=119800, grad_norm=5.449892044067383, loss=1.3935942649841309
I0128 19:02:13.419782 140026042091264 logging_writer.py:48] [119900] global_step=119900, grad_norm=5.5337066650390625, loss=1.4220762252807617
I0128 19:02:47.319891 140026050483968 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.307424545288086, loss=1.3789020776748657
I0128 19:03:21.185191 140026042091264 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.750616550445557, loss=1.375276803970337
I0128 19:03:55.113437 140026050483968 logging_writer.py:48] [120200] global_step=120200, grad_norm=5.904373645782471, loss=1.465360164642334
I0128 19:04:22.021523 140187804313408 spec.py:321] Evaluating on the training split.
I0128 19:04:28.248524 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 19:04:37.261808 140187804313408 spec.py:349] Evaluating on the test split.
I0128 19:04:39.876965 140187804313408 submission_runner.py:408] Time since start: 42291.08s, 	Step: 120281, 	{'train/accuracy': 0.77543044090271, 'train/loss': 0.8411076664924622, 'validation/accuracy': 0.6890400052070618, 'validation/loss': 1.254690408706665, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 1.9643940925598145, 'test/num_examples': 10000, 'score': 40836.02567625046, 'total_duration': 42291.07681274414, 'accumulated_submission_time': 40836.02567625046, 'accumulated_eval_time': 1446.361969947815, 'accumulated_logging_time': 4.311857223510742}
I0128 19:04:39.915920 140026167916288 logging_writer.py:48] [120281] accumulated_eval_time=1446.361970, accumulated_logging_time=4.311857, accumulated_submission_time=40836.025676, global_step=120281, preemption_count=0, score=40836.025676, test/accuracy=0.560500, test/loss=1.964394, test/num_examples=10000, total_duration=42291.076813, train/accuracy=0.775430, train/loss=0.841108, validation/accuracy=0.689040, validation/loss=1.254690, validation/num_examples=50000
I0128 19:04:46.683180 140026176308992 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.163465976715088, loss=1.3754023313522339
I0128 19:05:20.509057 140026167916288 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.101945877075195, loss=1.546987771987915
I0128 19:05:54.406693 140026176308992 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.469071388244629, loss=1.3719403743743896
I0128 19:06:28.276611 140026167916288 logging_writer.py:48] [120600] global_step=120600, grad_norm=5.009729862213135, loss=1.2787985801696777
I0128 19:07:02.184365 140026176308992 logging_writer.py:48] [120700] global_step=120700, grad_norm=5.454417705535889, loss=1.4484126567840576
I0128 19:07:36.058923 140026167916288 logging_writer.py:48] [120800] global_step=120800, grad_norm=5.379203796386719, loss=1.3756141662597656
I0128 19:08:09.969909 140026176308992 logging_writer.py:48] [120900] global_step=120900, grad_norm=5.16532564163208, loss=1.4676399230957031
I0128 19:08:43.962850 140026167916288 logging_writer.py:48] [121000] global_step=121000, grad_norm=5.109973430633545, loss=1.4085140228271484
I0128 19:09:17.846495 140026176308992 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.823263168334961, loss=1.34259033203125
I0128 19:09:51.777366 140026167916288 logging_writer.py:48] [121200] global_step=121200, grad_norm=5.304097652435303, loss=1.477563500404358
I0128 19:10:25.650585 140026176308992 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.4969162940979, loss=1.4538911581039429
I0128 19:10:59.553704 140026167916288 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.171266078948975, loss=1.3599369525909424
I0128 19:11:33.472462 140026176308992 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.57656717300415, loss=1.3546602725982666
I0128 19:12:07.405905 140026167916288 logging_writer.py:48] [121600] global_step=121600, grad_norm=5.612006664276123, loss=1.4628101587295532
I0128 19:12:41.284185 140026176308992 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.909593105316162, loss=1.3677064180374146
I0128 19:13:09.925854 140187804313408 spec.py:321] Evaluating on the training split.
I0128 19:13:16.124137 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 19:13:24.948875 140187804313408 spec.py:349] Evaluating on the test split.
I0128 19:13:27.548317 140187804313408 submission_runner.py:408] Time since start: 42818.75s, 	Step: 121786, 	{'train/accuracy': 0.7752710580825806, 'train/loss': 0.8376134634017944, 'validation/accuracy': 0.6941999793052673, 'validation/loss': 1.22850501537323, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9029667377471924, 'test/num_examples': 10000, 'score': 41345.97124314308, 'total_duration': 42818.748262643814, 'accumulated_submission_time': 41345.97124314308, 'accumulated_eval_time': 1463.9844024181366, 'accumulated_logging_time': 4.359987735748291}
I0128 19:13:27.593517 140026058876672 logging_writer.py:48] [121786] accumulated_eval_time=1463.984402, accumulated_logging_time=4.359988, accumulated_submission_time=41345.971243, global_step=121786, preemption_count=0, score=41345.971243, test/accuracy=0.567800, test/loss=1.902967, test/num_examples=10000, total_duration=42818.748263, train/accuracy=0.775271, train/loss=0.837613, validation/accuracy=0.694200, validation/loss=1.228505, validation/num_examples=50000
I0128 19:13:32.695690 140026067269376 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.0057692527771, loss=1.2965302467346191
I0128 19:14:06.521637 140026058876672 logging_writer.py:48] [121900] global_step=121900, grad_norm=5.928442001342773, loss=1.4466320276260376
I0128 19:14:40.381477 140026067269376 logging_writer.py:48] [122000] global_step=122000, grad_norm=6.091413497924805, loss=1.4814502000808716
I0128 19:15:14.367238 140026058876672 logging_writer.py:48] [122100] global_step=122100, grad_norm=5.712930679321289, loss=1.3394416570663452
I0128 19:15:48.242576 140026067269376 logging_writer.py:48] [122200] global_step=122200, grad_norm=6.41702938079834, loss=1.505894422531128
I0128 19:16:22.137165 140026058876672 logging_writer.py:48] [122300] global_step=122300, grad_norm=5.319727420806885, loss=1.3978170156478882
I0128 19:16:56.010987 140026067269376 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.884404182434082, loss=1.4309828281402588
I0128 19:17:29.875423 140026058876672 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.388575553894043, loss=1.3366031646728516
I0128 19:18:03.768064 140026067269376 logging_writer.py:48] [122600] global_step=122600, grad_norm=5.010085105895996, loss=1.430957555770874
I0128 19:18:37.659290 140026058876672 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.451151371002197, loss=1.3341326713562012
I0128 19:19:11.532918 140026067269376 logging_writer.py:48] [122800] global_step=122800, grad_norm=5.768067359924316, loss=1.3589245080947876
I0128 19:19:45.424573 140026058876672 logging_writer.py:48] [122900] global_step=122900, grad_norm=5.482477188110352, loss=1.4040518999099731
I0128 19:20:19.284705 140026067269376 logging_writer.py:48] [123000] global_step=123000, grad_norm=5.650319576263428, loss=1.3309613466262817
I0128 19:20:53.144931 140026058876672 logging_writer.py:48] [123100] global_step=123100, grad_norm=5.066764831542969, loss=1.3749407529830933
I0128 19:21:27.134636 140026067269376 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.43992280960083, loss=1.4127250909805298
I0128 19:21:57.748876 140187804313408 spec.py:321] Evaluating on the training split.
I0128 19:22:04.170274 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 19:22:12.990366 140187804313408 spec.py:349] Evaluating on the test split.
I0128 19:22:15.604807 140187804313408 submission_runner.py:408] Time since start: 43346.80s, 	Step: 123292, 	{'train/accuracy': 0.7697106003761292, 'train/loss': 0.8630506992340088, 'validation/accuracy': 0.6924200057983398, 'validation/loss': 1.2353243827819824, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.924597978591919, 'test/num_examples': 10000, 'score': 41856.063671827316, 'total_duration': 43346.80474662781, 'accumulated_submission_time': 41856.063671827316, 'accumulated_eval_time': 1481.8402979373932, 'accumulated_logging_time': 4.414201498031616}
I0128 19:22:15.643557 140026151130880 logging_writer.py:48] [123292] accumulated_eval_time=1481.840298, accumulated_logging_time=4.414201, accumulated_submission_time=41856.063672, global_step=123292, preemption_count=0, score=41856.063672, test/accuracy=0.570100, test/loss=1.924598, test/num_examples=10000, total_duration=43346.804747, train/accuracy=0.769711, train/loss=0.863051, validation/accuracy=0.692420, validation/loss=1.235324, validation/num_examples=50000
I0128 19:22:18.694168 140026159523584 logging_writer.py:48] [123300] global_step=123300, grad_norm=6.412402629852295, loss=1.479196310043335
I0128 19:22:52.548258 140026151130880 logging_writer.py:48] [123400] global_step=123400, grad_norm=6.0443034172058105, loss=1.3563117980957031
I0128 19:23:26.420292 140026159523584 logging_writer.py:48] [123500] global_step=123500, grad_norm=5.0895233154296875, loss=1.3828182220458984
I0128 19:24:00.293716 140026151130880 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.208608150482178, loss=1.4470982551574707
I0128 19:24:34.217299 140026159523584 logging_writer.py:48] [123700] global_step=123700, grad_norm=5.4972333908081055, loss=1.4581284523010254
I0128 19:25:08.095022 140026151130880 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.534082889556885, loss=1.3393319845199585
I0128 19:25:41.999481 140026159523584 logging_writer.py:48] [123900] global_step=123900, grad_norm=5.269308090209961, loss=1.43910813331604
I0128 19:26:15.844523 140026151130880 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.349242210388184, loss=1.3170849084854126
I0128 19:26:49.737365 140026159523584 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.582784652709961, loss=1.436023235321045
I0128 19:27:23.644756 140026151130880 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.440183162689209, loss=1.4251508712768555
I0128 19:27:57.739083 140026159523584 logging_writer.py:48] [124300] global_step=124300, grad_norm=5.60090446472168, loss=1.392879843711853
I0128 19:28:31.608644 140026151130880 logging_writer.py:48] [124400] global_step=124400, grad_norm=5.265214920043945, loss=1.3264981508255005
I0128 19:29:05.508216 140026159523584 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.399106502532959, loss=1.3321433067321777
I0128 19:29:39.399335 140026151130880 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.808334827423096, loss=1.3365355730056763
I0128 19:30:13.326630 140026159523584 logging_writer.py:48] [124700] global_step=124700, grad_norm=5.4227447509765625, loss=1.368348240852356
I0128 19:30:45.646471 140187804313408 spec.py:321] Evaluating on the training split.
I0128 19:30:52.538105 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 19:31:01.517464 140187804313408 spec.py:349] Evaluating on the test split.
I0128 19:31:04.210037 140187804313408 submission_runner.py:408] Time since start: 43875.41s, 	Step: 124797, 	{'train/accuracy': 0.7671197056770325, 'train/loss': 0.862572968006134, 'validation/accuracy': 0.6951199769973755, 'validation/loss': 1.2315285205841064, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.9317371845245361, 'test/num_examples': 10000, 'score': 42366.00140285492, 'total_duration': 43875.40986657143, 'accumulated_submission_time': 42366.00140285492, 'accumulated_eval_time': 1500.4037234783173, 'accumulated_logging_time': 4.462406873703003}
I0128 19:31:04.252468 140026075662080 logging_writer.py:48] [124797] accumulated_eval_time=1500.403723, accumulated_logging_time=4.462407, accumulated_submission_time=42366.001403, global_step=124797, preemption_count=0, score=42366.001403, test/accuracy=0.569600, test/loss=1.931737, test/num_examples=10000, total_duration=43875.409867, train/accuracy=0.767120, train/loss=0.862573, validation/accuracy=0.695120, validation/loss=1.231529, validation/num_examples=50000
I0128 19:31:05.614257 140026151130880 logging_writer.py:48] [124800] global_step=124800, grad_norm=5.5823798179626465, loss=1.2795288562774658
I0128 19:31:39.459796 140026075662080 logging_writer.py:48] [124900] global_step=124900, grad_norm=5.306727886199951, loss=1.4305381774902344
I0128 19:32:13.327628 140026151130880 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.9206647872924805, loss=1.3374935388565063
I0128 19:32:47.210901 140026075662080 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.719600200653076, loss=1.3153380155563354
I0128 19:33:21.102995 140026151130880 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.44672155380249, loss=1.3023650646209717
I0128 19:33:55.021288 140026075662080 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.609953880310059, loss=1.345841884613037
I0128 19:34:29.001478 140026151130880 logging_writer.py:48] [125400] global_step=125400, grad_norm=5.290534973144531, loss=1.39842689037323
I0128 19:35:02.854945 140026075662080 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.881986141204834, loss=1.3562501668930054
I0128 19:35:36.770888 140026151130880 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.676827907562256, loss=1.3720498085021973
I0128 19:36:10.630254 140026075662080 logging_writer.py:48] [125700] global_step=125700, grad_norm=6.03915548324585, loss=1.3785274028778076
I0128 19:36:44.509798 140026151130880 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.8583598136901855, loss=1.3680249452590942
I0128 19:37:18.400309 140026075662080 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.785227298736572, loss=1.352756381034851
I0128 19:37:52.257706 140026151130880 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.948166847229004, loss=1.4078692197799683
I0128 19:38:26.121831 140026075662080 logging_writer.py:48] [126100] global_step=126100, grad_norm=6.285511493682861, loss=1.291954517364502
I0128 19:39:00.019818 140026151130880 logging_writer.py:48] [126200] global_step=126200, grad_norm=5.367445468902588, loss=1.3089704513549805
I0128 19:39:33.902277 140026075662080 logging_writer.py:48] [126300] global_step=126300, grad_norm=5.4567999839782715, loss=1.3648852109909058
I0128 19:39:34.387725 140187804313408 spec.py:321] Evaluating on the training split.
I0128 19:39:40.596749 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 19:39:49.257690 140187804313408 spec.py:349] Evaluating on the test split.
I0128 19:39:51.857485 140187804313408 submission_runner.py:408] Time since start: 44403.06s, 	Step: 126303, 	{'train/accuracy': 0.776387095451355, 'train/loss': 0.831611156463623, 'validation/accuracy': 0.7033199667930603, 'validation/loss': 1.20175302028656, 'validation/num_examples': 50000, 'test/accuracy': 0.579200029373169, 'test/loss': 1.8880500793457031, 'test/num_examples': 10000, 'score': 42876.07256484032, 'total_duration': 44403.05740857124, 'accumulated_submission_time': 42876.07256484032, 'accumulated_eval_time': 1517.8734288215637, 'accumulated_logging_time': 4.515023231506348}
I0128 19:39:51.899297 140026050483968 logging_writer.py:48] [126303] accumulated_eval_time=1517.873429, accumulated_logging_time=4.515023, accumulated_submission_time=42876.072565, global_step=126303, preemption_count=0, score=42876.072565, test/accuracy=0.579200, test/loss=1.888050, test/num_examples=10000, total_duration=44403.057409, train/accuracy=0.776387, train/loss=0.831611, validation/accuracy=0.703320, validation/loss=1.201753, validation/num_examples=50000
I0128 19:40:25.063078 140026058876672 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.730917930603027, loss=1.3116739988327026
I0128 19:40:58.971459 140026050483968 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.549294948577881, loss=1.2238892316818237
I0128 19:41:32.879235 140026058876672 logging_writer.py:48] [126600] global_step=126600, grad_norm=7.024567604064941, loss=1.404824137687683
I0128 19:42:06.787878 140026050483968 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.7449517250061035, loss=1.4578559398651123
I0128 19:42:40.666967 140026058876672 logging_writer.py:48] [126800] global_step=126800, grad_norm=5.8045477867126465, loss=1.4185374975204468
I0128 19:43:14.592172 140026050483968 logging_writer.py:48] [126900] global_step=126900, grad_norm=6.163105010986328, loss=1.3519448041915894
I0128 19:43:48.515877 140026058876672 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.700394630432129, loss=1.337021827697754
I0128 19:44:22.409129 140026050483968 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.935840606689453, loss=1.371591567993164
I0128 19:44:56.317993 140026058876672 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.869277000427246, loss=1.270139217376709
I0128 19:45:30.225433 140026050483968 logging_writer.py:48] [127300] global_step=127300, grad_norm=5.3075079917907715, loss=1.372551441192627
I0128 19:46:04.156407 140026058876672 logging_writer.py:48] [127400] global_step=127400, grad_norm=5.481136798858643, loss=1.3383615016937256
I0128 19:46:38.049265 140026050483968 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.969620227813721, loss=1.336211085319519
I0128 19:47:11.990857 140026058876672 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.734567165374756, loss=1.297044038772583
I0128 19:47:45.892503 140026050483968 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.695403575897217, loss=1.2245993614196777
I0128 19:48:19.812724 140026058876672 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.298725128173828, loss=1.3904452323913574
I0128 19:48:21.990365 140187804313408 spec.py:321] Evaluating on the training split.
I0128 19:48:28.208365 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 19:48:37.293732 140187804313408 spec.py:349] Evaluating on the test split.
I0128 19:48:39.847710 140187804313408 submission_runner.py:408] Time since start: 44931.05s, 	Step: 127808, 	{'train/accuracy': 0.808035671710968, 'train/loss': 0.7111296057701111, 'validation/accuracy': 0.7016400098800659, 'validation/loss': 1.2024482488632202, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 1.917343020439148, 'test/num_examples': 10000, 'score': 43386.099005937576, 'total_duration': 44931.04763793945, 'accumulated_submission_time': 43386.099005937576, 'accumulated_eval_time': 1535.7307217121124, 'accumulated_logging_time': 4.566674709320068}
I0128 19:48:39.893327 140026058876672 logging_writer.py:48] [127808] accumulated_eval_time=1535.730722, accumulated_logging_time=4.566675, accumulated_submission_time=43386.099006, global_step=127808, preemption_count=0, score=43386.099006, test/accuracy=0.575200, test/loss=1.917343, test/num_examples=10000, total_duration=44931.047638, train/accuracy=0.808036, train/loss=0.711130, validation/accuracy=0.701640, validation/loss=1.202448, validation/num_examples=50000
I0128 19:49:11.335528 140026075662080 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.20594596862793, loss=1.2434290647506714
I0128 19:49:45.155067 140026058876672 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.639887809753418, loss=1.3918673992156982
I0128 19:50:19.012120 140026075662080 logging_writer.py:48] [128100] global_step=128100, grad_norm=6.133432388305664, loss=1.326370358467102
I0128 19:50:52.879245 140026058876672 logging_writer.py:48] [128200] global_step=128200, grad_norm=6.2331390380859375, loss=1.343601942062378
I0128 19:51:26.805694 140026075662080 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.358668327331543, loss=1.313145637512207
I0128 19:52:00.677254 140026058876672 logging_writer.py:48] [128400] global_step=128400, grad_norm=6.015100955963135, loss=1.2580509185791016
I0128 19:52:34.556625 140026075662080 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.680389404296875, loss=1.3749911785125732
I0128 19:53:08.432316 140026058876672 logging_writer.py:48] [128600] global_step=128600, grad_norm=6.38197660446167, loss=1.258286952972412
I0128 19:53:42.413925 140026075662080 logging_writer.py:48] [128700] global_step=128700, grad_norm=5.977463722229004, loss=1.30582594871521
I0128 19:54:16.286249 140026058876672 logging_writer.py:48] [128800] global_step=128800, grad_norm=6.221950054168701, loss=1.493627905845642
I0128 19:54:50.183268 140026075662080 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.541472911834717, loss=1.3617618083953857
I0128 19:55:24.075657 140026058876672 logging_writer.py:48] [129000] global_step=129000, grad_norm=6.144256114959717, loss=1.3252769708633423
I0128 19:55:57.961145 140026075662080 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.231399059295654, loss=1.368343472480774
I0128 19:56:31.864277 140026058876672 logging_writer.py:48] [129200] global_step=129200, grad_norm=6.105222225189209, loss=1.2898013591766357
I0128 19:57:05.733166 140026075662080 logging_writer.py:48] [129300] global_step=129300, grad_norm=6.741363525390625, loss=1.39757239818573
I0128 19:57:09.958438 140187804313408 spec.py:321] Evaluating on the training split.
I0128 19:57:16.277138 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 19:57:25.032953 140187804313408 spec.py:349] Evaluating on the test split.
I0128 19:57:27.628835 140187804313408 submission_runner.py:408] Time since start: 45458.83s, 	Step: 129314, 	{'train/accuracy': 0.7916733026504517, 'train/loss': 0.7742475271224976, 'validation/accuracy': 0.6988999843597412, 'validation/loss': 1.205346941947937, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 1.8980298042297363, 'test/num_examples': 10000, 'score': 43896.101038217545, 'total_duration': 45458.82877635956, 'accumulated_submission_time': 43896.101038217545, 'accumulated_eval_time': 1553.4010951519012, 'accumulated_logging_time': 4.621357679367065}
I0128 19:57:27.668913 140026067269376 logging_writer.py:48] [129314] accumulated_eval_time=1553.401095, accumulated_logging_time=4.621358, accumulated_submission_time=43896.101038, global_step=129314, preemption_count=0, score=43896.101038, test/accuracy=0.573800, test/loss=1.898030, test/num_examples=10000, total_duration=45458.828776, train/accuracy=0.791673, train/loss=0.774248, validation/accuracy=0.698900, validation/loss=1.205347, validation/num_examples=50000
I0128 19:57:57.112617 140026159523584 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.783458232879639, loss=1.3412951231002808
I0128 19:58:30.964007 140026067269376 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.815867900848389, loss=1.4434895515441895
I0128 19:59:04.831236 140026159523584 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.623167037963867, loss=1.312966227531433
I0128 19:59:38.821857 140026067269376 logging_writer.py:48] [129700] global_step=129700, grad_norm=6.205418109893799, loss=1.2474664449691772
I0128 20:00:12.742367 140026159523584 logging_writer.py:48] [129800] global_step=129800, grad_norm=6.391081809997559, loss=1.3457587957382202
I0128 20:00:46.623027 140026067269376 logging_writer.py:48] [129900] global_step=129900, grad_norm=6.527135848999023, loss=1.331856369972229
I0128 20:01:20.533923 140026159523584 logging_writer.py:48] [130000] global_step=130000, grad_norm=6.152486324310303, loss=1.3306310176849365
I0128 20:01:54.431179 140026067269376 logging_writer.py:48] [130100] global_step=130100, grad_norm=6.659297943115234, loss=1.2770445346832275
I0128 20:02:28.332189 140026159523584 logging_writer.py:48] [130200] global_step=130200, grad_norm=6.297985553741455, loss=1.2363135814666748
I0128 20:03:02.247770 140026067269376 logging_writer.py:48] [130300] global_step=130300, grad_norm=6.069835662841797, loss=1.3888304233551025
I0128 20:03:36.122979 140026159523584 logging_writer.py:48] [130400] global_step=130400, grad_norm=5.455429553985596, loss=1.3277807235717773
I0128 20:04:10.043821 140026067269376 logging_writer.py:48] [130500] global_step=130500, grad_norm=6.408341884613037, loss=1.4615143537521362
I0128 20:04:43.915707 140026159523584 logging_writer.py:48] [130600] global_step=130600, grad_norm=6.035573959350586, loss=1.4668854475021362
I0128 20:05:17.839841 140026067269376 logging_writer.py:48] [130700] global_step=130700, grad_norm=6.226123332977295, loss=1.2220604419708252
I0128 20:05:51.797040 140026159523584 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.836258411407471, loss=1.2956500053405762
I0128 20:05:57.735909 140187804313408 spec.py:321] Evaluating on the training split.
I0128 20:06:04.132983 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 20:06:12.962874 140187804313408 spec.py:349] Evaluating on the test split.
I0128 20:06:15.475961 140187804313408 submission_runner.py:408] Time since start: 45986.68s, 	Step: 130819, 	{'train/accuracy': 0.7887834906578064, 'train/loss': 0.7834305763244629, 'validation/accuracy': 0.7028999924659729, 'validation/loss': 1.1931791305541992, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9065872430801392, 'test/num_examples': 10000, 'score': 44406.101815223694, 'total_duration': 45986.675889253616, 'accumulated_submission_time': 44406.101815223694, 'accumulated_eval_time': 1571.1410930156708, 'accumulated_logging_time': 4.6731743812561035}
I0128 20:06:15.519371 140026058876672 logging_writer.py:48] [130819] accumulated_eval_time=1571.141093, accumulated_logging_time=4.673174, accumulated_submission_time=44406.101815, global_step=130819, preemption_count=0, score=44406.101815, test/accuracy=0.572900, test/loss=1.906587, test/num_examples=10000, total_duration=45986.675889, train/accuracy=0.788783, train/loss=0.783431, validation/accuracy=0.702900, validation/loss=1.193179, validation/num_examples=50000
I0128 20:06:43.279810 140026067269376 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.670999526977539, loss=1.318943977355957
I0128 20:07:17.161203 140026058876672 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.618890285491943, loss=1.2592138051986694
I0128 20:07:51.011846 140026067269376 logging_writer.py:48] [131100] global_step=131100, grad_norm=6.136836051940918, loss=1.3611912727355957
I0128 20:08:24.875954 140026058876672 logging_writer.py:48] [131200] global_step=131200, grad_norm=6.2141194343566895, loss=1.227257490158081
I0128 20:08:58.773663 140026067269376 logging_writer.py:48] [131300] global_step=131300, grad_norm=6.454116344451904, loss=1.3186390399932861
I0128 20:09:32.633286 140026058876672 logging_writer.py:48] [131400] global_step=131400, grad_norm=7.286131858825684, loss=1.368388056755066
I0128 20:10:06.519412 140026067269376 logging_writer.py:48] [131500] global_step=131500, grad_norm=5.5163187980651855, loss=1.182096004486084
I0128 20:10:40.364943 140026058876672 logging_writer.py:48] [131600] global_step=131600, grad_norm=6.503943920135498, loss=1.3540188074111938
I0128 20:11:14.269665 140026067269376 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.323449611663818, loss=1.1884926557540894
I0128 20:11:48.179066 140026058876672 logging_writer.py:48] [131800] global_step=131800, grad_norm=6.422794342041016, loss=1.166839599609375
I0128 20:12:22.179666 140026067269376 logging_writer.py:48] [131900] global_step=131900, grad_norm=6.3168158531188965, loss=1.3413307666778564
I0128 20:12:56.037257 140026058876672 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.654880046844482, loss=1.2829453945159912
I0128 20:13:29.926670 140026067269376 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.800506114959717, loss=1.375981330871582
I0128 20:14:03.857666 140026058876672 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.522542953491211, loss=1.3304359912872314
I0128 20:14:37.723939 140026067269376 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.836900234222412, loss=1.3569263219833374
I0128 20:14:45.655551 140187804313408 spec.py:321] Evaluating on the training split.
I0128 20:14:51.990060 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 20:15:00.740158 140187804313408 spec.py:349] Evaluating on the test split.
I0128 20:15:03.515703 140187804313408 submission_runner.py:408] Time since start: 46514.72s, 	Step: 132325, 	{'train/accuracy': 0.7939453125, 'train/loss': 0.7579774260520935, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.1673496961593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8415168523788452, 'test/num_examples': 10000, 'score': 44916.1741373539, 'total_duration': 46514.71562767029, 'accumulated_submission_time': 44916.1741373539, 'accumulated_eval_time': 1589.0011916160583, 'accumulated_logging_time': 4.726458549499512}
I0128 20:15:03.557559 140026050483968 logging_writer.py:48] [132325] accumulated_eval_time=1589.001192, accumulated_logging_time=4.726459, accumulated_submission_time=44916.174137, global_step=132325, preemption_count=0, score=44916.174137, test/accuracy=0.593300, test/loss=1.841517, test/num_examples=10000, total_duration=46514.715628, train/accuracy=0.793945, train/loss=0.757977, validation/accuracy=0.707920, validation/loss=1.167350, validation/num_examples=50000
I0128 20:15:29.282377 140026058876672 logging_writer.py:48] [132400] global_step=132400, grad_norm=6.284045219421387, loss=1.2905255556106567
I0128 20:16:03.164254 140026050483968 logging_writer.py:48] [132500] global_step=132500, grad_norm=6.161669731140137, loss=1.319521427154541
I0128 20:16:37.040549 140026058876672 logging_writer.py:48] [132600] global_step=132600, grad_norm=6.007937908172607, loss=1.25604248046875
I0128 20:17:10.962611 140026050483968 logging_writer.py:48] [132700] global_step=132700, grad_norm=6.1497392654418945, loss=1.3284868001937866
I0128 20:17:44.821212 140026058876672 logging_writer.py:48] [132800] global_step=132800, grad_norm=6.683549880981445, loss=1.2799861431121826
I0128 20:18:18.704608 140026050483968 logging_writer.py:48] [132900] global_step=132900, grad_norm=6.383335113525391, loss=1.3027584552764893
I0128 20:18:52.667874 140026058876672 logging_writer.py:48] [133000] global_step=133000, grad_norm=6.130668640136719, loss=1.3632481098175049
I0128 20:19:26.524307 140026050483968 logging_writer.py:48] [133100] global_step=133100, grad_norm=6.162040710449219, loss=1.2643475532531738
I0128 20:20:00.446934 140026058876672 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.911287784576416, loss=1.3361438512802124
I0128 20:20:34.323899 140026050483968 logging_writer.py:48] [133300] global_step=133300, grad_norm=6.038154602050781, loss=1.247570276260376
I0128 20:21:08.258517 140026058876672 logging_writer.py:48] [133400] global_step=133400, grad_norm=6.24678373336792, loss=1.2955067157745361
I0128 20:21:42.136691 140026050483968 logging_writer.py:48] [133500] global_step=133500, grad_norm=7.3677825927734375, loss=1.2366247177124023
I0128 20:22:16.040025 140026058876672 logging_writer.py:48] [133600] global_step=133600, grad_norm=6.1756062507629395, loss=1.2508068084716797
I0128 20:22:49.933509 140026050483968 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.8265509605407715, loss=1.1789329051971436
I0128 20:23:23.855147 140026058876672 logging_writer.py:48] [133800] global_step=133800, grad_norm=6.411604881286621, loss=1.3617980480194092
I0128 20:23:33.826630 140187804313408 spec.py:321] Evaluating on the training split.
I0128 20:23:40.070613 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 20:23:49.142991 140187804313408 spec.py:349] Evaluating on the test split.
I0128 20:23:51.738844 140187804313408 submission_runner.py:408] Time since start: 47042.94s, 	Step: 133831, 	{'train/accuracy': 0.7958984375, 'train/loss': 0.757809579372406, 'validation/accuracy': 0.708620011806488, 'validation/loss': 1.1658791303634644, 'validation/num_examples': 50000, 'test/accuracy': 0.5860000252723694, 'test/loss': 1.8372397422790527, 'test/num_examples': 10000, 'score': 45426.37725400925, 'total_duration': 47042.93878364563, 'accumulated_submission_time': 45426.37725400925, 'accumulated_eval_time': 1606.9133660793304, 'accumulated_logging_time': 4.778220176696777}
I0128 20:23:51.783959 140026159523584 logging_writer.py:48] [133831] accumulated_eval_time=1606.913366, accumulated_logging_time=4.778220, accumulated_submission_time=45426.377254, global_step=133831, preemption_count=0, score=45426.377254, test/accuracy=0.586000, test/loss=1.837240, test/num_examples=10000, total_duration=47042.938784, train/accuracy=0.795898, train/loss=0.757810, validation/accuracy=0.708620, validation/loss=1.165879, validation/num_examples=50000
I0128 20:24:15.474749 140026167916288 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.822691440582275, loss=1.2778152227401733
I0128 20:24:49.324668 140026159523584 logging_writer.py:48] [134000] global_step=134000, grad_norm=6.140584945678711, loss=1.361026644706726
I0128 20:25:23.373748 140026167916288 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.842912197113037, loss=1.3536323308944702
I0128 20:25:57.273914 140026159523584 logging_writer.py:48] [134200] global_step=134200, grad_norm=7.017158508300781, loss=1.2949011325836182
I0128 20:26:31.144673 140026167916288 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.727806568145752, loss=1.2178969383239746
I0128 20:27:05.053332 140026159523584 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.513876438140869, loss=1.2223937511444092
I0128 20:27:38.950863 140026167916288 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.721989631652832, loss=1.2285765409469604
I0128 20:28:12.828943 140026159523584 logging_writer.py:48] [134600] global_step=134600, grad_norm=5.574173927307129, loss=1.2183356285095215
I0128 20:28:46.746205 140026167916288 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.975751876831055, loss=1.2543683052062988
I0128 20:29:20.592722 140026159523584 logging_writer.py:48] [134800] global_step=134800, grad_norm=6.414268493652344, loss=1.2759343385696411
I0128 20:29:54.476673 140026167916288 logging_writer.py:48] [134900] global_step=134900, grad_norm=6.893610000610352, loss=1.3419885635375977
I0128 20:30:28.340825 140026159523584 logging_writer.py:48] [135000] global_step=135000, grad_norm=5.6358866691589355, loss=1.2047033309936523
I0128 20:31:02.222013 140026167916288 logging_writer.py:48] [135100] global_step=135100, grad_norm=7.83375358581543, loss=1.3584915399551392
I0128 20:31:36.322187 140026159523584 logging_writer.py:48] [135200] global_step=135200, grad_norm=6.718262195587158, loss=1.3046139478683472
I0128 20:32:10.220731 140026167916288 logging_writer.py:48] [135300] global_step=135300, grad_norm=6.5064215660095215, loss=1.3161146640777588
I0128 20:32:21.894396 140187804313408 spec.py:321] Evaluating on the training split.
I0128 20:32:28.119738 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 20:32:36.839676 140187804313408 spec.py:349] Evaluating on the test split.
I0128 20:32:39.448045 140187804313408 submission_runner.py:408] Time since start: 47570.65s, 	Step: 135336, 	{'train/accuracy': 0.7983697056770325, 'train/loss': 0.737368106842041, 'validation/accuracy': 0.7152799963951111, 'validation/loss': 1.1455098390579224, 'validation/num_examples': 50000, 'test/accuracy': 0.5838000178337097, 'test/loss': 1.8571194410324097, 'test/num_examples': 10000, 'score': 45936.42216873169, 'total_duration': 47570.64797115326, 'accumulated_submission_time': 45936.42216873169, 'accumulated_eval_time': 1624.4669604301453, 'accumulated_logging_time': 4.833124399185181}
I0128 20:32:39.488710 140026075662080 logging_writer.py:48] [135336] accumulated_eval_time=1624.466960, accumulated_logging_time=4.833124, accumulated_submission_time=45936.422169, global_step=135336, preemption_count=0, score=45936.422169, test/accuracy=0.583800, test/loss=1.857119, test/num_examples=10000, total_duration=47570.647971, train/accuracy=0.798370, train/loss=0.737368, validation/accuracy=0.715280, validation/loss=1.145510, validation/num_examples=50000
I0128 20:33:01.463877 140026151130880 logging_writer.py:48] [135400] global_step=135400, grad_norm=6.592905044555664, loss=1.3022596836090088
I0128 20:33:35.324638 140026075662080 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.841980457305908, loss=1.1800930500030518
I0128 20:34:09.230506 140026151130880 logging_writer.py:48] [135600] global_step=135600, grad_norm=6.465489387512207, loss=1.3569321632385254
I0128 20:34:43.145296 140026075662080 logging_writer.py:48] [135700] global_step=135700, grad_norm=5.988889217376709, loss=1.2525694370269775
I0128 20:35:17.052927 140026151130880 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.578445911407471, loss=1.2436058521270752
I0128 20:35:50.975909 140026075662080 logging_writer.py:48] [135900] global_step=135900, grad_norm=5.888339519500732, loss=1.2009177207946777
I0128 20:36:24.904288 140026151130880 logging_writer.py:48] [136000] global_step=136000, grad_norm=6.129553318023682, loss=1.2449983358383179
I0128 20:36:58.802241 140026075662080 logging_writer.py:48] [136100] global_step=136100, grad_norm=6.702934741973877, loss=1.2950937747955322
I0128 20:37:32.672564 140026151130880 logging_writer.py:48] [136200] global_step=136200, grad_norm=6.227941989898682, loss=1.1603983640670776
I0128 20:38:06.766110 140026075662080 logging_writer.py:48] [136300] global_step=136300, grad_norm=6.6403489112854, loss=1.1555756330490112
I0128 20:38:40.666671 140026151130880 logging_writer.py:48] [136400] global_step=136400, grad_norm=6.396077632904053, loss=1.1953344345092773
I0128 20:39:14.569151 140026075662080 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.163559913635254, loss=1.3177272081375122
I0128 20:39:48.458677 140026151130880 logging_writer.py:48] [136600] global_step=136600, grad_norm=6.666452884674072, loss=1.214158058166504
I0128 20:40:22.341228 140026075662080 logging_writer.py:48] [136700] global_step=136700, grad_norm=6.267289161682129, loss=1.2033157348632812
I0128 20:40:56.276558 140026151130880 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.9568071365356445, loss=1.1740384101867676
I0128 20:41:09.631265 140187804313408 spec.py:321] Evaluating on the training split.
I0128 20:41:15.843313 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 20:41:24.529457 140187804313408 spec.py:349] Evaluating on the test split.
I0128 20:41:27.153283 140187804313408 submission_runner.py:408] Time since start: 48098.35s, 	Step: 136841, 	{'train/accuracy': 0.8374122977256775, 'train/loss': 0.5970585942268372, 'validation/accuracy': 0.7161399722099304, 'validation/loss': 1.1301541328430176, 'validation/num_examples': 50000, 'test/accuracy': 0.5967000126838684, 'test/loss': 1.7885128259658813, 'test/num_examples': 10000, 'score': 46446.49920320511, 'total_duration': 48098.353201150894, 'accumulated_submission_time': 46446.49920320511, 'accumulated_eval_time': 1641.9889187812805, 'accumulated_logging_time': 4.884382009506226}
I0128 20:41:27.212519 140026067269376 logging_writer.py:48] [136841] accumulated_eval_time=1641.988919, accumulated_logging_time=4.884382, accumulated_submission_time=46446.499203, global_step=136841, preemption_count=0, score=46446.499203, test/accuracy=0.596700, test/loss=1.788513, test/num_examples=10000, total_duration=48098.353201, train/accuracy=0.837412, train/loss=0.597059, validation/accuracy=0.716140, validation/loss=1.130154, validation/num_examples=50000
I0128 20:41:47.502003 140026075662080 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.50466251373291, loss=1.0763068199157715
I0128 20:42:21.329079 140026067269376 logging_writer.py:48] [137000] global_step=137000, grad_norm=6.965311527252197, loss=1.3535279035568237
I0128 20:42:55.193523 140026075662080 logging_writer.py:48] [137100] global_step=137100, grad_norm=6.4331135749816895, loss=1.2723952531814575
I0128 20:43:29.064839 140026067269376 logging_writer.py:48] [137200] global_step=137200, grad_norm=6.558582305908203, loss=1.251203179359436
I0128 20:44:03.321563 140026075662080 logging_writer.py:48] [137300] global_step=137300, grad_norm=6.32535457611084, loss=1.1589224338531494
I0128 20:44:37.310896 140026067269376 logging_writer.py:48] [137400] global_step=137400, grad_norm=6.498640537261963, loss=1.2739593982696533
I0128 20:45:11.195194 140026075662080 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.669374465942383, loss=1.1797852516174316
I0128 20:45:45.076113 140026067269376 logging_writer.py:48] [137600] global_step=137600, grad_norm=6.069193363189697, loss=1.218953251838684
I0128 20:46:18.956972 140026075662080 logging_writer.py:48] [137700] global_step=137700, grad_norm=7.279107570648193, loss=1.2844873666763306
I0128 20:46:52.846142 140026067269376 logging_writer.py:48] [137800] global_step=137800, grad_norm=6.630146503448486, loss=1.2388584613800049
I0128 20:47:26.757337 140026075662080 logging_writer.py:48] [137900] global_step=137900, grad_norm=6.389238357543945, loss=1.2338756322860718
I0128 20:48:00.654947 140026067269376 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.891329765319824, loss=1.1769025325775146
I0128 20:48:34.563021 140026075662080 logging_writer.py:48] [138100] global_step=138100, grad_norm=6.860827922821045, loss=1.2938991785049438
I0128 20:49:08.433580 140026067269376 logging_writer.py:48] [138200] global_step=138200, grad_norm=6.114697456359863, loss=1.225158929824829
I0128 20:49:42.375714 140026075662080 logging_writer.py:48] [138300] global_step=138300, grad_norm=6.164547443389893, loss=1.2859183549880981
I0128 20:49:57.397810 140187804313408 spec.py:321] Evaluating on the training split.
I0128 20:50:03.826414 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 20:50:12.580161 140187804313408 spec.py:349] Evaluating on the test split.
I0128 20:50:15.183935 140187804313408 submission_runner.py:408] Time since start: 48626.38s, 	Step: 138346, 	{'train/accuracy': 0.8237802982330322, 'train/loss': 0.637283980846405, 'validation/accuracy': 0.7233399748802185, 'validation/loss': 1.104615330696106, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.7960389852523804, 'test/num_examples': 10000, 'score': 46956.61994481087, 'total_duration': 48626.38387274742, 'accumulated_submission_time': 46956.61994481087, 'accumulated_eval_time': 1659.7750248908997, 'accumulated_logging_time': 4.95368218421936}
I0128 20:50:15.229665 140026050483968 logging_writer.py:48] [138346] accumulated_eval_time=1659.775025, accumulated_logging_time=4.953682, accumulated_submission_time=46956.619945, global_step=138346, preemption_count=0, score=46956.619945, test/accuracy=0.599900, test/loss=1.796039, test/num_examples=10000, total_duration=48626.383873, train/accuracy=0.823780, train/loss=0.637284, validation/accuracy=0.723340, validation/loss=1.104615, validation/num_examples=50000
I0128 20:50:33.855211 140026058876672 logging_writer.py:48] [138400] global_step=138400, grad_norm=5.980234146118164, loss=1.1685583591461182
I0128 20:51:07.752067 140026050483968 logging_writer.py:48] [138500] global_step=138500, grad_norm=7.41706657409668, loss=1.2076525688171387
I0128 20:51:41.625603 140026058876672 logging_writer.py:48] [138600] global_step=138600, grad_norm=6.774522304534912, loss=1.151609182357788
I0128 20:52:15.495409 140026050483968 logging_writer.py:48] [138700] global_step=138700, grad_norm=7.272353172302246, loss=1.2197954654693604
I0128 20:52:49.408937 140026058876672 logging_writer.py:48] [138800] global_step=138800, grad_norm=7.136131763458252, loss=1.2487428188323975
I0128 20:53:23.294033 140026050483968 logging_writer.py:48] [138900] global_step=138900, grad_norm=6.258989334106445, loss=1.2074440717697144
I0128 20:53:57.232657 140026058876672 logging_writer.py:48] [139000] global_step=139000, grad_norm=5.993138313293457, loss=1.3026179075241089
I0128 20:54:31.119969 140026050483968 logging_writer.py:48] [139100] global_step=139100, grad_norm=5.588773250579834, loss=1.1835576295852661
I0128 20:55:05.032827 140026058876672 logging_writer.py:48] [139200] global_step=139200, grad_norm=6.038233280181885, loss=1.1533267498016357
I0128 20:55:38.925918 140026050483968 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.376896381378174, loss=1.1760597229003906
I0128 20:56:12.824822 140026058876672 logging_writer.py:48] [139400] global_step=139400, grad_norm=6.316087245941162, loss=1.1752291917800903
I0128 20:56:46.711982 140026050483968 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.491457462310791, loss=1.2274394035339355
I0128 20:57:20.685964 140026058876672 logging_writer.py:48] [139600] global_step=139600, grad_norm=6.004854679107666, loss=1.2065224647521973
I0128 20:57:54.594760 140026050483968 logging_writer.py:48] [139700] global_step=139700, grad_norm=7.0619378089904785, loss=1.200015902519226
I0128 20:58:28.472167 140026058876672 logging_writer.py:48] [139800] global_step=139800, grad_norm=7.02486515045166, loss=1.1431851387023926
I0128 20:58:45.236442 140187804313408 spec.py:321] Evaluating on the training split.
I0128 20:58:51.432039 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 20:59:00.473123 140187804313408 spec.py:349] Evaluating on the test split.
I0128 20:59:03.088579 140187804313408 submission_runner.py:408] Time since start: 49154.29s, 	Step: 139851, 	{'train/accuracy': 0.8210897445678711, 'train/loss': 0.6470831036567688, 'validation/accuracy': 0.7231400012969971, 'validation/loss': 1.1141338348388672, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.7942116260528564, 'test/num_examples': 10000, 'score': 47466.56193733215, 'total_duration': 49154.288517951965, 'accumulated_submission_time': 47466.56193733215, 'accumulated_eval_time': 1677.6271243095398, 'accumulated_logging_time': 5.008821725845337}
I0128 20:59:03.131845 140026050483968 logging_writer.py:48] [139851] accumulated_eval_time=1677.627124, accumulated_logging_time=5.008822, accumulated_submission_time=47466.561937, global_step=139851, preemption_count=0, score=47466.561937, test/accuracy=0.601100, test/loss=1.794212, test/num_examples=10000, total_duration=49154.288518, train/accuracy=0.821090, train/loss=0.647083, validation/accuracy=0.723140, validation/loss=1.114134, validation/num_examples=50000
I0128 20:59:20.056227 140026159523584 logging_writer.py:48] [139900] global_step=139900, grad_norm=6.182214260101318, loss=1.2399715185165405
I0128 20:59:53.928115 140026050483968 logging_writer.py:48] [140000] global_step=140000, grad_norm=6.635210990905762, loss=1.2465989589691162
I0128 21:00:27.781299 140026159523584 logging_writer.py:48] [140100] global_step=140100, grad_norm=6.769287109375, loss=1.1952879428863525
I0128 21:01:01.630386 140026050483968 logging_writer.py:48] [140200] global_step=140200, grad_norm=6.485033988952637, loss=1.1950632333755493
I0128 21:01:35.540840 140026159523584 logging_writer.py:48] [140300] global_step=140300, grad_norm=7.0767927169799805, loss=1.2263457775115967
I0128 21:02:09.451967 140026050483968 logging_writer.py:48] [140400] global_step=140400, grad_norm=7.014736652374268, loss=1.2279274463653564
I0128 21:02:43.341403 140026159523584 logging_writer.py:48] [140500] global_step=140500, grad_norm=6.34382438659668, loss=1.231009602546692
I0128 21:03:17.235771 140026050483968 logging_writer.py:48] [140600] global_step=140600, grad_norm=6.29957914352417, loss=1.184610366821289
I0128 21:03:51.190321 140026159523584 logging_writer.py:48] [140700] global_step=140700, grad_norm=7.005753517150879, loss=1.1959218978881836
I0128 21:04:25.094178 140026050483968 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.708428859710693, loss=1.1992409229278564
I0128 21:04:58.971248 140026159523584 logging_writer.py:48] [140900] global_step=140900, grad_norm=6.56949520111084, loss=1.2372649908065796
I0128 21:05:32.856049 140026050483968 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.638563632965088, loss=1.2431063652038574
I0128 21:06:06.739336 140026159523584 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.275874614715576, loss=1.2023847103118896
I0128 21:06:40.660290 140026050483968 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.319324493408203, loss=1.162051796913147
I0128 21:07:14.557216 140026159523584 logging_writer.py:48] [141300] global_step=141300, grad_norm=6.9644317626953125, loss=1.1337660551071167
I0128 21:07:33.328713 140187804313408 spec.py:321] Evaluating on the training split.
I0128 21:07:39.512858 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 21:07:48.311421 140187804313408 spec.py:349] Evaluating on the test split.
I0128 21:07:50.906312 140187804313408 submission_runner.py:408] Time since start: 49682.11s, 	Step: 141357, 	{'train/accuracy': 0.8201530575752258, 'train/loss': 0.6532760858535767, 'validation/accuracy': 0.7235199809074402, 'validation/loss': 1.1173137426376343, 'validation/num_examples': 50000, 'test/accuracy': 0.5990000367164612, 'test/loss': 1.7983546257019043, 'test/num_examples': 10000, 'score': 47976.69469380379, 'total_duration': 49682.10616827011, 'accumulated_submission_time': 47976.69469380379, 'accumulated_eval_time': 1695.204603433609, 'accumulated_logging_time': 5.06158185005188}
I0128 21:07:50.952584 140026067269376 logging_writer.py:48] [141357] accumulated_eval_time=1695.204603, accumulated_logging_time=5.061582, accumulated_submission_time=47976.694694, global_step=141357, preemption_count=0, score=47976.694694, test/accuracy=0.599000, test/loss=1.798355, test/num_examples=10000, total_duration=49682.106168, train/accuracy=0.820153, train/loss=0.653276, validation/accuracy=0.723520, validation/loss=1.117314, validation/num_examples=50000
I0128 21:08:05.878816 140026075662080 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.456221580505371, loss=1.147831678390503
I0128 21:08:39.701451 140026067269376 logging_writer.py:48] [141500] global_step=141500, grad_norm=6.893975257873535, loss=1.1426348686218262
I0128 21:09:13.560998 140026075662080 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.521117210388184, loss=1.1360772848129272
I0128 21:09:47.705067 140026067269376 logging_writer.py:48] [141700] global_step=141700, grad_norm=6.96842098236084, loss=1.3049957752227783
I0128 21:10:21.561829 140026075662080 logging_writer.py:48] [141800] global_step=141800, grad_norm=7.048386573791504, loss=1.216417670249939
I0128 21:10:55.500980 140026067269376 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.578663349151611, loss=1.1073400974273682
I0128 21:11:29.381757 140026075662080 logging_writer.py:48] [142000] global_step=142000, grad_norm=6.528735637664795, loss=1.0859620571136475
I0128 21:12:03.299630 140026067269376 logging_writer.py:48] [142100] global_step=142100, grad_norm=6.256194114685059, loss=1.1192982196807861
I0128 21:12:37.160581 140026075662080 logging_writer.py:48] [142200] global_step=142200, grad_norm=6.195710182189941, loss=1.1193313598632812
I0128 21:13:11.094674 140026067269376 logging_writer.py:48] [142300] global_step=142300, grad_norm=7.646390914916992, loss=1.2428034543991089
I0128 21:13:44.984990 140026075662080 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.3922119140625, loss=1.1355688571929932
I0128 21:14:18.928724 140026067269376 logging_writer.py:48] [142500] global_step=142500, grad_norm=6.4024128913879395, loss=1.172167420387268
I0128 21:14:52.822039 140026075662080 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.15926456451416, loss=1.1026510000228882
I0128 21:15:26.744692 140026067269376 logging_writer.py:48] [142700] global_step=142700, grad_norm=7.446349620819092, loss=1.1398224830627441
I0128 21:16:00.618615 140026075662080 logging_writer.py:48] [142800] global_step=142800, grad_norm=6.623601913452148, loss=1.16373610496521
I0128 21:16:21.213612 140187804313408 spec.py:321] Evaluating on the training split.
I0128 21:16:27.478376 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 21:16:36.392155 140187804313408 spec.py:349] Evaluating on the test split.
I0128 21:16:39.131577 140187804313408 submission_runner.py:408] Time since start: 50210.33s, 	Step: 142862, 	{'train/accuracy': 0.8167450428009033, 'train/loss': 0.6668460965156555, 'validation/accuracy': 0.7207799553871155, 'validation/loss': 1.1217389106750488, 'validation/num_examples': 50000, 'test/accuracy': 0.5948000550270081, 'test/loss': 1.8159778118133545, 'test/num_examples': 10000, 'score': 48486.89080500603, 'total_duration': 50210.33151769638, 'accumulated_submission_time': 48486.89080500603, 'accumulated_eval_time': 1713.1225311756134, 'accumulated_logging_time': 5.118523836135864}
I0128 21:16:39.173264 140026050483968 logging_writer.py:48] [142862] accumulated_eval_time=1713.122531, accumulated_logging_time=5.118524, accumulated_submission_time=48486.890805, global_step=142862, preemption_count=0, score=48486.890805, test/accuracy=0.594800, test/loss=1.815978, test/num_examples=10000, total_duration=50210.331518, train/accuracy=0.816745, train/loss=0.666846, validation/accuracy=0.720780, validation/loss=1.121739, validation/num_examples=50000
I0128 21:16:52.375668 140026058876672 logging_writer.py:48] [142900] global_step=142900, grad_norm=6.985395908355713, loss=1.314984917640686
I0128 21:17:26.198362 140026050483968 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.323948383331299, loss=1.1630479097366333
I0128 21:18:00.092163 140026058876672 logging_writer.py:48] [143100] global_step=143100, grad_norm=6.757782936096191, loss=1.125020980834961
I0128 21:18:33.955546 140026050483968 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.373439788818359, loss=1.0513399839401245
I0128 21:19:07.820110 140026058876672 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.817915916442871, loss=1.238997220993042
I0128 21:19:41.711301 140026050483968 logging_writer.py:48] [143400] global_step=143400, grad_norm=6.126743316650391, loss=1.1361068487167358
I0128 21:20:15.589810 140026058876672 logging_writer.py:48] [143500] global_step=143500, grad_norm=7.186723232269287, loss=1.1316518783569336
I0128 21:20:49.458343 140026050483968 logging_writer.py:48] [143600] global_step=143600, grad_norm=7.622063636779785, loss=1.2136884927749634
I0128 21:21:23.334182 140026058876672 logging_writer.py:48] [143700] global_step=143700, grad_norm=6.7240424156188965, loss=1.1340181827545166
I0128 21:21:57.213834 140026050483968 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.583342552185059, loss=1.0878217220306396
I0128 21:22:31.167863 140026058876672 logging_writer.py:48] [143900] global_step=143900, grad_norm=6.34408712387085, loss=1.1201207637786865
I0128 21:23:05.076171 140026050483968 logging_writer.py:48] [144000] global_step=144000, grad_norm=7.637700080871582, loss=1.190513253211975
I0128 21:23:38.966327 140026058876672 logging_writer.py:48] [144100] global_step=144100, grad_norm=6.469933986663818, loss=1.1998473405838013
I0128 21:24:12.868177 140026050483968 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.569514274597168, loss=1.1893044710159302
I0128 21:24:46.769024 140026058876672 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.963666915893555, loss=1.1522260904312134
I0128 21:25:09.268886 140187804313408 spec.py:321] Evaluating on the training split.
I0128 21:25:15.450381 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 21:25:24.235866 140187804313408 spec.py:349] Evaluating on the test split.
I0128 21:25:26.848412 140187804313408 submission_runner.py:408] Time since start: 50738.05s, 	Step: 144368, 	{'train/accuracy': 0.8262914419174194, 'train/loss': 0.6314259767532349, 'validation/accuracy': 0.7257599830627441, 'validation/loss': 1.0909123420715332, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.7762413024902344, 'test/num_examples': 10000, 'score': 48996.922676324844, 'total_duration': 50738.048347473145, 'accumulated_submission_time': 48996.922676324844, 'accumulated_eval_time': 1730.7020156383514, 'accumulated_logging_time': 5.1694557666778564}
I0128 21:25:26.898918 140026159523584 logging_writer.py:48] [144368] accumulated_eval_time=1730.702016, accumulated_logging_time=5.169456, accumulated_submission_time=48996.922676, global_step=144368, preemption_count=0, score=48996.922676, test/accuracy=0.601000, test/loss=1.776241, test/num_examples=10000, total_duration=50738.048347, train/accuracy=0.826291, train/loss=0.631426, validation/accuracy=0.725760, validation/loss=1.090912, validation/num_examples=50000
I0128 21:25:38.092362 140026167916288 logging_writer.py:48] [144400] global_step=144400, grad_norm=7.215527534484863, loss=1.1752663850784302
I0128 21:26:11.910628 140026159523584 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.453895568847656, loss=1.1571494340896606
I0128 21:26:45.782570 140026167916288 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.972940921783447, loss=1.1352128982543945
I0128 21:27:19.676487 140026159523584 logging_writer.py:48] [144700] global_step=144700, grad_norm=7.206160068511963, loss=1.2336502075195312
I0128 21:27:53.553304 140026167916288 logging_writer.py:48] [144800] global_step=144800, grad_norm=7.309403896331787, loss=1.1855316162109375
I0128 21:28:27.445307 140026159523584 logging_writer.py:48] [144900] global_step=144900, grad_norm=6.768700122833252, loss=1.1707545518875122
I0128 21:29:01.399762 140026167916288 logging_writer.py:48] [145000] global_step=145000, grad_norm=7.100604057312012, loss=1.1143854856491089
I0128 21:29:35.296026 140026159523584 logging_writer.py:48] [145100] global_step=145100, grad_norm=7.232025623321533, loss=1.1191812753677368
I0128 21:30:09.168720 140026167916288 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.72270393371582, loss=1.1158533096313477
I0128 21:30:43.058159 140026159523584 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.8992018699646, loss=1.1229591369628906
I0128 21:31:16.952528 140026167916288 logging_writer.py:48] [145400] global_step=145400, grad_norm=6.901646614074707, loss=1.0533254146575928
I0128 21:31:50.866632 140026159523584 logging_writer.py:48] [145500] global_step=145500, grad_norm=6.709591388702393, loss=1.1043164730072021
I0128 21:32:24.780664 140026167916288 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.993575096130371, loss=1.0577783584594727
I0128 21:32:58.697203 140026159523584 logging_writer.py:48] [145700] global_step=145700, grad_norm=6.980823993682861, loss=1.1133143901824951
I0128 21:33:32.599773 140026167916288 logging_writer.py:48] [145800] global_step=145800, grad_norm=7.426849365234375, loss=1.253983974456787
I0128 21:33:57.160087 140187804313408 spec.py:321] Evaluating on the training split.
I0128 21:34:03.568449 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 21:34:12.426656 140187804313408 spec.py:349] Evaluating on the test split.
I0128 21:34:14.878516 140187804313408 submission_runner.py:408] Time since start: 51266.08s, 	Step: 145874, 	{'train/accuracy': 0.8601721525192261, 'train/loss': 0.5021381378173828, 'validation/accuracy': 0.732479989528656, 'validation/loss': 1.0820106267929077, 'validation/num_examples': 50000, 'test/accuracy': 0.6047000288963318, 'test/loss': 1.778953194618225, 'test/num_examples': 10000, 'score': 49507.11916804314, 'total_duration': 51266.078447818756, 'accumulated_submission_time': 49507.11916804314, 'accumulated_eval_time': 1748.4203968048096, 'accumulated_logging_time': 5.229321718215942}
I0128 21:34:14.922089 140026075662080 logging_writer.py:48] [145874] accumulated_eval_time=1748.420397, accumulated_logging_time=5.229322, accumulated_submission_time=49507.119168, global_step=145874, preemption_count=0, score=49507.119168, test/accuracy=0.604700, test/loss=1.778953, test/num_examples=10000, total_duration=51266.078448, train/accuracy=0.860172, train/loss=0.502138, validation/accuracy=0.732480, validation/loss=1.082011, validation/num_examples=50000
I0128 21:34:24.055860 140026151130880 logging_writer.py:48] [145900] global_step=145900, grad_norm=6.760234832763672, loss=1.0606672763824463
I0128 21:34:57.910112 140026075662080 logging_writer.py:48] [146000] global_step=146000, grad_norm=8.04835319519043, loss=1.1455918550491333
I0128 21:35:31.833165 140026151130880 logging_writer.py:48] [146100] global_step=146100, grad_norm=6.806695938110352, loss=1.038955807685852
I0128 21:36:05.753489 140026075662080 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.709886074066162, loss=1.0635464191436768
I0128 21:36:39.642269 140026151130880 logging_writer.py:48] [146300] global_step=146300, grad_norm=7.282581806182861, loss=1.1105775833129883
I0128 21:37:13.502042 140026075662080 logging_writer.py:48] [146400] global_step=146400, grad_norm=8.2510404586792, loss=1.1228039264678955
I0128 21:37:47.414592 140026151130880 logging_writer.py:48] [146500] global_step=146500, grad_norm=7.303922176361084, loss=1.1096835136413574
I0128 21:38:21.299621 140026075662080 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.6122589111328125, loss=1.1575292348861694
I0128 21:38:55.223187 140026151130880 logging_writer.py:48] [146700] global_step=146700, grad_norm=7.1597771644592285, loss=1.0735892057418823
I0128 21:39:29.101093 140026075662080 logging_writer.py:48] [146800] global_step=146800, grad_norm=7.28653621673584, loss=1.1116275787353516
I0128 21:40:02.974288 140026151130880 logging_writer.py:48] [146900] global_step=146900, grad_norm=7.293272495269775, loss=1.1561191082000732
I0128 21:40:36.866266 140026075662080 logging_writer.py:48] [147000] global_step=147000, grad_norm=7.759393692016602, loss=1.1587848663330078
I0128 21:41:10.725661 140026151130880 logging_writer.py:48] [147100] global_step=147100, grad_norm=7.212089538574219, loss=1.1491260528564453
I0128 21:41:44.710992 140026075662080 logging_writer.py:48] [147200] global_step=147200, grad_norm=6.607454776763916, loss=1.0767570734024048
I0128 21:42:18.575371 140026151130880 logging_writer.py:48] [147300] global_step=147300, grad_norm=7.157264709472656, loss=1.0753735303878784
I0128 21:42:45.141401 140187804313408 spec.py:321] Evaluating on the training split.
I0128 21:42:51.368509 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 21:43:00.136494 140187804313408 spec.py:349] Evaluating on the test split.
I0128 21:43:02.762396 140187804313408 submission_runner.py:408] Time since start: 51793.96s, 	Step: 147380, 	{'train/accuracy': 0.8510442972183228, 'train/loss': 0.5359805822372437, 'validation/accuracy': 0.7305399775505066, 'validation/loss': 1.0758004188537598, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.7504609823226929, 'test/num_examples': 10000, 'score': 50017.27359175682, 'total_duration': 51793.96232128143, 'accumulated_submission_time': 50017.27359175682, 'accumulated_eval_time': 1766.0413398742676, 'accumulated_logging_time': 5.282959222793579}
I0128 21:43:02.809687 140026042091264 logging_writer.py:48] [147380] accumulated_eval_time=1766.041340, accumulated_logging_time=5.282959, accumulated_submission_time=50017.273592, global_step=147380, preemption_count=0, score=50017.273592, test/accuracy=0.608900, test/loss=1.750461, test/num_examples=10000, total_duration=51793.962321, train/accuracy=0.851044, train/loss=0.535981, validation/accuracy=0.730540, validation/loss=1.075800, validation/num_examples=50000
I0128 21:43:09.946008 140026050483968 logging_writer.py:48] [147400] global_step=147400, grad_norm=7.139333248138428, loss=1.0589632987976074
I0128 21:43:43.750771 140026042091264 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.198481559753418, loss=1.0655776262283325
I0128 21:44:17.631865 140026050483968 logging_writer.py:48] [147600] global_step=147600, grad_norm=7.334766387939453, loss=1.121086835861206
I0128 21:44:51.560041 140026042091264 logging_writer.py:48] [147700] global_step=147700, grad_norm=7.64885139465332, loss=1.1411572694778442
I0128 21:45:25.463163 140026050483968 logging_writer.py:48] [147800] global_step=147800, grad_norm=7.050488471984863, loss=1.0919643640518188
I0128 21:45:59.397146 140026042091264 logging_writer.py:48] [147900] global_step=147900, grad_norm=7.400414943695068, loss=1.191933274269104
I0128 21:46:33.283763 140026050483968 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.120337009429932, loss=1.101902723312378
I0128 21:47:07.203754 140026042091264 logging_writer.py:48] [148100] global_step=148100, grad_norm=8.286779403686523, loss=1.0541154146194458
I0128 21:47:41.113237 140026050483968 logging_writer.py:48] [148200] global_step=148200, grad_norm=7.314601421356201, loss=1.0672297477722168
I0128 21:48:15.092767 140026042091264 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.851492404937744, loss=0.9562890529632568
I0128 21:48:49.011883 140026050483968 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.796742916107178, loss=1.0540424585342407
I0128 21:49:22.884029 140026042091264 logging_writer.py:48] [148500] global_step=148500, grad_norm=6.8710618019104, loss=1.127875804901123
I0128 21:49:56.816291 140026050483968 logging_writer.py:48] [148600] global_step=148600, grad_norm=7.096286296844482, loss=1.0862842798233032
I0128 21:50:30.688882 140026042091264 logging_writer.py:48] [148700] global_step=148700, grad_norm=7.290996551513672, loss=1.1226627826690674
I0128 21:51:04.615045 140026050483968 logging_writer.py:48] [148800] global_step=148800, grad_norm=7.88344669342041, loss=1.0263956785202026
I0128 21:51:32.877269 140187804313408 spec.py:321] Evaluating on the training split.
I0128 21:51:39.251397 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 21:51:48.010091 140187804313408 spec.py:349] Evaluating on the test split.
I0128 21:51:50.686616 140187804313408 submission_runner.py:408] Time since start: 52321.89s, 	Step: 148885, 	{'train/accuracy': 0.8487523794174194, 'train/loss': 0.5430769324302673, 'validation/accuracy': 0.7354199886322021, 'validation/loss': 1.058487057685852, 'validation/num_examples': 50000, 'test/accuracy': 0.6100000143051147, 'test/loss': 1.7439473867416382, 'test/num_examples': 10000, 'score': 50527.27390384674, 'total_duration': 52321.88641309738, 'accumulated_submission_time': 50527.27390384674, 'accumulated_eval_time': 1783.8505229949951, 'accumulated_logging_time': 5.341572046279907}
I0128 21:51:50.736630 140026159523584 logging_writer.py:48] [148885] accumulated_eval_time=1783.850523, accumulated_logging_time=5.341572, accumulated_submission_time=50527.273904, global_step=148885, preemption_count=0, score=50527.273904, test/accuracy=0.610000, test/loss=1.743947, test/num_examples=10000, total_duration=52321.886413, train/accuracy=0.848752, train/loss=0.543077, validation/accuracy=0.735420, validation/loss=1.058487, validation/num_examples=50000
I0128 21:51:56.155238 140026167916288 logging_writer.py:48] [148900] global_step=148900, grad_norm=6.858046531677246, loss=1.0402745008468628
I0128 21:52:29.983784 140026159523584 logging_writer.py:48] [149000] global_step=149000, grad_norm=7.203763008117676, loss=1.1480488777160645
I0128 21:53:03.862346 140026167916288 logging_writer.py:48] [149100] global_step=149100, grad_norm=7.529423236846924, loss=1.1420296430587769
I0128 21:53:37.775463 140026159523584 logging_writer.py:48] [149200] global_step=149200, grad_norm=6.928211212158203, loss=1.045229196548462
I0128 21:54:11.675671 140026167916288 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.835110187530518, loss=1.1040210723876953
I0128 21:54:45.651457 140026159523584 logging_writer.py:48] [149400] global_step=149400, grad_norm=7.287618637084961, loss=1.068508505821228
I0128 21:55:19.575968 140026167916288 logging_writer.py:48] [149500] global_step=149500, grad_norm=7.293938159942627, loss=1.1419605016708374
I0128 21:55:53.465515 140026159523584 logging_writer.py:48] [149600] global_step=149600, grad_norm=7.743621349334717, loss=1.1063308715820312
I0128 21:56:27.373540 140026167916288 logging_writer.py:48] [149700] global_step=149700, grad_norm=7.259373188018799, loss=1.0767743587493896
I0128 21:57:01.279464 140026159523584 logging_writer.py:48] [149800] global_step=149800, grad_norm=7.712944984436035, loss=1.0696556568145752
I0128 21:57:35.201843 140026167916288 logging_writer.py:48] [149900] global_step=149900, grad_norm=7.003842830657959, loss=1.074530839920044
I0128 21:58:09.090857 140026159523584 logging_writer.py:48] [150000] global_step=150000, grad_norm=7.206485748291016, loss=1.0032727718353271
I0128 21:58:43.000069 140026167916288 logging_writer.py:48] [150100] global_step=150100, grad_norm=7.305998802185059, loss=1.0689128637313843
I0128 21:59:16.915654 140026159523584 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.661478519439697, loss=0.982346773147583
I0128 21:59:50.830388 140026167916288 logging_writer.py:48] [150300] global_step=150300, grad_norm=7.555048942565918, loss=1.087448000907898
I0128 22:00:20.799992 140187804313408 spec.py:321] Evaluating on the training split.
I0128 22:00:27.100649 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 22:00:36.041607 140187804313408 spec.py:349] Evaluating on the test split.
I0128 22:00:38.632613 140187804313408 submission_runner.py:408] Time since start: 52849.83s, 	Step: 150390, 	{'train/accuracy': 0.8517817258834839, 'train/loss': 0.5283650159835815, 'validation/accuracy': 0.7335999608039856, 'validation/loss': 1.0614495277404785, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.7548463344573975, 'test/num_examples': 10000, 'score': 51037.273169994354, 'total_duration': 52849.83254790306, 'accumulated_submission_time': 51037.273169994354, 'accumulated_eval_time': 1801.683106660843, 'accumulated_logging_time': 5.4006664752960205}
I0128 22:00:38.678945 140026058876672 logging_writer.py:48] [150390] accumulated_eval_time=1801.683107, accumulated_logging_time=5.400666, accumulated_submission_time=51037.273170, global_step=150390, preemption_count=0, score=51037.273170, test/accuracy=0.602800, test/loss=1.754846, test/num_examples=10000, total_duration=52849.832548, train/accuracy=0.851782, train/loss=0.528365, validation/accuracy=0.733600, validation/loss=1.061450, validation/num_examples=50000
I0128 22:00:42.424903 140026067269376 logging_writer.py:48] [150400] global_step=150400, grad_norm=7.769596576690674, loss=0.9654484987258911
I0128 22:01:16.421198 140026058876672 logging_writer.py:48] [150500] global_step=150500, grad_norm=6.9591240882873535, loss=1.1270155906677246
I0128 22:01:50.289010 140026067269376 logging_writer.py:48] [150600] global_step=150600, grad_norm=7.095384120941162, loss=1.1480841636657715
I0128 22:02:24.207400 140026058876672 logging_writer.py:48] [150700] global_step=150700, grad_norm=7.5886101722717285, loss=1.032300591468811
I0128 22:02:58.078947 140026067269376 logging_writer.py:48] [150800] global_step=150800, grad_norm=7.511800765991211, loss=1.0092216730117798
I0128 22:03:32.009949 140026058876672 logging_writer.py:48] [150900] global_step=150900, grad_norm=8.257582664489746, loss=1.1863096952438354
I0128 22:04:05.912593 140026067269376 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.684237003326416, loss=0.9545931816101074
I0128 22:04:39.825165 140026058876672 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.637970924377441, loss=1.0897423028945923
I0128 22:05:13.702790 140026067269376 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.605355262756348, loss=1.050255298614502
I0128 22:05:47.636359 140026058876672 logging_writer.py:48] [151300] global_step=151300, grad_norm=7.257942199707031, loss=1.0764869451522827
I0128 22:06:21.531909 140026067269376 logging_writer.py:48] [151400] global_step=151400, grad_norm=8.246135711669922, loss=1.0753253698349
I0128 22:06:55.450419 140026058876672 logging_writer.py:48] [151500] global_step=151500, grad_norm=7.499073028564453, loss=1.0627202987670898
I0128 22:07:29.439150 140026067269376 logging_writer.py:48] [151600] global_step=151600, grad_norm=8.026384353637695, loss=1.0537484884262085
I0128 22:08:03.388265 140026058876672 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.830391883850098, loss=0.9968744516372681
I0128 22:08:37.294304 140026067269376 logging_writer.py:48] [151800] global_step=151800, grad_norm=7.71054744720459, loss=1.0568392276763916
I0128 22:09:08.953315 140187804313408 spec.py:321] Evaluating on the training split.
I0128 22:09:15.170701 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 22:09:23.931346 140187804313408 spec.py:349] Evaluating on the test split.
I0128 22:09:26.535285 140187804313408 submission_runner.py:408] Time since start: 53377.74s, 	Step: 151895, 	{'train/accuracy': 0.8530372977256775, 'train/loss': 0.5189365148544312, 'validation/accuracy': 0.7375400066375732, 'validation/loss': 1.058759093284607, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.7296031713485718, 'test/num_examples': 10000, 'score': 51547.484359025955, 'total_duration': 53377.73521447182, 'accumulated_submission_time': 51547.484359025955, 'accumulated_eval_time': 1819.2650346755981, 'accumulated_logging_time': 5.456265211105347}
I0128 22:09:26.579897 140026058876672 logging_writer.py:48] [151895] accumulated_eval_time=1819.265035, accumulated_logging_time=5.456265, accumulated_submission_time=51547.484359, global_step=151895, preemption_count=0, score=51547.484359, test/accuracy=0.614800, test/loss=1.729603, test/num_examples=10000, total_duration=53377.735214, train/accuracy=0.853037, train/loss=0.518937, validation/accuracy=0.737540, validation/loss=1.058759, validation/num_examples=50000
I0128 22:09:28.628076 140026151130880 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.928321838378906, loss=1.1104321479797363
I0128 22:10:02.451520 140026058876672 logging_writer.py:48] [152000] global_step=152000, grad_norm=7.29703426361084, loss=1.0906141996383667
I0128 22:10:36.331415 140026151130880 logging_writer.py:48] [152100] global_step=152100, grad_norm=7.155725002288818, loss=1.0774929523468018
I0128 22:11:10.236513 140026058876672 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.674264907836914, loss=1.020073413848877
I0128 22:11:44.139349 140026151130880 logging_writer.py:48] [152300] global_step=152300, grad_norm=7.620876789093018, loss=1.0814896821975708
I0128 22:12:18.012256 140026058876672 logging_writer.py:48] [152400] global_step=152400, grad_norm=7.513594627380371, loss=0.9983429312705994
I0128 22:12:51.923591 140026151130880 logging_writer.py:48] [152500] global_step=152500, grad_norm=7.6153974533081055, loss=0.9606351256370544
I0128 22:13:25.821618 140026058876672 logging_writer.py:48] [152600] global_step=152600, grad_norm=7.1187520027160645, loss=1.1123815774917603
I0128 22:13:59.770451 140026151130880 logging_writer.py:48] [152700] global_step=152700, grad_norm=7.159648418426514, loss=0.9791794419288635
I0128 22:14:33.658912 140026058876672 logging_writer.py:48] [152800] global_step=152800, grad_norm=7.013692855834961, loss=1.005570888519287
I0128 22:15:07.610464 140026151130880 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.4855523109436035, loss=1.0720945596694946
I0128 22:15:41.462620 140026058876672 logging_writer.py:48] [153000] global_step=153000, grad_norm=7.40302038192749, loss=1.0461418628692627
I0128 22:16:15.389903 140026151130880 logging_writer.py:48] [153100] global_step=153100, grad_norm=7.594090461730957, loss=1.0447947978973389
I0128 22:16:49.286574 140026058876672 logging_writer.py:48] [153200] global_step=153200, grad_norm=8.237913131713867, loss=1.0376617908477783
I0128 22:17:23.188041 140026151130880 logging_writer.py:48] [153300] global_step=153300, grad_norm=8.17746639251709, loss=1.0829122066497803
I0128 22:17:57.067295 140026058876672 logging_writer.py:48] [153400] global_step=153400, grad_norm=7.963542461395264, loss=1.1624031066894531
I0128 22:17:57.075412 140187804313408 spec.py:321] Evaluating on the training split.
I0128 22:18:03.432966 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 22:18:12.229585 140187804313408 spec.py:349] Evaluating on the test split.
I0128 22:18:14.833507 140187804313408 submission_runner.py:408] Time since start: 53906.03s, 	Step: 153401, 	{'train/accuracy': 0.8583585619926453, 'train/loss': 0.5022905468940735, 'validation/accuracy': 0.7417399883270264, 'validation/loss': 1.0438984632492065, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.733088731765747, 'test/num_examples': 10000, 'score': 52057.91526436806, 'total_duration': 53906.03312087059, 'accumulated_submission_time': 52057.91526436806, 'accumulated_eval_time': 1837.022742509842, 'accumulated_logging_time': 5.511146545410156}
I0128 22:18:14.880578 140026058876672 logging_writer.py:48] [153401] accumulated_eval_time=1837.022743, accumulated_logging_time=5.511147, accumulated_submission_time=52057.915264, global_step=153401, preemption_count=0, score=52057.915264, test/accuracy=0.617200, test/loss=1.733089, test/num_examples=10000, total_duration=53906.033121, train/accuracy=0.858359, train/loss=0.502291, validation/accuracy=0.741740, validation/loss=1.043898, validation/num_examples=50000
I0128 22:18:48.720741 140026067269376 logging_writer.py:48] [153500] global_step=153500, grad_norm=7.622861385345459, loss=1.073778510093689
I0128 22:19:22.587477 140026058876672 logging_writer.py:48] [153600] global_step=153600, grad_norm=7.967565059661865, loss=1.0555702447891235
I0128 22:19:56.467524 140026067269376 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.981876373291016, loss=0.9523917436599731
I0128 22:20:30.450998 140026058876672 logging_writer.py:48] [153800] global_step=153800, grad_norm=7.7417144775390625, loss=0.9388510584831238
I0128 22:21:04.363998 140026067269376 logging_writer.py:48] [153900] global_step=153900, grad_norm=7.328578472137451, loss=0.965350329875946
I0128 22:21:38.281860 140026058876672 logging_writer.py:48] [154000] global_step=154000, grad_norm=7.637963771820068, loss=1.0083729028701782
I0128 22:22:12.193175 140026067269376 logging_writer.py:48] [154100] global_step=154100, grad_norm=8.770834922790527, loss=1.1318614482879639
I0128 22:22:46.059283 140026058876672 logging_writer.py:48] [154200] global_step=154200, grad_norm=7.3105669021606445, loss=0.9318729639053345
I0128 22:23:19.970477 140026067269376 logging_writer.py:48] [154300] global_step=154300, grad_norm=7.694564342498779, loss=1.0341153144836426
I0128 22:23:53.858362 140026058876672 logging_writer.py:48] [154400] global_step=154400, grad_norm=7.451058864593506, loss=0.9086881279945374
I0128 22:24:27.755623 140026067269376 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.307801246643066, loss=1.09227454662323
I0128 22:25:01.641610 140026058876672 logging_writer.py:48] [154600] global_step=154600, grad_norm=7.070200443267822, loss=0.9226540327072144
I0128 22:25:35.560580 140026067269376 logging_writer.py:48] [154700] global_step=154700, grad_norm=7.091061592102051, loss=0.9716823697090149
I0128 22:26:09.464583 140026058876672 logging_writer.py:48] [154800] global_step=154800, grad_norm=7.47951078414917, loss=1.0797717571258545
I0128 22:26:43.413621 140026067269376 logging_writer.py:48] [154900] global_step=154900, grad_norm=8.925623893737793, loss=0.8964872360229492
I0128 22:26:44.912229 140187804313408 spec.py:321] Evaluating on the training split.
I0128 22:26:51.121577 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 22:26:59.910955 140187804313408 spec.py:349] Evaluating on the test split.
I0128 22:27:02.543032 140187804313408 submission_runner.py:408] Time since start: 54433.74s, 	Step: 154906, 	{'train/accuracy': 0.8868981003761292, 'train/loss': 0.4032793939113617, 'validation/accuracy': 0.741159975528717, 'validation/loss': 1.0439447164535522, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.7265610694885254, 'test/num_examples': 10000, 'score': 52567.88165092468, 'total_duration': 54433.74296832085, 'accumulated_submission_time': 52567.88165092468, 'accumulated_eval_time': 1854.6535007953644, 'accumulated_logging_time': 5.569386959075928}
I0128 22:27:02.591121 140026075662080 logging_writer.py:48] [154906] accumulated_eval_time=1854.653501, accumulated_logging_time=5.569387, accumulated_submission_time=52567.881651, global_step=154906, preemption_count=0, score=52567.881651, test/accuracy=0.612600, test/loss=1.726561, test/num_examples=10000, total_duration=54433.742968, train/accuracy=0.886898, train/loss=0.403279, validation/accuracy=0.741160, validation/loss=1.043945, validation/num_examples=50000
I0128 22:27:34.721094 140026151130880 logging_writer.py:48] [155000] global_step=155000, grad_norm=7.56438684463501, loss=1.0166802406311035
I0128 22:28:08.594377 140026075662080 logging_writer.py:48] [155100] global_step=155100, grad_norm=7.788368225097656, loss=0.9624960422515869
I0128 22:28:42.495246 140026151130880 logging_writer.py:48] [155200] global_step=155200, grad_norm=8.068836212158203, loss=1.0612400770187378
I0128 22:29:16.339117 140026075662080 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.996786117553711, loss=1.0731230974197388
I0128 22:29:50.256949 140026151130880 logging_writer.py:48] [155400] global_step=155400, grad_norm=7.962256908416748, loss=1.0198280811309814
I0128 22:30:24.133062 140026075662080 logging_writer.py:48] [155500] global_step=155500, grad_norm=7.1496381759643555, loss=0.912587583065033
I0128 22:30:58.023691 140026151130880 logging_writer.py:48] [155600] global_step=155600, grad_norm=7.7114081382751465, loss=0.9692973494529724
I0128 22:31:31.919292 140026075662080 logging_writer.py:48] [155700] global_step=155700, grad_norm=8.933284759521484, loss=1.0185158252716064
I0128 22:32:05.786300 140026151130880 logging_writer.py:48] [155800] global_step=155800, grad_norm=8.040650367736816, loss=1.0189732313156128
I0128 22:32:39.679420 140026075662080 logging_writer.py:48] [155900] global_step=155900, grad_norm=9.169998168945312, loss=1.071154236793518
I0128 22:33:13.619388 140026151130880 logging_writer.py:48] [156000] global_step=156000, grad_norm=8.612203598022461, loss=0.9909000396728516
I0128 22:33:47.518218 140026075662080 logging_writer.py:48] [156100] global_step=156100, grad_norm=6.966257572174072, loss=0.9347569942474365
I0128 22:34:21.391489 140026151130880 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.54124641418457, loss=1.00424063205719
I0128 22:34:55.269737 140026075662080 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.747526168823242, loss=0.9997430443763733
I0128 22:35:29.166724 140026151130880 logging_writer.py:48] [156400] global_step=156400, grad_norm=7.395843029022217, loss=0.9848768711090088
I0128 22:35:32.693439 140187804313408 spec.py:321] Evaluating on the training split.
I0128 22:35:38.914026 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 22:35:47.866093 140187804313408 spec.py:349] Evaluating on the test split.
I0128 22:35:50.504178 140187804313408 submission_runner.py:408] Time since start: 54961.70s, 	Step: 156412, 	{'train/accuracy': 0.8819355964660645, 'train/loss': 0.42019084095954895, 'validation/accuracy': 0.7441399693489075, 'validation/loss': 1.0274673700332642, 'validation/num_examples': 50000, 'test/accuracy': 0.6198000311851501, 'test/loss': 1.7015371322631836, 'test/num_examples': 10000, 'score': 53077.91947197914, 'total_duration': 54961.70411801338, 'accumulated_submission_time': 53077.91947197914, 'accumulated_eval_time': 1872.464199066162, 'accumulated_logging_time': 5.626893997192383}
I0128 22:35:50.552404 140026067269376 logging_writer.py:48] [156412] accumulated_eval_time=1872.464199, accumulated_logging_time=5.626894, accumulated_submission_time=53077.919472, global_step=156412, preemption_count=0, score=53077.919472, test/accuracy=0.619800, test/loss=1.701537, test/num_examples=10000, total_duration=54961.704118, train/accuracy=0.881936, train/loss=0.420191, validation/accuracy=0.744140, validation/loss=1.027467, validation/num_examples=50000
I0128 22:36:20.695754 140026176308992 logging_writer.py:48] [156500] global_step=156500, grad_norm=8.197565078735352, loss=1.0100003480911255
I0128 22:36:54.577000 140026067269376 logging_writer.py:48] [156600] global_step=156600, grad_norm=7.69728422164917, loss=1.036360263824463
I0128 22:37:28.460657 140026176308992 logging_writer.py:48] [156700] global_step=156700, grad_norm=7.430027008056641, loss=0.9481137990951538
I0128 22:38:02.392807 140026067269376 logging_writer.py:48] [156800] global_step=156800, grad_norm=8.084707260131836, loss=0.9104591608047485
I0128 22:38:36.266963 140026176308992 logging_writer.py:48] [156900] global_step=156900, grad_norm=7.344716548919678, loss=0.8907119631767273
I0128 22:39:10.243024 140026067269376 logging_writer.py:48] [157000] global_step=157000, grad_norm=8.51380443572998, loss=1.0039960145950317
I0128 22:39:44.133774 140026176308992 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.9112043380737305, loss=1.0017544031143188
I0128 22:40:18.032115 140026067269376 logging_writer.py:48] [157200] global_step=157200, grad_norm=8.052506446838379, loss=0.9397397041320801
I0128 22:40:51.965014 140026176308992 logging_writer.py:48] [157300] global_step=157300, grad_norm=8.111717224121094, loss=0.9573256373405457
I0128 22:41:25.833484 140026067269376 logging_writer.py:48] [157400] global_step=157400, grad_norm=8.85211181640625, loss=0.988495945930481
I0128 22:41:59.726685 140026176308992 logging_writer.py:48] [157500] global_step=157500, grad_norm=7.847053050994873, loss=0.8898566961288452
I0128 22:42:33.615658 140026067269376 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.56233549118042, loss=0.975570559501648
I0128 22:43:07.527998 140026176308992 logging_writer.py:48] [157700] global_step=157700, grad_norm=8.751049041748047, loss=1.0126335620880127
I0128 22:43:41.407304 140026067269376 logging_writer.py:48] [157800] global_step=157800, grad_norm=8.533378601074219, loss=0.9267738461494446
I0128 22:44:15.303021 140026176308992 logging_writer.py:48] [157900] global_step=157900, grad_norm=9.278702735900879, loss=1.0324897766113281
I0128 22:44:20.549526 140187804313408 spec.py:321] Evaluating on the training split.
I0128 22:44:26.814291 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 22:44:35.738659 140187804313408 spec.py:349] Evaluating on the test split.
I0128 22:44:38.352653 140187804313408 submission_runner.py:408] Time since start: 55489.55s, 	Step: 157917, 	{'train/accuracy': 0.8812978267669678, 'train/loss': 0.41859835386276245, 'validation/accuracy': 0.7456600069999695, 'validation/loss': 1.027588129043579, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.7277756929397583, 'test/num_examples': 10000, 'score': 53587.85387945175, 'total_duration': 55489.55252146721, 'accumulated_submission_time': 53587.85387945175, 'accumulated_eval_time': 1890.2672145366669, 'accumulated_logging_time': 5.684285640716553}
I0128 22:44:38.400515 140026151130880 logging_writer.py:48] [157917] accumulated_eval_time=1890.267215, accumulated_logging_time=5.684286, accumulated_submission_time=53587.853879, global_step=157917, preemption_count=0, score=53587.853879, test/accuracy=0.618900, test/loss=1.727776, test/num_examples=10000, total_duration=55489.552521, train/accuracy=0.881298, train/loss=0.418598, validation/accuracy=0.745660, validation/loss=1.027588, validation/num_examples=50000
I0128 22:45:06.856772 140026159523584 logging_writer.py:48] [158000] global_step=158000, grad_norm=7.9413299560546875, loss=0.8926598429679871
I0128 22:45:40.788593 140026151130880 logging_writer.py:48] [158100] global_step=158100, grad_norm=8.537554740905762, loss=0.9430638551712036
I0128 22:46:14.669039 140026159523584 logging_writer.py:48] [158200] global_step=158200, grad_norm=8.74136734008789, loss=0.9655416011810303
I0128 22:46:48.582461 140026151130880 logging_writer.py:48] [158300] global_step=158300, grad_norm=7.905381202697754, loss=0.9913486838340759
I0128 22:47:22.462903 140026159523584 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.676555156707764, loss=1.039944052696228
I0128 22:47:56.367637 140026151130880 logging_writer.py:48] [158500] global_step=158500, grad_norm=8.453585624694824, loss=1.0722095966339111
I0128 22:48:30.272209 140026159523584 logging_writer.py:48] [158600] global_step=158600, grad_norm=7.584767818450928, loss=0.8852939009666443
I0128 22:49:04.193775 140026151130880 logging_writer.py:48] [158700] global_step=158700, grad_norm=8.181936264038086, loss=0.9615985751152039
I0128 22:49:38.089173 140026159523584 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.977840900421143, loss=1.0171725749969482
I0128 22:50:12.020019 140026151130880 logging_writer.py:48] [158900] global_step=158900, grad_norm=8.002047538757324, loss=1.0081312656402588
I0128 22:50:45.913506 140026159523584 logging_writer.py:48] [159000] global_step=159000, grad_norm=8.476195335388184, loss=0.961428701877594
I0128 22:51:19.776428 140026151130880 logging_writer.py:48] [159100] global_step=159100, grad_norm=9.39733600616455, loss=0.9695441722869873
I0128 22:51:53.715348 140026159523584 logging_writer.py:48] [159200] global_step=159200, grad_norm=8.117223739624023, loss=0.9301334619522095
I0128 22:52:27.579123 140026151130880 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.748831748962402, loss=0.9574490785598755
I0128 22:53:01.453193 140026159523584 logging_writer.py:48] [159400] global_step=159400, grad_norm=7.804779529571533, loss=0.925581693649292
I0128 22:53:08.417861 140187804313408 spec.py:321] Evaluating on the training split.
I0128 22:53:14.669447 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 22:53:23.648241 140187804313408 spec.py:349] Evaluating on the test split.
I0128 22:53:26.196731 140187804313408 submission_runner.py:408] Time since start: 56017.40s, 	Step: 159422, 	{'train/accuracy': 0.8851044178009033, 'train/loss': 0.404142826795578, 'validation/accuracy': 0.7461400032043457, 'validation/loss': 1.024307131767273, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.7184984683990479, 'test/num_examples': 10000, 'score': 54097.80532884598, 'total_duration': 56017.39664173126, 'accumulated_submission_time': 54097.80532884598, 'accumulated_eval_time': 1908.046015739441, 'accumulated_logging_time': 5.742774486541748}
I0128 22:53:26.244557 140026058876672 logging_writer.py:48] [159422] accumulated_eval_time=1908.046016, accumulated_logging_time=5.742774, accumulated_submission_time=54097.805329, global_step=159422, preemption_count=0, score=54097.805329, test/accuracy=0.623000, test/loss=1.718498, test/num_examples=10000, total_duration=56017.396642, train/accuracy=0.885104, train/loss=0.404143, validation/accuracy=0.746140, validation/loss=1.024307, validation/num_examples=50000
I0128 22:53:53.005373 140026067269376 logging_writer.py:48] [159500] global_step=159500, grad_norm=7.836546897888184, loss=1.019248604774475
I0128 22:54:26.848366 140026058876672 logging_writer.py:48] [159600] global_step=159600, grad_norm=7.958754539489746, loss=0.9137030839920044
I0128 22:55:00.740839 140026067269376 logging_writer.py:48] [159700] global_step=159700, grad_norm=8.535691261291504, loss=0.9456660747528076
I0128 22:55:34.620728 140026058876672 logging_writer.py:48] [159800] global_step=159800, grad_norm=7.869909286499023, loss=0.9750999808311462
I0128 22:56:08.531670 140026067269376 logging_writer.py:48] [159900] global_step=159900, grad_norm=7.9657745361328125, loss=0.9430407285690308
I0128 22:56:42.384705 140026058876672 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.635812282562256, loss=1.0210546255111694
I0128 22:57:16.298188 140026067269376 logging_writer.py:48] [160100] global_step=160100, grad_norm=8.654667854309082, loss=0.9412400722503662
I0128 22:57:50.192519 140026058876672 logging_writer.py:48] [160200] global_step=160200, grad_norm=7.445872783660889, loss=0.8941818475723267
I0128 22:58:24.143044 140026067269376 logging_writer.py:48] [160300] global_step=160300, grad_norm=9.019412994384766, loss=1.035355567932129
I0128 22:58:58.010154 140026058876672 logging_writer.py:48] [160400] global_step=160400, grad_norm=8.389171600341797, loss=0.9896559715270996
I0128 22:59:31.957571 140026067269376 logging_writer.py:48] [160500] global_step=160500, grad_norm=9.025357246398926, loss=0.9321019053459167
I0128 23:00:05.862990 140026058876672 logging_writer.py:48] [160600] global_step=160600, grad_norm=7.751241683959961, loss=0.875638484954834
I0128 23:00:39.778901 140026067269376 logging_writer.py:48] [160700] global_step=160700, grad_norm=8.750205039978027, loss=0.9130631685256958
I0128 23:01:13.651989 140026058876672 logging_writer.py:48] [160800] global_step=160800, grad_norm=8.184844017028809, loss=0.9070166945457458
I0128 23:01:47.574802 140026067269376 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.537041187286377, loss=0.8581355810165405
I0128 23:01:56.522111 140187804313408 spec.py:321] Evaluating on the training split.
I0128 23:02:02.897658 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 23:02:11.747263 140187804313408 spec.py:349] Evaluating on the test split.
I0128 23:02:14.341057 140187804313408 submission_runner.py:408] Time since start: 56545.54s, 	Step: 160928, 	{'train/accuracy': 0.8844267725944519, 'train/loss': 0.4088110625743866, 'validation/accuracy': 0.7474600076675415, 'validation/loss': 1.0270127058029175, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.7309526205062866, 'test/num_examples': 10000, 'score': 54608.0150744915, 'total_duration': 56545.54099678993, 'accumulated_submission_time': 54608.0150744915, 'accumulated_eval_time': 1925.8649232387543, 'accumulated_logging_time': 5.801524877548218}
I0128 23:02:14.386543 140026042091264 logging_writer.py:48] [160928] accumulated_eval_time=1925.864923, accumulated_logging_time=5.801525, accumulated_submission_time=54608.015074, global_step=160928, preemption_count=0, score=54608.015074, test/accuracy=0.618900, test/loss=1.730953, test/num_examples=10000, total_duration=56545.540997, train/accuracy=0.884427, train/loss=0.408811, validation/accuracy=0.747460, validation/loss=1.027013, validation/num_examples=50000
I0128 23:02:39.102535 140026050483968 logging_writer.py:48] [161000] global_step=161000, grad_norm=8.16440486907959, loss=0.8877800703048706
I0128 23:03:12.934459 140026042091264 logging_writer.py:48] [161100] global_step=161100, grad_norm=7.998162746429443, loss=0.8528657555580139
I0128 23:03:46.836418 140026050483968 logging_writer.py:48] [161200] global_step=161200, grad_norm=8.535563468933105, loss=0.9474952220916748
I0128 23:04:20.706524 140026042091264 logging_writer.py:48] [161300] global_step=161300, grad_norm=7.910818099975586, loss=0.9816619157791138
I0128 23:04:54.691708 140026050483968 logging_writer.py:48] [161400] global_step=161400, grad_norm=8.430323600769043, loss=0.9232850074768066
I0128 23:05:28.574012 140026042091264 logging_writer.py:48] [161500] global_step=161500, grad_norm=8.331459999084473, loss=0.8612372875213623
I0128 23:06:02.456429 140026050483968 logging_writer.py:48] [161600] global_step=161600, grad_norm=8.177144050598145, loss=0.9000219106674194
I0128 23:06:36.331591 140026042091264 logging_writer.py:48] [161700] global_step=161700, grad_norm=8.046162605285645, loss=0.9098435640335083
I0128 23:07:10.220646 140026050483968 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.396378517150879, loss=0.8702352046966553
I0128 23:07:44.088074 140026042091264 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.686387538909912, loss=0.8735531568527222
I0128 23:08:17.957839 140026050483968 logging_writer.py:48] [162000] global_step=162000, grad_norm=8.213669776916504, loss=0.8935968279838562
I0128 23:08:51.847749 140026042091264 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.876366138458252, loss=0.9394037127494812
I0128 23:09:25.744268 140026050483968 logging_writer.py:48] [162200] global_step=162200, grad_norm=8.649223327636719, loss=0.9796130657196045
I0128 23:09:59.606452 140026042091264 logging_writer.py:48] [162300] global_step=162300, grad_norm=8.574458122253418, loss=0.9202241897583008
I0128 23:10:33.524118 140026050483968 logging_writer.py:48] [162400] global_step=162400, grad_norm=8.497308731079102, loss=0.9252986311912537
I0128 23:10:44.493467 140187804313408 spec.py:321] Evaluating on the training split.
I0128 23:10:50.728054 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 23:10:59.765904 140187804313408 spec.py:349] Evaluating on the test split.
I0128 23:11:02.707866 140187804313408 submission_runner.py:408] Time since start: 57073.91s, 	Step: 162434, 	{'train/accuracy': 0.8910036683082581, 'train/loss': 0.38116320967674255, 'validation/accuracy': 0.7514399886131287, 'validation/loss': 1.0032466650009155, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.6860774755477905, 'test/num_examples': 10000, 'score': 55118.058817625046, 'total_duration': 57073.90779566765, 'accumulated_submission_time': 55118.058817625046, 'accumulated_eval_time': 1944.0792744159698, 'accumulated_logging_time': 5.855859756469727}
I0128 23:11:02.755674 140026067269376 logging_writer.py:48] [162434] accumulated_eval_time=1944.079274, accumulated_logging_time=5.855860, accumulated_submission_time=55118.058818, global_step=162434, preemption_count=0, score=55118.058818, test/accuracy=0.627000, test/loss=1.686077, test/num_examples=10000, total_duration=57073.907796, train/accuracy=0.891004, train/loss=0.381163, validation/accuracy=0.751440, validation/loss=1.003247, validation/num_examples=50000
I0128 23:11:25.428650 140026075662080 logging_writer.py:48] [162500] global_step=162500, grad_norm=8.32162857055664, loss=0.868478000164032
I0128 23:11:59.287005 140026067269376 logging_writer.py:48] [162600] global_step=162600, grad_norm=8.302847862243652, loss=0.8860517740249634
I0128 23:12:33.162374 140026075662080 logging_writer.py:48] [162700] global_step=162700, grad_norm=8.150880813598633, loss=0.8672938346862793
I0128 23:13:07.049489 140026067269376 logging_writer.py:48] [162800] global_step=162800, grad_norm=8.253337860107422, loss=0.8726906180381775
I0128 23:13:40.951809 140026075662080 logging_writer.py:48] [162900] global_step=162900, grad_norm=7.8786163330078125, loss=0.864686906337738
I0128 23:14:14.869709 140026067269376 logging_writer.py:48] [163000] global_step=163000, grad_norm=8.315932273864746, loss=0.8832341432571411
I0128 23:14:48.754036 140026075662080 logging_writer.py:48] [163100] global_step=163100, grad_norm=8.65383243560791, loss=0.9063228964805603
I0128 23:15:22.628680 140026067269376 logging_writer.py:48] [163200] global_step=163200, grad_norm=8.59363842010498, loss=0.8602863550186157
I0128 23:15:56.547244 140026075662080 logging_writer.py:48] [163300] global_step=163300, grad_norm=8.442214012145996, loss=0.9525159597396851
I0128 23:16:30.458799 140026067269376 logging_writer.py:48] [163400] global_step=163400, grad_norm=7.906186580657959, loss=0.8635694980621338
I0128 23:17:04.375493 140026075662080 logging_writer.py:48] [163500] global_step=163500, grad_norm=8.48743724822998, loss=0.8792515397071838
I0128 23:17:38.289771 140026067269376 logging_writer.py:48] [163600] global_step=163600, grad_norm=9.189187049865723, loss=0.8852978944778442
I0128 23:18:12.214825 140026075662080 logging_writer.py:48] [163700] global_step=163700, grad_norm=8.405619621276855, loss=0.8448557257652283
I0128 23:18:46.123741 140026067269376 logging_writer.py:48] [163800] global_step=163800, grad_norm=8.024896621704102, loss=0.8570492267608643
I0128 23:19:20.018684 140026075662080 logging_writer.py:48] [163900] global_step=163900, grad_norm=8.389453887939453, loss=0.8750091791152954
I0128 23:19:32.718457 140187804313408 spec.py:321] Evaluating on the training split.
I0128 23:19:38.915848 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 23:19:47.692245 140187804313408 spec.py:349] Evaluating on the test split.
I0128 23:19:50.302973 140187804313408 submission_runner.py:408] Time since start: 57601.50s, 	Step: 163939, 	{'train/accuracy': 0.90921950340271, 'train/loss': 0.3235359787940979, 'validation/accuracy': 0.7534599900245667, 'validation/loss': 0.9915898442268372, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.6824558973312378, 'test/num_examples': 10000, 'score': 55627.95647931099, 'total_duration': 57601.502833366394, 'accumulated_submission_time': 55627.95647931099, 'accumulated_eval_time': 1961.6636843681335, 'accumulated_logging_time': 5.913207292556763}
I0128 23:19:50.350636 140026058876672 logging_writer.py:48] [163939] accumulated_eval_time=1961.663684, accumulated_logging_time=5.913207, accumulated_submission_time=55627.956479, global_step=163939, preemption_count=0, score=55627.956479, test/accuracy=0.630300, test/loss=1.682456, test/num_examples=10000, total_duration=57601.502833, train/accuracy=0.909220, train/loss=0.323536, validation/accuracy=0.753460, validation/loss=0.991590, validation/num_examples=50000
I0128 23:20:11.340104 140026151130880 logging_writer.py:48] [164000] global_step=164000, grad_norm=8.73879337310791, loss=0.792414665222168
I0128 23:20:45.192882 140026058876672 logging_writer.py:48] [164100] global_step=164100, grad_norm=8.646109580993652, loss=0.8451657295227051
I0128 23:21:19.087508 140026151130880 logging_writer.py:48] [164200] global_step=164200, grad_norm=7.941037654876709, loss=0.8639795184135437
I0128 23:21:52.961536 140026058876672 logging_writer.py:48] [164300] global_step=164300, grad_norm=8.24994945526123, loss=0.9085127115249634
I0128 23:22:26.819778 140026151130880 logging_writer.py:48] [164400] global_step=164400, grad_norm=8.4100923538208, loss=0.8737923502922058
I0128 23:23:00.740537 140026058876672 logging_writer.py:48] [164500] global_step=164500, grad_norm=8.386768341064453, loss=0.8874089121818542
I0128 23:23:34.623408 140026151130880 logging_writer.py:48] [164600] global_step=164600, grad_norm=8.46738052368164, loss=0.8670895099639893
I0128 23:24:08.587110 140026058876672 logging_writer.py:48] [164700] global_step=164700, grad_norm=8.185041427612305, loss=0.8067360520362854
I0128 23:24:42.471106 140026151130880 logging_writer.py:48] [164800] global_step=164800, grad_norm=8.140057563781738, loss=0.8471404314041138
I0128 23:25:16.375797 140026058876672 logging_writer.py:48] [164900] global_step=164900, grad_norm=8.784488677978516, loss=0.9680544137954712
I0128 23:25:50.260269 140026151130880 logging_writer.py:48] [165000] global_step=165000, grad_norm=8.639650344848633, loss=0.8318654298782349
I0128 23:26:24.168404 140026058876672 logging_writer.py:48] [165100] global_step=165100, grad_norm=8.323371887207031, loss=0.8849477767944336
I0128 23:26:58.048542 140026151130880 logging_writer.py:48] [165200] global_step=165200, grad_norm=7.438546657562256, loss=0.7411825060844421
I0128 23:27:31.922485 140026058876672 logging_writer.py:48] [165300] global_step=165300, grad_norm=8.95454216003418, loss=0.9129588007926941
I0128 23:28:05.834269 140026151130880 logging_writer.py:48] [165400] global_step=165400, grad_norm=7.9896931648254395, loss=0.7843940258026123
I0128 23:28:20.565830 140187804313408 spec.py:321] Evaluating on the training split.
I0128 23:28:26.788536 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 23:28:35.829990 140187804313408 spec.py:349] Evaluating on the test split.
I0128 23:28:38.427940 140187804313408 submission_runner.py:408] Time since start: 58129.63s, 	Step: 165445, 	{'train/accuracy': 0.9094586968421936, 'train/loss': 0.3204634189605713, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 0.9951203465461731, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.6766432523727417, 'test/num_examples': 10000, 'score': 56138.10664725304, 'total_duration': 58129.62777304649, 'accumulated_submission_time': 56138.10664725304, 'accumulated_eval_time': 1979.5256507396698, 'accumulated_logging_time': 5.970006227493286}
I0128 23:28:38.478029 140026075662080 logging_writer.py:48] [165445] accumulated_eval_time=1979.525651, accumulated_logging_time=5.970006, accumulated_submission_time=56138.106647, global_step=165445, preemption_count=0, score=56138.106647, test/accuracy=0.629900, test/loss=1.676643, test/num_examples=10000, total_duration=58129.627773, train/accuracy=0.909459, train/loss=0.320463, validation/accuracy=0.754900, validation/loss=0.995120, validation/num_examples=50000
I0128 23:28:57.455480 140026159523584 logging_writer.py:48] [165500] global_step=165500, grad_norm=9.48572826385498, loss=0.854125440120697
I0128 23:29:31.262032 140026075662080 logging_writer.py:48] [165600] global_step=165600, grad_norm=8.048580169677734, loss=0.8808103203773499
I0128 23:30:05.161756 140026159523584 logging_writer.py:48] [165700] global_step=165700, grad_norm=7.580617904663086, loss=0.860313355922699
I0128 23:30:39.133492 140026075662080 logging_writer.py:48] [165800] global_step=165800, grad_norm=8.931334495544434, loss=0.8497224450111389
I0128 23:31:13.000794 140026159523584 logging_writer.py:48] [165900] global_step=165900, grad_norm=8.669374465942383, loss=0.8785648941993713
I0128 23:31:46.912819 140026075662080 logging_writer.py:48] [166000] global_step=166000, grad_norm=9.487905502319336, loss=0.8909161686897278
I0128 23:32:20.852173 140026159523584 logging_writer.py:48] [166100] global_step=166100, grad_norm=8.653396606445312, loss=0.787367582321167
I0128 23:32:54.780352 140026075662080 logging_writer.py:48] [166200] global_step=166200, grad_norm=8.528169631958008, loss=0.8211326599121094
I0128 23:33:28.656420 140026159523584 logging_writer.py:48] [166300] global_step=166300, grad_norm=9.069026947021484, loss=0.9477558732032776
I0128 23:34:02.571969 140026075662080 logging_writer.py:48] [166400] global_step=166400, grad_norm=9.141911506652832, loss=0.8852154612541199
I0128 23:34:36.456158 140026159523584 logging_writer.py:48] [166500] global_step=166500, grad_norm=8.453964233398438, loss=0.8426400423049927
I0128 23:35:10.352974 140026075662080 logging_writer.py:48] [166600] global_step=166600, grad_norm=8.490396499633789, loss=0.9249303340911865
I0128 23:35:44.252769 140026159523584 logging_writer.py:48] [166700] global_step=166700, grad_norm=9.065572738647461, loss=0.8543957471847534
I0128 23:36:18.118832 140026075662080 logging_writer.py:48] [166800] global_step=166800, grad_norm=9.133621215820312, loss=0.8880809545516968
I0128 23:36:52.078994 140026159523584 logging_writer.py:48] [166900] global_step=166900, grad_norm=9.069816589355469, loss=0.9040082097053528
I0128 23:37:08.502768 140187804313408 spec.py:321] Evaluating on the training split.
I0128 23:37:14.687769 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 23:37:23.590320 140187804313408 spec.py:349] Evaluating on the test split.
I0128 23:37:26.240615 140187804313408 submission_runner.py:408] Time since start: 58657.44s, 	Step: 166950, 	{'train/accuracy': 0.9108538031578064, 'train/loss': 0.31472885608673096, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 0.9941707849502563, 'validation/num_examples': 50000, 'test/accuracy': 0.6342000365257263, 'test/loss': 1.6934216022491455, 'test/num_examples': 10000, 'score': 56648.064351558685, 'total_duration': 58657.44054579735, 'accumulated_submission_time': 56648.064351558685, 'accumulated_eval_time': 1997.2634472846985, 'accumulated_logging_time': 6.031470537185669}
I0128 23:37:26.292536 140026058876672 logging_writer.py:48] [166950] accumulated_eval_time=1997.263447, accumulated_logging_time=6.031471, accumulated_submission_time=56648.064352, global_step=166950, preemption_count=0, score=56648.064352, test/accuracy=0.634200, test/loss=1.693422, test/num_examples=10000, total_duration=58657.440546, train/accuracy=0.910854, train/loss=0.314729, validation/accuracy=0.756260, validation/loss=0.994171, validation/num_examples=50000
I0128 23:37:43.575084 140026067269376 logging_writer.py:48] [167000] global_step=167000, grad_norm=8.897828102111816, loss=0.877773642539978
I0128 23:38:17.382495 140026058876672 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.718945503234863, loss=0.7958157062530518
I0128 23:38:51.225284 140026067269376 logging_writer.py:48] [167200] global_step=167200, grad_norm=8.884515762329102, loss=0.7846342325210571
I0128 23:39:25.153123 140026058876672 logging_writer.py:48] [167300] global_step=167300, grad_norm=8.391942977905273, loss=0.8075429797172546
I0128 23:39:59.039288 140026067269376 logging_writer.py:48] [167400] global_step=167400, grad_norm=8.904191017150879, loss=0.8510180115699768
I0128 23:40:32.932954 140026058876672 logging_writer.py:48] [167500] global_step=167500, grad_norm=8.535309791564941, loss=0.8854997754096985
I0128 23:41:06.842486 140026067269376 logging_writer.py:48] [167600] global_step=167600, grad_norm=8.900492668151855, loss=0.7741174697875977
I0128 23:41:40.735053 140026058876672 logging_writer.py:48] [167700] global_step=167700, grad_norm=9.360258102416992, loss=0.9109613299369812
I0128 23:42:14.567710 140026067269376 logging_writer.py:48] [167800] global_step=167800, grad_norm=9.703303337097168, loss=0.8215848803520203
I0128 23:42:48.456109 140026058876672 logging_writer.py:48] [167900] global_step=167900, grad_norm=9.345283508300781, loss=0.9000124931335449
I0128 23:43:22.362238 140026067269376 logging_writer.py:48] [168000] global_step=168000, grad_norm=8.601394653320312, loss=0.7797497510910034
I0128 23:43:56.280665 140026058876672 logging_writer.py:48] [168100] global_step=168100, grad_norm=9.326933860778809, loss=0.9333146810531616
I0128 23:44:30.172841 140026067269376 logging_writer.py:48] [168200] global_step=168200, grad_norm=9.887578964233398, loss=0.8418359756469727
I0128 23:45:04.085099 140026058876672 logging_writer.py:48] [168300] global_step=168300, grad_norm=8.761719703674316, loss=0.8690851330757141
I0128 23:45:37.971816 140026067269376 logging_writer.py:48] [168400] global_step=168400, grad_norm=8.91987133026123, loss=0.8763445019721985
I0128 23:45:56.414771 140187804313408 spec.py:321] Evaluating on the training split.
I0128 23:46:02.768601 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 23:46:11.529856 140187804313408 spec.py:349] Evaluating on the test split.
I0128 23:46:14.113028 140187804313408 submission_runner.py:408] Time since start: 59185.31s, 	Step: 168456, 	{'train/accuracy': 0.9100167155265808, 'train/loss': 0.3191568851470947, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 0.9903133511543274, 'validation/num_examples': 50000, 'test/accuracy': 0.6330000162124634, 'test/loss': 1.6975325345993042, 'test/num_examples': 10000, 'score': 57158.12152385712, 'total_duration': 59185.312942266464, 'accumulated_submission_time': 57158.12152385712, 'accumulated_eval_time': 2014.9616417884827, 'accumulated_logging_time': 6.093728303909302}
I0128 23:46:14.161364 140026058876672 logging_writer.py:48] [168456] accumulated_eval_time=2014.961642, accumulated_logging_time=6.093728, accumulated_submission_time=57158.121524, global_step=168456, preemption_count=0, score=57158.121524, test/accuracy=0.633000, test/loss=1.697533, test/num_examples=10000, total_duration=59185.312942, train/accuracy=0.910017, train/loss=0.319157, validation/accuracy=0.755400, validation/loss=0.990313, validation/num_examples=50000
I0128 23:46:29.399725 140026159523584 logging_writer.py:48] [168500] global_step=168500, grad_norm=8.038979530334473, loss=0.8190807700157166
I0128 23:47:03.249465 140026058876672 logging_writer.py:48] [168600] global_step=168600, grad_norm=8.912004470825195, loss=0.8130069971084595
I0128 23:47:37.091727 140026159523584 logging_writer.py:48] [168700] global_step=168700, grad_norm=8.68865966796875, loss=0.8917919993400574
I0128 23:48:10.960979 140026058876672 logging_writer.py:48] [168800] global_step=168800, grad_norm=9.105606079101562, loss=0.8413817882537842
I0128 23:48:44.866555 140026159523584 logging_writer.py:48] [168900] global_step=168900, grad_norm=8.901914596557617, loss=0.8325179219245911
I0128 23:49:18.853567 140026058876672 logging_writer.py:48] [169000] global_step=169000, grad_norm=8.605403900146484, loss=0.8467146754264832
I0128 23:49:52.755925 140026159523584 logging_writer.py:48] [169100] global_step=169100, grad_norm=8.620701789855957, loss=0.8402456045150757
I0128 23:50:26.674705 140026058876672 logging_writer.py:48] [169200] global_step=169200, grad_norm=7.959996700286865, loss=0.7574462890625
I0128 23:51:00.564763 140026159523584 logging_writer.py:48] [169300] global_step=169300, grad_norm=8.414045333862305, loss=0.8343064785003662
I0128 23:51:34.481326 140026058876672 logging_writer.py:48] [169400] global_step=169400, grad_norm=9.061100006103516, loss=0.8538633584976196
I0128 23:52:08.374079 140026159523584 logging_writer.py:48] [169500] global_step=169500, grad_norm=8.538440704345703, loss=0.8686539530754089
I0128 23:52:42.293766 140026058876672 logging_writer.py:48] [169600] global_step=169600, grad_norm=8.507243156433105, loss=0.7672956585884094
I0128 23:53:16.203758 140026159523584 logging_writer.py:48] [169700] global_step=169700, grad_norm=8.918631553649902, loss=0.8630872964859009
I0128 23:53:50.098167 140026058876672 logging_writer.py:48] [169800] global_step=169800, grad_norm=8.524410247802734, loss=0.8211031556129456
I0128 23:54:24.007361 140026159523584 logging_writer.py:48] [169900] global_step=169900, grad_norm=8.679917335510254, loss=0.8156318068504333
I0128 23:54:44.159969 140187804313408 spec.py:321] Evaluating on the training split.
I0128 23:54:50.429305 140187804313408 spec.py:333] Evaluating on the validation split.
I0128 23:54:59.226783 140187804313408 spec.py:349] Evaluating on the test split.
I0128 23:55:01.859232 140187804313408 submission_runner.py:408] Time since start: 59713.06s, 	Step: 169961, 	{'train/accuracy': 0.9138432741165161, 'train/loss': 0.3032202124595642, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 0.9809739589691162, 'validation/num_examples': 50000, 'test/accuracy': 0.6330000162124634, 'test/loss': 1.6725350618362427, 'test/num_examples': 10000, 'score': 57668.05534219742, 'total_duration': 59713.05906128883, 'accumulated_submission_time': 57668.05534219742, 'accumulated_eval_time': 2032.6607563495636, 'accumulated_logging_time': 6.152337074279785}
I0128 23:55:01.906067 140026050483968 logging_writer.py:48] [169961] accumulated_eval_time=2032.660756, accumulated_logging_time=6.152337, accumulated_submission_time=57668.055342, global_step=169961, preemption_count=0, score=57668.055342, test/accuracy=0.633000, test/loss=1.672535, test/num_examples=10000, total_duration=59713.059061, train/accuracy=0.913843, train/loss=0.303220, validation/accuracy=0.757360, validation/loss=0.980974, validation/num_examples=50000
I0128 23:55:15.451359 140026067269376 logging_writer.py:48] [170000] global_step=170000, grad_norm=8.119277954101562, loss=0.8023194074630737
I0128 23:55:49.338406 140026050483968 logging_writer.py:48] [170100] global_step=170100, grad_norm=7.744203090667725, loss=0.7714680433273315
I0128 23:56:23.194789 140026067269376 logging_writer.py:48] [170200] global_step=170200, grad_norm=8.387452125549316, loss=0.7309699654579163
I0128 23:56:57.100848 140026050483968 logging_writer.py:48] [170300] global_step=170300, grad_norm=8.482311248779297, loss=0.8019957542419434
I0128 23:57:30.983328 140026067269376 logging_writer.py:48] [170400] global_step=170400, grad_norm=8.206523895263672, loss=0.7567813396453857
I0128 23:58:04.885273 140026050483968 logging_writer.py:48] [170500] global_step=170500, grad_norm=8.470090866088867, loss=0.8692888021469116
I0128 23:58:38.759008 140026067269376 logging_writer.py:48] [170600] global_step=170600, grad_norm=10.269437789916992, loss=0.746899425983429
I0128 23:59:12.661287 140026050483968 logging_writer.py:48] [170700] global_step=170700, grad_norm=9.025297164916992, loss=0.8206709623336792
I0128 23:59:46.555629 140026067269376 logging_writer.py:48] [170800] global_step=170800, grad_norm=9.669612884521484, loss=0.7369322776794434
I0129 00:00:20.450558 140026050483968 logging_writer.py:48] [170900] global_step=170900, grad_norm=8.221778869628906, loss=0.7615715861320496
I0129 00:00:54.335178 140026067269376 logging_writer.py:48] [171000] global_step=171000, grad_norm=9.463760375976562, loss=0.7932178378105164
I0129 00:01:28.174913 140026050483968 logging_writer.py:48] [171100] global_step=171100, grad_norm=8.538305282592773, loss=0.773307204246521
I0129 00:02:02.139110 140026067269376 logging_writer.py:48] [171200] global_step=171200, grad_norm=9.801411628723145, loss=0.7484501600265503
I0129 00:02:35.989433 140026050483968 logging_writer.py:48] [171300] global_step=171300, grad_norm=9.807942390441895, loss=0.8454753756523132
I0129 00:03:09.865378 140026067269376 logging_writer.py:48] [171400] global_step=171400, grad_norm=8.709369659423828, loss=0.8120924234390259
I0129 00:03:32.076513 140187804313408 spec.py:321] Evaluating on the training split.
I0129 00:03:38.293761 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 00:03:47.244108 140187804313408 spec.py:349] Evaluating on the test split.
I0129 00:03:49.892183 140187804313408 submission_runner.py:408] Time since start: 60241.09s, 	Step: 171467, 	{'train/accuracy': 0.9151387214660645, 'train/loss': 0.29794761538505554, 'validation/accuracy': 0.7592200040817261, 'validation/loss': 0.9786510467529297, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.6685278415679932, 'test/num_examples': 10000, 'score': 58178.16114640236, 'total_duration': 60241.09210109711, 'accumulated_submission_time': 58178.16114640236, 'accumulated_eval_time': 2050.4763667583466, 'accumulated_logging_time': 6.209350109100342}
I0129 00:03:49.942136 140026167916288 logging_writer.py:48] [171467] accumulated_eval_time=2050.476367, accumulated_logging_time=6.209350, accumulated_submission_time=58178.161146, global_step=171467, preemption_count=0, score=58178.161146, test/accuracy=0.636700, test/loss=1.668528, test/num_examples=10000, total_duration=60241.092101, train/accuracy=0.915139, train/loss=0.297948, validation/accuracy=0.759220, validation/loss=0.978651, validation/num_examples=50000
I0129 00:04:01.476597 140026176308992 logging_writer.py:48] [171500] global_step=171500, grad_norm=8.971467971801758, loss=0.7682955861091614
I0129 00:04:35.328626 140026167916288 logging_writer.py:48] [171600] global_step=171600, grad_norm=8.026164054870605, loss=0.7586969137191772
I0129 00:05:09.215591 140026176308992 logging_writer.py:48] [171700] global_step=171700, grad_norm=9.169055938720703, loss=0.7652764916419983
I0129 00:05:43.126392 140026167916288 logging_writer.py:48] [171800] global_step=171800, grad_norm=8.715973854064941, loss=0.6677641868591309
I0129 00:06:17.018813 140026176308992 logging_writer.py:48] [171900] global_step=171900, grad_norm=9.758628845214844, loss=0.8098291754722595
I0129 00:06:50.962298 140026167916288 logging_writer.py:48] [172000] global_step=172000, grad_norm=9.271376609802246, loss=0.9011954665184021
I0129 00:07:24.856550 140026176308992 logging_writer.py:48] [172100] global_step=172100, grad_norm=8.706138610839844, loss=0.777098536491394
I0129 00:07:58.770745 140026167916288 logging_writer.py:48] [172200] global_step=172200, grad_norm=8.820711135864258, loss=0.8077713251113892
I0129 00:08:32.707075 140026176308992 logging_writer.py:48] [172300] global_step=172300, grad_norm=8.781903266906738, loss=0.7679785490036011
I0129 00:09:06.595833 140026167916288 logging_writer.py:48] [172400] global_step=172400, grad_norm=8.332613945007324, loss=0.7738071084022522
I0129 00:09:40.523077 140026176308992 logging_writer.py:48] [172500] global_step=172500, grad_norm=7.895977020263672, loss=0.75673508644104
I0129 00:10:14.391975 140026167916288 logging_writer.py:48] [172600] global_step=172600, grad_norm=8.755182266235352, loss=0.7427487969398499
I0129 00:10:48.309903 140026176308992 logging_writer.py:48] [172700] global_step=172700, grad_norm=9.634291648864746, loss=0.7702913880348206
I0129 00:11:22.195398 140026167916288 logging_writer.py:48] [172800] global_step=172800, grad_norm=9.340139389038086, loss=0.859229326248169
I0129 00:11:56.080883 140026176308992 logging_writer.py:48] [172900] global_step=172900, grad_norm=9.96168041229248, loss=0.8678268790245056
I0129 00:12:19.954787 140187804313408 spec.py:321] Evaluating on the training split.
I0129 00:12:26.270494 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 00:12:34.985637 140187804313408 spec.py:349] Evaluating on the test split.
I0129 00:12:37.680169 140187804313408 submission_runner.py:408] Time since start: 60768.88s, 	Step: 172972, 	{'train/accuracy': 0.92386794090271, 'train/loss': 0.27077406644821167, 'validation/accuracy': 0.759619951248169, 'validation/loss': 0.9723941683769226, 'validation/num_examples': 50000, 'test/accuracy': 0.6366000175476074, 'test/loss': 1.6629676818847656, 'test/num_examples': 10000, 'score': 58688.1078979969, 'total_duration': 60768.88009810448, 'accumulated_submission_time': 58688.1078979969, 'accumulated_eval_time': 2068.201717376709, 'accumulated_logging_time': 6.271347761154175}
I0129 00:12:37.732107 140026058876672 logging_writer.py:48] [172972] accumulated_eval_time=2068.201717, accumulated_logging_time=6.271348, accumulated_submission_time=58688.107898, global_step=172972, preemption_count=0, score=58688.107898, test/accuracy=0.636600, test/loss=1.662968, test/num_examples=10000, total_duration=60768.880098, train/accuracy=0.923868, train/loss=0.270774, validation/accuracy=0.759620, validation/loss=0.972394, validation/num_examples=50000
I0129 00:12:47.562236 140026067269376 logging_writer.py:48] [173000] global_step=173000, grad_norm=8.594854354858398, loss=0.759611964225769
I0129 00:13:21.396066 140026058876672 logging_writer.py:48] [173100] global_step=173100, grad_norm=9.58592414855957, loss=0.8060177564620972
I0129 00:13:55.286151 140026067269376 logging_writer.py:48] [173200] global_step=173200, grad_norm=9.575039863586426, loss=0.7807369232177734
I0129 00:14:29.155381 140026058876672 logging_writer.py:48] [173300] global_step=173300, grad_norm=9.233518600463867, loss=0.8151207566261292
I0129 00:15:03.119120 140026067269376 logging_writer.py:48] [173400] global_step=173400, grad_norm=10.137301445007324, loss=0.7828534245491028
I0129 00:15:37.063190 140026058876672 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.907896995544434, loss=0.8038730025291443
I0129 00:16:10.934955 140026067269376 logging_writer.py:48] [173600] global_step=173600, grad_norm=10.294261932373047, loss=0.795474648475647
I0129 00:16:44.800364 140026058876672 logging_writer.py:48] [173700] global_step=173700, grad_norm=9.659908294677734, loss=0.8204020261764526
I0129 00:17:18.701264 140026067269376 logging_writer.py:48] [173800] global_step=173800, grad_norm=10.923583030700684, loss=0.7572963833808899
I0129 00:17:52.555673 140026058876672 logging_writer.py:48] [173900] global_step=173900, grad_norm=9.698725700378418, loss=0.8545551300048828
I0129 00:18:26.401817 140026067269376 logging_writer.py:48] [174000] global_step=174000, grad_norm=10.025696754455566, loss=0.7387781739234924
I0129 00:19:00.291207 140026058876672 logging_writer.py:48] [174100] global_step=174100, grad_norm=8.947298049926758, loss=0.7958952784538269
I0129 00:19:34.191251 140026067269376 logging_writer.py:48] [174200] global_step=174200, grad_norm=8.982893943786621, loss=0.8561482429504395
I0129 00:20:08.061033 140026058876672 logging_writer.py:48] [174300] global_step=174300, grad_norm=8.777734756469727, loss=0.8208799362182617
I0129 00:20:41.982493 140026067269376 logging_writer.py:48] [174400] global_step=174400, grad_norm=8.96030330657959, loss=0.6981106400489807
I0129 00:21:07.917048 140187804313408 spec.py:321] Evaluating on the training split.
I0129 00:21:14.131366 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 00:21:22.938521 140187804313408 spec.py:349] Evaluating on the test split.
I0129 00:21:25.525296 140187804313408 submission_runner.py:408] Time since start: 61296.73s, 	Step: 174478, 	{'train/accuracy': 0.9291493892669678, 'train/loss': 0.2514609694480896, 'validation/accuracy': 0.7603799700737, 'validation/loss': 0.9728773236274719, 'validation/num_examples': 50000, 'test/accuracy': 0.6362000107765198, 'test/loss': 1.6596471071243286, 'test/num_examples': 10000, 'score': 59198.22804784775, 'total_duration': 61296.72523832321, 'accumulated_submission_time': 59198.22804784775, 'accumulated_eval_time': 2085.809932947159, 'accumulated_logging_time': 6.333725452423096}
I0129 00:21:25.573156 140026050483968 logging_writer.py:48] [174478] accumulated_eval_time=2085.809933, accumulated_logging_time=6.333725, accumulated_submission_time=59198.228048, global_step=174478, preemption_count=0, score=59198.228048, test/accuracy=0.636200, test/loss=1.659647, test/num_examples=10000, total_duration=61296.725238, train/accuracy=0.929149, train/loss=0.251461, validation/accuracy=0.760380, validation/loss=0.972877, validation/num_examples=50000
I0129 00:21:33.384042 140026058876672 logging_writer.py:48] [174500] global_step=174500, grad_norm=8.424346923828125, loss=0.7046167254447937
I0129 00:22:07.242156 140026050483968 logging_writer.py:48] [174600] global_step=174600, grad_norm=8.570311546325684, loss=0.724321186542511
I0129 00:22:41.096298 140026058876672 logging_writer.py:48] [174700] global_step=174700, grad_norm=7.652474880218506, loss=0.6468329429626465
I0129 00:23:15.028416 140026050483968 logging_writer.py:48] [174800] global_step=174800, grad_norm=8.705971717834473, loss=0.7017972469329834
I0129 00:23:48.923996 140026058876672 logging_writer.py:48] [174900] global_step=174900, grad_norm=9.30063533782959, loss=0.7776124477386475
I0129 00:24:22.836649 140026050483968 logging_writer.py:48] [175000] global_step=175000, grad_norm=8.62000560760498, loss=0.7874844074249268
I0129 00:24:56.738831 140026058876672 logging_writer.py:48] [175100] global_step=175100, grad_norm=9.523096084594727, loss=0.8401469588279724
I0129 00:25:30.637772 140026050483968 logging_writer.py:48] [175200] global_step=175200, grad_norm=9.149161338806152, loss=0.7219761610031128
I0129 00:26:04.559276 140026058876672 logging_writer.py:48] [175300] global_step=175300, grad_norm=8.867311477661133, loss=0.6979324817657471
I0129 00:26:38.469861 140026050483968 logging_writer.py:48] [175400] global_step=175400, grad_norm=9.11281967163086, loss=0.7705695033073425
I0129 00:27:12.396038 140026058876672 logging_writer.py:48] [175500] global_step=175500, grad_norm=8.675820350646973, loss=0.7225603461265564
I0129 00:27:46.336172 140026050483968 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.377939224243164, loss=0.7983055114746094
I0129 00:28:20.235724 140026058876672 logging_writer.py:48] [175700] global_step=175700, grad_norm=8.856939315795898, loss=0.7636600732803345
I0129 00:28:54.157539 140026050483968 logging_writer.py:48] [175800] global_step=175800, grad_norm=8.080408096313477, loss=0.7352811098098755
I0129 00:29:28.041968 140026058876672 logging_writer.py:48] [175900] global_step=175900, grad_norm=10.54234504699707, loss=0.7309541702270508
I0129 00:29:55.671682 140187804313408 spec.py:321] Evaluating on the training split.
I0129 00:30:01.876957 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 00:30:10.861191 140187804313408 spec.py:349] Evaluating on the test split.
I0129 00:30:13.435695 140187804313408 submission_runner.py:408] Time since start: 61824.64s, 	Step: 175983, 	{'train/accuracy': 0.9298469424247742, 'train/loss': 0.2528488039970398, 'validation/accuracy': 0.7617799639701843, 'validation/loss': 0.9658573865890503, 'validation/num_examples': 50000, 'test/accuracy': 0.636900007724762, 'test/loss': 1.6625109910964966, 'test/num_examples': 10000, 'score': 59708.26288509369, 'total_duration': 61824.635633945465, 'accumulated_submission_time': 59708.26288509369, 'accumulated_eval_time': 2103.5739080905914, 'accumulated_logging_time': 6.391595363616943}
I0129 00:30:13.485100 140026050483968 logging_writer.py:48] [175983] accumulated_eval_time=2103.573908, accumulated_logging_time=6.391595, accumulated_submission_time=59708.262885, global_step=175983, preemption_count=0, score=59708.262885, test/accuracy=0.636900, test/loss=1.662511, test/num_examples=10000, total_duration=61824.635634, train/accuracy=0.929847, train/loss=0.252849, validation/accuracy=0.761780, validation/loss=0.965857, validation/num_examples=50000
I0129 00:30:19.597977 140026058876672 logging_writer.py:48] [176000] global_step=176000, grad_norm=8.583758354187012, loss=0.7091793417930603
I0129 00:30:53.438510 140026050483968 logging_writer.py:48] [176100] global_step=176100, grad_norm=9.87814998626709, loss=0.7165991067886353
I0129 00:31:27.300677 140026058876672 logging_writer.py:48] [176200] global_step=176200, grad_norm=8.486223220825195, loss=0.7688682675361633
I0129 00:32:01.231318 140026050483968 logging_writer.py:48] [176300] global_step=176300, grad_norm=9.144441604614258, loss=0.7468669414520264
I0129 00:32:35.126131 140026058876672 logging_writer.py:48] [176400] global_step=176400, grad_norm=8.793000221252441, loss=0.672112226486206
I0129 00:33:09.014672 140026050483968 logging_writer.py:48] [176500] global_step=176500, grad_norm=9.671788215637207, loss=0.7168070077896118
I0129 00:33:42.942576 140026058876672 logging_writer.py:48] [176600] global_step=176600, grad_norm=9.065034866333008, loss=0.7507216930389404
I0129 00:34:16.916535 140026050483968 logging_writer.py:48] [176700] global_step=176700, grad_norm=10.010794639587402, loss=0.7056915760040283
I0129 00:34:50.802506 140026058876672 logging_writer.py:48] [176800] global_step=176800, grad_norm=8.730183601379395, loss=0.8247609734535217
I0129 00:35:24.714456 140026050483968 logging_writer.py:48] [176900] global_step=176900, grad_norm=8.517240524291992, loss=0.6762125492095947
I0129 00:35:58.601098 140026058876672 logging_writer.py:48] [177000] global_step=177000, grad_norm=8.603063583374023, loss=0.7561097741127014
I0129 00:36:32.504289 140026050483968 logging_writer.py:48] [177100] global_step=177100, grad_norm=9.126188278198242, loss=0.777595043182373
I0129 00:37:06.400540 140026058876672 logging_writer.py:48] [177200] global_step=177200, grad_norm=9.490808486938477, loss=0.8608351945877075
I0129 00:37:40.291953 140026050483968 logging_writer.py:48] [177300] global_step=177300, grad_norm=8.946759223937988, loss=0.7201988697052002
I0129 00:38:14.227565 140026058876672 logging_writer.py:48] [177400] global_step=177400, grad_norm=9.108384132385254, loss=0.7462239265441895
I0129 00:38:43.507954 140187804313408 spec.py:321] Evaluating on the training split.
I0129 00:38:49.791607 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 00:38:58.467301 140187804313408 spec.py:349] Evaluating on the test split.
I0129 00:39:01.234291 140187804313408 submission_runner.py:408] Time since start: 62352.43s, 	Step: 177488, 	{'train/accuracy': 0.928730845451355, 'train/loss': 0.25656646490097046, 'validation/accuracy': 0.7629799842834473, 'validation/loss': 0.9615415930747986, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.6531953811645508, 'test/num_examples': 10000, 'score': 60218.22137951851, 'total_duration': 62352.43422079086, 'accumulated_submission_time': 60218.22137951851, 'accumulated_eval_time': 2121.300199508667, 'accumulated_logging_time': 6.451011419296265}
I0129 00:39:01.290689 140026042091264 logging_writer.py:48] [177488] accumulated_eval_time=2121.300200, accumulated_logging_time=6.451011, accumulated_submission_time=60218.221380, global_step=177488, preemption_count=0, score=60218.221380, test/accuracy=0.638300, test/loss=1.653195, test/num_examples=10000, total_duration=62352.434221, train/accuracy=0.928731, train/loss=0.256566, validation/accuracy=0.762980, validation/loss=0.961542, validation/num_examples=50000
I0129 00:39:05.710292 140026050483968 logging_writer.py:48] [177500] global_step=177500, grad_norm=9.57067584991455, loss=0.7543113231658936
I0129 00:39:39.586103 140026042091264 logging_writer.py:48] [177600] global_step=177600, grad_norm=9.711719512939453, loss=0.7396354079246521
I0129 00:40:13.562337 140026050483968 logging_writer.py:48] [177700] global_step=177700, grad_norm=8.261297225952148, loss=0.6816258430480957
I0129 00:40:47.485517 140026042091264 logging_writer.py:48] [177800] global_step=177800, grad_norm=9.281864166259766, loss=0.706351637840271
I0129 00:41:21.399969 140026050483968 logging_writer.py:48] [177900] global_step=177900, grad_norm=9.829367637634277, loss=0.7403432130813599
I0129 00:41:55.311355 140026042091264 logging_writer.py:48] [178000] global_step=178000, grad_norm=9.851308822631836, loss=0.7806230783462524
I0129 00:42:29.225857 140026050483968 logging_writer.py:48] [178100] global_step=178100, grad_norm=9.055078506469727, loss=0.725488543510437
I0129 00:43:03.122238 140026042091264 logging_writer.py:48] [178200] global_step=178200, grad_norm=9.313773155212402, loss=0.7971868515014648
I0129 00:43:37.037772 140026050483968 logging_writer.py:48] [178300] global_step=178300, grad_norm=9.697166442871094, loss=0.833083987236023
I0129 00:44:10.945000 140026042091264 logging_writer.py:48] [178400] global_step=178400, grad_norm=8.535954475402832, loss=0.771852970123291
I0129 00:44:44.877014 140026050483968 logging_writer.py:48] [178500] global_step=178500, grad_norm=9.12364387512207, loss=0.7548755407333374
I0129 00:45:18.763373 140026042091264 logging_writer.py:48] [178600] global_step=178600, grad_norm=10.13378620147705, loss=0.8121524453163147
I0129 00:45:52.683740 140026050483968 logging_writer.py:48] [178700] global_step=178700, grad_norm=9.467582702636719, loss=0.6946388483047485
I0129 00:46:26.574532 140026042091264 logging_writer.py:48] [178800] global_step=178800, grad_norm=9.953080177307129, loss=0.751242458820343
I0129 00:47:00.538407 140026050483968 logging_writer.py:48] [178900] global_step=178900, grad_norm=9.12773609161377, loss=0.8229788541793823
I0129 00:47:31.517246 140187804313408 spec.py:321] Evaluating on the training split.
I0129 00:47:37.739987 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 00:47:46.769630 140187804313408 spec.py:349] Evaluating on the test split.
I0129 00:47:49.377803 140187804313408 submission_runner.py:408] Time since start: 62880.58s, 	Step: 178993, 	{'train/accuracy': 0.9295678734779358, 'train/loss': 0.25035443902015686, 'validation/accuracy': 0.7623199820518494, 'validation/loss': 0.9605156183242798, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.6532292366027832, 'test/num_examples': 10000, 'score': 60728.37961912155, 'total_duration': 62880.57772922516, 'accumulated_submission_time': 60728.37961912155, 'accumulated_eval_time': 2139.1607053279877, 'accumulated_logging_time': 6.521094560623169}
I0129 00:47:49.427861 140026058876672 logging_writer.py:48] [178993] accumulated_eval_time=2139.160705, accumulated_logging_time=6.521095, accumulated_submission_time=60728.379619, global_step=178993, preemption_count=0, score=60728.379619, test/accuracy=0.638100, test/loss=1.653229, test/num_examples=10000, total_duration=62880.577729, train/accuracy=0.929568, train/loss=0.250354, validation/accuracy=0.762320, validation/loss=0.960516, validation/num_examples=50000
I0129 00:47:52.125222 140026075662080 logging_writer.py:48] [179000] global_step=179000, grad_norm=8.222973823547363, loss=0.8218986392021179
I0129 00:48:25.984704 140026058876672 logging_writer.py:48] [179100] global_step=179100, grad_norm=8.965131759643555, loss=0.7662254571914673
I0129 00:48:59.891731 140026075662080 logging_writer.py:48] [179200] global_step=179200, grad_norm=9.218552589416504, loss=0.7604548931121826
I0129 00:49:33.723929 140026058876672 logging_writer.py:48] [179300] global_step=179300, grad_norm=9.964560508728027, loss=0.7527454495429993
I0129 00:50:07.609108 140026075662080 logging_writer.py:48] [179400] global_step=179400, grad_norm=9.454602241516113, loss=0.7449003458023071
I0129 00:50:41.560317 140026058876672 logging_writer.py:48] [179500] global_step=179500, grad_norm=10.015554428100586, loss=0.8246038556098938
I0129 00:51:15.430674 140026075662080 logging_writer.py:48] [179600] global_step=179600, grad_norm=8.532604217529297, loss=0.752223014831543
I0129 00:51:49.348418 140026058876672 logging_writer.py:48] [179700] global_step=179700, grad_norm=8.711237907409668, loss=0.707425594329834
I0129 00:52:23.225662 140026075662080 logging_writer.py:48] [179800] global_step=179800, grad_norm=8.93290901184082, loss=0.7752033472061157
I0129 00:52:57.219907 140026058876672 logging_writer.py:48] [179900] global_step=179900, grad_norm=8.664107322692871, loss=0.7591540813446045
I0129 00:53:31.073851 140026075662080 logging_writer.py:48] [180000] global_step=180000, grad_norm=9.88076400756836, loss=0.7335593104362488
I0129 00:54:04.986715 140026058876672 logging_writer.py:48] [180100] global_step=180100, grad_norm=8.377816200256348, loss=0.7396016716957092
I0129 00:54:38.877009 140026075662080 logging_writer.py:48] [180200] global_step=180200, grad_norm=9.130871772766113, loss=0.756412923336029
I0129 00:55:12.808039 140026058876672 logging_writer.py:48] [180300] global_step=180300, grad_norm=9.200826644897461, loss=0.6835066080093384
I0129 00:55:46.688156 140026075662080 logging_writer.py:48] [180400] global_step=180400, grad_norm=9.360336303710938, loss=0.7417365312576294
I0129 00:56:19.387919 140187804313408 spec.py:321] Evaluating on the training split.
I0129 00:56:25.623884 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 00:56:34.480608 140187804313408 spec.py:349] Evaluating on the test split.
I0129 00:56:37.067335 140187804313408 submission_runner.py:408] Time since start: 63408.27s, 	Step: 180498, 	{'train/accuracy': 0.9304248690605164, 'train/loss': 0.244537353515625, 'validation/accuracy': 0.7628200054168701, 'validation/loss': 0.9591808319091797, 'validation/num_examples': 50000, 'test/accuracy': 0.638200044631958, 'test/loss': 1.6477570533752441, 'test/num_examples': 10000, 'score': 61238.27491044998, 'total_duration': 63408.26725912094, 'accumulated_submission_time': 61238.27491044998, 'accumulated_eval_time': 2156.840073823929, 'accumulated_logging_time': 6.580701112747192}
I0129 00:56:37.117201 140026067269376 logging_writer.py:48] [180498] accumulated_eval_time=2156.840074, accumulated_logging_time=6.580701, accumulated_submission_time=61238.274910, global_step=180498, preemption_count=0, score=61238.274910, test/accuracy=0.638200, test/loss=1.647757, test/num_examples=10000, total_duration=63408.267259, train/accuracy=0.930425, train/loss=0.244537, validation/accuracy=0.762820, validation/loss=0.959181, validation/num_examples=50000
I0129 00:56:38.139490 140026159523584 logging_writer.py:48] [180500] global_step=180500, grad_norm=8.313468933105469, loss=0.7884446978569031
I0129 00:57:11.980480 140026067269376 logging_writer.py:48] [180600] global_step=180600, grad_norm=9.098681449890137, loss=0.7724201679229736
I0129 00:57:45.827611 140026159523584 logging_writer.py:48] [180700] global_step=180700, grad_norm=10.245217323303223, loss=0.7459005117416382
I0129 00:58:19.708584 140026067269376 logging_writer.py:48] [180800] global_step=180800, grad_norm=9.130897521972656, loss=0.7309524416923523
I0129 00:58:53.619242 140026159523584 logging_writer.py:48] [180900] global_step=180900, grad_norm=9.392126083374023, loss=0.7851852774620056
I0129 00:59:27.724169 140026067269376 logging_writer.py:48] [181000] global_step=181000, grad_norm=8.340415954589844, loss=0.6638748049736023
I0129 01:00:01.605582 140026159523584 logging_writer.py:48] [181100] global_step=181100, grad_norm=9.012317657470703, loss=0.7844552993774414
I0129 01:00:35.528734 140026067269376 logging_writer.py:48] [181200] global_step=181200, grad_norm=9.329055786132812, loss=0.7519989013671875
I0129 01:01:09.414535 140026159523584 logging_writer.py:48] [181300] global_step=181300, grad_norm=9.399155616760254, loss=0.7300211191177368
I0129 01:01:43.353331 140026067269376 logging_writer.py:48] [181400] global_step=181400, grad_norm=8.530985832214355, loss=0.7361461520195007
I0129 01:02:17.245507 140026159523584 logging_writer.py:48] [181500] global_step=181500, grad_norm=9.646076202392578, loss=0.763918399810791
I0129 01:02:51.165082 140026067269376 logging_writer.py:48] [181600] global_step=181600, grad_norm=8.796009063720703, loss=0.7810177803039551
I0129 01:03:25.106600 140026159523584 logging_writer.py:48] [181700] global_step=181700, grad_norm=9.455641746520996, loss=0.7381169199943542
I0129 01:03:58.994950 140026067269376 logging_writer.py:48] [181800] global_step=181800, grad_norm=8.277153968811035, loss=0.7104387283325195
I0129 01:04:32.918212 140026159523584 logging_writer.py:48] [181900] global_step=181900, grad_norm=8.924979209899902, loss=0.729801595211029
I0129 01:05:06.811887 140026067269376 logging_writer.py:48] [182000] global_step=182000, grad_norm=8.79289722442627, loss=0.7299534678459167
I0129 01:05:07.308223 140187804313408 spec.py:321] Evaluating on the training split.
I0129 01:05:13.500584 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 01:05:22.239198 140187804313408 spec.py:349] Evaluating on the test split.
I0129 01:05:24.830137 140187804313408 submission_runner.py:408] Time since start: 63936.03s, 	Step: 182003, 	{'train/accuracy': 0.9319595098495483, 'train/loss': 0.24809786677360535, 'validation/accuracy': 0.7633199691772461, 'validation/loss': 0.9583838582038879, 'validation/num_examples': 50000, 'test/accuracy': 0.6396000385284424, 'test/loss': 1.6491045951843262, 'test/num_examples': 10000, 'score': 61748.39874982834, 'total_duration': 63936.03000330925, 'accumulated_submission_time': 61748.39874982834, 'accumulated_eval_time': 2174.361873626709, 'accumulated_logging_time': 6.64280366897583}
I0129 01:05:24.881455 140026159523584 logging_writer.py:48] [182003] accumulated_eval_time=2174.361874, accumulated_logging_time=6.642804, accumulated_submission_time=61748.398750, global_step=182003, preemption_count=0, score=61748.398750, test/accuracy=0.639600, test/loss=1.649105, test/num_examples=10000, total_duration=63936.030003, train/accuracy=0.931960, train/loss=0.248098, validation/accuracy=0.763320, validation/loss=0.958384, validation/num_examples=50000
I0129 01:05:58.150589 140026167916288 logging_writer.py:48] [182100] global_step=182100, grad_norm=8.594621658325195, loss=0.7423986196517944
I0129 01:06:32.022064 140026159523584 logging_writer.py:48] [182200] global_step=182200, grad_norm=9.128986358642578, loss=0.7719408273696899
I0129 01:07:05.930771 140026167916288 logging_writer.py:48] [182300] global_step=182300, grad_norm=9.193796157836914, loss=0.7191367149353027
I0129 01:07:39.819585 140026159523584 logging_writer.py:48] [182400] global_step=182400, grad_norm=9.155400276184082, loss=0.7365853190422058
I0129 01:08:13.707456 140026167916288 logging_writer.py:48] [182500] global_step=182500, grad_norm=8.221519470214844, loss=0.6734039187431335
I0129 01:08:47.634525 140026159523584 logging_writer.py:48] [182600] global_step=182600, grad_norm=9.257146835327148, loss=0.7734644412994385
I0129 01:09:21.531397 140026167916288 logging_writer.py:48] [182700] global_step=182700, grad_norm=8.6118803024292, loss=0.7343214154243469
I0129 01:09:55.452433 140026159523584 logging_writer.py:48] [182800] global_step=182800, grad_norm=9.319537162780762, loss=0.8104249238967896
I0129 01:10:29.331116 140026167916288 logging_writer.py:48] [182900] global_step=182900, grad_norm=8.997934341430664, loss=0.7184774875640869
I0129 01:11:03.274031 140026159523584 logging_writer.py:48] [183000] global_step=183000, grad_norm=8.860897064208984, loss=0.7893984317779541
I0129 01:11:37.156482 140026167916288 logging_writer.py:48] [183100] global_step=183100, grad_norm=8.8945951461792, loss=0.7057593464851379
I0129 01:12:11.147018 140026159523584 logging_writer.py:48] [183200] global_step=183200, grad_norm=9.05747127532959, loss=0.7616729140281677
I0129 01:12:45.078000 140026167916288 logging_writer.py:48] [183300] global_step=183300, grad_norm=9.481911659240723, loss=0.7134592533111572
I0129 01:13:18.950554 140026159523584 logging_writer.py:48] [183400] global_step=183400, grad_norm=8.026165962219238, loss=0.7023067474365234
I0129 01:13:52.872075 140026167916288 logging_writer.py:48] [183500] global_step=183500, grad_norm=8.58170223236084, loss=0.7821124792098999
I0129 01:13:55.053198 140187804313408 spec.py:321] Evaluating on the training split.
I0129 01:14:01.310128 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 01:14:10.459426 140187804313408 spec.py:349] Evaluating on the test split.
I0129 01:14:13.075606 140187804313408 submission_runner.py:408] Time since start: 64464.28s, 	Step: 183508, 	{'train/accuracy': 0.93359375, 'train/loss': 0.2431531846523285, 'validation/accuracy': 0.7633199691772461, 'validation/loss': 0.9569819569587708, 'validation/num_examples': 50000, 'test/accuracy': 0.6368000507354736, 'test/loss': 1.650039553642273, 'test/num_examples': 10000, 'score': 62258.506739377975, 'total_duration': 64464.27554774284, 'accumulated_submission_time': 62258.506739377975, 'accumulated_eval_time': 2192.3842589855194, 'accumulated_logging_time': 6.70346999168396}
I0129 01:14:13.129714 140026067269376 logging_writer.py:48] [183508] accumulated_eval_time=2192.384259, accumulated_logging_time=6.703470, accumulated_submission_time=62258.506739, global_step=183508, preemption_count=0, score=62258.506739, test/accuracy=0.636800, test/loss=1.650040, test/num_examples=10000, total_duration=64464.275548, train/accuracy=0.933594, train/loss=0.243153, validation/accuracy=0.763320, validation/loss=0.956982, validation/num_examples=50000
I0129 01:14:44.641082 140026075662080 logging_writer.py:48] [183600] global_step=183600, grad_norm=10.261045455932617, loss=0.7802157998085022
I0129 01:15:18.479825 140026067269376 logging_writer.py:48] [183700] global_step=183700, grad_norm=8.910693168640137, loss=0.8079883456230164
I0129 01:15:52.367533 140026075662080 logging_writer.py:48] [183800] global_step=183800, grad_norm=8.806255340576172, loss=0.7309091091156006
I0129 01:16:26.257041 140026067269376 logging_writer.py:48] [183900] global_step=183900, grad_norm=9.036112785339355, loss=0.8634082078933716
I0129 01:17:00.154676 140026075662080 logging_writer.py:48] [184000] global_step=184000, grad_norm=10.21773624420166, loss=0.7414354085922241
I0129 01:17:34.073256 140026067269376 logging_writer.py:48] [184100] global_step=184100, grad_norm=8.418112754821777, loss=0.6234017610549927
I0129 01:18:07.957683 140026075662080 logging_writer.py:48] [184200] global_step=184200, grad_norm=10.160118103027344, loss=0.7762836217880249
I0129 01:18:41.962964 140026067269376 logging_writer.py:48] [184300] global_step=184300, grad_norm=9.686909675598145, loss=0.7806283235549927
I0129 01:19:15.858928 140026075662080 logging_writer.py:48] [184400] global_step=184400, grad_norm=9.362570762634277, loss=0.7255314588546753
I0129 01:19:49.766865 140026067269376 logging_writer.py:48] [184500] global_step=184500, grad_norm=9.52765941619873, loss=0.7100239396095276
I0129 01:20:23.686024 140026075662080 logging_writer.py:48] [184600] global_step=184600, grad_norm=8.404278755187988, loss=0.7152991890907288
I0129 01:20:57.576097 140026067269376 logging_writer.py:48] [184700] global_step=184700, grad_norm=8.201759338378906, loss=0.7309008836746216
I0129 01:21:31.491670 140026075662080 logging_writer.py:48] [184800] global_step=184800, grad_norm=8.771570205688477, loss=0.702069878578186
I0129 01:22:05.406559 140026067269376 logging_writer.py:48] [184900] global_step=184900, grad_norm=9.142816543579102, loss=0.7402828931808472
I0129 01:22:39.318651 140026075662080 logging_writer.py:48] [185000] global_step=185000, grad_norm=9.01934814453125, loss=0.7218454480171204
I0129 01:22:43.190024 140187804313408 spec.py:321] Evaluating on the training split.
I0129 01:22:49.470841 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 01:22:58.275738 140187804313408 spec.py:349] Evaluating on the test split.
I0129 01:23:00.954070 140187804313408 submission_runner.py:408] Time since start: 64992.15s, 	Step: 185013, 	{'train/accuracy': 0.9331752061843872, 'train/loss': 0.24071836471557617, 'validation/accuracy': 0.7636199593544006, 'validation/loss': 0.957119882106781, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.648453950881958, 'test/num_examples': 10000, 'score': 62768.50212907791, 'total_duration': 64992.15400886536, 'accumulated_submission_time': 62768.50212907791, 'accumulated_eval_time': 2210.148278236389, 'accumulated_logging_time': 6.768203496932983}
I0129 01:23:01.013962 140026050483968 logging_writer.py:48] [185013] accumulated_eval_time=2210.148278, accumulated_logging_time=6.768203, accumulated_submission_time=62768.502129, global_step=185013, preemption_count=0, score=62768.502129, test/accuracy=0.637800, test/loss=1.648454, test/num_examples=10000, total_duration=64992.154009, train/accuracy=0.933175, train/loss=0.240718, validation/accuracy=0.763620, validation/loss=0.957120, validation/num_examples=50000
I0129 01:23:30.789089 140026058876672 logging_writer.py:48] [185100] global_step=185100, grad_norm=9.462385177612305, loss=0.7412752509117126
I0129 01:24:04.645472 140026050483968 logging_writer.py:48] [185200] global_step=185200, grad_norm=8.946919441223145, loss=0.6756020784378052
I0129 01:24:38.568495 140026058876672 logging_writer.py:48] [185300] global_step=185300, grad_norm=9.991116523742676, loss=0.7908383011817932
I0129 01:25:12.528158 140026050483968 logging_writer.py:48] [185400] global_step=185400, grad_norm=10.325287818908691, loss=0.743273138999939
I0129 01:25:46.410290 140026058876672 logging_writer.py:48] [185500] global_step=185500, grad_norm=9.68974494934082, loss=0.7382237911224365
I0129 01:26:20.298769 140026050483968 logging_writer.py:48] [185600] global_step=185600, grad_norm=8.840703964233398, loss=0.6971449851989746
I0129 01:26:54.202678 140026058876672 logging_writer.py:48] [185700] global_step=185700, grad_norm=9.807121276855469, loss=0.7607864141464233
I0129 01:27:00.832685 140026050483968 logging_writer.py:48] [185721] global_step=185721, preemption_count=0, score=63008.252862
I0129 01:27:01.254546 140187804313408 checkpoints.py:490] Saving checkpoint at step: 185721
I0129 01:27:02.465708 140187804313408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_3/checkpoint_185721
I0129 01:27:02.490833 140187804313408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_3/checkpoint_185721.
I0129 01:27:03.251317 140187804313408 submission_runner.py:583] Tuning trial 3/5
I0129 01:27:03.251524 140187804313408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0129 01:27:03.258658 140187804313408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000996492337435484, 'train/loss': 6.91312313079834, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 30.50831913948059, 'total_duration': 48.360055446624756, 'accumulated_submission_time': 30.50831913948059, 'accumulated_eval_time': 17.85161852836609, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1497, {'train/accuracy': 0.06869818270206451, 'train/loss': 5.348140716552734, 'validation/accuracy': 0.06425999850034714, 'validation/loss': 5.3968071937561035, 'validation/num_examples': 50000, 'test/accuracy': 0.04620000347495079, 'test/loss': 5.633206367492676, 'test/num_examples': 10000, 'score': 540.5099921226501, 'total_duration': 576.5030663013458, 'accumulated_submission_time': 540.5099921226501, 'accumulated_eval_time': 35.9192590713501, 'accumulated_logging_time': 0.019089937210083008, 'global_step': 1497, 'preemption_count': 0}), (2993, {'train/accuracy': 0.16974250972270966, 'train/loss': 4.304430961608887, 'validation/accuracy': 0.15215998888015747, 'validation/loss': 4.434348106384277, 'validation/num_examples': 50000, 'test/accuracy': 0.10750000178813934, 'test/loss': 4.922767639160156, 'test/num_examples': 10000, 'score': 1050.4253158569336, 'total_duration': 1104.4395382404327, 'accumulated_submission_time': 1050.4253158569336, 'accumulated_eval_time': 53.85086178779602, 'accumulated_logging_time': 0.05376577377319336, 'global_step': 2993, 'preemption_count': 0}), (4489, {'train/accuracy': 0.2671595811843872, 'train/loss': 3.5422446727752686, 'validation/accuracy': 0.245619997382164, 'validation/loss': 3.673764944076538, 'validation/num_examples': 50000, 'test/accuracy': 0.1746000051498413, 'test/loss': 4.269079208374023, 'test/num_examples': 10000, 'score': 1560.6546158790588, 'total_duration': 1632.4659051895142, 'accumulated_submission_time': 1560.6546158790588, 'accumulated_eval_time': 71.5679407119751, 'accumulated_logging_time': 0.08060026168823242, 'global_step': 4489, 'preemption_count': 0}), (5985, {'train/accuracy': 0.3645368218421936, 'train/loss': 2.9136829376220703, 'validation/accuracy': 0.340719997882843, 'validation/loss': 3.063777446746826, 'validation/num_examples': 50000, 'test/accuracy': 0.2565000057220459, 'test/loss': 3.7345449924468994, 'test/num_examples': 10000, 'score': 2070.620194196701, 'total_duration': 2160.280205488205, 'accumulated_submission_time': 2070.620194196701, 'accumulated_eval_time': 89.33231997489929, 'accumulated_logging_time': 0.11035013198852539, 'global_step': 5985, 'preemption_count': 0}), (7481, {'train/accuracy': 0.442402720451355, 'train/loss': 2.474280834197998, 'validation/accuracy': 0.38402000069618225, 'validation/loss': 2.817934513092041, 'validation/num_examples': 50000, 'test/accuracy': 0.28950002789497375, 'test/loss': 3.5531110763549805, 'test/num_examples': 10000, 'score': 2580.7226872444153, 'total_duration': 2688.5322892665863, 'accumulated_submission_time': 2580.7226872444153, 'accumulated_eval_time': 107.3965380191803, 'accumulated_logging_time': 0.142303466796875, 'global_step': 7481, 'preemption_count': 0}), (8978, {'train/accuracy': 0.4957549273967743, 'train/loss': 2.1577329635620117, 'validation/accuracy': 0.4500799775123596, 'validation/loss': 2.436403274536133, 'validation/num_examples': 50000, 'test/accuracy': 0.34530001878738403, 'test/loss': 3.1740477085113525, 'test/num_examples': 10000, 'score': 3090.6557648181915, 'total_duration': 3217.1792571544647, 'accumulated_submission_time': 3090.6557648181915, 'accumulated_eval_time': 126.0262610912323, 'accumulated_logging_time': 0.1721491813659668, 'global_step': 8978, 'preemption_count': 0}), (10475, {'train/accuracy': 0.521882951259613, 'train/loss': 2.0246825218200684, 'validation/accuracy': 0.4817799925804138, 'validation/loss': 2.2595558166503906, 'validation/num_examples': 50000, 'test/accuracy': 0.3767000138759613, 'test/loss': 2.959792375564575, 'test/num_examples': 10000, 'score': 3600.778788328171, 'total_duration': 3745.4569323062897, 'accumulated_submission_time': 3600.778788328171, 'accumulated_eval_time': 144.09838795661926, 'accumulated_logging_time': 0.2016129493713379, 'global_step': 10475, 'preemption_count': 0}), (11973, {'train/accuracy': 0.5642538070678711, 'train/loss': 1.8322232961654663, 'validation/accuracy': 0.5165799856185913, 'validation/loss': 2.0682852268218994, 'validation/num_examples': 50000, 'test/accuracy': 0.397100031375885, 'test/loss': 2.806823492050171, 'test/num_examples': 10000, 'score': 4110.76530623436, 'total_duration': 4273.276990890503, 'accumulated_submission_time': 4110.76530623436, 'accumulated_eval_time': 161.8473880290985, 'accumulated_logging_time': 0.2317366600036621, 'global_step': 11973, 'preemption_count': 0}), (13472, {'train/accuracy': 0.5740792155265808, 'train/loss': 1.7880722284317017, 'validation/accuracy': 0.5296199917793274, 'validation/loss': 2.0117011070251465, 'validation/num_examples': 50000, 'test/accuracy': 0.4182000160217285, 'test/loss': 2.717848777770996, 'test/num_examples': 10000, 'score': 4620.987542629242, 'total_duration': 4801.707133054733, 'accumulated_submission_time': 4620.987542629242, 'accumulated_eval_time': 179.95350456237793, 'accumulated_logging_time': 0.2778005599975586, 'global_step': 13472, 'preemption_count': 0}), (14972, {'train/accuracy': 0.5755141973495483, 'train/loss': 1.759779930114746, 'validation/accuracy': 0.540399968624115, 'validation/loss': 1.9560120105743408, 'validation/num_examples': 50000, 'test/accuracy': 0.4134000241756439, 'test/loss': 2.7285807132720947, 'test/num_examples': 10000, 'score': 5131.111432313919, 'total_duration': 5329.801098108292, 'accumulated_submission_time': 5131.111432313919, 'accumulated_eval_time': 197.83906412124634, 'accumulated_logging_time': 0.3077218532562256, 'global_step': 14972, 'preemption_count': 0}), (16471, {'train/accuracy': 0.6073421239852905, 'train/loss': 1.608778953552246, 'validation/accuracy': 0.5467000007629395, 'validation/loss': 1.9259779453277588, 'validation/num_examples': 50000, 'test/accuracy': 0.43570002913475037, 'test/loss': 2.652667284011841, 'test/num_examples': 10000, 'score': 5641.149353981018, 'total_duration': 5857.610563755035, 'accumulated_submission_time': 5641.149353981018, 'accumulated_eval_time': 215.52660512924194, 'accumulated_logging_time': 0.33754658699035645, 'global_step': 16471, 'preemption_count': 0}), (17972, {'train/accuracy': 0.6112284660339355, 'train/loss': 1.6073660850524902, 'validation/accuracy': 0.5543400049209595, 'validation/loss': 1.8950927257537842, 'validation/num_examples': 50000, 'test/accuracy': 0.43220001459121704, 'test/loss': 2.6180338859558105, 'test/num_examples': 10000, 'score': 6151.183410406113, 'total_duration': 6385.664488315582, 'accumulated_submission_time': 6151.183410406113, 'accumulated_eval_time': 233.45811223983765, 'accumulated_logging_time': 0.37097597122192383, 'global_step': 17972, 'preemption_count': 0}), (19473, {'train/accuracy': 0.6114277839660645, 'train/loss': 1.5878651142120361, 'validation/accuracy': 0.5617799758911133, 'validation/loss': 1.8461424112319946, 'validation/num_examples': 50000, 'test/accuracy': 0.4406000077724457, 'test/loss': 2.583203077316284, 'test/num_examples': 10000, 'score': 6661.199192047119, 'total_duration': 6913.575413227081, 'accumulated_submission_time': 6661.199192047119, 'accumulated_eval_time': 251.256432056427, 'accumulated_logging_time': 0.4122631549835205, 'global_step': 19473, 'preemption_count': 0}), (20974, {'train/accuracy': 0.6094148755073547, 'train/loss': 1.5837292671203613, 'validation/accuracy': 0.5648800134658813, 'validation/loss': 1.8347382545471191, 'validation/num_examples': 50000, 'test/accuracy': 0.4406000077724457, 'test/loss': 2.576707363128662, 'test/num_examples': 10000, 'score': 7171.261469125748, 'total_duration': 7441.400032997131, 'accumulated_submission_time': 7171.261469125748, 'accumulated_eval_time': 268.92280554771423, 'accumulated_logging_time': 0.4528038501739502, 'global_step': 20974, 'preemption_count': 0}), (22476, {'train/accuracy': 0.6075215339660645, 'train/loss': 1.6099942922592163, 'validation/accuracy': 0.5667600035667419, 'validation/loss': 1.828776240348816, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.5712270736694336, 'test/num_examples': 10000, 'score': 7681.3733949661255, 'total_duration': 7969.5034058094025, 'accumulated_submission_time': 7681.3733949661255, 'accumulated_eval_time': 286.82858443260193, 'accumulated_logging_time': 0.48407411575317383, 'global_step': 22476, 'preemption_count': 0}), (23978, {'train/accuracy': 0.6109893321990967, 'train/loss': 1.584595799446106, 'validation/accuracy': 0.5736599564552307, 'validation/loss': 1.797381043434143, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.56406831741333, 'test/num_examples': 10000, 'score': 8191.5354125499725, 'total_duration': 8497.370985507965, 'accumulated_submission_time': 8191.5354125499725, 'accumulated_eval_time': 304.44716572761536, 'accumulated_logging_time': 0.5166065692901611, 'global_step': 23978, 'preemption_count': 0}), (25479, {'train/accuracy': 0.6316764950752258, 'train/loss': 1.4977443218231201, 'validation/accuracy': 0.5838800072669983, 'validation/loss': 1.7310395240783691, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.4388608932495117, 'test/num_examples': 10000, 'score': 8701.451757669449, 'total_duration': 9025.565544128418, 'accumulated_submission_time': 8701.451757669449, 'accumulated_eval_time': 322.6385922431946, 'accumulated_logging_time': 0.5493104457855225, 'global_step': 25479, 'preemption_count': 0}), (26982, {'train/accuracy': 0.6469228267669678, 'train/loss': 1.421025037765503, 'validation/accuracy': 0.5805799961090088, 'validation/loss': 1.7613977193832397, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.5006914138793945, 'test/num_examples': 10000, 'score': 9211.581802606583, 'total_duration': 9553.536763191223, 'accumulated_submission_time': 9211.581802606583, 'accumulated_eval_time': 340.3928325176239, 'accumulated_logging_time': 0.5819401741027832, 'global_step': 26982, 'preemption_count': 0}), (28484, {'train/accuracy': 0.6459661722183228, 'train/loss': 1.4371527433395386, 'validation/accuracy': 0.5914199948310852, 'validation/loss': 1.6988749504089355, 'validation/num_examples': 50000, 'test/accuracy': 0.46550002694129944, 'test/loss': 2.420314311981201, 'test/num_examples': 10000, 'score': 9721.777070045471, 'total_duration': 10081.70643401146, 'accumulated_submission_time': 9721.777070045471, 'accumulated_eval_time': 358.2810490131378, 'accumulated_logging_time': 0.6139397621154785, 'global_step': 28484, 'preemption_count': 0}), (29988, {'train/accuracy': 0.6504305005073547, 'train/loss': 1.4086905717849731, 'validation/accuracy': 0.5985400080680847, 'validation/loss': 1.6694458723068237, 'validation/num_examples': 50000, 'test/accuracy': 0.4718000292778015, 'test/loss': 2.4125912189483643, 'test/num_examples': 10000, 'score': 10232.031190395355, 'total_duration': 10609.787842988968, 'accumulated_submission_time': 10232.031190395355, 'accumulated_eval_time': 376.0208065509796, 'accumulated_logging_time': 0.6470503807067871, 'global_step': 29988, 'preemption_count': 0}), (31491, {'train/accuracy': 0.6327327489852905, 'train/loss': 1.4778612852096558, 'validation/accuracy': 0.5869199633598328, 'validation/loss': 1.7286511659622192, 'validation/num_examples': 50000, 'test/accuracy': 0.4706000089645386, 'test/loss': 2.442723035812378, 'test/num_examples': 10000, 'score': 10742.144251823425, 'total_duration': 11137.651804208755, 'accumulated_submission_time': 10742.144251823425, 'accumulated_eval_time': 393.6835870742798, 'accumulated_logging_time': 0.680239200592041, 'global_step': 31491, 'preemption_count': 0}), (32994, {'train/accuracy': 0.6381337642669678, 'train/loss': 1.4649090766906738, 'validation/accuracy': 0.5950799584388733, 'validation/loss': 1.6732388734817505, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.3845551013946533, 'test/num_examples': 10000, 'score': 11252.236163139343, 'total_duration': 11665.972929954529, 'accumulated_submission_time': 11252.236163139343, 'accumulated_eval_time': 411.8246719837189, 'accumulated_logging_time': 0.7132534980773926, 'global_step': 32994, 'preemption_count': 0}), (34497, {'train/accuracy': 0.647480845451355, 'train/loss': 1.4073861837387085, 'validation/accuracy': 0.6032800078392029, 'validation/loss': 1.6388838291168213, 'validation/num_examples': 50000, 'test/accuracy': 0.4724000096321106, 'test/loss': 2.404728889465332, 'test/num_examples': 10000, 'score': 11762.316961288452, 'total_duration': 12193.809639453888, 'accumulated_submission_time': 11762.316961288452, 'accumulated_eval_time': 429.4828112125397, 'accumulated_logging_time': 0.7562506198883057, 'global_step': 34497, 'preemption_count': 0}), (36000, {'train/accuracy': 0.6731704473495483, 'train/loss': 1.2901920080184937, 'validation/accuracy': 0.5949400067329407, 'validation/loss': 1.6796997785568237, 'validation/num_examples': 50000, 'test/accuracy': 0.4748000204563141, 'test/loss': 2.377642869949341, 'test/num_examples': 10000, 'score': 12272.395174503326, 'total_duration': 12721.819028377533, 'accumulated_submission_time': 12272.395174503326, 'accumulated_eval_time': 447.32536363601685, 'accumulated_logging_time': 0.7911787033081055, 'global_step': 36000, 'preemption_count': 0}), (37501, {'train/accuracy': 0.6614716053009033, 'train/loss': 1.3486958742141724, 'validation/accuracy': 0.602679967880249, 'validation/loss': 1.6470184326171875, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.3493621349334717, 'test/num_examples': 10000, 'score': 12782.483393192291, 'total_duration': 13249.76772403717, 'accumulated_submission_time': 12782.483393192291, 'accumulated_eval_time': 465.09705877304077, 'accumulated_logging_time': 0.8253750801086426, 'global_step': 37501, 'preemption_count': 0}), (39005, {'train/accuracy': 0.6461654901504517, 'train/loss': 1.4173647165298462, 'validation/accuracy': 0.5987799763679504, 'validation/loss': 1.6708097457885742, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.3809192180633545, 'test/num_examples': 10000, 'score': 13292.577105998993, 'total_duration': 13777.983241558075, 'accumulated_submission_time': 13292.577105998993, 'accumulated_eval_time': 483.1254951953888, 'accumulated_logging_time': 0.8638536930084229, 'global_step': 39005, 'preemption_count': 0}), (40508, {'train/accuracy': 0.6537986397743225, 'train/loss': 1.3841086626052856, 'validation/accuracy': 0.6029999852180481, 'validation/loss': 1.6390022039413452, 'validation/num_examples': 50000, 'test/accuracy': 0.48850002884864807, 'test/loss': 2.363870143890381, 'test/num_examples': 10000, 'score': 13802.59973692894, 'total_duration': 14306.094644546509, 'accumulated_submission_time': 13802.59973692894, 'accumulated_eval_time': 501.1253571510315, 'accumulated_logging_time': 0.8978090286254883, 'global_step': 40508, 'preemption_count': 0}), (42012, {'train/accuracy': 0.644949734210968, 'train/loss': 1.4297263622283936, 'validation/accuracy': 0.5964999794960022, 'validation/loss': 1.6860605478286743, 'validation/num_examples': 50000, 'test/accuracy': 0.4715000092983246, 'test/loss': 2.4117226600646973, 'test/num_examples': 10000, 'score': 14312.696018218994, 'total_duration': 14833.994425058365, 'accumulated_submission_time': 14312.696018218994, 'accumulated_eval_time': 518.831524848938, 'accumulated_logging_time': 0.941624641418457, 'global_step': 42012, 'preemption_count': 0}), (43515, {'train/accuracy': 0.6421595811843872, 'train/loss': 1.4424054622650146, 'validation/accuracy': 0.5974400043487549, 'validation/loss': 1.672808289527893, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.345468759536743, 'test/num_examples': 10000, 'score': 14822.645218849182, 'total_duration': 15361.96886754036, 'accumulated_submission_time': 14822.645218849182, 'accumulated_eval_time': 536.7683305740356, 'accumulated_logging_time': 0.976036548614502, 'global_step': 43515, 'preemption_count': 0}), (45019, {'train/accuracy': 0.6957908272743225, 'train/loss': 1.203609585762024, 'validation/accuracy': 0.6162199974060059, 'validation/loss': 1.5929490327835083, 'validation/num_examples': 50000, 'test/accuracy': 0.4869000315666199, 'test/loss': 2.3410987854003906, 'test/num_examples': 10000, 'score': 15332.848045110703, 'total_duration': 15889.959641456604, 'accumulated_submission_time': 15332.848045110703, 'accumulated_eval_time': 554.4605889320374, 'accumulated_logging_time': 1.0155045986175537, 'global_step': 45019, 'preemption_count': 0}), (46523, {'train/accuracy': 0.6674505472183228, 'train/loss': 1.308665156364441, 'validation/accuracy': 0.6080999970436096, 'validation/loss': 1.629995584487915, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.37629771232605, 'test/num_examples': 10000, 'score': 15842.934381008148, 'total_duration': 16418.139196634293, 'accumulated_submission_time': 15842.934381008148, 'accumulated_eval_time': 572.4644169807434, 'accumulated_logging_time': 1.0506083965301514, 'global_step': 46523, 'preemption_count': 0}), (48027, {'train/accuracy': 0.6508689522743225, 'train/loss': 1.3953670263290405, 'validation/accuracy': 0.5989199876785278, 'validation/loss': 1.662032961845398, 'validation/num_examples': 50000, 'test/accuracy': 0.47510001063346863, 'test/loss': 2.403264045715332, 'test/num_examples': 10000, 'score': 16352.884350776672, 'total_duration': 16946.79745745659, 'accumulated_submission_time': 16352.884350776672, 'accumulated_eval_time': 591.0792412757874, 'accumulated_logging_time': 1.088939905166626, 'global_step': 48027, 'preemption_count': 0}), (49531, {'train/accuracy': 0.6604352593421936, 'train/loss': 1.361952304840088, 'validation/accuracy': 0.6140999794006348, 'validation/loss': 1.5894731283187866, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.311565637588501, 'test/num_examples': 10000, 'score': 16863.04797935486, 'total_duration': 17474.806631326675, 'accumulated_submission_time': 16863.04797935486, 'accumulated_eval_time': 608.8275811672211, 'accumulated_logging_time': 1.131298542022705, 'global_step': 49531, 'preemption_count': 0}), (51036, {'train/accuracy': 0.6528220772743225, 'train/loss': 1.377036690711975, 'validation/accuracy': 0.6045599579811096, 'validation/loss': 1.6280007362365723, 'validation/num_examples': 50000, 'test/accuracy': 0.4839000105857849, 'test/loss': 2.3610148429870605, 'test/num_examples': 10000, 'score': 17373.247692346573, 'total_duration': 18002.78094768524, 'accumulated_submission_time': 17373.247692346573, 'accumulated_eval_time': 626.5100448131561, 'accumulated_logging_time': 1.1692650318145752, 'global_step': 51036, 'preemption_count': 0}), (52540, {'train/accuracy': 0.6638432741165161, 'train/loss': 1.3441935777664185, 'validation/accuracy': 0.6182799935340881, 'validation/loss': 1.5748586654663086, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.280700206756592, 'test/num_examples': 10000, 'score': 17883.32636666298, 'total_duration': 18530.83543086052, 'accumulated_submission_time': 17883.32636666298, 'accumulated_eval_time': 644.3940415382385, 'accumulated_logging_time': 1.2060277462005615, 'global_step': 52540, 'preemption_count': 0}), (54045, {'train/accuracy': 0.6905492544174194, 'train/loss': 1.2071837186813354, 'validation/accuracy': 0.604699969291687, 'validation/loss': 1.6392205953598022, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.356093168258667, 'test/num_examples': 10000, 'score': 18393.53794503212, 'total_duration': 19058.91395521164, 'accumulated_submission_time': 18393.53794503212, 'accumulated_eval_time': 662.1686675548553, 'accumulated_logging_time': 1.2445552349090576, 'global_step': 54045, 'preemption_count': 0}), (55549, {'train/accuracy': 0.6698620915412903, 'train/loss': 1.3108255863189697, 'validation/accuracy': 0.608959972858429, 'validation/loss': 1.6123535633087158, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.349951982498169, 'test/num_examples': 10000, 'score': 18903.47699022293, 'total_duration': 19586.715607881546, 'accumulated_submission_time': 18903.47699022293, 'accumulated_eval_time': 679.9338908195496, 'accumulated_logging_time': 1.2859973907470703, 'global_step': 55549, 'preemption_count': 0}), (57054, {'train/accuracy': 0.6702606678009033, 'train/loss': 1.301419734954834, 'validation/accuracy': 0.6184399724006653, 'validation/loss': 1.5728846788406372, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.252488136291504, 'test/num_examples': 10000, 'score': 19413.604824781418, 'total_duration': 20114.741649627686, 'accumulated_submission_time': 19413.604824781418, 'accumulated_eval_time': 697.7373259067535, 'accumulated_logging_time': 1.3273625373840332, 'global_step': 57054, 'preemption_count': 0}), (58558, {'train/accuracy': 0.6655771732330322, 'train/loss': 1.3216028213500977, 'validation/accuracy': 0.6157599687576294, 'validation/loss': 1.5845470428466797, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.278593063354492, 'test/num_examples': 10000, 'score': 19923.637528181076, 'total_duration': 20642.72845196724, 'accumulated_submission_time': 19923.637528181076, 'accumulated_eval_time': 715.5969526767731, 'accumulated_logging_time': 1.3676202297210693, 'global_step': 58558, 'preemption_count': 0}), (60063, {'train/accuracy': 0.6641222834587097, 'train/loss': 1.3311790227890015, 'validation/accuracy': 0.6173999905586243, 'validation/loss': 1.5765674114227295, 'validation/num_examples': 50000, 'test/accuracy': 0.4969000220298767, 'test/loss': 2.2979726791381836, 'test/num_examples': 10000, 'score': 20433.673320770264, 'total_duration': 21170.53536248207, 'accumulated_submission_time': 20433.673320770264, 'accumulated_eval_time': 733.2713446617126, 'accumulated_logging_time': 1.408951997756958, 'global_step': 60063, 'preemption_count': 0}), (61568, {'train/accuracy': 0.6678690910339355, 'train/loss': 1.3077174425125122, 'validation/accuracy': 0.622439980506897, 'validation/loss': 1.5476828813552856, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.2889938354492188, 'test/num_examples': 10000, 'score': 20943.756311655045, 'total_duration': 21698.42645382881, 'accumulated_submission_time': 20943.756311655045, 'accumulated_eval_time': 750.9849836826324, 'accumulated_logging_time': 1.4479811191558838, 'global_step': 61568, 'preemption_count': 0}), (63074, {'train/accuracy': 0.7181122303009033, 'train/loss': 1.0941146612167358, 'validation/accuracy': 0.6288599967956543, 'validation/loss': 1.528099536895752, 'validation/num_examples': 50000, 'test/accuracy': 0.5130000114440918, 'test/loss': 2.2014949321746826, 'test/num_examples': 10000, 'score': 21454.002192497253, 'total_duration': 22226.784834861755, 'accumulated_submission_time': 21454.002192497253, 'accumulated_eval_time': 769.0031280517578, 'accumulated_logging_time': 1.4872050285339355, 'global_step': 63074, 'preemption_count': 0}), (64579, {'train/accuracy': 0.6921635866165161, 'train/loss': 1.2201071977615356, 'validation/accuracy': 0.6279599666595459, 'validation/loss': 1.5359902381896973, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.242102861404419, 'test/num_examples': 10000, 'score': 21964.22785615921, 'total_duration': 22754.996037244797, 'accumulated_submission_time': 21964.22785615921, 'accumulated_eval_time': 786.8960626125336, 'accumulated_logging_time': 1.5244412422180176, 'global_step': 64579, 'preemption_count': 0}), (66085, {'train/accuracy': 0.6760801672935486, 'train/loss': 1.270575761795044, 'validation/accuracy': 0.623199999332428, 'validation/loss': 1.5508557558059692, 'validation/num_examples': 50000, 'test/accuracy': 0.4992000162601471, 'test/loss': 2.274237632751465, 'test/num_examples': 10000, 'score': 22474.41934657097, 'total_duration': 23283.187956809998, 'accumulated_submission_time': 22474.41934657097, 'accumulated_eval_time': 804.7957236766815, 'accumulated_logging_time': 1.5703482627868652, 'global_step': 66085, 'preemption_count': 0}), (67590, {'train/accuracy': 0.675203263759613, 'train/loss': 1.2853842973709106, 'validation/accuracy': 0.6229400038719177, 'validation/loss': 1.5472275018692017, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.2707667350769043, 'test/num_examples': 10000, 'score': 22984.531358480453, 'total_duration': 23811.220719575882, 'accumulated_submission_time': 22984.531358480453, 'accumulated_eval_time': 822.6233458518982, 'accumulated_logging_time': 1.609081745147705, 'global_step': 67590, 'preemption_count': 0}), (69095, {'train/accuracy': 0.682039201259613, 'train/loss': 1.2529497146606445, 'validation/accuracy': 0.6260799765586853, 'validation/loss': 1.544729471206665, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2389981746673584, 'test/num_examples': 10000, 'score': 23494.504409313202, 'total_duration': 24338.96833062172, 'accumulated_submission_time': 23494.504409313202, 'accumulated_eval_time': 840.2972972393036, 'accumulated_logging_time': 1.6531808376312256, 'global_step': 69095, 'preemption_count': 0}), (70600, {'train/accuracy': 0.6823381781578064, 'train/loss': 1.261600375175476, 'validation/accuracy': 0.6331599950790405, 'validation/loss': 1.5052260160446167, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.193894147872925, 'test/num_examples': 10000, 'score': 24004.64289021492, 'total_duration': 24866.995292663574, 'accumulated_submission_time': 24004.64289021492, 'accumulated_eval_time': 858.0913376808167, 'accumulated_logging_time': 1.6934871673583984, 'global_step': 70600, 'preemption_count': 0}), (72105, {'train/accuracy': 0.7115353941917419, 'train/loss': 1.123003363609314, 'validation/accuracy': 0.6309399604797363, 'validation/loss': 1.522621989250183, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2285428047180176, 'test/num_examples': 10000, 'score': 24514.733004808426, 'total_duration': 25394.98640203476, 'accumulated_submission_time': 24514.733004808426, 'accumulated_eval_time': 875.8923208713531, 'accumulated_logging_time': 1.7388732433319092, 'global_step': 72105, 'preemption_count': 0}), (73610, {'train/accuracy': 0.7038823366165161, 'train/loss': 1.1389704942703247, 'validation/accuracy': 0.6362000107765198, 'validation/loss': 1.4967260360717773, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.229922294616699, 'test/num_examples': 10000, 'score': 25024.659342050552, 'total_duration': 25922.627481222153, 'accumulated_submission_time': 25024.659342050552, 'accumulated_eval_time': 893.5084192752838, 'accumulated_logging_time': 1.7819523811340332, 'global_step': 73610, 'preemption_count': 0}), (75116, {'train/accuracy': 0.6950334906578064, 'train/loss': 1.1846526861190796, 'validation/accuracy': 0.6358999609947205, 'validation/loss': 1.490337610244751, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.216155767440796, 'test/num_examples': 10000, 'score': 25534.734695911407, 'total_duration': 26450.662058591843, 'accumulated_submission_time': 25534.734695911407, 'accumulated_eval_time': 911.3698537349701, 'accumulated_logging_time': 1.823401689529419, 'global_step': 75116, 'preemption_count': 0}), (76621, {'train/accuracy': 0.6759606003761292, 'train/loss': 1.2714606523513794, 'validation/accuracy': 0.6222000122070312, 'validation/loss': 1.5547786951065063, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.2797510623931885, 'test/num_examples': 10000, 'score': 26044.821749687195, 'total_duration': 26978.580530405045, 'accumulated_submission_time': 26044.821749687195, 'accumulated_eval_time': 929.1047916412354, 'accumulated_logging_time': 1.8654460906982422, 'global_step': 76621, 'preemption_count': 0}), (78127, {'train/accuracy': 0.696687638759613, 'train/loss': 1.187483787536621, 'validation/accuracy': 0.6402400135993958, 'validation/loss': 1.469867467880249, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.1450984477996826, 'test/num_examples': 10000, 'score': 26555.042127132416, 'total_duration': 27506.72307229042, 'accumulated_submission_time': 26555.042127132416, 'accumulated_eval_time': 946.9249217510223, 'accumulated_logging_time': 1.913517951965332, 'global_step': 78127, 'preemption_count': 0}), (79632, {'train/accuracy': 0.6890544891357422, 'train/loss': 1.2304526567459106, 'validation/accuracy': 0.635919988155365, 'validation/loss': 1.4974093437194824, 'validation/num_examples': 50000, 'test/accuracy': 0.5033000111579895, 'test/loss': 2.213355302810669, 'test/num_examples': 10000, 'score': 27065.05168557167, 'total_duration': 28034.80432486534, 'accumulated_submission_time': 27065.05168557167, 'accumulated_eval_time': 964.8990716934204, 'accumulated_logging_time': 1.9566450119018555, 'global_step': 79632, 'preemption_count': 0}), (81138, {'train/accuracy': 0.7074896097183228, 'train/loss': 1.138108491897583, 'validation/accuracy': 0.6323999762535095, 'validation/loss': 1.5167434215545654, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.267728090286255, 'test/num_examples': 10000, 'score': 27575.18598818779, 'total_duration': 28562.895799398422, 'accumulated_submission_time': 27575.18598818779, 'accumulated_eval_time': 982.7576727867126, 'accumulated_logging_time': 2.000518560409546, 'global_step': 81138, 'preemption_count': 0}), (82643, {'train/accuracy': 0.7142258882522583, 'train/loss': 1.0972977876663208, 'validation/accuracy': 0.6411199569702148, 'validation/loss': 1.474290132522583, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.1949095726013184, 'test/num_examples': 10000, 'score': 28085.256512880325, 'total_duration': 29090.73493361473, 'accumulated_submission_time': 28085.256512880325, 'accumulated_eval_time': 1000.4262602329254, 'accumulated_logging_time': 2.045931100845337, 'global_step': 82643, 'preemption_count': 0}), (84148, {'train/accuracy': 0.7117147445678711, 'train/loss': 1.1199214458465576, 'validation/accuracy': 0.6462799906730652, 'validation/loss': 1.4425675868988037, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.1688649654388428, 'test/num_examples': 10000, 'score': 28595.185331583023, 'total_duration': 29618.681260347366, 'accumulated_submission_time': 28595.185331583023, 'accumulated_eval_time': 1018.3473055362701, 'accumulated_logging_time': 2.0879697799682617, 'global_step': 84148, 'preemption_count': 0}), (85654, {'train/accuracy': 0.7042809128761292, 'train/loss': 1.1611301898956299, 'validation/accuracy': 0.6452400088310242, 'validation/loss': 1.4567824602127075, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.153033971786499, 'test/num_examples': 10000, 'score': 29105.374361276627, 'total_duration': 30147.205749988556, 'accumulated_submission_time': 29105.374361276627, 'accumulated_eval_time': 1036.582043170929, 'accumulated_logging_time': 2.1334755420684814, 'global_step': 85654, 'preemption_count': 0}), (87160, {'train/accuracy': 0.7081273794174194, 'train/loss': 1.1381235122680664, 'validation/accuracy': 0.6510800123214722, 'validation/loss': 1.4280773401260376, 'validation/num_examples': 50000, 'test/accuracy': 0.5284000039100647, 'test/loss': 2.1416590213775635, 'test/num_examples': 10000, 'score': 29615.305659532547, 'total_duration': 30675.376630306244, 'accumulated_submission_time': 29615.305659532547, 'accumulated_eval_time': 1054.7166481018066, 'accumulated_logging_time': 2.183864116668701, 'global_step': 87160, 'preemption_count': 0}), (88662, {'train/accuracy': 0.706074595451355, 'train/loss': 1.1489758491516113, 'validation/accuracy': 0.6509799957275391, 'validation/loss': 1.4250391721725464, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.128875970840454, 'test/num_examples': 10000, 'score': 30124.241693258286, 'total_duration': 31203.1488199234, 'accumulated_submission_time': 30124.241693258286, 'accumulated_eval_time': 1072.3675389289856, 'accumulated_logging_time': 3.314368963241577, 'global_step': 88662, 'preemption_count': 0}), (90168, {'train/accuracy': 0.7113161683082581, 'train/loss': 1.1303361654281616, 'validation/accuracy': 0.6467599868774414, 'validation/loss': 1.444667100906372, 'validation/num_examples': 50000, 'test/accuracy': 0.5240000486373901, 'test/loss': 2.1700925827026367, 'test/num_examples': 10000, 'score': 30634.328468084335, 'total_duration': 31731.047548294067, 'accumulated_submission_time': 30634.328468084335, 'accumulated_eval_time': 1090.077528476715, 'accumulated_logging_time': 3.361715793609619, 'global_step': 90168, 'preemption_count': 0}), (91673, {'train/accuracy': 0.7281768321990967, 'train/loss': 1.0502692461013794, 'validation/accuracy': 0.6510199904441833, 'validation/loss': 1.4127343893051147, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.0981850624084473, 'test/num_examples': 10000, 'score': 31144.406147003174, 'total_duration': 32258.84111380577, 'accumulated_submission_time': 31144.406147003174, 'accumulated_eval_time': 1107.6928596496582, 'accumulated_logging_time': 3.4071123600006104, 'global_step': 91673, 'preemption_count': 0}), (93179, {'train/accuracy': 0.7147042155265808, 'train/loss': 1.1151316165924072, 'validation/accuracy': 0.6510599851608276, 'validation/loss': 1.43448007106781, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1194303035736084, 'test/num_examples': 10000, 'score': 31654.537391901016, 'total_duration': 32786.896673202515, 'accumulated_submission_time': 31654.537391901016, 'accumulated_eval_time': 1125.5177392959595, 'accumulated_logging_time': 3.4525933265686035, 'global_step': 93179, 'preemption_count': 0}), (94684, {'train/accuracy': 0.7174146771430969, 'train/loss': 1.0910450220108032, 'validation/accuracy': 0.6616799831390381, 'validation/loss': 1.3794057369232178, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.0857291221618652, 'test/num_examples': 10000, 'score': 32164.470888614655, 'total_duration': 33314.628957271576, 'accumulated_submission_time': 32164.470888614655, 'accumulated_eval_time': 1143.2149093151093, 'accumulated_logging_time': 3.5000839233398438, 'global_step': 94684, 'preemption_count': 0}), (96190, {'train/accuracy': 0.7141262888908386, 'train/loss': 1.1053730249404907, 'validation/accuracy': 0.6563599705696106, 'validation/loss': 1.3915393352508545, 'validation/num_examples': 50000, 'test/accuracy': 0.530500054359436, 'test/loss': 2.102585792541504, 'test/num_examples': 10000, 'score': 32674.50990009308, 'total_duration': 33842.723615169525, 'accumulated_submission_time': 32674.50990009308, 'accumulated_eval_time': 1161.172973394394, 'accumulated_logging_time': 3.5434374809265137, 'global_step': 96190, 'preemption_count': 0}), (97696, {'train/accuracy': 0.7200254797935486, 'train/loss': 1.0727653503417969, 'validation/accuracy': 0.6601399779319763, 'validation/loss': 1.3852746486663818, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.091149091720581, 'test/num_examples': 10000, 'score': 33184.651337623596, 'total_duration': 34370.78517818451, 'accumulated_submission_time': 33184.651337623596, 'accumulated_eval_time': 1178.9914045333862, 'accumulated_logging_time': 3.5905425548553467, 'global_step': 97696, 'preemption_count': 0}), (99202, {'train/accuracy': 0.7228953838348389, 'train/loss': 1.0721486806869507, 'validation/accuracy': 0.6584399938583374, 'validation/loss': 1.3808057308197021, 'validation/num_examples': 50000, 'test/accuracy': 0.5306000113487244, 'test/loss': 2.0861809253692627, 'test/num_examples': 10000, 'score': 33694.69447398186, 'total_duration': 34898.85359764099, 'accumulated_submission_time': 33694.69447398186, 'accumulated_eval_time': 1196.9177355766296, 'accumulated_logging_time': 3.6347944736480713, 'global_step': 99202, 'preemption_count': 0}), (100707, {'train/accuracy': 0.7236925959587097, 'train/loss': 1.0682148933410645, 'validation/accuracy': 0.6428999900817871, 'validation/loss': 1.46091890335083, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.180105209350586, 'test/num_examples': 10000, 'score': 34204.65418744087, 'total_duration': 35426.71285367012, 'accumulated_submission_time': 34204.65418744087, 'accumulated_eval_time': 1214.7148866653442, 'accumulated_logging_time': 3.682264804840088, 'global_step': 100707, 'preemption_count': 0}), (102213, {'train/accuracy': 0.7354512214660645, 'train/loss': 1.0134378671646118, 'validation/accuracy': 0.6638399958610535, 'validation/loss': 1.3594108819961548, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.092923641204834, 'test/num_examples': 10000, 'score': 34714.8821105957, 'total_duration': 35954.948600530624, 'accumulated_submission_time': 34714.8821105957, 'accumulated_eval_time': 1232.6231932640076, 'accumulated_logging_time': 3.7265255451202393, 'global_step': 102213, 'preemption_count': 0}), (103718, {'train/accuracy': 0.735371470451355, 'train/loss': 1.017613410949707, 'validation/accuracy': 0.6706799864768982, 'validation/loss': 1.3379477262496948, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.0400118827819824, 'test/num_examples': 10000, 'score': 35225.05365109444, 'total_duration': 36482.834594249725, 'accumulated_submission_time': 35225.05365109444, 'accumulated_eval_time': 1250.2376444339752, 'accumulated_logging_time': 3.770775318145752, 'global_step': 103718, 'preemption_count': 0}), (105224, {'train/accuracy': 0.7281967401504517, 'train/loss': 1.0495648384094238, 'validation/accuracy': 0.6665599942207336, 'validation/loss': 1.3577643632888794, 'validation/num_examples': 50000, 'test/accuracy': 0.5378000140190125, 'test/loss': 2.0792884826660156, 'test/num_examples': 10000, 'score': 35735.07813715935, 'total_duration': 37010.63081288338, 'accumulated_submission_time': 35735.07813715935, 'accumulated_eval_time': 1267.9070928096771, 'accumulated_logging_time': 3.817821502685547, 'global_step': 105224, 'preemption_count': 0}), (106730, {'train/accuracy': 0.7329201102256775, 'train/loss': 1.0269001722335815, 'validation/accuracy': 0.6684399843215942, 'validation/loss': 1.3400194644927979, 'validation/num_examples': 50000, 'test/accuracy': 0.5428000092506409, 'test/loss': 2.031836748123169, 'test/num_examples': 10000, 'score': 36245.17560458183, 'total_duration': 37538.59745979309, 'accumulated_submission_time': 36245.17560458183, 'accumulated_eval_time': 1285.6735136508942, 'accumulated_logging_time': 3.8665313720703125, 'global_step': 106730, 'preemption_count': 0}), (108236, {'train/accuracy': 0.7391581535339355, 'train/loss': 1.0007308721542358, 'validation/accuracy': 0.6759799718856812, 'validation/loss': 1.3103091716766357, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.014052152633667, 'test/num_examples': 10000, 'score': 36755.24262714386, 'total_duration': 38066.58639526367, 'accumulated_submission_time': 36755.24262714386, 'accumulated_eval_time': 1303.4949452877045, 'accumulated_logging_time': 3.911306619644165, 'global_step': 108236, 'preemption_count': 0}), (109741, {'train/accuracy': 0.7607222199440002, 'train/loss': 0.9054629802703857, 'validation/accuracy': 0.6724399924278259, 'validation/loss': 1.3212249279022217, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.0401992797851562, 'test/num_examples': 10000, 'score': 37265.14801168442, 'total_duration': 38594.18107557297, 'accumulated_submission_time': 37265.14801168442, 'accumulated_eval_time': 1321.0843846797943, 'accumulated_logging_time': 3.95632004737854, 'global_step': 109741, 'preemption_count': 0}), (111246, {'train/accuracy': 0.7552016973495483, 'train/loss': 0.9197171330451965, 'validation/accuracy': 0.6800199747085571, 'validation/loss': 1.3009675741195679, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.99822998046875, 'test/num_examples': 10000, 'score': 37775.20650601387, 'total_duration': 39122.22860813141, 'accumulated_submission_time': 37775.20650601387, 'accumulated_eval_time': 1338.969208240509, 'accumulated_logging_time': 4.006459951400757, 'global_step': 111246, 'preemption_count': 0}), (112752, {'train/accuracy': 0.752949595451355, 'train/loss': 0.9380612969398499, 'validation/accuracy': 0.6771599650382996, 'validation/loss': 1.3039065599441528, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.019953727722168, 'test/num_examples': 10000, 'score': 38285.428384542465, 'total_duration': 39650.31968307495, 'accumulated_submission_time': 38285.428384542465, 'accumulated_eval_time': 1356.734162569046, 'accumulated_logging_time': 4.056137800216675, 'global_step': 112752, 'preemption_count': 0}), (114258, {'train/accuracy': 0.7528300285339355, 'train/loss': 0.9420802593231201, 'validation/accuracy': 0.6818199753761292, 'validation/loss': 1.2975364923477173, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 1.993594765663147, 'test/num_examples': 10000, 'score': 38795.5488409996, 'total_duration': 40178.211097717285, 'accumulated_submission_time': 38795.5488409996, 'accumulated_eval_time': 1374.3892624378204, 'accumulated_logging_time': 4.116119623184204, 'global_step': 114258, 'preemption_count': 0}), (115763, {'train/accuracy': 0.74418044090271, 'train/loss': 0.970153272151947, 'validation/accuracy': 0.678059995174408, 'validation/loss': 1.2999392747879028, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.0128350257873535, 'test/num_examples': 10000, 'score': 39305.58757019043, 'total_duration': 40706.71689796448, 'accumulated_submission_time': 39305.58757019043, 'accumulated_eval_time': 1392.7512967586517, 'accumulated_logging_time': 4.165997743606567, 'global_step': 115763, 'preemption_count': 0}), (117269, {'train/accuracy': 0.7614795565605164, 'train/loss': 0.9109672904014587, 'validation/accuracy': 0.6902799606323242, 'validation/loss': 1.2496405839920044, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 1.921729564666748, 'test/num_examples': 10000, 'score': 39815.647740364075, 'total_duration': 41234.668796777725, 'accumulated_submission_time': 39815.647740364075, 'accumulated_eval_time': 1410.537811756134, 'accumulated_logging_time': 4.214937686920166, 'global_step': 117269, 'preemption_count': 0}), (118775, {'train/accuracy': 0.7867307066917419, 'train/loss': 0.7906128764152527, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.260579228401184, 'validation/num_examples': 50000, 'test/accuracy': 0.5646000504493713, 'test/loss': 1.9624043703079224, 'test/num_examples': 10000, 'score': 40325.84529519081, 'total_duration': 41762.935829401016, 'accumulated_submission_time': 40325.84529519081, 'accumulated_eval_time': 1428.5066511631012, 'accumulated_logging_time': 4.262691259384155, 'global_step': 118775, 'preemption_count': 0}), (120281, {'train/accuracy': 0.77543044090271, 'train/loss': 0.8411076664924622, 'validation/accuracy': 0.6890400052070618, 'validation/loss': 1.254690408706665, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 1.9643940925598145, 'test/num_examples': 10000, 'score': 40836.02567625046, 'total_duration': 42291.07681274414, 'accumulated_submission_time': 40836.02567625046, 'accumulated_eval_time': 1446.361969947815, 'accumulated_logging_time': 4.311857223510742, 'global_step': 120281, 'preemption_count': 0}), (121786, {'train/accuracy': 0.7752710580825806, 'train/loss': 0.8376134634017944, 'validation/accuracy': 0.6941999793052673, 'validation/loss': 1.22850501537323, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9029667377471924, 'test/num_examples': 10000, 'score': 41345.97124314308, 'total_duration': 42818.748262643814, 'accumulated_submission_time': 41345.97124314308, 'accumulated_eval_time': 1463.9844024181366, 'accumulated_logging_time': 4.359987735748291, 'global_step': 121786, 'preemption_count': 0}), (123292, {'train/accuracy': 0.7697106003761292, 'train/loss': 0.8630506992340088, 'validation/accuracy': 0.6924200057983398, 'validation/loss': 1.2353243827819824, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.924597978591919, 'test/num_examples': 10000, 'score': 41856.063671827316, 'total_duration': 43346.80474662781, 'accumulated_submission_time': 41856.063671827316, 'accumulated_eval_time': 1481.8402979373932, 'accumulated_logging_time': 4.414201498031616, 'global_step': 123292, 'preemption_count': 0}), (124797, {'train/accuracy': 0.7671197056770325, 'train/loss': 0.862572968006134, 'validation/accuracy': 0.6951199769973755, 'validation/loss': 1.2315285205841064, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.9317371845245361, 'test/num_examples': 10000, 'score': 42366.00140285492, 'total_duration': 43875.40986657143, 'accumulated_submission_time': 42366.00140285492, 'accumulated_eval_time': 1500.4037234783173, 'accumulated_logging_time': 4.462406873703003, 'global_step': 124797, 'preemption_count': 0}), (126303, {'train/accuracy': 0.776387095451355, 'train/loss': 0.831611156463623, 'validation/accuracy': 0.7033199667930603, 'validation/loss': 1.20175302028656, 'validation/num_examples': 50000, 'test/accuracy': 0.579200029373169, 'test/loss': 1.8880500793457031, 'test/num_examples': 10000, 'score': 42876.07256484032, 'total_duration': 44403.05740857124, 'accumulated_submission_time': 42876.07256484032, 'accumulated_eval_time': 1517.8734288215637, 'accumulated_logging_time': 4.515023231506348, 'global_step': 126303, 'preemption_count': 0}), (127808, {'train/accuracy': 0.808035671710968, 'train/loss': 0.7111296057701111, 'validation/accuracy': 0.7016400098800659, 'validation/loss': 1.2024482488632202, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 1.917343020439148, 'test/num_examples': 10000, 'score': 43386.099005937576, 'total_duration': 44931.04763793945, 'accumulated_submission_time': 43386.099005937576, 'accumulated_eval_time': 1535.7307217121124, 'accumulated_logging_time': 4.566674709320068, 'global_step': 127808, 'preemption_count': 0}), (129314, {'train/accuracy': 0.7916733026504517, 'train/loss': 0.7742475271224976, 'validation/accuracy': 0.6988999843597412, 'validation/loss': 1.205346941947937, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 1.8980298042297363, 'test/num_examples': 10000, 'score': 43896.101038217545, 'total_duration': 45458.82877635956, 'accumulated_submission_time': 43896.101038217545, 'accumulated_eval_time': 1553.4010951519012, 'accumulated_logging_time': 4.621357679367065, 'global_step': 129314, 'preemption_count': 0}), (130819, {'train/accuracy': 0.7887834906578064, 'train/loss': 0.7834305763244629, 'validation/accuracy': 0.7028999924659729, 'validation/loss': 1.1931791305541992, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9065872430801392, 'test/num_examples': 10000, 'score': 44406.101815223694, 'total_duration': 45986.675889253616, 'accumulated_submission_time': 44406.101815223694, 'accumulated_eval_time': 1571.1410930156708, 'accumulated_logging_time': 4.6731743812561035, 'global_step': 130819, 'preemption_count': 0}), (132325, {'train/accuracy': 0.7939453125, 'train/loss': 0.7579774260520935, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.1673496961593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8415168523788452, 'test/num_examples': 10000, 'score': 44916.1741373539, 'total_duration': 46514.71562767029, 'accumulated_submission_time': 44916.1741373539, 'accumulated_eval_time': 1589.0011916160583, 'accumulated_logging_time': 4.726458549499512, 'global_step': 132325, 'preemption_count': 0}), (133831, {'train/accuracy': 0.7958984375, 'train/loss': 0.757809579372406, 'validation/accuracy': 0.708620011806488, 'validation/loss': 1.1658791303634644, 'validation/num_examples': 50000, 'test/accuracy': 0.5860000252723694, 'test/loss': 1.8372397422790527, 'test/num_examples': 10000, 'score': 45426.37725400925, 'total_duration': 47042.93878364563, 'accumulated_submission_time': 45426.37725400925, 'accumulated_eval_time': 1606.9133660793304, 'accumulated_logging_time': 4.778220176696777, 'global_step': 133831, 'preemption_count': 0}), (135336, {'train/accuracy': 0.7983697056770325, 'train/loss': 0.737368106842041, 'validation/accuracy': 0.7152799963951111, 'validation/loss': 1.1455098390579224, 'validation/num_examples': 50000, 'test/accuracy': 0.5838000178337097, 'test/loss': 1.8571194410324097, 'test/num_examples': 10000, 'score': 45936.42216873169, 'total_duration': 47570.64797115326, 'accumulated_submission_time': 45936.42216873169, 'accumulated_eval_time': 1624.4669604301453, 'accumulated_logging_time': 4.833124399185181, 'global_step': 135336, 'preemption_count': 0}), (136841, {'train/accuracy': 0.8374122977256775, 'train/loss': 0.5970585942268372, 'validation/accuracy': 0.7161399722099304, 'validation/loss': 1.1301541328430176, 'validation/num_examples': 50000, 'test/accuracy': 0.5967000126838684, 'test/loss': 1.7885128259658813, 'test/num_examples': 10000, 'score': 46446.49920320511, 'total_duration': 48098.353201150894, 'accumulated_submission_time': 46446.49920320511, 'accumulated_eval_time': 1641.9889187812805, 'accumulated_logging_time': 4.884382009506226, 'global_step': 136841, 'preemption_count': 0}), (138346, {'train/accuracy': 0.8237802982330322, 'train/loss': 0.637283980846405, 'validation/accuracy': 0.7233399748802185, 'validation/loss': 1.104615330696106, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.7960389852523804, 'test/num_examples': 10000, 'score': 46956.61994481087, 'total_duration': 48626.38387274742, 'accumulated_submission_time': 46956.61994481087, 'accumulated_eval_time': 1659.7750248908997, 'accumulated_logging_time': 4.95368218421936, 'global_step': 138346, 'preemption_count': 0}), (139851, {'train/accuracy': 0.8210897445678711, 'train/loss': 0.6470831036567688, 'validation/accuracy': 0.7231400012969971, 'validation/loss': 1.1141338348388672, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.7942116260528564, 'test/num_examples': 10000, 'score': 47466.56193733215, 'total_duration': 49154.288517951965, 'accumulated_submission_time': 47466.56193733215, 'accumulated_eval_time': 1677.6271243095398, 'accumulated_logging_time': 5.008821725845337, 'global_step': 139851, 'preemption_count': 0}), (141357, {'train/accuracy': 0.8201530575752258, 'train/loss': 0.6532760858535767, 'validation/accuracy': 0.7235199809074402, 'validation/loss': 1.1173137426376343, 'validation/num_examples': 50000, 'test/accuracy': 0.5990000367164612, 'test/loss': 1.7983546257019043, 'test/num_examples': 10000, 'score': 47976.69469380379, 'total_duration': 49682.10616827011, 'accumulated_submission_time': 47976.69469380379, 'accumulated_eval_time': 1695.204603433609, 'accumulated_logging_time': 5.06158185005188, 'global_step': 141357, 'preemption_count': 0}), (142862, {'train/accuracy': 0.8167450428009033, 'train/loss': 0.6668460965156555, 'validation/accuracy': 0.7207799553871155, 'validation/loss': 1.1217389106750488, 'validation/num_examples': 50000, 'test/accuracy': 0.5948000550270081, 'test/loss': 1.8159778118133545, 'test/num_examples': 10000, 'score': 48486.89080500603, 'total_duration': 50210.33151769638, 'accumulated_submission_time': 48486.89080500603, 'accumulated_eval_time': 1713.1225311756134, 'accumulated_logging_time': 5.118523836135864, 'global_step': 142862, 'preemption_count': 0}), (144368, {'train/accuracy': 0.8262914419174194, 'train/loss': 0.6314259767532349, 'validation/accuracy': 0.7257599830627441, 'validation/loss': 1.0909123420715332, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.7762413024902344, 'test/num_examples': 10000, 'score': 48996.922676324844, 'total_duration': 50738.048347473145, 'accumulated_submission_time': 48996.922676324844, 'accumulated_eval_time': 1730.7020156383514, 'accumulated_logging_time': 5.1694557666778564, 'global_step': 144368, 'preemption_count': 0}), (145874, {'train/accuracy': 0.8601721525192261, 'train/loss': 0.5021381378173828, 'validation/accuracy': 0.732479989528656, 'validation/loss': 1.0820106267929077, 'validation/num_examples': 50000, 'test/accuracy': 0.6047000288963318, 'test/loss': 1.778953194618225, 'test/num_examples': 10000, 'score': 49507.11916804314, 'total_duration': 51266.078447818756, 'accumulated_submission_time': 49507.11916804314, 'accumulated_eval_time': 1748.4203968048096, 'accumulated_logging_time': 5.229321718215942, 'global_step': 145874, 'preemption_count': 0}), (147380, {'train/accuracy': 0.8510442972183228, 'train/loss': 0.5359805822372437, 'validation/accuracy': 0.7305399775505066, 'validation/loss': 1.0758004188537598, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.7504609823226929, 'test/num_examples': 10000, 'score': 50017.27359175682, 'total_duration': 51793.96232128143, 'accumulated_submission_time': 50017.27359175682, 'accumulated_eval_time': 1766.0413398742676, 'accumulated_logging_time': 5.282959222793579, 'global_step': 147380, 'preemption_count': 0}), (148885, {'train/accuracy': 0.8487523794174194, 'train/loss': 0.5430769324302673, 'validation/accuracy': 0.7354199886322021, 'validation/loss': 1.058487057685852, 'validation/num_examples': 50000, 'test/accuracy': 0.6100000143051147, 'test/loss': 1.7439473867416382, 'test/num_examples': 10000, 'score': 50527.27390384674, 'total_duration': 52321.88641309738, 'accumulated_submission_time': 50527.27390384674, 'accumulated_eval_time': 1783.8505229949951, 'accumulated_logging_time': 5.341572046279907, 'global_step': 148885, 'preemption_count': 0}), (150390, {'train/accuracy': 0.8517817258834839, 'train/loss': 0.5283650159835815, 'validation/accuracy': 0.7335999608039856, 'validation/loss': 1.0614495277404785, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.7548463344573975, 'test/num_examples': 10000, 'score': 51037.273169994354, 'total_duration': 52849.83254790306, 'accumulated_submission_time': 51037.273169994354, 'accumulated_eval_time': 1801.683106660843, 'accumulated_logging_time': 5.4006664752960205, 'global_step': 150390, 'preemption_count': 0}), (151895, {'train/accuracy': 0.8530372977256775, 'train/loss': 0.5189365148544312, 'validation/accuracy': 0.7375400066375732, 'validation/loss': 1.058759093284607, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.7296031713485718, 'test/num_examples': 10000, 'score': 51547.484359025955, 'total_duration': 53377.73521447182, 'accumulated_submission_time': 51547.484359025955, 'accumulated_eval_time': 1819.2650346755981, 'accumulated_logging_time': 5.456265211105347, 'global_step': 151895, 'preemption_count': 0}), (153401, {'train/accuracy': 0.8583585619926453, 'train/loss': 0.5022905468940735, 'validation/accuracy': 0.7417399883270264, 'validation/loss': 1.0438984632492065, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.733088731765747, 'test/num_examples': 10000, 'score': 52057.91526436806, 'total_duration': 53906.03312087059, 'accumulated_submission_time': 52057.91526436806, 'accumulated_eval_time': 1837.022742509842, 'accumulated_logging_time': 5.511146545410156, 'global_step': 153401, 'preemption_count': 0}), (154906, {'train/accuracy': 0.8868981003761292, 'train/loss': 0.4032793939113617, 'validation/accuracy': 0.741159975528717, 'validation/loss': 1.0439447164535522, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.7265610694885254, 'test/num_examples': 10000, 'score': 52567.88165092468, 'total_duration': 54433.74296832085, 'accumulated_submission_time': 52567.88165092468, 'accumulated_eval_time': 1854.6535007953644, 'accumulated_logging_time': 5.569386959075928, 'global_step': 154906, 'preemption_count': 0}), (156412, {'train/accuracy': 0.8819355964660645, 'train/loss': 0.42019084095954895, 'validation/accuracy': 0.7441399693489075, 'validation/loss': 1.0274673700332642, 'validation/num_examples': 50000, 'test/accuracy': 0.6198000311851501, 'test/loss': 1.7015371322631836, 'test/num_examples': 10000, 'score': 53077.91947197914, 'total_duration': 54961.70411801338, 'accumulated_submission_time': 53077.91947197914, 'accumulated_eval_time': 1872.464199066162, 'accumulated_logging_time': 5.626893997192383, 'global_step': 156412, 'preemption_count': 0}), (157917, {'train/accuracy': 0.8812978267669678, 'train/loss': 0.41859835386276245, 'validation/accuracy': 0.7456600069999695, 'validation/loss': 1.027588129043579, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.7277756929397583, 'test/num_examples': 10000, 'score': 53587.85387945175, 'total_duration': 55489.55252146721, 'accumulated_submission_time': 53587.85387945175, 'accumulated_eval_time': 1890.2672145366669, 'accumulated_logging_time': 5.684285640716553, 'global_step': 157917, 'preemption_count': 0}), (159422, {'train/accuracy': 0.8851044178009033, 'train/loss': 0.404142826795578, 'validation/accuracy': 0.7461400032043457, 'validation/loss': 1.024307131767273, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.7184984683990479, 'test/num_examples': 10000, 'score': 54097.80532884598, 'total_duration': 56017.39664173126, 'accumulated_submission_time': 54097.80532884598, 'accumulated_eval_time': 1908.046015739441, 'accumulated_logging_time': 5.742774486541748, 'global_step': 159422, 'preemption_count': 0}), (160928, {'train/accuracy': 0.8844267725944519, 'train/loss': 0.4088110625743866, 'validation/accuracy': 0.7474600076675415, 'validation/loss': 1.0270127058029175, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.7309526205062866, 'test/num_examples': 10000, 'score': 54608.0150744915, 'total_duration': 56545.54099678993, 'accumulated_submission_time': 54608.0150744915, 'accumulated_eval_time': 1925.8649232387543, 'accumulated_logging_time': 5.801524877548218, 'global_step': 160928, 'preemption_count': 0}), (162434, {'train/accuracy': 0.8910036683082581, 'train/loss': 0.38116320967674255, 'validation/accuracy': 0.7514399886131287, 'validation/loss': 1.0032466650009155, 'validation/num_examples': 50000, 'test/accuracy': 0.6270000338554382, 'test/loss': 1.6860774755477905, 'test/num_examples': 10000, 'score': 55118.058817625046, 'total_duration': 57073.90779566765, 'accumulated_submission_time': 55118.058817625046, 'accumulated_eval_time': 1944.0792744159698, 'accumulated_logging_time': 5.855859756469727, 'global_step': 162434, 'preemption_count': 0}), (163939, {'train/accuracy': 0.90921950340271, 'train/loss': 0.3235359787940979, 'validation/accuracy': 0.7534599900245667, 'validation/loss': 0.9915898442268372, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.6824558973312378, 'test/num_examples': 10000, 'score': 55627.95647931099, 'total_duration': 57601.502833366394, 'accumulated_submission_time': 55627.95647931099, 'accumulated_eval_time': 1961.6636843681335, 'accumulated_logging_time': 5.913207292556763, 'global_step': 163939, 'preemption_count': 0}), (165445, {'train/accuracy': 0.9094586968421936, 'train/loss': 0.3204634189605713, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 0.9951203465461731, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.6766432523727417, 'test/num_examples': 10000, 'score': 56138.10664725304, 'total_duration': 58129.62777304649, 'accumulated_submission_time': 56138.10664725304, 'accumulated_eval_time': 1979.5256507396698, 'accumulated_logging_time': 5.970006227493286, 'global_step': 165445, 'preemption_count': 0}), (166950, {'train/accuracy': 0.9108538031578064, 'train/loss': 0.31472885608673096, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 0.9941707849502563, 'validation/num_examples': 50000, 'test/accuracy': 0.6342000365257263, 'test/loss': 1.6934216022491455, 'test/num_examples': 10000, 'score': 56648.064351558685, 'total_duration': 58657.44054579735, 'accumulated_submission_time': 56648.064351558685, 'accumulated_eval_time': 1997.2634472846985, 'accumulated_logging_time': 6.031470537185669, 'global_step': 166950, 'preemption_count': 0}), (168456, {'train/accuracy': 0.9100167155265808, 'train/loss': 0.3191568851470947, 'validation/accuracy': 0.7554000020027161, 'validation/loss': 0.9903133511543274, 'validation/num_examples': 50000, 'test/accuracy': 0.6330000162124634, 'test/loss': 1.6975325345993042, 'test/num_examples': 10000, 'score': 57158.12152385712, 'total_duration': 59185.312942266464, 'accumulated_submission_time': 57158.12152385712, 'accumulated_eval_time': 2014.9616417884827, 'accumulated_logging_time': 6.093728303909302, 'global_step': 168456, 'preemption_count': 0}), (169961, {'train/accuracy': 0.9138432741165161, 'train/loss': 0.3032202124595642, 'validation/accuracy': 0.7573599815368652, 'validation/loss': 0.9809739589691162, 'validation/num_examples': 50000, 'test/accuracy': 0.6330000162124634, 'test/loss': 1.6725350618362427, 'test/num_examples': 10000, 'score': 57668.05534219742, 'total_duration': 59713.05906128883, 'accumulated_submission_time': 57668.05534219742, 'accumulated_eval_time': 2032.6607563495636, 'accumulated_logging_time': 6.152337074279785, 'global_step': 169961, 'preemption_count': 0}), (171467, {'train/accuracy': 0.9151387214660645, 'train/loss': 0.29794761538505554, 'validation/accuracy': 0.7592200040817261, 'validation/loss': 0.9786510467529297, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.6685278415679932, 'test/num_examples': 10000, 'score': 58178.16114640236, 'total_duration': 60241.09210109711, 'accumulated_submission_time': 58178.16114640236, 'accumulated_eval_time': 2050.4763667583466, 'accumulated_logging_time': 6.209350109100342, 'global_step': 171467, 'preemption_count': 0}), (172972, {'train/accuracy': 0.92386794090271, 'train/loss': 0.27077406644821167, 'validation/accuracy': 0.759619951248169, 'validation/loss': 0.9723941683769226, 'validation/num_examples': 50000, 'test/accuracy': 0.6366000175476074, 'test/loss': 1.6629676818847656, 'test/num_examples': 10000, 'score': 58688.1078979969, 'total_duration': 60768.88009810448, 'accumulated_submission_time': 58688.1078979969, 'accumulated_eval_time': 2068.201717376709, 'accumulated_logging_time': 6.271347761154175, 'global_step': 172972, 'preemption_count': 0}), (174478, {'train/accuracy': 0.9291493892669678, 'train/loss': 0.2514609694480896, 'validation/accuracy': 0.7603799700737, 'validation/loss': 0.9728773236274719, 'validation/num_examples': 50000, 'test/accuracy': 0.6362000107765198, 'test/loss': 1.6596471071243286, 'test/num_examples': 10000, 'score': 59198.22804784775, 'total_duration': 61296.72523832321, 'accumulated_submission_time': 59198.22804784775, 'accumulated_eval_time': 2085.809932947159, 'accumulated_logging_time': 6.333725452423096, 'global_step': 174478, 'preemption_count': 0}), (175983, {'train/accuracy': 0.9298469424247742, 'train/loss': 0.2528488039970398, 'validation/accuracy': 0.7617799639701843, 'validation/loss': 0.9658573865890503, 'validation/num_examples': 50000, 'test/accuracy': 0.636900007724762, 'test/loss': 1.6625109910964966, 'test/num_examples': 10000, 'score': 59708.26288509369, 'total_duration': 61824.635633945465, 'accumulated_submission_time': 59708.26288509369, 'accumulated_eval_time': 2103.5739080905914, 'accumulated_logging_time': 6.391595363616943, 'global_step': 175983, 'preemption_count': 0}), (177488, {'train/accuracy': 0.928730845451355, 'train/loss': 0.25656646490097046, 'validation/accuracy': 0.7629799842834473, 'validation/loss': 0.9615415930747986, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.6531953811645508, 'test/num_examples': 10000, 'score': 60218.22137951851, 'total_duration': 62352.43422079086, 'accumulated_submission_time': 60218.22137951851, 'accumulated_eval_time': 2121.300199508667, 'accumulated_logging_time': 6.451011419296265, 'global_step': 177488, 'preemption_count': 0}), (178993, {'train/accuracy': 0.9295678734779358, 'train/loss': 0.25035443902015686, 'validation/accuracy': 0.7623199820518494, 'validation/loss': 0.9605156183242798, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.6532292366027832, 'test/num_examples': 10000, 'score': 60728.37961912155, 'total_duration': 62880.57772922516, 'accumulated_submission_time': 60728.37961912155, 'accumulated_eval_time': 2139.1607053279877, 'accumulated_logging_time': 6.521094560623169, 'global_step': 178993, 'preemption_count': 0}), (180498, {'train/accuracy': 0.9304248690605164, 'train/loss': 0.244537353515625, 'validation/accuracy': 0.7628200054168701, 'validation/loss': 0.9591808319091797, 'validation/num_examples': 50000, 'test/accuracy': 0.638200044631958, 'test/loss': 1.6477570533752441, 'test/num_examples': 10000, 'score': 61238.27491044998, 'total_duration': 63408.26725912094, 'accumulated_submission_time': 61238.27491044998, 'accumulated_eval_time': 2156.840073823929, 'accumulated_logging_time': 6.580701112747192, 'global_step': 180498, 'preemption_count': 0}), (182003, {'train/accuracy': 0.9319595098495483, 'train/loss': 0.24809786677360535, 'validation/accuracy': 0.7633199691772461, 'validation/loss': 0.9583838582038879, 'validation/num_examples': 50000, 'test/accuracy': 0.6396000385284424, 'test/loss': 1.6491045951843262, 'test/num_examples': 10000, 'score': 61748.39874982834, 'total_duration': 63936.03000330925, 'accumulated_submission_time': 61748.39874982834, 'accumulated_eval_time': 2174.361873626709, 'accumulated_logging_time': 6.64280366897583, 'global_step': 182003, 'preemption_count': 0}), (183508, {'train/accuracy': 0.93359375, 'train/loss': 0.2431531846523285, 'validation/accuracy': 0.7633199691772461, 'validation/loss': 0.9569819569587708, 'validation/num_examples': 50000, 'test/accuracy': 0.6368000507354736, 'test/loss': 1.650039553642273, 'test/num_examples': 10000, 'score': 62258.506739377975, 'total_duration': 64464.27554774284, 'accumulated_submission_time': 62258.506739377975, 'accumulated_eval_time': 2192.3842589855194, 'accumulated_logging_time': 6.70346999168396, 'global_step': 183508, 'preemption_count': 0}), (185013, {'train/accuracy': 0.9331752061843872, 'train/loss': 0.24071836471557617, 'validation/accuracy': 0.7636199593544006, 'validation/loss': 0.957119882106781, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.648453950881958, 'test/num_examples': 10000, 'score': 62768.50212907791, 'total_duration': 64992.15400886536, 'accumulated_submission_time': 62768.50212907791, 'accumulated_eval_time': 2210.148278236389, 'accumulated_logging_time': 6.768203496932983, 'global_step': 185013, 'preemption_count': 0})], 'global_step': 185721}
I0129 01:27:03.258905 140187804313408 submission_runner.py:586] Timing: 63008.252861738205
I0129 01:27:03.258967 140187804313408 submission_runner.py:588] Total number of evals: 124
I0129 01:27:03.259010 140187804313408 submission_runner.py:589] ====================
I0129 01:27:03.259054 140187804313408 submission_runner.py:542] Using RNG seed 3827130657
I0129 01:27:03.260491 140187804313408 submission_runner.py:551] --- Tuning run 4/5 ---
I0129 01:27:03.260590 140187804313408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_4.
I0129 01:27:03.261793 140187804313408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_4/hparams.json.
I0129 01:27:03.262503 140187804313408 submission_runner.py:206] Initializing dataset.
I0129 01:27:03.271837 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0129 01:27:03.281821 140187804313408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0129 01:27:04.047484 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0129 01:27:04.278815 140187804313408 submission_runner.py:213] Initializing model.
I0129 01:27:09.903424 140187804313408 submission_runner.py:255] Initializing optimizer.
I0129 01:27:10.301445 140187804313408 submission_runner.py:262] Initializing metrics bundle.
I0129 01:27:10.301598 140187804313408 submission_runner.py:280] Initializing checkpoint and logger.
I0129 01:27:10.317727 140187804313408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0129 01:27:10.317854 140187804313408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0129 01:27:21.268196 140187804313408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0129 01:27:32.040893 140187804313408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_4/flags_0.json.
I0129 01:27:32.045202 140187804313408 submission_runner.py:314] Starting training loop.
I0129 01:28:04.659294 140026058876672 logging_writer.py:48] [0] global_step=0, grad_norm=0.6558058857917786, loss=6.934148788452148
I0129 01:28:04.672091 140187804313408 spec.py:321] Evaluating on the training split.
I0129 01:28:10.926384 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 01:28:20.056245 140187804313408 spec.py:349] Evaluating on the test split.
I0129 01:28:22.701447 140187804313408 submission_runner.py:408] Time since start: 50.66s, 	Step: 1, 	{'train/accuracy': 0.0010961415246129036, 'train/loss': 6.912662982940674, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 32.62680268287659, 'total_duration': 50.656187772750854, 'accumulated_submission_time': 32.62680268287659, 'accumulated_eval_time': 18.02929162979126, 'accumulated_logging_time': 0}
I0129 01:28:22.711304 140026159523584 logging_writer.py:48] [1] accumulated_eval_time=18.029292, accumulated_logging_time=0, accumulated_submission_time=32.626803, global_step=1, preemption_count=0, score=32.626803, test/accuracy=0.000600, test/loss=6.912549, test/num_examples=10000, total_duration=50.656188, train/accuracy=0.001096, train/loss=6.912663, validation/accuracy=0.000760, validation/loss=6.913175, validation/num_examples=50000
I0129 01:28:56.693630 140026167916288 logging_writer.py:48] [100] global_step=100, grad_norm=0.7810716032981873, loss=6.672267913818359
I0129 01:29:30.739983 140026159523584 logging_writer.py:48] [200] global_step=200, grad_norm=1.0219444036483765, loss=6.297262191772461
I0129 01:30:04.802179 140026167916288 logging_writer.py:48] [300] global_step=300, grad_norm=6.208789348602295, loss=6.054475784301758
I0129 01:30:38.891477 140026159523584 logging_writer.py:48] [400] global_step=400, grad_norm=4.533118724822998, loss=5.679991722106934
I0129 01:31:12.992340 140026167916288 logging_writer.py:48] [500] global_step=500, grad_norm=5.16917610168457, loss=5.634751796722412
I0129 01:31:47.099940 140026159523584 logging_writer.py:48] [600] global_step=600, grad_norm=6.997704982757568, loss=5.3803510665893555
I0129 01:32:21.198943 140026167916288 logging_writer.py:48] [700] global_step=700, grad_norm=4.025707721710205, loss=5.267663955688477
I0129 01:32:55.301127 140026159523584 logging_writer.py:48] [800] global_step=800, grad_norm=3.036806106567383, loss=5.053461074829102
I0129 01:33:29.405746 140026167916288 logging_writer.py:48] [900] global_step=900, grad_norm=6.092737674713135, loss=4.837925434112549
I0129 01:34:03.515811 140026159523584 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.313483238220215, loss=4.71279239654541
I0129 01:34:37.673693 140026167916288 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.073484897613525, loss=4.53924560546875
I0129 01:35:11.757874 140026159523584 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.4220073223114014, loss=4.396842956542969
I0129 01:35:45.872630 140026167916288 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.9949018955230713, loss=4.337839126586914
I0129 01:36:19.978560 140026159523584 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.7875988483428955, loss=4.076362133026123
I0129 01:36:52.836369 140187804313408 spec.py:321] Evaluating on the training split.
I0129 01:36:59.118429 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 01:37:08.156012 140187804313408 spec.py:349] Evaluating on the test split.
I0129 01:37:10.955786 140187804313408 submission_runner.py:408] Time since start: 578.91s, 	Step: 1498, 	{'train/accuracy': 0.20161032676696777, 'train/loss': 4.02529239654541, 'validation/accuracy': 0.18140000104904175, 'validation/loss': 4.143918514251709, 'validation/num_examples': 50000, 'test/accuracy': 0.13580000400543213, 'test/loss': 4.661078929901123, 'test/num_examples': 10000, 'score': 542.689546585083, 'total_duration': 578.9105360507965, 'accumulated_submission_time': 542.689546585083, 'accumulated_eval_time': 36.148682594299316, 'accumulated_logging_time': 0.01851630210876465}
I0129 01:37:10.975352 140026151130880 logging_writer.py:48] [1498] accumulated_eval_time=36.148683, accumulated_logging_time=0.018516, accumulated_submission_time=542.689547, global_step=1498, preemption_count=0, score=542.689547, test/accuracy=0.135800, test/loss=4.661079, test/num_examples=10000, total_duration=578.910536, train/accuracy=0.201610, train/loss=4.025292, validation/accuracy=0.181400, validation/loss=4.143919, validation/num_examples=50000
I0129 01:37:11.999488 140026176308992 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.5834362506866455, loss=4.097170352935791
I0129 01:37:46.039196 140026151130880 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.536219596862793, loss=3.900606155395508
I0129 01:38:20.059109 140026176308992 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.032226324081421, loss=3.7790582180023193
I0129 01:38:54.095861 140026151130880 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.1933367252349854, loss=3.7735300064086914
I0129 01:39:28.169213 140026176308992 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.1986265182495117, loss=3.7950382232666016
I0129 01:40:02.241835 140026151130880 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.16794490814209, loss=3.723435163497925
I0129 01:40:36.317592 140026176308992 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.666123390197754, loss=3.518958568572998
I0129 01:41:10.449327 140026151130880 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.643664002418518, loss=3.5774548053741455
I0129 01:41:44.490162 140026176308992 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.4902218580245972, loss=3.454833507537842
I0129 01:42:18.537988 140026151130880 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.104901671409607, loss=3.4938786029815674
I0129 01:42:52.571610 140026176308992 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.016598701477051, loss=3.323223352432251
I0129 01:43:26.591311 140026151130880 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.1022348403930664, loss=3.283916473388672
I0129 01:44:00.661508 140026176308992 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.03507661819458, loss=3.2241625785827637
I0129 01:44:34.703481 140026151130880 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2950162887573242, loss=3.1968984603881836
I0129 01:45:08.744513 140026176308992 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.494966983795166, loss=3.1876187324523926
I0129 01:45:41.210104 140187804313408 spec.py:321] Evaluating on the training split.
I0129 01:45:47.493561 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 01:45:56.592273 140187804313408 spec.py:349] Evaluating on the test split.
I0129 01:45:59.267216 140187804313408 submission_runner.py:408] Time since start: 1107.22s, 	Step: 2997, 	{'train/accuracy': 0.3574816584587097, 'train/loss': 2.9869542121887207, 'validation/accuracy': 0.33052000403404236, 'validation/loss': 3.145972728729248, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 3.8006327152252197, 'test/num_examples': 10000, 'score': 1052.8628568649292, 'total_duration': 1107.221934556961, 'accumulated_submission_time': 1052.8628568649292, 'accumulated_eval_time': 54.20573401451111, 'accumulated_logging_time': 0.04628777503967285}
I0129 01:45:59.287010 140026075662080 logging_writer.py:48] [2997] accumulated_eval_time=54.205734, accumulated_logging_time=0.046288, accumulated_submission_time=1052.862857, global_step=2997, preemption_count=0, score=1052.862857, test/accuracy=0.249700, test/loss=3.800633, test/num_examples=10000, total_duration=1107.221935, train/accuracy=0.357482, train/loss=2.986954, validation/accuracy=0.330520, validation/loss=3.145973, validation/num_examples=50000
I0129 01:46:00.655408 140026159523584 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.3317197561264038, loss=3.0252866744995117
I0129 01:46:34.635093 140026075662080 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.148226022720337, loss=3.1465837955474854
I0129 01:47:08.615964 140026159523584 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2535004615783691, loss=3.1704585552215576
I0129 01:47:42.725219 140026075662080 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.2909877300262451, loss=3.0019729137420654
I0129 01:48:16.736391 140026159523584 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9998950362205505, loss=3.1714961528778076
I0129 01:48:50.744806 140026075662080 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.4223672151565552, loss=3.083204507827759
I0129 01:49:24.742165 140026159523584 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8520222902297974, loss=3.037830352783203
I0129 01:49:58.738431 140026075662080 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.014981746673584, loss=2.9314656257629395
I0129 01:50:32.771871 140026159523584 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9228157997131348, loss=2.9042932987213135
I0129 01:51:06.780272 140026075662080 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.3157800436019897, loss=2.869954824447632
I0129 01:51:40.784664 140026159523584 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8089478015899658, loss=3.0122122764587402
I0129 01:52:14.757133 140026075662080 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7885020971298218, loss=2.9168128967285156
I0129 01:52:48.730197 140026159523584 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9977813959121704, loss=2.9631292819976807
I0129 01:53:22.747698 140026075662080 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7248213291168213, loss=2.7879226207733154
I0129 01:53:56.750045 140026159523584 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8038738369941711, loss=2.805393934249878
I0129 01:54:29.286893 140187804313408 spec.py:321] Evaluating on the training split.
I0129 01:54:35.574868 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 01:54:44.618631 140187804313408 spec.py:349] Evaluating on the test split.
I0129 01:54:47.255632 140187804313408 submission_runner.py:408] Time since start: 1635.21s, 	Step: 4497, 	{'train/accuracy': 0.3845065236091614, 'train/loss': 2.834846258163452, 'validation/accuracy': 0.3623199760913849, 'validation/loss': 2.950068712234497, 'validation/num_examples': 50000, 'test/accuracy': 0.27980002760887146, 'test/loss': 3.6492860317230225, 'test/num_examples': 10000, 'score': 1562.7982881069183, 'total_duration': 1635.2103700637817, 'accumulated_submission_time': 1562.7982881069183, 'accumulated_eval_time': 72.17443251609802, 'accumulated_logging_time': 0.07693171501159668}
I0129 01:54:47.273904 140026067269376 logging_writer.py:48] [4497] accumulated_eval_time=72.174433, accumulated_logging_time=0.076932, accumulated_submission_time=1562.798288, global_step=4497, preemption_count=0, score=1562.798288, test/accuracy=0.279800, test/loss=3.649286, test/num_examples=10000, total_duration=1635.210370, train/accuracy=0.384507, train/loss=2.834846, validation/accuracy=0.362320, validation/loss=2.950069, validation/num_examples=50000
I0129 01:54:48.629297 140026151130880 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7877585887908936, loss=2.7325804233551025
I0129 01:55:22.520676 140026067269376 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.2071752548217773, loss=2.738173484802246
I0129 01:55:56.472387 140026151130880 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9939975738525391, loss=2.742182970046997
I0129 01:56:30.445813 140026067269376 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.885587751865387, loss=2.683933973312378
I0129 01:57:04.427487 140026151130880 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7930294871330261, loss=2.7896835803985596
I0129 01:57:38.404597 140026067269376 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7110402584075928, loss=2.647991895675659
I0129 01:58:12.382175 140026151130880 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.1151903867721558, loss=2.6948513984680176
I0129 01:58:46.341220 140026067269376 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0309208631515503, loss=2.7543396949768066
I0129 01:59:20.321095 140026151130880 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8340626358985901, loss=2.8780877590179443
I0129 01:59:54.285722 140026067269376 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8980827927589417, loss=2.6214981079101562
I0129 02:00:28.422371 140026151130880 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.1553659439086914, loss=2.6752662658691406
I0129 02:01:02.407654 140026067269376 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.89348304271698, loss=2.718754291534424
I0129 02:01:36.388843 140026151130880 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9010592699050903, loss=2.5813732147216797
I0129 02:02:10.360363 140026067269376 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8883833289146423, loss=2.7209043502807617
I0129 02:02:44.318020 140026151130880 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.0386847257614136, loss=2.699415445327759
I0129 02:03:17.396489 140187804313408 spec.py:321] Evaluating on the training split.
I0129 02:03:23.606517 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 02:03:32.314806 140187804313408 spec.py:349] Evaluating on the test split.
I0129 02:03:34.968086 140187804313408 submission_runner.py:408] Time since start: 2162.92s, 	Step: 5999, 	{'train/accuracy': 0.3507254421710968, 'train/loss': 3.0492608547210693, 'validation/accuracy': 0.30963999032974243, 'validation/loss': 3.407315969467163, 'validation/num_examples': 50000, 'test/accuracy': 0.23880000412464142, 'test/loss': 4.093076705932617, 'test/num_examples': 10000, 'score': 2072.8568086624146, 'total_duration': 2162.92281293869, 'accumulated_submission_time': 2072.8568086624146, 'accumulated_eval_time': 89.74597930908203, 'accumulated_logging_time': 0.10487532615661621}
I0129 02:03:34.988140 140026042091264 logging_writer.py:48] [5999] accumulated_eval_time=89.745979, accumulated_logging_time=0.104875, accumulated_submission_time=2072.856809, global_step=5999, preemption_count=0, score=2072.856809, test/accuracy=0.238800, test/loss=4.093077, test/num_examples=10000, total_duration=2162.922813, train/accuracy=0.350725, train/loss=3.049261, validation/accuracy=0.309640, validation/loss=3.407316, validation/num_examples=50000
I0129 02:03:35.671811 140026050483968 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8566074967384338, loss=2.6558239459991455
I0129 02:04:09.597750 140026042091264 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.0328885316848755, loss=2.79197359085083
I0129 02:04:43.546334 140026050483968 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.1114051342010498, loss=2.597198963165283
I0129 02:05:17.513067 140026042091264 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8957926034927368, loss=2.628283977508545
I0129 02:05:51.491916 140026050483968 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9290852546691895, loss=2.5813114643096924
I0129 02:06:25.419923 140026042091264 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8604776263237, loss=2.4763522148132324
I0129 02:06:59.503475 140026050483968 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8485208749771118, loss=2.7306227684020996
I0129 02:07:33.454893 140026042091264 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8928012847900391, loss=2.473708152770996
I0129 02:08:07.402827 140026050483968 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8653878569602966, loss=2.616149425506592
I0129 02:08:41.344802 140026042091264 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.1169052124023438, loss=2.5412652492523193
I0129 02:09:15.254481 140026050483968 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.0213156938552856, loss=2.4714157581329346
I0129 02:09:49.194832 140026042091264 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9949206709861755, loss=2.556502342224121
I0129 02:10:23.139097 140026050483968 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.0219148397445679, loss=2.5620715618133545
I0129 02:10:57.074072 140026042091264 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9257643222808838, loss=2.530094861984253
I0129 02:11:31.039880 140026050483968 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9879348874092102, loss=2.4789931774139404
I0129 02:12:04.994362 140026042091264 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9403707385063171, loss=2.50702166557312
I0129 02:12:05.002779 140187804313408 spec.py:321] Evaluating on the training split.
I0129 02:12:11.590986 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 02:12:20.487057 140187804313408 spec.py:349] Evaluating on the test split.
I0129 02:12:23.142133 140187804313408 submission_runner.py:408] Time since start: 2691.10s, 	Step: 7501, 	{'train/accuracy': 0.07164780050516129, 'train/loss': 5.990123271942139, 'validation/accuracy': 0.06967999786138535, 'validation/loss': 6.023623466491699, 'validation/num_examples': 50000, 'test/accuracy': 0.04700000211596489, 'test/loss': 6.509797096252441, 'test/num_examples': 10000, 'score': 2582.8044941425323, 'total_duration': 2691.0967836380005, 'accumulated_submission_time': 2582.8044941425323, 'accumulated_eval_time': 107.88517737388611, 'accumulated_logging_time': 0.13736391067504883}
I0129 02:12:23.164423 140026159523584 logging_writer.py:48] [7501] accumulated_eval_time=107.885177, accumulated_logging_time=0.137364, accumulated_submission_time=2582.804494, global_step=7501, preemption_count=0, score=2582.804494, test/accuracy=0.047000, test/loss=6.509797, test/num_examples=10000, total_duration=2691.096784, train/accuracy=0.071648, train/loss=5.990123, validation/accuracy=0.069680, validation/loss=6.023623, validation/num_examples=50000
I0129 02:12:57.012504 140026167916288 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.099115014076233, loss=2.5019688606262207
I0129 02:13:30.995989 140026159523584 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9435856938362122, loss=2.687319278717041
I0129 02:14:04.934728 140026167916288 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.0454363822937012, loss=2.5313942432403564
I0129 02:14:38.831796 140026159523584 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.1603631973266602, loss=2.702976942062378
I0129 02:15:12.753281 140026167916288 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9601969122886658, loss=2.5123157501220703
I0129 02:15:46.668105 140026159523584 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9489945769309998, loss=2.51033616065979
I0129 02:16:20.570473 140026167916288 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9429532289505005, loss=2.4903650283813477
I0129 02:16:54.499032 140026159523584 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0100548267364502, loss=2.507997751235962
I0129 02:17:28.412256 140026167916288 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.0318151712417603, loss=2.4810962677001953
I0129 02:18:02.351603 140026159523584 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9445649981498718, loss=2.4333324432373047
I0129 02:18:36.281839 140026167916288 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0615334510803223, loss=2.582144260406494
I0129 02:19:10.195821 140026159523584 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.066762924194336, loss=2.5666568279266357
I0129 02:19:44.153771 140026167916288 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9972907900810242, loss=2.5120370388031006
I0129 02:20:18.095079 140026159523584 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9774854779243469, loss=2.507160186767578
I0129 02:20:52.032015 140026167916288 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.027647614479065, loss=2.436366081237793
I0129 02:20:53.201277 140187804313408 spec.py:321] Evaluating on the training split.
I0129 02:20:59.448519 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 02:21:08.624796 140187804313408 spec.py:349] Evaluating on the test split.
I0129 02:21:11.706942 140187804313408 submission_runner.py:408] Time since start: 3219.66s, 	Step: 9005, 	{'train/accuracy': 0.4273158311843872, 'train/loss': 2.571629285812378, 'validation/accuracy': 0.39673998951911926, 'validation/loss': 2.782094717025757, 'validation/num_examples': 50000, 'test/accuracy': 0.29430001974105835, 'test/loss': 3.5717177391052246, 'test/num_examples': 10000, 'score': 3092.7759182453156, 'total_duration': 3219.6616904735565, 'accumulated_submission_time': 3092.7759182453156, 'accumulated_eval_time': 126.39080882072449, 'accumulated_logging_time': 0.16942358016967773}
I0129 02:21:11.723853 140026050483968 logging_writer.py:48] [9005] accumulated_eval_time=126.390809, accumulated_logging_time=0.169424, accumulated_submission_time=3092.775918, global_step=9005, preemption_count=0, score=3092.775918, test/accuracy=0.294300, test/loss=3.571718, test/num_examples=10000, total_duration=3219.661690, train/accuracy=0.427316, train/loss=2.571629, validation/accuracy=0.396740, validation/loss=2.782095, validation/num_examples=50000
I0129 02:21:44.258145 140026058876672 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9746526479721069, loss=2.424698829650879
I0129 02:22:18.131090 140026050483968 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.03989577293396, loss=2.5793824195861816
I0129 02:22:52.058762 140026058876672 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8945198655128479, loss=2.4392638206481934
I0129 02:23:26.030343 140026050483968 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0572649240493774, loss=2.4877233505249023
I0129 02:23:59.949608 140026058876672 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.0361509323120117, loss=2.591759204864502
I0129 02:24:33.894201 140026050483968 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.959909200668335, loss=2.417492151260376
I0129 02:25:07.766106 140026058876672 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.0208334922790527, loss=2.5740652084350586
I0129 02:25:41.722211 140026050483968 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.0143667459487915, loss=2.4825825691223145
I0129 02:26:15.706821 140026058876672 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.0213589668273926, loss=2.462028980255127
I0129 02:26:49.590031 140026050483968 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9485483765602112, loss=2.3719980716705322
I0129 02:27:23.503763 140026058876672 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9222192168235779, loss=2.6402273178100586
I0129 02:27:57.440909 140026050483968 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9690929055213928, loss=2.575319528579712
I0129 02:28:31.384515 140026058876672 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9207740426063538, loss=2.3652524948120117
I0129 02:29:05.283465 140026050483968 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.0128121376037598, loss=2.4644711017608643
I0129 02:29:39.207951 140026058876672 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.1199573278427124, loss=2.4736499786376953
I0129 02:29:41.739629 140187804313408 spec.py:321] Evaluating on the training split.
I0129 02:29:48.049010 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 02:29:56.750653 140187804313408 spec.py:349] Evaluating on the test split.
I0129 02:29:59.478831 140187804313408 submission_runner.py:408] Time since start: 3747.43s, 	Step: 10509, 	{'train/accuracy': 0.19814252853393555, 'train/loss': 4.607287406921387, 'validation/accuracy': 0.17875999212265015, 'validation/loss': 4.729046821594238, 'validation/num_examples': 50000, 'test/accuracy': 0.133200004696846, 'test/loss': 5.446406364440918, 'test/num_examples': 10000, 'score': 3602.7280580997467, 'total_duration': 3747.433574438095, 'accumulated_submission_time': 3602.7280580997467, 'accumulated_eval_time': 144.12997436523438, 'accumulated_logging_time': 0.1947331428527832}
I0129 02:29:59.498036 140026042091264 logging_writer.py:48] [10509] accumulated_eval_time=144.129974, accumulated_logging_time=0.194733, accumulated_submission_time=3602.728058, global_step=10509, preemption_count=0, score=3602.728058, test/accuracy=0.133200, test/loss=5.446406, test/num_examples=10000, total_duration=3747.433574, train/accuracy=0.198143, train/loss=4.607287, validation/accuracy=0.178760, validation/loss=4.729047, validation/num_examples=50000
I0129 02:30:30.605013 140026151130880 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.04740571975708, loss=2.3921279907226562
I0129 02:31:04.458136 140026042091264 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.159799337387085, loss=2.511369228363037
I0129 02:31:38.364110 140026151130880 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.0132638216018677, loss=2.486192226409912
I0129 02:32:12.283680 140026042091264 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.0737923383712769, loss=2.5502138137817383
I0129 02:32:46.238620 140026151130880 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.0645865201950073, loss=2.579108953475952
I0129 02:33:20.107173 140026042091264 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9957957863807678, loss=2.3873848915100098
I0129 02:33:53.970356 140026151130880 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9825701117515564, loss=2.3745625019073486
I0129 02:34:27.848935 140026042091264 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.1347897052764893, loss=2.41941237449646
I0129 02:35:01.748279 140026151130880 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0781779289245605, loss=2.309175491333008
I0129 02:35:35.615851 140026042091264 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.092098593711853, loss=2.414609909057617
I0129 02:36:09.473434 140026151130880 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.019413709640503, loss=2.2947633266448975
I0129 02:36:43.386990 140026042091264 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0071566104888916, loss=2.450376510620117
I0129 02:37:17.281999 140026151130880 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9649271965026855, loss=2.361084222793579
I0129 02:37:51.171705 140026042091264 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0210148096084595, loss=2.4537606239318848
I0129 02:38:25.093985 140026151130880 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.065958023071289, loss=2.5009007453918457
I0129 02:38:29.649356 140187804313408 spec.py:321] Evaluating on the training split.
I0129 02:38:35.896580 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 02:38:44.723795 140187804313408 spec.py:349] Evaluating on the test split.
I0129 02:38:47.384713 140187804313408 submission_runner.py:408] Time since start: 4275.34s, 	Step: 12015, 	{'train/accuracy': 0.06770168989896774, 'train/loss': 6.2029805183410645, 'validation/accuracy': 0.0630199983716011, 'validation/loss': 6.263773441314697, 'validation/num_examples': 50000, 'test/accuracy': 0.044200003147125244, 'test/loss': 6.679653644561768, 'test/num_examples': 10000, 'score': 4112.816953420639, 'total_duration': 4275.339448213577, 'accumulated_submission_time': 4112.816953420639, 'accumulated_eval_time': 161.86528515815735, 'accumulated_logging_time': 0.22316813468933105}
I0129 02:38:47.405726 140026058876672 logging_writer.py:48] [12015] accumulated_eval_time=161.865285, accumulated_logging_time=0.223168, accumulated_submission_time=4112.816953, global_step=12015, preemption_count=0, score=4112.816953, test/accuracy=0.044200, test/loss=6.679654, test/num_examples=10000, total_duration=4275.339448, train/accuracy=0.067702, train/loss=6.202981, validation/accuracy=0.063020, validation/loss=6.263773, validation/num_examples=50000
I0129 02:39:16.566550 140026067269376 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.0937594175338745, loss=2.421865940093994
I0129 02:39:50.449557 140026058876672 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.1406739950180054, loss=2.5501761436462402
I0129 02:40:24.344977 140026067269376 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.1027384996414185, loss=2.4750075340270996
I0129 02:40:58.252865 140026058876672 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.9461284279823303, loss=2.3102850914001465
I0129 02:41:32.162200 140026067269376 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9514883756637573, loss=2.372910499572754
I0129 02:42:06.066226 140026058876672 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.062317132949829, loss=2.3753833770751953
I0129 02:42:39.972015 140026067269376 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.9103019833564758, loss=2.3054089546203613
I0129 02:43:13.889942 140026058876672 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.2190113067626953, loss=2.464144229888916
I0129 02:43:47.769532 140026067269376 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9553207159042358, loss=2.4781832695007324
I0129 02:44:21.650975 140026058876672 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9125772714614868, loss=2.3634402751922607
I0129 02:44:55.497512 140026067269376 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9543274641036987, loss=2.445748805999756
I0129 02:45:29.410605 140026058876672 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.941275954246521, loss=2.404764413833618
I0129 02:46:03.370434 140026067269376 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.1091976165771484, loss=2.4344887733459473
I0129 02:46:37.263920 140026058876672 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0197640657424927, loss=2.4313971996307373
I0129 02:47:11.117887 140026067269376 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0606093406677246, loss=2.3569533824920654
I0129 02:47:17.386570 140187804313408 spec.py:321] Evaluating on the training split.
I0129 02:47:23.658502 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 02:47:32.452235 140187804313408 spec.py:349] Evaluating on the test split.
I0129 02:47:35.141204 140187804313408 submission_runner.py:408] Time since start: 4803.10s, 	Step: 13520, 	{'train/accuracy': 0.3900669515132904, 'train/loss': 2.8671154975891113, 'validation/accuracy': 0.3664799928665161, 'validation/loss': 3.0192630290985107, 'validation/num_examples': 50000, 'test/accuracy': 0.2696000039577484, 'test/loss': 3.811286449432373, 'test/num_examples': 10000, 'score': 4622.73382973671, 'total_duration': 4803.095787525177, 'accumulated_submission_time': 4622.73382973671, 'accumulated_eval_time': 179.61972188949585, 'accumulated_logging_time': 0.2544693946838379}
I0129 02:47:35.161154 140026151130880 logging_writer.py:48] [13520] accumulated_eval_time=179.619722, accumulated_logging_time=0.254469, accumulated_submission_time=4622.733830, global_step=13520, preemption_count=0, score=4622.733830, test/accuracy=0.269600, test/loss=3.811286, test/num_examples=10000, total_duration=4803.095788, train/accuracy=0.390067, train/loss=2.867115, validation/accuracy=0.366480, validation/loss=3.019263, validation/num_examples=50000
I0129 02:48:02.535579 140026159523584 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0038626194000244, loss=2.3029587268829346
I0129 02:48:36.416775 140026151130880 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0515902042388916, loss=2.4313204288482666
I0129 02:49:10.272495 140026159523584 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.101747751235962, loss=2.353182554244995
I0129 02:49:44.122690 140026151130880 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0851378440856934, loss=2.3192074298858643
I0129 02:50:17.993829 140026159523584 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.974652886390686, loss=2.377153158187866
I0129 02:50:51.893056 140026151130880 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.048719048500061, loss=2.379041910171509
I0129 02:51:25.754086 140026159523584 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.094079613685608, loss=2.4496703147888184
I0129 02:51:59.628621 140026151130880 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.1306407451629639, loss=2.3772683143615723
I0129 02:52:33.593729 140026159523584 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0712369680404663, loss=2.449697732925415
I0129 02:53:07.446459 140026151130880 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.0061893463134766, loss=2.511070489883423
I0129 02:53:41.306464 140026159523584 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.067337989807129, loss=2.335151433944702
I0129 02:54:15.214140 140026151130880 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0433359146118164, loss=2.345097064971924
I0129 02:54:49.098565 140026159523584 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.12033212184906, loss=2.291858196258545
I0129 02:55:22.950730 140026151130880 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.9712454080581665, loss=2.2694997787475586
I0129 02:55:56.790014 140026159523584 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.0379074811935425, loss=2.461300849914551
I0129 02:56:05.398921 140187804313408 spec.py:321] Evaluating on the training split.
I0129 02:56:12.559999 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 02:56:21.544417 140187804313408 spec.py:349] Evaluating on the test split.
I0129 02:56:24.203691 140187804313408 submission_runner.py:408] Time since start: 5332.16s, 	Step: 15027, 	{'train/accuracy': 0.21578045189380646, 'train/loss': 4.200702667236328, 'validation/accuracy': 0.19119998812675476, 'validation/loss': 4.4994025230407715, 'validation/num_examples': 50000, 'test/accuracy': 0.14730000495910645, 'test/loss': 5.18111515045166, 'test/num_examples': 10000, 'score': 5132.905265569687, 'total_duration': 5332.158295869827, 'accumulated_submission_time': 5132.905265569687, 'accumulated_eval_time': 198.42433190345764, 'accumulated_logging_time': 0.2853665351867676}
I0129 02:56:24.224033 140026058876672 logging_writer.py:48] [15027] accumulated_eval_time=198.424332, accumulated_logging_time=0.285367, accumulated_submission_time=5132.905266, global_step=15027, preemption_count=0, score=5132.905266, test/accuracy=0.147300, test/loss=5.181115, test/num_examples=10000, total_duration=5332.158296, train/accuracy=0.215780, train/loss=4.200703, validation/accuracy=0.191200, validation/loss=4.499403, validation/num_examples=50000
I0129 02:56:49.236287 140026067269376 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9873828887939453, loss=2.3725807666778564
I0129 02:57:23.059316 140026058876672 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0940052270889282, loss=2.3361802101135254
I0129 02:57:56.933471 140026067269376 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9785429239273071, loss=2.3857808113098145
I0129 02:58:30.811922 140026058876672 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9912421703338623, loss=2.2833290100097656
I0129 02:59:04.742775 140026067269376 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.0094783306121826, loss=2.413804054260254
I0129 02:59:38.611188 140026058876672 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9488799571990967, loss=2.3305962085723877
I0129 03:00:12.472022 140026067269376 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0850698947906494, loss=2.3060598373413086
I0129 03:00:46.358823 140026058876672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.8975415229797363, loss=2.3720998764038086
I0129 03:01:20.231508 140026067269376 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0207453966140747, loss=2.371154308319092
I0129 03:01:54.077346 140026058876672 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.1127238273620605, loss=2.3899643421173096
I0129 03:02:27.942545 140026067269376 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.0282189846038818, loss=2.415809154510498
I0129 03:03:01.802461 140026058876672 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.0483498573303223, loss=2.422342538833618
I0129 03:03:35.644847 140026067269376 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.122262954711914, loss=2.3429672718048096
I0129 03:04:09.516736 140026058876672 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.1079483032226562, loss=2.4328770637512207
I0129 03:04:43.413140 140026067269376 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.068138837814331, loss=2.3409583568573
I0129 03:04:54.387107 140187804313408 spec.py:321] Evaluating on the training split.
I0129 03:05:01.077948 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 03:05:10.186904 140187804313408 spec.py:349] Evaluating on the test split.
I0129 03:05:12.861237 140187804313408 submission_runner.py:408] Time since start: 5860.82s, 	Step: 16534, 	{'train/accuracy': 0.2916533648967743, 'train/loss': 3.804319143295288, 'validation/accuracy': 0.26729997992515564, 'validation/loss': 4.0446624755859375, 'validation/num_examples': 50000, 'test/accuracy': 0.19860000908374786, 'test/loss': 4.769838333129883, 'test/num_examples': 10000, 'score': 5643.002831459045, 'total_duration': 5860.815819740295, 'accumulated_submission_time': 5643.002831459045, 'accumulated_eval_time': 216.898282289505, 'accumulated_logging_time': 0.3170950412750244}
I0129 03:05:12.881187 140026042091264 logging_writer.py:48] [16534] accumulated_eval_time=216.898282, accumulated_logging_time=0.317095, accumulated_submission_time=5643.002831, global_step=16534, preemption_count=0, score=5643.002831, test/accuracy=0.198600, test/loss=4.769838, test/num_examples=10000, total_duration=5860.815820, train/accuracy=0.291653, train/loss=3.804319, validation/accuracy=0.267300, validation/loss=4.044662, validation/num_examples=50000
I0129 03:05:35.494532 140026050483968 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.9449347257614136, loss=2.341092348098755
I0129 03:06:09.297741 140026042091264 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.991771399974823, loss=2.3046979904174805
I0129 03:06:43.110944 140026050483968 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.000175952911377, loss=2.337303638458252
I0129 03:07:16.965334 140026042091264 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.012587070465088, loss=2.2860803604125977
I0129 03:07:50.814088 140026050483968 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.0981817245483398, loss=2.460965871810913
I0129 03:08:24.649516 140026042091264 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.171717643737793, loss=2.392239570617676
I0129 03:08:58.522220 140026050483968 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0392639636993408, loss=2.2912991046905518
I0129 03:09:32.330242 140026042091264 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0986268520355225, loss=2.2669496536254883
I0129 03:10:06.201980 140026050483968 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.0093448162078857, loss=2.4771971702575684
I0129 03:10:40.014621 140026042091264 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0548619031906128, loss=2.2325937747955322
I0129 03:11:13.856382 140026050483968 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0074436664581299, loss=2.2383925914764404
I0129 03:11:47.735841 140026042091264 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.2162435054779053, loss=2.25612211227417
I0129 03:12:21.573875 140026050483968 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.9807531833648682, loss=2.2865185737609863
I0129 03:12:55.394743 140026042091264 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9901071786880493, loss=2.232412338256836
I0129 03:13:29.250717 140026050483968 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.025622844696045, loss=2.311683177947998
I0129 03:13:42.937268 140187804313408 spec.py:321] Evaluating on the training split.
I0129 03:13:49.241316 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 03:13:58.162742 140187804313408 spec.py:349] Evaluating on the test split.
I0129 03:14:00.871518 140187804313408 submission_runner.py:408] Time since start: 6388.83s, 	Step: 18042, 	{'train/accuracy': 0.29175302386283875, 'train/loss': 3.487156391143799, 'validation/accuracy': 0.2727600038051605, 'validation/loss': 3.6579201221466064, 'validation/num_examples': 50000, 'test/accuracy': 0.19460001587867737, 'test/loss': 4.378692626953125, 'test/num_examples': 10000, 'score': 6152.994265079498, 'total_duration': 6388.826258897781, 'accumulated_submission_time': 6152.994265079498, 'accumulated_eval_time': 234.83249616622925, 'accumulated_logging_time': 0.34627366065979004}
I0129 03:14:00.892947 140026159523584 logging_writer.py:48] [18042] accumulated_eval_time=234.832496, accumulated_logging_time=0.346274, accumulated_submission_time=6152.994265, global_step=18042, preemption_count=0, score=6152.994265, test/accuracy=0.194600, test/loss=4.378693, test/num_examples=10000, total_duration=6388.826259, train/accuracy=0.291753, train/loss=3.487156, validation/accuracy=0.272760, validation/loss=3.657920, validation/num_examples=50000
I0129 03:14:20.846683 140026167916288 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0185725688934326, loss=2.469710350036621
I0129 03:14:54.655330 140026159523584 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9877211451530457, loss=2.3453845977783203
I0129 03:15:28.479739 140026167916288 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.0418397188186646, loss=2.3973875045776367
I0129 03:16:02.321836 140026159523584 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.190205693244934, loss=2.2929527759552
I0129 03:16:36.144683 140026167916288 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0643850564956665, loss=2.358088731765747
I0129 03:17:09.982892 140026159523584 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.058211088180542, loss=2.241438388824463
I0129 03:17:43.808894 140026167916288 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.1682161092758179, loss=2.542975902557373
I0129 03:18:17.722180 140026159523584 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0523765087127686, loss=2.2872257232666016
I0129 03:18:51.594735 140026167916288 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.0284336805343628, loss=2.431065797805786
I0129 03:19:25.448303 140026159523584 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.2964822053909302, loss=2.3688559532165527
I0129 03:19:59.283182 140026167916288 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0785936117172241, loss=2.401993751525879
I0129 03:20:33.128159 140026159523584 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.0760060548782349, loss=2.3561251163482666
I0129 03:21:06.961960 140026167916288 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.1083624362945557, loss=2.33366322517395
I0129 03:21:40.817931 140026159523584 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0522700548171997, loss=2.3237857818603516
I0129 03:22:14.651293 140026167916288 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.1389113664627075, loss=2.471221446990967
I0129 03:22:31.058382 140187804313408 spec.py:321] Evaluating on the training split.
I0129 03:22:37.283543 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 03:22:46.200682 140187804313408 spec.py:349] Evaluating on the test split.
I0129 03:22:48.844464 140187804313408 submission_runner.py:408] Time since start: 6916.80s, 	Step: 19550, 	{'train/accuracy': 0.2448979616165161, 'train/loss': 4.03620719909668, 'validation/accuracy': 0.2307799905538559, 'validation/loss': 4.1693949699401855, 'validation/num_examples': 50000, 'test/accuracy': 0.17520001530647278, 'test/loss': 4.855306148529053, 'test/num_examples': 10000, 'score': 6663.09573674202, 'total_duration': 6916.799047708511, 'accumulated_submission_time': 6663.09573674202, 'accumulated_eval_time': 252.61838150024414, 'accumulated_logging_time': 0.3780183792114258}
I0129 03:22:48.866232 140026067269376 logging_writer.py:48] [19550] accumulated_eval_time=252.618382, accumulated_logging_time=0.378018, accumulated_submission_time=6663.095737, global_step=19550, preemption_count=0, score=6663.095737, test/accuracy=0.175200, test/loss=4.855306, test/num_examples=10000, total_duration=6916.799048, train/accuracy=0.244898, train/loss=4.036207, validation/accuracy=0.230780, validation/loss=4.169395, validation/num_examples=50000
I0129 03:23:06.106260 140026075662080 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.0979704856872559, loss=2.4795167446136475
I0129 03:23:39.872412 140026067269376 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0774911642074585, loss=2.317493200302124
I0129 03:24:13.655621 140026075662080 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.2031553983688354, loss=2.263986110687256
I0129 03:24:47.572226 140026067269376 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9716452360153198, loss=2.3551063537597656
I0129 03:25:21.346343 140026075662080 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.0913939476013184, loss=2.19761061668396
I0129 03:25:55.216316 140026067269376 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9761987924575806, loss=2.3149125576019287
I0129 03:26:28.993631 140026075662080 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1353391408920288, loss=2.4374184608459473
I0129 03:27:02.839296 140026067269376 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.1180016994476318, loss=2.3264174461364746
I0129 03:27:36.620660 140026075662080 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.966658353805542, loss=2.260697603225708
I0129 03:28:10.455006 140026067269376 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.0646560192108154, loss=2.409987211227417
I0129 03:28:44.281475 140026075662080 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.1864136457443237, loss=2.351470947265625
I0129 03:29:18.141760 140026067269376 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.0487810373306274, loss=2.2540178298950195
I0129 03:29:51.926701 140026075662080 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.076658844947815, loss=2.3733949661254883
I0129 03:30:25.771474 140026067269376 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.0684877634048462, loss=2.1985318660736084
I0129 03:30:59.675381 140026075662080 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0192347764968872, loss=2.3743441104888916
I0129 03:31:19.092301 140187804313408 spec.py:321] Evaluating on the training split.
I0129 03:31:25.338791 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 03:31:34.369829 140187804313408 spec.py:349] Evaluating on the test split.
I0129 03:31:37.047769 140187804313408 submission_runner.py:408] Time since start: 7445.00s, 	Step: 21059, 	{'train/accuracy': 0.11348054558038712, 'train/loss': 5.853148460388184, 'validation/accuracy': 0.1053599938750267, 'validation/loss': 6.068696022033691, 'validation/num_examples': 50000, 'test/accuracy': 0.08340000361204147, 'test/loss': 6.58660888671875, 'test/num_examples': 10000, 'score': 7173.257848501205, 'total_duration': 7445.002496004105, 'accumulated_submission_time': 7173.257848501205, 'accumulated_eval_time': 270.57379508018494, 'accumulated_logging_time': 0.40931153297424316}
I0129 03:31:37.069983 140026050483968 logging_writer.py:48] [21059] accumulated_eval_time=270.573795, accumulated_logging_time=0.409312, accumulated_submission_time=7173.257849, global_step=21059, preemption_count=0, score=7173.257849, test/accuracy=0.083400, test/loss=6.586609, test/num_examples=10000, total_duration=7445.002496, train/accuracy=0.113481, train/loss=5.853148, validation/accuracy=0.105360, validation/loss=6.068696, validation/num_examples=50000
I0129 03:31:51.290797 140026058876672 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.9707028269767761, loss=2.2807435989379883
I0129 03:32:25.031455 140026050483968 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.042206883430481, loss=2.372675895690918
I0129 03:32:58.860170 140026058876672 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.107836127281189, loss=2.367236614227295
I0129 03:33:32.693580 140026050483968 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.1069989204406738, loss=2.3831543922424316
I0129 03:34:06.527487 140026058876672 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.168116807937622, loss=2.3967814445495605
I0129 03:34:40.340984 140026050483968 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.1159130334854126, loss=2.3339319229125977
I0129 03:35:14.172390 140026058876672 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0718212127685547, loss=2.2806029319763184
I0129 03:35:48.011010 140026050483968 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.052081823348999, loss=2.3001818656921387
I0129 03:36:21.831382 140026058876672 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.1123684644699097, loss=2.354929208755493
I0129 03:36:55.663571 140026050483968 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.1816279888153076, loss=2.3141229152679443
I0129 03:37:29.578521 140026058876672 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.110992670059204, loss=2.3659019470214844
I0129 03:38:03.420322 140026050483968 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.1755759716033936, loss=2.3510031700134277
I0129 03:38:37.283534 140026058876672 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9920234680175781, loss=2.239821195602417
I0129 03:39:11.111090 140026050483968 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.0718848705291748, loss=2.3263871669769287
I0129 03:39:44.953324 140026058876672 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.9719997644424438, loss=2.176194190979004
I0129 03:40:07.057978 140187804313408 spec.py:321] Evaluating on the training split.
I0129 03:40:13.267834 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 03:40:22.313187 140187804313408 spec.py:349] Evaluating on the test split.
I0129 03:40:24.970854 140187804313408 submission_runner.py:408] Time since start: 7972.93s, 	Step: 22567, 	{'train/accuracy': 0.20862562954425812, 'train/loss': 4.400444030761719, 'validation/accuracy': 0.19873999059200287, 'validation/loss': 4.497589111328125, 'validation/num_examples': 50000, 'test/accuracy': 0.14230000972747803, 'test/loss': 5.258711814880371, 'test/num_examples': 10000, 'score': 7683.180972576141, 'total_duration': 7972.925594329834, 'accumulated_submission_time': 7683.180972576141, 'accumulated_eval_time': 288.48663353919983, 'accumulated_logging_time': 0.44240808486938477}
I0129 03:40:24.993066 140026159523584 logging_writer.py:48] [22567] accumulated_eval_time=288.486634, accumulated_logging_time=0.442408, accumulated_submission_time=7683.180973, global_step=22567, preemption_count=0, score=7683.180973, test/accuracy=0.142300, test/loss=5.258712, test/num_examples=10000, total_duration=7972.925594, train/accuracy=0.208626, train/loss=4.400444, validation/accuracy=0.198740, validation/loss=4.497589, validation/num_examples=50000
I0129 03:40:36.467986 140026167916288 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0295543670654297, loss=2.267223358154297
I0129 03:41:10.214653 140026159523584 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.0274959802627563, loss=2.39581298828125
I0129 03:41:44.015194 140026167916288 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1650956869125366, loss=2.401273727416992
I0129 03:42:17.827559 140026159523584 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.1405773162841797, loss=2.242722511291504
I0129 03:42:51.660363 140026167916288 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0902079343795776, loss=2.357240915298462
I0129 03:43:25.501253 140026159523584 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0514163970947266, loss=2.2702622413635254
I0129 03:43:59.385057 140026167916288 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.3251230716705322, loss=2.431729793548584
I0129 03:44:33.187277 140026159523584 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.128138780593872, loss=2.253349542617798
I0129 03:45:07.005219 140026167916288 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.1125503778457642, loss=2.373610258102417
I0129 03:45:40.829348 140026159523584 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.112030267715454, loss=2.305985689163208
I0129 03:46:14.659430 140026167916288 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.0554505586624146, loss=2.39808988571167
I0129 03:46:48.483481 140026159523584 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.1174232959747314, loss=2.4098589420318604
I0129 03:47:22.308420 140026167916288 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.096801519393921, loss=2.3764805793762207
I0129 03:47:56.101752 140026159523584 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.059225082397461, loss=2.403663158416748
I0129 03:48:29.960345 140026167916288 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2600748538970947, loss=2.3632078170776367
I0129 03:48:55.116472 140187804313408 spec.py:321] Evaluating on the training split.
I0129 03:49:01.474661 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 03:49:10.236080 140187804313408 spec.py:349] Evaluating on the test split.
I0129 03:49:12.858986 140187804313408 submission_runner.py:408] Time since start: 8500.81s, 	Step: 24076, 	{'train/accuracy': 0.29976481199264526, 'train/loss': 3.4221742153167725, 'validation/accuracy': 0.2604199945926666, 'validation/loss': 3.7820355892181396, 'validation/num_examples': 50000, 'test/accuracy': 0.19260001182556152, 'test/loss': 4.4531474113464355, 'test/num_examples': 10000, 'score': 8193.239090681076, 'total_duration': 8500.813712358475, 'accumulated_submission_time': 8193.239090681076, 'accumulated_eval_time': 306.22909712791443, 'accumulated_logging_time': 0.473712682723999}
I0129 03:49:12.880799 140026058876672 logging_writer.py:48] [24076] accumulated_eval_time=306.229097, accumulated_logging_time=0.473713, accumulated_submission_time=8193.239091, global_step=24076, preemption_count=0, score=8193.239091, test/accuracy=0.192600, test/loss=4.453147, test/num_examples=10000, total_duration=8500.813712, train/accuracy=0.299765, train/loss=3.422174, validation/accuracy=0.260420, validation/loss=3.782036, validation/num_examples=50000
I0129 03:49:21.357428 140026067269376 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0789694786071777, loss=2.297842264175415
I0129 03:49:55.147298 140026058876672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9958640933036804, loss=2.2708005905151367
I0129 03:50:29.079555 140026067269376 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0042091608047485, loss=2.3628361225128174
I0129 03:51:02.897004 140026058876672 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0712015628814697, loss=2.361818790435791
I0129 03:51:36.713256 140026067269376 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.189136028289795, loss=2.392353057861328
I0129 03:52:10.562123 140026058876672 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1005512475967407, loss=2.316404104232788
I0129 03:52:44.374237 140026067269376 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.064451813697815, loss=2.211374521255493
I0129 03:53:18.222232 140026058876672 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9953815340995789, loss=2.271472454071045
I0129 03:53:52.037473 140026067269376 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.07794189453125, loss=2.337667465209961
I0129 03:54:25.880120 140026058876672 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.110585331916809, loss=2.342820882797241
I0129 03:54:59.697311 140026067269376 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0543197393417358, loss=2.3498342037200928
I0129 03:55:33.515421 140026058876672 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.1693040132522583, loss=2.4521336555480957
I0129 03:56:07.318891 140026067269376 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.1433587074279785, loss=2.4154388904571533
I0129 03:56:41.157459 140026058876672 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0392651557922363, loss=2.3399696350097656
I0129 03:57:15.024207 140026067269376 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1395037174224854, loss=2.3616585731506348
I0129 03:57:42.932919 140187804313408 spec.py:321] Evaluating on the training split.
I0129 03:57:49.176662 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 03:57:58.190710 140187804313408 spec.py:349] Evaluating on the test split.
I0129 03:58:00.747912 140187804313408 submission_runner.py:408] Time since start: 9028.70s, 	Step: 25584, 	{'train/accuracy': 0.08533960580825806, 'train/loss': 5.972334861755371, 'validation/accuracy': 0.08336000144481659, 'validation/loss': 5.932559967041016, 'validation/num_examples': 50000, 'test/accuracy': 0.05340000241994858, 'test/loss': 6.625743389129639, 'test/num_examples': 10000, 'score': 8703.223582744598, 'total_duration': 9028.702632427216, 'accumulated_submission_time': 8703.223582744598, 'accumulated_eval_time': 324.04403138160706, 'accumulated_logging_time': 0.506464958190918}
I0129 03:58:00.770163 140026167916288 logging_writer.py:48] [25584] accumulated_eval_time=324.044031, accumulated_logging_time=0.506465, accumulated_submission_time=8703.223583, global_step=25584, preemption_count=0, score=8703.223583, test/accuracy=0.053400, test/loss=6.625743, test/num_examples=10000, total_duration=9028.702632, train/accuracy=0.085340, train/loss=5.972335, validation/accuracy=0.083360, validation/loss=5.932560, validation/num_examples=50000
I0129 03:58:06.522471 140026176308992 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1993697881698608, loss=2.286853075027466
I0129 03:58:40.297166 140026167916288 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1061745882034302, loss=2.404465436935425
I0129 03:59:14.047013 140026176308992 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.071452021598816, loss=2.240358829498291
I0129 03:59:47.855033 140026167916288 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0652562379837036, loss=2.2167882919311523
I0129 04:00:21.675020 140026176308992 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.1243747472763062, loss=2.463366985321045
I0129 04:00:55.482147 140026167916288 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.1590754985809326, loss=2.2202939987182617
I0129 04:01:29.287088 140026176308992 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.1131712198257446, loss=2.3202009201049805
I0129 04:02:03.061021 140026167916288 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0815372467041016, loss=2.352457046508789
I0129 04:02:36.892011 140026176308992 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.0949832201004028, loss=2.232879400253296
I0129 04:03:10.702243 140026167916288 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.178335189819336, loss=2.39084529876709
I0129 04:03:44.579956 140026176308992 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.073263168334961, loss=2.3346080780029297
I0129 04:04:18.418059 140026167916288 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0997908115386963, loss=2.3099005222320557
I0129 04:04:52.215477 140026176308992 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.175644874572754, loss=2.324411153793335
I0129 04:05:26.049402 140026167916288 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1250344514846802, loss=2.1180644035339355
I0129 04:05:59.849900 140026176308992 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.021707534790039, loss=2.3917086124420166
I0129 04:06:30.778713 140187804313408 spec.py:321] Evaluating on the training split.
I0129 04:06:37.159108 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 04:06:46.139599 140187804313408 spec.py:349] Evaluating on the test split.
I0129 04:06:48.763930 140187804313408 submission_runner.py:408] Time since start: 9556.72s, 	Step: 27093, 	{'train/accuracy': 0.28204718232154846, 'train/loss': 3.6558451652526855, 'validation/accuracy': 0.26405999064445496, 'validation/loss': 3.7624459266662598, 'validation/num_examples': 50000, 'test/accuracy': 0.20600001513957977, 'test/loss': 4.422956943511963, 'test/num_examples': 10000, 'score': 9213.1674451828, 'total_duration': 9556.718665838242, 'accumulated_submission_time': 9213.1674451828, 'accumulated_eval_time': 342.0292069911957, 'accumulated_logging_time': 0.5385315418243408}
I0129 04:06:48.788514 140026075662080 logging_writer.py:48] [27093] accumulated_eval_time=342.029207, accumulated_logging_time=0.538532, accumulated_submission_time=9213.167445, global_step=27093, preemption_count=0, score=9213.167445, test/accuracy=0.206000, test/loss=4.422957, test/num_examples=10000, total_duration=9556.718666, train/accuracy=0.282047, train/loss=3.655845, validation/accuracy=0.264060, validation/loss=3.762446, validation/num_examples=50000
I0129 04:06:51.502662 140026151130880 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1163749694824219, loss=2.2692463397979736
I0129 04:07:25.237603 140026075662080 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.0626343488693237, loss=2.391131639480591
I0129 04:07:59.003173 140026151130880 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.0179779529571533, loss=2.3950674533843994
I0129 04:08:32.811923 140026075662080 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.153682827949524, loss=2.3466362953186035
I0129 04:09:06.654209 140026151130880 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1047170162200928, loss=2.307777166366577
I0129 04:09:40.548734 140026075662080 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.1092652082443237, loss=2.306030750274658
I0129 04:10:14.353410 140026151130880 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.1197564601898193, loss=2.2707388401031494
I0129 04:10:48.182828 140026075662080 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.201322078704834, loss=2.3985595703125
I0129 04:11:21.991258 140026151130880 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.1000272035598755, loss=2.408156394958496
I0129 04:11:55.819857 140026075662080 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.137909173965454, loss=2.337311267852783
I0129 04:12:29.632123 140026151130880 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.125845193862915, loss=2.342893123626709
I0129 04:13:03.453171 140026075662080 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.0571622848510742, loss=2.376746416091919
I0129 04:13:37.251884 140026151130880 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.027129888534546, loss=2.288054943084717
I0129 04:14:11.111871 140026075662080 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.1150072813034058, loss=2.275892734527588
I0129 04:14:44.887665 140026151130880 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.0979187488555908, loss=2.141207695007324
I0129 04:15:18.722618 140026075662080 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.0915542840957642, loss=2.3586666584014893
I0129 04:15:18.880502 140187804313408 spec.py:321] Evaluating on the training split.
I0129 04:15:25.119988 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 04:15:33.849570 140187804313408 spec.py:349] Evaluating on the test split.
I0129 04:15:36.568746 140187804313408 submission_runner.py:408] Time since start: 10084.52s, 	Step: 28602, 	{'train/accuracy': 0.21342872083187103, 'train/loss': 4.641663551330566, 'validation/accuracy': 0.19767999649047852, 'validation/loss': 4.830415725708008, 'validation/num_examples': 50000, 'test/accuracy': 0.15360000729560852, 'test/loss': 5.504821300506592, 'test/num_examples': 10000, 'score': 9723.193513393402, 'total_duration': 10084.52348947525, 'accumulated_submission_time': 9723.193513393402, 'accumulated_eval_time': 359.71742606163025, 'accumulated_logging_time': 0.5759317874908447}
I0129 04:15:36.595729 140026058876672 logging_writer.py:48] [28602] accumulated_eval_time=359.717426, accumulated_logging_time=0.575932, accumulated_submission_time=9723.193513, global_step=28602, preemption_count=0, score=9723.193513, test/accuracy=0.153600, test/loss=5.504821, test/num_examples=10000, total_duration=10084.523489, train/accuracy=0.213429, train/loss=4.641664, validation/accuracy=0.197680, validation/loss=4.830416, validation/num_examples=50000
I0129 04:16:10.014897 140026067269376 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.0949656963348389, loss=2.378225564956665
I0129 04:16:43.924731 140026058876672 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.0322656631469727, loss=2.2555925846099854
I0129 04:17:17.726030 140026067269376 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.098327875137329, loss=2.3379313945770264
I0129 04:17:51.556921 140026058876672 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9860699772834778, loss=2.235518455505371
I0129 04:18:25.341008 140026067269376 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9877071380615234, loss=2.3044979572296143
I0129 04:18:59.164722 140026058876672 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.0799720287322998, loss=2.2597715854644775
I0129 04:19:32.944869 140026067269376 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0548758506774902, loss=2.3814282417297363
I0129 04:20:06.731220 140026058876672 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.099795937538147, loss=2.3834147453308105
I0129 04:20:40.521789 140026067269376 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.195381999015808, loss=2.3796963691711426
I0129 04:21:14.307949 140026058876672 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.1641671657562256, loss=2.3731541633605957
I0129 04:21:48.094246 140026067269376 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0101840496063232, loss=2.1881322860717773
I0129 04:22:29.969691 140026058876672 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.198793649673462, loss=2.239119529724121
I0129 04:23:17.295556 140026067269376 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.1237989664077759, loss=2.3639650344848633
I0129 04:23:51.011799 140026058876672 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.1288083791732788, loss=2.4220166206359863
I0129 04:24:06.660171 140187804313408 spec.py:321] Evaluating on the training split.
I0129 04:24:12.994862 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 04:24:21.768109 140187804313408 spec.py:349] Evaluating on the test split.
I0129 04:24:24.423802 140187804313408 submission_runner.py:408] Time since start: 10612.38s, 	Step: 30048, 	{'train/accuracy': 0.15519371628761292, 'train/loss': 4.965692043304443, 'validation/accuracy': 0.15135999023914337, 'validation/loss': 5.025054454803467, 'validation/num_examples': 50000, 'test/accuracy': 0.11060000211000443, 'test/loss': 5.6790618896484375, 'test/num_examples': 10000, 'score': 10233.195832252502, 'total_duration': 10612.37854552269, 'accumulated_submission_time': 10233.195832252502, 'accumulated_eval_time': 377.4810211658478, 'accumulated_logging_time': 0.612412691116333}
I0129 04:24:24.449068 140026058876672 logging_writer.py:48] [30048] accumulated_eval_time=377.481021, accumulated_logging_time=0.612413, accumulated_submission_time=10233.195832, global_step=30048, preemption_count=0, score=10233.195832, test/accuracy=0.110600, test/loss=5.679062, test/num_examples=10000, total_duration=10612.378546, train/accuracy=0.155194, train/loss=4.965692, validation/accuracy=0.151360, validation/loss=5.025054, validation/num_examples=50000
I0129 04:24:42.286538 140026159523584 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.1544607877731323, loss=2.3563034534454346
I0129 04:25:15.994410 140026058876672 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.159103512763977, loss=2.4039244651794434
I0129 04:25:49.772101 140026159523584 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.000237226486206, loss=2.2938942909240723
I0129 04:26:23.563560 140026058876672 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.1327546834945679, loss=2.265472650527954
I0129 04:26:57.291093 140026159523584 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0246984958648682, loss=2.2876663208007812
I0129 04:27:31.080364 140026058876672 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.1280505657196045, loss=2.345963716506958
I0129 04:28:04.912197 140026159523584 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.12059485912323, loss=2.2883689403533936
I0129 04:28:38.736753 140026058876672 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.0712289810180664, loss=2.294255256652832
I0129 04:29:12.583248 140026159523584 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0236107110977173, loss=2.3865878582000732
I0129 04:29:46.452110 140026058876672 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.0709354877471924, loss=2.305204391479492
I0129 04:30:20.222864 140026159523584 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.4029585123062134, loss=2.3144567012786865
I0129 04:30:54.022343 140026058876672 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.267925500869751, loss=2.2794723510742188
I0129 04:31:27.752473 140026159523584 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0606346130371094, loss=2.281372308731079
I0129 04:32:01.573159 140026058876672 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.0359270572662354, loss=2.3528263568878174
I0129 04:32:35.413726 140026159523584 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.0408114194869995, loss=2.328549385070801
I0129 04:32:54.465248 140187804313408 spec.py:321] Evaluating on the training split.
I0129 04:33:00.707591 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 04:33:09.636962 140187804313408 spec.py:349] Evaluating on the test split.
I0129 04:33:12.971041 140187804313408 submission_runner.py:408] Time since start: 11140.93s, 	Step: 31558, 	{'train/accuracy': 0.2456154227256775, 'train/loss': 4.217416763305664, 'validation/accuracy': 0.23111999034881592, 'validation/loss': 4.373788833618164, 'validation/num_examples': 50000, 'test/accuracy': 0.1656000018119812, 'test/loss': 5.351226806640625, 'test/num_examples': 10000, 'score': 10743.147997140884, 'total_duration': 11140.925688028336, 'accumulated_submission_time': 10743.147997140884, 'accumulated_eval_time': 395.98668384552, 'accumulated_logging_time': 0.6475231647491455}
I0129 04:33:12.993237 140026050483968 logging_writer.py:48] [31558] accumulated_eval_time=395.986684, accumulated_logging_time=0.647523, accumulated_submission_time=10743.147997, global_step=31558, preemption_count=0, score=10743.147997, test/accuracy=0.165600, test/loss=5.351227, test/num_examples=10000, total_duration=11140.925688, train/accuracy=0.245615, train/loss=4.217417, validation/accuracy=0.231120, validation/loss=4.373789, validation/num_examples=50000
I0129 04:33:27.508302 140026067269376 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0555074214935303, loss=2.157215118408203
I0129 04:34:01.207053 140026050483968 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.123754858970642, loss=2.2673161029815674
I0129 04:34:34.967299 140026067269376 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.2897206544876099, loss=2.3099799156188965
I0129 04:35:08.755453 140026050483968 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.3032196760177612, loss=2.2732558250427246
I0129 04:35:42.489139 140026067269376 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.293938159942627, loss=2.2480878829956055
I0129 04:36:16.283823 140026050483968 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0316402912139893, loss=2.3294708728790283
I0129 04:36:49.987786 140026067269376 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.150240421295166, loss=2.3241705894470215
I0129 04:37:23.786355 140026050483968 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.158380150794983, loss=2.1833596229553223
I0129 04:37:57.623664 140026067269376 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.0439908504486084, loss=2.3581135272979736
I0129 04:38:31.420992 140026050483968 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0689560174942017, loss=2.2415082454681396
I0129 04:39:05.134664 140026067269376 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0938823223114014, loss=2.290358781814575
I0129 04:39:38.897675 140026050483968 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9715567827224731, loss=2.339080810546875
I0129 04:40:12.645184 140026067269376 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.178217887878418, loss=2.3004374504089355
I0129 04:40:46.430660 140026050483968 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.1932169198989868, loss=2.2558183670043945
I0129 04:41:20.149436 140026067269376 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0532747507095337, loss=2.258216381072998
I0129 04:41:43.309248 140187804313408 spec.py:321] Evaluating on the training split.
I0129 04:41:49.614946 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 04:41:58.630923 140187804313408 spec.py:349] Evaluating on the test split.
I0129 04:42:01.258248 140187804313408 submission_runner.py:408] Time since start: 11669.21s, 	Step: 33070, 	{'train/accuracy': 0.17233338952064514, 'train/loss': 4.832897663116455, 'validation/accuracy': 0.1630599945783615, 'validation/loss': 4.949711322784424, 'validation/num_examples': 50000, 'test/accuracy': 0.12480000406503677, 'test/loss': 5.6210150718688965, 'test/num_examples': 10000, 'score': 11253.401313781738, 'total_duration': 11669.212977647781, 'accumulated_submission_time': 11253.401313781738, 'accumulated_eval_time': 413.93563413619995, 'accumulated_logging_time': 0.678107738494873}
I0129 04:42:01.285214 140026050483968 logging_writer.py:48] [33070] accumulated_eval_time=413.935634, accumulated_logging_time=0.678108, accumulated_submission_time=11253.401314, global_step=33070, preemption_count=0, score=11253.401314, test/accuracy=0.124800, test/loss=5.621015, test/num_examples=10000, total_duration=11669.212978, train/accuracy=0.172333, train/loss=4.832898, validation/accuracy=0.163060, validation/loss=4.949711, validation/num_examples=50000
I0129 04:42:11.727178 140026058876672 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.1389716863632202, loss=2.376559019088745
I0129 04:42:45.571572 140026050483968 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.115522027015686, loss=2.2948994636535645
I0129 04:43:19.330015 140026058876672 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.0459620952606201, loss=2.2491796016693115
I0129 04:43:53.106462 140026050483968 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1600183248519897, loss=2.401548385620117
I0129 04:44:26.903936 140026058876672 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.137148380279541, loss=2.33290433883667
I0129 04:45:00.636702 140026050483968 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.172658085823059, loss=2.497760772705078
I0129 04:45:34.414491 140026058876672 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.2783015966415405, loss=2.1190242767333984
I0129 04:46:08.166375 140026050483968 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1212458610534668, loss=2.324122667312622
I0129 04:46:41.950688 140026058876672 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.0257045030593872, loss=2.1600966453552246
I0129 04:47:15.752839 140026050483968 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.037919044494629, loss=2.3263931274414062
I0129 04:47:49.555811 140026058876672 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.1366443634033203, loss=2.2776477336883545
I0129 04:48:23.320252 140026050483968 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.094191312789917, loss=2.1394450664520264
I0129 04:48:57.185583 140026058876672 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0747593641281128, loss=2.3645265102386475
I0129 04:49:30.990130 140026050483968 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.0972068309783936, loss=2.251687526702881
I0129 04:50:04.773543 140026058876672 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.0742160081863403, loss=2.177788734436035
I0129 04:50:31.312044 140187804313408 spec.py:321] Evaluating on the training split.
I0129 04:50:37.577376 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 04:50:46.503943 140187804313408 spec.py:349] Evaluating on the test split.
I0129 04:50:49.164860 140187804313408 submission_runner.py:408] Time since start: 12197.12s, 	Step: 34580, 	{'train/accuracy': 0.13695789873600006, 'train/loss': 5.655052661895752, 'validation/accuracy': 0.12545999884605408, 'validation/loss': 5.8631768226623535, 'validation/num_examples': 50000, 'test/accuracy': 0.09600000083446503, 'test/loss': 6.327773094177246, 'test/num_examples': 10000, 'score': 11763.362285137177, 'total_duration': 12197.119595527649, 'accumulated_submission_time': 11763.362285137177, 'accumulated_eval_time': 431.78840684890747, 'accumulated_logging_time': 0.7146234512329102}
I0129 04:50:49.189302 140026067269376 logging_writer.py:48] [34580] accumulated_eval_time=431.788407, accumulated_logging_time=0.714623, accumulated_submission_time=11763.362285, global_step=34580, preemption_count=0, score=11763.362285, test/accuracy=0.096000, test/loss=6.327773, test/num_examples=10000, total_duration=12197.119596, train/accuracy=0.136958, train/loss=5.655053, validation/accuracy=0.125460, validation/loss=5.863177, validation/num_examples=50000
I0129 04:50:56.294417 140026075662080 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.129440188407898, loss=2.2209410667419434
I0129 04:51:30.066031 140026067269376 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.0914138555526733, loss=2.2381560802459717
I0129 04:52:03.736213 140026075662080 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.3389760255813599, loss=2.4665491580963135
I0129 04:52:37.523355 140026067269376 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.1925557851791382, loss=2.313490867614746
I0129 04:53:11.296609 140026075662080 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.2664684057235718, loss=2.292656660079956
I0129 04:53:45.043214 140026067269376 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1004455089569092, loss=2.265066146850586
I0129 04:54:18.831954 140026075662080 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.0206323862075806, loss=2.276113986968994
I0129 04:54:52.565177 140026067269376 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.0592504739761353, loss=2.2628965377807617
I0129 04:55:26.455769 140026075662080 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.2223597764968872, loss=2.2262215614318848
I0129 04:56:00.233218 140026067269376 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.1191837787628174, loss=2.262202262878418
I0129 04:56:34.024447 140026075662080 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.2657097578048706, loss=2.4009461402893066
I0129 04:57:07.823211 140026067269376 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.136994481086731, loss=2.206782817840576
I0129 04:57:41.624689 140026075662080 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1393146514892578, loss=2.234377384185791
I0129 04:58:15.367716 140026067269376 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.3037699460983276, loss=2.1880686283111572
I0129 04:58:49.131983 140026075662080 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.13608980178833, loss=2.1993601322174072
I0129 04:59:19.341858 140187804313408 spec.py:321] Evaluating on the training split.
I0129 04:59:25.601201 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 04:59:34.574886 140187804313408 spec.py:349] Evaluating on the test split.
I0129 04:59:37.180805 140187804313408 submission_runner.py:408] Time since start: 12725.14s, 	Step: 36091, 	{'train/accuracy': 0.2906768023967743, 'train/loss': 3.589463233947754, 'validation/accuracy': 0.2702600061893463, 'validation/loss': 3.7571001052856445, 'validation/num_examples': 50000, 'test/accuracy': 0.19180001318454742, 'test/loss': 4.572559833526611, 'test/num_examples': 10000, 'score': 12273.451851844788, 'total_duration': 12725.135549068451, 'accumulated_submission_time': 12273.451851844788, 'accumulated_eval_time': 449.6273202896118, 'accumulated_logging_time': 0.7484467029571533}
I0129 04:59:37.204372 140026058876672 logging_writer.py:48] [36091] accumulated_eval_time=449.627320, accumulated_logging_time=0.748447, accumulated_submission_time=12273.451852, global_step=36091, preemption_count=0, score=12273.451852, test/accuracy=0.191800, test/loss=4.572560, test/num_examples=10000, total_duration=12725.135549, train/accuracy=0.290677, train/loss=3.589463, validation/accuracy=0.270260, validation/loss=3.757100, validation/num_examples=50000
I0129 04:59:40.581933 140026159523584 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.0979758501052856, loss=2.3038229942321777
I0129 05:00:14.362851 140026058876672 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.166865587234497, loss=2.2767996788024902
I0129 05:00:48.097526 140026159523584 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1803807020187378, loss=2.328369379043579
I0129 05:01:21.881700 140026058876672 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.1424888372421265, loss=2.3389649391174316
I0129 05:01:55.742405 140026159523584 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1535966396331787, loss=2.3271586894989014
I0129 05:02:29.526751 140026058876672 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.1808725595474243, loss=2.3769967555999756
I0129 05:03:03.281821 140026159523584 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0505043268203735, loss=2.1962671279907227
I0129 05:03:37.049921 140026058876672 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.074047565460205, loss=2.284654140472412
I0129 05:04:10.819369 140026159523584 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.0509635210037231, loss=2.2417471408843994
I0129 05:04:44.632547 140026058876672 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.1082018613815308, loss=2.3102822303771973
I0129 05:05:18.377620 140026159523584 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.0590028762817383, loss=2.309509754180908
I0129 05:05:52.167837 140026058876672 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1747851371765137, loss=2.305250406265259
I0129 05:06:25.964237 140026159523584 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.306471586227417, loss=2.2841503620147705
I0129 05:06:59.777868 140026058876672 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.2806901931762695, loss=2.2428743839263916
I0129 05:07:33.536863 140026159523584 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1813220977783203, loss=2.391686201095581
I0129 05:08:07.307776 140026058876672 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1973670721054077, loss=2.2435266971588135
I0129 05:08:07.315818 140187804313408 spec.py:321] Evaluating on the training split.
I0129 05:08:13.586271 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 05:08:22.284015 140187804313408 spec.py:349] Evaluating on the test split.
I0129 05:08:25.004096 140187804313408 submission_runner.py:408] Time since start: 13252.96s, 	Step: 37601, 	{'train/accuracy': 0.3878348171710968, 'train/loss': 2.8524887561798096, 'validation/accuracy': 0.36010000109672546, 'validation/loss': 3.0229721069335938, 'validation/num_examples': 50000, 'test/accuracy': 0.2678000032901764, 'test/loss': 3.7928645610809326, 'test/num_examples': 10000, 'score': 12783.497530221939, 'total_duration': 13252.958825826645, 'accumulated_submission_time': 12783.497530221939, 'accumulated_eval_time': 467.3155233860016, 'accumulated_logging_time': 0.7832720279693604}
I0129 05:08:25.032269 140026167916288 logging_writer.py:48] [37601] accumulated_eval_time=467.315523, accumulated_logging_time=0.783272, accumulated_submission_time=12783.497530, global_step=37601, preemption_count=0, score=12783.497530, test/accuracy=0.267800, test/loss=3.792865, test/num_examples=10000, total_duration=13252.958826, train/accuracy=0.387835, train/loss=2.852489, validation/accuracy=0.360100, validation/loss=3.022972, validation/num_examples=50000
I0129 05:08:58.745898 140026176308992 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.2119406461715698, loss=2.3685572147369385
I0129 05:09:32.481957 140026167916288 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1633702516555786, loss=2.2911314964294434
I0129 05:10:06.266595 140026176308992 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1171362400054932, loss=2.4215991497039795
I0129 05:10:40.070221 140026167916288 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.090688705444336, loss=2.34488582611084
I0129 05:11:13.825633 140026176308992 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1679216623306274, loss=2.3546202182769775
I0129 05:11:47.594923 140026167916288 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1187289953231812, loss=2.1682190895080566
I0129 05:12:21.373427 140026176308992 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.1655080318450928, loss=2.4191086292266846
I0129 05:12:55.099734 140026167916288 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.1091375350952148, loss=2.320568323135376
I0129 05:13:28.879538 140026176308992 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.291176676750183, loss=2.3836050033569336
I0129 05:14:02.700928 140026167916288 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.2825504541397095, loss=2.3145501613616943
I0129 05:14:36.477714 140026176308992 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.2251325845718384, loss=2.2257649898529053
I0129 05:15:10.372039 140026167916288 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.2708277702331543, loss=2.239506721496582
I0129 05:15:44.117262 140026176308992 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.1150280237197876, loss=2.228817939758301
I0129 05:16:17.882105 140026167916288 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.0793166160583496, loss=2.1642045974731445
I0129 05:16:51.670152 140026176308992 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.2231048345565796, loss=2.297126293182373
I0129 05:16:55.178253 140187804313408 spec.py:321] Evaluating on the training split.
I0129 05:17:01.417739 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 05:17:10.521686 140187804313408 spec.py:349] Evaluating on the test split.
I0129 05:17:13.207073 140187804313408 submission_runner.py:408] Time since start: 13781.16s, 	Step: 39112, 	{'train/accuracy': 0.06728316098451614, 'train/loss': 6.174365043640137, 'validation/accuracy': 0.06808000057935715, 'validation/loss': 6.197511196136475, 'validation/num_examples': 50000, 'test/accuracy': 0.046300001442432404, 'test/loss': 6.7392120361328125, 'test/num_examples': 10000, 'score': 13293.57846236229, 'total_duration': 13781.161801338196, 'accumulated_submission_time': 13293.57846236229, 'accumulated_eval_time': 485.3442895412445, 'accumulated_logging_time': 0.820784330368042}
I0129 05:17:13.237523 140026067269376 logging_writer.py:48] [39112] accumulated_eval_time=485.344290, accumulated_logging_time=0.820784, accumulated_submission_time=13293.578462, global_step=39112, preemption_count=0, score=13293.578462, test/accuracy=0.046300, test/loss=6.739212, test/num_examples=10000, total_duration=13781.161801, train/accuracy=0.067283, train/loss=6.174365, validation/accuracy=0.068080, validation/loss=6.197511, validation/num_examples=50000
I0129 05:17:43.303033 140026075662080 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.1749053001403809, loss=2.2183151245117188
I0129 05:18:17.012344 140026067269376 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.3255691528320312, loss=2.3112895488739014
I0129 05:18:50.733115 140026075662080 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.121207594871521, loss=2.311171770095825
I0129 05:19:24.539926 140026067269376 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.1036491394042969, loss=2.275068521499634
I0129 05:19:58.319186 140026075662080 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1920489072799683, loss=2.1742124557495117
I0129 05:20:32.147371 140026067269376 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.1412488222122192, loss=2.3156676292419434
I0129 05:21:05.921027 140026075662080 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.167362093925476, loss=2.2968130111694336
I0129 05:21:39.803419 140026067269376 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.1499433517456055, loss=2.2557449340820312
I0129 05:22:13.523309 140026075662080 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.2589938640594482, loss=2.3274428844451904
I0129 05:22:47.300774 140026067269376 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0664016008377075, loss=2.1916282176971436
I0129 05:23:21.043198 140026075662080 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.9993459582328796, loss=2.1719861030578613
I0129 05:23:54.788017 140026067269376 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.1510525941848755, loss=2.3363749980926514
I0129 05:24:28.548863 140026075662080 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.2263011932373047, loss=2.339372158050537
I0129 05:25:02.305517 140026067269376 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.1496968269348145, loss=2.239905595779419
I0129 05:25:36.085014 140026075662080 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.1726449728012085, loss=2.1460721492767334
I0129 05:25:43.313966 140187804313408 spec.py:321] Evaluating on the training split.
I0129 05:25:49.570800 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 05:25:58.542062 140187804313408 spec.py:349] Evaluating on the test split.
I0129 05:26:01.234926 140187804313408 submission_runner.py:408] Time since start: 14309.19s, 	Step: 40623, 	{'train/accuracy': 0.10668446868658066, 'train/loss': 6.06245756149292, 'validation/accuracy': 0.09827999770641327, 'validation/loss': 6.124608516693115, 'validation/num_examples': 50000, 'test/accuracy': 0.06650000065565109, 'test/loss': 6.820237159729004, 'test/num_examples': 10000, 'score': 13803.59059214592, 'total_duration': 14309.18965625763, 'accumulated_submission_time': 13803.59059214592, 'accumulated_eval_time': 503.2651972770691, 'accumulated_logging_time': 0.8609738349914551}
I0129 05:26:01.272167 140026058876672 logging_writer.py:48] [40623] accumulated_eval_time=503.265197, accumulated_logging_time=0.860974, accumulated_submission_time=13803.590592, global_step=40623, preemption_count=0, score=13803.590592, test/accuracy=0.066500, test/loss=6.820237, test/num_examples=10000, total_duration=14309.189656, train/accuracy=0.106684, train/loss=6.062458, validation/accuracy=0.098280, validation/loss=6.124609, validation/num_examples=50000
I0129 05:26:27.572699 140026167916288 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.2522114515304565, loss=2.263500213623047
I0129 05:27:01.339596 140026058876672 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.2290356159210205, loss=2.3971612453460693
I0129 05:27:35.067429 140026167916288 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.208338737487793, loss=2.2792930603027344
I0129 05:28:09.014992 140026058876672 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.12896728515625, loss=2.209017038345337
I0129 05:28:42.762232 140026167916288 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1688214540481567, loss=2.31603741645813
I0129 05:29:16.536983 140026058876672 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.2826464176177979, loss=2.203375816345215
I0129 05:29:50.255560 140026167916288 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.2500159740447998, loss=2.2684175968170166
I0129 05:30:24.029412 140026058876672 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.33992600440979, loss=2.3295845985412598
I0129 05:30:57.763523 140026167916288 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.1598328351974487, loss=2.2966034412384033
I0129 05:31:31.544742 140026058876672 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1946659088134766, loss=2.2274842262268066
I0129 05:32:05.304630 140026167916288 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1578208208084106, loss=2.185279369354248
I0129 05:32:39.042887 140026058876672 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.2320151329040527, loss=2.221644639968872
I0129 05:33:12.746970 140026167916288 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1018249988555908, loss=2.316663980484009
I0129 05:33:46.531023 140026058876672 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.1818174123764038, loss=2.178697109222412
I0129 05:34:20.380546 140026167916288 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.3535165786743164, loss=2.3179214000701904
I0129 05:34:31.296742 140187804313408 spec.py:321] Evaluating on the training split.
I0129 05:34:37.591612 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 05:34:46.406938 140187804313408 spec.py:349] Evaluating on the test split.
I0129 05:34:49.078855 140187804313408 submission_runner.py:408] Time since start: 14837.03s, 	Step: 42134, 	{'train/accuracy': 0.17036032676696777, 'train/loss': 4.943631649017334, 'validation/accuracy': 0.16232000291347504, 'validation/loss': 5.046461582183838, 'validation/num_examples': 50000, 'test/accuracy': 0.11220000684261322, 'test/loss': 5.937928199768066, 'test/num_examples': 10000, 'score': 14313.546487808228, 'total_duration': 14837.033590316772, 'accumulated_submission_time': 14313.546487808228, 'accumulated_eval_time': 521.0472767353058, 'accumulated_logging_time': 0.9110279083251953}
I0129 05:34:49.103576 140026058876672 logging_writer.py:48] [42134] accumulated_eval_time=521.047277, accumulated_logging_time=0.911028, accumulated_submission_time=14313.546488, global_step=42134, preemption_count=0, score=14313.546488, test/accuracy=0.112200, test/loss=5.937928, test/num_examples=10000, total_duration=14837.033590, train/accuracy=0.170360, train/loss=4.943632, validation/accuracy=0.162320, validation/loss=5.046462, validation/num_examples=50000
I0129 05:35:12.775391 140026067269376 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.2482812404632568, loss=2.3502869606018066
I0129 05:35:46.505342 140026058876672 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1402132511138916, loss=2.27530574798584
I0129 05:36:20.200873 140026067269376 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.2108526229858398, loss=2.2642323970794678
I0129 05:36:53.981704 140026058876672 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.1508557796478271, loss=2.237440347671509
I0129 05:37:27.793317 140026067269376 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.1188774108886719, loss=2.2133092880249023
I0129 05:38:01.561235 140026058876672 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.1595085859298706, loss=2.3145036697387695
I0129 05:38:35.294239 140026067269376 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2136286497116089, loss=2.226473331451416
I0129 05:39:09.063195 140026058876672 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.1788573265075684, loss=2.127570629119873
I0129 05:39:42.804742 140026067269376 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.22102689743042, loss=2.2209386825561523
I0129 05:40:16.586594 140026058876672 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.1001886129379272, loss=2.233776807785034
I0129 05:40:50.413169 140026067269376 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.110945463180542, loss=2.1822798252105713
I0129 05:41:24.117719 140026058876672 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.1750375032424927, loss=2.251298189163208
I0129 05:41:57.882891 140026067269376 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.2178155183792114, loss=2.347381830215454
I0129 05:42:31.637270 140026058876672 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.1082699298858643, loss=2.3454132080078125
I0129 05:43:05.396561 140026067269376 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.2228134870529175, loss=2.2491888999938965
I0129 05:43:19.365848 140187804313408 spec.py:321] Evaluating on the training split.
I0129 05:43:25.598902 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 05:43:34.601008 140187804313408 spec.py:349] Evaluating on the test split.
I0129 05:43:37.220626 140187804313408 submission_runner.py:408] Time since start: 15365.18s, 	Step: 43643, 	{'train/accuracy': 0.20894451439380646, 'train/loss': 4.4410600662231445, 'validation/accuracy': 0.1917800009250641, 'validation/loss': 4.693207263946533, 'validation/num_examples': 50000, 'test/accuracy': 0.15620000660419464, 'test/loss': 5.226414203643799, 'test/num_examples': 10000, 'score': 14822.672254800797, 'total_duration': 15365.175357818604, 'accumulated_submission_time': 14822.672254800797, 'accumulated_eval_time': 538.9020249843597, 'accumulated_logging_time': 2.0156309604644775}
I0129 05:43:37.247101 140026159523584 logging_writer.py:48] [43643] accumulated_eval_time=538.902025, accumulated_logging_time=2.015631, accumulated_submission_time=14822.672255, global_step=43643, preemption_count=0, score=14822.672255, test/accuracy=0.156200, test/loss=5.226414, test/num_examples=10000, total_duration=15365.175358, train/accuracy=0.208945, train/loss=4.441060, validation/accuracy=0.191780, validation/loss=4.693207, validation/num_examples=50000
I0129 05:43:56.827930 140026167916288 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1796290874481201, loss=2.2475781440734863
I0129 05:44:30.546313 140026159523584 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.210461139678955, loss=2.219482660293579
I0129 05:45:04.282068 140026167916288 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2438244819641113, loss=2.3568289279937744
I0129 05:45:38.035958 140026159523584 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.2014340162277222, loss=2.2963223457336426
I0129 05:46:11.718118 140026167916288 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.0778874158859253, loss=2.2419533729553223
I0129 05:46:45.443313 140026159523584 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.2149293422698975, loss=2.185065269470215
I0129 05:47:19.285792 140026167916288 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.0497339963912964, loss=2.2041144371032715
I0129 05:47:53.065947 140026159523584 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.313222050666809, loss=2.249311685562134
I0129 05:48:26.853368 140026167916288 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.224130392074585, loss=2.238168954849243
I0129 05:49:00.619751 140026159523584 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1687283515930176, loss=2.2023162841796875
I0129 05:49:34.382564 140026167916288 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.1925711631774902, loss=2.167689085006714
I0129 05:50:08.082053 140026159523584 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.1593741178512573, loss=2.157660484313965
I0129 05:50:41.835277 140026167916288 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.2819873094558716, loss=2.2950615882873535
I0129 05:51:15.578473 140026159523584 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.116070032119751, loss=2.199340581893921
I0129 05:51:49.368920 140026167916288 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.184451937675476, loss=2.398653984069824
I0129 05:52:07.380178 140187804313408 spec.py:321] Evaluating on the training split.
I0129 05:52:13.602502 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 05:52:22.263572 140187804313408 spec.py:349] Evaluating on the test split.
I0129 05:52:24.922155 140187804313408 submission_runner.py:408] Time since start: 15892.88s, 	Step: 45155, 	{'train/accuracy': 0.18227837979793549, 'train/loss': 4.9106268882751465, 'validation/accuracy': 0.17061999440193176, 'validation/loss': 5.011672019958496, 'validation/num_examples': 50000, 'test/accuracy': 0.1218000054359436, 'test/loss': 5.921328067779541, 'test/num_examples': 10000, 'score': 15332.741003751755, 'total_duration': 15892.876887321472, 'accumulated_submission_time': 15332.741003751755, 'accumulated_eval_time': 556.4439558982849, 'accumulated_logging_time': 2.052147626876831}
I0129 05:52:24.952668 140026067269376 logging_writer.py:48] [45155] accumulated_eval_time=556.443956, accumulated_logging_time=2.052148, accumulated_submission_time=15332.741004, global_step=45155, preemption_count=0, score=15332.741004, test/accuracy=0.121800, test/loss=5.921328, test/num_examples=10000, total_duration=15892.876887, train/accuracy=0.182278, train/loss=4.910627, validation/accuracy=0.170620, validation/loss=5.011672, validation/num_examples=50000
I0129 05:52:40.492958 140026075662080 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.2587109804153442, loss=2.295135498046875
I0129 05:53:14.207584 140026067269376 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.296358346939087, loss=2.2822470664978027
I0129 05:53:48.017770 140026075662080 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1897566318511963, loss=2.2391793727874756
I0129 05:54:21.834085 140026067269376 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.3013564348220825, loss=2.3353796005249023
I0129 05:54:55.602394 140026075662080 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.2007412910461426, loss=2.3196635246276855
I0129 05:55:29.319632 140026067269376 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.2417093515396118, loss=2.2095959186553955
I0129 05:56:03.087640 140026075662080 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.2107278108596802, loss=2.2670559883117676
I0129 05:56:36.794714 140026067269376 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.2891923189163208, loss=2.383629322052002
I0129 05:57:10.568772 140026075662080 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1564737558364868, loss=2.2002944946289062
I0129 05:57:44.281455 140026067269376 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2329626083374023, loss=2.1545238494873047
I0129 05:58:18.050209 140026075662080 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.1671169996261597, loss=2.2326862812042236
I0129 05:58:51.777086 140026067269376 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.2421966791152954, loss=2.1841001510620117
I0129 05:59:25.523363 140026075662080 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.1953468322753906, loss=2.114842414855957
I0129 05:59:59.258840 140026067269376 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1700581312179565, loss=2.192321538925171
I0129 06:00:33.072252 140026075662080 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1274319887161255, loss=2.1733014583587646
I0129 06:00:55.147660 140187804313408 spec.py:321] Evaluating on the training split.
I0129 06:01:01.449597 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 06:01:10.558323 140187804313408 spec.py:349] Evaluating on the test split.
I0129 06:01:13.194325 140187804313408 submission_runner.py:408] Time since start: 16421.15s, 	Step: 46667, 	{'train/accuracy': 0.0934709832072258, 'train/loss': 5.805931568145752, 'validation/accuracy': 0.08568000048398972, 'validation/loss': 5.914002418518066, 'validation/num_examples': 50000, 'test/accuracy': 0.06300000101327896, 'test/loss': 6.328012466430664, 'test/num_examples': 10000, 'score': 15842.871906518936, 'total_duration': 16421.14906358719, 'accumulated_submission_time': 15842.871906518936, 'accumulated_eval_time': 574.490583896637, 'accumulated_logging_time': 2.092453956604004}
I0129 06:01:13.224402 140026159523584 logging_writer.py:48] [46667] accumulated_eval_time=574.490584, accumulated_logging_time=2.092454, accumulated_submission_time=15842.871907, global_step=46667, preemption_count=0, score=15842.871907, test/accuracy=0.063000, test/loss=6.328012, test/num_examples=10000, total_duration=16421.149064, train/accuracy=0.093471, train/loss=5.805932, validation/accuracy=0.085680, validation/loss=5.914002, validation/num_examples=50000
I0129 06:01:24.708769 140026167916288 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.2053872346878052, loss=2.2309951782226562
I0129 06:01:58.447016 140026159523584 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.283191204071045, loss=2.178581714630127
I0129 06:02:32.141987 140026167916288 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.1749651432037354, loss=2.254239797592163
I0129 06:03:05.926162 140026159523584 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.2481073141098022, loss=2.2951035499572754
I0129 06:03:39.589788 140026167916288 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.3061366081237793, loss=2.2205307483673096
I0129 06:04:13.286221 140026159523584 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.1859524250030518, loss=2.3121836185455322
I0129 06:04:46.995977 140026167916288 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.203389048576355, loss=2.1841464042663574
I0129 06:05:20.791862 140026159523584 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1386555433273315, loss=2.122934579849243
I0129 06:05:54.501321 140026167916288 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.5352097749710083, loss=2.2676138877868652
I0129 06:06:28.284916 140026159523584 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.2464197874069214, loss=2.2042741775512695
I0129 06:07:02.104908 140026167916288 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2559020519256592, loss=2.1871554851531982
I0129 06:07:35.943921 140026159523584 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.2007166147232056, loss=2.2036561965942383
I0129 06:08:09.641026 140026167916288 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.3516263961791992, loss=2.234361410140991
I0129 06:08:43.355234 140026159523584 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.2145987749099731, loss=2.2655820846557617
I0129 06:09:17.089920 140026167916288 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.2355985641479492, loss=2.2430081367492676
I0129 06:09:43.230329 140187804313408 spec.py:321] Evaluating on the training split.
I0129 06:09:49.559933 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 06:09:58.341699 140187804313408 spec.py:349] Evaluating on the test split.
I0129 06:10:00.982109 140187804313408 submission_runner.py:408] Time since start: 16948.94s, 	Step: 48179, 	{'train/accuracy': 0.09851323068141937, 'train/loss': 5.712707996368408, 'validation/accuracy': 0.09359999746084213, 'validation/loss': 5.7977824211120605, 'validation/num_examples': 50000, 'test/accuracy': 0.07240000367164612, 'test/loss': 6.2724175453186035, 'test/num_examples': 10000, 'score': 16352.813577651978, 'total_duration': 16948.93683218956, 'accumulated_submission_time': 16352.813577651978, 'accumulated_eval_time': 592.2423067092896, 'accumulated_logging_time': 2.1319072246551514}
I0129 06:10:01.012230 140026067269376 logging_writer.py:48] [48179] accumulated_eval_time=592.242307, accumulated_logging_time=2.131907, accumulated_submission_time=16352.813578, global_step=48179, preemption_count=0, score=16352.813578, test/accuracy=0.072400, test/loss=6.272418, test/num_examples=10000, total_duration=16948.936832, train/accuracy=0.098513, train/loss=5.712708, validation/accuracy=0.093600, validation/loss=5.797782, validation/num_examples=50000
I0129 06:10:08.444432 140026075662080 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.254818320274353, loss=2.1662118434906006
I0129 06:10:42.141566 140026067269376 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.1899231672286987, loss=2.342137336730957
I0129 06:11:15.874165 140026075662080 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.219344973564148, loss=2.2193660736083984
I0129 06:11:49.631722 140026067269376 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2100269794464111, loss=2.2450296878814697
I0129 06:12:23.402424 140026075662080 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.155099630355835, loss=2.2759718894958496
I0129 06:12:57.170655 140026067269376 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.1470723152160645, loss=2.238398790359497
I0129 06:13:31.008361 140026075662080 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.2065297365188599, loss=2.263453483581543
I0129 06:14:04.773591 140026067269376 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.2288485765457153, loss=2.2378365993499756
I0129 06:14:38.500525 140026075662080 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.2654494047164917, loss=2.174642562866211
I0129 06:15:12.281642 140026067269376 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.2880542278289795, loss=2.317655563354492
I0129 06:15:46.007306 140026075662080 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.1585015058517456, loss=2.1959891319274902
I0129 06:16:19.763317 140026067269376 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.1547515392303467, loss=2.155378580093384
I0129 06:16:53.512952 140026075662080 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.2953966856002808, loss=2.1598660945892334
I0129 06:17:27.289054 140026067269376 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.3009358644485474, loss=2.1152493953704834
I0129 06:18:01.059412 140026075662080 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.2021453380584717, loss=2.228240966796875
I0129 06:18:31.255732 140187804313408 spec.py:321] Evaluating on the training split.
I0129 06:18:37.573251 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 06:18:46.325194 140187804313408 spec.py:349] Evaluating on the test split.
I0129 06:18:48.945981 140187804313408 submission_runner.py:408] Time since start: 17476.90s, 	Step: 49691, 	{'train/accuracy': 0.3025948703289032, 'train/loss': 3.468749523162842, 'validation/accuracy': 0.28519999980926514, 'validation/loss': 3.5776867866516113, 'validation/num_examples': 50000, 'test/accuracy': 0.20410001277923584, 'test/loss': 4.3102006912231445, 'test/num_examples': 10000, 'score': 16862.992620944977, 'total_duration': 17476.90072107315, 'accumulated_submission_time': 16862.992620944977, 'accumulated_eval_time': 609.932549238205, 'accumulated_logging_time': 2.1713497638702393}
I0129 06:18:48.975804 140026058876672 logging_writer.py:48] [49691] accumulated_eval_time=609.932549, accumulated_logging_time=2.171350, accumulated_submission_time=16862.992621, global_step=49691, preemption_count=0, score=16862.992621, test/accuracy=0.204100, test/loss=4.310201, test/num_examples=10000, total_duration=17476.900721, train/accuracy=0.302595, train/loss=3.468750, validation/accuracy=0.285200, validation/loss=3.577687, validation/num_examples=50000
I0129 06:18:52.357050 140026159523584 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.1723946332931519, loss=2.183253526687622
I0129 06:19:26.037195 140026058876672 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.191298246383667, loss=2.1995668411254883
I0129 06:19:59.883189 140026159523584 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.2265853881835938, loss=2.3400707244873047
I0129 06:20:33.587375 140026058876672 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.3730781078338623, loss=2.330530881881714
I0129 06:21:07.392009 140026159523584 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.3119571208953857, loss=2.292212724685669
I0129 06:21:41.077818 140026058876672 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.2365857362747192, loss=2.273054599761963
I0129 06:22:14.886809 140026159523584 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.116237759590149, loss=2.062757968902588
I0129 06:22:48.598938 140026058876672 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.1662375926971436, loss=2.137650966644287
I0129 06:23:22.390099 140026159523584 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.3132140636444092, loss=2.206357717514038
I0129 06:23:56.130338 140026058876672 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.2533992528915405, loss=2.2197813987731934
I0129 06:24:29.920206 140026159523584 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.1994585990905762, loss=2.2157320976257324
I0129 06:25:03.679649 140026058876672 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.1807701587677002, loss=2.35017466545105
I0129 06:25:37.425174 140026159523584 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.3225722312927246, loss=2.238680362701416
I0129 06:26:11.224817 140026058876672 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1979053020477295, loss=2.2322919368743896
I0129 06:26:44.934962 140026159523584 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.1187527179718018, loss=2.277071952819824
I0129 06:27:18.658686 140026058876672 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.6313201189041138, loss=2.207087278366089
I0129 06:27:19.152522 140187804313408 spec.py:321] Evaluating on the training split.
I0129 06:27:25.473861 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 06:27:34.382807 140187804313408 spec.py:349] Evaluating on the test split.
I0129 06:27:36.981680 140187804313408 submission_runner.py:408] Time since start: 18004.94s, 	Step: 51203, 	{'train/accuracy': 0.33033719658851624, 'train/loss': 3.299055576324463, 'validation/accuracy': 0.2969200015068054, 'validation/loss': 3.578896999359131, 'validation/num_examples': 50000, 'test/accuracy': 0.21730001270771027, 'test/loss': 4.417811393737793, 'test/num_examples': 10000, 'score': 17373.104477643967, 'total_duration': 18004.936414718628, 'accumulated_submission_time': 17373.104477643967, 'accumulated_eval_time': 627.7616715431213, 'accumulated_logging_time': 2.2113375663757324}
I0129 06:27:37.012232 140026058876672 logging_writer.py:48] [51203] accumulated_eval_time=627.761672, accumulated_logging_time=2.211338, accumulated_submission_time=17373.104478, global_step=51203, preemption_count=0, score=17373.104478, test/accuracy=0.217300, test/loss=4.417811, test/num_examples=10000, total_duration=18004.936415, train/accuracy=0.330337, train/loss=3.299056, validation/accuracy=0.296920, validation/loss=3.578897, validation/num_examples=50000
I0129 06:28:10.035187 140026067269376 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.228652000427246, loss=2.3201990127563477
I0129 06:28:43.703379 140026058876672 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.2837369441986084, loss=2.1357502937316895
I0129 06:29:17.403367 140026067269376 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.15670645236969, loss=2.187432050704956
I0129 06:29:51.144598 140026058876672 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.1261932849884033, loss=2.146458148956299
I0129 06:30:24.899651 140026067269376 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.112671136856079, loss=2.2429542541503906
I0129 06:30:58.685777 140026058876672 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.3350932598114014, loss=2.283877372741699
I0129 06:31:32.394120 140026067269376 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2796489000320435, loss=2.2832982540130615
I0129 06:32:06.174986 140026058876672 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.414842963218689, loss=2.202629804611206
I0129 06:32:40.025913 140026067269376 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.1674349308013916, loss=2.2304940223693848
I0129 06:33:13.784427 140026058876672 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.3175444602966309, loss=2.1479685306549072
I0129 06:33:47.527726 140026067269376 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.2811980247497559, loss=2.2146129608154297
I0129 06:34:21.282241 140026058876672 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.2428661584854126, loss=2.137326240539551
I0129 06:34:55.027384 140026067269376 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2112613916397095, loss=2.148937463760376
I0129 06:35:28.824946 140026058876672 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.17333984375, loss=2.2338552474975586
I0129 06:36:02.540082 140026067269376 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.1853827238082886, loss=2.2557907104492188
I0129 06:36:07.068175 140187804313408 spec.py:321] Evaluating on the training split.
I0129 06:36:13.287508 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 06:36:22.342185 140187804313408 spec.py:349] Evaluating on the test split.
I0129 06:36:25.008546 140187804313408 submission_runner.py:408] Time since start: 18532.96s, 	Step: 52715, 	{'train/accuracy': 0.1820591539144516, 'train/loss': 4.70458459854126, 'validation/accuracy': 0.17093999683856964, 'validation/loss': 4.80840539932251, 'validation/num_examples': 50000, 'test/accuracy': 0.13040000200271606, 'test/loss': 5.461226463317871, 'test/num_examples': 10000, 'score': 17883.095085144043, 'total_duration': 18532.963203907013, 'accumulated_submission_time': 17883.095085144043, 'accumulated_eval_time': 645.7019193172455, 'accumulated_logging_time': 2.252317190170288}
I0129 06:36:25.039474 140026159523584 logging_writer.py:48] [52715] accumulated_eval_time=645.701919, accumulated_logging_time=2.252317, accumulated_submission_time=17883.095085, global_step=52715, preemption_count=0, score=17883.095085, test/accuracy=0.130400, test/loss=5.461226, test/num_examples=10000, total_duration=18532.963204, train/accuracy=0.182059, train/loss=4.704585, validation/accuracy=0.170940, validation/loss=4.808405, validation/num_examples=50000
I0129 06:36:54.021878 140026167916288 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.2059823274612427, loss=2.183844566345215
I0129 06:37:27.692312 140026159523584 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.2664657831192017, loss=2.3141627311706543
I0129 06:38:01.391153 140026167916288 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.383240818977356, loss=2.276500701904297
I0129 06:38:35.108759 140026159523584 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2783453464508057, loss=2.1238350868225098
I0129 06:39:08.956249 140026167916288 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2248766422271729, loss=2.3140716552734375
I0129 06:39:42.641653 140026159523584 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.1757279634475708, loss=2.1768646240234375
I0129 06:40:16.358397 140026167916288 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.2227190732955933, loss=2.2877492904663086
I0129 06:40:50.100456 140026159523584 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.311600685119629, loss=2.2313995361328125
I0129 06:41:23.859899 140026167916288 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3171226978302002, loss=2.2824504375457764
I0129 06:41:57.629742 140026159523584 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.269608974456787, loss=2.2875943183898926
I0129 06:42:31.360365 140026167916288 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.4005727767944336, loss=2.264064311981201
I0129 06:43:05.132900 140026159523584 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.223495602607727, loss=2.1849472522735596
I0129 06:43:38.856321 140026167916288 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.2592837810516357, loss=2.2075753211975098
I0129 06:44:12.563894 140026159523584 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.2574963569641113, loss=2.27937388420105
I0129 06:44:46.327022 140026167916288 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.2740325927734375, loss=2.233759880065918
I0129 06:44:55.239775 140187804313408 spec.py:321] Evaluating on the training split.
I0129 06:45:01.468672 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 06:45:10.366691 140187804313408 spec.py:349] Evaluating on the test split.
I0129 06:45:12.989990 140187804313408 submission_runner.py:408] Time since start: 19060.94s, 	Step: 54228, 	{'train/accuracy': 0.1476801633834839, 'train/loss': 5.142282009124756, 'validation/accuracy': 0.14037999510765076, 'validation/loss': 5.216727256774902, 'validation/num_examples': 50000, 'test/accuracy': 0.11010000854730606, 'test/loss': 5.7479472160339355, 'test/num_examples': 10000, 'score': 18393.22921514511, 'total_duration': 19060.944731235504, 'accumulated_submission_time': 18393.22921514511, 'accumulated_eval_time': 663.4520955085754, 'accumulated_logging_time': 2.2944045066833496}
I0129 06:45:13.018602 140026058876672 logging_writer.py:48] [54228] accumulated_eval_time=663.452096, accumulated_logging_time=2.294405, accumulated_submission_time=18393.229215, global_step=54228, preemption_count=0, score=18393.229215, test/accuracy=0.110100, test/loss=5.747947, test/num_examples=10000, total_duration=19060.944731, train/accuracy=0.147680, train/loss=5.142282, validation/accuracy=0.140380, validation/loss=5.216727, validation/num_examples=50000
I0129 06:45:37.738818 140026067269376 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.2001137733459473, loss=2.1723520755767822
I0129 06:46:11.448475 140026058876672 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2441226243972778, loss=2.347195625305176
I0129 06:46:45.244121 140026067269376 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2058038711547852, loss=2.234682083129883
I0129 06:47:19.006961 140026058876672 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.5140553712844849, loss=2.249438524246216
I0129 06:47:52.752778 140026067269376 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.2572993040084839, loss=2.2421188354492188
I0129 06:48:26.505171 140026058876672 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.17874014377594, loss=2.204810380935669
I0129 06:49:00.249570 140026067269376 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.3261369466781616, loss=2.266258955001831
I0129 06:49:33.934783 140026058876672 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.165462613105774, loss=2.2047548294067383
I0129 06:50:07.736568 140026067269376 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.2773141860961914, loss=2.2943460941314697
I0129 06:50:41.446963 140026058876672 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.2085602283477783, loss=2.113581895828247
I0129 06:51:15.219338 140026067269376 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.216294765472412, loss=2.175353765487671
I0129 06:51:49.026786 140026058876672 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.3477321863174438, loss=2.1648049354553223
I0129 06:52:22.762107 140026067269376 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.4007571935653687, loss=2.199596881866455
I0129 06:52:56.508965 140026058876672 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.4148856401443481, loss=2.236281394958496
I0129 06:53:30.316410 140026067269376 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.212833046913147, loss=2.2533297538757324
I0129 06:53:43.280216 140187804313408 spec.py:321] Evaluating on the training split.
I0129 06:53:49.537120 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 06:53:58.623637 140187804313408 spec.py:349] Evaluating on the test split.
I0129 06:54:01.169070 140187804313408 submission_runner.py:408] Time since start: 19589.12s, 	Step: 55740, 	{'train/accuracy': 0.21615912020206451, 'train/loss': 4.4626359939575195, 'validation/accuracy': 0.2102999985218048, 'validation/loss': 4.564239025115967, 'validation/num_examples': 50000, 'test/accuracy': 0.15570001304149628, 'test/loss': 5.351230621337891, 'test/num_examples': 10000, 'score': 18903.425624847412, 'total_duration': 19589.123804807663, 'accumulated_submission_time': 18903.425624847412, 'accumulated_eval_time': 681.3409023284912, 'accumulated_logging_time': 2.3332133293151855}
I0129 06:54:01.196847 140026058876672 logging_writer.py:48] [55740] accumulated_eval_time=681.340902, accumulated_logging_time=2.333213, accumulated_submission_time=18903.425625, global_step=55740, preemption_count=0, score=18903.425625, test/accuracy=0.155700, test/loss=5.351231, test/num_examples=10000, total_duration=19589.123805, train/accuracy=0.216159, train/loss=4.462636, validation/accuracy=0.210300, validation/loss=4.564239, validation/num_examples=50000
I0129 06:54:21.775510 140026159523584 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.3547037839889526, loss=2.2009401321411133
I0129 06:54:55.498787 140026058876672 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.2810637950897217, loss=2.238302707672119
I0129 06:55:29.203603 140026159523584 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.2198634147644043, loss=2.2260026931762695
I0129 06:56:02.993933 140026058876672 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.3147456645965576, loss=2.315452814102173
I0129 06:56:36.708598 140026159523584 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.2885370254516602, loss=2.295114040374756
I0129 06:57:10.471712 140026058876672 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.1951404809951782, loss=2.1888933181762695
I0129 06:57:44.120759 140026159523584 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.229429841041565, loss=2.16391921043396
I0129 06:58:17.951609 140026058876672 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2870386838912964, loss=2.247755765914917
I0129 06:58:51.644277 140026159523584 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.2832211256027222, loss=2.178300380706787
I0129 06:59:25.348747 140026058876672 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2526042461395264, loss=2.1937272548675537
I0129 06:59:59.104779 140026159523584 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.23001229763031, loss=2.3053510189056396
I0129 07:00:32.838071 140026058876672 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.3074606657028198, loss=2.3272852897644043
I0129 07:01:06.608629 140026159523584 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.2914514541625977, loss=2.2940797805786133
I0129 07:01:40.331661 140026058876672 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.243353009223938, loss=2.112157106399536
I0129 07:02:14.088793 140026159523584 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.2168536186218262, loss=2.102407932281494
I0129 07:02:31.456064 140187804313408 spec.py:321] Evaluating on the training split.
I0129 07:02:37.768094 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 07:02:46.524030 140187804313408 spec.py:349] Evaluating on the test split.
I0129 07:02:49.155221 140187804313408 submission_runner.py:408] Time since start: 20117.11s, 	Step: 57253, 	{'train/accuracy': 0.2811702787876129, 'train/loss': 3.8798627853393555, 'validation/accuracy': 0.2648199796676636, 'validation/loss': 4.011868476867676, 'validation/num_examples': 50000, 'test/accuracy': 0.20630000531673431, 'test/loss': 4.711694240570068, 'test/num_examples': 10000, 'score': 19413.61908721924, 'total_duration': 20117.109936237335, 'accumulated_submission_time': 19413.61908721924, 'accumulated_eval_time': 699.0399971008301, 'accumulated_logging_time': 2.3717331886291504}
I0129 07:02:49.186052 140026067269376 logging_writer.py:48] [57253] accumulated_eval_time=699.039997, accumulated_logging_time=2.371733, accumulated_submission_time=19413.619087, global_step=57253, preemption_count=0, score=19413.619087, test/accuracy=0.206300, test/loss=4.711694, test/num_examples=10000, total_duration=20117.109936, train/accuracy=0.281170, train/loss=3.879863, validation/accuracy=0.264820, validation/loss=4.011868, validation/num_examples=50000
I0129 07:03:05.425099 140026075662080 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2383559942245483, loss=2.183279037475586
I0129 07:03:39.157052 140026067269376 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.3065879344940186, loss=2.3360486030578613
I0129 07:04:12.863834 140026075662080 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.2684872150421143, loss=2.224836587905884
I0129 07:04:46.734466 140026067269376 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.406216025352478, loss=2.174111843109131
I0129 07:05:20.497454 140026075662080 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.3243176937103271, loss=2.228436231613159
I0129 07:05:54.272056 140026067269376 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.328609585762024, loss=2.1838574409484863
I0129 07:06:27.970910 140026075662080 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.3526084423065186, loss=2.2478885650634766
I0129 07:07:01.773543 140026067269376 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.322542428970337, loss=2.268827199935913
I0129 07:07:35.558962 140026075662080 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.2614967823028564, loss=2.3182449340820312
I0129 07:08:09.253152 140026067269376 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.2786198854446411, loss=2.3641085624694824
I0129 07:08:42.910123 140026075662080 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.187017560005188, loss=2.189965009689331
I0129 07:09:16.608301 140026067269376 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.3418874740600586, loss=2.1484155654907227
I0129 07:09:50.285297 140026075662080 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.4219584465026855, loss=2.2310359477996826
I0129 07:10:23.984284 140026067269376 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.1990187168121338, loss=2.13091778755188
I0129 07:10:57.760059 140026075662080 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2882343530654907, loss=2.18894100189209
I0129 07:11:19.385191 140187804313408 spec.py:321] Evaluating on the training split.
I0129 07:11:25.630914 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 07:11:34.621373 140187804313408 spec.py:349] Evaluating on the test split.
I0129 07:11:37.267277 140187804313408 submission_runner.py:408] Time since start: 20645.22s, 	Step: 58765, 	{'train/accuracy': 0.2714046537876129, 'train/loss': 3.7277042865753174, 'validation/accuracy': 0.2528599798679352, 'validation/loss': 3.881164312362671, 'validation/num_examples': 50000, 'test/accuracy': 0.18570001423358917, 'test/loss': 4.536026477813721, 'test/num_examples': 10000, 'score': 19923.753940820694, 'total_duration': 20645.222013950348, 'accumulated_submission_time': 19923.753940820694, 'accumulated_eval_time': 716.9220430850983, 'accumulated_logging_time': 2.4119575023651123}
I0129 07:11:37.300071 140026058876672 logging_writer.py:48] [58765] accumulated_eval_time=716.922043, accumulated_logging_time=2.411958, accumulated_submission_time=19923.753941, global_step=58765, preemption_count=0, score=19923.753941, test/accuracy=0.185700, test/loss=4.536026, test/num_examples=10000, total_duration=20645.222014, train/accuracy=0.271405, train/loss=3.727704, validation/accuracy=0.252860, validation/loss=3.881164, validation/num_examples=50000
I0129 07:11:49.435315 140026159523584 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.3285595178604126, loss=2.1122043132781982
I0129 07:12:23.101202 140026058876672 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.233615517616272, loss=2.2329113483428955
I0129 07:12:56.802959 140026159523584 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2843494415283203, loss=2.1529440879821777
I0129 07:13:30.449920 140026058876672 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.30160391330719, loss=2.1478586196899414
I0129 07:14:04.165508 140026159523584 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.2540239095687866, loss=2.368957042694092
I0129 07:14:37.857699 140026058876672 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.2256237268447876, loss=2.1770310401916504
I0129 07:15:11.620908 140026159523584 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.3660612106323242, loss=2.3110833168029785
I0129 07:15:45.403264 140026058876672 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.2880350351333618, loss=2.2509944438934326
I0129 07:16:19.109195 140026159523584 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.2091354131698608, loss=2.2300992012023926
I0129 07:16:52.830400 140026058876672 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.410662055015564, loss=2.1601662635803223
I0129 07:17:26.585069 140026159523584 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.4117028713226318, loss=2.1194257736206055
I0129 07:18:00.389471 140026058876672 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.550397515296936, loss=2.2759618759155273
I0129 07:18:34.095237 140026159523584 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.364211082458496, loss=2.1526291370391846
I0129 07:19:07.857881 140026058876672 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2079558372497559, loss=2.2941625118255615
I0129 07:19:41.612851 140026159523584 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.2032346725463867, loss=2.181640148162842
I0129 07:20:07.389145 140187804313408 spec.py:321] Evaluating on the training split.
I0129 07:20:13.602081 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 07:20:22.408601 140187804313408 spec.py:349] Evaluating on the test split.
I0129 07:20:24.971441 140187804313408 submission_runner.py:408] Time since start: 21172.93s, 	Step: 60278, 	{'train/accuracy': 0.15573182702064514, 'train/loss': 5.1116814613342285, 'validation/accuracy': 0.1419599950313568, 'validation/loss': 5.236762523651123, 'validation/num_examples': 50000, 'test/accuracy': 0.09450000524520874, 'test/loss': 6.115200519561768, 'test/num_examples': 10000, 'score': 20433.7780482769, 'total_duration': 21172.926019191742, 'accumulated_submission_time': 20433.7780482769, 'accumulated_eval_time': 734.5041456222534, 'accumulated_logging_time': 2.4547340869903564}
I0129 07:20:24.998921 140026151130880 logging_writer.py:48] [60278] accumulated_eval_time=734.504146, accumulated_logging_time=2.454734, accumulated_submission_time=20433.778048, global_step=60278, preemption_count=0, score=20433.778048, test/accuracy=0.094500, test/loss=6.115201, test/num_examples=10000, total_duration=21172.926019, train/accuracy=0.155732, train/loss=5.111681, validation/accuracy=0.141960, validation/loss=5.236763, validation/num_examples=50000
I0129 07:20:32.755368 140026176308992 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.3527575731277466, loss=2.224841594696045
I0129 07:21:06.488068 140026151130880 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.2400826215744019, loss=2.1478214263916016
I0129 07:21:40.204508 140026176308992 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.2117156982421875, loss=2.033024311065674
I0129 07:22:13.949534 140026151130880 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.331994652748108, loss=2.1989729404449463
I0129 07:22:47.664556 140026176308992 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.2248573303222656, loss=2.011833906173706
I0129 07:23:21.434256 140026151130880 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.2358789443969727, loss=2.254814386367798
I0129 07:23:55.145786 140026176308992 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2423213720321655, loss=2.2265920639038086
I0129 07:24:28.968062 140026151130880 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2153501510620117, loss=2.1547441482543945
I0129 07:25:02.677025 140026176308992 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.1972099542617798, loss=2.0807430744171143
I0129 07:25:36.403609 140026151130880 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.4232890605926514, loss=2.2964022159576416
I0129 07:26:10.160475 140026176308992 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.3401716947555542, loss=2.2341952323913574
I0129 07:26:43.819135 140026151130880 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.284077525138855, loss=2.1875147819519043
I0129 07:27:17.540853 140026176308992 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.317113995552063, loss=2.34853196144104
I0129 07:27:51.255523 140026151130880 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.6127194166183472, loss=2.253054618835449
I0129 07:28:25.012432 140026176308992 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.4160763025283813, loss=2.1324784755706787
I0129 07:28:55.203783 140187804313408 spec.py:321] Evaluating on the training split.
I0129 07:29:01.435407 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 07:29:10.609685 140187804313408 spec.py:349] Evaluating on the test split.
I0129 07:29:13.179662 140187804313408 submission_runner.py:408] Time since start: 21701.13s, 	Step: 61791, 	{'train/accuracy': 0.2585897445678711, 'train/loss': 3.929126262664795, 'validation/accuracy': 0.24305999279022217, 'validation/loss': 4.085925579071045, 'validation/num_examples': 50000, 'test/accuracy': 0.1754000037908554, 'test/loss': 4.864076614379883, 'test/num_examples': 10000, 'score': 20943.91853928566, 'total_duration': 21701.13439130783, 'accumulated_submission_time': 20943.91853928566, 'accumulated_eval_time': 752.4799780845642, 'accumulated_logging_time': 2.491457223892212}
I0129 07:29:13.210949 140026058876672 logging_writer.py:48] [61791] accumulated_eval_time=752.479978, accumulated_logging_time=2.491457, accumulated_submission_time=20943.918539, global_step=61791, preemption_count=0, score=20943.918539, test/accuracy=0.175400, test/loss=4.864077, test/num_examples=10000, total_duration=21701.134391, train/accuracy=0.258590, train/loss=3.929126, validation/accuracy=0.243060, validation/loss=4.085926, validation/num_examples=50000
I0129 07:29:16.579940 140026067269376 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.3233838081359863, loss=2.2058963775634766
I0129 07:29:50.254747 140026058876672 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.2667075395584106, loss=2.1370882987976074
I0129 07:30:23.946931 140026067269376 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.4209418296813965, loss=2.1685266494750977
I0129 07:30:57.818220 140026058876672 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.3188552856445312, loss=2.238481044769287
I0129 07:31:31.502443 140026067269376 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.3099769353866577, loss=2.2281689643859863
I0129 07:32:05.224934 140026058876672 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.2472333908081055, loss=2.072327136993408
I0129 07:32:38.974440 140026067269376 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.3949414491653442, loss=2.251850128173828
I0129 07:33:12.671463 140026058876672 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.291485071182251, loss=2.1927733421325684
I0129 07:33:46.367186 140026067269376 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.4646780490875244, loss=2.003131866455078
I0129 07:34:20.120896 140026058876672 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.3476295471191406, loss=2.405900001525879
I0129 07:34:53.851900 140026067269376 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.2807379961013794, loss=2.138611316680908
I0129 07:35:27.587259 140026058876672 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.181395173072815, loss=2.0910120010375977
I0129 07:36:01.339678 140026067269376 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.2981010675430298, loss=2.2861993312835693
I0129 07:36:35.064466 140026058876672 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.291222095489502, loss=2.1537957191467285
I0129 07:37:08.889104 140026067269376 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.3755896091461182, loss=2.1879706382751465
I0129 07:37:42.656696 140026058876672 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.3368490934371948, loss=2.182063579559326
I0129 07:37:43.477615 140187804313408 spec.py:321] Evaluating on the training split.
I0129 07:37:49.849187 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 07:37:58.548973 140187804313408 spec.py:349] Evaluating on the test split.
I0129 07:38:01.368024 140187804313408 submission_runner.py:408] Time since start: 22229.32s, 	Step: 63304, 	{'train/accuracy': 0.19387754797935486, 'train/loss': 4.898688316345215, 'validation/accuracy': 0.18491999804973602, 'validation/loss': 4.995222091674805, 'validation/num_examples': 50000, 'test/accuracy': 0.1388000100851059, 'test/loss': 5.745086193084717, 'test/num_examples': 10000, 'score': 21454.120364904404, 'total_duration': 22229.322627067566, 'accumulated_submission_time': 21454.120364904404, 'accumulated_eval_time': 770.3702204227448, 'accumulated_logging_time': 2.534074068069458}
I0129 07:38:01.412111 140026058876672 logging_writer.py:48] [63304] accumulated_eval_time=770.370220, accumulated_logging_time=2.534074, accumulated_submission_time=21454.120365, global_step=63304, preemption_count=0, score=21454.120365, test/accuracy=0.138800, test/loss=5.745086, test/num_examples=10000, total_duration=22229.322627, train/accuracy=0.193878, train/loss=4.898688, validation/accuracy=0.184920, validation/loss=4.995222, validation/num_examples=50000
I0129 07:38:34.150114 140026151130880 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.4308204650878906, loss=2.2070565223693848
I0129 07:39:07.806912 140026058876672 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2408827543258667, loss=2.045496940612793
I0129 07:39:41.503038 140026151130880 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3139238357543945, loss=2.02262020111084
I0129 07:40:15.207772 140026058876672 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.3389357328414917, loss=2.212824821472168
I0129 07:40:49.000214 140026151130880 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.3621771335601807, loss=2.209304094314575
I0129 07:41:22.669881 140026058876672 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3651255369186401, loss=2.0626425743103027
I0129 07:41:56.463489 140026151130880 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1545135974884033, loss=2.1216611862182617
I0129 07:42:30.115680 140026058876672 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.3676706552505493, loss=2.2871930599212646
I0129 07:43:03.850437 140026151130880 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.3558506965637207, loss=2.1899237632751465
I0129 07:43:37.626020 140026058876672 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.2394905090332031, loss=2.1554512977600098
I0129 07:44:11.383229 140026151130880 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.2298054695129395, loss=2.116152286529541
I0129 07:44:45.137688 140026058876672 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3078222274780273, loss=2.2722504138946533
I0129 07:45:18.866485 140026151130880 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.2240889072418213, loss=2.104379653930664
I0129 07:45:52.531428 140026058876672 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2396234273910522, loss=2.1536049842834473
I0129 07:46:26.212247 140026151130880 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.2746570110321045, loss=2.074408531188965
I0129 07:46:31.413163 140187804313408 spec.py:321] Evaluating on the training split.
I0129 07:46:37.720066 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 07:46:46.410919 140187804313408 spec.py:349] Evaluating on the test split.
I0129 07:46:49.082353 140187804313408 submission_runner.py:408] Time since start: 22757.04s, 	Step: 64817, 	{'train/accuracy': 0.19919881224632263, 'train/loss': 4.335831165313721, 'validation/accuracy': 0.18441998958587646, 'validation/loss': 4.489870548248291, 'validation/num_examples': 50000, 'test/accuracy': 0.13370001316070557, 'test/loss': 5.076531410217285, 'test/num_examples': 10000, 'score': 21964.04625606537, 'total_duration': 22757.037092924118, 'accumulated_submission_time': 21964.04625606537, 'accumulated_eval_time': 788.0393702983856, 'accumulated_logging_time': 2.598047971725464}
I0129 07:46:49.111674 140026050483968 logging_writer.py:48] [64817] accumulated_eval_time=788.039370, accumulated_logging_time=2.598048, accumulated_submission_time=21964.046256, global_step=64817, preemption_count=0, score=21964.046256, test/accuracy=0.133700, test/loss=5.076531, test/num_examples=10000, total_duration=22757.037093, train/accuracy=0.199199, train/loss=4.335831, validation/accuracy=0.184420, validation/loss=4.489871, validation/num_examples=50000
I0129 07:47:17.504316 140026067269376 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2790108919143677, loss=2.163855791091919
I0129 07:47:51.208130 140026050483968 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.3961023092269897, loss=2.1654751300811768
I0129 07:48:24.886721 140026067269376 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.586967945098877, loss=2.236165761947632
I0129 07:48:58.575305 140026050483968 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.4883400201797485, loss=2.077404499053955
I0129 07:49:32.364004 140026067269376 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.3583961725234985, loss=2.318093776702881
I0129 07:50:06.187633 140026050483968 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.3856499195098877, loss=2.0557243824005127
I0129 07:50:39.990503 140026067269376 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.4077776670455933, loss=2.0187056064605713
I0129 07:51:13.682739 140026050483968 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.3976526260375977, loss=2.2472715377807617
I0129 07:51:47.473853 140026067269376 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.5664546489715576, loss=2.2611303329467773
I0129 07:52:21.141421 140026050483968 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.3564732074737549, loss=2.084998607635498
I0129 07:52:54.847477 140026067269376 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.2965742349624634, loss=2.1156129837036133
I0129 07:53:28.546582 140026050483968 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.3424752950668335, loss=2.062429189682007
I0129 07:54:02.336411 140026067269376 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.272344708442688, loss=2.171335458755493
I0129 07:54:36.035909 140026050483968 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.3990968465805054, loss=2.1764276027679443
I0129 07:55:09.732264 140026067269376 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3979359865188599, loss=2.1613261699676514
I0129 07:55:19.310908 140187804313408 spec.py:321] Evaluating on the training split.
I0129 07:55:25.589042 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 07:55:34.587546 140187804313408 spec.py:349] Evaluating on the test split.
I0129 07:55:37.211000 140187804313408 submission_runner.py:408] Time since start: 23285.17s, 	Step: 66330, 	{'train/accuracy': 0.3051060140132904, 'train/loss': 3.4450433254241943, 'validation/accuracy': 0.2985999882221222, 'validation/loss': 3.5019185543060303, 'validation/num_examples': 50000, 'test/accuracy': 0.20680001378059387, 'test/loss': 4.329137802124023, 'test/num_examples': 10000, 'score': 22474.17966222763, 'total_duration': 23285.16569185257, 'accumulated_submission_time': 22474.17966222763, 'accumulated_eval_time': 805.9393737316132, 'accumulated_logging_time': 2.6385395526885986}
I0129 07:55:37.242630 140026050483968 logging_writer.py:48] [66330] accumulated_eval_time=805.939374, accumulated_logging_time=2.638540, accumulated_submission_time=22474.179662, global_step=66330, preemption_count=0, score=22474.179662, test/accuracy=0.206800, test/loss=4.329138, test/num_examples=10000, total_duration=23285.165692, train/accuracy=0.305106, train/loss=3.445043, validation/accuracy=0.298600, validation/loss=3.501919, validation/num_examples=50000
I0129 07:56:01.185556 140026058876672 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.364364504814148, loss=2.2665257453918457
I0129 07:56:34.990211 140026050483968 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.3065026998519897, loss=2.096863269805908
I0129 07:57:08.706848 140026058876672 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.486162781715393, loss=2.191713571548462
I0129 07:57:42.434359 140026050483968 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.1746748685836792, loss=2.0636544227600098
I0129 07:58:16.209505 140026058876672 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.3549243211746216, loss=2.096538543701172
I0129 07:58:49.951520 140026050483968 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.4300607442855835, loss=2.2360963821411133
I0129 07:59:23.623773 140026058876672 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.3059124946594238, loss=2.0477399826049805
I0129 07:59:57.356153 140026050483968 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.4427345991134644, loss=2.2074193954467773
I0129 08:00:31.124196 140026058876672 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.3280937671661377, loss=2.1718485355377197
I0129 08:01:04.841223 140026050483968 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.5569946765899658, loss=2.217761278152466
I0129 08:01:38.616342 140026058876672 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2315442562103271, loss=2.070856809616089
I0129 08:02:12.348899 140026050483968 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3091458082199097, loss=2.1391563415527344
I0129 08:02:46.111400 140026058876672 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3937777280807495, loss=2.2188963890075684
I0129 08:03:20.003925 140026050483968 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.4373176097869873, loss=2.140812873840332
I0129 08:03:53.767680 140026058876672 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.321511149406433, loss=2.23043155670166
I0129 08:04:07.415212 140187804313408 spec.py:321] Evaluating on the training split.
I0129 08:04:13.775626 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 08:04:22.806856 140187804313408 spec.py:349] Evaluating on the test split.
I0129 08:04:25.374475 140187804313408 submission_runner.py:408] Time since start: 23813.33s, 	Step: 67842, 	{'train/accuracy': 0.3987165093421936, 'train/loss': 2.7879419326782227, 'validation/accuracy': 0.37814000248908997, 'validation/loss': 2.944031238555908, 'validation/num_examples': 50000, 'test/accuracy': 0.2841000258922577, 'test/loss': 3.7177770137786865, 'test/num_examples': 10000, 'score': 22984.28769302368, 'total_duration': 23813.329062461853, 'accumulated_submission_time': 22984.28769302368, 'accumulated_eval_time': 823.8984444141388, 'accumulated_logging_time': 2.679395914077759}
I0129 08:04:25.407866 140026075662080 logging_writer.py:48] [67842] accumulated_eval_time=823.898444, accumulated_logging_time=2.679396, accumulated_submission_time=22984.287693, global_step=67842, preemption_count=0, score=22984.287693, test/accuracy=0.284100, test/loss=3.717777, test/num_examples=10000, total_duration=23813.329062, train/accuracy=0.398717, train/loss=2.787942, validation/accuracy=0.378140, validation/loss=2.944031, validation/num_examples=50000
I0129 08:04:45.284724 140026159523584 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.3847570419311523, loss=2.1783018112182617
I0129 08:05:18.990352 140026075662080 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3373581171035767, loss=2.1132864952087402
I0129 08:05:52.655181 140026159523584 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.395274043083191, loss=2.2334067821502686
I0129 08:06:26.342915 140026075662080 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.3679207563400269, loss=2.070131301879883
I0129 08:07:00.088690 140026159523584 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.4277511835098267, loss=2.147852897644043
I0129 08:07:33.808289 140026075662080 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2967596054077148, loss=2.1840546131134033
I0129 08:08:07.550181 140026159523584 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.3340588808059692, loss=2.076017379760742
I0129 08:08:41.341954 140026075662080 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.303952932357788, loss=2.1062827110290527
I0129 08:09:15.030368 140026159523584 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.2936029434204102, loss=2.215850353240967
I0129 08:09:48.910099 140026075662080 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2648487091064453, loss=2.063035726547241
I0129 08:10:22.581758 140026159523584 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.3147873878479004, loss=2.1382322311401367
I0129 08:10:56.329354 140026075662080 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.2213771343231201, loss=2.1086676120758057
I0129 08:11:30.025456 140026159523584 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.2052768468856812, loss=2.033294200897217
I0129 08:12:03.722725 140026075662080 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.301503300666809, loss=1.9803378582000732
I0129 08:12:37.417702 140026159523584 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2505536079406738, loss=2.1240389347076416
I0129 08:12:55.459704 140187804313408 spec.py:321] Evaluating on the training split.
I0129 08:13:01.949106 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 08:13:10.836944 140187804313408 spec.py:349] Evaluating on the test split.
I0129 08:13:13.527404 140187804313408 submission_runner.py:408] Time since start: 24341.48s, 	Step: 69355, 	{'train/accuracy': 0.24583466351032257, 'train/loss': 4.067709922790527, 'validation/accuracy': 0.22181999683380127, 'validation/loss': 4.309610843658447, 'validation/num_examples': 50000, 'test/accuracy': 0.1639000028371811, 'test/loss': 5.0794901847839355, 'test/num_examples': 10000, 'score': 23494.273504018784, 'total_duration': 24341.482144355774, 'accumulated_submission_time': 23494.273504018784, 'accumulated_eval_time': 841.9661107063293, 'accumulated_logging_time': 2.723883628845215}
I0129 08:13:13.557288 140026058876672 logging_writer.py:48] [69355] accumulated_eval_time=841.966111, accumulated_logging_time=2.723884, accumulated_submission_time=23494.273504, global_step=69355, preemption_count=0, score=23494.273504, test/accuracy=0.163900, test/loss=5.079490, test/num_examples=10000, total_duration=24341.482144, train/accuracy=0.245835, train/loss=4.067710, validation/accuracy=0.221820, validation/loss=4.309611, validation/num_examples=50000
I0129 08:13:29.082557 140026067269376 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.4945846796035767, loss=2.2235212326049805
I0129 08:14:02.795927 140026058876672 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.475969910621643, loss=2.2143397331237793
I0129 08:14:36.508065 140026067269376 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.2934266328811646, loss=2.1762773990631104
I0129 08:15:10.279661 140026058876672 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2701910734176636, loss=2.0257647037506104
I0129 08:15:44.020527 140026067269376 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.4868824481964111, loss=2.227813482284546
I0129 08:16:17.882098 140026058876672 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3875318765640259, loss=2.172422170639038
I0129 08:16:51.626592 140026067269376 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2007702589035034, loss=2.1193082332611084
I0129 08:17:25.383333 140026058876672 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.3299058675765991, loss=2.1543350219726562
I0129 08:17:59.102255 140026067269376 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.3564645051956177, loss=2.072397232055664
I0129 08:18:32.887788 140026058876672 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.3902051448822021, loss=2.0883755683898926
I0129 08:19:06.587717 140026067269376 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.477992296218872, loss=2.225999116897583
I0129 08:19:40.359371 140026058876672 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.5652422904968262, loss=2.1052370071411133
I0129 08:20:14.083029 140026067269376 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.6226613521575928, loss=2.1930997371673584
I0129 08:20:47.856449 140026058876672 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.3272944688796997, loss=2.1730263233184814
I0129 08:21:21.566377 140026067269376 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.4490288496017456, loss=2.2358758449554443
I0129 08:21:43.676137 140187804313408 spec.py:321] Evaluating on the training split.
I0129 08:21:49.898213 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 08:21:58.820740 140187804313408 spec.py:349] Evaluating on the test split.
I0129 08:22:01.535357 140187804313408 submission_runner.py:408] Time since start: 24869.49s, 	Step: 70867, 	{'train/accuracy': 0.31736287474632263, 'train/loss': 3.2800393104553223, 'validation/accuracy': 0.2933799922466278, 'validation/loss': 3.4317257404327393, 'validation/num_examples': 50000, 'test/accuracy': 0.22180001437664032, 'test/loss': 4.1046953201293945, 'test/num_examples': 10000, 'score': 24004.327831745148, 'total_duration': 24869.49009847641, 'accumulated_submission_time': 24004.327831745148, 'accumulated_eval_time': 859.8252913951874, 'accumulated_logging_time': 2.7633161544799805}
I0129 08:22:01.569520 140026159523584 logging_writer.py:48] [70867] accumulated_eval_time=859.825291, accumulated_logging_time=2.763316, accumulated_submission_time=24004.327832, global_step=70867, preemption_count=0, score=24004.327832, test/accuracy=0.221800, test/loss=4.104695, test/num_examples=10000, total_duration=24869.490098, train/accuracy=0.317363, train/loss=3.280039, validation/accuracy=0.293380, validation/loss=3.431726, validation/num_examples=50000
I0129 08:22:13.025969 140026167916288 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.408854365348816, loss=2.2661375999450684
I0129 08:22:46.812083 140026159523584 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.334883689880371, loss=2.080195665359497
I0129 08:23:20.540973 140026167916288 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2335431575775146, loss=1.9869858026504517
I0129 08:23:54.232401 140026159523584 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.5174319744110107, loss=2.1684412956237793
I0129 08:24:27.995756 140026167916288 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.366485834121704, loss=2.1963744163513184
I0129 08:25:01.746816 140026159523584 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.304351806640625, loss=2.0634260177612305
I0129 08:25:35.480227 140026167916288 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.384591817855835, loss=2.2262039184570312
I0129 08:26:09.239500 140026159523584 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2833926677703857, loss=2.2017128467559814
I0129 08:26:42.961745 140026167916288 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.4714769124984741, loss=2.1493449211120605
I0129 08:27:16.670707 140026159523584 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.21318781375885, loss=2.0485987663269043
I0129 08:27:50.444026 140026167916288 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.371411681175232, loss=2.176713466644287
I0129 08:28:24.122673 140026159523584 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.326736569404602, loss=2.051438570022583
I0129 08:28:58.096460 140026167916288 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.634118914604187, loss=2.191906452178955
I0129 08:29:31.777246 140026159523584 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.4936391115188599, loss=2.124767541885376
I0129 08:30:05.488701 140026167916288 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.435325264930725, loss=2.1437833309173584
I0129 08:30:31.553305 140187804313408 spec.py:321] Evaluating on the training split.
I0129 08:30:37.861512 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 08:30:46.639227 140187804313408 spec.py:349] Evaluating on the test split.
I0129 08:30:49.323447 140187804313408 submission_runner.py:408] Time since start: 25397.28s, 	Step: 72379, 	{'train/accuracy': 0.4204001724720001, 'train/loss': 2.6156625747680664, 'validation/accuracy': 0.3950199782848358, 'validation/loss': 2.788191795349121, 'validation/num_examples': 50000, 'test/accuracy': 0.2957000136375427, 'test/loss': 3.51944899559021, 'test/num_examples': 10000, 'score': 24514.24355506897, 'total_duration': 25397.27818083763, 'accumulated_submission_time': 24514.24355506897, 'accumulated_eval_time': 877.5954036712646, 'accumulated_logging_time': 2.8088531494140625}
I0129 08:30:49.352966 140026058876672 logging_writer.py:48] [72379] accumulated_eval_time=877.595404, accumulated_logging_time=2.808853, accumulated_submission_time=24514.243555, global_step=72379, preemption_count=0, score=24514.243555, test/accuracy=0.295700, test/loss=3.519449, test/num_examples=10000, total_duration=25397.278181, train/accuracy=0.420400, train/loss=2.615663, validation/accuracy=0.395020, validation/loss=2.788192, validation/num_examples=50000
I0129 08:30:56.775171 140026067269376 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.5910803079605103, loss=2.2014224529266357
I0129 08:31:30.459962 140026058876672 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.441899061203003, loss=2.127185583114624
I0129 08:32:04.179548 140026067269376 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.6495506763458252, loss=1.9807641506195068
I0129 08:32:37.927641 140026058876672 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.4527403116226196, loss=2.0198583602905273
I0129 08:33:11.725214 140026067269376 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3929460048675537, loss=2.1115362644195557
I0129 08:33:45.418216 140026058876672 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.2895618677139282, loss=2.080418109893799
I0129 08:34:19.207766 140026067269376 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3006749153137207, loss=2.1404645442962646
I0129 08:34:52.915911 140026058876672 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.332787036895752, loss=2.1436924934387207
I0129 08:35:26.838388 140026067269376 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.343225121498108, loss=2.1599462032318115
I0129 08:36:00.544629 140026058876672 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3980296850204468, loss=2.0126004219055176
I0129 08:36:34.310714 140026067269376 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.3790868520736694, loss=2.038940191268921
I0129 08:37:08.009474 140026058876672 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.348808765411377, loss=2.122915029525757
I0129 08:37:41.771900 140026067269376 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.4274513721466064, loss=1.9452450275421143
I0129 08:38:15.481306 140026058876672 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3906418085098267, loss=2.0537335872650146
I0129 08:38:49.264498 140026067269376 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.273969054222107, loss=2.0491995811462402
I0129 08:39:19.406595 140187804313408 spec.py:321] Evaluating on the training split.
I0129 08:39:25.794259 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 08:39:34.489760 140187804313408 spec.py:349] Evaluating on the test split.
I0129 08:39:37.128570 140187804313408 submission_runner.py:408] Time since start: 25925.08s, 	Step: 73891, 	{'train/accuracy': 0.39176100492477417, 'train/loss': 2.7942934036254883, 'validation/accuracy': 0.3677600026130676, 'validation/loss': 2.944324016571045, 'validation/num_examples': 50000, 'test/accuracy': 0.27300000190734863, 'test/loss': 3.748530149459839, 'test/num_examples': 10000, 'score': 25024.23308992386, 'total_duration': 25925.08330845833, 'accumulated_submission_time': 25024.23308992386, 'accumulated_eval_time': 895.3173720836639, 'accumulated_logging_time': 2.8476545810699463}
I0129 08:39:37.163548 140026058876672 logging_writer.py:48] [73891] accumulated_eval_time=895.317372, accumulated_logging_time=2.847655, accumulated_submission_time=25024.233090, global_step=73891, preemption_count=0, score=25024.233090, test/accuracy=0.273000, test/loss=3.748530, test/num_examples=10000, total_duration=25925.083308, train/accuracy=0.391761, train/loss=2.794293, validation/accuracy=0.367760, validation/loss=2.944324, validation/num_examples=50000
I0129 08:39:40.536667 140026067269376 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.5565494298934937, loss=2.271941661834717
I0129 08:40:14.262905 140026058876672 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.3331217765808105, loss=2.006711483001709
I0129 08:40:47.942996 140026067269376 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.6028361320495605, loss=2.026700973510742
I0129 08:41:21.662639 140026058876672 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3608934879302979, loss=2.189004421234131
I0129 08:41:55.564314 140026067269376 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.3697359561920166, loss=2.161865711212158
I0129 08:42:29.293246 140026058876672 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.3452342748641968, loss=2.1022863388061523
I0129 08:43:02.951080 140026067269376 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3389052152633667, loss=2.096966505050659
I0129 08:43:36.672831 140026058876672 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.4191572666168213, loss=2.272629737854004
I0129 08:44:10.421278 140026067269376 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4188812971115112, loss=2.1587159633636475
I0129 08:44:44.155024 140026058876672 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.4284883737564087, loss=2.2541542053222656
I0129 08:45:17.815700 140026067269376 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.314853549003601, loss=2.0741360187530518
I0129 08:45:51.529538 140026058876672 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.4781785011291504, loss=2.1391663551330566
I0129 08:46:25.286263 140026067269376 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.465970516204834, loss=2.1726388931274414
I0129 08:46:59.021872 140026058876672 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.4127373695373535, loss=2.217278480529785
I0129 08:47:32.687868 140026067269376 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.3166780471801758, loss=2.045098304748535
I0129 08:48:06.498401 140026058876672 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.3746904134750366, loss=2.110811710357666
I0129 08:48:07.322088 140187804313408 spec.py:321] Evaluating on the training split.
I0129 08:48:13.619594 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 08:48:22.311386 140187804313408 spec.py:349] Evaluating on the test split.
I0129 08:48:24.935599 140187804313408 submission_runner.py:408] Time since start: 26452.89s, 	Step: 75404, 	{'train/accuracy': 0.09494578838348389, 'train/loss': 6.09211540222168, 'validation/accuracy': 0.08621999621391296, 'validation/loss': 6.208457946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.0658000037074089, 'test/loss': 6.601757526397705, 'test/num_examples': 10000, 'score': 25534.324366092682, 'total_duration': 26452.890166282654, 'accumulated_submission_time': 25534.324366092682, 'accumulated_eval_time': 912.9306666851044, 'accumulated_logging_time': 2.89243745803833}
I0129 08:48:24.969022 140026151130880 logging_writer.py:48] [75404] accumulated_eval_time=912.930667, accumulated_logging_time=2.892437, accumulated_submission_time=25534.324366, global_step=75404, preemption_count=0, score=25534.324366, test/accuracy=0.065800, test/loss=6.601758, test/num_examples=10000, total_duration=26452.890166, train/accuracy=0.094946, train/loss=6.092115, validation/accuracy=0.086220, validation/loss=6.208458, validation/num_examples=50000
I0129 08:48:57.697873 140026159523584 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.3992928266525269, loss=2.1361308097839355
I0129 08:49:31.396249 140026151130880 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.3111786842346191, loss=2.139219045639038
I0129 08:50:05.115836 140026159523584 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.4653820991516113, loss=2.0900537967681885
I0129 08:50:38.832312 140026151130880 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.4132589101791382, loss=2.192779064178467
I0129 08:51:12.641561 140026159523584 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3551995754241943, loss=2.213890790939331
I0129 08:51:46.355860 140026151130880 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.439396619796753, loss=2.177523374557495
I0129 08:52:20.133039 140026159523584 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.4763762950897217, loss=2.1648831367492676
I0129 08:52:53.842966 140026151130880 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.3634456396102905, loss=2.087877035140991
I0129 08:53:27.626951 140026159523584 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.5894867181777954, loss=2.01252818107605
I0129 08:54:01.293746 140026151130880 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.338926911354065, loss=2.161757230758667
I0129 08:54:34.990826 140026159523584 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.4104602336883545, loss=2.094562530517578
I0129 08:55:08.802727 140026151130880 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.3997026681900024, loss=2.1124894618988037
I0129 08:55:42.610524 140026159523584 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4697507619857788, loss=2.1591403484344482
I0129 08:56:16.288532 140026151130880 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.5375415086746216, loss=2.0792794227600098
I0129 08:56:49.994970 140026159523584 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.3436169624328613, loss=2.0519697666168213
I0129 08:56:55.193229 140187804313408 spec.py:321] Evaluating on the training split.
I0129 08:57:01.548772 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 08:57:10.529905 140187804313408 spec.py:349] Evaluating on the test split.
I0129 08:57:13.089959 140187804313408 submission_runner.py:408] Time since start: 26981.04s, 	Step: 76917, 	{'train/accuracy': 0.4280731678009033, 'train/loss': 2.589535713195801, 'validation/accuracy': 0.40323999524116516, 'validation/loss': 2.7519068717956543, 'validation/num_examples': 50000, 'test/accuracy': 0.3020000159740448, 'test/loss': 3.514342784881592, 'test/num_examples': 10000, 'score': 26044.48452091217, 'total_duration': 26981.044637680054, 'accumulated_submission_time': 26044.48452091217, 'accumulated_eval_time': 930.827305316925, 'accumulated_logging_time': 2.9355061054229736}
I0129 08:57:13.122653 140026067269376 logging_writer.py:48] [76917] accumulated_eval_time=930.827305, accumulated_logging_time=2.935506, accumulated_submission_time=26044.484521, global_step=76917, preemption_count=0, score=26044.484521, test/accuracy=0.302000, test/loss=3.514343, test/num_examples=10000, total_duration=26981.044638, train/accuracy=0.428073, train/loss=2.589536, validation/accuracy=0.403240, validation/loss=2.751907, validation/num_examples=50000
I0129 08:57:41.446184 140026075662080 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.5053930282592773, loss=2.2444639205932617
I0129 08:58:15.160240 140026067269376 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.492628812789917, loss=2.1180548667907715
I0129 08:58:48.817160 140026075662080 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.7390633821487427, loss=2.1553382873535156
I0129 08:59:22.580926 140026067269376 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.5832149982452393, loss=2.19828200340271
I0129 08:59:56.277907 140026075662080 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.686139464378357, loss=2.1969528198242188
I0129 09:00:30.059669 140026067269376 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.4219640493392944, loss=2.0572547912597656
I0129 09:01:03.755397 140026075662080 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.5288294553756714, loss=2.0714428424835205
I0129 09:01:37.711606 140026067269376 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.4634287357330322, loss=1.986552119255066
I0129 09:02:11.462998 140026075662080 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.6237760782241821, loss=2.1679115295410156
I0129 09:02:45.200829 140026067269376 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.579241394996643, loss=2.1918933391571045
I0129 09:03:18.955691 140026075662080 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.2914378643035889, loss=1.976259469985962
I0129 09:03:52.676702 140026067269376 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3090673685073853, loss=1.9934710264205933
I0129 09:04:26.342775 140026075662080 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3468714952468872, loss=2.159113645553589
I0129 09:05:00.070320 140026067269376 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.3279266357421875, loss=2.0587408542633057
I0129 09:05:33.775003 140026075662080 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.6304057836532593, loss=2.184084892272949
I0129 09:05:43.374292 140187804313408 spec.py:321] Evaluating on the training split.
I0129 09:05:49.650950 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 09:05:58.708337 140187804313408 spec.py:349] Evaluating on the test split.
I0129 09:06:01.366143 140187804313408 submission_runner.py:408] Time since start: 27509.32s, 	Step: 78430, 	{'train/accuracy': 0.3382294178009033, 'train/loss': 3.221769094467163, 'validation/accuracy': 0.3022799789905548, 'validation/loss': 3.51043963432312, 'validation/num_examples': 50000, 'test/accuracy': 0.23030000925064087, 'test/loss': 4.192265510559082, 'test/num_examples': 10000, 'score': 26554.670390605927, 'total_duration': 27509.320876836777, 'accumulated_submission_time': 26554.670390605927, 'accumulated_eval_time': 948.8191111087799, 'accumulated_logging_time': 2.9777910709381104}
I0129 09:06:01.397683 140026159523584 logging_writer.py:48] [78430] accumulated_eval_time=948.819111, accumulated_logging_time=2.977791, accumulated_submission_time=26554.670391, global_step=78430, preemption_count=0, score=26554.670391, test/accuracy=0.230300, test/loss=4.192266, test/num_examples=10000, total_duration=27509.320877, train/accuracy=0.338229, train/loss=3.221769, validation/accuracy=0.302280, validation/loss=3.510440, validation/num_examples=50000
I0129 09:06:25.339597 140026167916288 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.4441193342208862, loss=2.1547844409942627
I0129 09:06:59.066877 140026159523584 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.5016021728515625, loss=2.178912401199341
I0129 09:07:32.731260 140026167916288 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.4093188047409058, loss=1.9926588535308838
I0129 09:08:06.579404 140026159523584 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.5055791139602661, loss=2.0865414142608643
I0129 09:08:40.318249 140026167916288 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.5282379388809204, loss=2.031914710998535
I0129 09:09:14.085005 140026159523584 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.3975796699523926, loss=2.2264671325683594
I0129 09:09:47.743691 140026167916288 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.3618272542953491, loss=2.13753342628479
I0129 09:10:21.551331 140026159523584 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.3569895029067993, loss=2.0088071823120117
I0129 09:10:55.228084 140026167916288 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.3555485010147095, loss=2.0555531978607178
I0129 09:11:28.932839 140026159523584 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.6291921138763428, loss=2.058314800262451
I0129 09:12:02.613772 140026167916288 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.4802430868148804, loss=2.1574978828430176
I0129 09:12:36.412898 140026159523584 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.5212433338165283, loss=2.119699001312256
I0129 09:13:10.129308 140026167916288 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.4720276594161987, loss=2.1696300506591797
I0129 09:13:43.900203 140026159523584 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.5521644353866577, loss=2.017399311065674
I0129 09:14:17.697381 140026167916288 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.4951212406158447, loss=2.01444149017334
I0129 09:14:31.653168 140187804313408 spec.py:321] Evaluating on the training split.
I0129 09:14:37.963056 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 09:14:46.662867 140187804313408 spec.py:349] Evaluating on the test split.
I0129 09:14:49.238893 140187804313408 submission_runner.py:408] Time since start: 28037.19s, 	Step: 79943, 	{'train/accuracy': 0.31275907158851624, 'train/loss': 3.484292507171631, 'validation/accuracy': 0.28571999073028564, 'validation/loss': 3.710571527481079, 'validation/num_examples': 50000, 'test/accuracy': 0.21790000796318054, 'test/loss': 4.380500316619873, 'test/num_examples': 10000, 'score': 27064.858870744705, 'total_duration': 28037.1936275959, 'accumulated_submission_time': 27064.858870744705, 'accumulated_eval_time': 966.4048013687134, 'accumulated_logging_time': 3.0196309089660645}
I0129 09:14:49.275323 140026050483968 logging_writer.py:48] [79943] accumulated_eval_time=966.404801, accumulated_logging_time=3.019631, accumulated_submission_time=27064.858871, global_step=79943, preemption_count=0, score=27064.858871, test/accuracy=0.217900, test/loss=4.380500, test/num_examples=10000, total_duration=28037.193628, train/accuracy=0.312759, train/loss=3.484293, validation/accuracy=0.285720, validation/loss=3.710572, validation/num_examples=50000
I0129 09:15:08.839092 140026058876672 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.4227226972579956, loss=1.9519295692443848
I0129 09:15:42.540727 140026050483968 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.4936357736587524, loss=2.0742223262786865
I0129 09:16:16.255646 140026058876672 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.4108566045761108, loss=2.0986015796661377
I0129 09:16:50.013483 140026050483968 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4178458452224731, loss=2.0747084617614746
I0129 09:17:23.765793 140026058876672 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.416206955909729, loss=2.081667900085449
I0129 09:17:57.530878 140026050483968 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.5765374898910522, loss=2.1208791732788086
I0129 09:18:31.261937 140026058876672 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.3329955339431763, loss=2.1366443634033203
I0129 09:19:04.940808 140026050483968 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.3882931470870972, loss=2.134371280670166
I0129 09:19:38.632600 140026058876672 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.4359890222549438, loss=2.0948941707611084
I0129 09:20:12.295403 140026050483968 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.566442847251892, loss=2.006352424621582
I0129 09:20:46.118828 140026058876672 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.3770524263381958, loss=2.0392379760742188
I0129 09:21:19.809087 140026050483968 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.3998007774353027, loss=1.9340249300003052
I0129 09:21:53.501212 140026058876672 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.5385336875915527, loss=1.9245721101760864
I0129 09:22:27.187786 140026050483968 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.6824299097061157, loss=1.9748731851577759
I0129 09:23:00.903536 140026058876672 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.5446503162384033, loss=2.0981638431549072
I0129 09:23:19.569284 140187804313408 spec.py:321] Evaluating on the training split.
I0129 09:23:25.814302 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 09:23:34.613879 140187804313408 spec.py:349] Evaluating on the test split.
I0129 09:23:37.249021 140187804313408 submission_runner.py:408] Time since start: 28565.20s, 	Step: 81457, 	{'train/accuracy': 0.09078045189380646, 'train/loss': 6.651303291320801, 'validation/accuracy': 0.08349999785423279, 'validation/loss': 6.782551288604736, 'validation/num_examples': 50000, 'test/accuracy': 0.05780000239610672, 'test/loss': 7.317497253417969, 'test/num_examples': 10000, 'score': 27575.083287000656, 'total_duration': 28565.20376110077, 'accumulated_submission_time': 27575.083287000656, 'accumulated_eval_time': 984.0845103263855, 'accumulated_logging_time': 3.068606376647949}
I0129 09:23:37.292060 140026151130880 logging_writer.py:48] [81457] accumulated_eval_time=984.084510, accumulated_logging_time=3.068606, accumulated_submission_time=27575.083287, global_step=81457, preemption_count=0, score=27575.083287, test/accuracy=0.057800, test/loss=7.317497, test/num_examples=10000, total_duration=28565.203761, train/accuracy=0.090780, train/loss=6.651303, validation/accuracy=0.083500, validation/loss=6.782551, validation/num_examples=50000
I0129 09:23:52.141014 140026159523584 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.5138155221939087, loss=2.078756809234619
I0129 09:24:25.836868 140026151130880 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.4865076541900635, loss=2.0128748416900635
I0129 09:24:59.544315 140026159523584 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.4802336692810059, loss=2.1657447814941406
I0129 09:25:33.202593 140026151130880 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.465431809425354, loss=2.0496389865875244
I0129 09:26:06.915377 140026159523584 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.456917643547058, loss=2.046449661254883
I0129 09:26:40.591537 140026151130880 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.418043851852417, loss=2.102522134780884
I0129 09:27:14.400829 140026159523584 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.4751173257827759, loss=2.005617141723633
I0129 09:27:48.146135 140026151130880 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.42890465259552, loss=2.0851986408233643
I0129 09:28:21.863288 140026159523584 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.5640724897384644, loss=2.0673654079437256
I0129 09:28:55.615057 140026151130880 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.5680190324783325, loss=2.0343551635742188
I0129 09:29:29.354040 140026159523584 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.5129371881484985, loss=2.0446996688842773
I0129 09:30:03.111614 140026151130880 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.4523375034332275, loss=1.8972678184509277
I0129 09:30:36.857679 140026159523584 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.6938318014144897, loss=2.222435474395752
I0129 09:31:10.496112 140026151130880 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.4504960775375366, loss=2.027157783508301
I0129 09:31:44.196098 140026159523584 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.6107962131500244, loss=2.1163458824157715
I0129 09:32:07.575222 140187804313408 spec.py:321] Evaluating on the training split.
I0129 09:32:13.930228 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 09:32:22.609419 140187804313408 spec.py:349] Evaluating on the test split.
I0129 09:32:25.282770 140187804313408 submission_runner.py:408] Time since start: 29093.24s, 	Step: 82971, 	{'train/accuracy': 0.32892218232154846, 'train/loss': 3.3418257236480713, 'validation/accuracy': 0.3080599904060364, 'validation/loss': 3.4806392192840576, 'validation/num_examples': 50000, 'test/accuracy': 0.23200000822544098, 'test/loss': 4.121487140655518, 'test/num_examples': 10000, 'score': 28085.3017513752, 'total_duration': 29093.237498044968, 'accumulated_submission_time': 28085.3017513752, 'accumulated_eval_time': 1001.7920260429382, 'accumulated_logging_time': 3.1208367347717285}
I0129 09:32:25.314589 140026042091264 logging_writer.py:48] [82971] accumulated_eval_time=1001.792026, accumulated_logging_time=3.120837, accumulated_submission_time=28085.301751, global_step=82971, preemption_count=0, score=28085.301751, test/accuracy=0.232000, test/loss=4.121487, test/num_examples=10000, total_duration=29093.237498, train/accuracy=0.328922, train/loss=3.341826, validation/accuracy=0.308060, validation/loss=3.480639, validation/num_examples=50000
I0129 09:32:35.447736 140026050483968 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.6472619771957397, loss=2.109060525894165
I0129 09:33:09.139060 140026042091264 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.4160021543502808, loss=2.0691003799438477
I0129 09:33:42.960780 140026050483968 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.8149973154067993, loss=2.0719785690307617
I0129 09:34:16.670519 140026042091264 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.521016001701355, loss=2.002103090286255
I0129 09:34:50.383715 140026050483968 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.445712924003601, loss=2.1041793823242188
I0129 09:35:24.119015 140026042091264 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.5798250436782837, loss=2.1095502376556396
I0129 09:35:57.836797 140026050483968 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.4580895900726318, loss=2.0918478965759277
I0129 09:36:31.590025 140026042091264 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.6269011497497559, loss=2.104037284851074
I0129 09:37:05.268976 140026050483968 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.5679231882095337, loss=2.0046844482421875
I0129 09:37:38.949388 140026042091264 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.3916674852371216, loss=2.0094692707061768
I0129 09:38:12.729822 140026050483968 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.4843066930770874, loss=2.031961441040039
I0129 09:38:46.409922 140026042091264 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.3761428594589233, loss=2.054888963699341
I0129 09:39:20.122498 140026050483968 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.602177381515503, loss=2.0217554569244385
I0129 09:39:53.899359 140026042091264 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.5529063940048218, loss=2.1229686737060547
I0129 09:40:27.649417 140026050483968 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.5518560409545898, loss=2.0323326587677
I0129 09:40:55.449300 140187804313408 spec.py:321] Evaluating on the training split.
I0129 09:41:01.674888 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 09:41:10.572137 140187804313408 spec.py:349] Evaluating on the test split.
I0129 09:41:13.227345 140187804313408 submission_runner.py:408] Time since start: 29621.18s, 	Step: 84484, 	{'train/accuracy': 0.39504942297935486, 'train/loss': 2.8487935066223145, 'validation/accuracy': 0.36965999007225037, 'validation/loss': 3.0001261234283447, 'validation/num_examples': 50000, 'test/accuracy': 0.26980000734329224, 'test/loss': 3.844083786010742, 'test/num_examples': 10000, 'score': 28595.369776010513, 'total_duration': 29621.18208193779, 'accumulated_submission_time': 28595.369776010513, 'accumulated_eval_time': 1019.5700325965881, 'accumulated_logging_time': 3.163076639175415}
I0129 09:41:13.263395 140026159523584 logging_writer.py:48] [84484] accumulated_eval_time=1019.570033, accumulated_logging_time=3.163077, accumulated_submission_time=28595.369776, global_step=84484, preemption_count=0, score=28595.369776, test/accuracy=0.269800, test/loss=3.844084, test/num_examples=10000, total_duration=29621.182082, train/accuracy=0.395049, train/loss=2.848794, validation/accuracy=0.369660, validation/loss=3.000126, validation/num_examples=50000
I0129 09:41:19.022711 140026167916288 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.4653998613357544, loss=2.058901071548462
I0129 09:41:52.749834 140026159523584 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.420952558517456, loss=2.1226160526275635
I0129 09:42:26.421798 140026167916288 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.5838258266448975, loss=2.06666898727417
I0129 09:43:00.121710 140026159523584 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.4585856199264526, loss=2.0020265579223633
I0129 09:43:33.836860 140026167916288 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.3552836179733276, loss=1.9708329439163208
I0129 09:44:07.571526 140026159523584 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.5677430629730225, loss=2.1023364067077637
I0129 09:44:41.325390 140026167916288 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.3985406160354614, loss=2.0476620197296143
I0129 09:45:15.048818 140026159523584 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.4204200506210327, loss=2.010244846343994
I0129 09:45:48.721515 140026167916288 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.418997049331665, loss=2.0948519706726074
I0129 09:46:22.558977 140026159523584 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.3710209131240845, loss=1.9769208431243896
I0129 09:46:56.292397 140026167916288 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.452947735786438, loss=2.053614854812622
I0129 09:47:29.988630 140026159523584 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.4656133651733398, loss=1.9994962215423584
I0129 09:48:03.742899 140026167916288 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.4523701667785645, loss=2.091430425643921
I0129 09:48:37.472127 140026159523584 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.52901291847229, loss=2.122190237045288
I0129 09:49:11.180827 140026167916288 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.4105496406555176, loss=2.084815740585327
I0129 09:49:43.401265 140187804313408 spec.py:321] Evaluating on the training split.
I0129 09:49:49.637119 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 09:49:58.302765 140187804313408 spec.py:349] Evaluating on the test split.
I0129 09:50:00.925524 140187804313408 submission_runner.py:408] Time since start: 30148.88s, 	Step: 85997, 	{'train/accuracy': 0.14728157222270966, 'train/loss': 5.7417826652526855, 'validation/accuracy': 0.13531999289989471, 'validation/loss': 5.94066047668457, 'validation/num_examples': 50000, 'test/accuracy': 0.101500004529953, 'test/loss': 6.495641708374023, 'test/num_examples': 10000, 'score': 29105.444316864014, 'total_duration': 30148.880268096924, 'accumulated_submission_time': 29105.444316864014, 'accumulated_eval_time': 1037.0942661762238, 'accumulated_logging_time': 3.2089014053344727}
I0129 09:50:00.958312 140026050483968 logging_writer.py:48] [85997] accumulated_eval_time=1037.094266, accumulated_logging_time=3.208901, accumulated_submission_time=29105.444317, global_step=85997, preemption_count=0, score=29105.444317, test/accuracy=0.101500, test/loss=6.495642, test/num_examples=10000, total_duration=30148.880268, train/accuracy=0.147282, train/loss=5.741783, validation/accuracy=0.135320, validation/loss=5.940660, validation/num_examples=50000
I0129 09:50:02.325348 140026058876672 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.45077645778656, loss=2.003492593765259
I0129 09:50:36.083437 140026050483968 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.4590590000152588, loss=1.9968281984329224
I0129 09:51:09.751830 140026058876672 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.7304424047470093, loss=2.1699748039245605
I0129 09:51:43.445642 140026050483968 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.6307049989700317, loss=2.038167953491211
I0129 09:52:17.129274 140026058876672 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.5178050994873047, loss=2.1203787326812744
I0129 09:52:50.988694 140026050483968 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.5330506563186646, loss=2.0873966217041016
I0129 09:53:24.726480 140026058876672 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.7576406002044678, loss=2.1213135719299316
I0129 09:53:58.521084 140026050483968 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.4468166828155518, loss=2.1831493377685547
I0129 09:54:32.188990 140026058876672 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.4992183446884155, loss=2.0473999977111816
I0129 09:55:05.880796 140026050483968 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.9072661399841309, loss=2.002314567565918
I0129 09:55:39.569516 140026058876672 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.590157151222229, loss=2.029341220855713
I0129 09:56:13.367312 140026050483968 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.5228575468063354, loss=2.1137993335723877
I0129 09:56:47.113648 140026058876672 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.4620229005813599, loss=2.064629077911377
I0129 09:57:20.860485 140026050483968 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.5128943920135498, loss=2.0341920852661133
I0129 09:57:54.573716 140026058876672 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.612511157989502, loss=2.0759291648864746
I0129 09:58:28.351809 140026050483968 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.5596730709075928, loss=2.0890839099884033
I0129 09:58:31.214605 140187804313408 spec.py:321] Evaluating on the training split.
I0129 09:58:37.506159 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 09:58:46.447170 140187804313408 spec.py:349] Evaluating on the test split.
I0129 09:58:49.096883 140187804313408 submission_runner.py:408] Time since start: 30677.05s, 	Step: 87510, 	{'train/accuracy': 0.38759565353393555, 'train/loss': 2.9370288848876953, 'validation/accuracy': 0.35019999742507935, 'validation/loss': 3.214970588684082, 'validation/num_examples': 50000, 'test/accuracy': 0.26420000195503235, 'test/loss': 4.0185370445251465, 'test/num_examples': 10000, 'score': 29615.636449813843, 'total_duration': 30677.051599264145, 'accumulated_submission_time': 29615.636449813843, 'accumulated_eval_time': 1054.9764783382416, 'accumulated_logging_time': 3.251824378967285}
I0129 09:58:49.142943 140026159523584 logging_writer.py:48] [87510] accumulated_eval_time=1054.976478, accumulated_logging_time=3.251824, accumulated_submission_time=29615.636450, global_step=87510, preemption_count=0, score=29615.636450, test/accuracy=0.264200, test/loss=4.018537, test/num_examples=10000, total_duration=30677.051599, train/accuracy=0.387596, train/loss=2.937029, validation/accuracy=0.350200, validation/loss=3.214971, validation/num_examples=50000
I0129 09:59:19.886001 140026167916288 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.3868958950042725, loss=1.9988573789596558
I0129 09:59:53.631306 140026159523584 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.4840351343154907, loss=2.1334617137908936
I0129 10:00:27.309144 140026167916288 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.593968391418457, loss=2.026662826538086
I0129 10:01:01.036081 140026159523584 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.55000638961792, loss=2.0731091499328613
I0129 10:01:34.749286 140026167916288 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.4763292074203491, loss=2.1177988052368164
I0129 10:02:08.504020 140026159523584 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.5890157222747803, loss=2.1386613845825195
I0129 10:02:42.156038 140026167916288 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.5560829639434814, loss=2.0925002098083496
I0129 10:03:15.867910 140026159523584 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.4557149410247803, loss=1.9797539710998535
I0129 10:03:49.612600 140026167916288 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.5249112844467163, loss=2.12082576751709
I0129 10:04:23.351395 140026159523584 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.6425771713256836, loss=2.1560816764831543
I0129 10:04:57.030000 140026167916288 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.4427411556243896, loss=1.9103450775146484
I0129 10:05:30.727388 140026159523584 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4309104681015015, loss=1.9632197618484497
I0129 10:06:04.513417 140026167916288 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.557913899421692, loss=2.13893461227417
I0129 10:06:38.239295 140026159523584 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.472845435142517, loss=2.034438133239746
I0129 10:07:11.902507 140026167916288 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.4801775217056274, loss=2.040280818939209
I0129 10:07:19.138177 140187804313408 spec.py:321] Evaluating on the training split.
I0129 10:07:25.381028 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 10:07:34.032959 140187804313408 spec.py:349] Evaluating on the test split.
I0129 10:07:36.728766 140187804313408 submission_runner.py:408] Time since start: 31204.68s, 	Step: 89023, 	{'train/accuracy': 0.40361925959587097, 'train/loss': 2.7422826290130615, 'validation/accuracy': 0.3774600028991699, 'validation/loss': 2.9350979328155518, 'validation/num_examples': 50000, 'test/accuracy': 0.2768000066280365, 'test/loss': 3.74910569190979, 'test/num_examples': 10000, 'score': 30125.565993785858, 'total_duration': 31204.683507680893, 'accumulated_submission_time': 30125.565993785858, 'accumulated_eval_time': 1072.5670273303986, 'accumulated_logging_time': 3.3075568675994873}
I0129 10:07:36.766754 140026067269376 logging_writer.py:48] [89023] accumulated_eval_time=1072.567027, accumulated_logging_time=3.307557, accumulated_submission_time=30125.565994, global_step=89023, preemption_count=0, score=30125.565994, test/accuracy=0.276800, test/loss=3.749106, test/num_examples=10000, total_duration=31204.683508, train/accuracy=0.403619, train/loss=2.742283, validation/accuracy=0.377460, validation/loss=2.935098, validation/num_examples=50000
I0129 10:08:03.073112 140026075662080 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.47420072555542, loss=1.9603978395462036
I0129 10:08:36.759901 140026067269376 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.6663528680801392, loss=2.086374282836914
I0129 10:09:10.436432 140026075662080 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.6159216165542603, loss=2.063809394836426
I0129 10:09:44.157082 140026067269376 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.7135200500488281, loss=2.0922276973724365
I0129 10:10:17.835189 140026075662080 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.5767292976379395, loss=2.0526204109191895
I0129 10:10:51.530908 140026067269376 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.595011830329895, loss=2.0905325412750244
I0129 10:11:25.173570 140026075662080 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.494397759437561, loss=1.94020414352417
I0129 10:11:58.960043 140026067269376 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.5510026216506958, loss=2.0720560550689697
I0129 10:12:32.780126 140026075662080 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.630365014076233, loss=2.1109023094177246
I0129 10:13:06.502658 140026067269376 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.556666612625122, loss=2.1310060024261475
I0129 10:13:40.171744 140026075662080 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6123126745224, loss=2.1560065746307373
I0129 10:14:13.971427 140026067269376 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.6050506830215454, loss=2.113293409347534
I0129 10:14:47.631464 140026075662080 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.521092414855957, loss=2.117784261703491
I0129 10:15:21.387157 140026067269376 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.555742621421814, loss=1.9370687007904053
I0129 10:15:55.096636 140026075662080 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.5586631298065186, loss=2.034836769104004
I0129 10:16:07.058153 140187804313408 spec.py:321] Evaluating on the training split.
I0129 10:16:13.308972 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 10:16:22.155034 140187804313408 spec.py:349] Evaluating on the test split.
I0129 10:16:24.699984 140187804313408 submission_runner.py:408] Time since start: 31732.65s, 	Step: 90537, 	{'train/accuracy': 0.2742745578289032, 'train/loss': 3.888530969619751, 'validation/accuracy': 0.26151999831199646, 'validation/loss': 4.029567241668701, 'validation/num_examples': 50000, 'test/accuracy': 0.18450000882148743, 'test/loss': 4.86121940612793, 'test/num_examples': 10000, 'score': 30635.79352426529, 'total_duration': 31732.65471124649, 'accumulated_submission_time': 30635.79352426529, 'accumulated_eval_time': 1090.2088098526, 'accumulated_logging_time': 3.3545897006988525}
I0129 10:16:24.732952 140026058876672 logging_writer.py:48] [90537] accumulated_eval_time=1090.208810, accumulated_logging_time=3.354590, accumulated_submission_time=30635.793524, global_step=90537, preemption_count=0, score=30635.793524, test/accuracy=0.184500, test/loss=4.861219, test/num_examples=10000, total_duration=31732.654711, train/accuracy=0.274275, train/loss=3.888531, validation/accuracy=0.261520, validation/loss=4.029567, validation/num_examples=50000
I0129 10:16:46.316714 140026067269376 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.4980043172836304, loss=1.9919856786727905
I0129 10:17:20.025076 140026058876672 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.5402615070343018, loss=2.0903449058532715
I0129 10:17:53.712060 140026067269376 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.614395022392273, loss=1.9559414386749268
I0129 10:18:27.418660 140026058876672 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.4292606115341187, loss=1.9746041297912598
I0129 10:19:01.251648 140026067269376 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.5421279668807983, loss=1.9671533107757568
I0129 10:19:34.985859 140026058876672 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.8017325401306152, loss=2.0243849754333496
I0129 10:20:08.729800 140026067269376 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.5959198474884033, loss=2.038837432861328
I0129 10:20:42.507329 140026058876672 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.5324695110321045, loss=2.0726678371429443
I0129 10:21:16.256503 140026067269376 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.6610993146896362, loss=2.0062880516052246
I0129 10:21:50.028140 140026058876672 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.4417800903320312, loss=1.9147639274597168
I0129 10:22:23.735416 140026067269376 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.5242538452148438, loss=2.0547256469726562
I0129 10:22:57.412647 140026058876672 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.5741149187088013, loss=2.090257167816162
I0129 10:23:31.165857 140026067269376 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.5798636674880981, loss=2.119345188140869
I0129 10:24:04.925064 140026058876672 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.5107786655426025, loss=2.0450968742370605
I0129 10:24:38.656561 140026067269376 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.6567602157592773, loss=2.0333735942840576
I0129 10:24:54.964527 140187804313408 spec.py:321] Evaluating on the training split.
I0129 10:25:01.969186 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 10:25:10.733771 140187804313408 spec.py:349] Evaluating on the test split.
I0129 10:25:13.395757 140187804313408 submission_runner.py:408] Time since start: 32261.35s, 	Step: 92050, 	{'train/accuracy': 0.3800821006298065, 'train/loss': 2.910261869430542, 'validation/accuracy': 0.3625999987125397, 'validation/loss': 3.03848934173584, 'validation/num_examples': 50000, 'test/accuracy': 0.25760000944137573, 'test/loss': 3.8726961612701416, 'test/num_examples': 10000, 'score': 31145.958006620407, 'total_duration': 32261.35049700737, 'accumulated_submission_time': 31145.958006620407, 'accumulated_eval_time': 1108.64000082016, 'accumulated_logging_time': 3.3997135162353516}
I0129 10:25:13.433008 140026151130880 logging_writer.py:48] [92050] accumulated_eval_time=1108.640001, accumulated_logging_time=3.399714, accumulated_submission_time=31145.958007, global_step=92050, preemption_count=0, score=31145.958007, test/accuracy=0.257600, test/loss=3.872696, test/num_examples=10000, total_duration=32261.350497, train/accuracy=0.380082, train/loss=2.910262, validation/accuracy=0.362600, validation/loss=3.038489, validation/num_examples=50000
I0129 10:25:31.810660 140026167916288 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.5979101657867432, loss=2.0583086013793945
I0129 10:26:05.468248 140026151130880 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.4791922569274902, loss=1.9666204452514648
I0129 10:26:39.188800 140026167916288 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.4823887348175049, loss=1.9680768251419067
I0129 10:27:12.860619 140026151130880 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.7898986339569092, loss=1.9856430292129517
I0129 10:27:46.565431 140026167916288 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.465324878692627, loss=1.9975346326828003
I0129 10:28:20.251495 140026151130880 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.6253811120986938, loss=1.8963537216186523
I0129 10:28:54.071872 140026167916288 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.6296297311782837, loss=2.026041269302368
I0129 10:29:27.715217 140026151130880 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.5574156045913696, loss=1.9253473281860352
I0129 10:30:01.401302 140026167916288 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.5774784088134766, loss=2.1069397926330566
I0129 10:30:35.056201 140026151130880 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.698430061340332, loss=2.103976011276245
I0129 10:31:08.759622 140026167916288 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.5845543146133423, loss=1.9008797407150269
I0129 10:31:42.498279 140026151130880 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.6215537786483765, loss=2.0037550926208496
I0129 10:32:16.304461 140026167916288 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.5261528491973877, loss=1.913012146949768
I0129 10:32:49.997488 140026151130880 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.6089063882827759, loss=2.1089446544647217
I0129 10:33:23.791356 140026167916288 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.7632523775100708, loss=1.9293773174285889
I0129 10:33:43.479061 140187804313408 spec.py:321] Evaluating on the training split.
I0129 10:33:49.745087 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 10:33:58.595473 140187804313408 spec.py:349] Evaluating on the test split.
I0129 10:34:01.214273 140187804313408 submission_runner.py:408] Time since start: 32789.17s, 	Step: 93560, 	{'train/accuracy': 0.43335458636283875, 'train/loss': 2.672496795654297, 'validation/accuracy': 0.4023999869823456, 'validation/loss': 2.8671212196350098, 'validation/num_examples': 50000, 'test/accuracy': 0.3085000216960907, 'test/loss': 3.6801254749298096, 'test/num_examples': 10000, 'score': 31654.82076215744, 'total_duration': 32789.16884326935, 'accumulated_submission_time': 31654.82076215744, 'accumulated_eval_time': 1126.3750030994415, 'accumulated_logging_time': 4.565646648406982}
I0129 10:34:01.252038 140026058876672 logging_writer.py:48] [93560] accumulated_eval_time=1126.375003, accumulated_logging_time=4.565647, accumulated_submission_time=31654.820762, global_step=93560, preemption_count=0, score=31654.820762, test/accuracy=0.308500, test/loss=3.680125, test/num_examples=10000, total_duration=32789.168843, train/accuracy=0.433355, train/loss=2.672497, validation/accuracy=0.402400, validation/loss=2.867121, validation/num_examples=50000
I0129 10:34:15.102574 140026067269376 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.733530044555664, loss=1.9525439739227295
I0129 10:34:48.791060 140026058876672 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.5272053480148315, loss=1.9697659015655518
I0129 10:35:22.579753 140026067269376 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.6428005695343018, loss=2.027907371520996
I0129 10:35:56.266202 140026058876672 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.514719843864441, loss=1.8695158958435059
I0129 10:36:30.042165 140026067269376 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.6821478605270386, loss=2.1272754669189453
I0129 10:37:03.727274 140026058876672 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.5771284103393555, loss=1.9825294017791748
I0129 10:37:37.432530 140026067269376 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.6156055927276611, loss=2.026686429977417
I0129 10:38:11.234296 140026058876672 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.9560025930404663, loss=1.9997376203536987
I0129 10:38:45.008453 140026067269376 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.6711770296096802, loss=1.8791921138763428
I0129 10:39:18.740497 140026058876672 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.5473792552947998, loss=2.0019235610961914
I0129 10:39:52.496864 140026067269376 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.4405200481414795, loss=1.940779447555542
I0129 10:40:26.185378 140026058876672 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.564656376838684, loss=1.9260441064834595
I0129 10:40:59.988720 140026067269376 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.627646565437317, loss=2.054506778717041
I0129 10:41:33.670073 140026058876672 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.7858326435089111, loss=1.9306597709655762
I0129 10:42:07.437136 140026067269376 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.6305320262908936, loss=2.035200834274292
I0129 10:42:31.546295 140187804313408 spec.py:321] Evaluating on the training split.
I0129 10:42:37.733731 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 10:42:46.841261 140187804313408 spec.py:349] Evaluating on the test split.
I0129 10:42:49.345616 140187804313408 submission_runner.py:408] Time since start: 33317.30s, 	Step: 95073, 	{'train/accuracy': 0.31594786047935486, 'train/loss': 3.5270910263061523, 'validation/accuracy': 0.2946999967098236, 'validation/loss': 3.6912097930908203, 'validation/num_examples': 50000, 'test/accuracy': 0.21230001747608185, 'test/loss': 4.5419158935546875, 'test/num_examples': 10000, 'score': 32165.047719717026, 'total_duration': 33317.30035114288, 'accumulated_submission_time': 32165.047719717026, 'accumulated_eval_time': 1144.1742820739746, 'accumulated_logging_time': 4.614187479019165}
I0129 10:42:49.380141 140026159523584 logging_writer.py:48] [95073] accumulated_eval_time=1144.174282, accumulated_logging_time=4.614187, accumulated_submission_time=32165.047720, global_step=95073, preemption_count=0, score=32165.047720, test/accuracy=0.212300, test/loss=4.541916, test/num_examples=10000, total_duration=33317.300351, train/accuracy=0.315948, train/loss=3.527091, validation/accuracy=0.294700, validation/loss=3.691210, validation/num_examples=50000
I0129 10:42:58.808066 140026167916288 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.5370255708694458, loss=2.01818585395813
I0129 10:43:32.505736 140026159523584 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.5258939266204834, loss=1.8927286863327026
I0129 10:44:06.182657 140026167916288 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.5939781665802002, loss=2.043201446533203
I0129 10:44:39.955348 140026159523584 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.8133865594863892, loss=1.9331185817718506
I0129 10:45:13.690433 140026167916288 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.7065186500549316, loss=1.9458801746368408
I0129 10:45:47.384555 140026159523584 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.6639792919158936, loss=1.9808008670806885
I0129 10:46:21.137236 140026167916288 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.5065144300460815, loss=1.988494634628296
I0129 10:46:54.839674 140026159523584 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.6564640998840332, loss=2.035231828689575
I0129 10:47:28.618277 140026167916288 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.5483099222183228, loss=1.9132905006408691
I0129 10:48:02.349058 140026159523584 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.7374762296676636, loss=2.1549441814422607
I0129 10:48:36.010561 140026167916288 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.878672480583191, loss=2.2284412384033203
I0129 10:49:09.738304 140026159523584 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.7551193237304688, loss=2.0516891479492188
I0129 10:49:43.494871 140026167916288 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.7363249063491821, loss=2.0577545166015625
I0129 10:50:17.222628 140026159523584 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.5486712455749512, loss=1.8973952531814575
I0129 10:50:50.995685 140026167916288 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.6755599975585938, loss=1.9899638891220093
I0129 10:51:19.579501 140187804313408 spec.py:321] Evaluating on the training split.
I0129 10:51:25.803470 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 10:51:34.781434 140187804313408 spec.py:349] Evaluating on the test split.
I0129 10:51:37.478863 140187804313408 submission_runner.py:408] Time since start: 33845.43s, 	Step: 96586, 	{'train/accuracy': 0.37107381224632263, 'train/loss': 2.9733757972717285, 'validation/accuracy': 0.33789998292922974, 'validation/loss': 3.213762044906616, 'validation/num_examples': 50000, 'test/accuracy': 0.25, 'test/loss': 4.006485462188721, 'test/num_examples': 10000, 'score': 32675.180696725845, 'total_duration': 33845.43358707428, 'accumulated_submission_time': 32675.180696725845, 'accumulated_eval_time': 1162.0735852718353, 'accumulated_logging_time': 4.659753799438477}
I0129 10:51:37.513957 140026067269376 logging_writer.py:48] [96586] accumulated_eval_time=1162.073585, accumulated_logging_time=4.659754, accumulated_submission_time=32675.180697, global_step=96586, preemption_count=0, score=32675.180697, test/accuracy=0.250000, test/loss=4.006485, test/num_examples=10000, total_duration=33845.433587, train/accuracy=0.371074, train/loss=2.973376, validation/accuracy=0.337900, validation/loss=3.213762, validation/num_examples=50000
I0129 10:51:42.593289 140026075662080 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.663458228111267, loss=2.03947377204895
I0129 10:52:16.334949 140026067269376 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.8062145709991455, loss=2.095801830291748
I0129 10:52:49.977479 140026075662080 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.7295112609863281, loss=1.8818020820617676
I0129 10:53:23.724263 140026067269376 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.6065863370895386, loss=2.0028433799743652
I0129 10:53:57.422676 140026075662080 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.6217637062072754, loss=1.9947960376739502
I0129 10:54:31.121019 140026067269376 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.5704213380813599, loss=1.9553468227386475
I0129 10:55:04.865818 140026075662080 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.9049583673477173, loss=1.8902018070220947
I0129 10:55:38.599718 140026067269376 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.6864244937896729, loss=1.9339839220046997
I0129 10:56:12.273074 140026075662080 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.6592471599578857, loss=1.9866931438446045
I0129 10:56:46.087328 140026067269376 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.704461693763733, loss=1.8315882682800293
I0129 10:57:19.837217 140026075662080 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.6794434785842896, loss=2.073607921600342
I0129 10:57:53.678727 140026067269376 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.5886038541793823, loss=1.93344247341156
I0129 10:58:27.370084 140026075662080 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.6668046712875366, loss=2.0413942337036133
I0129 10:59:01.053203 140026067269376 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.7024770975112915, loss=1.8632776737213135
I0129 10:59:34.801196 140026075662080 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.6994513273239136, loss=2.1098062992095947
I0129 11:00:07.673585 140187804313408 spec.py:321] Evaluating on the training split.
I0129 11:00:13.894706 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 11:00:22.706306 140187804313408 spec.py:349] Evaluating on the test split.
I0129 11:00:25.317737 140187804313408 submission_runner.py:408] Time since start: 34373.27s, 	Step: 98099, 	{'train/accuracy': 0.4164939224720001, 'train/loss': 2.686826467514038, 'validation/accuracy': 0.3877999782562256, 'validation/loss': 2.8848915100097656, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.734477996826172, 'test/num_examples': 10000, 'score': 33185.274107694626, 'total_duration': 34373.27248048782, 'accumulated_submission_time': 33185.274107694626, 'accumulated_eval_time': 1179.7177047729492, 'accumulated_logging_time': 4.704460382461548}
I0129 11:00:25.358259 140026058876672 logging_writer.py:48] [98099] accumulated_eval_time=1179.717705, accumulated_logging_time=4.704460, accumulated_submission_time=33185.274108, global_step=98099, preemption_count=0, score=33185.274108, test/accuracy=0.286500, test/loss=3.734478, test/num_examples=10000, total_duration=34373.272480, train/accuracy=0.416494, train/loss=2.686826, validation/accuracy=0.387800, validation/loss=2.884892, validation/num_examples=50000
I0129 11:00:26.048055 140026159523584 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.5785273313522339, loss=1.9937291145324707
I0129 11:00:59.783427 140026058876672 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.6987053155899048, loss=2.0846662521362305
I0129 11:01:33.505574 140026159523584 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.7082037925720215, loss=2.005275011062622
I0129 11:02:07.153382 140026058876672 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.9523347616195679, loss=1.8435372114181519
I0129 11:02:40.869415 140026159523584 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.6204824447631836, loss=1.9869407415390015
I0129 11:03:14.551296 140026058876672 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.6450438499450684, loss=2.039217948913574
I0129 11:03:48.225407 140026159523584 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.7408019304275513, loss=1.9277998208999634
I0129 11:04:22.052345 140026058876672 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.648264765739441, loss=2.051020622253418
I0129 11:04:55.791778 140026159523584 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.6191691160202026, loss=2.0324738025665283
I0129 11:05:29.558681 140026058876672 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.008220672607422, loss=1.8808389902114868
I0129 11:06:03.284800 140026159523584 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.639884114265442, loss=1.8408761024475098
I0129 11:06:37.039344 140026058876672 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.8496979475021362, loss=1.9398716688156128
I0129 11:07:10.755846 140026159523584 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.061950206756592, loss=2.148026704788208
I0129 11:07:44.527435 140026058876672 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.580055594444275, loss=1.9156450033187866
I0129 11:08:18.237900 140026159523584 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.7422924041748047, loss=2.0216758251190186
I0129 11:08:51.905438 140026058876672 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.5980931520462036, loss=1.9022235870361328
I0129 11:08:55.432786 140187804313408 spec.py:321] Evaluating on the training split.
I0129 11:09:01.887572 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 11:09:10.859644 140187804313408 spec.py:349] Evaluating on the test split.
I0129 11:09:13.382142 140187804313408 submission_runner.py:408] Time since start: 34901.34s, 	Step: 99612, 	{'train/accuracy': 0.5056201815605164, 'train/loss': 2.150707960128784, 'validation/accuracy': 0.4710799753665924, 'validation/loss': 2.3676950931549072, 'validation/num_examples': 50000, 'test/accuracy': 0.3606000244617462, 'test/loss': 3.1262447834014893, 'test/num_examples': 10000, 'score': 33695.28577399254, 'total_duration': 34901.336842536926, 'accumulated_submission_time': 33695.28577399254, 'accumulated_eval_time': 1197.6669921875, 'accumulated_logging_time': 4.754195690155029}
I0129 11:09:13.422898 140026167916288 logging_writer.py:48] [99612] accumulated_eval_time=1197.666992, accumulated_logging_time=4.754196, accumulated_submission_time=33695.285774, global_step=99612, preemption_count=0, score=33695.285774, test/accuracy=0.360600, test/loss=3.126245, test/num_examples=10000, total_duration=34901.336843, train/accuracy=0.505620, train/loss=2.150708, validation/accuracy=0.471080, validation/loss=2.367695, validation/num_examples=50000
I0129 11:09:43.391582 140026176308992 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.6565786600112915, loss=1.9637129306793213
I0129 11:10:17.118662 140026167916288 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.7521017789840698, loss=1.9744348526000977
I0129 11:10:50.969907 140026176308992 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.7657912969589233, loss=1.9371302127838135
I0129 11:11:24.671966 140026167916288 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.6259194612503052, loss=1.96083402633667
I0129 11:11:58.324832 140026176308992 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.771366834640503, loss=1.919691562652588
I0129 11:12:32.045297 140026167916288 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.846967339515686, loss=1.897580862045288
I0129 11:13:05.790726 140026176308992 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.67217218875885, loss=1.9434988498687744
I0129 11:13:39.532447 140026167916288 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.619815468788147, loss=1.8780264854431152
I0129 11:14:13.280643 140026176308992 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.769270420074463, loss=1.94285249710083
I0129 11:14:47.001471 140026167916288 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.8066633939743042, loss=1.8425145149230957
I0129 11:15:20.779329 140026176308992 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.6812422275543213, loss=1.8823795318603516
I0129 11:15:54.496785 140026167916288 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.4336867332458496, loss=1.990289330482483
I0129 11:16:28.226090 140026176308992 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.6621911525726318, loss=1.9981974363327026
I0129 11:17:02.153196 140026167916288 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.751847505569458, loss=1.941283106803894
I0129 11:17:35.891072 140026176308992 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.8342726230621338, loss=2.0424327850341797
I0129 11:17:43.457338 140187804313408 spec.py:321] Evaluating on the training split.
I0129 11:17:49.814985 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 11:17:58.762749 140187804313408 spec.py:349] Evaluating on the test split.
I0129 11:18:01.619071 140187804313408 submission_runner.py:408] Time since start: 35429.57s, 	Step: 101124, 	{'train/accuracy': 0.44365832209587097, 'train/loss': 2.589078903198242, 'validation/accuracy': 0.41331997513771057, 'validation/loss': 2.795851230621338, 'validation/num_examples': 50000, 'test/accuracy': 0.313400000333786, 'test/loss': 3.591003179550171, 'test/num_examples': 10000, 'score': 34205.252484321594, 'total_duration': 35429.5738132, 'accumulated_submission_time': 34205.252484321594, 'accumulated_eval_time': 1215.8287003040314, 'accumulated_logging_time': 4.804080247879028}
I0129 11:18:01.661491 140026050483968 logging_writer.py:48] [101124] accumulated_eval_time=1215.828700, accumulated_logging_time=4.804080, accumulated_submission_time=34205.252484, global_step=101124, preemption_count=0, score=34205.252484, test/accuracy=0.313400, test/loss=3.591003, test/num_examples=10000, total_duration=35429.573813, train/accuracy=0.443658, train/loss=2.589079, validation/accuracy=0.413320, validation/loss=2.795851, validation/num_examples=50000
I0129 11:18:27.649816 140026058876672 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.7151994705200195, loss=1.887411117553711
I0129 11:19:01.352606 140026050483968 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.7942848205566406, loss=1.9646546840667725
I0129 11:19:35.042273 140026058876672 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.7218176126480103, loss=1.9029045104980469
I0129 11:20:08.724940 140026050483968 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.975013017654419, loss=1.9085556268692017
I0129 11:20:42.401427 140026058876672 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.775681495666504, loss=1.9684627056121826
I0129 11:21:16.114307 140026050483968 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.7862060070037842, loss=2.052622079849243
I0129 11:21:49.820099 140026058876672 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.8278443813323975, loss=1.9669206142425537
I0129 11:22:23.588809 140026050483968 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.8850774765014648, loss=2.076587200164795
I0129 11:22:57.290962 140026058876672 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.9090396165847778, loss=2.02858829498291
I0129 11:23:31.208803 140026050483968 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.6889681816101074, loss=1.8452544212341309
I0129 11:24:04.920729 140026058876672 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.702636480331421, loss=1.9971646070480347
I0129 11:24:38.692746 140026050483968 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.7106959819793701, loss=1.9201359748840332
I0129 11:25:12.403440 140026058876672 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.636036992073059, loss=1.7616175413131714
I0129 11:25:46.081884 140026050483968 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.6530115604400635, loss=1.8711962699890137
I0129 11:26:19.777256 140026058876672 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.00681209564209, loss=1.8385956287384033
I0129 11:26:31.706764 140187804313408 spec.py:321] Evaluating on the training split.
I0129 11:26:38.040373 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 11:26:46.865992 140187804313408 spec.py:349] Evaluating on the test split.
I0129 11:26:49.501961 140187804313408 submission_runner.py:408] Time since start: 35957.46s, 	Step: 102637, 	{'train/accuracy': 0.5242147445678711, 'train/loss': 2.068694829940796, 'validation/accuracy': 0.49167999625205994, 'validation/loss': 2.279438018798828, 'validation/num_examples': 50000, 'test/accuracy': 0.3857000172138214, 'test/loss': 3.0341577529907227, 'test/num_examples': 10000, 'score': 34715.23096227646, 'total_duration': 35957.45670199394, 'accumulated_submission_time': 34715.23096227646, 'accumulated_eval_time': 1233.6238696575165, 'accumulated_logging_time': 4.856037855148315}
I0129 11:26:49.540964 140026058876672 logging_writer.py:48] [102637] accumulated_eval_time=1233.623870, accumulated_logging_time=4.856038, accumulated_submission_time=34715.230962, global_step=102637, preemption_count=0, score=34715.230962, test/accuracy=0.385700, test/loss=3.034158, test/num_examples=10000, total_duration=35957.456702, train/accuracy=0.524215, train/loss=2.068695, validation/accuracy=0.491680, validation/loss=2.279438, validation/num_examples=50000
I0129 11:27:11.063948 140026151130880 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.6376583576202393, loss=1.822420597076416
I0129 11:27:44.772769 140026058876672 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.7635118961334229, loss=2.039785623550415
I0129 11:28:18.497829 140026151130880 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.7789242267608643, loss=2.0689399242401123
I0129 11:28:52.149723 140026058876672 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.7172348499298096, loss=1.8358371257781982
I0129 11:29:25.848115 140026151130880 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.782171607017517, loss=1.9751055240631104
I0129 11:29:59.694643 140026058876672 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.8746891021728516, loss=1.9302148818969727
I0129 11:30:33.472475 140026151130880 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.8137568235397339, loss=1.8337360620498657
I0129 11:31:07.228537 140026058876672 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.797705054283142, loss=1.820890188217163
I0129 11:31:40.965142 140026151130880 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.9623504877090454, loss=2.0660245418548584
I0129 11:32:14.669770 140026058876672 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.7189264297485352, loss=1.9128435850143433
I0129 11:32:48.435445 140026151130880 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.7512502670288086, loss=2.002115488052368
I0129 11:33:22.202303 140026058876672 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.6478477716445923, loss=1.7998294830322266
I0129 11:33:55.915070 140026151130880 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.81582510471344, loss=1.9489696025848389
I0129 11:34:29.681663 140026058876672 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.8272745609283447, loss=1.9672653675079346
I0129 11:35:03.421647 140026151130880 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.7073549032211304, loss=1.9463932514190674
I0129 11:35:19.782345 140187804313408 spec.py:321] Evaluating on the training split.
I0129 11:35:26.101300 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 11:35:35.035872 140187804313408 spec.py:349] Evaluating on the test split.
I0129 11:35:37.648350 140187804313408 submission_runner.py:408] Time since start: 36485.60s, 	Step: 104150, 	{'train/accuracy': 0.44082826375961304, 'train/loss': 2.609811305999756, 'validation/accuracy': 0.4175199866294861, 'validation/loss': 2.7619614601135254, 'validation/num_examples': 50000, 'test/accuracy': 0.31770002841949463, 'test/loss': 3.5497262477874756, 'test/num_examples': 10000, 'score': 35225.40841984749, 'total_duration': 36485.60284900665, 'accumulated_submission_time': 35225.40841984749, 'accumulated_eval_time': 1251.489592075348, 'accumulated_logging_time': 4.904949903488159}
I0129 11:35:37.684006 140026042091264 logging_writer.py:48] [104150] accumulated_eval_time=1251.489592, accumulated_logging_time=4.904950, accumulated_submission_time=35225.408420, global_step=104150, preemption_count=0, score=35225.408420, test/accuracy=0.317700, test/loss=3.549726, test/num_examples=10000, total_duration=36485.602849, train/accuracy=0.440828, train/loss=2.609811, validation/accuracy=0.417520, validation/loss=2.761961, validation/num_examples=50000
I0129 11:35:54.901865 140026050483968 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.6078191995620728, loss=1.8633824586868286
I0129 11:36:28.727567 140026042091264 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.6961156129837036, loss=1.886982798576355
I0129 11:37:02.455569 140026050483968 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.8461545705795288, loss=1.9568945169448853
I0129 11:37:36.145402 140026042091264 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.7715994119644165, loss=2.028644561767578
I0129 11:38:09.950638 140026050483968 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.7650166749954224, loss=1.883871078491211
I0129 11:38:43.622970 140026042091264 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.7779008150100708, loss=1.9423067569732666
I0129 11:39:17.428998 140026050483968 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.860146164894104, loss=1.9156605005264282
I0129 11:39:51.111535 140026042091264 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.8623783588409424, loss=1.820570468902588
I0129 11:40:24.826474 140026050483968 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.7313607931137085, loss=1.935953140258789
I0129 11:40:58.623183 140026042091264 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.8798991441726685, loss=1.9918923377990723
I0129 11:41:32.328664 140026050483968 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.7831320762634277, loss=1.8529250621795654
I0129 11:42:06.049572 140026042091264 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.9313757419586182, loss=2.103548288345337
I0129 11:42:39.918759 140026050483968 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.7544089555740356, loss=1.8841384649276733
I0129 11:43:13.666345 140026042091264 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.715625286102295, loss=1.8422616720199585
I0129 11:43:47.456327 140026050483968 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.8518133163452148, loss=1.8992241621017456
I0129 11:44:07.833419 140187804313408 spec.py:321] Evaluating on the training split.
I0129 11:44:14.154507 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 11:44:22.850114 140187804313408 spec.py:349] Evaluating on the test split.
I0129 11:44:25.520507 140187804313408 submission_runner.py:408] Time since start: 37013.48s, 	Step: 105662, 	{'train/accuracy': 0.27955594658851624, 'train/loss': 3.90887188911438, 'validation/accuracy': 0.25769999623298645, 'validation/loss': 4.139658451080322, 'validation/num_examples': 50000, 'test/accuracy': 0.19850000739097595, 'test/loss': 4.91162109375, 'test/num_examples': 10000, 'score': 35735.49154949188, 'total_duration': 37013.47520160675, 'accumulated_submission_time': 35735.49154949188, 'accumulated_eval_time': 1269.1765999794006, 'accumulated_logging_time': 4.950104713439941}
I0129 11:44:25.555657 140026050483968 logging_writer.py:48] [105662] accumulated_eval_time=1269.176600, accumulated_logging_time=4.950105, accumulated_submission_time=35735.491549, global_step=105662, preemption_count=0, score=35735.491549, test/accuracy=0.198500, test/loss=4.911621, test/num_examples=10000, total_duration=37013.475202, train/accuracy=0.279556, train/loss=3.908872, validation/accuracy=0.257700, validation/loss=4.139658, validation/num_examples=50000
I0129 11:44:38.732139 140026151130880 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.7121771574020386, loss=1.8671318292617798
I0129 11:45:12.458968 140026050483968 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.7258509397506714, loss=1.9090341329574585
I0129 11:45:46.127100 140026151130880 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.779314398765564, loss=1.8883615732192993
I0129 11:46:19.843007 140026050483968 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.1448028087615967, loss=1.8704150915145874
I0129 11:46:53.618550 140026151130880 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.000396490097046, loss=1.8575907945632935
I0129 11:47:27.356783 140026050483968 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.886549711227417, loss=1.954648733139038
I0129 11:48:01.133041 140026151130880 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.9375319480895996, loss=2.0183489322662354
I0129 11:48:34.862908 140026050483968 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.1914098262786865, loss=1.9228456020355225
I0129 11:49:08.615676 140026151130880 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.775928020477295, loss=1.8453056812286377
I0129 11:49:42.405556 140026050483968 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.7943671941757202, loss=1.9008058309555054
I0129 11:50:16.143908 140026151130880 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.9447028636932373, loss=1.933704137802124
I0129 11:50:49.874243 140026050483968 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.9997702836990356, loss=1.9203462600708008
I0129 11:51:23.657615 140026151130880 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.0481817722320557, loss=1.9129115343093872
I0129 11:51:57.394538 140026050483968 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.8768136501312256, loss=1.905000925064087
I0129 11:52:31.085357 140026151130880 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.9632844924926758, loss=1.9268561601638794
I0129 11:52:55.562612 140187804313408 spec.py:321] Evaluating on the training split.
I0129 11:53:01.941173 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 11:53:10.696818 140187804313408 spec.py:349] Evaluating on the test split.
I0129 11:53:13.346781 140187804313408 submission_runner.py:408] Time since start: 37541.30s, 	Step: 107174, 	{'train/accuracy': 0.500019907951355, 'train/loss': 2.244053363800049, 'validation/accuracy': 0.46041998267173767, 'validation/loss': 2.513838768005371, 'validation/num_examples': 50000, 'test/accuracy': 0.35680001974105835, 'test/loss': 3.284795045852661, 'test/num_examples': 10000, 'score': 36245.43359160423, 'total_duration': 37541.301339387894, 'accumulated_submission_time': 36245.43359160423, 'accumulated_eval_time': 1286.9605538845062, 'accumulated_logging_time': 4.994918346405029}
I0129 11:53:13.385045 140026058876672 logging_writer.py:48] [107174] accumulated_eval_time=1286.960554, accumulated_logging_time=4.994918, accumulated_submission_time=36245.433592, global_step=107174, preemption_count=0, score=36245.433592, test/accuracy=0.356800, test/loss=3.284795, test/num_examples=10000, total_duration=37541.301339, train/accuracy=0.500020, train/loss=2.244053, validation/accuracy=0.460420, validation/loss=2.513839, validation/num_examples=50000
I0129 11:53:22.494205 140026067269376 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.7301353216171265, loss=1.9716581106185913
I0129 11:53:56.255601 140026058876672 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.8953289985656738, loss=1.8376909494400024
I0129 11:54:29.966942 140026067269376 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.7146432399749756, loss=1.8524303436279297
I0129 11:55:03.677052 140026058876672 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.7979648113250732, loss=1.8240392208099365
I0129 11:55:37.423066 140026067269376 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.725226640701294, loss=1.942814826965332
I0129 11:56:11.206181 140026058876672 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.7112990617752075, loss=1.9510502815246582
I0129 11:56:44.920986 140026067269376 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.8731344938278198, loss=1.9730675220489502
I0129 11:57:18.673133 140026058876672 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.776231050491333, loss=1.7655208110809326
I0129 11:57:52.349515 140026067269376 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.9902679920196533, loss=2.0446646213531494
I0129 11:58:26.151205 140026058876672 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.858762502670288, loss=1.8539302349090576
I0129 11:58:59.824014 140026067269376 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.8828868865966797, loss=1.8735315799713135
I0129 11:59:33.613207 140026058876672 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.8919579982757568, loss=1.9665122032165527
I0129 12:00:07.368444 140026067269376 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.7791379690170288, loss=1.88833749294281
I0129 12:00:41.114290 140026058876672 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.0948398113250732, loss=1.9659324884414673
I0129 12:01:14.870062 140026067269376 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.0275821685791016, loss=1.8783109188079834
I0129 12:01:43.364662 140187804313408 spec.py:321] Evaluating on the training split.
I0129 12:01:49.623231 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 12:01:58.279147 140187804313408 spec.py:349] Evaluating on the test split.
I0129 12:02:00.909126 140187804313408 submission_runner.py:408] Time since start: 38068.86s, 	Step: 108686, 	{'train/accuracy': 0.4937818646430969, 'train/loss': 2.252739191055298, 'validation/accuracy': 0.4607200026512146, 'validation/loss': 2.472726345062256, 'validation/num_examples': 50000, 'test/accuracy': 0.35510000586509705, 'test/loss': 3.2092649936676025, 'test/num_examples': 10000, 'score': 36755.34883475304, 'total_duration': 38068.86385965347, 'accumulated_submission_time': 36755.34883475304, 'accumulated_eval_time': 1304.5049712657928, 'accumulated_logging_time': 5.042736291885376}
I0129 12:02:00.950141 140026151130880 logging_writer.py:48] [108686] accumulated_eval_time=1304.504971, accumulated_logging_time=5.042736, accumulated_submission_time=36755.348835, global_step=108686, preemption_count=0, score=36755.348835, test/accuracy=0.355100, test/loss=3.209265, test/num_examples=10000, total_duration=38068.863860, train/accuracy=0.493782, train/loss=2.252739, validation/accuracy=0.460720, validation/loss=2.472726, validation/num_examples=50000
I0129 12:02:06.207432 140026159523584 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.9646414518356323, loss=1.988559365272522
I0129 12:02:39.897212 140026151130880 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.935348629951477, loss=1.9664803743362427
I0129 12:03:13.617445 140026159523584 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.9772318601608276, loss=1.90665864944458
I0129 12:03:47.373735 140026151130880 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.989521861076355, loss=1.8709410429000854
I0129 12:04:21.122998 140026159523584 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.8752011060714722, loss=1.9938502311706543
I0129 12:04:54.889424 140026151130880 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.8656485080718994, loss=1.9058343172073364
I0129 12:05:28.595766 140026159523584 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.846236228942871, loss=1.8133504390716553
I0129 12:06:02.288080 140026151130880 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.8587933778762817, loss=1.9253509044647217
I0129 12:06:36.035950 140026159523584 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.804941177368164, loss=1.8630865812301636
I0129 12:07:09.719004 140026151130880 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.8228209018707275, loss=1.8293331861495972
I0129 12:07:43.470607 140026159523584 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.9545024633407593, loss=1.857452154159546
I0129 12:08:17.145256 140026151130880 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.1258389949798584, loss=1.9609781503677368
I0129 12:08:50.999196 140026159523584 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.8857669830322266, loss=1.851921558380127
I0129 12:09:24.668978 140026151130880 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.9774900674819946, loss=1.9289417266845703
I0129 12:09:58.373783 140026159523584 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.968924880027771, loss=1.8467175960540771
I0129 12:10:31.165426 140187804313408 spec.py:321] Evaluating on the training split.
I0129 12:10:37.435305 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 12:10:46.324184 140187804313408 spec.py:349] Evaluating on the test split.
I0129 12:10:48.964483 140187804313408 submission_runner.py:408] Time since start: 38596.92s, 	Step: 110199, 	{'train/accuracy': 0.5153260231018066, 'train/loss': 2.0981321334838867, 'validation/accuracy': 0.4820399880409241, 'validation/loss': 2.304888963699341, 'validation/num_examples': 50000, 'test/accuracy': 0.3678000271320343, 'test/loss': 3.0940093994140625, 'test/num_examples': 10000, 'score': 37265.49964928627, 'total_duration': 38596.919221162796, 'accumulated_submission_time': 37265.49964928627, 'accumulated_eval_time': 1322.303986787796, 'accumulated_logging_time': 5.093266010284424}
I0129 12:10:49.004185 140026075662080 logging_writer.py:48] [110199] accumulated_eval_time=1322.303987, accumulated_logging_time=5.093266, accumulated_submission_time=37265.499649, global_step=110199, preemption_count=0, score=37265.499649, test/accuracy=0.367800, test/loss=3.094009, test/num_examples=10000, total_duration=38596.919221, train/accuracy=0.515326, train/loss=2.098132, validation/accuracy=0.482040, validation/loss=2.304889, validation/num_examples=50000
I0129 12:10:49.688600 140026167916288 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.9262422323226929, loss=1.8747296333312988
I0129 12:11:23.371653 140026075662080 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.014589309692383, loss=1.8614121675491333
I0129 12:11:57.097294 140026167916288 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.7986721992492676, loss=1.9244956970214844
I0129 12:12:30.769884 140026075662080 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.9683808088302612, loss=1.891867756843567
I0129 12:13:04.494961 140026167916288 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.95968759059906, loss=1.9861533641815186
I0129 12:13:38.256828 140026075662080 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.1240875720977783, loss=1.9269657135009766
I0129 12:14:11.950435 140026167916288 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.9340243339538574, loss=1.905909538269043
I0129 12:14:45.628322 140026075662080 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.017681360244751, loss=1.9654251337051392
I0129 12:15:19.523495 140026167916288 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.0076942443847656, loss=1.8238685131072998
I0129 12:15:53.250222 140026075662080 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.8372474908828735, loss=1.876393437385559
I0129 12:16:27.052593 140026167916288 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.2012808322906494, loss=2.013230323791504
I0129 12:17:00.720966 140026075662080 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.9260716438293457, loss=1.8539507389068604
I0129 12:17:34.506247 140026167916288 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.973388910293579, loss=1.7762677669525146
I0129 12:18:08.171881 140026075662080 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.0941243171691895, loss=1.8391313552856445
I0129 12:18:41.876117 140026167916288 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.1962132453918457, loss=1.9707751274108887
I0129 12:19:15.530589 140026075662080 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.8176621198654175, loss=1.8669404983520508
I0129 12:19:19.050349 140187804313408 spec.py:321] Evaluating on the training split.
I0129 12:19:25.392590 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 12:19:34.169810 140187804313408 spec.py:349] Evaluating on the test split.
I0129 12:19:36.807488 140187804313408 submission_runner.py:408] Time since start: 39124.76s, 	Step: 111712, 	{'train/accuracy': 0.5056201815605164, 'train/loss': 2.1512796878814697, 'validation/accuracy': 0.47079998254776, 'validation/loss': 2.3727433681488037, 'validation/num_examples': 50000, 'test/accuracy': 0.35680001974105835, 'test/loss': 3.1949143409729004, 'test/num_examples': 10000, 'score': 37775.48208498955, 'total_duration': 39124.7622282505, 'accumulated_submission_time': 37775.48208498955, 'accumulated_eval_time': 1340.0610992908478, 'accumulated_logging_time': 5.142434120178223}
I0129 12:19:36.850066 140026058876672 logging_writer.py:48] [111712] accumulated_eval_time=1340.061099, accumulated_logging_time=5.142434, accumulated_submission_time=37775.482085, global_step=111712, preemption_count=0, score=37775.482085, test/accuracy=0.356800, test/loss=3.194914, test/num_examples=10000, total_duration=39124.762228, train/accuracy=0.505620, train/loss=2.151280, validation/accuracy=0.470800, validation/loss=2.372743, validation/num_examples=50000
I0129 12:20:06.830060 140026067269376 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.8753840923309326, loss=1.7322580814361572
I0129 12:20:40.555073 140026058876672 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.0320613384246826, loss=1.8676044940948486
I0129 12:21:14.253390 140026067269376 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.9000672101974487, loss=1.9209237098693848
I0129 12:21:48.075309 140026058876672 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.152505874633789, loss=1.9200183153152466
I0129 12:22:21.814941 140026067269376 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.041266679763794, loss=1.7757833003997803
I0129 12:22:55.581564 140026058876672 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.972253441810608, loss=1.7281014919281006
I0129 12:23:29.272800 140026067269376 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.056623935699463, loss=1.9504090547561646
I0129 12:24:02.979881 140026058876672 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.8630321025848389, loss=1.8803097009658813
I0129 12:24:36.729196 140026067269376 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.9064310789108276, loss=1.7984437942504883
I0129 12:25:10.515007 140026058876672 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.972978115081787, loss=1.8498620986938477
I0129 12:25:44.222572 140026067269376 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.8717042207717896, loss=1.9257919788360596
I0129 12:26:17.954348 140026058876672 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.025984764099121, loss=1.9942626953125
I0129 12:26:51.697309 140026067269376 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.9079450368881226, loss=1.8731104135513306
I0129 12:27:25.362901 140026058876672 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.9991298913955688, loss=1.9387954473495483
I0129 12:27:59.135219 140026067269376 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.9851467609405518, loss=1.7959034442901611
I0129 12:28:07.064304 140187804313408 spec.py:321] Evaluating on the training split.
I0129 12:28:13.416308 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 12:28:22.081348 140187804313408 spec.py:349] Evaluating on the test split.
I0129 12:28:24.744235 140187804313408 submission_runner.py:408] Time since start: 39652.70s, 	Step: 113225, 	{'train/accuracy': 0.5051419138908386, 'train/loss': 2.175563335418701, 'validation/accuracy': 0.4692799746990204, 'validation/loss': 2.409187078475952, 'validation/num_examples': 50000, 'test/accuracy': 0.3579000234603882, 'test/loss': 3.2229323387145996, 'test/num_examples': 10000, 'score': 38285.63218688965, 'total_duration': 39652.69889450073, 'accumulated_submission_time': 38285.63218688965, 'accumulated_eval_time': 1357.7409224510193, 'accumulated_logging_time': 5.193975210189819}
I0129 12:28:24.781322 140026042091264 logging_writer.py:48] [113225] accumulated_eval_time=1357.740922, accumulated_logging_time=5.193975, accumulated_submission_time=38285.632187, global_step=113225, preemption_count=0, score=38285.632187, test/accuracy=0.357900, test/loss=3.222932, test/num_examples=10000, total_duration=39652.698895, train/accuracy=0.505142, train/loss=2.175563, validation/accuracy=0.469280, validation/loss=2.409187, validation/num_examples=50000
I0129 12:28:50.357947 140026050483968 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.8989105224609375, loss=1.7752461433410645
I0129 12:29:24.011119 140026042091264 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.0090692043304443, loss=1.8968000411987305
I0129 12:29:57.736079 140026050483968 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.021185874938965, loss=1.9535048007965088
I0129 12:30:31.382380 140026042091264 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.9273490905761719, loss=1.7337915897369385
I0129 12:31:05.135818 140026050483968 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.02042555809021, loss=1.855913758277893
I0129 12:31:38.817261 140026042091264 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.9764838218688965, loss=1.8067331314086914
I0129 12:32:12.547308 140026050483968 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.0333757400512695, loss=1.785712718963623
I0129 12:32:46.203610 140026042091264 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.021097183227539, loss=1.7675461769104004
I0129 12:33:19.924418 140026050483968 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.8691926002502441, loss=1.7883116006851196
I0129 12:33:53.579113 140026042091264 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.1196467876434326, loss=1.9928401708602905
I0129 12:34:27.390552 140026050483968 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.0129244327545166, loss=1.8299438953399658
I0129 12:35:01.084014 140026042091264 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.887639045715332, loss=1.8393727540969849
I0129 12:35:34.801177 140026050483968 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.9230962991714478, loss=1.8595130443572998
I0129 12:36:08.559904 140026042091264 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.9603883028030396, loss=1.8595435619354248
I0129 12:36:42.274180 140026050483968 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.011080741882324, loss=1.8969025611877441
I0129 12:36:54.869808 140187804313408 spec.py:321] Evaluating on the training split.
I0129 12:37:01.208978 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 12:37:10.118891 140187804313408 spec.py:349] Evaluating on the test split.
I0129 12:37:13.318438 140187804313408 submission_runner.py:408] Time since start: 40181.27s, 	Step: 114739, 	{'train/accuracy': 0.5345184803009033, 'train/loss': 2.0138721466064453, 'validation/accuracy': 0.4868199825286865, 'validation/loss': 2.304708957672119, 'validation/num_examples': 50000, 'test/accuracy': 0.3734000325202942, 'test/loss': 3.111175060272217, 'test/num_examples': 10000, 'score': 38795.65815782547, 'total_duration': 40181.27318763733, 'accumulated_submission_time': 38795.65815782547, 'accumulated_eval_time': 1376.1895372867584, 'accumulated_logging_time': 5.24059534072876}
I0129 12:37:13.350434 140026151130880 logging_writer.py:48] [114739] accumulated_eval_time=1376.189537, accumulated_logging_time=5.240595, accumulated_submission_time=38795.658158, global_step=114739, preemption_count=0, score=38795.658158, test/accuracy=0.373400, test/loss=3.111175, test/num_examples=10000, total_duration=40181.273188, train/accuracy=0.534518, train/loss=2.013872, validation/accuracy=0.486820, validation/loss=2.304709, validation/num_examples=50000
I0129 12:37:34.247379 140026159523584 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.2486042976379395, loss=1.9068282842636108
I0129 12:38:07.925955 140026151130880 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.021200656890869, loss=1.8408761024475098
I0129 12:38:41.622892 140026159523584 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.1352460384368896, loss=1.8941516876220703
I0129 12:39:15.328024 140026151130880 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.8639252185821533, loss=1.6926414966583252
I0129 12:39:49.097177 140026159523584 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.8487178087234497, loss=1.786329984664917
I0129 12:40:22.738870 140026151130880 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.9424642324447632, loss=1.8021866083145142
I0129 12:40:56.678109 140026159523584 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.8501075506210327, loss=1.7435917854309082
I0129 12:41:30.372714 140026151130880 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.8158183097839355, loss=1.8489599227905273
I0129 12:42:04.090213 140026159523584 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.9945313930511475, loss=1.7471728324890137
I0129 12:42:37.750921 140026151130880 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.9102064371109009, loss=1.8364994525909424
I0129 12:43:11.477473 140026159523584 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.9520647525787354, loss=1.7468070983886719
I0129 12:43:45.207064 140026151130880 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.062671422958374, loss=1.7909088134765625
I0129 12:44:18.959889 140026159523584 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.0649378299713135, loss=1.8450522422790527
I0129 12:44:52.673757 140026151130880 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.041879653930664, loss=1.9524316787719727
I0129 12:45:26.472373 140026159523584 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.259129524230957, loss=1.888319730758667
I0129 12:45:43.474862 140187804313408 spec.py:321] Evaluating on the training split.
I0129 12:45:49.737237 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 12:45:58.378640 140187804313408 spec.py:349] Evaluating on the test split.
I0129 12:46:01.061030 140187804313408 submission_runner.py:408] Time since start: 40709.02s, 	Step: 116252, 	{'train/accuracy': 0.6103116869926453, 'train/loss': 1.5996730327606201, 'validation/accuracy': 0.5604000091552734, 'validation/loss': 1.880994200706482, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.634089708328247, 'test/num_examples': 10000, 'score': 39305.71879410744, 'total_duration': 40709.015604019165, 'accumulated_submission_time': 39305.71879410744, 'accumulated_eval_time': 1393.7754967212677, 'accumulated_logging_time': 5.281083822250366}
I0129 12:46:01.099366 140026058876672 logging_writer.py:48] [116252] accumulated_eval_time=1393.775497, accumulated_logging_time=5.281084, accumulated_submission_time=39305.718794, global_step=116252, preemption_count=0, score=39305.718794, test/accuracy=0.441000, test/loss=2.634090, test/num_examples=10000, total_duration=40709.015604, train/accuracy=0.610312, train/loss=1.599673, validation/accuracy=0.560400, validation/loss=1.880994, validation/num_examples=50000
I0129 12:46:17.605127 140026067269376 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.1540005207061768, loss=1.8759517669677734
I0129 12:46:51.315958 140026058876672 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.0406415462493896, loss=1.7976019382476807
I0129 12:47:25.153306 140026067269376 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.1782002449035645, loss=1.9875459671020508
I0129 12:47:58.829078 140026058876672 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.115379571914673, loss=1.9508459568023682
I0129 12:48:32.495395 140026067269376 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.1059443950653076, loss=1.8376731872558594
I0129 12:49:06.206544 140026058876672 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.859959363937378, loss=1.7947169542312622
I0129 12:49:39.908194 140026067269376 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.0483005046844482, loss=1.7960206270217896
I0129 12:50:13.684969 140026058876672 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.0012035369873047, loss=1.8367841243743896
I0129 12:50:47.425916 140026067269376 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.1843128204345703, loss=1.8941706418991089
I0129 12:51:21.186688 140026058876672 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.176962375640869, loss=1.760369062423706
I0129 12:51:54.939445 140026067269376 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.9930849075317383, loss=1.8709266185760498
I0129 12:52:28.679319 140026058876672 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.117098569869995, loss=1.8220903873443604
I0129 12:53:02.451054 140026067269376 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.0903162956237793, loss=1.8351919651031494
I0129 12:53:36.163113 140026058876672 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.3778069019317627, loss=1.9053995609283447
I0129 12:54:09.986220 140026067269376 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.118499279022217, loss=1.835458755493164
I0129 12:54:31.382971 140187804313408 spec.py:321] Evaluating on the training split.
I0129 12:54:37.652984 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 12:54:46.284087 140187804313408 spec.py:349] Evaluating on the test split.
I0129 12:54:48.904396 140187804313408 submission_runner.py:408] Time since start: 41236.86s, 	Step: 117765, 	{'train/accuracy': 0.5663065910339355, 'train/loss': 1.824598789215088, 'validation/accuracy': 0.5275200009346008, 'validation/loss': 2.038508892059326, 'validation/num_examples': 50000, 'test/accuracy': 0.4052000045776367, 'test/loss': 2.835646152496338, 'test/num_examples': 10000, 'score': 39815.936584711075, 'total_duration': 41236.85900259018, 'accumulated_submission_time': 39815.936584711075, 'accumulated_eval_time': 1411.296749830246, 'accumulated_logging_time': 5.329135894775391}
I0129 12:54:48.946780 140026159523584 logging_writer.py:48] [117765] accumulated_eval_time=1411.296750, accumulated_logging_time=5.329136, accumulated_submission_time=39815.936585, global_step=117765, preemption_count=0, score=39815.936585, test/accuracy=0.405200, test/loss=2.835646, test/num_examples=10000, total_duration=41236.859003, train/accuracy=0.566307, train/loss=1.824599, validation/accuracy=0.527520, validation/loss=2.038509, validation/num_examples=50000
I0129 12:55:01.126881 140026167916288 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.1948907375335693, loss=1.8302109241485596
I0129 12:55:34.859711 140026159523584 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.119948387145996, loss=1.8692879676818848
I0129 12:56:08.536435 140026167916288 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.15885591506958, loss=1.8069465160369873
I0129 12:56:42.321758 140026159523584 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.194653034210205, loss=1.7655035257339478
I0129 12:57:15.998397 140026167916288 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.267538547515869, loss=1.830298900604248
I0129 12:57:49.791792 140026159523584 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.03104305267334, loss=1.7007114887237549
I0129 12:58:23.528419 140026167916288 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.0311496257781982, loss=1.8180298805236816
I0129 12:58:57.252871 140026159523584 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.167189121246338, loss=1.9610930681228638
I0129 12:59:30.993539 140026167916288 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.154268503189087, loss=1.799367070198059
I0129 13:00:04.720611 140026159523584 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.0846331119537354, loss=1.7273948192596436
I0129 13:00:38.539152 140026167916288 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.34624981880188, loss=1.9556206464767456
I0129 13:01:12.232886 140026159523584 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.1340417861938477, loss=1.742063283920288
I0129 13:01:45.880092 140026167916288 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.351088285446167, loss=1.8297264575958252
I0129 13:02:19.708621 140026159523584 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.9712607860565186, loss=1.834125280380249
I0129 13:02:53.392913 140026167916288 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.1574904918670654, loss=1.7765171527862549
I0129 13:03:19.164103 140187804313408 spec.py:321] Evaluating on the training split.
I0129 13:03:25.402332 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 13:03:34.227486 140187804313408 spec.py:349] Evaluating on the test split.
I0129 13:03:36.845687 140187804313408 submission_runner.py:408] Time since start: 41764.80s, 	Step: 119278, 	{'train/accuracy': 0.29412466287612915, 'train/loss': 3.7996959686279297, 'validation/accuracy': 0.28205999732017517, 'validation/loss': 3.9288508892059326, 'validation/num_examples': 50000, 'test/accuracy': 0.20280000567436218, 'test/loss': 4.84499979019165, 'test/num_examples': 10000, 'score': 40326.08441853523, 'total_duration': 41764.800423145294, 'accumulated_submission_time': 40326.08441853523, 'accumulated_eval_time': 1428.9782931804657, 'accumulated_logging_time': 5.38359808921814}
I0129 13:03:36.889451 140026058876672 logging_writer.py:48] [119278] accumulated_eval_time=1428.978293, accumulated_logging_time=5.383598, accumulated_submission_time=40326.084419, global_step=119278, preemption_count=0, score=40326.084419, test/accuracy=0.202800, test/loss=4.845000, test/num_examples=10000, total_duration=41764.800423, train/accuracy=0.294125, train/loss=3.799696, validation/accuracy=0.282060, validation/loss=3.928851, validation/num_examples=50000
I0129 13:03:44.678828 140026067269376 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.0139501094818115, loss=1.7487982511520386
I0129 13:04:18.383121 140026058876672 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.1158900260925293, loss=1.8350178003311157
I0129 13:04:52.095167 140026067269376 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.012282371520996, loss=1.7941856384277344
I0129 13:05:25.864058 140026058876672 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.0384762287139893, loss=1.9459534883499146
I0129 13:05:59.575934 140026067269376 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.1962168216705322, loss=1.8677362203598022
I0129 13:06:33.366109 140026058876672 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.185206413269043, loss=1.8281666040420532
I0129 13:07:07.053754 140026067269376 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.120154857635498, loss=1.791149616241455
I0129 13:07:40.820185 140026058876672 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.1432673931121826, loss=1.7427709102630615
I0129 13:08:14.541425 140026067269376 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.063610553741455, loss=1.7310255765914917
I0129 13:08:48.214687 140026058876672 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.259268283843994, loss=1.8126485347747803
I0129 13:09:21.913210 140026067269376 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.217834234237671, loss=1.7650339603424072
I0129 13:09:55.693183 140026058876672 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.1296257972717285, loss=1.8753387928009033
I0129 13:10:29.416680 140026067269376 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.1151156425476074, loss=1.7341536283493042
I0129 13:11:03.190130 140026058876672 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.152205467224121, loss=1.681754231452942
I0129 13:11:36.945049 140026067269376 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.2509512901306152, loss=1.8392722606658936
I0129 13:12:07.113900 140187804313408 spec.py:321] Evaluating on the training split.
I0129 13:12:13.500053 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 13:12:22.122295 140187804313408 spec.py:349] Evaluating on the test split.
I0129 13:12:24.788397 140187804313408 submission_runner.py:408] Time since start: 42292.74s, 	Step: 120791, 	{'train/accuracy': 0.5305325388908386, 'train/loss': 2.0072436332702637, 'validation/accuracy': 0.4994799792766571, 'validation/loss': 2.1964006423950195, 'validation/num_examples': 50000, 'test/accuracy': 0.3850000202655792, 'test/loss': 3.043755292892456, 'test/num_examples': 10000, 'score': 40836.241518974304, 'total_duration': 42292.74313831329, 'accumulated_submission_time': 40836.241518974304, 'accumulated_eval_time': 1446.6527774333954, 'accumulated_logging_time': 5.439541339874268}
I0129 13:12:24.828299 140026050483968 logging_writer.py:48] [120791] accumulated_eval_time=1446.652777, accumulated_logging_time=5.439541, accumulated_submission_time=40836.241519, global_step=120791, preemption_count=0, score=40836.241519, test/accuracy=0.385000, test/loss=3.043755, test/num_examples=10000, total_duration=42292.743138, train/accuracy=0.530533, train/loss=2.007244, validation/accuracy=0.499480, validation/loss=2.196401, validation/num_examples=50000
I0129 13:12:28.210244 140026159523584 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.1123483180999756, loss=1.7579357624053955
I0129 13:13:02.128099 140026050483968 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.413191080093384, loss=1.7971431016921997
I0129 13:13:35.876732 140026159523584 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.9890426397323608, loss=1.7830348014831543
I0129 13:14:09.598155 140026050483968 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.124211549758911, loss=1.705210566520691
I0129 13:14:43.395025 140026159523584 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.506983995437622, loss=1.8044754266738892
I0129 13:15:17.149250 140026050483968 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.2231826782226562, loss=1.8405959606170654
I0129 13:15:50.881343 140026159523584 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.3355324268341064, loss=1.7662912607192993
I0129 13:16:24.529597 140026050483968 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.4698598384857178, loss=1.71738600730896
I0129 13:16:58.230456 140026159523584 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.422775983810425, loss=1.795797348022461
I0129 13:17:31.984327 140026050483968 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.206023931503296, loss=1.7403497695922852
I0129 13:18:05.706445 140026159523584 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.23651123046875, loss=1.694119930267334
I0129 13:18:39.361415 140026050483968 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.2450780868530273, loss=1.7966163158416748
I0129 13:19:13.119082 140026159523584 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.146366596221924, loss=1.817674160003662
I0129 13:19:47.064234 140026050483968 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.1913652420043945, loss=1.7175301313400269
I0129 13:20:20.788340 140026159523584 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.2680654525756836, loss=1.8553588390350342
I0129 13:20:54.528423 140026050483968 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.5449087619781494, loss=1.7797964811325073
I0129 13:20:55.025211 140187804313408 spec.py:321] Evaluating on the training split.
I0129 13:21:01.491388 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 13:21:10.331977 140187804313408 spec.py:349] Evaluating on the test split.
I0129 13:21:13.688377 140187804313408 submission_runner.py:408] Time since start: 42821.64s, 	Step: 122303, 	{'train/accuracy': 0.5705317258834839, 'train/loss': 1.8204600811004639, 'validation/accuracy': 0.5156199932098389, 'validation/loss': 2.1237943172454834, 'validation/num_examples': 50000, 'test/accuracy': 0.3849000036716461, 'test/loss': 3.013498067855835, 'test/num_examples': 10000, 'score': 41346.374436855316, 'total_duration': 42821.643122434616, 'accumulated_submission_time': 41346.374436855316, 'accumulated_eval_time': 1465.3159244060516, 'accumulated_logging_time': 5.488732576370239}
I0129 13:21:13.720805 140026058876672 logging_writer.py:48] [122303] accumulated_eval_time=1465.315924, accumulated_logging_time=5.488733, accumulated_submission_time=41346.374437, global_step=122303, preemption_count=0, score=41346.374437, test/accuracy=0.384900, test/loss=3.013498, test/num_examples=10000, total_duration=42821.643122, train/accuracy=0.570532, train/loss=1.820460, validation/accuracy=0.515620, validation/loss=2.123794, validation/num_examples=50000
I0129 13:21:46.740104 140026067269376 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.1007161140441895, loss=1.7790837287902832
I0129 13:22:20.442978 140026058876672 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.249638557434082, loss=1.7280796766281128
I0129 13:22:54.119300 140026067269376 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.2004401683807373, loss=1.7911473512649536
I0129 13:23:27.840114 140026058876672 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.1227123737335205, loss=1.6730625629425049
I0129 13:24:01.497089 140026067269376 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.1803576946258545, loss=1.7865371704101562
I0129 13:24:35.296829 140026058876672 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.529709577560425, loss=1.7652199268341064
I0129 13:25:08.978963 140026067269376 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.0043702125549316, loss=1.6650739908218384
I0129 13:25:42.678455 140026058876672 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.158240556716919, loss=1.7155685424804688
I0129 13:26:16.563477 140026067269376 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.4271154403686523, loss=1.7305099964141846
I0129 13:26:50.279269 140026058876672 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.394350290298462, loss=1.8891786336898804
I0129 13:27:23.959152 140026067269376 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.3773086071014404, loss=1.7989404201507568
I0129 13:27:57.730378 140026058876672 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.3037054538726807, loss=1.702762484550476
I0129 13:28:31.425752 140026067269376 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.2991840839385986, loss=1.797284722328186
I0129 13:29:05.129791 140026058876672 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.435229778289795, loss=1.8869409561157227
I0129 13:29:38.789188 140026067269376 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.137352705001831, loss=1.7029467821121216
I0129 13:29:43.987023 140187804313408 spec.py:321] Evaluating on the training split.
I0129 13:29:50.286863 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 13:29:59.194832 140187804313408 spec.py:349] Evaluating on the test split.
I0129 13:30:01.837732 140187804313408 submission_runner.py:408] Time since start: 43349.79s, 	Step: 123817, 	{'train/accuracy': 0.6350645422935486, 'train/loss': 1.4768481254577637, 'validation/accuracy': 0.5727599859237671, 'validation/loss': 1.8264350891113281, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.5984723567962646, 'test/num_examples': 10000, 'score': 41856.57735204697, 'total_duration': 43349.79244160652, 'accumulated_submission_time': 41856.57735204697, 'accumulated_eval_time': 1483.166562795639, 'accumulated_logging_time': 5.5297324657440186}
I0129 13:30:01.892936 140026159523584 logging_writer.py:48] [123817] accumulated_eval_time=1483.166563, accumulated_logging_time=5.529732, accumulated_submission_time=41856.577352, global_step=123817, preemption_count=0, score=41856.577352, test/accuracy=0.450200, test/loss=2.598472, test/num_examples=10000, total_duration=43349.792442, train/accuracy=0.635065, train/loss=1.476848, validation/accuracy=0.572760, validation/loss=1.826435, validation/num_examples=50000
I0129 13:30:30.233788 140026167916288 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.4757018089294434, loss=1.795956015586853
I0129 13:31:03.912689 140026159523584 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.266242504119873, loss=1.7236649990081787
I0129 13:31:37.628646 140026167916288 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.4789655208587646, loss=1.8318803310394287
I0129 13:32:11.393030 140026159523584 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.4206643104553223, loss=1.7217110395431519
I0129 13:32:45.287424 140026167916288 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.3973135948181152, loss=1.7190054655075073
I0129 13:33:18.957201 140026159523584 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.210968255996704, loss=1.7236512899398804
I0129 13:33:52.646348 140026167916288 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.317476511001587, loss=1.6977250576019287
I0129 13:34:26.401935 140026159523584 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.683277130126953, loss=1.749725103378296
I0129 13:35:00.129254 140026167916288 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.283473491668701, loss=1.7381874322891235
I0129 13:35:33.811205 140026159523584 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.46649169921875, loss=1.6627031564712524
I0129 13:36:07.564863 140026167916288 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.265533685684204, loss=1.7905540466308594
I0129 13:36:41.246756 140026159523584 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.235253095626831, loss=1.7176543474197388
I0129 13:37:14.949476 140026167916288 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.209120512008667, loss=1.7022112607955933
I0129 13:37:48.650064 140026159523584 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.2143502235412598, loss=1.612472414970398
I0129 13:38:22.423412 140026167916288 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.284722328186035, loss=1.733863353729248
I0129 13:38:31.987289 140187804313408 spec.py:321] Evaluating on the training split.
I0129 13:38:38.290315 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 13:38:46.980619 140187804313408 spec.py:349] Evaluating on the test split.
I0129 13:38:49.618664 140187804313408 submission_runner.py:408] Time since start: 43877.57s, 	Step: 125330, 	{'train/accuracy': 0.5938695669174194, 'train/loss': 1.693787932395935, 'validation/accuracy': 0.5395399928092957, 'validation/loss': 2.0015382766723633, 'validation/num_examples': 50000, 'test/accuracy': 0.42020002007484436, 'test/loss': 2.7875430583953857, 'test/num_examples': 10000, 'score': 42366.60649180412, 'total_duration': 43877.573405981064, 'accumulated_submission_time': 42366.60649180412, 'accumulated_eval_time': 1500.7979154586792, 'accumulated_logging_time': 5.595268964767456}
I0129 13:38:49.657640 140026067269376 logging_writer.py:48] [125330] accumulated_eval_time=1500.797915, accumulated_logging_time=5.595269, accumulated_submission_time=42366.606492, global_step=125330, preemption_count=0, score=42366.606492, test/accuracy=0.420200, test/loss=2.787543, test/num_examples=10000, total_duration=43877.573406, train/accuracy=0.593870, train/loss=1.693788, validation/accuracy=0.539540, validation/loss=2.001538, validation/num_examples=50000
I0129 13:39:13.656770 140026075662080 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.1969006061553955, loss=1.7438172101974487
I0129 13:39:47.369931 140026067269376 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.2234251499176025, loss=1.7009754180908203
I0129 13:40:21.160512 140026075662080 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.3494880199432373, loss=1.758030652999878
I0129 13:40:54.852112 140026067269376 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.141474723815918, loss=1.6924840211868286
I0129 13:41:28.570775 140026075662080 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.3818979263305664, loss=1.7680271863937378
I0129 13:42:02.319412 140026067269376 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.3042409420013428, loss=1.7030258178710938
I0129 13:42:36.041638 140026075662080 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.458338975906372, loss=1.7843587398529053
I0129 13:43:09.711337 140026067269376 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.20160174369812, loss=1.6796356439590454
I0129 13:43:43.382949 140026075662080 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.380429983139038, loss=1.7328193187713623
I0129 13:44:17.139825 140026067269376 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.4939229488372803, loss=1.6858985424041748
I0129 13:44:50.944361 140026075662080 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.359752655029297, loss=1.7047581672668457
I0129 13:45:24.711014 140026067269376 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.555436134338379, loss=1.6418201923370361
I0129 13:45:58.500817 140026075662080 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.613478183746338, loss=1.840814471244812
I0129 13:46:32.179031 140026067269376 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.3574390411376953, loss=1.834035873413086
I0129 13:47:05.856498 140026075662080 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.4726409912109375, loss=1.8434255123138428
I0129 13:47:19.809010 140187804313408 spec.py:321] Evaluating on the training split.
I0129 13:47:26.049551 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 13:47:34.780884 140187804313408 spec.py:349] Evaluating on the test split.
I0129 13:47:37.332178 140187804313408 submission_runner.py:408] Time since start: 44405.29s, 	Step: 126843, 	{'train/accuracy': 0.5989516973495483, 'train/loss': 1.6441659927368164, 'validation/accuracy': 0.5536400079727173, 'validation/loss': 1.9106814861297607, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.6522583961486816, 'test/num_examples': 10000, 'score': 42876.69231629372, 'total_duration': 44405.286915779114, 'accumulated_submission_time': 42876.69231629372, 'accumulated_eval_time': 1518.3210427761078, 'accumulated_logging_time': 5.644772291183472}
I0129 13:47:37.372271 140026159523584 logging_writer.py:48] [126843] accumulated_eval_time=1518.321043, accumulated_logging_time=5.644772, accumulated_submission_time=42876.692316, global_step=126843, preemption_count=0, score=42876.692316, test/accuracy=0.443200, test/loss=2.652258, test/num_examples=10000, total_duration=44405.286916, train/accuracy=0.598952, train/loss=1.644166, validation/accuracy=0.553640, validation/loss=1.910681, validation/num_examples=50000
I0129 13:47:56.957212 140026167916288 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.287935972213745, loss=1.6932997703552246
I0129 13:48:30.670707 140026159523584 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.331439256668091, loss=1.7111055850982666
I0129 13:49:04.323017 140026167916288 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.328695774078369, loss=1.7202558517456055
I0129 13:49:38.050441 140026159523584 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.4239840507507324, loss=1.6578627824783325
I0129 13:50:11.836846 140026167916288 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.371706962585449, loss=1.7417399883270264
I0129 13:50:45.545907 140026159523584 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.493330955505371, loss=1.7201313972473145
I0129 13:51:19.296910 140026167916288 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.825913906097412, loss=1.6737215518951416
I0129 13:51:53.130724 140026159523584 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.313887596130371, loss=1.6397387981414795
I0129 13:52:26.851767 140026167916288 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.25687313079834, loss=1.6354172229766846
I0129 13:53:00.569628 140026159523584 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.3473148345947266, loss=1.8248968124389648
I0129 13:53:34.331223 140026167916288 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.343273162841797, loss=1.5740680694580078
I0129 13:54:08.055274 140026159523584 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.745215654373169, loss=1.7840850353240967
I0129 13:54:41.721493 140026167916288 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.340428590774536, loss=1.6125671863555908
I0129 13:55:15.436163 140026159523584 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.2995047569274902, loss=1.6886435747146606
I0129 13:55:49.106673 140026167916288 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.3239572048187256, loss=1.6957203149795532
I0129 13:56:07.476335 140187804313408 spec.py:321] Evaluating on the training split.
I0129 13:56:13.745108 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 13:56:22.436036 140187804313408 spec.py:349] Evaluating on the test split.
I0129 13:56:25.060348 140187804313408 submission_runner.py:408] Time since start: 44933.02s, 	Step: 128356, 	{'train/accuracy': 0.6427175998687744, 'train/loss': 1.4331533908843994, 'validation/accuracy': 0.5913599729537964, 'validation/loss': 1.713654637336731, 'validation/num_examples': 50000, 'test/accuracy': 0.4610000252723694, 'test/loss': 2.505321979522705, 'test/num_examples': 10000, 'score': 43386.7288172245, 'total_duration': 44933.01505827904, 'accumulated_submission_time': 43386.7288172245, 'accumulated_eval_time': 1535.9049890041351, 'accumulated_logging_time': 5.6943440437316895}
I0129 13:56:25.104198 140026067269376 logging_writer.py:48] [128356] accumulated_eval_time=1535.904989, accumulated_logging_time=5.694344, accumulated_submission_time=43386.728817, global_step=128356, preemption_count=0, score=43386.728817, test/accuracy=0.461000, test/loss=2.505322, test/num_examples=10000, total_duration=44933.015058, train/accuracy=0.642718, train/loss=1.433153, validation/accuracy=0.591360, validation/loss=1.713655, validation/num_examples=50000
I0129 13:56:40.253531 140026075662080 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.3866374492645264, loss=1.6169912815093994
I0129 13:57:13.936923 140026067269376 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.540100336074829, loss=1.7466669082641602
I0129 13:57:47.596746 140026075662080 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.4007434844970703, loss=1.617283821105957
I0129 13:58:21.382245 140026067269376 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.3793203830718994, loss=1.6603662967681885
I0129 13:58:55.137321 140026075662080 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.564140558242798, loss=1.8449122905731201
I0129 13:59:28.849857 140026067269376 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.491482734680176, loss=1.7256715297698975
I0129 14:00:02.559534 140026075662080 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.406195640563965, loss=1.6953614950180054
I0129 14:00:36.372625 140026067269376 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.533005714416504, loss=1.7479248046875
I0129 14:01:10.072696 140026075662080 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.8104188442230225, loss=1.653067946434021
I0129 14:01:43.840306 140026067269376 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.5223710536956787, loss=1.7936450242996216
I0129 14:02:17.595893 140026075662080 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.539353370666504, loss=1.6923364400863647
I0129 14:02:51.338398 140026067269376 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.5999159812927246, loss=1.833492636680603
I0129 14:03:25.015918 140026075662080 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.43233585357666, loss=1.599715232849121
I0129 14:03:58.773302 140026067269376 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.472618818283081, loss=1.6335437297821045
I0129 14:04:32.462039 140026075662080 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.6409504413604736, loss=1.750569462776184
I0129 14:04:55.287451 140187804313408 spec.py:321] Evaluating on the training split.
I0129 14:05:01.624096 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 14:05:10.659270 140187804313408 spec.py:349] Evaluating on the test split.
I0129 14:05:13.281041 140187804313408 submission_runner.py:408] Time since start: 45461.24s, 	Step: 129869, 	{'train/accuracy': 0.5605269074440002, 'train/loss': 1.8894774913787842, 'validation/accuracy': 0.5187999606132507, 'validation/loss': 2.140009880065918, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.9258809089660645, 'test/num_examples': 10000, 'score': 43896.84852051735, 'total_duration': 45461.23577904701, 'accumulated_submission_time': 43896.84852051735, 'accumulated_eval_time': 1553.898535490036, 'accumulated_logging_time': 5.7474260330200195}
I0129 14:05:13.325756 140026058876672 logging_writer.py:48] [129869] accumulated_eval_time=1553.898535, accumulated_logging_time=5.747426, accumulated_submission_time=43896.848521, global_step=129869, preemption_count=0, score=43896.848521, test/accuracy=0.406100, test/loss=2.925881, test/num_examples=10000, total_duration=45461.235779, train/accuracy=0.560527, train/loss=1.889477, validation/accuracy=0.518800, validation/loss=2.140010, validation/num_examples=50000
I0129 14:05:24.104084 140026159523584 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.5625200271606445, loss=1.6189863681793213
I0129 14:05:57.767846 140026058876672 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.441169500350952, loss=1.6987707614898682
I0129 14:06:31.465071 140026159523584 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.481788158416748, loss=1.6847240924835205
I0129 14:07:05.113876 140026058876672 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.444716691970825, loss=1.582382321357727
I0129 14:07:38.807439 140026159523584 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.5711920261383057, loss=1.7608529329299927
I0129 14:08:12.479356 140026058876672 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.7538859844207764, loss=1.746157169342041
I0129 14:08:46.143501 140026159523584 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.485304832458496, loss=1.8026821613311768
I0129 14:09:19.906117 140026058876672 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.5672502517700195, loss=1.8101420402526855
I0129 14:09:53.627705 140026159523584 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.655083417892456, loss=1.5985819101333618
I0129 14:10:27.408868 140026058876672 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.510601043701172, loss=1.6133816242218018
I0129 14:11:01.100704 140026159523584 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.5160882472991943, loss=1.655479907989502
I0129 14:11:34.894284 140026058876672 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.634970188140869, loss=1.6358356475830078
I0129 14:12:08.575776 140026159523584 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.5048913955688477, loss=1.677462100982666
I0129 14:12:42.252686 140026058876672 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.3857510089874268, loss=1.6097029447555542
I0129 14:13:15.924089 140026159523584 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.5912601947784424, loss=1.6799465417861938
I0129 14:13:43.339054 140187804313408 spec.py:321] Evaluating on the training split.
I0129 14:13:49.587009 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 14:13:58.429856 140187804313408 spec.py:349] Evaluating on the test split.
I0129 14:14:01.059492 140187804313408 submission_runner.py:408] Time since start: 45989.01s, 	Step: 131383, 	{'train/accuracy': 0.6741071343421936, 'train/loss': 1.2939298152923584, 'validation/accuracy': 0.5915399789810181, 'validation/loss': 1.7248343229293823, 'validation/num_examples': 50000, 'test/accuracy': 0.4692000150680542, 'test/loss': 2.4976091384887695, 'test/num_examples': 10000, 'score': 44406.79634475708, 'total_duration': 45989.0142326355, 'accumulated_submission_time': 44406.79634475708, 'accumulated_eval_time': 1571.618933916092, 'accumulated_logging_time': 5.801463842391968}
I0129 14:14:01.104227 140026075662080 logging_writer.py:48] [131383] accumulated_eval_time=1571.618934, accumulated_logging_time=5.801464, accumulated_submission_time=44406.796345, global_step=131383, preemption_count=0, score=44406.796345, test/accuracy=0.469200, test/loss=2.497609, test/num_examples=10000, total_duration=45989.014233, train/accuracy=0.674107, train/loss=1.293930, validation/accuracy=0.591540, validation/loss=1.724834, validation/num_examples=50000
I0129 14:14:07.188210 140026151130880 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.534977912902832, loss=1.6527831554412842
I0129 14:14:40.908866 140026075662080 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.431765556335449, loss=1.524383544921875
I0129 14:15:14.654307 140026151130880 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.590759515762329, loss=1.737937331199646
I0129 14:15:48.382716 140026075662080 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.6427626609802246, loss=1.5803371667861938
I0129 14:16:22.135352 140026151130880 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.8142921924591064, loss=1.595644235610962
I0129 14:16:55.902571 140026075662080 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.4813315868377686, loss=1.7187309265136719
I0129 14:17:29.633133 140026151130880 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.5866594314575195, loss=1.674277901649475
I0129 14:18:03.573729 140026075662080 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.5651347637176514, loss=1.7544430494308472
I0129 14:18:37.328471 140026151130880 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.6476376056671143, loss=1.7110140323638916
I0129 14:19:11.100557 140026075662080 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.5094799995422363, loss=1.7441082000732422
I0129 14:19:44.820756 140026151130880 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.584594488143921, loss=1.6509428024291992
I0129 14:20:18.513833 140026075662080 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.5573437213897705, loss=1.698950171470642
I0129 14:20:52.295252 140026151130880 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.5651419162750244, loss=1.614316701889038
I0129 14:21:25.954603 140026075662080 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.709085702896118, loss=1.69056236743927
I0129 14:21:59.651938 140026151130880 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.7281720638275146, loss=1.6517112255096436
I0129 14:22:31.158911 140187804313408 spec.py:321] Evaluating on the training split.
I0129 14:22:37.410287 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 14:22:46.086740 140187804313408 spec.py:349] Evaluating on the test split.
I0129 14:22:48.705297 140187804313408 submission_runner.py:408] Time since start: 46516.66s, 	Step: 132895, 	{'train/accuracy': 0.6228475570678711, 'train/loss': 1.5460199117660522, 'validation/accuracy': 0.5662800073623657, 'validation/loss': 1.8539618253707886, 'validation/num_examples': 50000, 'test/accuracy': 0.444100022315979, 'test/loss': 2.6210970878601074, 'test/num_examples': 10000, 'score': 44916.785944223404, 'total_duration': 46516.66002821922, 'accumulated_submission_time': 44916.785944223404, 'accumulated_eval_time': 1589.165275335312, 'accumulated_logging_time': 5.855878114700317}
I0129 14:22:48.749129 140026067269376 logging_writer.py:48] [132895] accumulated_eval_time=1589.165275, accumulated_logging_time=5.855878, accumulated_submission_time=44916.785944, global_step=132895, preemption_count=0, score=44916.785944, test/accuracy=0.444100, test/loss=2.621097, test/num_examples=10000, total_duration=46516.660028, train/accuracy=0.622848, train/loss=1.546020, validation/accuracy=0.566280, validation/loss=1.853962, validation/num_examples=50000
I0129 14:22:50.780199 140026159523584 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.5918588638305664, loss=1.6959426403045654
I0129 14:23:24.516999 140026067269376 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.7597053050994873, loss=1.7275357246398926
I0129 14:23:58.246809 140026159523584 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.634997606277466, loss=1.624680519104004
I0129 14:24:32.079463 140026067269376 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.5970897674560547, loss=1.661260724067688
I0129 14:25:05.800632 140026159523584 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.7277629375457764, loss=1.6108324527740479
I0129 14:25:39.496566 140026067269376 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.5095746517181396, loss=1.6487830877304077
I0129 14:26:13.275952 140026159523584 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.864499568939209, loss=1.5979654788970947
I0129 14:26:46.978234 140026067269376 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.5542378425598145, loss=1.6174352169036865
I0129 14:27:20.748357 140026159523584 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.559534788131714, loss=1.586380124092102
I0129 14:27:54.415713 140026067269376 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.6117589473724365, loss=1.702397346496582
I0129 14:28:28.115178 140026159523584 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.620009183883667, loss=1.650599718093872
I0129 14:29:01.791003 140026067269376 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.67956280708313, loss=1.749548316001892
I0129 14:29:35.475108 140026159523584 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.7518279552459717, loss=1.7395007610321045
I0129 14:30:09.241883 140026067269376 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.874297618865967, loss=1.6909325122833252
I0129 14:30:43.065074 140026159523584 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.6937131881713867, loss=1.624505639076233
I0129 14:31:16.747936 140026067269376 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.7498815059661865, loss=1.5289835929870605
I0129 14:31:18.923265 140187804313408 spec.py:321] Evaluating on the training split.
I0129 14:31:25.137889 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 14:31:34.101548 140187804313408 spec.py:349] Evaluating on the test split.
I0129 14:31:36.758667 140187804313408 submission_runner.py:408] Time since start: 47044.71s, 	Step: 134408, 	{'train/accuracy': 0.6938576102256775, 'train/loss': 1.2104731798171997, 'validation/accuracy': 0.6318199634552002, 'validation/loss': 1.532360315322876, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2486801147460938, 'test/num_examples': 10000, 'score': 45426.89608311653, 'total_duration': 47044.71316599846, 'accumulated_submission_time': 45426.89608311653, 'accumulated_eval_time': 1607.0003921985626, 'accumulated_logging_time': 5.908795595169067}
I0129 14:31:36.800817 140026042091264 logging_writer.py:48] [134408] accumulated_eval_time=1607.000392, accumulated_logging_time=5.908796, accumulated_submission_time=45426.896083, global_step=134408, preemption_count=0, score=45426.896083, test/accuracy=0.512100, test/loss=2.248680, test/num_examples=10000, total_duration=47044.713166, train/accuracy=0.693858, train/loss=1.210473, validation/accuracy=0.631820, validation/loss=1.532360, validation/num_examples=50000
I0129 14:32:08.145722 140026050483968 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.6188747882843018, loss=1.600203514099121
I0129 14:32:41.794154 140026042091264 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.6718711853027344, loss=1.6424617767333984
I0129 14:33:15.521943 140026050483968 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.8221983909606934, loss=1.6878139972686768
I0129 14:33:49.171072 140026042091264 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.708048105239868, loss=1.6453578472137451
I0129 14:34:22.903055 140026050483968 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.7750754356384277, loss=1.6863353252410889
I0129 14:34:56.560442 140026042091264 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.788097381591797, loss=1.595855712890625
I0129 14:35:30.285926 140026050483968 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.0073695182800293, loss=1.7571474313735962
I0129 14:36:03.999921 140026042091264 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.840275287628174, loss=1.6720348596572876
I0129 14:36:37.768517 140026050483968 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.8839588165283203, loss=1.7192068099975586
I0129 14:37:11.553637 140026042091264 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.7555971145629883, loss=1.6899833679199219
I0129 14:37:45.256226 140026050483968 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.609339952468872, loss=1.538358211517334
I0129 14:38:18.967259 140026042091264 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.5780835151672363, loss=1.6586796045303345
I0129 14:38:52.734330 140026050483968 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.5837581157684326, loss=1.6021392345428467
I0129 14:39:26.381915 140026042091264 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.606390953063965, loss=1.6198831796646118
I0129 14:40:00.095350 140026050483968 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.813612937927246, loss=1.5007355213165283
I0129 14:40:06.978778 140187804313408 spec.py:321] Evaluating on the training split.
I0129 14:40:13.301821 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 14:40:22.068939 140187804313408 spec.py:349] Evaluating on the test split.
I0129 14:40:24.615741 140187804313408 submission_runner.py:408] Time since start: 47572.57s, 	Step: 135922, 	{'train/accuracy': 0.6696428656578064, 'train/loss': 1.3166732788085938, 'validation/accuracy': 0.6161999702453613, 'validation/loss': 1.6147363185882568, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.346391439437866, 'test/num_examples': 10000, 'score': 45937.009231090546, 'total_duration': 47572.57046985626, 'accumulated_submission_time': 45937.009231090546, 'accumulated_eval_time': 1624.6373193264008, 'accumulated_logging_time': 5.960654020309448}
I0129 14:40:24.656870 140026050483968 logging_writer.py:48] [135922] accumulated_eval_time=1624.637319, accumulated_logging_time=5.960654, accumulated_submission_time=45937.009231, global_step=135922, preemption_count=0, score=45937.009231, test/accuracy=0.491700, test/loss=2.346391, test/num_examples=10000, total_duration=47572.570470, train/accuracy=0.669643, train/loss=1.316673, validation/accuracy=0.616200, validation/loss=1.614736, validation/num_examples=50000
I0129 14:40:51.258136 140026159523584 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.6104719638824463, loss=1.5653687715530396
I0129 14:41:24.983593 140026050483968 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.918314218521118, loss=1.6736189126968384
I0129 14:41:58.700911 140026159523584 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.6207382678985596, loss=1.5566153526306152
I0129 14:42:32.469777 140026050483968 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.7123024463653564, loss=1.5855106115341187
I0129 14:43:06.179231 140026159523584 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.580101728439331, loss=1.5892926454544067
I0129 14:43:39.992176 140026050483968 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.8981730937957764, loss=1.6971367597579956
I0129 14:44:13.732549 140026159523584 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.932985782623291, loss=1.6197400093078613
I0129 14:44:47.446596 140026050483968 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.7033042907714844, loss=1.5123467445373535
I0129 14:45:21.195820 140026159523584 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.5656769275665283, loss=1.4716280698776245
I0129 14:45:54.857082 140026050483968 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.4748446941375732, loss=1.369537115097046
I0129 14:46:28.580989 140026159523584 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.8617770671844482, loss=1.7045515775680542
I0129 14:47:02.222980 140026050483968 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.8316731452941895, loss=1.596850872039795
I0129 14:47:35.908986 140026159523584 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.8688395023345947, loss=1.6248270273208618
I0129 14:48:09.580732 140026050483968 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.7133235931396484, loss=1.4399148225784302
I0129 14:48:43.278749 140026159523584 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.8984556198120117, loss=1.63322913646698
I0129 14:48:54.871035 140187804313408 spec.py:321] Evaluating on the training split.
I0129 14:49:01.151450 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 14:49:10.146440 140187804313408 spec.py:349] Evaluating on the test split.
I0129 14:49:13.212975 140187804313408 submission_runner.py:408] Time since start: 48101.17s, 	Step: 137436, 	{'train/accuracy': 0.6374163031578064, 'train/loss': 1.4651330709457397, 'validation/accuracy': 0.5859400033950806, 'validation/loss': 1.7452064752578735, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.4913787841796875, 'test/num_examples': 10000, 'score': 46447.159615039825, 'total_duration': 48101.16772198677, 'accumulated_submission_time': 46447.159615039825, 'accumulated_eval_time': 1642.9792296886444, 'accumulated_logging_time': 6.0111939907073975}
I0129 14:49:13.246955 140026058876672 logging_writer.py:48] [137436] accumulated_eval_time=1642.979230, accumulated_logging_time=6.011194, accumulated_submission_time=46447.159615, global_step=137436, preemption_count=0, score=46447.159615, test/accuracy=0.462200, test/loss=2.491379, test/num_examples=10000, total_duration=48101.167722, train/accuracy=0.637416, train/loss=1.465133, validation/accuracy=0.585940, validation/loss=1.745206, validation/num_examples=50000
I0129 14:49:35.163555 140026067269376 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.6563563346862793, loss=1.4964886903762817
I0129 14:50:08.974353 140026058876672 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.86086106300354, loss=1.6018859148025513
I0129 14:50:42.678552 140026067269376 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.8301045894622803, loss=1.6307224035263062
I0129 14:51:16.381788 140026058876672 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.798027515411377, loss=1.5667858123779297
I0129 14:51:50.145032 140026067269376 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.81813907623291, loss=1.544043779373169
I0129 14:52:23.837784 140026058876672 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.6313090324401855, loss=1.5538901090621948
I0129 14:52:57.496496 140026067269376 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.829120635986328, loss=1.6156761646270752
I0129 14:53:31.205928 140026058876672 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.8737680912017822, loss=1.5477489233016968
I0129 14:54:04.959289 140026067269376 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.911942958831787, loss=1.650604248046875
I0129 14:54:38.671401 140026058876672 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.7843260765075684, loss=1.5555601119995117
I0129 14:55:12.337834 140026067269376 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.7149035930633545, loss=1.5646121501922607
I0129 14:55:46.057053 140026058876672 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.9040277004241943, loss=1.5659301280975342
I0129 14:56:19.804816 140026067269376 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.0618741512298584, loss=1.5786690711975098
I0129 14:56:53.547836 140026058876672 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.9327948093414307, loss=1.593825340270996
I0129 14:57:27.223295 140026067269376 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.681929111480713, loss=1.4897232055664062
I0129 14:57:43.240599 140187804313408 spec.py:321] Evaluating on the training split.
I0129 14:57:49.566181 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 14:57:58.475594 140187804313408 spec.py:349] Evaluating on the test split.
I0129 14:58:01.071371 140187804313408 submission_runner.py:408] Time since start: 48629.03s, 	Step: 138949, 	{'train/accuracy': 0.6802853941917419, 'train/loss': 1.2585314512252808, 'validation/accuracy': 0.6293399930000305, 'validation/loss': 1.5424376726150513, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.300657033920288, 'test/num_examples': 10000, 'score': 46957.08819317818, 'total_duration': 48629.026055812836, 'accumulated_submission_time': 46957.08819317818, 'accumulated_eval_time': 1660.8099303245544, 'accumulated_logging_time': 6.053507328033447}
I0129 14:58:01.134144 140026151130880 logging_writer.py:48] [138949] accumulated_eval_time=1660.809930, accumulated_logging_time=6.053507, accumulated_submission_time=46957.088193, global_step=138949, preemption_count=0, score=46957.088193, test/accuracy=0.495900, test/loss=2.300657, test/num_examples=10000, total_duration=48629.026056, train/accuracy=0.680285, train/loss=1.258531, validation/accuracy=0.629340, validation/loss=1.542438, validation/num_examples=50000
I0129 14:58:18.692138 140026159523584 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.7925562858581543, loss=1.622015118598938
I0129 14:58:52.397963 140026151130880 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.854995012283325, loss=1.557032585144043
I0129 14:59:26.055920 140026159523584 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.14453387260437, loss=1.4825800657272339
I0129 14:59:59.760185 140026151130880 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.9040720462799072, loss=1.5377463102340698
I0129 15:00:33.428227 140026159523584 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.7204372882843018, loss=1.5425710678100586
I0129 15:01:07.194736 140026151130880 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.9252641201019287, loss=1.5200680494308472
I0129 15:01:40.896161 140026159523584 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.965334177017212, loss=1.593759298324585
I0129 15:02:14.684424 140026151130880 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.9672768115997314, loss=1.5758192539215088
I0129 15:02:48.357296 140026159523584 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.877826452255249, loss=1.5538508892059326
I0129 15:03:22.167881 140026151130880 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.901721715927124, loss=1.5664560794830322
I0129 15:03:55.825625 140026159523584 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.667846918106079, loss=1.589786410331726
I0129 15:04:29.617475 140026151130880 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.95245099067688, loss=1.5622795820236206
I0129 15:05:03.362841 140026159523584 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.922651529312134, loss=1.5676144361495972
I0129 15:05:37.123359 140026151130880 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.372319459915161, loss=1.6135501861572266
I0129 15:06:10.868415 140026159523584 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.8241684436798096, loss=1.522956132888794
I0129 15:06:31.261676 140187804313408 spec.py:321] Evaluating on the training split.
I0129 15:06:37.599799 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 15:06:46.448392 140187804313408 spec.py:349] Evaluating on the test split.
I0129 15:06:49.108415 140187804313408 submission_runner.py:408] Time since start: 49157.06s, 	Step: 140462, 	{'train/accuracy': 0.7210419178009033, 'train/loss': 1.0726977586746216, 'validation/accuracy': 0.6291199922561646, 'validation/loss': 1.5267126560211182, 'validation/num_examples': 50000, 'test/accuracy': 0.5039000511169434, 'test/loss': 2.279113292694092, 'test/num_examples': 10000, 'score': 47467.14528131485, 'total_duration': 49157.062989234924, 'accumulated_submission_time': 47467.14528131485, 'accumulated_eval_time': 1678.656478881836, 'accumulated_logging_time': 6.13153076171875}
I0129 15:06:49.149164 140026058876672 logging_writer.py:48] [140462] accumulated_eval_time=1678.656479, accumulated_logging_time=6.131531, accumulated_submission_time=47467.145281, global_step=140462, preemption_count=0, score=47467.145281, test/accuracy=0.503900, test/loss=2.279113, test/num_examples=10000, total_duration=49157.062989, train/accuracy=0.721042, train/loss=1.072698, validation/accuracy=0.629120, validation/loss=1.526713, validation/num_examples=50000
I0129 15:07:02.306999 140026067269376 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.975515127182007, loss=1.5505154132843018
I0129 15:07:36.027844 140026058876672 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.786886215209961, loss=1.5592844486236572
I0129 15:08:09.718143 140026067269376 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.283881664276123, loss=1.5184370279312134
I0129 15:08:43.390051 140026058876672 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.0908472537994385, loss=1.5638943910598755
I0129 15:09:17.097701 140026067269376 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.167747974395752, loss=1.632143259048462
I0129 15:09:50.941715 140026058876672 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.11299729347229, loss=1.6301977634429932
I0129 15:10:24.655519 140026067269376 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.9720892906188965, loss=1.5312528610229492
I0129 15:10:58.400968 140026058876672 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.9698119163513184, loss=1.515197992324829
I0129 15:11:32.099960 140026067269376 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.2037317752838135, loss=1.4832123517990112
I0129 15:12:05.788782 140026058876672 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.0850393772125244, loss=1.514601707458496
I0129 15:12:39.490645 140026067269376 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.0575952529907227, loss=1.5175330638885498
I0129 15:13:13.165795 140026058876672 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.216226100921631, loss=1.5287189483642578
I0129 15:13:46.854080 140026067269376 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.223548412322998, loss=1.6519088745117188
I0129 15:14:20.558749 140026058876672 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.985938310623169, loss=1.5462772846221924
I0129 15:14:54.312650 140026067269376 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.959486484527588, loss=1.398931860923767
I0129 15:15:19.364649 140187804313408 spec.py:321] Evaluating on the training split.
I0129 15:15:25.785300 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 15:15:34.414538 140187804313408 spec.py:349] Evaluating on the test split.
I0129 15:15:37.087210 140187804313408 submission_runner.py:408] Time since start: 49685.04s, 	Step: 141976, 	{'train/accuracy': 0.7115154266357422, 'train/loss': 1.1081956624984741, 'validation/accuracy': 0.6423599720001221, 'validation/loss': 1.4797382354736328, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.2130398750305176, 'test/num_examples': 10000, 'score': 47977.29451775551, 'total_duration': 49685.04183840752, 'accumulated_submission_time': 47977.29451775551, 'accumulated_eval_time': 1696.3789055347443, 'accumulated_logging_time': 6.181941986083984}
I0129 15:15:37.130526 140026042091264 logging_writer.py:48] [141976] accumulated_eval_time=1696.378906, accumulated_logging_time=6.181942, accumulated_submission_time=47977.294518, global_step=141976, preemption_count=0, score=47977.294518, test/accuracy=0.509200, test/loss=2.213040, test/num_examples=10000, total_duration=49685.041838, train/accuracy=0.711515, train/loss=1.108196, validation/accuracy=0.642360, validation/loss=1.479738, validation/num_examples=50000
I0129 15:15:45.588216 140026050483968 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.0848119258880615, loss=1.4700126647949219
I0129 15:16:19.408209 140026042091264 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.255526065826416, loss=1.4930610656738281
I0129 15:16:53.113430 140026050483968 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.9521563053131104, loss=1.5192198753356934
I0129 15:17:27.264532 140026042091264 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.11958909034729, loss=1.6016675233840942
I0129 15:18:01.031688 140026050483968 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.996159315109253, loss=1.527743935585022
I0129 15:18:34.703668 140026042091264 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.8802597522735596, loss=1.4817745685577393
I0129 15:19:08.400156 140026050483968 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.1709163188934326, loss=1.4553775787353516
I0129 15:19:42.132184 140026042091264 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.995119333267212, loss=1.5081757307052612
I0129 15:20:15.898384 140026050483968 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.0948827266693115, loss=1.4807701110839844
I0129 15:20:49.642295 140026042091264 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.2533607482910156, loss=1.7099378108978271
I0129 15:21:23.366872 140026050483968 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.0154032707214355, loss=1.5668987035751343
I0129 15:21:57.078114 140026042091264 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.0684902667999268, loss=1.4634695053100586
I0129 15:22:30.963931 140026050483968 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.0447702407836914, loss=1.4291335344314575
I0129 15:23:04.713240 140026042091264 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.1402056217193604, loss=1.5922741889953613
I0129 15:23:38.436805 140026050483968 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.173628568649292, loss=1.5054450035095215
I0129 15:24:07.204837 140187804313408 spec.py:321] Evaluating on the training split.
I0129 15:24:13.508628 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 15:24:22.226733 140187804313408 spec.py:349] Evaluating on the test split.
I0129 15:24:24.892419 140187804313408 submission_runner.py:408] Time since start: 50212.85s, 	Step: 143487, 	{'train/accuracy': 0.7179726958274841, 'train/loss': 1.1000560522079468, 'validation/accuracy': 0.6461799740791321, 'validation/loss': 1.4582358598709106, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.1681156158447266, 'test/num_examples': 10000, 'score': 48487.306156396866, 'total_duration': 50212.84715676308, 'accumulated_submission_time': 48487.306156396866, 'accumulated_eval_time': 1714.0664644241333, 'accumulated_logging_time': 6.234250068664551}
I0129 15:24:24.934857 140026058876672 logging_writer.py:48] [143487] accumulated_eval_time=1714.066464, accumulated_logging_time=6.234250, accumulated_submission_time=48487.306156, global_step=143487, preemption_count=0, score=48487.306156, test/accuracy=0.521900, test/loss=2.168116, test/num_examples=10000, total_duration=50212.847157, train/accuracy=0.717973, train/loss=1.100056, validation/accuracy=0.646180, validation/loss=1.458236, validation/num_examples=50000
I0129 15:24:29.655158 140026167916288 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.347500801086426, loss=1.5290378332138062
I0129 15:25:03.377880 140026058876672 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.213610887527466, loss=1.5809179544448853
I0129 15:25:37.076830 140026167916288 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.093474864959717, loss=1.4837836027145386
I0129 15:26:10.736938 140026058876672 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.1729679107666016, loss=1.4479246139526367
I0129 15:26:44.549348 140026167916288 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.16695499420166, loss=1.500141978263855
I0129 15:27:18.244026 140026058876672 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.223545789718628, loss=1.5626397132873535
I0129 15:27:52.014458 140026167916288 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.381530284881592, loss=1.589342713356018
I0129 15:28:25.724615 140026058876672 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.204958915710449, loss=1.5836691856384277
I0129 15:28:59.541001 140026167916288 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.072131633758545, loss=1.4843032360076904
I0129 15:29:33.203677 140026058876672 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.3426716327667236, loss=1.5502773523330688
I0129 15:30:06.891950 140026167916288 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.9327683448791504, loss=1.5081250667572021
I0129 15:30:40.553529 140026058876672 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.9906857013702393, loss=1.4646292924880981
I0129 15:31:14.234285 140026167916288 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.223906993865967, loss=1.6085456609725952
I0129 15:31:47.939495 140026058876672 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.1526670455932617, loss=1.505611777305603
I0129 15:32:21.713809 140026167916288 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.232830762863159, loss=1.4945427179336548
I0129 15:32:55.391373 140026058876672 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.451113224029541, loss=1.4924473762512207
I0129 15:32:55.399282 140187804313408 spec.py:321] Evaluating on the training split.
I0129 15:33:01.876312 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 15:33:10.774169 140187804313408 spec.py:349] Evaluating on the test split.
I0129 15:33:13.413653 140187804313408 submission_runner.py:408] Time since start: 50741.37s, 	Step: 145001, 	{'train/accuracy': 0.7001753449440002, 'train/loss': 1.1895331144332886, 'validation/accuracy': 0.639519989490509, 'validation/loss': 1.4978569746017456, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.2445452213287354, 'test/num_examples': 10000, 'score': 48997.7052898407, 'total_duration': 50741.36831307411, 'accumulated_submission_time': 48997.7052898407, 'accumulated_eval_time': 1732.0806908607483, 'accumulated_logging_time': 6.286098003387451}
I0129 15:33:13.461692 140026075662080 logging_writer.py:48] [145001] accumulated_eval_time=1732.080691, accumulated_logging_time=6.286098, accumulated_submission_time=48997.705290, global_step=145001, preemption_count=0, score=48997.705290, test/accuracy=0.511200, test/loss=2.244545, test/num_examples=10000, total_duration=50741.368313, train/accuracy=0.700175, train/loss=1.189533, validation/accuracy=0.639520, validation/loss=1.497857, validation/num_examples=50000
I0129 15:33:47.179408 140026151130880 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.165254831314087, loss=1.4218378067016602
I0129 15:34:20.840757 140026075662080 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.16792631149292, loss=1.497811198234558
I0129 15:34:54.520741 140026151130880 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.055015802383423, loss=1.4705636501312256
I0129 15:35:28.332258 140026075662080 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.4100501537323, loss=1.3452911376953125
I0129 15:36:02.056385 140026151130880 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.2172152996063232, loss=1.4844920635223389
I0129 15:36:35.737131 140026075662080 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.1095738410949707, loss=1.4261562824249268
I0129 15:37:09.522856 140026151130880 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.14815354347229, loss=1.5073161125183105
I0129 15:37:43.302835 140026075662080 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.204537868499756, loss=1.609140396118164
I0129 15:38:17.024435 140026151130880 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.074636936187744, loss=1.4012054204940796
I0129 15:38:50.789951 140026075662080 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.3002800941467285, loss=1.5203444957733154
I0129 15:39:24.504263 140026151130880 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.157092809677124, loss=1.416541337966919
I0129 15:39:58.282632 140026075662080 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.2607474327087402, loss=1.4603288173675537
I0129 15:40:32.025397 140026151130880 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.4108989238739014, loss=1.461808681488037
I0129 15:41:05.789074 140026075662080 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.32098650932312, loss=1.4886581897735596
I0129 15:41:39.606290 140026151130880 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.188530206680298, loss=1.4278006553649902
I0129 15:41:43.473976 140187804313408 spec.py:321] Evaluating on the training split.
I0129 15:41:49.754824 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 15:41:58.567451 140187804313408 spec.py:349] Evaluating on the test split.
I0129 15:42:01.235552 140187804313408 submission_runner.py:408] Time since start: 51269.19s, 	Step: 146513, 	{'train/accuracy': 0.6998565196990967, 'train/loss': 1.1637591123580933, 'validation/accuracy': 0.6406199932098389, 'validation/loss': 1.486459493637085, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.2266359329223633, 'test/num_examples': 10000, 'score': 49507.652215242386, 'total_duration': 51269.190279722214, 'accumulated_submission_time': 49507.652215242386, 'accumulated_eval_time': 1749.8422105312347, 'accumulated_logging_time': 6.3434507846832275}
I0129 15:42:01.278363 140026058876672 logging_writer.py:48] [146513] accumulated_eval_time=1749.842211, accumulated_logging_time=6.343451, accumulated_submission_time=49507.652215, global_step=146513, preemption_count=0, score=49507.652215, test/accuracy=0.517900, test/loss=2.226636, test/num_examples=10000, total_duration=51269.190280, train/accuracy=0.699857, train/loss=1.163759, validation/accuracy=0.640620, validation/loss=1.486459, validation/num_examples=50000
I0129 15:42:30.948712 140026067269376 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.1062002182006836, loss=1.4833860397338867
I0129 15:43:04.615709 140026058876672 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.1780457496643066, loss=1.3925104141235352
I0129 15:43:38.359305 140026067269376 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.681025743484497, loss=1.4915050268173218
I0129 15:44:12.032707 140026058876672 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.1744260787963867, loss=1.5480756759643555
I0129 15:44:45.759541 140026067269376 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.28759503364563, loss=1.571550726890564
I0129 15:45:19.521172 140026058876672 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.20229172706604, loss=1.481864333152771
I0129 15:45:53.260784 140026067269376 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.2025129795074463, loss=1.3961385488510132
I0129 15:46:26.949327 140026058876672 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.451282501220703, loss=1.3910489082336426
I0129 15:47:00.758178 140026067269376 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.1983256340026855, loss=1.4407247304916382
I0129 15:47:34.451580 140026058876672 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.316537857055664, loss=1.4283076524734497
I0129 15:48:08.344355 140026067269376 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.255228281021118, loss=1.5004053115844727
I0129 15:48:42.077482 140026058876672 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.214942455291748, loss=1.45760178565979
I0129 15:49:15.739799 140026067269376 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.854856491088867, loss=1.510693907737732
I0129 15:49:49.377911 140026058876672 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.2827022075653076, loss=1.4973340034484863
I0129 15:50:23.074908 140026067269376 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.4319908618927, loss=1.389915108680725
I0129 15:50:31.312367 140187804313408 spec.py:321] Evaluating on the training split.
I0129 15:50:37.528871 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 15:50:46.419344 140187804313408 spec.py:349] Evaluating on the test split.
I0129 15:50:49.100804 140187804313408 submission_runner.py:408] Time since start: 51797.06s, 	Step: 148026, 	{'train/accuracy': 0.7155213356018066, 'train/loss': 1.1146916151046753, 'validation/accuracy': 0.6486999988555908, 'validation/loss': 1.4627606868743896, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.186429262161255, 'test/num_examples': 10000, 'score': 50017.61883044243, 'total_duration': 51797.05554533005, 'accumulated_submission_time': 50017.61883044243, 'accumulated_eval_time': 1767.6306097507477, 'accumulated_logging_time': 6.3975958824157715}
I0129 15:50:49.147361 140026159523584 logging_writer.py:48] [148026] accumulated_eval_time=1767.630610, accumulated_logging_time=6.397596, accumulated_submission_time=50017.618830, global_step=148026, preemption_count=0, score=50017.618830, test/accuracy=0.524400, test/loss=2.186429, test/num_examples=10000, total_duration=51797.055545, train/accuracy=0.715521, train/loss=1.114692, validation/accuracy=0.648700, validation/loss=1.462761, validation/num_examples=50000
I0129 15:51:14.462055 140026167916288 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.334979772567749, loss=1.4029755592346191
I0129 15:51:48.183676 140026159523584 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.3310916423797607, loss=1.3603352308273315
I0129 15:52:21.925835 140026167916288 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.401890993118286, loss=1.289462924003601
I0129 15:52:55.675583 140026159523584 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.41141676902771, loss=1.358031988143921
I0129 15:53:29.443267 140026167916288 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.269960880279541, loss=1.4766416549682617
I0129 15:54:03.150791 140026159523584 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.3525333404541016, loss=1.4666399955749512
I0129 15:54:36.981517 140026167916288 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.3743817806243896, loss=1.4728840589523315
I0129 15:55:10.750841 140026159523584 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.5135881900787354, loss=1.3654074668884277
I0129 15:55:44.440543 140026167916288 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.0981178283691406, loss=1.3112143278121948
I0129 15:56:18.151210 140026159523584 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.487077236175537, loss=1.4866247177124023
I0129 15:56:51.922978 140026167916288 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.8091378211975098, loss=1.4956644773483276
I0129 15:57:25.642417 140026159523584 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.484848737716675, loss=1.3476521968841553
I0129 15:57:59.431598 140026167916288 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.5779857635498047, loss=1.4713796377182007
I0129 15:58:33.213047 140026159523584 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.3903627395629883, loss=1.4048559665679932
I0129 15:59:06.936811 140026167916288 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.3300271034240723, loss=1.4577198028564453
I0129 15:59:19.235677 140187804313408 spec.py:321] Evaluating on the training split.
I0129 15:59:25.560814 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 15:59:34.435151 140187804313408 spec.py:349] Evaluating on the test split.
I0129 15:59:37.069770 140187804313408 submission_runner.py:408] Time since start: 52325.02s, 	Step: 149538, 	{'train/accuracy': 0.7698501348495483, 'train/loss': 0.8804860711097717, 'validation/accuracy': 0.6730799674987793, 'validation/loss': 1.3303167819976807, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.0321311950683594, 'test/num_examples': 10000, 'score': 50527.641800403595, 'total_duration': 52325.024513721466, 'accumulated_submission_time': 50527.641800403595, 'accumulated_eval_time': 1785.4646661281586, 'accumulated_logging_time': 6.454508543014526}
I0129 15:59:37.113423 140026050483968 logging_writer.py:48] [149538] accumulated_eval_time=1785.464666, accumulated_logging_time=6.454509, accumulated_submission_time=50527.641800, global_step=149538, preemption_count=0, score=50527.641800, test/accuracy=0.539900, test/loss=2.032131, test/num_examples=10000, total_duration=52325.024514, train/accuracy=0.769850, train/loss=0.880486, validation/accuracy=0.673080, validation/loss=1.330317, validation/num_examples=50000
I0129 15:59:58.369815 140026058876672 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.508543014526367, loss=1.4631011486053467
I0129 16:00:32.116723 140026050483968 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.1562283039093018, loss=1.3997536897659302
I0129 16:01:05.874907 140026058876672 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.554213523864746, loss=1.4245953559875488
I0129 16:01:39.616625 140026050483968 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.49837327003479, loss=1.3823251724243164
I0129 16:02:13.362879 140026058876672 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.526613712310791, loss=1.3610326051712036
I0129 16:02:47.106024 140026050483968 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.4888436794281006, loss=1.4618476629257202
I0129 16:03:20.752084 140026058876672 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.361482620239258, loss=1.2821840047836304
I0129 16:03:54.514618 140026050483968 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.7277729511260986, loss=1.4389309883117676
I0129 16:04:28.195597 140026058876672 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.567918062210083, loss=1.3246407508850098
I0129 16:05:01.908952 140026050483968 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.528336524963379, loss=1.4496718645095825
I0129 16:05:35.576756 140026058876672 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.420121908187866, loss=1.4806705713272095
I0129 16:06:09.328293 140026050483968 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.584104061126709, loss=1.3601734638214111
I0129 16:06:43.020128 140026058876672 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.5391252040863037, loss=1.3232412338256836
I0129 16:07:16.772059 140026050483968 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.7357468605041504, loss=1.5671993494033813
I0129 16:07:50.607012 140026058876672 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.5031304359436035, loss=1.2907991409301758
I0129 16:08:07.268941 140187804313408 spec.py:321] Evaluating on the training split.
I0129 16:08:13.576791 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 16:08:22.210546 140187804313408 spec.py:349] Evaluating on the test split.
I0129 16:08:24.930625 140187804313408 submission_runner.py:408] Time since start: 52852.88s, 	Step: 151051, 	{'train/accuracy': 0.7565170526504517, 'train/loss': 0.9239798188209534, 'validation/accuracy': 0.6729199886322021, 'validation/loss': 1.331039309501648, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.099137544631958, 'test/num_examples': 10000, 'score': 51037.73308491707, 'total_duration': 52852.88480973244, 'accumulated_submission_time': 51037.73308491707, 'accumulated_eval_time': 1803.1257722377777, 'accumulated_logging_time': 6.508534669876099}
I0129 16:08:24.975708 140026167916288 logging_writer.py:48] [151051] accumulated_eval_time=1803.125772, accumulated_logging_time=6.508535, accumulated_submission_time=51037.733085, global_step=151051, preemption_count=0, score=51037.733085, test/accuracy=0.539300, test/loss=2.099138, test/num_examples=10000, total_duration=52852.884810, train/accuracy=0.756517, train/loss=0.923980, validation/accuracy=0.672920, validation/loss=1.331039, validation/num_examples=50000
I0129 16:08:41.790740 140026176308992 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.410952091217041, loss=1.4281153678894043
I0129 16:09:15.463877 140026167916288 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.6260342597961426, loss=1.4127253293991089
I0129 16:09:49.147875 140026176308992 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.48173451423645, loss=1.3786789178848267
I0129 16:10:22.867826 140026167916288 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.6527435779571533, loss=1.4092226028442383
I0129 16:10:56.614927 140026176308992 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.479126214981079, loss=1.3907032012939453
I0129 16:11:30.309858 140026167916288 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.6363420486450195, loss=1.444553017616272
I0129 16:12:04.055058 140026176308992 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.538604974746704, loss=1.330611228942871
I0129 16:12:37.831722 140026167916288 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.47247314453125, loss=1.3301069736480713
I0129 16:13:11.542352 140026176308992 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.817040205001831, loss=1.431634545326233
I0129 16:13:45.285757 140026167916288 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.6035327911376953, loss=1.4089465141296387
I0129 16:14:19.131923 140026176308992 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.589743137359619, loss=1.4064723253250122
I0129 16:14:52.850111 140026167916288 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.6885135173797607, loss=1.4215490818023682
I0129 16:15:26.577500 140026176308992 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.708662986755371, loss=1.4214146137237549
I0129 16:16:00.332873 140026167916288 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.7028162479400635, loss=1.3973549604415894
I0129 16:16:34.021478 140026176308992 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.2706844806671143, loss=1.2907496690750122
I0129 16:16:55.123003 140187804313408 spec.py:321] Evaluating on the training split.
I0129 16:17:01.351944 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 16:17:10.452078 140187804313408 spec.py:349] Evaluating on the test split.
I0129 16:17:13.797644 140187804313408 submission_runner.py:408] Time since start: 53381.75s, 	Step: 152564, 	{'train/accuracy': 0.7428451776504517, 'train/loss': 0.9893755316734314, 'validation/accuracy': 0.6662999987602234, 'validation/loss': 1.3725179433822632, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.163541555404663, 'test/num_examples': 10000, 'score': 51547.81756424904, 'total_duration': 53381.75227236748, 'accumulated_submission_time': 51547.81756424904, 'accumulated_eval_time': 1821.8002681732178, 'accumulated_logging_time': 6.5628204345703125}
I0129 16:17:13.838053 140026042091264 logging_writer.py:48] [152564] accumulated_eval_time=1821.800268, accumulated_logging_time=6.562820, accumulated_submission_time=51547.817564, global_step=152564, preemption_count=0, score=51547.817564, test/accuracy=0.527000, test/loss=2.163542, test/num_examples=10000, total_duration=53381.752272, train/accuracy=0.742845, train/loss=0.989376, validation/accuracy=0.666300, validation/loss=1.372518, validation/num_examples=50000
I0129 16:17:26.312469 140026050483968 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.571115255355835, loss=1.455336093902588
I0129 16:17:59.977647 140026042091264 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.95226788520813, loss=1.2973883152008057
I0129 16:18:33.707968 140026050483968 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.569056987762451, loss=1.3500162363052368
I0129 16:19:07.376895 140026042091264 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.9700822830200195, loss=1.4184551239013672
I0129 16:19:41.108314 140026050483968 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.732818603515625, loss=1.3535910844802856
I0129 16:20:14.761161 140026042091264 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.8585565090179443, loss=1.4213080406188965
I0129 16:20:48.587029 140026050483968 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.746746301651001, loss=1.3013222217559814
I0129 16:21:22.319366 140026042091264 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.230438232421875, loss=1.4655637741088867
I0129 16:21:56.048805 140026050483968 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.8488385677337646, loss=1.461086392402649
I0129 16:22:29.705518 140026042091264 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.836709976196289, loss=1.4197301864624023
I0129 16:23:03.472956 140026050483968 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.6537744998931885, loss=1.389243721961975
I0129 16:23:37.150762 140026042091264 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.51369571685791, loss=1.2853715419769287
I0129 16:24:10.853432 140026050483968 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.7744994163513184, loss=1.3190150260925293
I0129 16:24:44.618197 140026042091264 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.407731771469116, loss=1.3290650844573975
I0129 16:25:18.337706 140026050483968 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.8958628177642822, loss=1.3298372030258179
I0129 16:25:43.816370 140187804313408 spec.py:321] Evaluating on the training split.
I0129 16:25:50.044412 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 16:25:58.594161 140187804313408 spec.py:349] Evaluating on the test split.
I0129 16:26:01.225556 140187804313408 submission_runner.py:408] Time since start: 53909.18s, 	Step: 154077, 	{'train/accuracy': 0.7563576102256775, 'train/loss': 0.9220529198646545, 'validation/accuracy': 0.6801199913024902, 'validation/loss': 1.3138034343719482, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.063292980194092, 'test/num_examples': 10000, 'score': 52057.731224775314, 'total_duration': 53909.18029880524, 'accumulated_submission_time': 52057.731224775314, 'accumulated_eval_time': 1839.2094180583954, 'accumulated_logging_time': 6.6128644943237305}
I0129 16:26:01.273514 140026159523584 logging_writer.py:48] [154077] accumulated_eval_time=1839.209418, accumulated_logging_time=6.612864, accumulated_submission_time=52057.731225, global_step=154077, preemption_count=0, score=52057.731225, test/accuracy=0.550600, test/loss=2.063293, test/num_examples=10000, total_duration=53909.180299, train/accuracy=0.756358, train/loss=0.922053, validation/accuracy=0.680120, validation/loss=1.313803, validation/num_examples=50000
I0129 16:26:09.393143 140026167916288 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.842609167098999, loss=1.4723589420318604
I0129 16:26:43.086564 140026159523584 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.466153621673584, loss=1.253909945487976
I0129 16:27:16.858273 140026167916288 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.867602586746216, loss=1.4136658906936646
I0129 16:27:50.581279 140026159523584 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.5955827236175537, loss=1.2005183696746826
I0129 16:28:24.386968 140026167916288 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.798307418823242, loss=1.4294400215148926
I0129 16:28:58.064362 140026159523584 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.658707618713379, loss=1.228269100189209
I0129 16:29:31.777056 140026167916288 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.744699716567993, loss=1.3325494527816772
I0129 16:30:05.486520 140026159523584 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.079055309295654, loss=1.4780323505401611
I0129 16:30:39.215609 140026167916288 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.5342915058135986, loss=1.1889543533325195
I0129 16:31:12.909126 140026159523584 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.713805675506592, loss=1.3292508125305176
I0129 16:31:46.692576 140026167916288 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.750682830810547, loss=1.28750479221344
I0129 16:32:20.404641 140026159523584 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.9794256687164307, loss=1.3727868795394897
I0129 16:32:54.179373 140026167916288 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.053749084472656, loss=1.4391623735427856
I0129 16:33:27.953182 140026159523584 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.950620651245117, loss=1.3158825635910034
I0129 16:34:01.631183 140026167916288 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.626866102218628, loss=1.2719790935516357
I0129 16:34:31.381184 140187804313408 spec.py:321] Evaluating on the training split.
I0129 16:34:37.577824 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 16:34:46.557236 140187804313408 spec.py:349] Evaluating on the test split.
I0129 16:34:49.231204 140187804313408 submission_runner.py:408] Time since start: 54437.19s, 	Step: 155590, 	{'train/accuracy': 0.7622169852256775, 'train/loss': 0.8947357535362244, 'validation/accuracy': 0.6869399547576904, 'validation/loss': 1.2795292139053345, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.034101963043213, 'test/num_examples': 10000, 'score': 52567.773965358734, 'total_duration': 54437.1859266758, 'accumulated_submission_time': 52567.773965358734, 'accumulated_eval_time': 1857.059386253357, 'accumulated_logging_time': 6.671154737472534}
I0129 16:34:49.276917 140026075662080 logging_writer.py:48] [155590] accumulated_eval_time=1857.059386, accumulated_logging_time=6.671155, accumulated_submission_time=52567.773965, global_step=155590, preemption_count=0, score=52567.773965, test/accuracy=0.555400, test/loss=2.034102, test/num_examples=10000, total_duration=54437.185927, train/accuracy=0.762217, train/loss=0.894736, validation/accuracy=0.686940, validation/loss=1.279529, validation/num_examples=50000
I0129 16:34:52.981765 140026151130880 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.7317025661468506, loss=1.3161263465881348
I0129 16:35:26.705868 140026075662080 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.9794998168945312, loss=1.3852616548538208
I0129 16:36:00.395253 140026151130880 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.75945782661438, loss=1.3366894721984863
I0129 16:36:34.103271 140026075662080 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.9755914211273193, loss=1.4323198795318604
I0129 16:37:07.791474 140026151130880 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.6010336875915527, loss=1.2518073320388794
I0129 16:37:41.530586 140026075662080 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.8123133182525635, loss=1.2203128337860107
I0129 16:38:15.198114 140026151130880 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.107533931732178, loss=1.356415033340454
I0129 16:38:48.919234 140026075662080 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.831683397293091, loss=1.2987147569656372
I0129 16:39:22.659156 140026151130880 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.0071563720703125, loss=1.3099995851516724
I0129 16:39:56.640614 140026075662080 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.116660118103027, loss=1.3387786149978638
I0129 16:40:30.385868 140026151130880 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.8838179111480713, loss=1.3486512899398804
I0129 16:41:04.099279 140026075662080 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.17268705368042, loss=1.3172461986541748
I0129 16:41:37.843326 140026151130880 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.293176651000977, loss=1.2566360235214233
I0129 16:42:11.560327 140026075662080 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.002711296081543, loss=1.2256078720092773
I0129 16:42:45.209121 140026151130880 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.890272617340088, loss=1.3198835849761963
I0129 16:43:18.922303 140026075662080 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.146550178527832, loss=1.3411945104599
I0129 16:43:19.404772 140187804313408 spec.py:321] Evaluating on the training split.
I0129 16:43:25.645795 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 16:43:34.281991 140187804313408 spec.py:349] Evaluating on the test split.
I0129 16:43:36.947563 140187804313408 submission_runner.py:408] Time since start: 54964.90s, 	Step: 157103, 	{'train/accuracy': 0.7747727632522583, 'train/loss': 0.8410025238990784, 'validation/accuracy': 0.6900999546051025, 'validation/loss': 1.2684522867202759, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.017871379852295, 'test/num_examples': 10000, 'score': 53077.835666418076, 'total_duration': 54964.90230464935, 'accumulated_submission_time': 53077.835666418076, 'accumulated_eval_time': 1874.6021373271942, 'accumulated_logging_time': 6.727156400680542}
I0129 16:43:36.993293 140026058876672 logging_writer.py:48] [157103] accumulated_eval_time=1874.602137, accumulated_logging_time=6.727156, accumulated_submission_time=53077.835666, global_step=157103, preemption_count=0, score=53077.835666, test/accuracy=0.560100, test/loss=2.017871, test/num_examples=10000, total_duration=54964.902305, train/accuracy=0.774773, train/loss=0.841003, validation/accuracy=0.690100, validation/loss=1.268452, validation/num_examples=50000
I0129 16:44:10.072225 140026067269376 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.922578811645508, loss=1.2368361949920654
I0129 16:44:43.748304 140026058876672 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.79170298576355, loss=1.2950953245162964
I0129 16:45:17.468737 140026067269376 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.126387119293213, loss=1.311147689819336
I0129 16:45:51.221785 140026058876672 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.134637832641602, loss=1.2546803951263428
I0129 16:46:25.096007 140026067269376 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.7754881381988525, loss=1.283699870109558
I0129 16:46:58.846122 140026058876672 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.050343990325928, loss=1.308359980583191
I0129 16:47:32.580528 140026067269376 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.218883037567139, loss=1.3231288194656372
I0129 16:48:06.296644 140026058876672 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.288704872131348, loss=1.3840999603271484
I0129 16:48:40.109299 140026067269376 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.076632022857666, loss=1.1740257740020752
I0129 16:49:13.858617 140026058876672 logging_writer.py:48] [158100] global_step=158100, grad_norm=3.9439525604248047, loss=1.2314954996109009
I0129 16:49:47.563737 140026067269376 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.039046287536621, loss=1.3367209434509277
I0129 16:50:21.244249 140026058876672 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.180216312408447, loss=1.3035465478897095
I0129 16:50:54.973821 140026067269376 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.943852663040161, loss=1.3363226652145386
I0129 16:51:28.646650 140026058876672 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.372042179107666, loss=1.423579454421997
I0129 16:52:02.398994 140026067269376 logging_writer.py:48] [158600] global_step=158600, grad_norm=3.9417340755462646, loss=1.2423409223556519
I0129 16:52:07.271770 140187804313408 spec.py:321] Evaluating on the training split.
I0129 16:52:13.529542 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 16:52:22.216419 140187804313408 spec.py:349] Evaluating on the test split.
I0129 16:52:24.888469 140187804313408 submission_runner.py:408] Time since start: 55492.84s, 	Step: 158616, 	{'train/accuracy': 0.8008609414100647, 'train/loss': 0.7219278812408447, 'validation/accuracy': 0.6961399912834167, 'validation/loss': 1.2478055953979492, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 1.9279481172561646, 'test/num_examples': 10000, 'score': 53588.04853415489, 'total_duration': 55492.84319996834, 'accumulated_submission_time': 53588.04853415489, 'accumulated_eval_time': 1892.2187869548798, 'accumulated_logging_time': 6.782275915145874}
I0129 16:52:24.933620 140026050483968 logging_writer.py:48] [158616] accumulated_eval_time=1892.218787, accumulated_logging_time=6.782276, accumulated_submission_time=53588.048534, global_step=158616, preemption_count=0, score=53588.048534, test/accuracy=0.578200, test/loss=1.927948, test/num_examples=10000, total_duration=55492.843200, train/accuracy=0.800861, train/loss=0.721928, validation/accuracy=0.696140, validation/loss=1.247806, validation/num_examples=50000
I0129 16:52:53.624919 140026058876672 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.383185863494873, loss=1.3005331754684448
I0129 16:53:27.351496 140026050483968 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.9279873371124268, loss=1.3229690790176392
I0129 16:54:01.030712 140026058876672 logging_writer.py:48] [158900] global_step=158900, grad_norm=3.8910739421844482, loss=1.2909774780273438
I0129 16:54:34.733406 140026050483968 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.9884026050567627, loss=1.2872705459594727
I0129 16:55:08.424509 140026058876672 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.467807769775391, loss=1.327207326889038
I0129 16:55:42.133290 140026050483968 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.356017589569092, loss=1.2847306728363037
I0129 16:56:15.898677 140026058876672 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.099856376647949, loss=1.262896180152893
I0129 16:56:49.617287 140026050483968 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.131668567657471, loss=1.2782124280929565
I0129 16:57:23.285863 140026058876672 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.171125888824463, loss=1.312172293663025
I0129 16:57:56.957569 140026050483968 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.304588317871094, loss=1.266634225845337
I0129 16:58:30.628512 140026058876672 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.833275318145752, loss=1.2355446815490723
I0129 16:59:04.315770 140026050483968 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.221222877502441, loss=1.2958338260650635
I0129 16:59:38.123677 140026058876672 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.654089450836182, loss=1.3149696588516235
I0129 17:00:11.824624 140026050483968 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.3065314292907715, loss=1.388494610786438
I0129 17:00:45.620314 140026058876672 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.402507305145264, loss=1.2447090148925781
I0129 17:00:55.217493 140187804313408 spec.py:321] Evaluating on the training split.
I0129 17:01:01.487456 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 17:01:10.413104 140187804313408 spec.py:349] Evaluating on the test split.
I0129 17:01:13.419325 140187804313408 submission_runner.py:408] Time since start: 56021.37s, 	Step: 160130, 	{'train/accuracy': 0.8089724183082581, 'train/loss': 0.7101982831954956, 'validation/accuracy': 0.7047399878501892, 'validation/loss': 1.194737195968628, 'validation/num_examples': 50000, 'test/accuracy': 0.5812000036239624, 'test/loss': 1.8732895851135254, 'test/num_examples': 10000, 'score': 54098.267798900604, 'total_duration': 56021.37393474579, 'accumulated_submission_time': 54098.267798900604, 'accumulated_eval_time': 1910.420448064804, 'accumulated_logging_time': 6.837030410766602}
I0129 17:01:13.455926 140026067269376 logging_writer.py:48] [160130] accumulated_eval_time=1910.420448, accumulated_logging_time=6.837030, accumulated_submission_time=54098.267799, global_step=160130, preemption_count=0, score=54098.267799, test/accuracy=0.581200, test/loss=1.873290, test/num_examples=10000, total_duration=56021.373935, train/accuracy=0.808972, train/loss=0.710198, validation/accuracy=0.704740, validation/loss=1.194737, validation/num_examples=50000
I0129 17:01:37.409659 140026075662080 logging_writer.py:48] [160200] global_step=160200, grad_norm=3.908017873764038, loss=1.1850910186767578
I0129 17:02:11.150158 140026067269376 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.646738529205322, loss=1.352264404296875
I0129 17:02:44.820694 140026075662080 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.647319793701172, loss=1.3246285915374756
I0129 17:03:18.589128 140026067269376 logging_writer.py:48] [160500] global_step=160500, grad_norm=3.9039788246154785, loss=1.2399859428405762
I0129 17:03:52.271840 140026075662080 logging_writer.py:48] [160600] global_step=160600, grad_norm=3.9511420726776123, loss=1.143491506576538
I0129 17:04:25.975107 140026067269376 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.108539581298828, loss=1.1935094594955444
I0129 17:04:59.720766 140026075662080 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.294334888458252, loss=1.1902599334716797
I0129 17:05:33.450634 140026067269376 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.221175193786621, loss=1.2057933807373047
I0129 17:06:07.262664 140026075662080 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.570474147796631, loss=1.2557967901229858
I0129 17:06:40.987950 140026067269376 logging_writer.py:48] [161100] global_step=161100, grad_norm=3.8935587406158447, loss=1.1309700012207031
I0129 17:07:14.747530 140026075662080 logging_writer.py:48] [161200] global_step=161200, grad_norm=3.9547886848449707, loss=1.2656564712524414
I0129 17:07:48.476125 140026067269376 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.364229679107666, loss=1.297763705253601
I0129 17:08:22.215573 140026075662080 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.644195556640625, loss=1.2686630487442017
I0129 17:08:55.943576 140026067269376 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.308327674865723, loss=1.153130054473877
I0129 17:09:29.717922 140026075662080 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.196877956390381, loss=1.1671421527862549
I0129 17:09:43.694947 140187804313408 spec.py:321] Evaluating on the training split.
I0129 17:09:50.079987 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 17:09:58.686631 140187804313408 spec.py:349] Evaluating on the test split.
I0129 17:10:01.369755 140187804313408 submission_runner.py:408] Time since start: 56549.32s, 	Step: 161643, 	{'train/accuracy': 0.7973134517669678, 'train/loss': 0.7421764135360718, 'validation/accuracy': 0.702739953994751, 'validation/loss': 1.2192460298538208, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 1.9370954036712646, 'test/num_examples': 10000, 'score': 54608.44234919548, 'total_duration': 56549.32446479797, 'accumulated_submission_time': 54608.44234919548, 'accumulated_eval_time': 1928.0951828956604, 'accumulated_logging_time': 6.881301641464233}
I0129 17:10:01.427899 140026058876672 logging_writer.py:48] [161643] accumulated_eval_time=1928.095183, accumulated_logging_time=6.881302, accumulated_submission_time=54608.442349, global_step=161643, preemption_count=0, score=54608.442349, test/accuracy=0.575800, test/loss=1.937095, test/num_examples=10000, total_duration=56549.324465, train/accuracy=0.797313, train/loss=0.742176, validation/accuracy=0.702740, validation/loss=1.219246, validation/num_examples=50000
I0129 17:10:21.004014 140026151130880 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.285398960113525, loss=1.2457255125045776
I0129 17:10:54.710078 140026058876672 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.267924785614014, loss=1.2000279426574707
I0129 17:11:28.394880 140026151130880 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.653656005859375, loss=1.2029098272323608
I0129 17:12:02.084907 140026058876672 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.441014766693115, loss=1.234532117843628
I0129 17:12:35.913629 140026151130880 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.348891258239746, loss=1.2809159755706787
I0129 17:13:09.631843 140026058876672 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.359267234802246, loss=1.3053438663482666
I0129 17:13:43.333283 140026151130880 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.4041428565979, loss=1.2379118204116821
I0129 17:14:17.090615 140026058876672 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.4219279289245605, loss=1.2331786155700684
I0129 17:14:50.792017 140026151130880 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.256279945373535, loss=1.1874151229858398
I0129 17:15:24.571879 140026058876672 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.4436564445495605, loss=1.1466240882873535
I0129 17:15:58.278262 140026151130880 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.333522796630859, loss=1.155527949333191
I0129 17:16:32.066939 140026058876672 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.2582526206970215, loss=1.1782296895980835
I0129 17:17:05.744713 140026151130880 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.341119766235352, loss=1.1471197605133057
I0129 17:17:39.477077 140026058876672 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.535534858703613, loss=1.1889346837997437
I0129 17:18:13.221590 140026151130880 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.362617015838623, loss=1.1912200450897217
I0129 17:18:31.554186 140187804313408 spec.py:321] Evaluating on the training split.
I0129 17:18:37.819285 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 17:18:46.846408 140187804313408 spec.py:349] Evaluating on the test split.
I0129 17:18:49.483787 140187804313408 submission_runner.py:408] Time since start: 57077.44s, 	Step: 163156, 	{'train/accuracy': 0.8103076815605164, 'train/loss': 0.7060856819152832, 'validation/accuracy': 0.7111200094223022, 'validation/loss': 1.171807885169983, 'validation/num_examples': 50000, 'test/accuracy': 0.5887000560760498, 'test/loss': 1.8791732788085938, 'test/num_examples': 10000, 'score': 55118.50452852249, 'total_duration': 57077.438390254974, 'accumulated_submission_time': 55118.50452852249, 'accumulated_eval_time': 1946.0246217250824, 'accumulated_logging_time': 6.949363708496094}
I0129 17:18:49.532959 140026067269376 logging_writer.py:48] [163156] accumulated_eval_time=1946.024622, accumulated_logging_time=6.949364, accumulated_submission_time=55118.504529, global_step=163156, preemption_count=0, score=55118.504529, test/accuracy=0.588700, test/loss=1.879173, test/num_examples=10000, total_duration=57077.438390, train/accuracy=0.810308, train/loss=0.706086, validation/accuracy=0.711120, validation/loss=1.171808, validation/num_examples=50000
I0129 17:19:04.726343 140026075662080 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.115137577056885, loss=1.0808608531951904
I0129 17:19:38.440150 140026067269376 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.601838111877441, loss=1.305351734161377
I0129 17:20:12.154808 140026075662080 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.591587543487549, loss=1.1778910160064697
I0129 17:20:45.840151 140026067269376 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.384960651397705, loss=1.1730704307556152
I0129 17:21:19.560353 140026075662080 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.573286056518555, loss=1.1806526184082031
I0129 17:21:53.281818 140026067269376 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.760895252227783, loss=1.1515929698944092
I0129 17:22:27.005494 140026075662080 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.509411811828613, loss=1.1671890020370483
I0129 17:23:00.770392 140026067269376 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.468714237213135, loss=1.1946547031402588
I0129 17:23:34.513310 140026075662080 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.226553916931152, loss=1.0624704360961914
I0129 17:24:08.241551 140026067269376 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.412477016448975, loss=1.119896650314331
I0129 17:24:42.020378 140026075662080 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.273655891418457, loss=1.1423110961914062
I0129 17:25:15.879090 140026067269376 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.760162830352783, loss=1.2271381616592407
I0129 17:25:49.634877 140026075662080 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.8149590492248535, loss=1.1803195476531982
I0129 17:26:23.409661 140026067269376 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.469831466674805, loss=1.1657612323760986
I0129 17:26:57.144991 140026075662080 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.355335235595703, loss=1.1881906986236572
I0129 17:27:19.546358 140187804313408 spec.py:321] Evaluating on the training split.
I0129 17:27:25.777386 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 17:27:34.698554 140187804313408 spec.py:349] Evaluating on the test split.
I0129 17:27:37.342417 140187804313408 submission_runner.py:408] Time since start: 57605.30s, 	Step: 164668, 	{'train/accuracy': 0.8194754123687744, 'train/loss': 0.6554054021835327, 'validation/accuracy': 0.7178800106048584, 'validation/loss': 1.144294261932373, 'validation/num_examples': 50000, 'test/accuracy': 0.5937000513076782, 'test/loss': 1.8554983139038086, 'test/num_examples': 10000, 'score': 55628.453688144684, 'total_duration': 57605.29715466499, 'accumulated_submission_time': 55628.453688144684, 'accumulated_eval_time': 1963.8206391334534, 'accumulated_logging_time': 7.007922172546387}
I0129 17:27:37.395293 140026167916288 logging_writer.py:48] [164668] accumulated_eval_time=1963.820639, accumulated_logging_time=7.007922, accumulated_submission_time=55628.453688, global_step=164668, preemption_count=0, score=55628.453688, test/accuracy=0.593700, test/loss=1.855498, test/num_examples=10000, total_duration=57605.297155, train/accuracy=0.819475, train/loss=0.655405, validation/accuracy=0.717880, validation/loss=1.144294, validation/num_examples=50000
I0129 17:27:48.528474 140026176308992 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.241608619689941, loss=1.0645779371261597
I0129 17:28:22.208811 140026167916288 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.715300559997559, loss=1.1857444047927856
I0129 17:28:55.926054 140026176308992 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.642285346984863, loss=1.2843852043151855
I0129 17:29:29.609260 140026167916288 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.789737701416016, loss=1.1483455896377563
I0129 17:30:03.321183 140026176308992 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.696995258331299, loss=1.1625958681106567
I0129 17:30:36.993983 140026167916288 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.343868732452393, loss=1.0303032398223877
I0129 17:31:10.788259 140026176308992 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.75099515914917, loss=1.2496695518493652
I0129 17:31:44.613138 140026167916288 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.476916790008545, loss=1.1145193576812744
I0129 17:32:18.355066 140026176308992 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.35648250579834, loss=1.0994538068771362
I0129 17:32:52.024553 140026167916288 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.671841621398926, loss=1.1917558908462524
I0129 17:33:25.838446 140026176308992 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.442986488342285, loss=1.1024494171142578
I0129 17:33:59.519185 140026167916288 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.353260040283203, loss=1.1140447854995728
I0129 17:34:33.307492 140026176308992 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.872026443481445, loss=1.1411992311477661
I0129 17:35:06.991491 140026167916288 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.427727222442627, loss=1.1340850591659546
I0129 17:35:40.785188 140026176308992 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.424195766448975, loss=1.0389668941497803
I0129 17:36:07.550711 140187804313408 spec.py:321] Evaluating on the training split.
I0129 17:36:13.791014 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 17:36:22.767647 140187804313408 spec.py:349] Evaluating on the test split.
I0129 17:36:25.413298 140187804313408 submission_runner.py:408] Time since start: 58133.37s, 	Step: 166181, 	{'train/accuracy': 0.8342036008834839, 'train/loss': 0.6015270948410034, 'validation/accuracy': 0.727840006351471, 'validation/loss': 1.113991379737854, 'validation/num_examples': 50000, 'test/accuracy': 0.6082000136375427, 'test/loss': 1.8160563707351685, 'test/num_examples': 10000, 'score': 56138.544397592545, 'total_duration': 58133.3680229187, 'accumulated_submission_time': 56138.544397592545, 'accumulated_eval_time': 1981.6831741333008, 'accumulated_logging_time': 7.071213245391846}
I0129 17:36:25.478519 140026058876672 logging_writer.py:48] [166181] accumulated_eval_time=1981.683174, accumulated_logging_time=7.071213, accumulated_submission_time=56138.544398, global_step=166181, preemption_count=0, score=56138.544398, test/accuracy=0.608200, test/loss=1.816056, test/num_examples=10000, total_duration=58133.368023, train/accuracy=0.834204, train/loss=0.601527, validation/accuracy=0.727840, validation/loss=1.113991, validation/num_examples=50000
I0129 17:36:32.226942 140026067269376 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.380921840667725, loss=1.0853235721588135
I0129 17:37:05.909046 140026058876672 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.937459945678711, loss=1.2893801927566528
I0129 17:37:39.569135 140026067269376 logging_writer.py:48] [166400] global_step=166400, grad_norm=5.0045270919799805, loss=1.1201449632644653
I0129 17:38:13.387387 140026058876672 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.804795742034912, loss=1.1292009353637695
I0129 17:38:47.118704 140026067269376 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.578128814697266, loss=1.212403416633606
I0129 17:39:20.819675 140026058876672 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.81842041015625, loss=1.1346997022628784
I0129 17:39:54.588016 140026067269376 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.801631927490234, loss=1.164465069770813
I0129 17:40:28.305917 140026058876672 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.531891822814941, loss=1.1114922761917114
I0129 17:41:01.961770 140026067269376 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.567472457885742, loss=1.1498013734817505
I0129 17:41:35.662428 140026058876672 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.717087268829346, loss=1.0244081020355225
I0129 17:42:09.355501 140026067269376 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.3540730476379395, loss=1.0420682430267334
I0129 17:42:43.106966 140026058876672 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.601362228393555, loss=1.0637227296829224
I0129 17:43:16.862011 140026067269376 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.738225936889648, loss=1.051649570465088
I0129 17:43:50.564679 140026058876672 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.600425720214844, loss=1.1359283924102783
I0129 17:44:24.244790 140026067269376 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.456747531890869, loss=1.0318341255187988
I0129 17:44:55.532986 140187804313408 spec.py:321] Evaluating on the training split.
I0129 17:45:01.938459 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 17:45:10.758453 140187804313408 spec.py:349] Evaluating on the test split.
I0129 17:45:14.118901 140187804313408 submission_runner.py:408] Time since start: 58662.07s, 	Step: 167694, 	{'train/accuracy': 0.8501076102256775, 'train/loss': 0.546515941619873, 'validation/accuracy': 0.7238799929618835, 'validation/loss': 1.1284018754959106, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.8255773782730103, 'test/num_examples': 10000, 'score': 56648.53292417526, 'total_duration': 58662.07354712486, 'accumulated_submission_time': 56648.53292417526, 'accumulated_eval_time': 2000.2689554691315, 'accumulated_logging_time': 7.146226406097412}
I0129 17:45:14.157172 140026050483968 logging_writer.py:48] [167694] accumulated_eval_time=2000.268955, accumulated_logging_time=7.146226, accumulated_submission_time=56648.532924, global_step=167694, preemption_count=0, score=56648.532924, test/accuracy=0.598200, test/loss=1.825577, test/num_examples=10000, total_duration=58662.073547, train/accuracy=0.850108, train/loss=0.546516, validation/accuracy=0.723880, validation/loss=1.128402, validation/num_examples=50000
I0129 17:45:16.518440 140026159523584 logging_writer.py:48] [167700] global_step=167700, grad_norm=5.2303242683410645, loss=1.1958277225494385
I0129 17:45:50.244786 140026050483968 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.677933216094971, loss=1.1023197174072266
I0129 17:46:23.926266 140026159523584 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.094013214111328, loss=1.1496490240097046
I0129 17:46:57.625684 140026050483968 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.3795881271362305, loss=1.020450234413147
I0129 17:47:31.275929 140026159523584 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.13727331161499, loss=1.2737445831298828
I0129 17:48:04.986869 140026050483968 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.958008766174316, loss=1.1659204959869385
I0129 17:48:38.661034 140026159523584 logging_writer.py:48] [168300] global_step=168300, grad_norm=5.054457664489746, loss=1.1520004272460938
I0129 17:49:12.425108 140026050483968 logging_writer.py:48] [168400] global_step=168400, grad_norm=5.004894256591797, loss=1.1348905563354492
I0129 17:49:46.129459 140026159523584 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.746409893035889, loss=1.0716865062713623
I0129 17:50:19.925105 140026050483968 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.9714837074279785, loss=1.0605716705322266
I0129 17:50:53.626906 140026159523584 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.8936567306518555, loss=1.1798478364944458
I0129 17:51:27.565178 140026050483968 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.043288230895996, loss=1.132049322128296
I0129 17:52:01.301139 140026159523584 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.414002418518066, loss=1.0723439455032349
I0129 17:52:35.055656 140026050483968 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.8981451988220215, loss=1.0911223888397217
I0129 17:53:08.825450 140026159523584 logging_writer.py:48] [169100] global_step=169100, grad_norm=5.4638261795043945, loss=1.1440107822418213
I0129 17:53:42.555357 140026050483968 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.77072286605835, loss=1.049867033958435
I0129 17:53:44.383513 140187804313408 spec.py:321] Evaluating on the training split.
I0129 17:53:51.376746 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 17:54:00.378988 140187804313408 spec.py:349] Evaluating on the test split.
I0129 17:54:03.018349 140187804313408 submission_runner.py:408] Time since start: 59190.97s, 	Step: 169207, 	{'train/accuracy': 0.8522600531578064, 'train/loss': 0.5386980772018433, 'validation/accuracy': 0.7333599925041199, 'validation/loss': 1.0911046266555786, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7918938398361206, 'test/num_examples': 10000, 'score': 57158.69640493393, 'total_duration': 59190.97307920456, 'accumulated_submission_time': 57158.69640493393, 'accumulated_eval_time': 2018.9037404060364, 'accumulated_logging_time': 7.192639112472534}
I0129 17:54:03.068089 140026058876672 logging_writer.py:48] [169207] accumulated_eval_time=2018.903740, accumulated_logging_time=7.192639, accumulated_submission_time=57158.696405, global_step=169207, preemption_count=0, score=57158.696405, test/accuracy=0.607000, test/loss=1.791894, test/num_examples=10000, total_duration=59190.973079, train/accuracy=0.852260, train/loss=0.538698, validation/accuracy=0.733360, validation/loss=1.091105, validation/num_examples=50000
I0129 17:54:34.802400 140026067269376 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.238467216491699, loss=1.0840907096862793
I0129 17:55:08.504152 140026058876672 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.909138202667236, loss=1.0971970558166504
I0129 17:55:42.182610 140026067269376 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.719043731689453, loss=1.1215856075286865
I0129 17:56:15.853081 140026058876672 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.401987075805664, loss=0.9882909059524536
I0129 17:56:49.634084 140026067269376 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.981296539306641, loss=1.127118468284607
I0129 17:57:23.347089 140026058876672 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.962093830108643, loss=1.1266553401947021
I0129 17:57:57.184438 140026067269376 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.688962936401367, loss=1.0320122241973877
I0129 17:58:30.876375 140026058876672 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.4447784423828125, loss=1.0536736249923706
I0129 17:59:04.549104 140026067269376 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.991675853729248, loss=1.0369540452957153
I0129 17:59:38.246043 140026058876672 logging_writer.py:48] [170200] global_step=170200, grad_norm=5.062521934509277, loss=0.9950710535049438
I0129 18:00:12.019021 140026067269376 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.714838981628418, loss=1.0331720113754272
I0129 18:00:45.759110 140026058876672 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.562142372131348, loss=0.9803874492645264
I0129 18:01:19.483027 140026067269376 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.869110107421875, loss=1.1097334623336792
I0129 18:01:53.161848 140026058876672 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.850425720214844, loss=0.9938638210296631
I0129 18:02:26.964125 140026067269376 logging_writer.py:48] [170700] global_step=170700, grad_norm=5.0696330070495605, loss=1.046104907989502
I0129 18:02:33.185437 140187804313408 spec.py:321] Evaluating on the training split.
I0129 18:02:39.447993 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 18:02:48.398055 140187804313408 spec.py:349] Evaluating on the test split.
I0129 18:02:51.060131 140187804313408 submission_runner.py:408] Time since start: 59719.01s, 	Step: 170720, 	{'train/accuracy': 0.8514229655265808, 'train/loss': 0.5230114459991455, 'validation/accuracy': 0.7345799803733826, 'validation/loss': 1.079545021057129, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.7668043375015259, 'test/num_examples': 10000, 'score': 57668.74922180176, 'total_duration': 59719.014672756195, 'accumulated_submission_time': 57668.74922180176, 'accumulated_eval_time': 2036.7781956195831, 'accumulated_logging_time': 7.252174377441406}
I0129 18:02:51.110676 140026151130880 logging_writer.py:48] [170720] accumulated_eval_time=2036.778196, accumulated_logging_time=7.252174, accumulated_submission_time=57668.749222, global_step=170720, preemption_count=0, score=57668.749222, test/accuracy=0.610500, test/loss=1.766804, test/num_examples=10000, total_duration=59719.014673, train/accuracy=0.851423, train/loss=0.523011, validation/accuracy=0.734580, validation/loss=1.079545, validation/num_examples=50000
I0129 18:03:18.427673 140026159523584 logging_writer.py:48] [170800] global_step=170800, grad_norm=5.071109771728516, loss=0.9679773449897766
I0129 18:03:52.141222 140026151130880 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.958873271942139, loss=1.0350608825683594
I0129 18:04:25.954246 140026159523584 logging_writer.py:48] [171000] global_step=171000, grad_norm=5.066298961639404, loss=1.0375945568084717
I0129 18:04:59.674355 140026151130880 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.829415798187256, loss=1.0806446075439453
I0129 18:05:33.321412 140026159523584 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.753137111663818, loss=1.0432380437850952
I0129 18:06:07.023700 140026151130880 logging_writer.py:48] [171300] global_step=171300, grad_norm=5.25303840637207, loss=1.1130449771881104
I0129 18:06:40.703822 140026159523584 logging_writer.py:48] [171400] global_step=171400, grad_norm=5.187699794769287, loss=1.0921670198440552
I0129 18:07:14.439103 140026151130880 logging_writer.py:48] [171500] global_step=171500, grad_norm=5.48822021484375, loss=1.071329951286316
I0129 18:07:48.168329 140026159523584 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.612459659576416, loss=0.9800497889518738
I0129 18:08:21.915572 140026151130880 logging_writer.py:48] [171700] global_step=171700, grad_norm=5.114556789398193, loss=1.0476828813552856
I0129 18:08:55.617866 140026159523584 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.794896602630615, loss=0.8982701301574707
I0129 18:09:29.367215 140026151130880 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.956385612487793, loss=1.0606187582015991
I0129 18:10:03.043612 140026159523584 logging_writer.py:48] [172000] global_step=172000, grad_norm=5.176217079162598, loss=1.208911418914795
I0129 18:10:36.877838 140026151130880 logging_writer.py:48] [172100] global_step=172100, grad_norm=5.251890182495117, loss=1.031808614730835
I0129 18:11:10.580888 140026159523584 logging_writer.py:48] [172200] global_step=172200, grad_norm=5.273313999176025, loss=1.083922266960144
I0129 18:11:21.183236 140187804313408 spec.py:321] Evaluating on the training split.
I0129 18:11:27.405180 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 18:11:36.237545 140187804313408 spec.py:349] Evaluating on the test split.
I0129 18:11:39.018620 140187804313408 submission_runner.py:408] Time since start: 60246.97s, 	Step: 172233, 	{'train/accuracy': 0.8605110049247742, 'train/loss': 0.5039197206497192, 'validation/accuracy': 0.7382400035858154, 'validation/loss': 1.062277913093567, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.7463383674621582, 'test/num_examples': 10000, 'score': 58178.75817775726, 'total_duration': 60246.973351955414, 'accumulated_submission_time': 58178.75817775726, 'accumulated_eval_time': 2054.613529920578, 'accumulated_logging_time': 7.311715364456177}
I0129 18:11:39.066781 140026050483968 logging_writer.py:48] [172233] accumulated_eval_time=2054.613530, accumulated_logging_time=7.311715, accumulated_submission_time=58178.758178, global_step=172233, preemption_count=0, score=58178.758178, test/accuracy=0.618900, test/loss=1.746338, test/num_examples=10000, total_duration=60246.973352, train/accuracy=0.860511, train/loss=0.503920, validation/accuracy=0.738240, validation/loss=1.062278, validation/num_examples=50000
I0129 18:12:02.024430 140026067269376 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.945623397827148, loss=1.0298070907592773
I0129 18:12:35.759398 140026050483968 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.893409729003906, loss=1.0097976922988892
I0129 18:13:09.434710 140026067269376 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.8564677238464355, loss=0.9744825959205627
I0129 18:13:43.184360 140026050483968 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.753774166107178, loss=1.0284526348114014
I0129 18:14:16.901217 140026067269376 logging_writer.py:48] [172700] global_step=172700, grad_norm=5.111790180206299, loss=0.9931403994560242
I0129 18:14:50.685257 140026050483968 logging_writer.py:48] [172800] global_step=172800, grad_norm=5.374655723571777, loss=1.0978929996490479
I0129 18:15:24.352477 140026067269376 logging_writer.py:48] [172900] global_step=172900, grad_norm=5.365886211395264, loss=1.0941498279571533
I0129 18:15:58.074719 140026050483968 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.980992794036865, loss=1.0253283977508545
I0129 18:16:31.813904 140026067269376 logging_writer.py:48] [173100] global_step=173100, grad_norm=5.367034912109375, loss=1.075809121131897
I0129 18:17:05.625748 140026050483968 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.869868278503418, loss=0.9885849952697754
I0129 18:17:39.363675 140026067269376 logging_writer.py:48] [173300] global_step=173300, grad_norm=5.371211051940918, loss=1.0694234371185303
I0129 18:18:13.087852 140026050483968 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.8333821296691895, loss=0.9854537844657898
I0129 18:18:46.773251 140026067269376 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.924317359924316, loss=1.0663069486618042
I0129 18:19:20.585022 140026050483968 logging_writer.py:48] [173600] global_step=173600, grad_norm=5.54360294342041, loss=1.0483309030532837
I0129 18:19:54.295370 140026067269376 logging_writer.py:48] [173700] global_step=173700, grad_norm=5.275639533996582, loss=1.0844355821609497
I0129 18:20:09.315750 140187804313408 spec.py:321] Evaluating on the training split.
I0129 18:20:15.609166 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 18:20:24.430756 140187804313408 spec.py:349] Evaluating on the test split.
I0129 18:20:27.076191 140187804313408 submission_runner.py:408] Time since start: 60775.03s, 	Step: 173746, 	{'train/accuracy': 0.8649752736091614, 'train/loss': 0.48539647459983826, 'validation/accuracy': 0.7422399520874023, 'validation/loss': 1.0512233972549438, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.7367162704467773, 'test/num_examples': 10000, 'score': 58688.94418215752, 'total_duration': 60775.03083705902, 'accumulated_submission_time': 58688.94418215752, 'accumulated_eval_time': 2072.3738420009613, 'accumulated_logging_time': 7.36877703666687}
I0129 18:20:27.123437 140026058876672 logging_writer.py:48] [173746] accumulated_eval_time=2072.373842, accumulated_logging_time=7.368777, accumulated_submission_time=58688.944182, global_step=173746, preemption_count=0, score=58688.944182, test/accuracy=0.620000, test/loss=1.736716, test/num_examples=10000, total_duration=60775.030837, train/accuracy=0.864975, train/loss=0.485396, validation/accuracy=0.742240, validation/loss=1.051223, validation/num_examples=50000
I0129 18:20:45.697786 140026067269376 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.949644088745117, loss=0.954841673374176
I0129 18:21:19.363378 140026058876672 logging_writer.py:48] [173900] global_step=173900, grad_norm=5.155918121337891, loss=1.0793606042861938
I0129 18:21:53.089133 140026067269376 logging_writer.py:48] [174000] global_step=174000, grad_norm=5.3106513023376465, loss=0.9794936180114746
I0129 18:22:26.748794 140026058876672 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.983445644378662, loss=1.0218886137008667
I0129 18:23:00.461891 140026067269376 logging_writer.py:48] [174200] global_step=174200, grad_norm=5.3472580909729, loss=1.0596284866333008
I0129 18:23:34.277743 140026058876672 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.929159641265869, loss=1.0460045337677002
I0129 18:24:07.974066 140026067269376 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.690567970275879, loss=0.9150936603546143
I0129 18:24:41.621530 140026058876672 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.68991231918335, loss=0.9120960831642151
I0129 18:25:15.324280 140026067269376 logging_writer.py:48] [174600] global_step=174600, grad_norm=5.128848075866699, loss=0.9451356530189514
I0129 18:25:48.996479 140026058876672 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.807046413421631, loss=0.8860635757446289
I0129 18:26:22.724731 140026067269376 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.86258602142334, loss=0.9213688373565674
I0129 18:26:56.475545 140026058876672 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.7884697914123535, loss=0.9980161190032959
I0129 18:27:30.184381 140026067269376 logging_writer.py:48] [175000] global_step=175000, grad_norm=5.301177501678467, loss=1.0628420114517212
I0129 18:28:03.932391 140026058876672 logging_writer.py:48] [175100] global_step=175100, grad_norm=5.4535088539123535, loss=1.0549845695495605
I0129 18:28:37.649880 140026067269376 logging_writer.py:48] [175200] global_step=175200, grad_norm=5.219935417175293, loss=0.9622445106506348
I0129 18:28:57.378084 140187804313408 spec.py:321] Evaluating on the training split.
I0129 18:29:03.789076 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 18:29:12.482651 140187804313408 spec.py:349] Evaluating on the test split.
I0129 18:29:15.139857 140187804313408 submission_runner.py:408] Time since start: 61303.09s, 	Step: 175260, 	{'train/accuracy': 0.8682437539100647, 'train/loss': 0.46756279468536377, 'validation/accuracy': 0.7414199709892273, 'validation/loss': 1.049114465713501, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.7439327239990234, 'test/num_examples': 10000, 'score': 59199.13371539116, 'total_duration': 61303.094465732574, 'accumulated_submission_time': 59199.13371539116, 'accumulated_eval_time': 2090.135448217392, 'accumulated_logging_time': 7.425171852111816}
I0129 18:29:15.190744 140026042091264 logging_writer.py:48] [175260] accumulated_eval_time=2090.135448, accumulated_logging_time=7.425172, accumulated_submission_time=59199.133715, global_step=175260, preemption_count=0, score=59199.133715, test/accuracy=0.620300, test/loss=1.743933, test/num_examples=10000, total_duration=61303.094466, train/accuracy=0.868244, train/loss=0.467563, validation/accuracy=0.741420, validation/loss=1.049114, validation/num_examples=50000
I0129 18:29:29.029735 140026050483968 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.919723987579346, loss=0.9098239541053772
I0129 18:30:02.836315 140026042091264 logging_writer.py:48] [175400] global_step=175400, grad_norm=5.32826566696167, loss=0.9919196367263794
I0129 18:30:36.604954 140026050483968 logging_writer.py:48] [175500] global_step=175500, grad_norm=5.181478023529053, loss=0.9487407207489014
I0129 18:31:10.303674 140026042091264 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.919377326965332, loss=0.9996834993362427
I0129 18:31:44.081612 140026050483968 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.998049259185791, loss=0.914271891117096
I0129 18:32:17.759896 140026042091264 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.974729061126709, loss=0.973511815071106
I0129 18:32:51.547856 140026050483968 logging_writer.py:48] [175900] global_step=175900, grad_norm=5.2769269943237305, loss=0.9592658877372742
I0129 18:33:25.257776 140026042091264 logging_writer.py:48] [176000] global_step=176000, grad_norm=5.0460662841796875, loss=0.9978784918785095
I0129 18:33:59.041142 140026050483968 logging_writer.py:48] [176100] global_step=176100, grad_norm=5.06521463394165, loss=0.9668654799461365
I0129 18:34:32.702025 140026042091264 logging_writer.py:48] [176200] global_step=176200, grad_norm=5.050487518310547, loss=1.0066639184951782
I0129 18:35:06.424989 140026050483968 logging_writer.py:48] [176300] global_step=176300, grad_norm=5.344239711761475, loss=0.9424641132354736
I0129 18:35:40.191313 140026042091264 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.761038780212402, loss=0.898698091506958
I0129 18:36:13.999469 140026050483968 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.879782676696777, loss=0.9545926451683044
I0129 18:36:47.728426 140026042091264 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.9205451011657715, loss=0.9567321538925171
I0129 18:37:21.528240 140026050483968 logging_writer.py:48] [176700] global_step=176700, grad_norm=5.160128593444824, loss=0.948627769947052
I0129 18:37:45.328811 140187804313408 spec.py:321] Evaluating on the training split.
I0129 18:37:51.564493 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 18:38:00.431406 140187804313408 spec.py:349] Evaluating on the test split.
I0129 18:38:03.271750 140187804313408 submission_runner.py:408] Time since start: 61831.23s, 	Step: 176772, 	{'train/accuracy': 0.8796038031578064, 'train/loss': 0.43470466136932373, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.036797285079956, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.7231155633926392, 'test/num_examples': 10000, 'score': 59709.20754027367, 'total_duration': 61831.22635412216, 'accumulated_submission_time': 59709.20754027367, 'accumulated_eval_time': 2108.078211784363, 'accumulated_logging_time': 7.485778331756592}
I0129 18:38:03.320590 140026067269376 logging_writer.py:48] [176772] accumulated_eval_time=2108.078212, accumulated_logging_time=7.485778, accumulated_submission_time=59709.207540, global_step=176772, preemption_count=0, score=59709.207540, test/accuracy=0.627100, test/loss=1.723116, test/num_examples=10000, total_duration=61831.226354, train/accuracy=0.879604, train/loss=0.434705, validation/accuracy=0.744800, validation/loss=1.036797, validation/num_examples=50000
I0129 18:38:13.087988 140026151130880 logging_writer.py:48] [176800] global_step=176800, grad_norm=5.381980895996094, loss=1.0116519927978516
I0129 18:38:46.798373 140026067269376 logging_writer.py:48] [176900] global_step=176900, grad_norm=5.007193088531494, loss=0.9151960611343384
I0129 18:39:20.470952 140026151130880 logging_writer.py:48] [177000] global_step=177000, grad_norm=5.175086498260498, loss=0.931374192237854
I0129 18:39:54.169762 140026067269376 logging_writer.py:48] [177100] global_step=177100, grad_norm=5.550713062286377, loss=0.971322774887085
I0129 18:40:27.885904 140026151130880 logging_writer.py:48] [177200] global_step=177200, grad_norm=5.594734191894531, loss=1.0661311149597168
I0129 18:41:01.636588 140026067269376 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.836760520935059, loss=0.9147571921348572
I0129 18:41:35.396984 140026151130880 logging_writer.py:48] [177400] global_step=177400, grad_norm=5.2357001304626465, loss=0.9515079259872437
I0129 18:42:09.112960 140026067269376 logging_writer.py:48] [177500] global_step=177500, grad_norm=5.486882209777832, loss=0.9418495893478394
I0129 18:42:42.864996 140026151130880 logging_writer.py:48] [177600] global_step=177600, grad_norm=5.0678253173828125, loss=0.9432184100151062
I0129 18:43:16.612537 140026067269376 logging_writer.py:48] [177700] global_step=177700, grad_norm=5.270625114440918, loss=0.8896926045417786
I0129 18:43:50.379553 140026151130880 logging_writer.py:48] [177800] global_step=177800, grad_norm=5.325237274169922, loss=0.8814661502838135
I0129 18:44:24.130476 140026067269376 logging_writer.py:48] [177900] global_step=177900, grad_norm=5.544595241546631, loss=1.0028563737869263
I0129 18:44:57.849490 140026151130880 logging_writer.py:48] [178000] global_step=178000, grad_norm=5.473300457000732, loss=1.0253965854644775
I0129 18:45:31.596012 140026067269376 logging_writer.py:48] [178100] global_step=178100, grad_norm=5.533215045928955, loss=0.911742627620697
I0129 18:46:05.330208 140026151130880 logging_writer.py:48] [178200] global_step=178200, grad_norm=5.367934703826904, loss=1.0245857238769531
I0129 18:46:33.490492 140187804313408 spec.py:321] Evaluating on the training split.
I0129 18:46:39.755083 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 18:46:48.656790 140187804313408 spec.py:349] Evaluating on the test split.
I0129 18:46:51.320832 140187804313408 submission_runner.py:408] Time since start: 62359.28s, 	Step: 178285, 	{'train/accuracy': 0.8785474896430969, 'train/loss': 0.42888569831848145, 'validation/accuracy': 0.746399998664856, 'validation/loss': 1.0343586206436157, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.7289866209030151, 'test/num_examples': 10000, 'score': 60219.31211185455, 'total_duration': 62359.2754983902, 'accumulated_submission_time': 60219.31211185455, 'accumulated_eval_time': 2125.9084413051605, 'accumulated_logging_time': 7.544202566146851}
I0129 18:46:51.374163 140026075662080 logging_writer.py:48] [178285] accumulated_eval_time=2125.908441, accumulated_logging_time=7.544203, accumulated_submission_time=60219.312112, global_step=178285, preemption_count=0, score=60219.312112, test/accuracy=0.627900, test/loss=1.728987, test/num_examples=10000, total_duration=62359.275498, train/accuracy=0.878547, train/loss=0.428886, validation/accuracy=0.746400, validation/loss=1.034359, validation/num_examples=50000
I0129 18:46:56.797464 140026167916288 logging_writer.py:48] [178300] global_step=178300, grad_norm=5.370604991912842, loss=1.045089602470398
I0129 18:47:30.534955 140026075662080 logging_writer.py:48] [178400] global_step=178400, grad_norm=5.291601181030273, loss=0.944583535194397
I0129 18:48:04.210659 140026167916288 logging_writer.py:48] [178500] global_step=178500, grad_norm=5.47528076171875, loss=0.9878315329551697
I0129 18:48:37.907050 140026075662080 logging_writer.py:48] [178600] global_step=178600, grad_norm=5.713726997375488, loss=1.0472807884216309
I0129 18:49:11.678266 140026167916288 logging_writer.py:48] [178700] global_step=178700, grad_norm=5.050799369812012, loss=0.894073486328125
I0129 18:49:45.548657 140026075662080 logging_writer.py:48] [178800] global_step=178800, grad_norm=5.441827297210693, loss=0.9744527339935303
I0129 18:50:19.327543 140026167916288 logging_writer.py:48] [178900] global_step=178900, grad_norm=5.141085147857666, loss=1.0344822406768799
I0129 18:50:53.048708 140026075662080 logging_writer.py:48] [179000] global_step=179000, grad_norm=5.686951160430908, loss=1.1005427837371826
I0129 18:51:26.824840 140026167916288 logging_writer.py:48] [179100] global_step=179100, grad_norm=5.364226818084717, loss=0.9950693845748901
I0129 18:52:00.570745 140026075662080 logging_writer.py:48] [179200] global_step=179200, grad_norm=5.099855899810791, loss=0.990391194820404
I0129 18:52:34.347618 140026167916288 logging_writer.py:48] [179300] global_step=179300, grad_norm=5.909411907196045, loss=0.9569622278213501
I0129 18:53:08.079746 140026075662080 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.957117080688477, loss=0.9529001116752625
I0129 18:53:41.856421 140026167916288 logging_writer.py:48] [179500] global_step=179500, grad_norm=5.277029514312744, loss=0.9809321165084839
I0129 18:54:15.566566 140026075662080 logging_writer.py:48] [179600] global_step=179600, grad_norm=5.447071075439453, loss=1.0046159029006958
I0129 18:54:49.326056 140026167916288 logging_writer.py:48] [179700] global_step=179700, grad_norm=5.362775802612305, loss=0.973533034324646
I0129 18:55:21.526048 140187804313408 spec.py:321] Evaluating on the training split.
I0129 18:55:27.749534 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 18:55:36.624388 140187804313408 spec.py:349] Evaluating on the test split.
I0129 18:55:39.296673 140187804313408 submission_runner.py:408] Time since start: 62887.25s, 	Step: 179797, 	{'train/accuracy': 0.8817960619926453, 'train/loss': 0.4216182827949524, 'validation/accuracy': 0.7474600076675415, 'validation/loss': 1.027718424797058, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.7195037603378296, 'test/num_examples': 10000, 'score': 60729.40054988861, 'total_duration': 62887.25140142441, 'accumulated_submission_time': 60729.40054988861, 'accumulated_eval_time': 2143.6790177822113, 'accumulated_logging_time': 7.6068150997161865}
I0129 18:55:39.350948 140026042091264 logging_writer.py:48] [179797] accumulated_eval_time=2143.679018, accumulated_logging_time=7.606815, accumulated_submission_time=60729.400550, global_step=179797, preemption_count=0, score=60729.400550, test/accuracy=0.624700, test/loss=1.719504, test/num_examples=10000, total_duration=62887.251401, train/accuracy=0.881796, train/loss=0.421618, validation/accuracy=0.747460, validation/loss=1.027718, validation/num_examples=50000
I0129 18:55:40.699303 140026050483968 logging_writer.py:48] [179800] global_step=179800, grad_norm=5.291419982910156, loss=0.9791431427001953
I0129 18:56:14.513107 140026042091264 logging_writer.py:48] [179900] global_step=179900, grad_norm=5.477518081665039, loss=0.94169682264328
I0129 18:56:48.194111 140026050483968 logging_writer.py:48] [180000] global_step=180000, grad_norm=5.318885803222656, loss=1.0066897869110107
I0129 18:57:21.915728 140026042091264 logging_writer.py:48] [180100] global_step=180100, grad_norm=5.061094760894775, loss=0.9298853874206543
I0129 18:57:55.648194 140026050483968 logging_writer.py:48] [180200] global_step=180200, grad_norm=5.477622985839844, loss=0.9890788793563843
I0129 18:58:29.327677 140026042091264 logging_writer.py:48] [180300] global_step=180300, grad_norm=5.404910564422607, loss=0.9561017751693726
I0129 18:59:03.086658 140026050483968 logging_writer.py:48] [180400] global_step=180400, grad_norm=5.616429328918457, loss=0.9473930597305298
I0129 18:59:36.839655 140026042091264 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.917049884796143, loss=0.9986037611961365
I0129 19:00:10.563931 140026050483968 logging_writer.py:48] [180600] global_step=180600, grad_norm=5.080844879150391, loss=0.9650000333786011
I0129 19:00:44.326017 140026042091264 logging_writer.py:48] [180700] global_step=180700, grad_norm=5.453268527984619, loss=1.0160329341888428
I0129 19:01:18.012007 140026050483968 logging_writer.py:48] [180800] global_step=180800, grad_norm=5.239461421966553, loss=0.9465798735618591
I0129 19:01:51.758624 140026042091264 logging_writer.py:48] [180900] global_step=180900, grad_norm=5.381522178649902, loss=1.008100152015686
I0129 19:02:25.625994 140026050483968 logging_writer.py:48] [181000] global_step=181000, grad_norm=5.113601207733154, loss=0.833048939704895
I0129 19:02:59.306982 140026042091264 logging_writer.py:48] [181100] global_step=181100, grad_norm=5.393920421600342, loss=0.9709598422050476
I0129 19:03:33.069966 140026050483968 logging_writer.py:48] [181200] global_step=181200, grad_norm=5.409844398498535, loss=0.9909262657165527
I0129 19:04:06.864671 140026042091264 logging_writer.py:48] [181300] global_step=181300, grad_norm=5.3562116622924805, loss=0.9488440155982971
I0129 19:04:09.378193 140187804313408 spec.py:321] Evaluating on the training split.
I0129 19:04:15.653470 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 19:04:24.430430 140187804313408 spec.py:349] Evaluating on the test split.
I0129 19:04:27.100253 140187804313408 submission_runner.py:408] Time since start: 63415.05s, 	Step: 181309, 	{'train/accuracy': 0.8843869566917419, 'train/loss': 0.4094845950603485, 'validation/accuracy': 0.7476199865341187, 'validation/loss': 1.02272629737854, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.7120555639266968, 'test/num_examples': 10000, 'score': 61239.36246538162, 'total_duration': 63415.05498671532, 'accumulated_submission_time': 61239.36246538162, 'accumulated_eval_time': 2161.401031255722, 'accumulated_logging_time': 7.670835256576538}
I0129 19:04:27.149718 140026151130880 logging_writer.py:48] [181309] accumulated_eval_time=2161.401031, accumulated_logging_time=7.670835, accumulated_submission_time=61239.362465, global_step=181309, preemption_count=0, score=61239.362465, test/accuracy=0.625600, test/loss=1.712056, test/num_examples=10000, total_duration=63415.054987, train/accuracy=0.884387, train/loss=0.409485, validation/accuracy=0.747620, validation/loss=1.022726, validation/num_examples=50000
I0129 19:04:58.162493 140026159523584 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.7818522453308105, loss=0.925538957118988
I0129 19:05:31.892211 140026151130880 logging_writer.py:48] [181500] global_step=181500, grad_norm=5.516922473907471, loss=0.9824355840682983
I0129 19:06:05.556489 140026159523584 logging_writer.py:48] [181600] global_step=181600, grad_norm=5.399963855743408, loss=1.036474585533142
I0129 19:06:39.286611 140026151130880 logging_writer.py:48] [181700] global_step=181700, grad_norm=5.626502990722656, loss=0.8837422132492065
I0129 19:07:12.993400 140026159523584 logging_writer.py:48] [181800] global_step=181800, grad_norm=5.16802453994751, loss=0.8867745995521545
I0129 19:07:46.706945 140026151130880 logging_writer.py:48] [181900] global_step=181900, grad_norm=5.274158954620361, loss=0.9376025199890137
I0129 19:08:20.457789 140026159523584 logging_writer.py:48] [182000] global_step=182000, grad_norm=5.218682289123535, loss=0.9612715244293213
I0129 19:08:54.372668 140026151130880 logging_writer.py:48] [182100] global_step=182100, grad_norm=5.209293842315674, loss=0.9865149259567261
I0129 19:09:28.120359 140026159523584 logging_writer.py:48] [182200] global_step=182200, grad_norm=5.254197120666504, loss=0.9730584621429443
I0129 19:10:01.839804 140026151130880 logging_writer.py:48] [182300] global_step=182300, grad_norm=5.474854946136475, loss=0.9480520486831665
I0129 19:10:35.620210 140026159523584 logging_writer.py:48] [182400] global_step=182400, grad_norm=5.408030033111572, loss=0.9800776243209839
I0129 19:11:09.362305 140026151130880 logging_writer.py:48] [182500] global_step=182500, grad_norm=5.188941478729248, loss=0.8736851215362549
I0129 19:11:43.131995 140026159523584 logging_writer.py:48] [182600] global_step=182600, grad_norm=5.590317249298096, loss=1.0133755207061768
I0129 19:12:16.850621 140026151130880 logging_writer.py:48] [182700] global_step=182700, grad_norm=5.488497257232666, loss=0.9512179493904114
I0129 19:12:50.623671 140026159523584 logging_writer.py:48] [182800] global_step=182800, grad_norm=5.791708469390869, loss=1.0491575002670288
I0129 19:12:57.173442 140187804313408 spec.py:321] Evaluating on the training split.
I0129 19:13:03.556047 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 19:13:12.429680 140187804313408 spec.py:349] Evaluating on the test split.
I0129 19:13:14.985327 140187804313408 submission_runner.py:408] Time since start: 63942.94s, 	Step: 182821, 	{'train/accuracy': 0.8831712007522583, 'train/loss': 0.4109700918197632, 'validation/accuracy': 0.7479400038719177, 'validation/loss': 1.0229642391204834, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.714728593826294, 'test/num_examples': 10000, 'score': 61749.32241177559, 'total_duration': 63942.94005918503, 'accumulated_submission_time': 61749.32241177559, 'accumulated_eval_time': 2179.2128698825836, 'accumulated_logging_time': 7.729203224182129}
I0129 19:13:15.036395 140026050483968 logging_writer.py:48] [182821] accumulated_eval_time=2179.212870, accumulated_logging_time=7.729203, accumulated_submission_time=61749.322412, global_step=182821, preemption_count=0, score=61749.322412, test/accuracy=0.628200, test/loss=1.714729, test/num_examples=10000, total_duration=63942.940059, train/accuracy=0.883171, train/loss=0.410970, validation/accuracy=0.747940, validation/loss=1.022964, validation/num_examples=50000
I0129 19:13:42.006455 140026058876672 logging_writer.py:48] [182900] global_step=182900, grad_norm=5.561827659606934, loss=0.940643846988678
I0129 19:14:15.727066 140026050483968 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.462430953979492, loss=0.958224892616272
I0129 19:14:49.402333 140026058876672 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.982019901275635, loss=0.8813539147377014
I0129 19:15:23.273584 140026050483968 logging_writer.py:48] [183200] global_step=183200, grad_norm=5.490492343902588, loss=0.9937001466751099
I0129 19:15:56.999209 140026058876672 logging_writer.py:48] [183300] global_step=183300, grad_norm=5.197696208953857, loss=0.9277253746986389
I0129 19:16:30.765631 140026050483968 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.885918617248535, loss=0.8962745666503906
I0129 19:17:04.444201 140026058876672 logging_writer.py:48] [183500] global_step=183500, grad_norm=5.368655204772949, loss=1.0084296464920044
I0129 19:17:38.253413 140026050483968 logging_writer.py:48] [183600] global_step=183600, grad_norm=5.3714423179626465, loss=1.0107053518295288
I0129 19:18:11.973576 140026058876672 logging_writer.py:48] [183700] global_step=183700, grad_norm=5.447845935821533, loss=0.9881312847137451
I0129 19:18:45.749337 140026050483968 logging_writer.py:48] [183800] global_step=183800, grad_norm=5.6277313232421875, loss=0.9529033899307251
I0129 19:19:19.519495 140026058876672 logging_writer.py:48] [183900] global_step=183900, grad_norm=5.6278862953186035, loss=1.0540372133255005
I0129 19:19:53.247531 140026050483968 logging_writer.py:48] [184000] global_step=184000, grad_norm=5.296561241149902, loss=0.9373044967651367
I0129 19:20:26.951974 140026058876672 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.962388515472412, loss=0.8131570816040039
I0129 19:21:00.740187 140026050483968 logging_writer.py:48] [184200] global_step=184200, grad_norm=5.39302921295166, loss=0.9664191007614136
I0129 19:21:34.415804 140026058876672 logging_writer.py:48] [184300] global_step=184300, grad_norm=5.789408206939697, loss=1.0109682083129883
I0129 19:21:45.117130 140187804313408 spec.py:321] Evaluating on the training split.
I0129 19:21:51.315981 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 19:22:00.150130 140187804313408 spec.py:349] Evaluating on the test split.
I0129 19:22:02.799835 140187804313408 submission_runner.py:408] Time since start: 64470.75s, 	Step: 184333, 	{'train/accuracy': 0.8838687539100647, 'train/loss': 0.41283562779426575, 'validation/accuracy': 0.7488600015640259, 'validation/loss': 1.020200252532959, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.7110487222671509, 'test/num_examples': 10000, 'score': 62259.33857059479, 'total_duration': 64470.75457596779, 'accumulated_submission_time': 62259.33857059479, 'accumulated_eval_time': 2196.8955330848694, 'accumulated_logging_time': 7.789600133895874}
I0129 19:22:02.853830 140026159523584 logging_writer.py:48] [184333] accumulated_eval_time=2196.895533, accumulated_logging_time=7.789600, accumulated_submission_time=62259.338571, global_step=184333, preemption_count=0, score=62259.338571, test/accuracy=0.628400, test/loss=1.711049, test/num_examples=10000, total_duration=64470.754576, train/accuracy=0.883869, train/loss=0.412836, validation/accuracy=0.748860, validation/loss=1.020200, validation/num_examples=50000
I0129 19:22:25.805004 140026167916288 logging_writer.py:48] [184400] global_step=184400, grad_norm=5.273094177246094, loss=0.9452543258666992
I0129 19:22:59.557589 140026159523584 logging_writer.py:48] [184500] global_step=184500, grad_norm=5.0449113845825195, loss=0.9254680275917053
I0129 19:23:33.279663 140026167916288 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.98940372467041, loss=0.9370724558830261
I0129 19:24:07.036964 140026159523584 logging_writer.py:48] [184700] global_step=184700, grad_norm=5.354465484619141, loss=0.9725453853607178
I0129 19:24:40.765552 140026167916288 logging_writer.py:48] [184800] global_step=184800, grad_norm=5.219006538391113, loss=0.9120327234268188
I0129 19:25:14.509116 140026159523584 logging_writer.py:48] [184900] global_step=184900, grad_norm=5.211831569671631, loss=0.9581812024116516
I0129 19:25:48.266025 140026167916288 logging_writer.py:48] [185000] global_step=185000, grad_norm=5.114475727081299, loss=0.9274989366531372
I0129 19:26:22.029171 140026159523584 logging_writer.py:48] [185100] global_step=185100, grad_norm=5.336350917816162, loss=0.9505723714828491
I0129 19:26:55.753217 140026167916288 logging_writer.py:48] [185200] global_step=185200, grad_norm=5.315153121948242, loss=0.888690173625946
I0129 19:27:29.510727 140026159523584 logging_writer.py:48] [185300] global_step=185300, grad_norm=5.8041253089904785, loss=1.0089253187179565
I0129 19:28:03.346359 140026167916288 logging_writer.py:48] [185400] global_step=185400, grad_norm=5.216513633728027, loss=0.9679489731788635
I0129 19:28:37.086643 140026159523584 logging_writer.py:48] [185500] global_step=185500, grad_norm=5.396514415740967, loss=0.9164292812347412
I0129 19:29:10.795468 140026167916288 logging_writer.py:48] [185600] global_step=185600, grad_norm=5.279720783233643, loss=0.8910394906997681
I0129 19:29:44.471533 140026159523584 logging_writer.py:48] [185700] global_step=185700, grad_norm=5.518393039703369, loss=1.000881314277649
I0129 19:30:18.216075 140026167916288 logging_writer.py:48] [185800] global_step=185800, grad_norm=5.287057399749756, loss=0.9667970538139343
I0129 19:30:32.903429 140187804313408 spec.py:321] Evaluating on the training split.
I0129 19:30:39.150731 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 19:30:47.943969 140187804313408 spec.py:349] Evaluating on the test split.
I0129 19:30:50.498746 140187804313408 submission_runner.py:408] Time since start: 64998.45s, 	Step: 185845, 	{'train/accuracy': 0.8853037357330322, 'train/loss': 0.4091644287109375, 'validation/accuracy': 0.7495599985122681, 'validation/loss': 1.0208008289337158, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.7125041484832764, 'test/num_examples': 10000, 'score': 62769.32061004639, 'total_duration': 64998.45347905159, 'accumulated_submission_time': 62769.32061004639, 'accumulated_eval_time': 2214.4908051490784, 'accumulated_logging_time': 7.853912591934204}
I0129 19:30:50.552483 140026058876672 logging_writer.py:48] [185845] accumulated_eval_time=2214.490805, accumulated_logging_time=7.853913, accumulated_submission_time=62769.320610, global_step=185845, preemption_count=0, score=62769.320610, test/accuracy=0.629600, test/loss=1.712504, test/num_examples=10000, total_duration=64998.453479, train/accuracy=0.885304, train/loss=0.409164, validation/accuracy=0.749560, validation/loss=1.020801, validation/num_examples=50000
I0129 19:31:09.431991 140026067269376 logging_writer.py:48] [185900] global_step=185900, grad_norm=5.625935077667236, loss=0.9990886449813843
I0129 19:31:43.111114 140026058876672 logging_writer.py:48] [186000] global_step=186000, grad_norm=5.647332668304443, loss=1.0033169984817505
I0129 19:32:16.816827 140026067269376 logging_writer.py:48] [186100] global_step=186100, grad_norm=4.784991264343262, loss=0.9046483039855957
I0129 19:32:50.479368 140026058876672 logging_writer.py:48] [186200] global_step=186200, grad_norm=5.047230243682861, loss=0.898293673992157
I0129 19:33:24.153423 140026067269376 logging_writer.py:48] [186300] global_step=186300, grad_norm=5.2065253257751465, loss=0.9164882898330688
I0129 19:33:57.875445 140026058876672 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.971190452575684, loss=0.8357143402099609
I0129 19:34:31.850640 140026067269376 logging_writer.py:48] [186500] global_step=186500, grad_norm=5.598560810089111, loss=0.9194703102111816
I0129 19:34:49.624396 140026058876672 logging_writer.py:48] [186554] global_step=186554, preemption_count=0, score=63008.322009
I0129 19:34:50.054045 140187804313408 checkpoints.py:490] Saving checkpoint at step: 186554
I0129 19:34:51.160244 140187804313408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_4/checkpoint_186554
I0129 19:34:51.183928 140187804313408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_4/checkpoint_186554.
I0129 19:34:51.905338 140187804313408 submission_runner.py:583] Tuning trial 4/5
I0129 19:34:51.905559 140187804313408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0129 19:34:51.938776 140187804313408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010961415246129036, 'train/loss': 6.912662982940674, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 32.62680268287659, 'total_duration': 50.656187772750854, 'accumulated_submission_time': 32.62680268287659, 'accumulated_eval_time': 18.02929162979126, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1498, {'train/accuracy': 0.20161032676696777, 'train/loss': 4.02529239654541, 'validation/accuracy': 0.18140000104904175, 'validation/loss': 4.143918514251709, 'validation/num_examples': 50000, 'test/accuracy': 0.13580000400543213, 'test/loss': 4.661078929901123, 'test/num_examples': 10000, 'score': 542.689546585083, 'total_duration': 578.9105360507965, 'accumulated_submission_time': 542.689546585083, 'accumulated_eval_time': 36.148682594299316, 'accumulated_logging_time': 0.01851630210876465, 'global_step': 1498, 'preemption_count': 0}), (2997, {'train/accuracy': 0.3574816584587097, 'train/loss': 2.9869542121887207, 'validation/accuracy': 0.33052000403404236, 'validation/loss': 3.145972728729248, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 3.8006327152252197, 'test/num_examples': 10000, 'score': 1052.8628568649292, 'total_duration': 1107.221934556961, 'accumulated_submission_time': 1052.8628568649292, 'accumulated_eval_time': 54.20573401451111, 'accumulated_logging_time': 0.04628777503967285, 'global_step': 2997, 'preemption_count': 0}), (4497, {'train/accuracy': 0.3845065236091614, 'train/loss': 2.834846258163452, 'validation/accuracy': 0.3623199760913849, 'validation/loss': 2.950068712234497, 'validation/num_examples': 50000, 'test/accuracy': 0.27980002760887146, 'test/loss': 3.6492860317230225, 'test/num_examples': 10000, 'score': 1562.7982881069183, 'total_duration': 1635.2103700637817, 'accumulated_submission_time': 1562.7982881069183, 'accumulated_eval_time': 72.17443251609802, 'accumulated_logging_time': 0.07693171501159668, 'global_step': 4497, 'preemption_count': 0}), (5999, {'train/accuracy': 0.3507254421710968, 'train/loss': 3.0492608547210693, 'validation/accuracy': 0.30963999032974243, 'validation/loss': 3.407315969467163, 'validation/num_examples': 50000, 'test/accuracy': 0.23880000412464142, 'test/loss': 4.093076705932617, 'test/num_examples': 10000, 'score': 2072.8568086624146, 'total_duration': 2162.92281293869, 'accumulated_submission_time': 2072.8568086624146, 'accumulated_eval_time': 89.74597930908203, 'accumulated_logging_time': 0.10487532615661621, 'global_step': 5999, 'preemption_count': 0}), (7501, {'train/accuracy': 0.07164780050516129, 'train/loss': 5.990123271942139, 'validation/accuracy': 0.06967999786138535, 'validation/loss': 6.023623466491699, 'validation/num_examples': 50000, 'test/accuracy': 0.04700000211596489, 'test/loss': 6.509797096252441, 'test/num_examples': 10000, 'score': 2582.8044941425323, 'total_duration': 2691.0967836380005, 'accumulated_submission_time': 2582.8044941425323, 'accumulated_eval_time': 107.88517737388611, 'accumulated_logging_time': 0.13736391067504883, 'global_step': 7501, 'preemption_count': 0}), (9005, {'train/accuracy': 0.4273158311843872, 'train/loss': 2.571629285812378, 'validation/accuracy': 0.39673998951911926, 'validation/loss': 2.782094717025757, 'validation/num_examples': 50000, 'test/accuracy': 0.29430001974105835, 'test/loss': 3.5717177391052246, 'test/num_examples': 10000, 'score': 3092.7759182453156, 'total_duration': 3219.6616904735565, 'accumulated_submission_time': 3092.7759182453156, 'accumulated_eval_time': 126.39080882072449, 'accumulated_logging_time': 0.16942358016967773, 'global_step': 9005, 'preemption_count': 0}), (10509, {'train/accuracy': 0.19814252853393555, 'train/loss': 4.607287406921387, 'validation/accuracy': 0.17875999212265015, 'validation/loss': 4.729046821594238, 'validation/num_examples': 50000, 'test/accuracy': 0.133200004696846, 'test/loss': 5.446406364440918, 'test/num_examples': 10000, 'score': 3602.7280580997467, 'total_duration': 3747.433574438095, 'accumulated_submission_time': 3602.7280580997467, 'accumulated_eval_time': 144.12997436523438, 'accumulated_logging_time': 0.1947331428527832, 'global_step': 10509, 'preemption_count': 0}), (12015, {'train/accuracy': 0.06770168989896774, 'train/loss': 6.2029805183410645, 'validation/accuracy': 0.0630199983716011, 'validation/loss': 6.263773441314697, 'validation/num_examples': 50000, 'test/accuracy': 0.044200003147125244, 'test/loss': 6.679653644561768, 'test/num_examples': 10000, 'score': 4112.816953420639, 'total_duration': 4275.339448213577, 'accumulated_submission_time': 4112.816953420639, 'accumulated_eval_time': 161.86528515815735, 'accumulated_logging_time': 0.22316813468933105, 'global_step': 12015, 'preemption_count': 0}), (13520, {'train/accuracy': 0.3900669515132904, 'train/loss': 2.8671154975891113, 'validation/accuracy': 0.3664799928665161, 'validation/loss': 3.0192630290985107, 'validation/num_examples': 50000, 'test/accuracy': 0.2696000039577484, 'test/loss': 3.811286449432373, 'test/num_examples': 10000, 'score': 4622.73382973671, 'total_duration': 4803.095787525177, 'accumulated_submission_time': 4622.73382973671, 'accumulated_eval_time': 179.61972188949585, 'accumulated_logging_time': 0.2544693946838379, 'global_step': 13520, 'preemption_count': 0}), (15027, {'train/accuracy': 0.21578045189380646, 'train/loss': 4.200702667236328, 'validation/accuracy': 0.19119998812675476, 'validation/loss': 4.4994025230407715, 'validation/num_examples': 50000, 'test/accuracy': 0.14730000495910645, 'test/loss': 5.18111515045166, 'test/num_examples': 10000, 'score': 5132.905265569687, 'total_duration': 5332.158295869827, 'accumulated_submission_time': 5132.905265569687, 'accumulated_eval_time': 198.42433190345764, 'accumulated_logging_time': 0.2853665351867676, 'global_step': 15027, 'preemption_count': 0}), (16534, {'train/accuracy': 0.2916533648967743, 'train/loss': 3.804319143295288, 'validation/accuracy': 0.26729997992515564, 'validation/loss': 4.0446624755859375, 'validation/num_examples': 50000, 'test/accuracy': 0.19860000908374786, 'test/loss': 4.769838333129883, 'test/num_examples': 10000, 'score': 5643.002831459045, 'total_duration': 5860.815819740295, 'accumulated_submission_time': 5643.002831459045, 'accumulated_eval_time': 216.898282289505, 'accumulated_logging_time': 0.3170950412750244, 'global_step': 16534, 'preemption_count': 0}), (18042, {'train/accuracy': 0.29175302386283875, 'train/loss': 3.487156391143799, 'validation/accuracy': 0.2727600038051605, 'validation/loss': 3.6579201221466064, 'validation/num_examples': 50000, 'test/accuracy': 0.19460001587867737, 'test/loss': 4.378692626953125, 'test/num_examples': 10000, 'score': 6152.994265079498, 'total_duration': 6388.826258897781, 'accumulated_submission_time': 6152.994265079498, 'accumulated_eval_time': 234.83249616622925, 'accumulated_logging_time': 0.34627366065979004, 'global_step': 18042, 'preemption_count': 0}), (19550, {'train/accuracy': 0.2448979616165161, 'train/loss': 4.03620719909668, 'validation/accuracy': 0.2307799905538559, 'validation/loss': 4.1693949699401855, 'validation/num_examples': 50000, 'test/accuracy': 0.17520001530647278, 'test/loss': 4.855306148529053, 'test/num_examples': 10000, 'score': 6663.09573674202, 'total_duration': 6916.799047708511, 'accumulated_submission_time': 6663.09573674202, 'accumulated_eval_time': 252.61838150024414, 'accumulated_logging_time': 0.3780183792114258, 'global_step': 19550, 'preemption_count': 0}), (21059, {'train/accuracy': 0.11348054558038712, 'train/loss': 5.853148460388184, 'validation/accuracy': 0.1053599938750267, 'validation/loss': 6.068696022033691, 'validation/num_examples': 50000, 'test/accuracy': 0.08340000361204147, 'test/loss': 6.58660888671875, 'test/num_examples': 10000, 'score': 7173.257848501205, 'total_duration': 7445.002496004105, 'accumulated_submission_time': 7173.257848501205, 'accumulated_eval_time': 270.57379508018494, 'accumulated_logging_time': 0.40931153297424316, 'global_step': 21059, 'preemption_count': 0}), (22567, {'train/accuracy': 0.20862562954425812, 'train/loss': 4.400444030761719, 'validation/accuracy': 0.19873999059200287, 'validation/loss': 4.497589111328125, 'validation/num_examples': 50000, 'test/accuracy': 0.14230000972747803, 'test/loss': 5.258711814880371, 'test/num_examples': 10000, 'score': 7683.180972576141, 'total_duration': 7972.925594329834, 'accumulated_submission_time': 7683.180972576141, 'accumulated_eval_time': 288.48663353919983, 'accumulated_logging_time': 0.44240808486938477, 'global_step': 22567, 'preemption_count': 0}), (24076, {'train/accuracy': 0.29976481199264526, 'train/loss': 3.4221742153167725, 'validation/accuracy': 0.2604199945926666, 'validation/loss': 3.7820355892181396, 'validation/num_examples': 50000, 'test/accuracy': 0.19260001182556152, 'test/loss': 4.4531474113464355, 'test/num_examples': 10000, 'score': 8193.239090681076, 'total_duration': 8500.813712358475, 'accumulated_submission_time': 8193.239090681076, 'accumulated_eval_time': 306.22909712791443, 'accumulated_logging_time': 0.473712682723999, 'global_step': 24076, 'preemption_count': 0}), (25584, {'train/accuracy': 0.08533960580825806, 'train/loss': 5.972334861755371, 'validation/accuracy': 0.08336000144481659, 'validation/loss': 5.932559967041016, 'validation/num_examples': 50000, 'test/accuracy': 0.05340000241994858, 'test/loss': 6.625743389129639, 'test/num_examples': 10000, 'score': 8703.223582744598, 'total_duration': 9028.702632427216, 'accumulated_submission_time': 8703.223582744598, 'accumulated_eval_time': 324.04403138160706, 'accumulated_logging_time': 0.506464958190918, 'global_step': 25584, 'preemption_count': 0}), (27093, {'train/accuracy': 0.28204718232154846, 'train/loss': 3.6558451652526855, 'validation/accuracy': 0.26405999064445496, 'validation/loss': 3.7624459266662598, 'validation/num_examples': 50000, 'test/accuracy': 0.20600001513957977, 'test/loss': 4.422956943511963, 'test/num_examples': 10000, 'score': 9213.1674451828, 'total_duration': 9556.718665838242, 'accumulated_submission_time': 9213.1674451828, 'accumulated_eval_time': 342.0292069911957, 'accumulated_logging_time': 0.5385315418243408, 'global_step': 27093, 'preemption_count': 0}), (28602, {'train/accuracy': 0.21342872083187103, 'train/loss': 4.641663551330566, 'validation/accuracy': 0.19767999649047852, 'validation/loss': 4.830415725708008, 'validation/num_examples': 50000, 'test/accuracy': 0.15360000729560852, 'test/loss': 5.504821300506592, 'test/num_examples': 10000, 'score': 9723.193513393402, 'total_duration': 10084.52348947525, 'accumulated_submission_time': 9723.193513393402, 'accumulated_eval_time': 359.71742606163025, 'accumulated_logging_time': 0.5759317874908447, 'global_step': 28602, 'preemption_count': 0}), (30048, {'train/accuracy': 0.15519371628761292, 'train/loss': 4.965692043304443, 'validation/accuracy': 0.15135999023914337, 'validation/loss': 5.025054454803467, 'validation/num_examples': 50000, 'test/accuracy': 0.11060000211000443, 'test/loss': 5.6790618896484375, 'test/num_examples': 10000, 'score': 10233.195832252502, 'total_duration': 10612.37854552269, 'accumulated_submission_time': 10233.195832252502, 'accumulated_eval_time': 377.4810211658478, 'accumulated_logging_time': 0.612412691116333, 'global_step': 30048, 'preemption_count': 0}), (31558, {'train/accuracy': 0.2456154227256775, 'train/loss': 4.217416763305664, 'validation/accuracy': 0.23111999034881592, 'validation/loss': 4.373788833618164, 'validation/num_examples': 50000, 'test/accuracy': 0.1656000018119812, 'test/loss': 5.351226806640625, 'test/num_examples': 10000, 'score': 10743.147997140884, 'total_duration': 11140.925688028336, 'accumulated_submission_time': 10743.147997140884, 'accumulated_eval_time': 395.98668384552, 'accumulated_logging_time': 0.6475231647491455, 'global_step': 31558, 'preemption_count': 0}), (33070, {'train/accuracy': 0.17233338952064514, 'train/loss': 4.832897663116455, 'validation/accuracy': 0.1630599945783615, 'validation/loss': 4.949711322784424, 'validation/num_examples': 50000, 'test/accuracy': 0.12480000406503677, 'test/loss': 5.6210150718688965, 'test/num_examples': 10000, 'score': 11253.401313781738, 'total_duration': 11669.212977647781, 'accumulated_submission_time': 11253.401313781738, 'accumulated_eval_time': 413.93563413619995, 'accumulated_logging_time': 0.678107738494873, 'global_step': 33070, 'preemption_count': 0}), (34580, {'train/accuracy': 0.13695789873600006, 'train/loss': 5.655052661895752, 'validation/accuracy': 0.12545999884605408, 'validation/loss': 5.8631768226623535, 'validation/num_examples': 50000, 'test/accuracy': 0.09600000083446503, 'test/loss': 6.327773094177246, 'test/num_examples': 10000, 'score': 11763.362285137177, 'total_duration': 12197.119595527649, 'accumulated_submission_time': 11763.362285137177, 'accumulated_eval_time': 431.78840684890747, 'accumulated_logging_time': 0.7146234512329102, 'global_step': 34580, 'preemption_count': 0}), (36091, {'train/accuracy': 0.2906768023967743, 'train/loss': 3.589463233947754, 'validation/accuracy': 0.2702600061893463, 'validation/loss': 3.7571001052856445, 'validation/num_examples': 50000, 'test/accuracy': 0.19180001318454742, 'test/loss': 4.572559833526611, 'test/num_examples': 10000, 'score': 12273.451851844788, 'total_duration': 12725.135549068451, 'accumulated_submission_time': 12273.451851844788, 'accumulated_eval_time': 449.6273202896118, 'accumulated_logging_time': 0.7484467029571533, 'global_step': 36091, 'preemption_count': 0}), (37601, {'train/accuracy': 0.3878348171710968, 'train/loss': 2.8524887561798096, 'validation/accuracy': 0.36010000109672546, 'validation/loss': 3.0229721069335938, 'validation/num_examples': 50000, 'test/accuracy': 0.2678000032901764, 'test/loss': 3.7928645610809326, 'test/num_examples': 10000, 'score': 12783.497530221939, 'total_duration': 13252.958825826645, 'accumulated_submission_time': 12783.497530221939, 'accumulated_eval_time': 467.3155233860016, 'accumulated_logging_time': 0.7832720279693604, 'global_step': 37601, 'preemption_count': 0}), (39112, {'train/accuracy': 0.06728316098451614, 'train/loss': 6.174365043640137, 'validation/accuracy': 0.06808000057935715, 'validation/loss': 6.197511196136475, 'validation/num_examples': 50000, 'test/accuracy': 0.046300001442432404, 'test/loss': 6.7392120361328125, 'test/num_examples': 10000, 'score': 13293.57846236229, 'total_duration': 13781.161801338196, 'accumulated_submission_time': 13293.57846236229, 'accumulated_eval_time': 485.3442895412445, 'accumulated_logging_time': 0.820784330368042, 'global_step': 39112, 'preemption_count': 0}), (40623, {'train/accuracy': 0.10668446868658066, 'train/loss': 6.06245756149292, 'validation/accuracy': 0.09827999770641327, 'validation/loss': 6.124608516693115, 'validation/num_examples': 50000, 'test/accuracy': 0.06650000065565109, 'test/loss': 6.820237159729004, 'test/num_examples': 10000, 'score': 13803.59059214592, 'total_duration': 14309.18965625763, 'accumulated_submission_time': 13803.59059214592, 'accumulated_eval_time': 503.2651972770691, 'accumulated_logging_time': 0.8609738349914551, 'global_step': 40623, 'preemption_count': 0}), (42134, {'train/accuracy': 0.17036032676696777, 'train/loss': 4.943631649017334, 'validation/accuracy': 0.16232000291347504, 'validation/loss': 5.046461582183838, 'validation/num_examples': 50000, 'test/accuracy': 0.11220000684261322, 'test/loss': 5.937928199768066, 'test/num_examples': 10000, 'score': 14313.546487808228, 'total_duration': 14837.033590316772, 'accumulated_submission_time': 14313.546487808228, 'accumulated_eval_time': 521.0472767353058, 'accumulated_logging_time': 0.9110279083251953, 'global_step': 42134, 'preemption_count': 0}), (43643, {'train/accuracy': 0.20894451439380646, 'train/loss': 4.4410600662231445, 'validation/accuracy': 0.1917800009250641, 'validation/loss': 4.693207263946533, 'validation/num_examples': 50000, 'test/accuracy': 0.15620000660419464, 'test/loss': 5.226414203643799, 'test/num_examples': 10000, 'score': 14822.672254800797, 'total_duration': 15365.175357818604, 'accumulated_submission_time': 14822.672254800797, 'accumulated_eval_time': 538.9020249843597, 'accumulated_logging_time': 2.0156309604644775, 'global_step': 43643, 'preemption_count': 0}), (45155, {'train/accuracy': 0.18227837979793549, 'train/loss': 4.9106268882751465, 'validation/accuracy': 0.17061999440193176, 'validation/loss': 5.011672019958496, 'validation/num_examples': 50000, 'test/accuracy': 0.1218000054359436, 'test/loss': 5.921328067779541, 'test/num_examples': 10000, 'score': 15332.741003751755, 'total_duration': 15892.876887321472, 'accumulated_submission_time': 15332.741003751755, 'accumulated_eval_time': 556.4439558982849, 'accumulated_logging_time': 2.052147626876831, 'global_step': 45155, 'preemption_count': 0}), (46667, {'train/accuracy': 0.0934709832072258, 'train/loss': 5.805931568145752, 'validation/accuracy': 0.08568000048398972, 'validation/loss': 5.914002418518066, 'validation/num_examples': 50000, 'test/accuracy': 0.06300000101327896, 'test/loss': 6.328012466430664, 'test/num_examples': 10000, 'score': 15842.871906518936, 'total_duration': 16421.14906358719, 'accumulated_submission_time': 15842.871906518936, 'accumulated_eval_time': 574.490583896637, 'accumulated_logging_time': 2.092453956604004, 'global_step': 46667, 'preemption_count': 0}), (48179, {'train/accuracy': 0.09851323068141937, 'train/loss': 5.712707996368408, 'validation/accuracy': 0.09359999746084213, 'validation/loss': 5.7977824211120605, 'validation/num_examples': 50000, 'test/accuracy': 0.07240000367164612, 'test/loss': 6.2724175453186035, 'test/num_examples': 10000, 'score': 16352.813577651978, 'total_duration': 16948.93683218956, 'accumulated_submission_time': 16352.813577651978, 'accumulated_eval_time': 592.2423067092896, 'accumulated_logging_time': 2.1319072246551514, 'global_step': 48179, 'preemption_count': 0}), (49691, {'train/accuracy': 0.3025948703289032, 'train/loss': 3.468749523162842, 'validation/accuracy': 0.28519999980926514, 'validation/loss': 3.5776867866516113, 'validation/num_examples': 50000, 'test/accuracy': 0.20410001277923584, 'test/loss': 4.3102006912231445, 'test/num_examples': 10000, 'score': 16862.992620944977, 'total_duration': 17476.90072107315, 'accumulated_submission_time': 16862.992620944977, 'accumulated_eval_time': 609.932549238205, 'accumulated_logging_time': 2.1713497638702393, 'global_step': 49691, 'preemption_count': 0}), (51203, {'train/accuracy': 0.33033719658851624, 'train/loss': 3.299055576324463, 'validation/accuracy': 0.2969200015068054, 'validation/loss': 3.578896999359131, 'validation/num_examples': 50000, 'test/accuracy': 0.21730001270771027, 'test/loss': 4.417811393737793, 'test/num_examples': 10000, 'score': 17373.104477643967, 'total_duration': 18004.936414718628, 'accumulated_submission_time': 17373.104477643967, 'accumulated_eval_time': 627.7616715431213, 'accumulated_logging_time': 2.2113375663757324, 'global_step': 51203, 'preemption_count': 0}), (52715, {'train/accuracy': 0.1820591539144516, 'train/loss': 4.70458459854126, 'validation/accuracy': 0.17093999683856964, 'validation/loss': 4.80840539932251, 'validation/num_examples': 50000, 'test/accuracy': 0.13040000200271606, 'test/loss': 5.461226463317871, 'test/num_examples': 10000, 'score': 17883.095085144043, 'total_duration': 18532.963203907013, 'accumulated_submission_time': 17883.095085144043, 'accumulated_eval_time': 645.7019193172455, 'accumulated_logging_time': 2.252317190170288, 'global_step': 52715, 'preemption_count': 0}), (54228, {'train/accuracy': 0.1476801633834839, 'train/loss': 5.142282009124756, 'validation/accuracy': 0.14037999510765076, 'validation/loss': 5.216727256774902, 'validation/num_examples': 50000, 'test/accuracy': 0.11010000854730606, 'test/loss': 5.7479472160339355, 'test/num_examples': 10000, 'score': 18393.22921514511, 'total_duration': 19060.944731235504, 'accumulated_submission_time': 18393.22921514511, 'accumulated_eval_time': 663.4520955085754, 'accumulated_logging_time': 2.2944045066833496, 'global_step': 54228, 'preemption_count': 0}), (55740, {'train/accuracy': 0.21615912020206451, 'train/loss': 4.4626359939575195, 'validation/accuracy': 0.2102999985218048, 'validation/loss': 4.564239025115967, 'validation/num_examples': 50000, 'test/accuracy': 0.15570001304149628, 'test/loss': 5.351230621337891, 'test/num_examples': 10000, 'score': 18903.425624847412, 'total_duration': 19589.123804807663, 'accumulated_submission_time': 18903.425624847412, 'accumulated_eval_time': 681.3409023284912, 'accumulated_logging_time': 2.3332133293151855, 'global_step': 55740, 'preemption_count': 0}), (57253, {'train/accuracy': 0.2811702787876129, 'train/loss': 3.8798627853393555, 'validation/accuracy': 0.2648199796676636, 'validation/loss': 4.011868476867676, 'validation/num_examples': 50000, 'test/accuracy': 0.20630000531673431, 'test/loss': 4.711694240570068, 'test/num_examples': 10000, 'score': 19413.61908721924, 'total_duration': 20117.109936237335, 'accumulated_submission_time': 19413.61908721924, 'accumulated_eval_time': 699.0399971008301, 'accumulated_logging_time': 2.3717331886291504, 'global_step': 57253, 'preemption_count': 0}), (58765, {'train/accuracy': 0.2714046537876129, 'train/loss': 3.7277042865753174, 'validation/accuracy': 0.2528599798679352, 'validation/loss': 3.881164312362671, 'validation/num_examples': 50000, 'test/accuracy': 0.18570001423358917, 'test/loss': 4.536026477813721, 'test/num_examples': 10000, 'score': 19923.753940820694, 'total_duration': 20645.222013950348, 'accumulated_submission_time': 19923.753940820694, 'accumulated_eval_time': 716.9220430850983, 'accumulated_logging_time': 2.4119575023651123, 'global_step': 58765, 'preemption_count': 0}), (60278, {'train/accuracy': 0.15573182702064514, 'train/loss': 5.1116814613342285, 'validation/accuracy': 0.1419599950313568, 'validation/loss': 5.236762523651123, 'validation/num_examples': 50000, 'test/accuracy': 0.09450000524520874, 'test/loss': 6.115200519561768, 'test/num_examples': 10000, 'score': 20433.7780482769, 'total_duration': 21172.926019191742, 'accumulated_submission_time': 20433.7780482769, 'accumulated_eval_time': 734.5041456222534, 'accumulated_logging_time': 2.4547340869903564, 'global_step': 60278, 'preemption_count': 0}), (61791, {'train/accuracy': 0.2585897445678711, 'train/loss': 3.929126262664795, 'validation/accuracy': 0.24305999279022217, 'validation/loss': 4.085925579071045, 'validation/num_examples': 50000, 'test/accuracy': 0.1754000037908554, 'test/loss': 4.864076614379883, 'test/num_examples': 10000, 'score': 20943.91853928566, 'total_duration': 21701.13439130783, 'accumulated_submission_time': 20943.91853928566, 'accumulated_eval_time': 752.4799780845642, 'accumulated_logging_time': 2.491457223892212, 'global_step': 61791, 'preemption_count': 0}), (63304, {'train/accuracy': 0.19387754797935486, 'train/loss': 4.898688316345215, 'validation/accuracy': 0.18491999804973602, 'validation/loss': 4.995222091674805, 'validation/num_examples': 50000, 'test/accuracy': 0.1388000100851059, 'test/loss': 5.745086193084717, 'test/num_examples': 10000, 'score': 21454.120364904404, 'total_duration': 22229.322627067566, 'accumulated_submission_time': 21454.120364904404, 'accumulated_eval_time': 770.3702204227448, 'accumulated_logging_time': 2.534074068069458, 'global_step': 63304, 'preemption_count': 0}), (64817, {'train/accuracy': 0.19919881224632263, 'train/loss': 4.335831165313721, 'validation/accuracy': 0.18441998958587646, 'validation/loss': 4.489870548248291, 'validation/num_examples': 50000, 'test/accuracy': 0.13370001316070557, 'test/loss': 5.076531410217285, 'test/num_examples': 10000, 'score': 21964.04625606537, 'total_duration': 22757.037092924118, 'accumulated_submission_time': 21964.04625606537, 'accumulated_eval_time': 788.0393702983856, 'accumulated_logging_time': 2.598047971725464, 'global_step': 64817, 'preemption_count': 0}), (66330, {'train/accuracy': 0.3051060140132904, 'train/loss': 3.4450433254241943, 'validation/accuracy': 0.2985999882221222, 'validation/loss': 3.5019185543060303, 'validation/num_examples': 50000, 'test/accuracy': 0.20680001378059387, 'test/loss': 4.329137802124023, 'test/num_examples': 10000, 'score': 22474.17966222763, 'total_duration': 23285.16569185257, 'accumulated_submission_time': 22474.17966222763, 'accumulated_eval_time': 805.9393737316132, 'accumulated_logging_time': 2.6385395526885986, 'global_step': 66330, 'preemption_count': 0}), (67842, {'train/accuracy': 0.3987165093421936, 'train/loss': 2.7879419326782227, 'validation/accuracy': 0.37814000248908997, 'validation/loss': 2.944031238555908, 'validation/num_examples': 50000, 'test/accuracy': 0.2841000258922577, 'test/loss': 3.7177770137786865, 'test/num_examples': 10000, 'score': 22984.28769302368, 'total_duration': 23813.329062461853, 'accumulated_submission_time': 22984.28769302368, 'accumulated_eval_time': 823.8984444141388, 'accumulated_logging_time': 2.679395914077759, 'global_step': 67842, 'preemption_count': 0}), (69355, {'train/accuracy': 0.24583466351032257, 'train/loss': 4.067709922790527, 'validation/accuracy': 0.22181999683380127, 'validation/loss': 4.309610843658447, 'validation/num_examples': 50000, 'test/accuracy': 0.1639000028371811, 'test/loss': 5.0794901847839355, 'test/num_examples': 10000, 'score': 23494.273504018784, 'total_duration': 24341.482144355774, 'accumulated_submission_time': 23494.273504018784, 'accumulated_eval_time': 841.9661107063293, 'accumulated_logging_time': 2.723883628845215, 'global_step': 69355, 'preemption_count': 0}), (70867, {'train/accuracy': 0.31736287474632263, 'train/loss': 3.2800393104553223, 'validation/accuracy': 0.2933799922466278, 'validation/loss': 3.4317257404327393, 'validation/num_examples': 50000, 'test/accuracy': 0.22180001437664032, 'test/loss': 4.1046953201293945, 'test/num_examples': 10000, 'score': 24004.327831745148, 'total_duration': 24869.49009847641, 'accumulated_submission_time': 24004.327831745148, 'accumulated_eval_time': 859.8252913951874, 'accumulated_logging_time': 2.7633161544799805, 'global_step': 70867, 'preemption_count': 0}), (72379, {'train/accuracy': 0.4204001724720001, 'train/loss': 2.6156625747680664, 'validation/accuracy': 0.3950199782848358, 'validation/loss': 2.788191795349121, 'validation/num_examples': 50000, 'test/accuracy': 0.2957000136375427, 'test/loss': 3.51944899559021, 'test/num_examples': 10000, 'score': 24514.24355506897, 'total_duration': 25397.27818083763, 'accumulated_submission_time': 24514.24355506897, 'accumulated_eval_time': 877.5954036712646, 'accumulated_logging_time': 2.8088531494140625, 'global_step': 72379, 'preemption_count': 0}), (73891, {'train/accuracy': 0.39176100492477417, 'train/loss': 2.7942934036254883, 'validation/accuracy': 0.3677600026130676, 'validation/loss': 2.944324016571045, 'validation/num_examples': 50000, 'test/accuracy': 0.27300000190734863, 'test/loss': 3.748530149459839, 'test/num_examples': 10000, 'score': 25024.23308992386, 'total_duration': 25925.08330845833, 'accumulated_submission_time': 25024.23308992386, 'accumulated_eval_time': 895.3173720836639, 'accumulated_logging_time': 2.8476545810699463, 'global_step': 73891, 'preemption_count': 0}), (75404, {'train/accuracy': 0.09494578838348389, 'train/loss': 6.09211540222168, 'validation/accuracy': 0.08621999621391296, 'validation/loss': 6.208457946777344, 'validation/num_examples': 50000, 'test/accuracy': 0.0658000037074089, 'test/loss': 6.601757526397705, 'test/num_examples': 10000, 'score': 25534.324366092682, 'total_duration': 26452.890166282654, 'accumulated_submission_time': 25534.324366092682, 'accumulated_eval_time': 912.9306666851044, 'accumulated_logging_time': 2.89243745803833, 'global_step': 75404, 'preemption_count': 0}), (76917, {'train/accuracy': 0.4280731678009033, 'train/loss': 2.589535713195801, 'validation/accuracy': 0.40323999524116516, 'validation/loss': 2.7519068717956543, 'validation/num_examples': 50000, 'test/accuracy': 0.3020000159740448, 'test/loss': 3.514342784881592, 'test/num_examples': 10000, 'score': 26044.48452091217, 'total_duration': 26981.044637680054, 'accumulated_submission_time': 26044.48452091217, 'accumulated_eval_time': 930.827305316925, 'accumulated_logging_time': 2.9355061054229736, 'global_step': 76917, 'preemption_count': 0}), (78430, {'train/accuracy': 0.3382294178009033, 'train/loss': 3.221769094467163, 'validation/accuracy': 0.3022799789905548, 'validation/loss': 3.51043963432312, 'validation/num_examples': 50000, 'test/accuracy': 0.23030000925064087, 'test/loss': 4.192265510559082, 'test/num_examples': 10000, 'score': 26554.670390605927, 'total_duration': 27509.320876836777, 'accumulated_submission_time': 26554.670390605927, 'accumulated_eval_time': 948.8191111087799, 'accumulated_logging_time': 2.9777910709381104, 'global_step': 78430, 'preemption_count': 0}), (79943, {'train/accuracy': 0.31275907158851624, 'train/loss': 3.484292507171631, 'validation/accuracy': 0.28571999073028564, 'validation/loss': 3.710571527481079, 'validation/num_examples': 50000, 'test/accuracy': 0.21790000796318054, 'test/loss': 4.380500316619873, 'test/num_examples': 10000, 'score': 27064.858870744705, 'total_duration': 28037.1936275959, 'accumulated_submission_time': 27064.858870744705, 'accumulated_eval_time': 966.4048013687134, 'accumulated_logging_time': 3.0196309089660645, 'global_step': 79943, 'preemption_count': 0}), (81457, {'train/accuracy': 0.09078045189380646, 'train/loss': 6.651303291320801, 'validation/accuracy': 0.08349999785423279, 'validation/loss': 6.782551288604736, 'validation/num_examples': 50000, 'test/accuracy': 0.05780000239610672, 'test/loss': 7.317497253417969, 'test/num_examples': 10000, 'score': 27575.083287000656, 'total_duration': 28565.20376110077, 'accumulated_submission_time': 27575.083287000656, 'accumulated_eval_time': 984.0845103263855, 'accumulated_logging_time': 3.068606376647949, 'global_step': 81457, 'preemption_count': 0}), (82971, {'train/accuracy': 0.32892218232154846, 'train/loss': 3.3418257236480713, 'validation/accuracy': 0.3080599904060364, 'validation/loss': 3.4806392192840576, 'validation/num_examples': 50000, 'test/accuracy': 0.23200000822544098, 'test/loss': 4.121487140655518, 'test/num_examples': 10000, 'score': 28085.3017513752, 'total_duration': 29093.237498044968, 'accumulated_submission_time': 28085.3017513752, 'accumulated_eval_time': 1001.7920260429382, 'accumulated_logging_time': 3.1208367347717285, 'global_step': 82971, 'preemption_count': 0}), (84484, {'train/accuracy': 0.39504942297935486, 'train/loss': 2.8487935066223145, 'validation/accuracy': 0.36965999007225037, 'validation/loss': 3.0001261234283447, 'validation/num_examples': 50000, 'test/accuracy': 0.26980000734329224, 'test/loss': 3.844083786010742, 'test/num_examples': 10000, 'score': 28595.369776010513, 'total_duration': 29621.18208193779, 'accumulated_submission_time': 28595.369776010513, 'accumulated_eval_time': 1019.5700325965881, 'accumulated_logging_time': 3.163076639175415, 'global_step': 84484, 'preemption_count': 0}), (85997, {'train/accuracy': 0.14728157222270966, 'train/loss': 5.7417826652526855, 'validation/accuracy': 0.13531999289989471, 'validation/loss': 5.94066047668457, 'validation/num_examples': 50000, 'test/accuracy': 0.101500004529953, 'test/loss': 6.495641708374023, 'test/num_examples': 10000, 'score': 29105.444316864014, 'total_duration': 30148.880268096924, 'accumulated_submission_time': 29105.444316864014, 'accumulated_eval_time': 1037.0942661762238, 'accumulated_logging_time': 3.2089014053344727, 'global_step': 85997, 'preemption_count': 0}), (87510, {'train/accuracy': 0.38759565353393555, 'train/loss': 2.9370288848876953, 'validation/accuracy': 0.35019999742507935, 'validation/loss': 3.214970588684082, 'validation/num_examples': 50000, 'test/accuracy': 0.26420000195503235, 'test/loss': 4.0185370445251465, 'test/num_examples': 10000, 'score': 29615.636449813843, 'total_duration': 30677.051599264145, 'accumulated_submission_time': 29615.636449813843, 'accumulated_eval_time': 1054.9764783382416, 'accumulated_logging_time': 3.251824378967285, 'global_step': 87510, 'preemption_count': 0}), (89023, {'train/accuracy': 0.40361925959587097, 'train/loss': 2.7422826290130615, 'validation/accuracy': 0.3774600028991699, 'validation/loss': 2.9350979328155518, 'validation/num_examples': 50000, 'test/accuracy': 0.2768000066280365, 'test/loss': 3.74910569190979, 'test/num_examples': 10000, 'score': 30125.565993785858, 'total_duration': 31204.683507680893, 'accumulated_submission_time': 30125.565993785858, 'accumulated_eval_time': 1072.5670273303986, 'accumulated_logging_time': 3.3075568675994873, 'global_step': 89023, 'preemption_count': 0}), (90537, {'train/accuracy': 0.2742745578289032, 'train/loss': 3.888530969619751, 'validation/accuracy': 0.26151999831199646, 'validation/loss': 4.029567241668701, 'validation/num_examples': 50000, 'test/accuracy': 0.18450000882148743, 'test/loss': 4.86121940612793, 'test/num_examples': 10000, 'score': 30635.79352426529, 'total_duration': 31732.65471124649, 'accumulated_submission_time': 30635.79352426529, 'accumulated_eval_time': 1090.2088098526, 'accumulated_logging_time': 3.3545897006988525, 'global_step': 90537, 'preemption_count': 0}), (92050, {'train/accuracy': 0.3800821006298065, 'train/loss': 2.910261869430542, 'validation/accuracy': 0.3625999987125397, 'validation/loss': 3.03848934173584, 'validation/num_examples': 50000, 'test/accuracy': 0.25760000944137573, 'test/loss': 3.8726961612701416, 'test/num_examples': 10000, 'score': 31145.958006620407, 'total_duration': 32261.35049700737, 'accumulated_submission_time': 31145.958006620407, 'accumulated_eval_time': 1108.64000082016, 'accumulated_logging_time': 3.3997135162353516, 'global_step': 92050, 'preemption_count': 0}), (93560, {'train/accuracy': 0.43335458636283875, 'train/loss': 2.672496795654297, 'validation/accuracy': 0.4023999869823456, 'validation/loss': 2.8671212196350098, 'validation/num_examples': 50000, 'test/accuracy': 0.3085000216960907, 'test/loss': 3.6801254749298096, 'test/num_examples': 10000, 'score': 31654.82076215744, 'total_duration': 32789.16884326935, 'accumulated_submission_time': 31654.82076215744, 'accumulated_eval_time': 1126.3750030994415, 'accumulated_logging_time': 4.565646648406982, 'global_step': 93560, 'preemption_count': 0}), (95073, {'train/accuracy': 0.31594786047935486, 'train/loss': 3.5270910263061523, 'validation/accuracy': 0.2946999967098236, 'validation/loss': 3.6912097930908203, 'validation/num_examples': 50000, 'test/accuracy': 0.21230001747608185, 'test/loss': 4.5419158935546875, 'test/num_examples': 10000, 'score': 32165.047719717026, 'total_duration': 33317.30035114288, 'accumulated_submission_time': 32165.047719717026, 'accumulated_eval_time': 1144.1742820739746, 'accumulated_logging_time': 4.614187479019165, 'global_step': 95073, 'preemption_count': 0}), (96586, {'train/accuracy': 0.37107381224632263, 'train/loss': 2.9733757972717285, 'validation/accuracy': 0.33789998292922974, 'validation/loss': 3.213762044906616, 'validation/num_examples': 50000, 'test/accuracy': 0.25, 'test/loss': 4.006485462188721, 'test/num_examples': 10000, 'score': 32675.180696725845, 'total_duration': 33845.43358707428, 'accumulated_submission_time': 32675.180696725845, 'accumulated_eval_time': 1162.0735852718353, 'accumulated_logging_time': 4.659753799438477, 'global_step': 96586, 'preemption_count': 0}), (98099, {'train/accuracy': 0.4164939224720001, 'train/loss': 2.686826467514038, 'validation/accuracy': 0.3877999782562256, 'validation/loss': 2.8848915100097656, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.734477996826172, 'test/num_examples': 10000, 'score': 33185.274107694626, 'total_duration': 34373.27248048782, 'accumulated_submission_time': 33185.274107694626, 'accumulated_eval_time': 1179.7177047729492, 'accumulated_logging_time': 4.704460382461548, 'global_step': 98099, 'preemption_count': 0}), (99612, {'train/accuracy': 0.5056201815605164, 'train/loss': 2.150707960128784, 'validation/accuracy': 0.4710799753665924, 'validation/loss': 2.3676950931549072, 'validation/num_examples': 50000, 'test/accuracy': 0.3606000244617462, 'test/loss': 3.1262447834014893, 'test/num_examples': 10000, 'score': 33695.28577399254, 'total_duration': 34901.336842536926, 'accumulated_submission_time': 33695.28577399254, 'accumulated_eval_time': 1197.6669921875, 'accumulated_logging_time': 4.754195690155029, 'global_step': 99612, 'preemption_count': 0}), (101124, {'train/accuracy': 0.44365832209587097, 'train/loss': 2.589078903198242, 'validation/accuracy': 0.41331997513771057, 'validation/loss': 2.795851230621338, 'validation/num_examples': 50000, 'test/accuracy': 0.313400000333786, 'test/loss': 3.591003179550171, 'test/num_examples': 10000, 'score': 34205.252484321594, 'total_duration': 35429.5738132, 'accumulated_submission_time': 34205.252484321594, 'accumulated_eval_time': 1215.8287003040314, 'accumulated_logging_time': 4.804080247879028, 'global_step': 101124, 'preemption_count': 0}), (102637, {'train/accuracy': 0.5242147445678711, 'train/loss': 2.068694829940796, 'validation/accuracy': 0.49167999625205994, 'validation/loss': 2.279438018798828, 'validation/num_examples': 50000, 'test/accuracy': 0.3857000172138214, 'test/loss': 3.0341577529907227, 'test/num_examples': 10000, 'score': 34715.23096227646, 'total_duration': 35957.45670199394, 'accumulated_submission_time': 34715.23096227646, 'accumulated_eval_time': 1233.6238696575165, 'accumulated_logging_time': 4.856037855148315, 'global_step': 102637, 'preemption_count': 0}), (104150, {'train/accuracy': 0.44082826375961304, 'train/loss': 2.609811305999756, 'validation/accuracy': 0.4175199866294861, 'validation/loss': 2.7619614601135254, 'validation/num_examples': 50000, 'test/accuracy': 0.31770002841949463, 'test/loss': 3.5497262477874756, 'test/num_examples': 10000, 'score': 35225.40841984749, 'total_duration': 36485.60284900665, 'accumulated_submission_time': 35225.40841984749, 'accumulated_eval_time': 1251.489592075348, 'accumulated_logging_time': 4.904949903488159, 'global_step': 104150, 'preemption_count': 0}), (105662, {'train/accuracy': 0.27955594658851624, 'train/loss': 3.90887188911438, 'validation/accuracy': 0.25769999623298645, 'validation/loss': 4.139658451080322, 'validation/num_examples': 50000, 'test/accuracy': 0.19850000739097595, 'test/loss': 4.91162109375, 'test/num_examples': 10000, 'score': 35735.49154949188, 'total_duration': 37013.47520160675, 'accumulated_submission_time': 35735.49154949188, 'accumulated_eval_time': 1269.1765999794006, 'accumulated_logging_time': 4.950104713439941, 'global_step': 105662, 'preemption_count': 0}), (107174, {'train/accuracy': 0.500019907951355, 'train/loss': 2.244053363800049, 'validation/accuracy': 0.46041998267173767, 'validation/loss': 2.513838768005371, 'validation/num_examples': 50000, 'test/accuracy': 0.35680001974105835, 'test/loss': 3.284795045852661, 'test/num_examples': 10000, 'score': 36245.43359160423, 'total_duration': 37541.301339387894, 'accumulated_submission_time': 36245.43359160423, 'accumulated_eval_time': 1286.9605538845062, 'accumulated_logging_time': 4.994918346405029, 'global_step': 107174, 'preemption_count': 0}), (108686, {'train/accuracy': 0.4937818646430969, 'train/loss': 2.252739191055298, 'validation/accuracy': 0.4607200026512146, 'validation/loss': 2.472726345062256, 'validation/num_examples': 50000, 'test/accuracy': 0.35510000586509705, 'test/loss': 3.2092649936676025, 'test/num_examples': 10000, 'score': 36755.34883475304, 'total_duration': 38068.86385965347, 'accumulated_submission_time': 36755.34883475304, 'accumulated_eval_time': 1304.5049712657928, 'accumulated_logging_time': 5.042736291885376, 'global_step': 108686, 'preemption_count': 0}), (110199, {'train/accuracy': 0.5153260231018066, 'train/loss': 2.0981321334838867, 'validation/accuracy': 0.4820399880409241, 'validation/loss': 2.304888963699341, 'validation/num_examples': 50000, 'test/accuracy': 0.3678000271320343, 'test/loss': 3.0940093994140625, 'test/num_examples': 10000, 'score': 37265.49964928627, 'total_duration': 38596.919221162796, 'accumulated_submission_time': 37265.49964928627, 'accumulated_eval_time': 1322.303986787796, 'accumulated_logging_time': 5.093266010284424, 'global_step': 110199, 'preemption_count': 0}), (111712, {'train/accuracy': 0.5056201815605164, 'train/loss': 2.1512796878814697, 'validation/accuracy': 0.47079998254776, 'validation/loss': 2.3727433681488037, 'validation/num_examples': 50000, 'test/accuracy': 0.35680001974105835, 'test/loss': 3.1949143409729004, 'test/num_examples': 10000, 'score': 37775.48208498955, 'total_duration': 39124.7622282505, 'accumulated_submission_time': 37775.48208498955, 'accumulated_eval_time': 1340.0610992908478, 'accumulated_logging_time': 5.142434120178223, 'global_step': 111712, 'preemption_count': 0}), (113225, {'train/accuracy': 0.5051419138908386, 'train/loss': 2.175563335418701, 'validation/accuracy': 0.4692799746990204, 'validation/loss': 2.409187078475952, 'validation/num_examples': 50000, 'test/accuracy': 0.3579000234603882, 'test/loss': 3.2229323387145996, 'test/num_examples': 10000, 'score': 38285.63218688965, 'total_duration': 39652.69889450073, 'accumulated_submission_time': 38285.63218688965, 'accumulated_eval_time': 1357.7409224510193, 'accumulated_logging_time': 5.193975210189819, 'global_step': 113225, 'preemption_count': 0}), (114739, {'train/accuracy': 0.5345184803009033, 'train/loss': 2.0138721466064453, 'validation/accuracy': 0.4868199825286865, 'validation/loss': 2.304708957672119, 'validation/num_examples': 50000, 'test/accuracy': 0.3734000325202942, 'test/loss': 3.111175060272217, 'test/num_examples': 10000, 'score': 38795.65815782547, 'total_duration': 40181.27318763733, 'accumulated_submission_time': 38795.65815782547, 'accumulated_eval_time': 1376.1895372867584, 'accumulated_logging_time': 5.24059534072876, 'global_step': 114739, 'preemption_count': 0}), (116252, {'train/accuracy': 0.6103116869926453, 'train/loss': 1.5996730327606201, 'validation/accuracy': 0.5604000091552734, 'validation/loss': 1.880994200706482, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.634089708328247, 'test/num_examples': 10000, 'score': 39305.71879410744, 'total_duration': 40709.015604019165, 'accumulated_submission_time': 39305.71879410744, 'accumulated_eval_time': 1393.7754967212677, 'accumulated_logging_time': 5.281083822250366, 'global_step': 116252, 'preemption_count': 0}), (117765, {'train/accuracy': 0.5663065910339355, 'train/loss': 1.824598789215088, 'validation/accuracy': 0.5275200009346008, 'validation/loss': 2.038508892059326, 'validation/num_examples': 50000, 'test/accuracy': 0.4052000045776367, 'test/loss': 2.835646152496338, 'test/num_examples': 10000, 'score': 39815.936584711075, 'total_duration': 41236.85900259018, 'accumulated_submission_time': 39815.936584711075, 'accumulated_eval_time': 1411.296749830246, 'accumulated_logging_time': 5.329135894775391, 'global_step': 117765, 'preemption_count': 0}), (119278, {'train/accuracy': 0.29412466287612915, 'train/loss': 3.7996959686279297, 'validation/accuracy': 0.28205999732017517, 'validation/loss': 3.9288508892059326, 'validation/num_examples': 50000, 'test/accuracy': 0.20280000567436218, 'test/loss': 4.84499979019165, 'test/num_examples': 10000, 'score': 40326.08441853523, 'total_duration': 41764.800423145294, 'accumulated_submission_time': 40326.08441853523, 'accumulated_eval_time': 1428.9782931804657, 'accumulated_logging_time': 5.38359808921814, 'global_step': 119278, 'preemption_count': 0}), (120791, {'train/accuracy': 0.5305325388908386, 'train/loss': 2.0072436332702637, 'validation/accuracy': 0.4994799792766571, 'validation/loss': 2.1964006423950195, 'validation/num_examples': 50000, 'test/accuracy': 0.3850000202655792, 'test/loss': 3.043755292892456, 'test/num_examples': 10000, 'score': 40836.241518974304, 'total_duration': 42292.74313831329, 'accumulated_submission_time': 40836.241518974304, 'accumulated_eval_time': 1446.6527774333954, 'accumulated_logging_time': 5.439541339874268, 'global_step': 120791, 'preemption_count': 0}), (122303, {'train/accuracy': 0.5705317258834839, 'train/loss': 1.8204600811004639, 'validation/accuracy': 0.5156199932098389, 'validation/loss': 2.1237943172454834, 'validation/num_examples': 50000, 'test/accuracy': 0.3849000036716461, 'test/loss': 3.013498067855835, 'test/num_examples': 10000, 'score': 41346.374436855316, 'total_duration': 42821.643122434616, 'accumulated_submission_time': 41346.374436855316, 'accumulated_eval_time': 1465.3159244060516, 'accumulated_logging_time': 5.488732576370239, 'global_step': 122303, 'preemption_count': 0}), (123817, {'train/accuracy': 0.6350645422935486, 'train/loss': 1.4768481254577637, 'validation/accuracy': 0.5727599859237671, 'validation/loss': 1.8264350891113281, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.5984723567962646, 'test/num_examples': 10000, 'score': 41856.57735204697, 'total_duration': 43349.79244160652, 'accumulated_submission_time': 41856.57735204697, 'accumulated_eval_time': 1483.166562795639, 'accumulated_logging_time': 5.5297324657440186, 'global_step': 123817, 'preemption_count': 0}), (125330, {'train/accuracy': 0.5938695669174194, 'train/loss': 1.693787932395935, 'validation/accuracy': 0.5395399928092957, 'validation/loss': 2.0015382766723633, 'validation/num_examples': 50000, 'test/accuracy': 0.42020002007484436, 'test/loss': 2.7875430583953857, 'test/num_examples': 10000, 'score': 42366.60649180412, 'total_duration': 43877.573405981064, 'accumulated_submission_time': 42366.60649180412, 'accumulated_eval_time': 1500.7979154586792, 'accumulated_logging_time': 5.595268964767456, 'global_step': 125330, 'preemption_count': 0}), (126843, {'train/accuracy': 0.5989516973495483, 'train/loss': 1.6441659927368164, 'validation/accuracy': 0.5536400079727173, 'validation/loss': 1.9106814861297607, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.6522583961486816, 'test/num_examples': 10000, 'score': 42876.69231629372, 'total_duration': 44405.286915779114, 'accumulated_submission_time': 42876.69231629372, 'accumulated_eval_time': 1518.3210427761078, 'accumulated_logging_time': 5.644772291183472, 'global_step': 126843, 'preemption_count': 0}), (128356, {'train/accuracy': 0.6427175998687744, 'train/loss': 1.4331533908843994, 'validation/accuracy': 0.5913599729537964, 'validation/loss': 1.713654637336731, 'validation/num_examples': 50000, 'test/accuracy': 0.4610000252723694, 'test/loss': 2.505321979522705, 'test/num_examples': 10000, 'score': 43386.7288172245, 'total_duration': 44933.01505827904, 'accumulated_submission_time': 43386.7288172245, 'accumulated_eval_time': 1535.9049890041351, 'accumulated_logging_time': 5.6943440437316895, 'global_step': 128356, 'preemption_count': 0}), (129869, {'train/accuracy': 0.5605269074440002, 'train/loss': 1.8894774913787842, 'validation/accuracy': 0.5187999606132507, 'validation/loss': 2.140009880065918, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.9258809089660645, 'test/num_examples': 10000, 'score': 43896.84852051735, 'total_duration': 45461.23577904701, 'accumulated_submission_time': 43896.84852051735, 'accumulated_eval_time': 1553.898535490036, 'accumulated_logging_time': 5.7474260330200195, 'global_step': 129869, 'preemption_count': 0}), (131383, {'train/accuracy': 0.6741071343421936, 'train/loss': 1.2939298152923584, 'validation/accuracy': 0.5915399789810181, 'validation/loss': 1.7248343229293823, 'validation/num_examples': 50000, 'test/accuracy': 0.4692000150680542, 'test/loss': 2.4976091384887695, 'test/num_examples': 10000, 'score': 44406.79634475708, 'total_duration': 45989.0142326355, 'accumulated_submission_time': 44406.79634475708, 'accumulated_eval_time': 1571.618933916092, 'accumulated_logging_time': 5.801463842391968, 'global_step': 131383, 'preemption_count': 0}), (132895, {'train/accuracy': 0.6228475570678711, 'train/loss': 1.5460199117660522, 'validation/accuracy': 0.5662800073623657, 'validation/loss': 1.8539618253707886, 'validation/num_examples': 50000, 'test/accuracy': 0.444100022315979, 'test/loss': 2.6210970878601074, 'test/num_examples': 10000, 'score': 44916.785944223404, 'total_duration': 46516.66002821922, 'accumulated_submission_time': 44916.785944223404, 'accumulated_eval_time': 1589.165275335312, 'accumulated_logging_time': 5.855878114700317, 'global_step': 132895, 'preemption_count': 0}), (134408, {'train/accuracy': 0.6938576102256775, 'train/loss': 1.2104731798171997, 'validation/accuracy': 0.6318199634552002, 'validation/loss': 1.532360315322876, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2486801147460938, 'test/num_examples': 10000, 'score': 45426.89608311653, 'total_duration': 47044.71316599846, 'accumulated_submission_time': 45426.89608311653, 'accumulated_eval_time': 1607.0003921985626, 'accumulated_logging_time': 5.908795595169067, 'global_step': 134408, 'preemption_count': 0}), (135922, {'train/accuracy': 0.6696428656578064, 'train/loss': 1.3166732788085938, 'validation/accuracy': 0.6161999702453613, 'validation/loss': 1.6147363185882568, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.346391439437866, 'test/num_examples': 10000, 'score': 45937.009231090546, 'total_duration': 47572.57046985626, 'accumulated_submission_time': 45937.009231090546, 'accumulated_eval_time': 1624.6373193264008, 'accumulated_logging_time': 5.960654020309448, 'global_step': 135922, 'preemption_count': 0}), (137436, {'train/accuracy': 0.6374163031578064, 'train/loss': 1.4651330709457397, 'validation/accuracy': 0.5859400033950806, 'validation/loss': 1.7452064752578735, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.4913787841796875, 'test/num_examples': 10000, 'score': 46447.159615039825, 'total_duration': 48101.16772198677, 'accumulated_submission_time': 46447.159615039825, 'accumulated_eval_time': 1642.9792296886444, 'accumulated_logging_time': 6.0111939907073975, 'global_step': 137436, 'preemption_count': 0}), (138949, {'train/accuracy': 0.6802853941917419, 'train/loss': 1.2585314512252808, 'validation/accuracy': 0.6293399930000305, 'validation/loss': 1.5424376726150513, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.300657033920288, 'test/num_examples': 10000, 'score': 46957.08819317818, 'total_duration': 48629.026055812836, 'accumulated_submission_time': 46957.08819317818, 'accumulated_eval_time': 1660.8099303245544, 'accumulated_logging_time': 6.053507328033447, 'global_step': 138949, 'preemption_count': 0}), (140462, {'train/accuracy': 0.7210419178009033, 'train/loss': 1.0726977586746216, 'validation/accuracy': 0.6291199922561646, 'validation/loss': 1.5267126560211182, 'validation/num_examples': 50000, 'test/accuracy': 0.5039000511169434, 'test/loss': 2.279113292694092, 'test/num_examples': 10000, 'score': 47467.14528131485, 'total_duration': 49157.062989234924, 'accumulated_submission_time': 47467.14528131485, 'accumulated_eval_time': 1678.656478881836, 'accumulated_logging_time': 6.13153076171875, 'global_step': 140462, 'preemption_count': 0}), (141976, {'train/accuracy': 0.7115154266357422, 'train/loss': 1.1081956624984741, 'validation/accuracy': 0.6423599720001221, 'validation/loss': 1.4797382354736328, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.2130398750305176, 'test/num_examples': 10000, 'score': 47977.29451775551, 'total_duration': 49685.04183840752, 'accumulated_submission_time': 47977.29451775551, 'accumulated_eval_time': 1696.3789055347443, 'accumulated_logging_time': 6.181941986083984, 'global_step': 141976, 'preemption_count': 0}), (143487, {'train/accuracy': 0.7179726958274841, 'train/loss': 1.1000560522079468, 'validation/accuracy': 0.6461799740791321, 'validation/loss': 1.4582358598709106, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.1681156158447266, 'test/num_examples': 10000, 'score': 48487.306156396866, 'total_duration': 50212.84715676308, 'accumulated_submission_time': 48487.306156396866, 'accumulated_eval_time': 1714.0664644241333, 'accumulated_logging_time': 6.234250068664551, 'global_step': 143487, 'preemption_count': 0}), (145001, {'train/accuracy': 0.7001753449440002, 'train/loss': 1.1895331144332886, 'validation/accuracy': 0.639519989490509, 'validation/loss': 1.4978569746017456, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.2445452213287354, 'test/num_examples': 10000, 'score': 48997.7052898407, 'total_duration': 50741.36831307411, 'accumulated_submission_time': 48997.7052898407, 'accumulated_eval_time': 1732.0806908607483, 'accumulated_logging_time': 6.286098003387451, 'global_step': 145001, 'preemption_count': 0}), (146513, {'train/accuracy': 0.6998565196990967, 'train/loss': 1.1637591123580933, 'validation/accuracy': 0.6406199932098389, 'validation/loss': 1.486459493637085, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.2266359329223633, 'test/num_examples': 10000, 'score': 49507.652215242386, 'total_duration': 51269.190279722214, 'accumulated_submission_time': 49507.652215242386, 'accumulated_eval_time': 1749.8422105312347, 'accumulated_logging_time': 6.3434507846832275, 'global_step': 146513, 'preemption_count': 0}), (148026, {'train/accuracy': 0.7155213356018066, 'train/loss': 1.1146916151046753, 'validation/accuracy': 0.6486999988555908, 'validation/loss': 1.4627606868743896, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.186429262161255, 'test/num_examples': 10000, 'score': 50017.61883044243, 'total_duration': 51797.05554533005, 'accumulated_submission_time': 50017.61883044243, 'accumulated_eval_time': 1767.6306097507477, 'accumulated_logging_time': 6.3975958824157715, 'global_step': 148026, 'preemption_count': 0}), (149538, {'train/accuracy': 0.7698501348495483, 'train/loss': 0.8804860711097717, 'validation/accuracy': 0.6730799674987793, 'validation/loss': 1.3303167819976807, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.0321311950683594, 'test/num_examples': 10000, 'score': 50527.641800403595, 'total_duration': 52325.024513721466, 'accumulated_submission_time': 50527.641800403595, 'accumulated_eval_time': 1785.4646661281586, 'accumulated_logging_time': 6.454508543014526, 'global_step': 149538, 'preemption_count': 0}), (151051, {'train/accuracy': 0.7565170526504517, 'train/loss': 0.9239798188209534, 'validation/accuracy': 0.6729199886322021, 'validation/loss': 1.331039309501648, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.099137544631958, 'test/num_examples': 10000, 'score': 51037.73308491707, 'total_duration': 52852.88480973244, 'accumulated_submission_time': 51037.73308491707, 'accumulated_eval_time': 1803.1257722377777, 'accumulated_logging_time': 6.508534669876099, 'global_step': 151051, 'preemption_count': 0}), (152564, {'train/accuracy': 0.7428451776504517, 'train/loss': 0.9893755316734314, 'validation/accuracy': 0.6662999987602234, 'validation/loss': 1.3725179433822632, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.163541555404663, 'test/num_examples': 10000, 'score': 51547.81756424904, 'total_duration': 53381.75227236748, 'accumulated_submission_time': 51547.81756424904, 'accumulated_eval_time': 1821.8002681732178, 'accumulated_logging_time': 6.5628204345703125, 'global_step': 152564, 'preemption_count': 0}), (154077, {'train/accuracy': 0.7563576102256775, 'train/loss': 0.9220529198646545, 'validation/accuracy': 0.6801199913024902, 'validation/loss': 1.3138034343719482, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.063292980194092, 'test/num_examples': 10000, 'score': 52057.731224775314, 'total_duration': 53909.18029880524, 'accumulated_submission_time': 52057.731224775314, 'accumulated_eval_time': 1839.2094180583954, 'accumulated_logging_time': 6.6128644943237305, 'global_step': 154077, 'preemption_count': 0}), (155590, {'train/accuracy': 0.7622169852256775, 'train/loss': 0.8947357535362244, 'validation/accuracy': 0.6869399547576904, 'validation/loss': 1.2795292139053345, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.034101963043213, 'test/num_examples': 10000, 'score': 52567.773965358734, 'total_duration': 54437.1859266758, 'accumulated_submission_time': 52567.773965358734, 'accumulated_eval_time': 1857.059386253357, 'accumulated_logging_time': 6.671154737472534, 'global_step': 155590, 'preemption_count': 0}), (157103, {'train/accuracy': 0.7747727632522583, 'train/loss': 0.8410025238990784, 'validation/accuracy': 0.6900999546051025, 'validation/loss': 1.2684522867202759, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.017871379852295, 'test/num_examples': 10000, 'score': 53077.835666418076, 'total_duration': 54964.90230464935, 'accumulated_submission_time': 53077.835666418076, 'accumulated_eval_time': 1874.6021373271942, 'accumulated_logging_time': 6.727156400680542, 'global_step': 157103, 'preemption_count': 0}), (158616, {'train/accuracy': 0.8008609414100647, 'train/loss': 0.7219278812408447, 'validation/accuracy': 0.6961399912834167, 'validation/loss': 1.2478055953979492, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 1.9279481172561646, 'test/num_examples': 10000, 'score': 53588.04853415489, 'total_duration': 55492.84319996834, 'accumulated_submission_time': 53588.04853415489, 'accumulated_eval_time': 1892.2187869548798, 'accumulated_logging_time': 6.782275915145874, 'global_step': 158616, 'preemption_count': 0}), (160130, {'train/accuracy': 0.8089724183082581, 'train/loss': 0.7101982831954956, 'validation/accuracy': 0.7047399878501892, 'validation/loss': 1.194737195968628, 'validation/num_examples': 50000, 'test/accuracy': 0.5812000036239624, 'test/loss': 1.8732895851135254, 'test/num_examples': 10000, 'score': 54098.267798900604, 'total_duration': 56021.37393474579, 'accumulated_submission_time': 54098.267798900604, 'accumulated_eval_time': 1910.420448064804, 'accumulated_logging_time': 6.837030410766602, 'global_step': 160130, 'preemption_count': 0}), (161643, {'train/accuracy': 0.7973134517669678, 'train/loss': 0.7421764135360718, 'validation/accuracy': 0.702739953994751, 'validation/loss': 1.2192460298538208, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 1.9370954036712646, 'test/num_examples': 10000, 'score': 54608.44234919548, 'total_duration': 56549.32446479797, 'accumulated_submission_time': 54608.44234919548, 'accumulated_eval_time': 1928.0951828956604, 'accumulated_logging_time': 6.881301641464233, 'global_step': 161643, 'preemption_count': 0}), (163156, {'train/accuracy': 0.8103076815605164, 'train/loss': 0.7060856819152832, 'validation/accuracy': 0.7111200094223022, 'validation/loss': 1.171807885169983, 'validation/num_examples': 50000, 'test/accuracy': 0.5887000560760498, 'test/loss': 1.8791732788085938, 'test/num_examples': 10000, 'score': 55118.50452852249, 'total_duration': 57077.438390254974, 'accumulated_submission_time': 55118.50452852249, 'accumulated_eval_time': 1946.0246217250824, 'accumulated_logging_time': 6.949363708496094, 'global_step': 163156, 'preemption_count': 0}), (164668, {'train/accuracy': 0.8194754123687744, 'train/loss': 0.6554054021835327, 'validation/accuracy': 0.7178800106048584, 'validation/loss': 1.144294261932373, 'validation/num_examples': 50000, 'test/accuracy': 0.5937000513076782, 'test/loss': 1.8554983139038086, 'test/num_examples': 10000, 'score': 55628.453688144684, 'total_duration': 57605.29715466499, 'accumulated_submission_time': 55628.453688144684, 'accumulated_eval_time': 1963.8206391334534, 'accumulated_logging_time': 7.007922172546387, 'global_step': 164668, 'preemption_count': 0}), (166181, {'train/accuracy': 0.8342036008834839, 'train/loss': 0.6015270948410034, 'validation/accuracy': 0.727840006351471, 'validation/loss': 1.113991379737854, 'validation/num_examples': 50000, 'test/accuracy': 0.6082000136375427, 'test/loss': 1.8160563707351685, 'test/num_examples': 10000, 'score': 56138.544397592545, 'total_duration': 58133.3680229187, 'accumulated_submission_time': 56138.544397592545, 'accumulated_eval_time': 1981.6831741333008, 'accumulated_logging_time': 7.071213245391846, 'global_step': 166181, 'preemption_count': 0}), (167694, {'train/accuracy': 0.8501076102256775, 'train/loss': 0.546515941619873, 'validation/accuracy': 0.7238799929618835, 'validation/loss': 1.1284018754959106, 'validation/num_examples': 50000, 'test/accuracy': 0.5982000231742859, 'test/loss': 1.8255773782730103, 'test/num_examples': 10000, 'score': 56648.53292417526, 'total_duration': 58662.07354712486, 'accumulated_submission_time': 56648.53292417526, 'accumulated_eval_time': 2000.2689554691315, 'accumulated_logging_time': 7.146226406097412, 'global_step': 167694, 'preemption_count': 0}), (169207, {'train/accuracy': 0.8522600531578064, 'train/loss': 0.5386980772018433, 'validation/accuracy': 0.7333599925041199, 'validation/loss': 1.0911046266555786, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7918938398361206, 'test/num_examples': 10000, 'score': 57158.69640493393, 'total_duration': 59190.97307920456, 'accumulated_submission_time': 57158.69640493393, 'accumulated_eval_time': 2018.9037404060364, 'accumulated_logging_time': 7.192639112472534, 'global_step': 169207, 'preemption_count': 0}), (170720, {'train/accuracy': 0.8514229655265808, 'train/loss': 0.5230114459991455, 'validation/accuracy': 0.7345799803733826, 'validation/loss': 1.079545021057129, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.7668043375015259, 'test/num_examples': 10000, 'score': 57668.74922180176, 'total_duration': 59719.014672756195, 'accumulated_submission_time': 57668.74922180176, 'accumulated_eval_time': 2036.7781956195831, 'accumulated_logging_time': 7.252174377441406, 'global_step': 170720, 'preemption_count': 0}), (172233, {'train/accuracy': 0.8605110049247742, 'train/loss': 0.5039197206497192, 'validation/accuracy': 0.7382400035858154, 'validation/loss': 1.062277913093567, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.7463383674621582, 'test/num_examples': 10000, 'score': 58178.75817775726, 'total_duration': 60246.973351955414, 'accumulated_submission_time': 58178.75817775726, 'accumulated_eval_time': 2054.613529920578, 'accumulated_logging_time': 7.311715364456177, 'global_step': 172233, 'preemption_count': 0}), (173746, {'train/accuracy': 0.8649752736091614, 'train/loss': 0.48539647459983826, 'validation/accuracy': 0.7422399520874023, 'validation/loss': 1.0512233972549438, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.7367162704467773, 'test/num_examples': 10000, 'score': 58688.94418215752, 'total_duration': 60775.03083705902, 'accumulated_submission_time': 58688.94418215752, 'accumulated_eval_time': 2072.3738420009613, 'accumulated_logging_time': 7.36877703666687, 'global_step': 173746, 'preemption_count': 0}), (175260, {'train/accuracy': 0.8682437539100647, 'train/loss': 0.46756279468536377, 'validation/accuracy': 0.7414199709892273, 'validation/loss': 1.049114465713501, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.7439327239990234, 'test/num_examples': 10000, 'score': 59199.13371539116, 'total_duration': 61303.094465732574, 'accumulated_submission_time': 59199.13371539116, 'accumulated_eval_time': 2090.135448217392, 'accumulated_logging_time': 7.425171852111816, 'global_step': 175260, 'preemption_count': 0}), (176772, {'train/accuracy': 0.8796038031578064, 'train/loss': 0.43470466136932373, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.036797285079956, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.7231155633926392, 'test/num_examples': 10000, 'score': 59709.20754027367, 'total_duration': 61831.22635412216, 'accumulated_submission_time': 59709.20754027367, 'accumulated_eval_time': 2108.078211784363, 'accumulated_logging_time': 7.485778331756592, 'global_step': 176772, 'preemption_count': 0}), (178285, {'train/accuracy': 0.8785474896430969, 'train/loss': 0.42888569831848145, 'validation/accuracy': 0.746399998664856, 'validation/loss': 1.0343586206436157, 'validation/num_examples': 50000, 'test/accuracy': 0.6279000043869019, 'test/loss': 1.7289866209030151, 'test/num_examples': 10000, 'score': 60219.31211185455, 'total_duration': 62359.2754983902, 'accumulated_submission_time': 60219.31211185455, 'accumulated_eval_time': 2125.9084413051605, 'accumulated_logging_time': 7.544202566146851, 'global_step': 178285, 'preemption_count': 0}), (179797, {'train/accuracy': 0.8817960619926453, 'train/loss': 0.4216182827949524, 'validation/accuracy': 0.7474600076675415, 'validation/loss': 1.027718424797058, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.7195037603378296, 'test/num_examples': 10000, 'score': 60729.40054988861, 'total_duration': 62887.25140142441, 'accumulated_submission_time': 60729.40054988861, 'accumulated_eval_time': 2143.6790177822113, 'accumulated_logging_time': 7.6068150997161865, 'global_step': 179797, 'preemption_count': 0}), (181309, {'train/accuracy': 0.8843869566917419, 'train/loss': 0.4094845950603485, 'validation/accuracy': 0.7476199865341187, 'validation/loss': 1.02272629737854, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.7120555639266968, 'test/num_examples': 10000, 'score': 61239.36246538162, 'total_duration': 63415.05498671532, 'accumulated_submission_time': 61239.36246538162, 'accumulated_eval_time': 2161.401031255722, 'accumulated_logging_time': 7.670835256576538, 'global_step': 181309, 'preemption_count': 0}), (182821, {'train/accuracy': 0.8831712007522583, 'train/loss': 0.4109700918197632, 'validation/accuracy': 0.7479400038719177, 'validation/loss': 1.0229642391204834, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.714728593826294, 'test/num_examples': 10000, 'score': 61749.32241177559, 'total_duration': 63942.94005918503, 'accumulated_submission_time': 61749.32241177559, 'accumulated_eval_time': 2179.2128698825836, 'accumulated_logging_time': 7.729203224182129, 'global_step': 182821, 'preemption_count': 0}), (184333, {'train/accuracy': 0.8838687539100647, 'train/loss': 0.41283562779426575, 'validation/accuracy': 0.7488600015640259, 'validation/loss': 1.020200252532959, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.7110487222671509, 'test/num_examples': 10000, 'score': 62259.33857059479, 'total_duration': 64470.75457596779, 'accumulated_submission_time': 62259.33857059479, 'accumulated_eval_time': 2196.8955330848694, 'accumulated_logging_time': 7.789600133895874, 'global_step': 184333, 'preemption_count': 0}), (185845, {'train/accuracy': 0.8853037357330322, 'train/loss': 0.4091644287109375, 'validation/accuracy': 0.7495599985122681, 'validation/loss': 1.0208008289337158, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.7125041484832764, 'test/num_examples': 10000, 'score': 62769.32061004639, 'total_duration': 64998.45347905159, 'accumulated_submission_time': 62769.32061004639, 'accumulated_eval_time': 2214.4908051490784, 'accumulated_logging_time': 7.853912591934204, 'global_step': 185845, 'preemption_count': 0})], 'global_step': 186554}
I0129 19:34:51.939229 140187804313408 submission_runner.py:586] Timing: 63008.32200908661
I0129 19:34:51.939313 140187804313408 submission_runner.py:588] Total number of evals: 124
I0129 19:34:51.939373 140187804313408 submission_runner.py:589] ====================
I0129 19:34:51.939417 140187804313408 submission_runner.py:542] Using RNG seed 3827130657
I0129 19:34:51.940941 140187804313408 submission_runner.py:551] --- Tuning run 5/5 ---
I0129 19:34:51.941060 140187804313408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_5.
I0129 19:34:51.942520 140187804313408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_5/hparams.json.
I0129 19:34:51.943410 140187804313408 submission_runner.py:206] Initializing dataset.
I0129 19:34:51.952660 140187804313408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0129 19:34:51.962430 140187804313408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0129 19:34:52.151028 140187804313408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0129 19:34:53.011216 140187804313408 submission_runner.py:213] Initializing model.
I0129 19:34:58.709317 140187804313408 submission_runner.py:255] Initializing optimizer.
I0129 19:34:59.102334 140187804313408 submission_runner.py:262] Initializing metrics bundle.
I0129 19:34:59.102494 140187804313408 submission_runner.py:280] Initializing checkpoint and logger.
I0129 19:34:59.118110 140187804313408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0129 19:34:59.118240 140187804313408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0129 19:35:10.729296 140187804313408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0129 19:35:21.796967 140187804313408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_5/flags_0.json.
I0129 19:35:21.801167 140187804313408 submission_runner.py:314] Starting training loop.
I0129 19:35:54.239527 140026033698560 logging_writer.py:48] [0] global_step=0, grad_norm=0.6558058857917786, loss=6.934148788452148
I0129 19:35:54.253873 140187804313408 spec.py:321] Evaluating on the training split.
I0129 19:36:00.510120 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 19:36:09.452897 140187804313408 spec.py:349] Evaluating on the test split.
I0129 19:36:12.174026 140187804313408 submission_runner.py:408] Time since start: 50.37s, 	Step: 1, 	{'train/accuracy': 0.0010363520123064518, 'train/loss': 6.912950038909912, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 32.45261359214783, 'total_duration': 50.37277865409851, 'accumulated_submission_time': 32.45261359214783, 'accumulated_eval_time': 17.920067310333252, 'accumulated_logging_time': 0}
I0129 19:36:12.183436 140025882715904 logging_writer.py:48] [1] accumulated_eval_time=17.920067, accumulated_logging_time=0, accumulated_submission_time=32.452614, global_step=1, preemption_count=0, score=32.452614, test/accuracy=0.000600, test/loss=6.912549, test/num_examples=10000, total_duration=50.372779, train/accuracy=0.001036, train/loss=6.912950, validation/accuracy=0.000760, validation/loss=6.913175, validation/num_examples=50000
I0129 19:36:46.204905 140025891108608 logging_writer.py:48] [100] global_step=100, grad_norm=0.6718987226486206, loss=6.827469348907471
I0129 19:37:20.272755 140025882715904 logging_writer.py:48] [200] global_step=200, grad_norm=0.7918666005134583, loss=6.563161373138428
I0129 19:37:54.428845 140025891108608 logging_writer.py:48] [300] global_step=300, grad_norm=1.023786187171936, loss=6.321254730224609
I0129 19:38:28.541795 140025882715904 logging_writer.py:48] [400] global_step=400, grad_norm=1.497582197189331, loss=5.974210739135742
I0129 19:39:02.639070 140025891108608 logging_writer.py:48] [500] global_step=500, grad_norm=3.1787490844726562, loss=5.888143539428711
I0129 19:39:36.737923 140025882715904 logging_writer.py:48] [600] global_step=600, grad_norm=5.810826301574707, loss=5.606250762939453
I0129 19:40:10.829689 140025891108608 logging_writer.py:48] [700] global_step=700, grad_norm=3.4054508209228516, loss=5.503830432891846
I0129 19:40:44.952365 140025882715904 logging_writer.py:48] [800] global_step=800, grad_norm=2.980320453643799, loss=5.338414192199707
I0129 19:41:19.074899 140025891108608 logging_writer.py:48] [900] global_step=900, grad_norm=7.14521598815918, loss=5.127811431884766
I0129 19:41:53.180952 140025882715904 logging_writer.py:48] [1000] global_step=1000, grad_norm=6.961597442626953, loss=5.027080059051514
I0129 19:42:27.317437 140025891108608 logging_writer.py:48] [1100] global_step=1100, grad_norm=5.881998538970947, loss=4.825664520263672
I0129 19:43:01.431231 140025882715904 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.115781784057617, loss=4.833921432495117
I0129 19:43:35.586792 140025891108608 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.747957229614258, loss=4.743616580963135
I0129 19:44:09.718349 140025882715904 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.2620387077331543, loss=4.506454944610596
I0129 19:44:42.342336 140187804313408 spec.py:321] Evaluating on the training split.
I0129 19:44:48.638838 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 19:44:57.333456 140187804313408 spec.py:349] Evaluating on the test split.
I0129 19:45:00.030231 140187804313408 submission_runner.py:408] Time since start: 578.23s, 	Step: 1497, 	{'train/accuracy': 0.16388313472270966, 'train/loss': 4.294137477874756, 'validation/accuracy': 0.14711999893188477, 'validation/loss': 4.409202575683594, 'validation/num_examples': 50000, 'test/accuracy': 0.10910000652074814, 'test/loss': 4.887209415435791, 'test/num_examples': 10000, 'score': 542.546288728714, 'total_duration': 578.2290046215057, 'accumulated_submission_time': 542.546288728714, 'accumulated_eval_time': 35.60792398452759, 'accumulated_logging_time': 0.02004528045654297}
I0129 19:45:00.048950 140026042091264 logging_writer.py:48] [1497] accumulated_eval_time=35.607924, accumulated_logging_time=0.020045, accumulated_submission_time=542.546289, global_step=1497, preemption_count=0, score=542.546289, test/accuracy=0.109100, test/loss=4.887209, test/num_examples=10000, total_duration=578.229005, train/accuracy=0.163883, train/loss=4.294137, validation/accuracy=0.147120, validation/loss=4.409203, validation/num_examples=50000
I0129 19:45:01.422891 140026050483968 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.436102867126465, loss=4.553399562835693
I0129 19:45:35.463779 140026042091264 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.34738302230835, loss=4.341347694396973
I0129 19:46:09.501215 140026050483968 logging_writer.py:48] [1700] global_step=1700, grad_norm=5.480703353881836, loss=4.259145259857178
I0129 19:46:43.612791 140026042091264 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.55258321762085, loss=4.179899215698242
I0129 19:47:17.685237 140026050483968 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.291357517242432, loss=4.164427280426025
I0129 19:47:51.790915 140026042091264 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.182656764984131, loss=4.173763275146484
I0129 19:48:25.895396 140026050483968 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.27543830871582, loss=3.9103569984436035
I0129 19:48:59.977838 140026042091264 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.1794512271881104, loss=3.9872143268585205
I0129 19:49:34.069117 140026050483968 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.250101089477539, loss=3.845454692840576
I0129 19:50:08.155278 140026042091264 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.510172367095947, loss=3.925715446472168
I0129 19:50:42.263588 140026050483968 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.83397912979126, loss=3.6831252574920654
I0129 19:51:16.468068 140026042091264 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.7704243659973145, loss=3.5967464447021484
I0129 19:51:50.543907 140026050483968 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.404623031616211, loss=3.559807300567627
I0129 19:52:24.651926 140026042091264 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.864950656890869, loss=3.537250280380249
I0129 19:52:58.741527 140026050483968 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.5798773765563965, loss=3.504606246948242
I0129 19:53:30.258803 140187804313408 spec.py:321] Evaluating on the training split.
I0129 19:53:36.539171 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 19:53:45.478922 140187804313408 spec.py:349] Evaluating on the test split.
I0129 19:53:48.133372 140187804313408 submission_runner.py:408] Time since start: 1106.33s, 	Step: 2994, 	{'train/accuracy': 0.3364756107330322, 'train/loss': 3.0776820182800293, 'validation/accuracy': 0.310839980840683, 'validation/loss': 3.213662624359131, 'validation/num_examples': 50000, 'test/accuracy': 0.24000000953674316, 'test/loss': 3.8412790298461914, 'test/num_examples': 10000, 'score': 1052.6925213336945, 'total_duration': 1106.3321468830109, 'accumulated_submission_time': 1052.6925213336945, 'accumulated_eval_time': 53.482457876205444, 'accumulated_logging_time': 0.04800295829772949}
I0129 19:53:48.151530 140025891108608 logging_writer.py:48] [2994] accumulated_eval_time=53.482458, accumulated_logging_time=0.048003, accumulated_submission_time=1052.692521, global_step=2994, preemption_count=0, score=1052.692521, test/accuracy=0.240000, test/loss=3.841279, test/num_examples=10000, total_duration=1106.332147, train/accuracy=0.336476, train/loss=3.077682, validation/accuracy=0.310840, validation/loss=3.213663, validation/num_examples=50000
I0129 19:53:50.527393 140026016913152 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.3053669929504395, loss=3.2925968170166016
I0129 19:54:24.531162 140025891108608 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.286717891693115, loss=3.4606072902679443
I0129 19:54:58.597455 140026016913152 logging_writer.py:48] [3200] global_step=3200, grad_norm=5.626606464385986, loss=3.3816514015197754
I0129 19:55:32.662906 140025891108608 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.5524508953094482, loss=3.251573324203491
I0129 19:56:06.716361 140026016913152 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.2499845027923584, loss=3.3218586444854736
I0129 19:56:40.800884 140025891108608 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.8776981830596924, loss=3.2771458625793457
I0129 19:57:14.856090 140026016913152 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.172431707382202, loss=3.184671401977539
I0129 19:57:48.977829 140025891108608 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.9869487285614014, loss=3.0538952350616455
I0129 19:58:23.060534 140026016913152 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.823566436767578, loss=3.0825114250183105
I0129 19:58:57.139101 140025891108608 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.543184280395508, loss=2.99753475189209
I0129 19:59:31.191754 140026016913152 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.211971759796143, loss=3.0503571033477783
I0129 20:00:05.238073 140025891108608 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.315650463104248, loss=2.9922847747802734
I0129 20:00:39.289407 140026016913152 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.096754550933838, loss=2.95945143699646
I0129 20:01:13.321913 140025891108608 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.097947597503662, loss=2.863349199295044
I0129 20:01:47.373518 140026016913152 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.3321175575256348, loss=2.851433515548706
I0129 20:02:18.147971 140187804313408 spec.py:321] Evaluating on the training split.
I0129 20:02:24.545807 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 20:02:33.299497 140187804313408 spec.py:349] Evaluating on the test split.
I0129 20:02:35.996961 140187804313408 submission_runner.py:408] Time since start: 1634.20s, 	Step: 4492, 	{'train/accuracy': 0.5123365521430969, 'train/loss': 2.0910186767578125, 'validation/accuracy': 0.4371799826622009, 'validation/loss': 2.503953695297241, 'validation/num_examples': 50000, 'test/accuracy': 0.33550000190734863, 'test/loss': 3.185488224029541, 'test/num_examples': 10000, 'score': 1562.6216881275177, 'total_duration': 1634.1957335472107, 'accumulated_submission_time': 1562.6216881275177, 'accumulated_eval_time': 71.33141088485718, 'accumulated_logging_time': 0.07839679718017578}
I0129 20:02:36.015224 140026016913152 logging_writer.py:48] [4492] accumulated_eval_time=71.331411, accumulated_logging_time=0.078397, accumulated_submission_time=1562.621688, global_step=4492, preemption_count=0, score=1562.621688, test/accuracy=0.335500, test/loss=3.185488, test/num_examples=10000, total_duration=1634.195734, train/accuracy=0.512337, train/loss=2.091019, validation/accuracy=0.437180, validation/loss=2.503954, validation/num_examples=50000
I0129 20:02:39.073196 140026042091264 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.7188634872436523, loss=2.7674903869628906
I0129 20:03:13.047840 140026016913152 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.582095146179199, loss=2.768287181854248
I0129 20:03:47.069555 140026042091264 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.247380495071411, loss=2.7712349891662598
I0129 20:04:21.168663 140026016913152 logging_writer.py:48] [4800] global_step=4800, grad_norm=5.154884338378906, loss=2.7190141677856445
I0129 20:04:55.188001 140026042091264 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.158066749572754, loss=2.733121156692505
I0129 20:05:29.209610 140026016913152 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.2043094635009766, loss=2.6126625537872314
I0129 20:06:03.248684 140026042091264 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.7180349826812744, loss=2.6094889640808105
I0129 20:06:37.319622 140026016913152 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.572845458984375, loss=2.6368496417999268
I0129 20:07:11.349651 140026042091264 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.7560131549835205, loss=2.7507855892181396
I0129 20:07:45.354019 140026016913152 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.4848971366882324, loss=2.544989585876465
I0129 20:08:19.347265 140026042091264 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.107499361038208, loss=2.591437339782715
I0129 20:08:53.378092 140026016913152 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.3791191577911377, loss=2.6082472801208496
I0129 20:09:27.406724 140026042091264 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.9766329526901245, loss=2.4551477432250977
I0129 20:10:01.404229 140026016913152 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.9006494283676147, loss=2.5554275512695312
I0129 20:10:35.444986 140026042091264 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.9953664541244507, loss=2.609123706817627
I0129 20:11:06.296959 140187804313408 spec.py:321] Evaluating on the training split.
I0129 20:11:12.571235 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 20:11:21.348278 140187804313408 spec.py:349] Evaluating on the test split.
I0129 20:11:24.057170 140187804313408 submission_runner.py:408] Time since start: 2162.26s, 	Step: 5992, 	{'train/accuracy': 0.5540696382522583, 'train/loss': 1.8846989870071411, 'validation/accuracy': 0.5013599991798401, 'validation/loss': 2.165916919708252, 'validation/num_examples': 50000, 'test/accuracy': 0.38910001516342163, 'test/loss': 2.906574010848999, 'test/num_examples': 10000, 'score': 2072.839183807373, 'total_duration': 2162.2559444904327, 'accumulated_submission_time': 2072.839183807373, 'accumulated_eval_time': 89.09158182144165, 'accumulated_logging_time': 0.10589408874511719}
I0129 20:11:24.075561 140026025305856 logging_writer.py:48] [5992] accumulated_eval_time=89.091582, accumulated_logging_time=0.105894, accumulated_submission_time=2072.839184, global_step=5992, preemption_count=0, score=2072.839184, test/accuracy=0.389100, test/loss=2.906574, test/num_examples=10000, total_duration=2162.255944, train/accuracy=0.554070, train/loss=1.884699, validation/accuracy=0.501360, validation/loss=2.165917, validation/num_examples=50000
I0129 20:11:27.164266 140026033698560 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.995280146598816, loss=2.5206713676452637
I0129 20:12:01.114834 140026025305856 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.1704046726226807, loss=2.595630168914795
I0129 20:12:35.091664 140026033698560 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.8525757789611816, loss=2.37488055229187
I0129 20:13:09.095840 140026025305856 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.5108983516693115, loss=2.429748058319092
I0129 20:13:43.083111 140026033698560 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.5043797492980957, loss=2.439514636993408
I0129 20:14:17.096493 140026025305856 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.804434895515442, loss=2.27530837059021
I0129 20:14:51.131997 140026033698560 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.647693157196045, loss=2.532358169555664
I0129 20:15:25.139637 140026025305856 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.224787473678589, loss=2.280540704727173
I0129 20:15:59.164714 140026033698560 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.614199161529541, loss=2.395120143890381
I0129 20:16:33.165562 140026025305856 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.9341769218444824, loss=2.271611452102661
I0129 20:17:07.218747 140026033698560 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.3427561521530151, loss=2.1996281147003174
I0129 20:17:41.314915 140026025305856 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.780992269515991, loss=2.3127734661102295
I0129 20:18:15.303369 140026033698560 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.891589879989624, loss=2.3366031646728516
I0129 20:18:49.296242 140026025305856 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.3653674125671387, loss=2.2158262729644775
I0129 20:19:23.311077 140026033698560 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.973833441734314, loss=2.209928512573242
I0129 20:19:54.065600 140187804313408 spec.py:321] Evaluating on the training split.
I0129 20:20:00.428660 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 20:20:09.362262 140187804313408 spec.py:349] Evaluating on the test split.
I0129 20:20:12.031249 140187804313408 submission_runner.py:408] Time since start: 2690.23s, 	Step: 7492, 	{'train/accuracy': 0.5963010191917419, 'train/loss': 1.6671967506408691, 'validation/accuracy': 0.5468400120735168, 'validation/loss': 1.9419997930526733, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.6873884201049805, 'test/num_examples': 10000, 'score': 2582.7628107070923, 'total_duration': 2690.230018377304, 'accumulated_submission_time': 2582.7628107070923, 'accumulated_eval_time': 107.05718922615051, 'accumulated_logging_time': 0.13532543182373047}
I0129 20:20:12.050160 140026025305856 logging_writer.py:48] [7492] accumulated_eval_time=107.057189, accumulated_logging_time=0.135325, accumulated_submission_time=2582.762811, global_step=7492, preemption_count=0, score=2582.762811, test/accuracy=0.422700, test/loss=2.687388, test/num_examples=10000, total_duration=2690.230018, train/accuracy=0.596301, train/loss=1.667197, validation/accuracy=0.546840, validation/loss=1.942000, validation/num_examples=50000
I0129 20:20:15.107061 140026033698560 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.775980830192566, loss=2.2052292823791504
I0129 20:20:49.063182 140026025305856 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.5266311168670654, loss=2.238677501678467
I0129 20:21:23.050179 140026033698560 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.9885845184326172, loss=2.3996570110321045
I0129 20:21:57.040481 140026025305856 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.192507266998291, loss=2.263671398162842
I0129 20:22:31.035067 140026033698560 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.4245569705963135, loss=2.350733757019043
I0129 20:23:05.065494 140026025305856 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.5591628551483154, loss=2.1510825157165527
I0129 20:23:39.040140 140026033698560 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.7569371461868286, loss=2.214165210723877
I0129 20:24:13.130271 140026025305856 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.5360157489776611, loss=2.1754684448242188
I0129 20:24:47.157613 140026033698560 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5284833908081055, loss=2.235491991043091
I0129 20:25:21.170948 140026025305856 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.3259512186050415, loss=2.1264240741729736
I0129 20:25:55.170795 140026033698560 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.4896045923233032, loss=2.142460584640503
I0129 20:26:29.174340 140026025305856 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.9434869289398193, loss=2.251760244369507
I0129 20:27:03.188258 140026033698560 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.7482757568359375, loss=2.1749610900878906
I0129 20:27:37.216109 140026025305856 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.9206271171569824, loss=2.168422222137451
I0129 20:28:11.231339 140026033698560 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.7018827199935913, loss=2.1234936714172363
I0129 20:28:42.350523 140187804313408 spec.py:321] Evaluating on the training split.
I0129 20:28:48.627751 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 20:28:57.567674 140187804313408 spec.py:349] Evaluating on the test split.
I0129 20:29:00.124564 140187804313408 submission_runner.py:408] Time since start: 3218.32s, 	Step: 8993, 	{'train/accuracy': 0.5882493257522583, 'train/loss': 1.7096151113510132, 'validation/accuracy': 0.5422999858856201, 'validation/loss': 1.9613946676254272, 'validation/num_examples': 50000, 'test/accuracy': 0.4240000247955322, 'test/loss': 2.697845935821533, 'test/num_examples': 10000, 'score': 3092.9980075359344, 'total_duration': 3218.3233294487, 'accumulated_submission_time': 3092.9980075359344, 'accumulated_eval_time': 124.83120155334473, 'accumulated_logging_time': 0.16447710990905762}
I0129 20:29:00.144604 140025882715904 logging_writer.py:48] [8993] accumulated_eval_time=124.831202, accumulated_logging_time=0.164477, accumulated_submission_time=3092.998008, global_step=8993, preemption_count=0, score=3092.998008, test/accuracy=0.424000, test/loss=2.697846, test/num_examples=10000, total_duration=3218.323329, train/accuracy=0.588249, train/loss=1.709615, validation/accuracy=0.542300, validation/loss=1.961395, validation/num_examples=50000
I0129 20:29:02.885371 140025891108608 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.390371561050415, loss=2.092531681060791
I0129 20:29:36.819377 140025882715904 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.791762351989746, loss=2.080868721008301
I0129 20:30:10.813102 140025891108608 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.6861194372177124, loss=2.1642203330993652
I0129 20:30:44.902188 140025882715904 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.6177715063095093, loss=2.093240976333618
I0129 20:31:18.901850 140025891108608 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.773250937461853, loss=2.094463348388672
I0129 20:31:52.899553 140025882715904 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.123945713043213, loss=2.187974214553833
I0129 20:32:26.888117 140025891108608 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.310840129852295, loss=2.0233654975891113
I0129 20:33:00.902695 140025882715904 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.6010185480117798, loss=2.1890978813171387
I0129 20:33:34.888192 140025891108608 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.3548610210418701, loss=2.119577407836914
I0129 20:34:08.861287 140025882715904 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.121845245361328, loss=2.095346689224243
I0129 20:34:42.845614 140025891108608 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.8139231204986572, loss=2.0094525814056396
I0129 20:35:16.801766 140025882715904 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.9862077236175537, loss=2.2990596294403076
I0129 20:35:50.796143 140025891108608 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.8280881643295288, loss=2.189998149871826
I0129 20:36:24.797611 140025882715904 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.800113558769226, loss=2.0042011737823486
I0129 20:36:58.778378 140025891108608 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.5689005851745605, loss=2.046032190322876
I0129 20:37:30.261193 140187804313408 spec.py:321] Evaluating on the training split.
I0129 20:37:36.585942 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 20:37:45.426947 140187804313408 spec.py:349] Evaluating on the test split.
I0129 20:37:48.028035 140187804313408 submission_runner.py:408] Time since start: 3746.23s, 	Step: 10494, 	{'train/accuracy': 0.6168686151504517, 'train/loss': 1.563010334968567, 'validation/accuracy': 0.5707399845123291, 'validation/loss': 1.820184588432312, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.5981123447418213, 'test/num_examples': 10000, 'score': 3603.048814535141, 'total_duration': 3746.226803779602, 'accumulated_submission_time': 3603.048814535141, 'accumulated_eval_time': 142.59801578521729, 'accumulated_logging_time': 0.19438743591308594}
I0129 20:37:48.049633 140026025305856 logging_writer.py:48] [10494] accumulated_eval_time=142.598016, accumulated_logging_time=0.194387, accumulated_submission_time=3603.048815, global_step=10494, preemption_count=0, score=3603.048815, test/accuracy=0.445600, test/loss=2.598112, test/num_examples=10000, total_duration=3746.226804, train/accuracy=0.616869, train/loss=1.563010, validation/accuracy=0.570740, validation/loss=1.820185, validation/num_examples=50000
I0129 20:37:50.424558 140026033698560 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.8015912771224976, loss=2.018392324447632
I0129 20:38:24.328496 140026025305856 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.6550474166870117, loss=1.9055207967758179
I0129 20:38:58.281481 140026033698560 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.5324909687042236, loss=2.1167430877685547
I0129 20:39:32.268314 140026025305856 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.6296837329864502, loss=2.1222705841064453
I0129 20:40:06.250868 140026033698560 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.2824865579605103, loss=2.119035005569458
I0129 20:40:40.262522 140026025305856 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.8464820384979248, loss=2.1067006587982178
I0129 20:41:14.257278 140026033698560 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.8342911005020142, loss=1.97234308719635
I0129 20:41:48.254793 140026025305856 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.4824070930480957, loss=1.971306562423706
I0129 20:42:22.200686 140026033698560 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.7470886707305908, loss=2.0180320739746094
I0129 20:42:56.188704 140026025305856 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.721644401550293, loss=1.9415348768234253
I0129 20:43:30.198746 140026033698560 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.6026591062545776, loss=1.9942986965179443
I0129 20:44:04.246872 140026025305856 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.7384980916976929, loss=1.952978491783142
I0129 20:44:38.250995 140026033698560 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.8674581050872803, loss=2.002211093902588
I0129 20:45:12.241092 140026025305856 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.7411428689956665, loss=1.964539885520935
I0129 20:45:46.252731 140026033698560 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.6444270610809326, loss=2.0391199588775635
I0129 20:46:18.032707 140187804313408 spec.py:321] Evaluating on the training split.
I0129 20:46:24.330275 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 20:46:33.234924 140187804313408 spec.py:349] Evaluating on the test split.
I0129 20:46:35.899433 140187804313408 submission_runner.py:408] Time since start: 4274.10s, 	Step: 11995, 	{'train/accuracy': 0.6272122263908386, 'train/loss': 1.5147466659545898, 'validation/accuracy': 0.5814799666404724, 'validation/loss': 1.769814372062683, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.5150883197784424, 'test/num_examples': 10000, 'score': 4112.968335390091, 'total_duration': 4274.098203659058, 'accumulated_submission_time': 4112.968335390091, 'accumulated_eval_time': 160.46470522880554, 'accumulated_logging_time': 0.22560787200927734}
I0129 20:46:35.918606 140026016913152 logging_writer.py:48] [11995] accumulated_eval_time=160.464705, accumulated_logging_time=0.225608, accumulated_submission_time=4112.968335, global_step=11995, preemption_count=0, score=4112.968335, test/accuracy=0.460400, test/loss=2.515088, test/num_examples=10000, total_duration=4274.098204, train/accuracy=0.627212, train/loss=1.514747, validation/accuracy=0.581480, validation/loss=1.769814, validation/num_examples=50000
I0129 20:46:37.957147 140026025305856 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.109149694442749, loss=2.0791568756103516
I0129 20:47:11.905035 140026016913152 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.014197587966919, loss=2.046133041381836
I0129 20:47:45.865072 140026025305856 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.9359121322631836, loss=2.0929038524627686
I0129 20:48:19.850435 140026016913152 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.5719355344772339, loss=2.0019407272338867
I0129 20:48:53.864973 140026025305856 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.2964824438095093, loss=1.8605525493621826
I0129 20:49:27.845425 140026016913152 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.829610824584961, loss=1.9342936277389526
I0129 20:50:01.837153 140026025305856 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.3353211879730225, loss=1.9217555522918701
I0129 20:50:35.927124 140026016913152 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.8714536428451538, loss=1.8650709390640259
I0129 20:51:09.917137 140026025305856 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.101289749145508, loss=1.9959509372711182
I0129 20:51:43.914548 140026016913152 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.7930607795715332, loss=2.009352922439575
I0129 20:52:17.896570 140026025305856 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.6810967922210693, loss=1.909837245941162
I0129 20:52:51.925014 140026016913152 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.343881368637085, loss=1.9320664405822754
I0129 20:53:25.919664 140026025305856 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4996192455291748, loss=1.9218882322311401
I0129 20:53:59.939243 140026016913152 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.5622845888137817, loss=1.9540736675262451
I0129 20:54:33.926913 140026025305856 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.5148634910583496, loss=1.9523670673370361
I0129 20:55:06.046673 140187804313408 spec.py:321] Evaluating on the training split.
I0129 20:55:12.330419 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 20:55:21.277162 140187804313408 spec.py:349] Evaluating on the test split.
I0129 20:55:23.980347 140187804313408 submission_runner.py:408] Time since start: 4802.18s, 	Step: 13496, 	{'train/accuracy': 0.6594387888908386, 'train/loss': 1.3796418905258179, 'validation/accuracy': 0.5920199751853943, 'validation/loss': 1.711396336555481, 'validation/num_examples': 50000, 'test/accuracy': 0.46400001645088196, 'test/loss': 2.4521992206573486, 'test/num_examples': 10000, 'score': 4623.030744314194, 'total_duration': 4802.1791207790375, 'accumulated_submission_time': 4623.030744314194, 'accumulated_eval_time': 178.39834880828857, 'accumulated_logging_time': 0.25389599800109863}
I0129 20:55:24.000815 140026050483968 logging_writer.py:48] [13496] accumulated_eval_time=178.398349, accumulated_logging_time=0.253896, accumulated_submission_time=4623.030744, global_step=13496, preemption_count=0, score=4623.030744, test/accuracy=0.464000, test/loss=2.452199, test/num_examples=10000, total_duration=4802.179121, train/accuracy=0.659439, train/loss=1.379642, validation/accuracy=0.592020, validation/loss=1.711396, validation/num_examples=50000
I0129 20:55:25.705760 140026058876672 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.9401946067810059, loss=1.9143996238708496
I0129 20:55:59.643751 140026050483968 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.206331491470337, loss=1.9195916652679443
I0129 20:56:33.612667 140026058876672 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.700989007949829, loss=1.9417561292648315
I0129 20:57:07.673010 140026050483968 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.3850959539413452, loss=1.8628041744232178
I0129 20:57:41.655105 140026058876672 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.5736297369003296, loss=1.8741317987442017
I0129 20:58:15.652161 140026050483968 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.5753488540649414, loss=1.949028491973877
I0129 20:58:49.678984 140026058876672 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.9436405897140503, loss=1.8841133117675781
I0129 20:59:23.673289 140026050483968 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.3227081298828125, loss=1.9948406219482422
I0129 20:59:57.654599 140026058876672 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.2729859352111816, loss=1.90302312374115
I0129 21:00:31.628979 140026050483968 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.5191364288330078, loss=2.0038163661956787
I0129 21:01:05.589796 140026058876672 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.6143105030059814, loss=1.9903271198272705
I0129 21:01:39.565059 140026050483968 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.486008882522583, loss=1.8804309368133545
I0129 21:02:13.545404 140026058876672 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.430096983909607, loss=1.8759467601776123
I0129 21:02:47.546704 140026050483968 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.5673043727874756, loss=1.827246904373169
I0129 21:03:21.523344 140026058876672 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.4484162330627441, loss=1.8386058807373047
I0129 21:03:54.044170 140187804313408 spec.py:321] Evaluating on the training split.
I0129 21:04:00.405053 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 21:04:09.365732 140187804313408 spec.py:349] Evaluating on the test split.
I0129 21:04:12.054688 140187804313408 submission_runner.py:408] Time since start: 5330.25s, 	Step: 14997, 	{'train/accuracy': 0.6680684089660645, 'train/loss': 1.324404001235962, 'validation/accuracy': 0.5925399661064148, 'validation/loss': 1.721736192703247, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.463068723678589, 'test/num_examples': 10000, 'score': 5133.006846666336, 'total_duration': 5330.253460884094, 'accumulated_submission_time': 5133.006846666336, 'accumulated_eval_time': 196.40883922576904, 'accumulated_logging_time': 0.28588294982910156}
I0129 21:04:12.077184 140025891108608 logging_writer.py:48] [14997] accumulated_eval_time=196.408839, accumulated_logging_time=0.285883, accumulated_submission_time=5133.006847, global_step=14997, preemption_count=0, score=5133.006847, test/accuracy=0.466900, test/loss=2.463069, test/num_examples=10000, total_duration=5330.253461, train/accuracy=0.668068, train/loss=1.324404, validation/accuracy=0.592540, validation/loss=1.721736, validation/num_examples=50000
I0129 21:04:13.436340 140026016913152 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.7505766153335571, loss=1.9662535190582275
I0129 21:04:47.368680 140025891108608 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.5196545124053955, loss=1.8456615209579468
I0129 21:05:21.312463 140026016913152 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.7325987815856934, loss=1.8408732414245605
I0129 21:05:55.293094 140025891108608 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4650403261184692, loss=1.9543466567993164
I0129 21:06:29.288115 140026016913152 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.5696128606796265, loss=1.776402235031128
I0129 21:07:03.286463 140025891108608 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.7103058099746704, loss=1.9285796880722046
I0129 21:07:37.285919 140026016913152 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.7422845363616943, loss=1.8716837167739868
I0129 21:08:11.271820 140025891108608 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.656561017036438, loss=1.9305787086486816
I0129 21:08:45.286628 140026016913152 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.7582426071166992, loss=1.9121419191360474
I0129 21:09:19.253761 140025891108608 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5264805555343628, loss=1.8778595924377441
I0129 21:09:53.244682 140026016913152 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.5467183589935303, loss=1.913407564163208
I0129 21:10:27.260772 140025891108608 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.4293063879013062, loss=1.9150607585906982
I0129 21:11:01.248677 140026016913152 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.5606436729431152, loss=1.8855239152908325
I0129 21:11:35.239712 140025891108608 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.9481397867202759, loss=1.8908637762069702
I0129 21:12:09.227593 140026016913152 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.493093490600586, loss=1.9336172342300415
I0129 21:12:42.340250 140187804313408 spec.py:321] Evaluating on the training split.
I0129 21:12:48.642069 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 21:12:57.390076 140187804313408 spec.py:349] Evaluating on the test split.
I0129 21:13:00.215870 140187804313408 submission_runner.py:408] Time since start: 5858.41s, 	Step: 16499, 	{'train/accuracy': 0.6616111397743225, 'train/loss': 1.3371883630752563, 'validation/accuracy': 0.6007199883460999, 'validation/loss': 1.6780246496200562, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.453063726425171, 'test/num_examples': 10000, 'score': 5643.204411029816, 'total_duration': 5858.414639472961, 'accumulated_submission_time': 5643.204411029816, 'accumulated_eval_time': 214.28442406654358, 'accumulated_logging_time': 0.31774473190307617}
I0129 21:13:00.237250 140025891108608 logging_writer.py:48] [16499] accumulated_eval_time=214.284424, accumulated_logging_time=0.317745, accumulated_submission_time=5643.204411, global_step=16499, preemption_count=0, score=5643.204411, test/accuracy=0.469300, test/loss=2.453064, test/num_examples=10000, total_duration=5858.414639, train/accuracy=0.661611, train/loss=1.337188, validation/accuracy=0.600720, validation/loss=1.678025, validation/num_examples=50000
I0129 21:13:00.941322 140026050483968 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.4872373342514038, loss=1.8948371410369873
I0129 21:13:34.839377 140025891108608 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.6186774969100952, loss=1.8539092540740967
I0129 21:14:08.767513 140026050483968 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.629952073097229, loss=1.7917661666870117
I0129 21:14:42.768422 140025891108608 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.58467435836792, loss=1.8682371377944946
I0129 21:15:16.724153 140026050483968 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.6062439680099487, loss=1.8262865543365479
I0129 21:15:50.690100 140025891108608 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.9221075773239136, loss=1.9090101718902588
I0129 21:16:24.657983 140026050483968 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.7255154848098755, loss=1.9284864664077759
I0129 21:16:58.737899 140025891108608 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.601572871208191, loss=1.8126022815704346
I0129 21:17:32.701089 140026050483968 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.7064785957336426, loss=1.7890812158584595
I0129 21:18:06.705062 140025891108608 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.4270920753479004, loss=2.011178731918335
I0129 21:18:40.665421 140026050483968 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.7198011875152588, loss=1.7771345376968384
I0129 21:19:14.633167 140025891108608 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.5518156290054321, loss=1.8132082223892212
I0129 21:19:48.607667 140026050483968 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.343780517578125, loss=1.7826653718948364
I0129 21:20:22.603178 140025891108608 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.6592589616775513, loss=1.816006064414978
I0129 21:20:56.556249 140026050483968 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.5069738626480103, loss=1.740790843963623
I0129 21:21:30.542830 140025891108608 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.5944427251815796, loss=1.828920841217041
I0129 21:21:30.550402 140187804313408 spec.py:321] Evaluating on the training split.
I0129 21:21:36.928680 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 21:21:46.055498 140187804313408 spec.py:349] Evaluating on the test split.
I0129 21:21:48.775926 140187804313408 submission_runner.py:408] Time since start: 6386.97s, 	Step: 18001, 	{'train/accuracy': 0.6668726205825806, 'train/loss': 1.3255809545516968, 'validation/accuracy': 0.6147199869155884, 'validation/loss': 1.603179693222046, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3434627056121826, 'test/num_examples': 10000, 'score': 6153.446529865265, 'total_duration': 6386.974694013596, 'accumulated_submission_time': 6153.446529865265, 'accumulated_eval_time': 232.5098798274994, 'accumulated_logging_time': 0.35366272926330566}
I0129 21:21:48.797703 140026042091264 logging_writer.py:48] [18001] accumulated_eval_time=232.509880, accumulated_logging_time=0.353663, accumulated_submission_time=6153.446530, global_step=18001, preemption_count=0, score=6153.446530, test/accuracy=0.486700, test/loss=2.343463, test/num_examples=10000, total_duration=6386.974694, train/accuracy=0.666873, train/loss=1.325581, validation/accuracy=0.614720, validation/loss=1.603180, validation/num_examples=50000
I0129 21:22:22.688477 140026058876672 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.502353310585022, loss=1.874643087387085
I0129 21:22:56.648212 140026042091264 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.6068105697631836, loss=1.8537713289260864
I0129 21:23:30.585073 140026058876672 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.196946620941162, loss=1.871451735496521
I0129 21:24:04.657433 140026042091264 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.980121374130249, loss=1.76933753490448
I0129 21:24:38.623633 140026058876672 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.486219882965088, loss=1.8556169271469116
I0129 21:25:12.635822 140026042091264 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.557300329208374, loss=1.7671546936035156
I0129 21:25:46.615987 140026058876672 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.768829107284546, loss=1.9663732051849365
I0129 21:26:20.566837 140026042091264 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.7864296436309814, loss=1.8193395137786865
I0129 21:26:54.568159 140026058876672 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.5029085874557495, loss=1.840872049331665
I0129 21:27:28.532545 140026042091264 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.898834466934204, loss=1.8034306764602661
I0129 21:28:02.517384 140026058876672 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.6227352619171143, loss=1.816648006439209
I0129 21:28:36.483572 140026042091264 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7176729440689087, loss=1.8885759115219116
I0129 21:29:10.497047 140026058876672 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.5991058349609375, loss=1.8549315929412842
I0129 21:29:44.458440 140026042091264 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.5699055194854736, loss=1.8497271537780762
I0129 21:30:18.501596 140026058876672 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.7065781354904175, loss=1.946386456489563
I0129 21:30:18.992243 140187804313408 spec.py:321] Evaluating on the training split.
I0129 21:30:25.265436 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 21:30:34.230153 140187804313408 spec.py:349] Evaluating on the test split.
I0129 21:30:36.924277 140187804313408 submission_runner.py:408] Time since start: 6915.12s, 	Step: 19503, 	{'train/accuracy': 0.6614915132522583, 'train/loss': 1.3690069913864136, 'validation/accuracy': 0.6030600070953369, 'validation/loss': 1.656613826751709, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.4080591201782227, 'test/num_examples': 10000, 'score': 6663.575809955597, 'total_duration': 6915.123049736023, 'accumulated_submission_time': 6663.575809955597, 'accumulated_eval_time': 250.44188237190247, 'accumulated_logging_time': 0.3847367763519287}
I0129 21:30:36.949482 140025891108608 logging_writer.py:48] [19503] accumulated_eval_time=250.441882, accumulated_logging_time=0.384737, accumulated_submission_time=6663.575810, global_step=19503, preemption_count=0, score=6663.575810, test/accuracy=0.477800, test/loss=2.408059, test/num_examples=10000, total_duration=6915.123050, train/accuracy=0.661492, train/loss=1.369007, validation/accuracy=0.603060, validation/loss=1.656614, validation/num_examples=50000
I0129 21:31:10.175934 140026016913152 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.7217457294464111, loss=1.9324908256530762
I0129 21:31:44.126676 140025891108608 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.8005330562591553, loss=1.7404757738113403
I0129 21:32:18.070745 140026016913152 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.7243537902832031, loss=1.759696125984192
I0129 21:32:52.039832 140025891108608 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.6035739183425903, loss=1.8052133321762085
I0129 21:33:26.014122 140026016913152 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.6479064226150513, loss=1.7117736339569092
I0129 21:33:59.971960 140025891108608 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9274760484695435, loss=1.8350144624710083
I0129 21:34:33.953984 140026016913152 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.7101964950561523, loss=1.901423454284668
I0129 21:35:07.909317 140025891108608 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.5534882545471191, loss=1.8138771057128906
I0129 21:35:41.883703 140026016913152 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.764647126197815, loss=1.7359825372695923
I0129 21:36:15.827121 140025891108608 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.6244994401931763, loss=1.8346863985061646
I0129 21:36:49.872093 140026016913152 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.5385360717773438, loss=1.7286068201065063
I0129 21:37:23.837749 140025891108608 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.8459268808364868, loss=1.7243705987930298
I0129 21:37:57.775357 140026016913152 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.5134752988815308, loss=1.7480039596557617
I0129 21:38:31.767023 140025891108608 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.7915514707565308, loss=1.7141538858413696
I0129 21:39:05.722182 140026016913152 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.8623263835906982, loss=1.8696024417877197
I0129 21:39:07.230021 140187804313408 spec.py:321] Evaluating on the training split.
I0129 21:39:14.158786 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 21:39:22.781152 140187804313408 spec.py:349] Evaluating on the test split.
I0129 21:39:25.422636 140187804313408 submission_runner.py:408] Time since start: 7443.62s, 	Step: 21006, 	{'train/accuracy': 0.6614915132522583, 'train/loss': 1.3474102020263672, 'validation/accuracy': 0.6093999743461609, 'validation/loss': 1.626532793045044, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.356985330581665, 'test/num_examples': 10000, 'score': 7173.793117523193, 'total_duration': 7443.621410608292, 'accumulated_submission_time': 7173.793117523193, 'accumulated_eval_time': 268.6344575881958, 'accumulated_logging_time': 0.41891908645629883}
I0129 21:39:25.444674 140026050483968 logging_writer.py:48] [21006] accumulated_eval_time=268.634458, accumulated_logging_time=0.418919, accumulated_submission_time=7173.793118, global_step=21006, preemption_count=0, score=7173.793118, test/accuracy=0.483800, test/loss=2.356985, test/num_examples=10000, total_duration=7443.621411, train/accuracy=0.661492, train/loss=1.347410, validation/accuracy=0.609400, validation/loss=1.626533, validation/num_examples=50000
I0129 21:39:57.634129 140026058876672 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.5865896940231323, loss=1.769193410873413
I0129 21:40:31.540625 140026050483968 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.077863931655884, loss=1.873124361038208
I0129 21:41:05.478847 140026058876672 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.746739149093628, loss=1.8430753946304321
I0129 21:41:39.447716 140026050483968 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.7024914026260376, loss=1.8406131267547607
I0129 21:42:13.408511 140026058876672 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.6742124557495117, loss=1.7857470512390137
I0129 21:42:47.385995 140026050483968 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.7431713342666626, loss=1.8263969421386719
I0129 21:43:21.362390 140026058876672 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.5901459455490112, loss=1.7421108484268188
I0129 21:43:55.423699 140026050483968 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.839035153388977, loss=1.7439250946044922
I0129 21:44:29.398371 140026058876672 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.6600004434585571, loss=1.7901291847229004
I0129 21:45:03.377774 140026050483968 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.7437394857406616, loss=1.768369197845459
I0129 21:45:37.367592 140026058876672 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.6483678817749023, loss=1.823026180267334
I0129 21:46:11.367660 140026050483968 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.8113791942596436, loss=1.8006486892700195
I0129 21:46:45.357940 140026058876672 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.6710985898971558, loss=1.7612792253494263
I0129 21:47:19.341565 140026050483968 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.6140129566192627, loss=1.8022067546844482
I0129 21:47:53.281051 140026058876672 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.0283203125, loss=1.6456583738327026
I0129 21:47:55.462133 140187804313408 spec.py:321] Evaluating on the training split.
I0129 21:48:01.775972 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 21:48:10.639974 140187804313408 spec.py:349] Evaluating on the test split.
I0129 21:48:13.310539 140187804313408 submission_runner.py:408] Time since start: 7971.51s, 	Step: 22508, 	{'train/accuracy': 0.6671914458274841, 'train/loss': 1.326819896697998, 'validation/accuracy': 0.6113399863243103, 'validation/loss': 1.6232869625091553, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.3702409267425537, 'test/num_examples': 10000, 'score': 7683.747024536133, 'total_duration': 7971.509313106537, 'accumulated_submission_time': 7683.747024536133, 'accumulated_eval_time': 286.4828236103058, 'accumulated_logging_time': 0.4501662254333496}
I0129 21:48:13.332341 140025891108608 logging_writer.py:48] [22508] accumulated_eval_time=286.482824, accumulated_logging_time=0.450166, accumulated_submission_time=7683.747025, global_step=22508, preemption_count=0, score=7683.747025, test/accuracy=0.481100, test/loss=2.370241, test/num_examples=10000, total_duration=7971.509313, train/accuracy=0.667191, train/loss=1.326820, validation/accuracy=0.611340, validation/loss=1.623287, validation/num_examples=50000
I0129 21:48:44.885444 140026016913152 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.8185615539550781, loss=1.7810678482055664
I0129 21:49:18.801190 140025891108608 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.8115993738174438, loss=1.8636963367462158
I0129 21:49:52.739063 140026016913152 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.6394375562667847, loss=1.8164877891540527
I0129 21:50:26.779467 140025891108608 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.5165210962295532, loss=1.6931794881820679
I0129 21:51:00.742327 140026016913152 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.6455883979797363, loss=1.8406085968017578
I0129 21:51:34.686866 140025891108608 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.7933790683746338, loss=1.757778525352478
I0129 21:52:08.683823 140026016913152 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.7460182905197144, loss=1.7784554958343506
I0129 21:52:42.642864 140025891108608 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.8659158945083618, loss=1.6830440759658813
I0129 21:53:16.607290 140026016913152 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.6082968711853027, loss=1.770161509513855
I0129 21:53:50.588309 140025891108608 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.6326143741607666, loss=1.7706547975540161
I0129 21:54:24.500841 140026016913152 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.876461148262024, loss=1.8251149654388428
I0129 21:54:58.487453 140025891108608 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.7563585042953491, loss=1.858094334602356
I0129 21:55:32.453474 140026016913152 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.76534104347229, loss=1.8122395277023315
I0129 21:56:06.422589 140025891108608 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.5786612033843994, loss=1.756696105003357
I0129 21:56:40.437552 140026016913152 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.7214157581329346, loss=1.80016028881073
I0129 21:56:43.640138 140187804313408 spec.py:321] Evaluating on the training split.
I0129 21:56:49.934678 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 21:56:58.777724 140187804313408 spec.py:349] Evaluating on the test split.
I0129 21:57:01.480626 140187804313408 submission_runner.py:408] Time since start: 8499.68s, 	Step: 24011, 	{'train/accuracy': 0.6983218789100647, 'train/loss': 1.184372901916504, 'validation/accuracy': 0.6136800050735474, 'validation/loss': 1.602954387664795, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.310030937194824, 'test/num_examples': 10000, 'score': 8193.990616083145, 'total_duration': 8499.679381370544, 'accumulated_submission_time': 8193.990616083145, 'accumulated_eval_time': 304.32325291633606, 'accumulated_logging_time': 0.48122096061706543}
I0129 21:57:01.503843 140026050483968 logging_writer.py:48] [24011] accumulated_eval_time=304.323253, accumulated_logging_time=0.481221, accumulated_submission_time=8193.990616, global_step=24011, preemption_count=0, score=8193.990616, test/accuracy=0.494800, test/loss=2.310031, test/num_examples=10000, total_duration=8499.679381, train/accuracy=0.698322, train/loss=1.184373, validation/accuracy=0.613680, validation/loss=1.602954, validation/num_examples=50000
I0129 21:57:32.019301 140026058876672 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.1490824222564697, loss=1.8047746419906616
I0129 21:58:05.996408 140026050483968 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.751893162727356, loss=1.7417503595352173
I0129 21:58:39.960029 140026058876672 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.941136121749878, loss=1.8029699325561523
I0129 21:59:13.956291 140026050483968 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.631272554397583, loss=1.8051819801330566
I0129 21:59:47.929633 140026058876672 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.7395015954971313, loss=1.832701563835144
I0129 22:00:21.888929 140026050483968 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.7354516983032227, loss=1.7402808666229248
I0129 22:00:55.840658 140026058876672 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.749696969985962, loss=1.7221544981002808
I0129 22:01:29.808566 140026050483968 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.8002376556396484, loss=1.7405520677566528
I0129 22:02:03.791399 140026058876672 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.016122579574585, loss=1.8224053382873535
I0129 22:02:37.761785 140026050483968 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.273179531097412, loss=1.8517615795135498
I0129 22:03:11.827404 140026058876672 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.7735539674758911, loss=1.8294715881347656
I0129 22:03:45.782628 140026050483968 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.867052435874939, loss=1.7972080707550049
I0129 22:04:19.726679 140026058876672 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.8639048337936401, loss=1.8822449445724487
I0129 22:04:53.717629 140026050483968 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.64296555519104, loss=1.7997387647628784
I0129 22:05:27.681410 140026058876672 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.0166690349578857, loss=1.8705302476882935
I0129 22:05:31.567429 140187804313408 spec.py:321] Evaluating on the training split.
I0129 22:05:37.930665 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 22:05:46.919278 140187804313408 spec.py:349] Evaluating on the test split.
I0129 22:05:49.663541 140187804313408 submission_runner.py:408] Time since start: 9027.86s, 	Step: 25513, 	{'train/accuracy': 0.6822385191917419, 'train/loss': 1.2491592168807983, 'validation/accuracy': 0.6146399974822998, 'validation/loss': 1.6218963861465454, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.3448243141174316, 'test/num_examples': 10000, 'score': 8703.985595703125, 'total_duration': 9027.862311124802, 'accumulated_submission_time': 8703.985595703125, 'accumulated_eval_time': 322.419335603714, 'accumulated_logging_time': 0.5180819034576416}
I0129 22:05:49.685669 140026025305856 logging_writer.py:48] [25513] accumulated_eval_time=322.419336, accumulated_logging_time=0.518082, accumulated_submission_time=8703.985596, global_step=25513, preemption_count=0, score=8703.985596, test/accuracy=0.494400, test/loss=2.344824, test/num_examples=10000, total_duration=9027.862311, train/accuracy=0.682239, train/loss=1.249159, validation/accuracy=0.614640, validation/loss=1.621896, validation/num_examples=50000
I0129 22:06:19.518934 140026033698560 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.5345556735992432, loss=1.7427860498428345
I0129 22:06:53.446446 140026025305856 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.6939133405685425, loss=1.8021599054336548
I0129 22:07:27.412344 140026033698560 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.854597568511963, loss=1.7078386545181274
I0129 22:08:01.339746 140026025305856 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.7018945217132568, loss=1.6947541236877441
I0129 22:08:35.290858 140026033698560 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.8747806549072266, loss=1.8838328123092651
I0129 22:09:09.218057 140026025305856 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.8030040264129639, loss=1.6677818298339844
I0129 22:09:43.134630 140026033698560 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.8254345655441284, loss=1.7163888216018677
I0129 22:10:17.128039 140026025305856 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.7695844173431396, loss=1.810637354850769
I0129 22:10:51.088577 140026033698560 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.8394691944122314, loss=1.7664268016815186
I0129 22:11:25.042747 140026025305856 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.844792366027832, loss=1.8540033102035522
I0129 22:11:58.991044 140026033698560 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.7813990116119385, loss=1.7685502767562866
I0129 22:12:32.947325 140026025305856 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.653254508972168, loss=1.808514952659607
I0129 22:13:06.928455 140026033698560 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.6605777740478516, loss=1.794622778892517
I0129 22:13:40.914561 140026025305856 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.6339703798294067, loss=1.5621353387832642
I0129 22:14:14.889783 140026033698560 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.6399474143981934, loss=1.802996039390564
I0129 22:14:19.782331 140187804313408 spec.py:321] Evaluating on the training split.
I0129 22:14:26.050059 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 22:14:34.732577 140187804313408 spec.py:349] Evaluating on the test split.
I0129 22:14:37.400022 140187804313408 submission_runner.py:408] Time since start: 9555.60s, 	Step: 27016, 	{'train/accuracy': 0.6840720772743225, 'train/loss': 1.2421938180923462, 'validation/accuracy': 0.6232399940490723, 'validation/loss': 1.559043526649475, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.2529296875, 'test/num_examples': 10000, 'score': 9214.018256664276, 'total_duration': 9555.59879374504, 'accumulated_submission_time': 9214.018256664276, 'accumulated_eval_time': 340.03698801994324, 'accumulated_logging_time': 0.5498857498168945}
I0129 22:14:37.424835 140026050483968 logging_writer.py:48] [27016] accumulated_eval_time=340.036988, accumulated_logging_time=0.549886, accumulated_submission_time=9214.018257, global_step=27016, preemption_count=0, score=9214.018257, test/accuracy=0.501800, test/loss=2.252930, test/num_examples=10000, total_duration=9555.598794, train/accuracy=0.684072, train/loss=1.242194, validation/accuracy=0.623240, validation/loss=1.559044, validation/num_examples=50000
I0129 22:15:06.250408 140026058876672 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.863686203956604, loss=1.7028840780258179
I0129 22:15:40.193869 140026050483968 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.9217396974563599, loss=1.837660551071167
I0129 22:16:14.177988 140026058876672 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.7899656295776367, loss=1.8097853660583496
I0129 22:16:48.224709 140026050483968 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.804308295249939, loss=1.7224801778793335
I0129 22:17:22.196941 140026058876672 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.7497633695602417, loss=1.6995841264724731
I0129 22:17:56.178371 140026050483968 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.9387180805206299, loss=1.7680752277374268
I0129 22:18:30.148779 140026058876672 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.9516440629959106, loss=1.6891200542449951
I0129 22:19:04.100792 140026050483968 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.859547734260559, loss=1.8110556602478027
I0129 22:19:38.078850 140026058876672 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.7096943855285645, loss=1.7765018939971924
I0129 22:20:12.027407 140026050483968 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.7646969556808472, loss=1.7659275531768799
I0129 22:20:45.988870 140026058876672 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.7318249940872192, loss=1.7251386642456055
I0129 22:21:19.981113 140026050483968 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.640872597694397, loss=1.7874603271484375
I0129 22:21:53.943760 140026058876672 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.6546250581741333, loss=1.7007427215576172
I0129 22:22:27.943048 140026050483968 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.6819030046463013, loss=1.642286777496338
I0129 22:23:02.013767 140026058876672 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.7052251100540161, loss=1.6176670789718628
I0129 22:23:07.583164 140187804313408 spec.py:321] Evaluating on the training split.
I0129 22:23:13.884671 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 22:23:22.855752 140187804313408 spec.py:349] Evaluating on the test split.
I0129 22:23:25.539666 140187804313408 submission_runner.py:408] Time since start: 10083.74s, 	Step: 28518, 	{'train/accuracy': 0.6774553656578064, 'train/loss': 1.2737376689910889, 'validation/accuracy': 0.6197400093078613, 'validation/loss': 1.5842479467391968, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.3093740940093994, 'test/num_examples': 10000, 'score': 9724.112269163132, 'total_duration': 10083.73843884468, 'accumulated_submission_time': 9724.112269163132, 'accumulated_eval_time': 357.99346256256104, 'accumulated_logging_time': 0.5839834213256836}
I0129 22:23:25.562755 140026016913152 logging_writer.py:48] [28518] accumulated_eval_time=357.993463, accumulated_logging_time=0.583983, accumulated_submission_time=9724.112269, global_step=28518, preemption_count=0, score=9724.112269, test/accuracy=0.495700, test/loss=2.309374, test/num_examples=10000, total_duration=10083.738439, train/accuracy=0.677455, train/loss=1.273738, validation/accuracy=0.619740, validation/loss=1.584248, validation/num_examples=50000
I0129 22:23:53.668337 140026025305856 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.764260172843933, loss=1.792836308479309
I0129 22:24:27.621301 140026016913152 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.6567596197128296, loss=1.7579394578933716
I0129 22:25:01.545984 140026025305856 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.8137866258621216, loss=1.7142040729522705
I0129 22:25:35.511102 140026016913152 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.6231679916381836, loss=1.771992564201355
I0129 22:26:09.469499 140026025305856 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.9208989143371582, loss=1.6799737215042114
I0129 22:26:43.432999 140026016913152 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.6709705591201782, loss=1.721832275390625
I0129 22:27:17.356135 140026025305856 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.8394840955734253, loss=1.7376991510391235
I0129 22:27:51.290354 140026016913152 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.7288765907287598, loss=1.7580957412719727
I0129 22:28:25.252769 140026025305856 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.8409583568572998, loss=1.776593804359436
I0129 22:28:59.218508 140026016913152 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.5986276865005493, loss=1.7755948305130005
I0129 22:29:33.251341 140026025305856 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.0637407302856445, loss=1.7211997509002686
I0129 22:30:07.227368 140026016913152 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.8508849143981934, loss=1.6589980125427246
I0129 22:30:41.198696 140026025305856 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.046107292175293, loss=1.669039011001587
I0129 22:31:15.156483 140026016913152 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.0083985328674316, loss=1.7593461275100708
I0129 22:31:49.121456 140026025305856 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.090447425842285, loss=1.8040794134140015
I0129 22:31:55.726123 140187804313408 spec.py:321] Evaluating on the training split.
I0129 22:32:02.196404 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 22:32:11.061002 140187804313408 spec.py:349] Evaluating on the test split.
I0129 22:32:13.893592 140187804313408 submission_runner.py:408] Time since start: 10612.09s, 	Step: 30021, 	{'train/accuracy': 0.6818000674247742, 'train/loss': 1.2576472759246826, 'validation/accuracy': 0.6284199953079224, 'validation/loss': 1.5533267259597778, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.2645206451416016, 'test/num_examples': 10000, 'score': 10234.21173286438, 'total_duration': 10612.092364549637, 'accumulated_submission_time': 10234.21173286438, 'accumulated_eval_time': 376.1608896255493, 'accumulated_logging_time': 0.6163861751556396}
I0129 22:32:13.920840 140026042091264 logging_writer.py:48] [30021] accumulated_eval_time=376.160890, accumulated_logging_time=0.616386, accumulated_submission_time=10234.211733, global_step=30021, preemption_count=0, score=10234.211733, test/accuracy=0.503500, test/loss=2.264521, test/num_examples=10000, total_duration=10612.092365, train/accuracy=0.681800, train/loss=1.257647, validation/accuracy=0.628420, validation/loss=1.553327, validation/num_examples=50000
I0129 22:32:41.055166 140026050483968 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.6534490585327148, loss=1.6795432567596436
I0129 22:33:15.001937 140026042091264 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.549627423286438, loss=1.7507044076919556
I0129 22:33:48.979601 140026050483968 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.9006226062774658, loss=1.7641505002975464
I0129 22:34:22.928939 140026042091264 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.818581461906433, loss=1.642041563987732
I0129 22:34:56.913899 140026050483968 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.7256886959075928, loss=1.7150609493255615
I0129 22:35:30.891176 140026042091264 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.649437427520752, loss=1.7633081674575806
I0129 22:36:04.917851 140026050483968 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.712432622909546, loss=1.726860761642456
I0129 22:36:38.894661 140026042091264 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.7964978218078613, loss=1.7189383506774902
I0129 22:37:12.837209 140026050483968 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.9308072328567505, loss=1.8294317722320557
I0129 22:37:46.817094 140026042091264 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.6828587055206299, loss=1.7121083736419678
I0129 22:38:20.772702 140026050483968 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.9140430688858032, loss=1.702805757522583
I0129 22:38:54.760236 140026042091264 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.7861964702606201, loss=1.6666417121887207
I0129 22:39:28.743683 140026050483968 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.8032044172286987, loss=1.7054293155670166
I0129 22:40:02.709509 140026042091264 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.6974220275878906, loss=1.7457185983657837
I0129 22:40:36.705106 140026050483968 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.8341946601867676, loss=1.7654732465744019
I0129 22:40:43.970317 140187804313408 spec.py:321] Evaluating on the training split.
I0129 22:40:50.212706 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 22:40:59.221825 140187804313408 spec.py:349] Evaluating on the test split.
I0129 22:41:01.905073 140187804313408 submission_runner.py:408] Time since start: 11140.10s, 	Step: 31523, 	{'train/accuracy': 0.6839724183082581, 'train/loss': 1.2390429973602295, 'validation/accuracy': 0.6254400014877319, 'validation/loss': 1.5558955669403076, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.269998788833618, 'test/num_examples': 10000, 'score': 10744.19499206543, 'total_duration': 11140.10380768776, 'accumulated_submission_time': 10744.19499206543, 'accumulated_eval_time': 394.0955708026886, 'accumulated_logging_time': 0.6547572612762451}
I0129 22:41:01.936749 140026016913152 logging_writer.py:48] [31523] accumulated_eval_time=394.095571, accumulated_logging_time=0.654757, accumulated_submission_time=10744.194992, global_step=31523, preemption_count=0, score=10744.194992, test/accuracy=0.501400, test/loss=2.269999, test/num_examples=10000, total_duration=11140.103808, train/accuracy=0.683972, train/loss=1.239043, validation/accuracy=0.625440, validation/loss=1.555896, validation/num_examples=50000
I0129 22:41:28.374014 140026025305856 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.4419068098068237, loss=1.595145583152771
I0129 22:42:02.308017 140026016913152 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.5609157085418701, loss=1.651675820350647
I0129 22:42:36.225979 140026025305856 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7085260152816772, loss=1.7024942636489868
I0129 22:43:10.385153 140026016913152 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.802078366279602, loss=1.7070276737213135
I0129 22:43:44.365538 140026025305856 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.8144418001174927, loss=1.6744353771209717
I0129 22:44:18.349761 140026016913152 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.7792783975601196, loss=1.7838973999023438
I0129 22:44:52.320989 140026025305856 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.6549330949783325, loss=1.733520746231079
I0129 22:45:26.312499 140026016913152 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.6636583805084229, loss=1.6099200248718262
I0129 22:46:00.260924 140026025305856 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.056382179260254, loss=1.7829205989837646
I0129 22:46:34.243424 140026016913152 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.7998188734054565, loss=1.6411713361740112
I0129 22:47:08.195476 140026025305856 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.8208651542663574, loss=1.726116418838501
I0129 22:47:42.137963 140026016913152 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.8021434545516968, loss=1.7646619081497192
I0129 22:48:16.086452 140026025305856 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.727486252784729, loss=1.7357449531555176
I0129 22:48:50.062161 140026016913152 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.828789472579956, loss=1.6096158027648926
I0129 22:49:24.160867 140026025305856 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.8030158281326294, loss=1.6402140855789185
I0129 22:49:32.135111 140187804313408 spec.py:321] Evaluating on the training split.
I0129 22:49:38.436681 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 22:49:47.088389 140187804313408 spec.py:349] Evaluating on the test split.
I0129 22:49:49.766572 140187804313408 submission_runner.py:408] Time since start: 11667.97s, 	Step: 33025, 	{'train/accuracy': 0.7113161683082581, 'train/loss': 1.1245791912078857, 'validation/accuracy': 0.6225999593734741, 'validation/loss': 1.5649104118347168, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.3233165740966797, 'test/num_examples': 10000, 'score': 11254.327833890915, 'total_duration': 11667.965344667435, 'accumulated_submission_time': 11254.327833890915, 'accumulated_eval_time': 411.72698998451233, 'accumulated_logging_time': 0.6967041492462158}
I0129 22:49:49.790472 140026058876672 logging_writer.py:48] [33025] accumulated_eval_time=411.726990, accumulated_logging_time=0.696704, accumulated_submission_time=11254.327834, global_step=33025, preemption_count=0, score=11254.327834, test/accuracy=0.492900, test/loss=2.323317, test/num_examples=10000, total_duration=11667.965345, train/accuracy=0.711316, train/loss=1.124579, validation/accuracy=0.622600, validation/loss=1.564910, validation/num_examples=50000
I0129 22:50:15.580275 140026067269376 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.7346018552780151, loss=1.7294132709503174
I0129 22:50:49.480847 140026058876672 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.5940748453140259, loss=1.7231545448303223
I0129 22:51:23.434099 140026067269376 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.7525030374526978, loss=1.6770204305648804
I0129 22:51:57.405372 140026058876672 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.8737832307815552, loss=1.7734509706497192
I0129 22:52:31.370090 140026067269376 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.9612330198287964, loss=1.7268562316894531
I0129 22:53:05.347820 140026058876672 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.8254624605178833, loss=1.9048032760620117
I0129 22:53:39.332377 140026067269376 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.603047490119934, loss=1.5323286056518555
I0129 22:54:13.305669 140026058876672 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.8428707122802734, loss=1.759061574935913
I0129 22:54:47.289104 140026067269376 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.5349618196487427, loss=1.574281930923462
I0129 22:55:21.276026 140026058876672 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.727979063987732, loss=1.7296229600906372
I0129 22:55:55.340109 140026067269376 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.0569703578948975, loss=1.7199182510375977
I0129 22:56:29.315335 140026058876672 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.6788783073425293, loss=1.5597659349441528
I0129 22:57:03.266474 140026067269376 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.6052814722061157, loss=1.694163203239441
I0129 22:57:37.245374 140026058876672 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7254562377929688, loss=1.5937881469726562
I0129 22:58:11.225465 140026067269376 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.877310037612915, loss=1.5948810577392578
I0129 22:58:19.876216 140187804313408 spec.py:321] Evaluating on the training split.
I0129 22:58:26.140945 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 22:58:35.187731 140187804313408 spec.py:349] Evaluating on the test split.
I0129 22:58:37.869513 140187804313408 submission_runner.py:408] Time since start: 12196.07s, 	Step: 34527, 	{'train/accuracy': 0.6932995915412903, 'train/loss': 1.201788306236267, 'validation/accuracy': 0.624459981918335, 'validation/loss': 1.5534310340881348, 'validation/num_examples': 50000, 'test/accuracy': 0.4936000108718872, 'test/loss': 2.312537670135498, 'test/num_examples': 10000, 'score': 11764.347550868988, 'total_duration': 12196.06828379631, 'accumulated_submission_time': 11764.347550868988, 'accumulated_eval_time': 429.72024512290955, 'accumulated_logging_time': 0.731212854385376}
I0129 22:58:37.895196 140025891108608 logging_writer.py:48] [34527] accumulated_eval_time=429.720245, accumulated_logging_time=0.731213, accumulated_submission_time=11764.347551, global_step=34527, preemption_count=0, score=11764.347551, test/accuracy=0.493600, test/loss=2.312538, test/num_examples=10000, total_duration=12196.068284, train/accuracy=0.693300, train/loss=1.201788, validation/accuracy=0.624460, validation/loss=1.553431, validation/num_examples=50000
I0129 22:59:02.974634 140026016913152 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.6642979383468628, loss=1.6249284744262695
I0129 22:59:36.876459 140025891108608 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.7043428421020508, loss=1.680692195892334
I0129 23:00:10.845181 140026016913152 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.92540442943573, loss=1.820837378501892
I0129 23:00:44.774301 140025891108608 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.777307152748108, loss=1.7285526990890503
I0129 23:01:18.736216 140026016913152 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.777445912361145, loss=1.6478914022445679
I0129 23:01:52.616058 140025891108608 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.250487804412842, loss=1.6904664039611816
I0129 23:02:26.579533 140026016913152 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.7528915405273438, loss=1.6620333194732666
I0129 23:03:00.581582 140025891108608 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.140821933746338, loss=1.6943025588989258
I0129 23:03:34.549575 140026016913152 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.081406354904175, loss=1.6765421628952026
I0129 23:04:08.540350 140025891108608 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.6908435821533203, loss=1.6836973428726196
I0129 23:04:42.497453 140026016913152 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.744181752204895, loss=1.7817035913467407
I0129 23:05:16.410310 140025891108608 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.740273118019104, loss=1.6660641431808472
I0129 23:05:50.369523 140026016913152 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.728454828262329, loss=1.6154911518096924
I0129 23:06:24.358668 140025891108608 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.9265412092208862, loss=1.6499731540679932
I0129 23:06:58.327879 140026016913152 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.7410223484039307, loss=1.6472361087799072
I0129 23:07:07.941655 140187804313408 spec.py:321] Evaluating on the training split.
I0129 23:07:14.234687 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 23:07:23.009856 140187804313408 spec.py:349] Evaluating on the test split.
I0129 23:07:25.699675 140187804313408 submission_runner.py:408] Time since start: 12723.90s, 	Step: 36030, 	{'train/accuracy': 0.6974449753761292, 'train/loss': 1.1905690431594849, 'validation/accuracy': 0.6278799772262573, 'validation/loss': 1.5417182445526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.2813334465026855, 'test/num_examples': 10000, 'score': 12274.329077005386, 'total_duration': 12723.898445367813, 'accumulated_submission_time': 12274.329077005386, 'accumulated_eval_time': 447.4782257080078, 'accumulated_logging_time': 0.7672967910766602}
I0129 23:07:25.727918 140026058876672 logging_writer.py:48] [36030] accumulated_eval_time=447.478226, accumulated_logging_time=0.767297, accumulated_submission_time=12274.329077, global_step=36030, preemption_count=0, score=12274.329077, test/accuracy=0.502000, test/loss=2.281333, test/num_examples=10000, total_duration=12723.898445, train/accuracy=0.697445, train/loss=1.190569, validation/accuracy=0.627880, validation/loss=1.541718, validation/num_examples=50000
I0129 23:07:49.778945 140026067269376 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7671194076538086, loss=1.802748203277588
I0129 23:08:23.675469 140026058876672 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.8757145404815674, loss=1.6721696853637695
I0129 23:08:57.626913 140026067269376 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.673520803451538, loss=1.7273730039596558
I0129 23:09:31.672349 140026058876672 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.8738211393356323, loss=1.8124409914016724
I0129 23:10:05.632116 140026067269376 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.7506352663040161, loss=1.6732177734375
I0129 23:10:39.563050 140026058876672 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.3566019535064697, loss=1.756211519241333
I0129 23:11:13.514590 140026067269376 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.5937303304672241, loss=1.6365634202957153
I0129 23:11:47.474056 140026058876672 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.8080174922943115, loss=1.6465165615081787
I0129 23:12:21.434269 140026067269376 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.6140319108963013, loss=1.6515394449234009
I0129 23:12:55.417932 140026058876672 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.8459111452102661, loss=1.747437596321106
I0129 23:13:29.383899 140026067269376 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.7321377992630005, loss=1.758590817451477
I0129 23:14:03.331174 140026058876672 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.8468332290649414, loss=1.6824077367782593
I0129 23:14:37.289288 140026067269376 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.120293378829956, loss=1.676131248474121
I0129 23:15:11.231109 140026058876672 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.771649718284607, loss=1.637387752532959
I0129 23:15:45.285356 140026067269376 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.91382896900177, loss=1.7897192239761353
I0129 23:15:55.953955 140187804313408 spec.py:321] Evaluating on the training split.
I0129 23:16:02.412448 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 23:16:11.410164 140187804313408 spec.py:349] Evaluating on the test split.
I0129 23:16:14.130964 140187804313408 submission_runner.py:408] Time since start: 13252.33s, 	Step: 37533, 	{'train/accuracy': 0.7012914419174194, 'train/loss': 1.1706738471984863, 'validation/accuracy': 0.6375399827957153, 'validation/loss': 1.4896409511566162, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.2257673740386963, 'test/num_examples': 10000, 'score': 12784.488025665283, 'total_duration': 13252.32969903946, 'accumulated_submission_time': 12784.488025665283, 'accumulated_eval_time': 465.655154466629, 'accumulated_logging_time': 0.8066210746765137}
I0129 23:16:14.172090 140026033698560 logging_writer.py:48] [37533] accumulated_eval_time=465.655154, accumulated_logging_time=0.806621, accumulated_submission_time=12784.488026, global_step=37533, preemption_count=0, score=12784.488026, test/accuracy=0.514100, test/loss=2.225767, test/num_examples=10000, total_duration=13252.329699, train/accuracy=0.701291, train/loss=1.170674, validation/accuracy=0.637540, validation/loss=1.489641, validation/num_examples=50000
I0129 23:16:37.218928 140026042091264 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.8177763223648071, loss=1.6420438289642334
I0129 23:17:11.145921 140026033698560 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.0792629718780518, loss=1.717261552810669
I0129 23:17:45.038952 140026042091264 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.5834205150604248, loss=1.6763678789138794
I0129 23:18:18.990592 140026033698560 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.9359564781188965, loss=1.8214017152786255
I0129 23:18:52.932782 140026042091264 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.963760256767273, loss=1.724783182144165
I0129 23:19:26.847193 140026033698560 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.8170806169509888, loss=1.7891366481781006
I0129 23:20:00.805489 140026042091264 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.849024772644043, loss=1.6407387256622314
I0129 23:20:34.744320 140026033698560 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.949316382408142, loss=1.784031629562378
I0129 23:21:08.688672 140026042091264 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.8773040771484375, loss=1.6815106868743896
I0129 23:21:42.641772 140026033698560 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.9735368490219116, loss=1.7148873805999756
I0129 23:22:16.574395 140026042091264 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.8318212032318115, loss=1.6569586992263794
I0129 23:22:50.552674 140026033698560 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.696824550628662, loss=1.6186268329620361
I0129 23:23:24.499511 140026042091264 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.7069178819656372, loss=1.6457635164260864
I0129 23:23:58.447578 140026033698560 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.567030668258667, loss=1.6659036874771118
I0129 23:24:32.389063 140026042091264 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.9469691514968872, loss=1.5959765911102295
I0129 23:24:44.410140 140187804313408 spec.py:321] Evaluating on the training split.
I0129 23:24:50.676769 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 23:24:59.607864 140187804313408 spec.py:349] Evaluating on the test split.
I0129 23:25:02.322073 140187804313408 submission_runner.py:408] Time since start: 13780.52s, 	Step: 39037, 	{'train/accuracy': 0.6754822731018066, 'train/loss': 1.2862061262130737, 'validation/accuracy': 0.6221799850463867, 'validation/loss': 1.5718142986297607, 'validation/num_examples': 50000, 'test/accuracy': 0.49310001730918884, 'test/loss': 2.3207204341888428, 'test/num_examples': 10000, 'score': 13294.657634973526, 'total_duration': 13780.520778179169, 'accumulated_submission_time': 13294.657634973526, 'accumulated_eval_time': 483.5669913291931, 'accumulated_logging_time': 0.8619179725646973}
I0129 23:25:02.351498 140025891108608 logging_writer.py:48] [39037] accumulated_eval_time=483.566991, accumulated_logging_time=0.861918, accumulated_submission_time=13294.657635, global_step=39037, preemption_count=0, score=13294.657635, test/accuracy=0.493100, test/loss=2.320720, test/num_examples=10000, total_duration=13780.520778, train/accuracy=0.675482, train/loss=1.286206, validation/accuracy=0.622180, validation/loss=1.571814, validation/num_examples=50000
I0129 23:25:24.039601 140026016913152 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.8752849102020264, loss=1.6552948951721191
I0129 23:25:57.954349 140025891108608 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.9281560182571411, loss=1.6287364959716797
I0129 23:26:31.943948 140026016913152 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.8084940910339355, loss=1.6737868785858154
I0129 23:27:05.888578 140025891108608 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.9740527868270874, loss=1.7349395751953125
I0129 23:27:39.902542 140026016913152 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.9180697202682495, loss=1.6321570873260498
I0129 23:28:13.870171 140025891108608 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7554224729537964, loss=1.5615475177764893
I0129 23:28:47.869268 140026016913152 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.977252721786499, loss=1.7183774709701538
I0129 23:29:21.931040 140025891108608 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.9890459775924683, loss=1.6974780559539795
I0129 23:29:55.882071 140026016913152 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.809295654296875, loss=1.6344029903411865
I0129 23:30:29.825266 140025891108608 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.7539911270141602, loss=1.6939855813980103
I0129 23:31:03.801392 140026016913152 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.5966376066207886, loss=1.6104072332382202
I0129 23:31:37.772670 140025891108608 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.7229628562927246, loss=1.6352614164352417
I0129 23:32:11.722592 140026016913152 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.8573799133300781, loss=1.6951048374176025
I0129 23:32:45.694632 140025891108608 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.9804470539093018, loss=1.6981589794158936
I0129 23:33:19.620407 140026016913152 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.2938482761383057, loss=1.6840133666992188
I0129 23:33:32.336685 140187804313408 spec.py:321] Evaluating on the training split.
I0129 23:33:38.641045 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 23:33:47.613369 140187804313408 spec.py:349] Evaluating on the test split.
I0129 23:33:50.317904 140187804313408 submission_runner.py:408] Time since start: 14308.52s, 	Step: 40539, 	{'train/accuracy': 0.7030452489852905, 'train/loss': 1.1629226207733154, 'validation/accuracy': 0.6412999629974365, 'validation/loss': 1.4571741819381714, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.200263738632202, 'test/num_examples': 10000, 'score': 13804.57847905159, 'total_duration': 14308.516256809235, 'accumulated_submission_time': 13804.57847905159, 'accumulated_eval_time': 501.5477590560913, 'accumulated_logging_time': 0.9012980461120605}
I0129 23:33:50.344968 140025891108608 logging_writer.py:48] [40539] accumulated_eval_time=501.547759, accumulated_logging_time=0.901298, accumulated_submission_time=13804.578479, global_step=40539, preemption_count=0, score=13804.578479, test/accuracy=0.514100, test/loss=2.200264, test/num_examples=10000, total_duration=14308.516257, train/accuracy=0.703045, train/loss=1.162923, validation/accuracy=0.641300, validation/loss=1.457174, validation/num_examples=50000
I0129 23:34:11.389524 140026042091264 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.5993165969848633, loss=1.5778446197509766
I0129 23:34:45.306789 140025891108608 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.7729055881500244, loss=1.639240026473999
I0129 23:35:19.245917 140026042091264 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6954885721206665, loss=1.7494230270385742
I0129 23:35:53.265602 140025891108608 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.7230417728424072, loss=1.689655065536499
I0129 23:36:27.173961 140026042091264 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.8199660778045654, loss=1.5989751815795898
I0129 23:37:01.133878 140025891108608 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7272650003433228, loss=1.7342023849487305
I0129 23:37:35.073296 140026042091264 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.7465221881866455, loss=1.5807287693023682
I0129 23:38:09.044843 140025891108608 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.894403100013733, loss=1.67174232006073
I0129 23:38:42.993803 140026042091264 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.753183126449585, loss=1.6200224161148071
I0129 23:39:16.944252 140025891108608 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.7681899070739746, loss=1.6565183401107788
I0129 23:39:50.916681 140026042091264 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.8935507535934448, loss=1.6011414527893066
I0129 23:40:24.885889 140025891108608 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.7728708982467651, loss=1.5958092212677002
I0129 23:40:58.836812 140026042091264 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.0679409503936768, loss=1.6319117546081543
I0129 23:41:32.799257 140025891108608 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.7384647130966187, loss=1.7117233276367188
I0129 23:42:06.956985 140026042091264 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7543927431106567, loss=1.5621377229690552
I0129 23:42:20.328536 140187804313408 spec.py:321] Evaluating on the training split.
I0129 23:42:26.586677 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 23:42:35.331499 140187804313408 spec.py:349] Evaluating on the test split.
I0129 23:42:38.021033 140187804313408 submission_runner.py:408] Time since start: 14836.22s, 	Step: 42041, 	{'train/accuracy': 0.7258250713348389, 'train/loss': 1.0648127794265747, 'validation/accuracy': 0.6235199570655823, 'validation/loss': 1.5412627458572388, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.3147168159484863, 'test/num_examples': 10000, 'score': 14314.496087789536, 'total_duration': 14836.219805955887, 'accumulated_submission_time': 14314.496087789536, 'accumulated_eval_time': 519.2402155399323, 'accumulated_logging_time': 0.9375874996185303}
I0129 23:42:38.049651 140026025305856 logging_writer.py:48] [42041] accumulated_eval_time=519.240216, accumulated_logging_time=0.937587, accumulated_submission_time=14314.496088, global_step=42041, preemption_count=0, score=14314.496088, test/accuracy=0.497600, test/loss=2.314717, test/num_examples=10000, total_duration=14836.219806, train/accuracy=0.725825, train/loss=1.064813, validation/accuracy=0.623520, validation/loss=1.541263, validation/num_examples=50000
I0129 23:42:58.411222 140026033698560 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.9260344505310059, loss=1.6796438694000244
I0129 23:43:32.266074 140026025305856 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.887500286102295, loss=1.7031351327896118
I0129 23:44:06.255761 140026033698560 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.7415567636489868, loss=1.64593505859375
I0129 23:44:40.199589 140026025305856 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.999936580657959, loss=1.649625301361084
I0129 23:45:14.148876 140026033698560 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.7752492427825928, loss=1.6993513107299805
I0129 23:45:48.109673 140026025305856 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.8488519191741943, loss=1.605492353439331
I0129 23:46:22.065675 140026033698560 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.9597657918930054, loss=1.65081787109375
I0129 23:46:56.049327 140026025305856 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.9865928888320923, loss=1.5687857866287231
I0129 23:47:30.031246 140026033698560 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.7951964139938354, loss=1.5798934698104858
I0129 23:48:04.004478 140026025305856 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.7785416841506958, loss=1.5982673168182373
I0129 23:48:38.091565 140026033698560 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.9240624904632568, loss=1.6223630905151367
I0129 23:49:12.064625 140026025305856 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.6075429916381836, loss=1.555484414100647
I0129 23:49:45.994484 140026033698560 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.6603636741638184, loss=1.681387186050415
I0129 23:50:19.955007 140026025305856 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.7825812101364136, loss=1.710773229598999
I0129 23:50:53.931427 140026033698560 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.8126821517944336, loss=1.7143959999084473
I0129 23:51:08.328459 140187804313408 spec.py:321] Evaluating on the training split.
I0129 23:51:14.604382 140187804313408 spec.py:333] Evaluating on the validation split.
I0129 23:51:23.562484 140187804313408 spec.py:349] Evaluating on the test split.
I0129 23:51:26.201300 140187804313408 submission_runner.py:408] Time since start: 15364.40s, 	Step: 43544, 	{'train/accuracy': 0.7102598547935486, 'train/loss': 1.1271930932998657, 'validation/accuracy': 0.6373999714851379, 'validation/loss': 1.4905970096588135, 'validation/num_examples': 50000, 'test/accuracy': 0.5039000511169434, 'test/loss': 2.2426929473876953, 'test/num_examples': 10000, 'score': 14824.710835456848, 'total_duration': 15364.400060892105, 'accumulated_submission_time': 14824.710835456848, 'accumulated_eval_time': 537.1130058765411, 'accumulated_logging_time': 0.9754059314727783}
I0129 23:51:26.230089 140026025305856 logging_writer.py:48] [43544] accumulated_eval_time=537.113006, accumulated_logging_time=0.975406, accumulated_submission_time=14824.710835, global_step=43544, preemption_count=0, score=14824.710835, test/accuracy=0.503900, test/loss=2.242693, test/num_examples=10000, total_duration=15364.400061, train/accuracy=0.710260, train/loss=1.127193, validation/accuracy=0.637400, validation/loss=1.490597, validation/num_examples=50000
I0129 23:51:45.552212 140026042091264 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.029738426208496, loss=1.642246127128601
I0129 23:52:19.420060 140026025305856 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.7585046291351318, loss=1.586827039718628
I0129 23:52:53.342821 140026042091264 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.7830796241760254, loss=1.585390567779541
I0129 23:53:27.281968 140026025305856 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7878073453903198, loss=1.7030855417251587
I0129 23:54:01.233453 140026042091264 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.1993768215179443, loss=1.6688977479934692
I0129 23:54:35.171946 140026025305856 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.9383565187454224, loss=1.6638094186782837
I0129 23:55:09.137921 140026042091264 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.7859374284744263, loss=1.566475749015808
I0129 23:55:43.159584 140026025305856 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.8387740850448608, loss=1.5980796813964844
I0129 23:56:17.100343 140026042091264 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.0412440299987793, loss=1.609905481338501
I0129 23:56:50.996728 140026025305856 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.9741926193237305, loss=1.58275306224823
I0129 23:57:24.940409 140026042091264 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.755258321762085, loss=1.5679341554641724
I0129 23:57:58.900769 140026025305856 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.7201732397079468, loss=1.584568738937378
I0129 23:58:32.838303 140026042091264 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.7011226415634155, loss=1.5680135488510132
I0129 23:59:06.771331 140026025305856 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.914284348487854, loss=1.6442347764968872
I0129 23:59:40.740424 140026042091264 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.8578866720199585, loss=1.6128407716751099
I0129 23:59:56.508869 140187804313408 spec.py:321] Evaluating on the training split.
I0130 00:00:02.935373 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 00:00:11.768277 140187804313408 spec.py:349] Evaluating on the test split.
I0130 00:00:14.455548 140187804313408 submission_runner.py:408] Time since start: 15892.65s, 	Step: 45048, 	{'train/accuracy': 0.7119140625, 'train/loss': 1.117753505706787, 'validation/accuracy': 0.6432799696922302, 'validation/loss': 1.4690630435943604, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.19850754737854, 'test/num_examples': 10000, 'score': 15334.921788215637, 'total_duration': 15892.65364933014, 'accumulated_submission_time': 15334.921788215637, 'accumulated_eval_time': 555.0589742660522, 'accumulated_logging_time': 1.0168430805206299}
I0130 00:00:14.493733 140026033698560 logging_writer.py:48] [45048] accumulated_eval_time=555.058974, accumulated_logging_time=1.016843, accumulated_submission_time=15334.921788, global_step=45048, preemption_count=0, score=15334.921788, test/accuracy=0.516500, test/loss=2.198508, test/num_examples=10000, total_duration=15892.653649, train/accuracy=0.711914, train/loss=1.117754, validation/accuracy=0.643280, validation/loss=1.469063, validation/num_examples=50000
I0130 00:00:32.475581 140026050483968 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.2530572414398193, loss=1.782577395439148
I0130 00:01:06.370089 140026033698560 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.7592430114746094, loss=1.674986720085144
I0130 00:01:40.343792 140026050483968 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.043891429901123, loss=1.6599944829940796
I0130 00:02:14.392258 140026033698560 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.915774941444397, loss=1.6328563690185547
I0130 00:02:48.327580 140026050483968 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.95716392993927, loss=1.6148734092712402
I0130 00:03:22.278902 140026033698560 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.746503233909607, loss=1.632645845413208
I0130 00:03:56.235489 140026050483968 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.8827130794525146, loss=1.582169771194458
I0130 00:04:30.197011 140026033698560 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.0158028602600098, loss=1.702285647392273
I0130 00:05:04.168497 140026050483968 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.8868802785873413, loss=1.746946930885315
I0130 00:05:38.121359 140026033698560 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.681289553642273, loss=1.6217916011810303
I0130 00:06:12.099661 140026050483968 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.879227876663208, loss=1.5754915475845337
I0130 00:06:46.006572 140026033698560 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.8042272329330444, loss=1.6387091875076294
I0130 00:07:19.961316 140026050483968 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.8259857892990112, loss=1.6025627851486206
I0130 00:07:53.944647 140026033698560 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.6962271928787231, loss=1.5594937801361084
I0130 00:08:27.968304 140026050483968 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.005819320678711, loss=1.6221553087234497
I0130 00:08:44.482788 140187804313408 spec.py:321] Evaluating on the training split.
I0130 00:08:50.914099 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 00:08:59.776033 140187804313408 spec.py:349] Evaluating on the test split.
I0130 00:09:02.347668 140187804313408 submission_runner.py:408] Time since start: 16420.55s, 	Step: 46549, 	{'train/accuracy': 0.6991389989852905, 'train/loss': 1.1732345819473267, 'validation/accuracy': 0.6344599723815918, 'validation/loss': 1.5220282077789307, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2613413333892822, 'test/num_examples': 10000, 'score': 15844.842143058777, 'total_duration': 16420.54642868042, 'accumulated_submission_time': 15844.842143058777, 'accumulated_eval_time': 572.9238193035126, 'accumulated_logging_time': 1.068709373474121}
I0130 00:09:02.375232 140026025305856 logging_writer.py:48] [46549] accumulated_eval_time=572.923819, accumulated_logging_time=1.068709, accumulated_submission_time=15844.842143, global_step=46549, preemption_count=0, score=15844.842143, test/accuracy=0.510100, test/loss=2.261341, test/num_examples=10000, total_duration=16420.546429, train/accuracy=0.699139, train/loss=1.173235, validation/accuracy=0.634460, validation/loss=1.522028, validation/num_examples=50000
I0130 00:09:19.988370 140026033698560 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.759858250617981, loss=1.6107538938522339
I0130 00:09:53.880524 140026025305856 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.8701213598251343, loss=1.6240966320037842
I0130 00:10:27.793919 140026033698560 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.8737927675247192, loss=1.60158109664917
I0130 00:11:01.713289 140026025305856 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.0637974739074707, loss=1.6546311378479004
I0130 00:11:35.668418 140026033698560 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.015289306640625, loss=1.660016655921936
I0130 00:12:09.614404 140026025305856 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7877867221832275, loss=1.602965235710144
I0130 00:12:43.571633 140026033698560 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.827833890914917, loss=1.6257758140563965
I0130 00:13:17.508325 140026025305856 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8022124767303467, loss=1.5479220151901245
I0130 00:13:51.471273 140026033698560 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7878717184066772, loss=1.5213606357574463
I0130 00:14:25.391401 140026025305856 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.0804033279418945, loss=1.6594107151031494
I0130 00:14:59.333609 140026033698560 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.936826467514038, loss=1.5913985967636108
I0130 00:15:33.365393 140026025305856 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.8182079792022705, loss=1.583445429801941
I0130 00:16:07.317620 140026033698560 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.9288886785507202, loss=1.5893428325653076
I0130 00:16:41.236764 140026025305856 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.943415880203247, loss=1.6509140729904175
I0130 00:17:15.181386 140026033698560 logging_writer.py:48] [48000] global_step=48000, grad_norm=2.161651134490967, loss=1.6674972772598267
I0130 00:17:32.638705 140187804313408 spec.py:321] Evaluating on the training split.
I0130 00:17:38.928856 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 00:17:47.633761 140187804313408 spec.py:349] Evaluating on the test split.
I0130 00:17:50.221169 140187804313408 submission_runner.py:408] Time since start: 16948.42s, 	Step: 48053, 	{'train/accuracy': 0.7051379084587097, 'train/loss': 1.1431678533554077, 'validation/accuracy': 0.6401799917221069, 'validation/loss': 1.4874356985092163, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.180363893508911, 'test/num_examples': 10000, 'score': 16355.0407371521, 'total_duration': 16948.419927835464, 'accumulated_submission_time': 16355.0407371521, 'accumulated_eval_time': 590.5062322616577, 'accumulated_logging_time': 1.105797290802002}
I0130 00:17:50.246953 140025882715904 logging_writer.py:48] [48053] accumulated_eval_time=590.506232, accumulated_logging_time=1.105797, accumulated_submission_time=16355.040737, global_step=48053, preemption_count=0, score=16355.040737, test/accuracy=0.520200, test/loss=2.180364, test/num_examples=10000, total_duration=16948.419928, train/accuracy=0.705138, train/loss=1.143168, validation/accuracy=0.640180, validation/loss=1.487436, validation/num_examples=50000
I0130 00:18:06.538309 140025891108608 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.8393548727035522, loss=1.63578200340271
I0130 00:18:40.457202 140025882715904 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7603086233139038, loss=1.5267910957336426
I0130 00:19:14.333599 140025891108608 logging_writer.py:48] [48300] global_step=48300, grad_norm=2.1262600421905518, loss=1.7354097366333008
I0130 00:19:48.287669 140025882715904 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.8818600177764893, loss=1.5997856855392456
I0130 00:20:22.265022 140025891108608 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.9143987894058228, loss=1.605011224746704
I0130 00:20:56.216010 140025882715904 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8048509359359741, loss=1.6218703985214233
I0130 00:21:30.141425 140025891108608 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.8927544355392456, loss=1.6111350059509277
I0130 00:22:04.212205 140025882715904 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.8118916749954224, loss=1.647437572479248
I0130 00:22:38.182722 140025891108608 logging_writer.py:48] [48900] global_step=48900, grad_norm=2.052830696105957, loss=1.6292548179626465
I0130 00:23:12.134234 140025882715904 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.784828782081604, loss=1.5617059469223022
I0130 00:23:46.073534 140025891108608 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.9088404178619385, loss=1.5972214937210083
I0130 00:24:20.022557 140025882715904 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.8299624919891357, loss=1.5616832971572876
I0130 00:24:53.981620 140025891108608 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.6431050300598145, loss=1.6265101432800293
I0130 00:25:27.951210 140025882715904 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.7272270917892456, loss=1.5144984722137451
I0130 00:26:01.910837 140025891108608 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.749617576599121, loss=1.472123384475708
I0130 00:26:20.405399 140187804313408 spec.py:321] Evaluating on the training split.
I0130 00:26:26.681196 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 00:26:35.487567 140187804313408 spec.py:349] Evaluating on the test split.
I0130 00:26:38.198507 140187804313408 submission_runner.py:408] Time since start: 17476.40s, 	Step: 49556, 	{'train/accuracy': 0.6994977593421936, 'train/loss': 1.168281078338623, 'validation/accuracy': 0.638759970664978, 'validation/loss': 1.4861197471618652, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2292864322662354, 'test/num_examples': 10000, 'score': 16865.1335606575, 'total_duration': 17476.397280454636, 'accumulated_submission_time': 16865.1335606575, 'accumulated_eval_time': 608.299302816391, 'accumulated_logging_time': 1.1421661376953125}
I0130 00:26:38.225018 140026050483968 logging_writer.py:48] [49556] accumulated_eval_time=608.299303, accumulated_logging_time=1.142166, accumulated_submission_time=16865.133561, global_step=49556, preemption_count=0, score=16865.133561, test/accuracy=0.505400, test/loss=2.229286, test/num_examples=10000, total_duration=17476.397280, train/accuracy=0.699498, train/loss=1.168281, validation/accuracy=0.638760, validation/loss=1.486120, validation/num_examples=50000
I0130 00:26:53.481139 140026058876672 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.7966620922088623, loss=1.59577214717865
I0130 00:27:27.332365 140026050483968 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.8921133279800415, loss=1.5713880062103271
I0130 00:28:01.277408 140026058876672 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8959355354309082, loss=1.592685580253601
I0130 00:28:35.268422 140026050483968 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.893463373184204, loss=1.6537915468215942
I0130 00:29:09.169096 140026058876672 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7499843835830688, loss=1.6554956436157227
I0130 00:29:43.123880 140026050483968 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8444610834121704, loss=1.6419007778167725
I0130 00:30:17.062730 140026058876672 logging_writer.py:48] [50200] global_step=50200, grad_norm=2.1244232654571533, loss=1.6118640899658203
I0130 00:30:50.969016 140026050483968 logging_writer.py:48] [50300] global_step=50300, grad_norm=2.0442397594451904, loss=1.5114126205444336
I0130 00:31:24.908713 140026058876672 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.739167332649231, loss=1.5245281457901
I0130 00:31:58.873338 140026050483968 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.91135835647583, loss=1.6324303150177002
I0130 00:32:32.768523 140026058876672 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.836297631263733, loss=1.6246002912521362
I0130 00:33:06.712461 140026050483968 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.8578956127166748, loss=1.5616018772125244
I0130 00:33:40.684502 140026058876672 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.1469016075134277, loss=1.7657042741775513
I0130 00:34:14.639779 140026050483968 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.77066969871521, loss=1.5765910148620605
I0130 00:34:48.665756 140026058876672 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.8419702053070068, loss=1.6388791799545288
I0130 00:35:08.486215 140187804313408 spec.py:321] Evaluating on the training split.
I0130 00:35:14.901948 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 00:35:23.954948 140187804313408 spec.py:349] Evaluating on the test split.
I0130 00:35:26.636400 140187804313408 submission_runner.py:408] Time since start: 18004.84s, 	Step: 51060, 	{'train/accuracy': 0.7434629797935486, 'train/loss': 0.9698396325111389, 'validation/accuracy': 0.6350199580192566, 'validation/loss': 1.5050801038742065, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.2390754222869873, 'test/num_examples': 10000, 'score': 17375.330893039703, 'total_duration': 18004.835172891617, 'accumulated_submission_time': 17375.330893039703, 'accumulated_eval_time': 626.4494802951813, 'accumulated_logging_time': 1.178267478942871}
I0130 00:35:26.666798 140025891108608 logging_writer.py:48] [51060] accumulated_eval_time=626.449480, accumulated_logging_time=1.178267, accumulated_submission_time=17375.330893, global_step=51060, preemption_count=0, score=17375.330893, test/accuracy=0.508000, test/loss=2.239075, test/num_examples=10000, total_duration=18004.835173, train/accuracy=0.743463, train/loss=0.969840, validation/accuracy=0.635020, validation/loss=1.505080, validation/num_examples=50000
I0130 00:35:40.573559 140026016913152 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.8359612226486206, loss=1.7093294858932495
I0130 00:36:14.457063 140025891108608 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.0542876720428467, loss=1.613602638244629
I0130 00:36:48.343962 140026016913152 logging_writer.py:48] [51300] global_step=51300, grad_norm=2.074580430984497, loss=1.6870033740997314
I0130 00:37:22.310001 140025891108608 logging_writer.py:48] [51400] global_step=51400, grad_norm=2.2562785148620605, loss=1.5357857942581177
I0130 00:37:56.271882 140026016913152 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.958191990852356, loss=1.5666837692260742
I0130 00:38:30.201879 140025891108608 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.9573408365249634, loss=1.566800594329834
I0130 00:39:04.141185 140026016913152 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.6650480031967163, loss=1.6329169273376465
I0130 00:39:38.076920 140025891108608 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.7443387508392334, loss=1.6147993803024292
I0130 00:40:12.044267 140026016913152 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.1087162494659424, loss=1.68323814868927
I0130 00:40:45.996161 140025891108608 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.8061984777450562, loss=1.55776047706604
I0130 00:41:20.016344 140026016913152 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8612346649169922, loss=1.633187174797058
I0130 00:41:53.964157 140025891108608 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.9479535818099976, loss=1.6300286054611206
I0130 00:42:27.904056 140026016913152 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.694337248802185, loss=1.5902361869812012
I0130 00:43:01.854976 140025891108608 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.8486541509628296, loss=1.5777064561843872
I0130 00:43:35.811819 140026016913152 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.81587553024292, loss=1.558024287223816
I0130 00:43:56.669553 140187804313408 spec.py:321] Evaluating on the training split.
I0130 00:44:03.092998 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 00:44:11.875177 140187804313408 spec.py:349] Evaluating on the test split.
I0130 00:44:14.584921 140187804313408 submission_runner.py:408] Time since start: 18532.78s, 	Step: 52563, 	{'train/accuracy': 0.7270607352256775, 'train/loss': 1.0618703365325928, 'validation/accuracy': 0.6416199803352356, 'validation/loss': 1.4836870431900024, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.173795223236084, 'test/num_examples': 10000, 'score': 17885.269745588303, 'total_duration': 18532.783682346344, 'accumulated_submission_time': 17885.269745588303, 'accumulated_eval_time': 644.3647968769073, 'accumulated_logging_time': 1.2180454730987549}
I0130 00:44:14.611855 140025891108608 logging_writer.py:48] [52563] accumulated_eval_time=644.364797, accumulated_logging_time=1.218045, accumulated_submission_time=17885.269746, global_step=52563, preemption_count=0, score=17885.269746, test/accuracy=0.517700, test/loss=2.173795, test/num_examples=10000, total_duration=18532.783682, train/accuracy=0.727061, train/loss=1.061870, validation/accuracy=0.641620, validation/loss=1.483687, validation/num_examples=50000
I0130 00:44:27.508560 140026042091264 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.9018914699554443, loss=1.6023004055023193
I0130 00:45:01.395583 140025891108608 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.86399507522583, loss=1.587883472442627
I0130 00:45:35.271178 140026042091264 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8698476552963257, loss=1.5749244689941406
I0130 00:46:09.227965 140025891108608 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.8959665298461914, loss=1.6948169469833374
I0130 00:46:43.171019 140026042091264 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8424272537231445, loss=1.6656274795532227
I0130 00:47:17.081517 140025891108608 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.8179019689559937, loss=1.5469093322753906
I0130 00:47:51.103684 140026042091264 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.7709975242614746, loss=1.6500442028045654
I0130 00:48:25.036669 140025891108608 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.0677387714385986, loss=1.594921350479126
I0130 00:48:58.986209 140026042091264 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.9355201721191406, loss=1.634293794631958
I0130 00:49:32.885584 140025891108608 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.0254158973693848, loss=1.573713779449463
I0130 00:50:06.828991 140026042091264 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.8588539361953735, loss=1.6436572074890137
I0130 00:50:40.743178 140025891108608 logging_writer.py:48] [53700] global_step=53700, grad_norm=2.141897201538086, loss=1.6933952569961548
I0130 00:51:14.648526 140026042091264 logging_writer.py:48] [53800] global_step=53800, grad_norm=2.1281204223632812, loss=1.5767289400100708
I0130 00:51:48.582538 140025891108608 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.893549919128418, loss=1.6105549335479736
I0130 00:52:22.512694 140026042091264 logging_writer.py:48] [54000] global_step=54000, grad_norm=2.0347518920898438, loss=1.5927181243896484
I0130 00:52:44.748412 140187804313408 spec.py:321] Evaluating on the training split.
I0130 00:52:51.048080 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 00:52:59.926042 140187804313408 spec.py:349] Evaluating on the test split.
I0130 00:53:02.617327 140187804313408 submission_runner.py:408] Time since start: 19060.82s, 	Step: 54067, 	{'train/accuracy': 0.70316481590271, 'train/loss': 1.1438255310058594, 'validation/accuracy': 0.6356399655342102, 'validation/loss': 1.5182785987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.5090000033378601, 'test/loss': 2.266648769378662, 'test/num_examples': 10000, 'score': 18395.33567595482, 'total_duration': 19060.81605863571, 'accumulated_submission_time': 18395.33567595482, 'accumulated_eval_time': 662.2336344718933, 'accumulated_logging_time': 1.2604291439056396}
I0130 00:53:02.652022 140026016913152 logging_writer.py:48] [54067] accumulated_eval_time=662.233634, accumulated_logging_time=1.260429, accumulated_submission_time=18395.335676, global_step=54067, preemption_count=0, score=18395.335676, test/accuracy=0.509000, test/loss=2.266649, test/num_examples=10000, total_duration=19060.816059, train/accuracy=0.703165, train/loss=1.143826, validation/accuracy=0.635640, validation/loss=1.518279, validation/num_examples=50000
I0130 00:53:14.203166 140026025305856 logging_writer.py:48] [54100] global_step=54100, grad_norm=2.014765501022339, loss=1.6404601335525513
I0130 00:53:48.091382 140026016913152 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.832613468170166, loss=1.5284717082977295
I0130 00:54:22.037242 140026025305856 logging_writer.py:48] [54300] global_step=54300, grad_norm=2.145308017730713, loss=1.599613070487976
I0130 00:54:56.064943 140026016913152 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.912187099456787, loss=1.664425253868103
I0130 00:55:30.007266 140026025305856 logging_writer.py:48] [54500] global_step=54500, grad_norm=2.0020556449890137, loss=1.5806440114974976
I0130 00:56:03.973748 140026016913152 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.14077091217041, loss=1.6223621368408203
I0130 00:56:37.907751 140026025305856 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.8904378414154053, loss=1.583033800125122
I0130 00:57:11.865504 140026016913152 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.8695731163024902, loss=1.6087877750396729
I0130 00:57:45.818715 140026025305856 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.915224313735962, loss=1.6591439247131348
I0130 00:58:19.763037 140026016913152 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.033234119415283, loss=1.597145438194275
I0130 00:58:53.678608 140026025305856 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.7837624549865723, loss=1.6051914691925049
I0130 00:59:27.636909 140026016913152 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.991176962852478, loss=1.5346214771270752
I0130 01:00:01.606597 140026025305856 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.9492548704147339, loss=1.5384521484375
I0130 01:00:35.568205 140026016913152 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8918002843856812, loss=1.544538140296936
I0130 01:01:09.600075 140026025305856 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.950178623199463, loss=1.60377037525177
I0130 01:01:32.847650 140187804313408 spec.py:321] Evaluating on the training split.
I0130 01:01:39.200360 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 01:01:48.209443 140187804313408 spec.py:349] Evaluating on the test split.
I0130 01:01:50.866138 140187804313408 submission_runner.py:408] Time since start: 19589.06s, 	Step: 55570, 	{'train/accuracy': 0.7121531963348389, 'train/loss': 1.1044635772705078, 'validation/accuracy': 0.6428200006484985, 'validation/loss': 1.4710193872451782, 'validation/num_examples': 50000, 'test/accuracy': 0.5169000029563904, 'test/loss': 2.2187652587890625, 'test/num_examples': 10000, 'score': 18905.46354651451, 'total_duration': 19589.064910411835, 'accumulated_submission_time': 18905.46354651451, 'accumulated_eval_time': 680.2520830631256, 'accumulated_logging_time': 1.3046739101409912}
I0130 01:01:50.894119 140026042091264 logging_writer.py:48] [55570] accumulated_eval_time=680.252083, accumulated_logging_time=1.304674, accumulated_submission_time=18905.463547, global_step=55570, preemption_count=0, score=18905.463547, test/accuracy=0.516900, test/loss=2.218765, test/num_examples=10000, total_duration=19589.064910, train/accuracy=0.712153, train/loss=1.104464, validation/accuracy=0.642820, validation/loss=1.471019, validation/num_examples=50000
I0130 01:02:01.405671 140026050483968 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.0398480892181396, loss=1.5896477699279785
I0130 01:02:35.274977 140026042091264 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.9225895404815674, loss=1.656467080116272
I0130 01:03:09.159116 140026050483968 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.9655157327651978, loss=1.5756956338882446
I0130 01:03:43.111708 140026042091264 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.9653576612472534, loss=1.6147164106369019
I0130 01:04:17.042270 140026050483968 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.0916354656219482, loss=1.549412488937378
I0130 01:04:50.963969 140026042091264 logging_writer.py:48] [56100] global_step=56100, grad_norm=2.019289255142212, loss=1.693800687789917
I0130 01:05:24.910155 140026050483968 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.9337354898452759, loss=1.6410188674926758
I0130 01:05:58.847790 140026042091264 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.698304533958435, loss=1.5449419021606445
I0130 01:06:32.796492 140026050483968 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.903052806854248, loss=1.5775216817855835
I0130 01:07:06.737042 140026042091264 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.9613189697265625, loss=1.606999397277832
I0130 01:07:40.736745 140026050483968 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9116483926773071, loss=1.56076979637146
I0130 01:08:14.678360 140026042091264 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.966869831085205, loss=1.561104655265808
I0130 01:08:48.619389 140026050483968 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.8948206901550293, loss=1.640023112297058
I0130 01:09:22.553826 140026042091264 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8318352699279785, loss=1.6649361848831177
I0130 01:09:56.492088 140026050483968 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.0161995887756348, loss=1.6594442129135132
I0130 01:10:21.070854 140187804313408 spec.py:321] Evaluating on the training split.
I0130 01:10:27.357234 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 01:10:36.000707 140187804313408 spec.py:349] Evaluating on the test split.
I0130 01:10:38.704065 140187804313408 submission_runner.py:408] Time since start: 20116.90s, 	Step: 57074, 	{'train/accuracy': 0.7025071382522583, 'train/loss': 1.1650789976119995, 'validation/accuracy': 0.6407999992370605, 'validation/loss': 1.4790819883346558, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.253450393676758, 'test/num_examples': 10000, 'score': 19415.575752735138, 'total_duration': 20116.90283894539, 'accumulated_submission_time': 19415.575752735138, 'accumulated_eval_time': 697.8852643966675, 'accumulated_logging_time': 1.3421645164489746}
I0130 01:10:38.734936 140026033698560 logging_writer.py:48] [57074] accumulated_eval_time=697.885264, accumulated_logging_time=1.342165, accumulated_submission_time=19415.575753, global_step=57074, preemption_count=0, score=19415.575753, test/accuracy=0.510300, test/loss=2.253450, test/num_examples=10000, total_duration=20116.902839, train/accuracy=0.702507, train/loss=1.165079, validation/accuracy=0.640800, validation/loss=1.479082, validation/num_examples=50000
I0130 01:10:47.863718 140026067269376 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.9443628787994385, loss=1.5667036771774292
I0130 01:11:21.765037 140026033698560 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.624176025390625, loss=1.4626061916351318
I0130 01:11:55.669911 140026067269376 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.051039218902588, loss=1.5993889570236206
I0130 01:12:29.622514 140026033698560 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.90509831905365, loss=1.6354622840881348
I0130 01:13:03.566478 140026067269376 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.9783263206481934, loss=1.6753835678100586
I0130 01:13:37.514687 140026033698560 logging_writer.py:48] [57600] global_step=57600, grad_norm=2.051938533782959, loss=1.5260004997253418
I0130 01:14:11.532241 140026067269376 logging_writer.py:48] [57700] global_step=57700, grad_norm=2.33296537399292, loss=1.629947543144226
I0130 01:14:45.472959 140026033698560 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8835575580596924, loss=1.5283480882644653
I0130 01:15:19.434720 140026067269376 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8295481204986572, loss=1.5902866125106812
I0130 01:15:53.378321 140026033698560 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.005728244781494, loss=1.5938018560409546
I0130 01:16:27.321091 140026067269376 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.052680253982544, loss=1.7023932933807373
I0130 01:17:01.244103 140026033698560 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.9283775091171265, loss=1.700890302658081
I0130 01:17:35.199292 140026067269376 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.783174753189087, loss=1.5804572105407715
I0130 01:18:09.168291 140026033698560 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.1096792221069336, loss=1.5013773441314697
I0130 01:18:43.121926 140026067269376 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.93916916847229, loss=1.579086422920227
I0130 01:19:08.706645 140187804313408 spec.py:321] Evaluating on the training split.
I0130 01:19:15.654491 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 01:19:24.589491 140187804313408 spec.py:349] Evaluating on the test split.
I0130 01:19:27.251006 140187804313408 submission_runner.py:408] Time since start: 20645.45s, 	Step: 58577, 	{'train/accuracy': 0.7065728306770325, 'train/loss': 1.1429468393325806, 'validation/accuracy': 0.6433799862861633, 'validation/loss': 1.4730268716812134, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.2345850467681885, 'test/num_examples': 10000, 'score': 19925.48170900345, 'total_duration': 20645.44976592064, 'accumulated_submission_time': 19925.48170900345, 'accumulated_eval_time': 716.4295771121979, 'accumulated_logging_time': 1.383310317993164}
I0130 01:19:27.287045 140025882715904 logging_writer.py:48] [58577] accumulated_eval_time=716.429577, accumulated_logging_time=1.383310, accumulated_submission_time=19925.481709, global_step=58577, preemption_count=0, score=19925.481709, test/accuracy=0.517900, test/loss=2.234585, test/num_examples=10000, total_duration=20645.449766, train/accuracy=0.706573, train/loss=1.142947, validation/accuracy=0.643380, validation/loss=1.473027, validation/num_examples=50000
I0130 01:19:35.442322 140025891108608 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.939185380935669, loss=1.5233800411224365
I0130 01:20:09.348299 140025882715904 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.924609899520874, loss=1.5359017848968506
I0130 01:20:43.312984 140025891108608 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.9295378923416138, loss=1.4536322355270386
I0130 01:21:17.245057 140025882715904 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8289709091186523, loss=1.6166694164276123
I0130 01:21:51.178169 140025891108608 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9544460773468018, loss=1.5664520263671875
I0130 01:22:25.126145 140025882715904 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.9282522201538086, loss=1.5022157430648804
I0130 01:22:59.040742 140025891108608 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.1138832569122314, loss=1.7683013677597046
I0130 01:23:32.981635 140025882715904 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.0479369163513184, loss=1.5188262462615967
I0130 01:24:06.942517 140025891108608 logging_writer.py:48] [59400] global_step=59400, grad_norm=2.0257513523101807, loss=1.6797831058502197
I0130 01:24:40.883332 140025882715904 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.9225562810897827, loss=1.5943005084991455
I0130 01:25:14.788644 140025891108608 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.818007230758667, loss=1.6107256412506104
I0130 01:25:48.715635 140025882715904 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9484304189682007, loss=1.525888442993164
I0130 01:26:22.627335 140025891108608 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.0572052001953125, loss=1.5567913055419922
I0130 01:26:56.556520 140025882715904 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.021556854248047, loss=1.5709283351898193
I0130 01:27:30.612592 140025891108608 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.1986074447631836, loss=1.550138235092163
I0130 01:27:57.580414 140187804313408 spec.py:321] Evaluating on the training split.
I0130 01:28:04.038080 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 01:28:12.705980 140187804313408 spec.py:349] Evaluating on the test split.
I0130 01:28:15.425657 140187804313408 submission_runner.py:408] Time since start: 21173.62s, 	Step: 60081, 	{'train/accuracy': 0.7592673897743225, 'train/loss': 0.929104745388031, 'validation/accuracy': 0.658079981803894, 'validation/loss': 1.402864933013916, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.1215386390686035, 'test/num_examples': 10000, 'score': 20435.709728956223, 'total_duration': 21173.62443089485, 'accumulated_submission_time': 20435.709728956223, 'accumulated_eval_time': 734.2747831344604, 'accumulated_logging_time': 1.4285414218902588}
I0130 01:28:15.449522 140025891108608 logging_writer.py:48] [60081] accumulated_eval_time=734.274783, accumulated_logging_time=1.428541, accumulated_submission_time=20435.709729, global_step=60081, preemption_count=0, score=20435.709729, test/accuracy=0.525400, test/loss=2.121539, test/num_examples=10000, total_duration=21173.624431, train/accuracy=0.759267, train/loss=0.929105, validation/accuracy=0.658080, validation/loss=1.402865, validation/num_examples=50000
I0130 01:28:22.242803 140026050483968 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.8213279247283936, loss=1.6367390155792236
I0130 01:28:56.133445 140025891108608 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.839674949645996, loss=1.5572280883789062
I0130 01:29:30.048026 140026050483968 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.8423618078231812, loss=1.5598299503326416
I0130 01:30:03.992664 140025891108608 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9078465700149536, loss=1.5563652515411377
I0130 01:30:37.906556 140026050483968 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8431038856506348, loss=1.391121745109558
I0130 01:31:11.847280 140025891108608 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.967820405960083, loss=1.5507493019104004
I0130 01:31:45.788177 140026050483968 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.941672444343567, loss=1.4545483589172363
I0130 01:32:19.723588 140025891108608 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.8287845849990845, loss=1.5748811960220337
I0130 01:32:53.676271 140026050483968 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.7239891290664673, loss=1.6307541131973267
I0130 01:33:27.618784 140025891108608 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.9743918180465698, loss=1.5158573389053345
I0130 01:34:01.668655 140026050483968 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.998794674873352, loss=1.455899953842163
I0130 01:34:35.626090 140025891108608 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.0454204082489014, loss=1.6250255107879639
I0130 01:35:09.577894 140026050483968 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9051523208618164, loss=1.5535694360733032
I0130 01:35:43.519697 140025891108608 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.923723578453064, loss=1.6097160577774048
I0130 01:36:17.450452 140026050483968 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8413492441177368, loss=1.6188795566558838
I0130 01:36:45.449766 140187804313408 spec.py:321] Evaluating on the training split.
I0130 01:36:51.702703 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 01:37:00.525938 140187804313408 spec.py:349] Evaluating on the test split.
I0130 01:37:03.411612 140187804313408 submission_runner.py:408] Time since start: 21701.61s, 	Step: 61584, 	{'train/accuracy': 0.73636794090271, 'train/loss': 1.0095868110656738, 'validation/accuracy': 0.6518799662590027, 'validation/loss': 1.4322763681411743, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.153595447540283, 'test/num_examples': 10000, 'score': 20945.643161058426, 'total_duration': 21701.610381364822, 'accumulated_submission_time': 20945.643161058426, 'accumulated_eval_time': 752.2365992069244, 'accumulated_logging_time': 1.4622957706451416}
I0130 01:37:03.440161 140026025305856 logging_writer.py:48] [61584] accumulated_eval_time=752.236599, accumulated_logging_time=1.462296, accumulated_submission_time=20945.643161, global_step=61584, preemption_count=0, score=20945.643161, test/accuracy=0.524200, test/loss=2.153595, test/num_examples=10000, total_duration=21701.610381, train/accuracy=0.736368, train/loss=1.009587, validation/accuracy=0.651880, validation/loss=1.432276, validation/num_examples=50000
I0130 01:37:09.216812 140026033698560 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.0521066188812256, loss=1.5640125274658203
I0130 01:37:43.097051 140026025305856 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9965505599975586, loss=1.5109155178070068
I0130 01:38:16.987390 140026033698560 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.8814971446990967, loss=1.6050198078155518
I0130 01:38:50.927049 140026025305856 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.8132833242416382, loss=1.4844772815704346
I0130 01:39:24.885864 140026033698560 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.891356110572815, loss=1.5818119049072266
I0130 01:39:58.816771 140026025305856 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.910233497619629, loss=1.5629032850265503
I0130 01:40:32.852025 140026033698560 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.012604236602783, loss=1.5800163745880127
I0130 01:41:06.782953 140026025305856 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.8700151443481445, loss=1.5384303331375122
I0130 01:41:40.700175 140026033698560 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.066124439239502, loss=1.5814436674118042
I0130 01:42:14.612522 140026025305856 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9956084489822388, loss=1.5181019306182861
I0130 01:42:48.552754 140026033698560 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.9363633394241333, loss=1.4320298433303833
I0130 01:43:22.486137 140026025305856 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.3198983669281006, loss=1.7753984928131104
I0130 01:43:56.429848 140026033698560 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.7528210878372192, loss=1.440590739250183
I0130 01:44:30.376446 140026025305856 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.7918604612350464, loss=1.4721159934997559
I0130 01:45:04.309022 140026033698560 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.200808048248291, loss=1.6765669584274292
I0130 01:45:33.620416 140187804313408 spec.py:321] Evaluating on the training split.
I0130 01:45:39.883604 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 01:45:48.663362 140187804313408 spec.py:349] Evaluating on the test split.
I0130 01:45:51.241472 140187804313408 submission_runner.py:408] Time since start: 22229.44s, 	Step: 63088, 	{'train/accuracy': 0.7388990521430969, 'train/loss': 0.9919620156288147, 'validation/accuracy': 0.6604399681091309, 'validation/loss': 1.3843040466308594, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.1002607345581055, 'test/num_examples': 10000, 'score': 21455.754417657852, 'total_duration': 22229.440237522125, 'accumulated_submission_time': 21455.754417657852, 'accumulated_eval_time': 769.8576102256775, 'accumulated_logging_time': 1.5023293495178223}
I0130 01:45:51.280648 140026050483968 logging_writer.py:48] [63088] accumulated_eval_time=769.857610, accumulated_logging_time=1.502329, accumulated_submission_time=21455.754418, global_step=63088, preemption_count=0, score=21455.754418, test/accuracy=0.532300, test/loss=2.100261, test/num_examples=10000, total_duration=22229.440238, train/accuracy=0.738899, train/loss=0.991962, validation/accuracy=0.660440, validation/loss=1.384304, validation/num_examples=50000
I0130 01:45:55.702624 140026058876672 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.956682801246643, loss=1.541882872581482
I0130 01:46:29.559006 140026050483968 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.8489676713943481, loss=1.5266157388687134
I0130 01:47:03.593912 140026058876672 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.7034168243408203, loss=1.5181347131729126
I0130 01:47:37.566079 140026050483968 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.8307323455810547, loss=1.5746259689331055
I0130 01:48:11.517233 140026058876672 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.8401298522949219, loss=1.4834110736846924
I0130 01:48:45.400291 140026050483968 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.9452366828918457, loss=1.394688606262207
I0130 01:49:19.340496 140026058876672 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.0969810485839844, loss=1.5207816362380981
I0130 01:49:53.284157 140026050483968 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.0235111713409424, loss=1.564182996749878
I0130 01:50:27.231235 140026058876672 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.786881446838379, loss=1.4450876712799072
I0130 01:51:01.152151 140026050483968 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.0444388389587402, loss=1.5586705207824707
I0130 01:51:35.100038 140026058876672 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.431016683578491, loss=1.6432209014892578
I0130 01:52:09.081674 140026050483968 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.011082172393799, loss=1.511291265487671
I0130 01:52:43.024830 140026058876672 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9723714590072632, loss=1.4940943717956543
I0130 01:53:16.951007 140026050483968 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.908976435661316, loss=1.5078073740005493
I0130 01:53:50.982217 140026058876672 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9230122566223145, loss=1.658582091331482
I0130 01:54:21.329732 140187804313408 spec.py:321] Evaluating on the training split.
I0130 01:54:27.596443 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 01:54:36.296346 140187804313408 spec.py:349] Evaluating on the test split.
I0130 01:54:38.997257 140187804313408 submission_runner.py:408] Time since start: 22757.20s, 	Step: 64591, 	{'train/accuracy': 0.7138074040412903, 'train/loss': 1.106141448020935, 'validation/accuracy': 0.6453199982643127, 'validation/loss': 1.4593579769134521, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.223273992538452, 'test/num_examples': 10000, 'score': 21965.739110708237, 'total_duration': 22757.195997476578, 'accumulated_submission_time': 21965.739110708237, 'accumulated_eval_time': 787.5250680446625, 'accumulated_logging_time': 1.551323413848877}
I0130 01:54:39.040254 140026016913152 logging_writer.py:48] [64591] accumulated_eval_time=787.525068, accumulated_logging_time=1.551323, accumulated_submission_time=21965.739111, global_step=64591, preemption_count=0, score=21965.739111, test/accuracy=0.519100, test/loss=2.223274, test/num_examples=10000, total_duration=22757.195997, train/accuracy=0.713807, train/loss=1.106141, validation/accuracy=0.645320, validation/loss=1.459358, validation/num_examples=50000
I0130 01:54:42.436927 140026025305856 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.9052884578704834, loss=1.4632667303085327
I0130 01:55:16.264434 140026016913152 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9303396940231323, loss=1.5353007316589355
I0130 01:55:50.158637 140026025305856 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.8416526317596436, loss=1.5264626741409302
I0130 01:56:24.072693 140026016913152 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9036983251571655, loss=1.5788977146148682
I0130 01:56:58.028793 140026025305856 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.0964319705963135, loss=1.4752352237701416
I0130 01:57:31.931153 140026016913152 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.033576488494873, loss=1.5781123638153076
I0130 01:58:05.876145 140026025305856 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8095604181289673, loss=1.431565761566162
I0130 01:58:39.839313 140026016913152 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.0932648181915283, loss=1.7007602453231812
I0130 01:59:13.792763 140026025305856 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.0192184448242188, loss=1.4177812337875366
I0130 01:59:47.694977 140026016913152 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.8899407386779785, loss=1.4326672554016113
I0130 02:00:21.679077 140026025305856 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.945777177810669, loss=1.5753346681594849
I0130 02:00:55.596947 140026016913152 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.1548023223876953, loss=1.637344479560852
I0130 02:01:29.533935 140026025305856 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.8392689228057861, loss=1.4768314361572266
I0130 02:02:03.451658 140026016913152 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.0988848209381104, loss=1.4918283224105835
I0130 02:02:37.370218 140026025305856 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.9037212133407593, loss=1.4619420766830444
I0130 02:03:09.049803 140187804313408 spec.py:321] Evaluating on the training split.
I0130 02:03:15.333858 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 02:03:24.110214 140187804313408 spec.py:349] Evaluating on the test split.
I0130 02:03:26.830177 140187804313408 submission_runner.py:408] Time since start: 23285.03s, 	Step: 66095, 	{'train/accuracy': 0.7254065275192261, 'train/loss': 1.0538322925567627, 'validation/accuracy': 0.6597200036048889, 'validation/loss': 1.3996851444244385, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.1329658031463623, 'test/num_examples': 10000, 'score': 22475.68429350853, 'total_duration': 23285.028936624527, 'accumulated_submission_time': 22475.68429350853, 'accumulated_eval_time': 805.305394411087, 'accumulated_logging_time': 1.60355544090271}
I0130 02:03:26.864123 140026050483968 logging_writer.py:48] [66095] accumulated_eval_time=805.305394, accumulated_logging_time=1.603555, accumulated_submission_time=22475.684294, global_step=66095, preemption_count=0, score=22475.684294, test/accuracy=0.535400, test/loss=2.132966, test/num_examples=10000, total_duration=23285.028937, train/accuracy=0.725407, train/loss=1.053832, validation/accuracy=0.659720, validation/loss=1.399685, validation/num_examples=50000
I0130 02:03:28.903942 140026058876672 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.842427134513855, loss=1.51356840133667
I0130 02:04:02.805845 140026050483968 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.2471742630004883, loss=1.5749768018722534
I0130 02:04:36.714108 140026058876672 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.2512779235839844, loss=1.5395534038543701
I0130 02:05:10.643407 140026050483968 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.083104133605957, loss=1.6194472312927246
I0130 02:05:44.592162 140026058876672 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.0853161811828613, loss=1.5278582572937012
I0130 02:06:18.552868 140026050483968 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.9065526723861694, loss=1.552948236465454
I0130 02:06:52.692222 140026058876672 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.8092817068099976, loss=1.4625749588012695
I0130 02:07:26.652035 140026050483968 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.9091697931289673, loss=1.428889513015747
I0130 02:08:00.624450 140026058876672 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.0591371059417725, loss=1.589582085609436
I0130 02:08:34.538000 140026050483968 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.0333008766174316, loss=1.4564499855041504
I0130 02:09:08.492299 140026058876672 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.044142484664917, loss=1.562647819519043
I0130 02:09:42.420466 140026050483968 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.0238592624664307, loss=1.5401848554611206
I0130 02:10:16.343441 140026058876672 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9641152620315552, loss=1.5794579982757568
I0130 02:10:50.317671 140026050483968 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.841067910194397, loss=1.4287950992584229
I0130 02:11:24.252055 140026058876672 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.7025604248046875, loss=1.4654200077056885
I0130 02:11:56.945521 140187804313408 spec.py:321] Evaluating on the training split.
I0130 02:12:03.378032 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 02:12:12.203457 140187804313408 spec.py:349] Evaluating on the test split.
I0130 02:12:14.891099 140187804313408 submission_runner.py:408] Time since start: 23813.09s, 	Step: 67598, 	{'train/accuracy': 0.7191087007522583, 'train/loss': 1.0847852230072021, 'validation/accuracy': 0.6524999737739563, 'validation/loss': 1.4338654279708862, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.1947619915008545, 'test/num_examples': 10000, 'score': 22985.69870376587, 'total_duration': 23813.08985543251, 'accumulated_submission_time': 22985.69870376587, 'accumulated_eval_time': 823.2509181499481, 'accumulated_logging_time': 1.6490552425384521}
I0130 02:12:14.928002 140026016913152 logging_writer.py:48] [67598] accumulated_eval_time=823.250918, accumulated_logging_time=1.649055, accumulated_submission_time=22985.698704, global_step=67598, preemption_count=0, score=22985.698704, test/accuracy=0.516500, test/loss=2.194762, test/num_examples=10000, total_duration=23813.089855, train/accuracy=0.719109, train/loss=1.084785, validation/accuracy=0.652500, validation/loss=1.433865, validation/num_examples=50000
I0130 02:12:15.979998 140026025305856 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.0326218605041504, loss=1.5428550243377686
I0130 02:12:49.897498 140026016913152 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0416910648345947, loss=1.505149483680725
I0130 02:13:23.939461 140026025305856 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.2160046100616455, loss=1.6202373504638672
I0130 02:13:57.882020 140026016913152 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.3817481994628906, loss=1.6006660461425781
I0130 02:14:31.863061 140026025305856 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.8930423259735107, loss=1.4474289417266846
I0130 02:15:05.801232 140026016913152 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.1898162364959717, loss=1.5545504093170166
I0130 02:15:39.720680 140026025305856 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.0688154697418213, loss=1.3883458375930786
I0130 02:16:13.670534 140026016913152 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.2523884773254395, loss=1.538888931274414
I0130 02:16:47.610661 140026025305856 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.8922505378723145, loss=1.5113599300384521
I0130 02:17:21.546305 140026016913152 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.9223065376281738, loss=1.4710538387298584
I0130 02:17:55.514856 140026025305856 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.8890959024429321, loss=1.5192664861679077
I0130 02:18:29.452904 140026016913152 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.079569101333618, loss=1.4654242992401123
I0130 02:19:03.352345 140026025305856 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.133382558822632, loss=1.4782437086105347
I0130 02:19:37.294425 140026016913152 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.959854006767273, loss=1.5200774669647217
I0130 02:20:11.342395 140026025305856 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.9079616069793701, loss=1.5362317562103271
I0130 02:20:45.272916 140026016913152 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.873524785041809, loss=1.403791069984436
I0130 02:20:45.280534 140187804313408 spec.py:321] Evaluating on the training split.
I0130 02:20:51.552601 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 02:21:00.407485 140187804313408 spec.py:349] Evaluating on the test split.
I0130 02:21:03.337241 140187804313408 submission_runner.py:408] Time since start: 24341.54s, 	Step: 69101, 	{'train/accuracy': 0.7221181392669678, 'train/loss': 1.0768852233886719, 'validation/accuracy': 0.6430599689483643, 'validation/loss': 1.46754789352417, 'validation/num_examples': 50000, 'test/accuracy': 0.5143000483512878, 'test/loss': 2.2291743755340576, 'test/num_examples': 10000, 'score': 23495.98047399521, 'total_duration': 24341.536016464233, 'accumulated_submission_time': 23495.98047399521, 'accumulated_eval_time': 841.3075633049011, 'accumulated_logging_time': 1.700188159942627}
I0130 02:21:03.366484 140025891108608 logging_writer.py:48] [69101] accumulated_eval_time=841.307563, accumulated_logging_time=1.700188, accumulated_submission_time=23495.980474, global_step=69101, preemption_count=0, score=23495.980474, test/accuracy=0.514300, test/loss=2.229174, test/num_examples=10000, total_duration=24341.536016, train/accuracy=0.722118, train/loss=1.076885, validation/accuracy=0.643060, validation/loss=1.467548, validation/num_examples=50000
I0130 02:21:37.246482 140026050483968 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.158632755279541, loss=1.4012278318405151
I0130 02:22:11.136111 140025891108608 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.025256633758545, loss=1.5599898099899292
I0130 02:22:45.070391 140026050483968 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.152268886566162, loss=1.5788559913635254
I0130 02:23:19.016649 140025891108608 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.8505128622055054, loss=1.5127313137054443
I0130 02:23:52.922140 140026050483968 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.9400761127471924, loss=1.4848432540893555
I0130 02:24:26.863726 140025891108608 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.9621186256408691, loss=1.458229422569275
I0130 02:25:00.801043 140026050483968 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.1215388774871826, loss=1.5469698905944824
I0130 02:25:34.749525 140025891108608 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.048088550567627, loss=1.4713845252990723
I0130 02:26:08.677031 140026050483968 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.9693740606307983, loss=1.539583444595337
I0130 02:26:42.702811 140025891108608 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.7853702306747437, loss=1.5461106300354004
I0130 02:27:16.639594 140026050483968 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.9604672193527222, loss=1.3869211673736572
I0130 02:27:50.554090 140025891108608 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.892307162284851, loss=1.4553625583648682
I0130 02:28:24.502788 140026050483968 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.3980963230133057, loss=1.6006165742874146
I0130 02:28:58.446234 140025891108608 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.0834240913391113, loss=1.4575557708740234
I0130 02:29:32.407659 140026050483968 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.9195135831832886, loss=1.4686410427093506
I0130 02:29:33.577640 140187804313408 spec.py:321] Evaluating on the training split.
I0130 02:29:39.864108 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 02:29:48.597183 140187804313408 spec.py:349] Evaluating on the test split.
I0130 02:29:51.294411 140187804313408 submission_runner.py:408] Time since start: 24869.49s, 	Step: 70605, 	{'train/accuracy': 0.7581911683082581, 'train/loss': 0.9060986638069153, 'validation/accuracy': 0.6655600070953369, 'validation/loss': 1.3723647594451904, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.08642315864563, 'test/num_examples': 10000, 'score': 24006.12465786934, 'total_duration': 24869.49318599701, 'accumulated_submission_time': 24006.12465786934, 'accumulated_eval_time': 859.0242967605591, 'accumulated_logging_time': 1.7394630908966064}
I0130 02:29:51.330025 140026033698560 logging_writer.py:48] [70605] accumulated_eval_time=859.024297, accumulated_logging_time=1.739463, accumulated_submission_time=24006.124658, global_step=70605, preemption_count=0, score=24006.124658, test/accuracy=0.543400, test/loss=2.086423, test/num_examples=10000, total_duration=24869.493186, train/accuracy=0.758191, train/loss=0.906099, validation/accuracy=0.665560, validation/loss=1.372365, validation/num_examples=50000
I0130 02:30:23.875813 140026042091264 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.0991551876068115, loss=1.529413104057312
I0130 02:30:57.773146 140026033698560 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.031522035598755, loss=1.5594818592071533
I0130 02:31:31.731757 140026042091264 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.205989122390747, loss=1.6438184976577759
I0130 02:32:05.681185 140026033698560 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.0845909118652344, loss=1.498661994934082
I0130 02:32:39.624258 140026042091264 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.7323861122131348, loss=1.3652201890945435
I0130 02:33:13.636521 140026033698560 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.1244068145751953, loss=1.5311049222946167
I0130 02:33:47.571643 140026042091264 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.0228593349456787, loss=1.6101634502410889
I0130 02:34:21.536741 140026033698560 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.1727709770202637, loss=1.4755001068115234
I0130 02:34:55.454308 140026042091264 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.9314759969711304, loss=1.5716720819473267
I0130 02:35:29.376515 140026033698560 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.354377269744873, loss=1.6239687204360962
I0130 02:36:03.315983 140026042091264 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.1660661697387695, loss=1.569322943687439
I0130 02:36:37.223434 140026033698560 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.9086050987243652, loss=1.4009203910827637
I0130 02:37:11.156300 140026042091264 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.156054735183716, loss=1.5506150722503662
I0130 02:37:45.129135 140026033698560 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.991182565689087, loss=1.4511921405792236
I0130 02:38:19.059166 140026042091264 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.909887433052063, loss=1.5071088075637817
I0130 02:38:21.586605 140187804313408 spec.py:321] Evaluating on the training split.
I0130 02:38:27.863616 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 02:38:36.527760 140187804313408 spec.py:349] Evaluating on the test split.
I0130 02:38:39.223997 140187804313408 submission_runner.py:408] Time since start: 25397.42s, 	Step: 72109, 	{'train/accuracy': 0.7310466766357422, 'train/loss': 1.0297045707702637, 'validation/accuracy': 0.6527999639511108, 'validation/loss': 1.4410232305526733, 'validation/num_examples': 50000, 'test/accuracy': 0.5228000283241272, 'test/loss': 2.215155601501465, 'test/num_examples': 10000, 'score': 24516.316374063492, 'total_duration': 25397.422772169113, 'accumulated_submission_time': 24516.316374063492, 'accumulated_eval_time': 876.6616532802582, 'accumulated_logging_time': 1.7846426963806152}
I0130 02:38:39.254121 140026016913152 logging_writer.py:48] [72109] accumulated_eval_time=876.661653, accumulated_logging_time=1.784643, accumulated_submission_time=24516.316374, global_step=72109, preemption_count=0, score=24516.316374, test/accuracy=0.522800, test/loss=2.215156, test/num_examples=10000, total_duration=25397.422772, train/accuracy=0.731047, train/loss=1.029705, validation/accuracy=0.652800, validation/loss=1.441023, validation/num_examples=50000
I0130 02:39:10.398266 140026025305856 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.117471933364868, loss=1.5220905542373657
I0130 02:39:44.309620 140026016913152 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.0668256282806396, loss=1.5348711013793945
I0130 02:40:18.423566 140026025305856 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.2521848678588867, loss=1.5818555355072021
I0130 02:40:52.371559 140026016913152 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.059748888015747, loss=1.5089049339294434
I0130 02:41:26.341003 140026025305856 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.1423802375793457, loss=1.3665015697479248
I0130 02:42:00.293399 140026016913152 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.9019920825958252, loss=1.4088222980499268
I0130 02:42:34.206519 140026025305856 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.133082866668701, loss=1.4699567556381226
I0130 02:43:08.150557 140026016913152 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.8463385105133057, loss=1.4664764404296875
I0130 02:43:42.131844 140026025305856 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.00545072555542, loss=1.5526782274246216
I0130 02:44:16.088660 140026016913152 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.1433205604553223, loss=1.570870280265808
I0130 02:44:50.061935 140026025305856 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.2058753967285156, loss=1.5159741640090942
I0130 02:45:24.027394 140026016913152 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.1624958515167236, loss=1.379894495010376
I0130 02:45:57.917764 140026025305856 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.907580018043518, loss=1.3928414583206177
I0130 02:46:32.101068 140026016913152 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.9778941869735718, loss=1.4848469495773315
I0130 02:47:06.061183 140026025305856 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.0491814613342285, loss=1.3815362453460693
I0130 02:47:09.275841 140187804313408 spec.py:321] Evaluating on the training split.
I0130 02:47:15.547816 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 02:47:24.623389 140187804313408 spec.py:349] Evaluating on the test split.
I0130 02:47:27.213766 140187804313408 submission_runner.py:408] Time since start: 25925.41s, 	Step: 73611, 	{'train/accuracy': 0.7375039458274841, 'train/loss': 0.9993064999580383, 'validation/accuracy': 0.6623799800872803, 'validation/loss': 1.3905607461929321, 'validation/num_examples': 50000, 'test/accuracy': 0.5325000286102295, 'test/loss': 2.110114812850952, 'test/num_examples': 10000, 'score': 25026.270292043686, 'total_duration': 25925.412529945374, 'accumulated_submission_time': 25026.270292043686, 'accumulated_eval_time': 894.5995259284973, 'accumulated_logging_time': 1.8255927562713623}
I0130 02:47:27.249451 140026058876672 logging_writer.py:48] [73611] accumulated_eval_time=894.599526, accumulated_logging_time=1.825593, accumulated_submission_time=25026.270292, global_step=73611, preemption_count=0, score=25026.270292, test/accuracy=0.532500, test/loss=2.110115, test/num_examples=10000, total_duration=25925.412530, train/accuracy=0.737504, train/loss=0.999306, validation/accuracy=0.662380, validation/loss=1.390561, validation/num_examples=50000
I0130 02:47:57.708881 140026067269376 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.9254999160766602, loss=1.421413540840149
I0130 02:48:31.589795 140026058876672 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.0999019145965576, loss=1.4278159141540527
I0130 02:49:05.543916 140026067269376 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.127434015274048, loss=1.5541672706604004
I0130 02:49:39.463194 140026058876672 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9598077535629272, loss=1.4044313430786133
I0130 02:50:13.396380 140026067269376 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0684335231781006, loss=1.4156666994094849
I0130 02:50:47.345116 140026058876672 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.06829833984375, loss=1.6111094951629639
I0130 02:51:21.279691 140026067269376 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.8947229385375977, loss=1.5344852209091187
I0130 02:51:55.236819 140026058876672 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.087146043777466, loss=1.4619340896606445
I0130 02:52:29.129487 140026067269376 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.1617679595947266, loss=1.486124873161316
I0130 02:53:03.206371 140026058876672 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.1760458946228027, loss=1.636515498161316
I0130 02:53:37.146782 140026067269376 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.117851972579956, loss=1.5028996467590332
I0130 02:54:11.093389 140026058876672 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.1653778553009033, loss=1.6270297765731812
I0130 02:54:45.040960 140026067269376 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.9790129661560059, loss=1.4533449411392212
I0130 02:55:18.990714 140026058876672 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.250183343887329, loss=1.4884366989135742
I0130 02:55:52.951382 140026067269376 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.2196221351623535, loss=1.506376028060913
I0130 02:55:57.533483 140187804313408 spec.py:321] Evaluating on the training split.
I0130 02:56:03.981754 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 02:56:12.643945 140187804313408 spec.py:349] Evaluating on the test split.
I0130 02:56:15.363445 140187804313408 submission_runner.py:408] Time since start: 26453.56s, 	Step: 75115, 	{'train/accuracy': 0.7437619566917419, 'train/loss': 0.9866275191307068, 'validation/accuracy': 0.6665399670600891, 'validation/loss': 1.3468725681304932, 'validation/num_examples': 50000, 'test/accuracy': 0.5391000509262085, 'test/loss': 2.0707874298095703, 'test/num_examples': 10000, 'score': 25536.4892745018, 'total_duration': 26453.562220811844, 'accumulated_submission_time': 25536.4892745018, 'accumulated_eval_time': 912.429455280304, 'accumulated_logging_time': 1.8706903457641602}
I0130 02:56:15.390839 140025882715904 logging_writer.py:48] [75115] accumulated_eval_time=912.429455, accumulated_logging_time=1.870690, accumulated_submission_time=25536.489275, global_step=75115, preemption_count=0, score=25536.489275, test/accuracy=0.539100, test/loss=2.070787, test/num_examples=10000, total_duration=26453.562221, train/accuracy=0.743762, train/loss=0.986628, validation/accuracy=0.666540, validation/loss=1.346873, validation/num_examples=50000
I0130 02:56:44.538539 140025891108608 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.4130337238311768, loss=1.5565447807312012
I0130 02:57:18.443707 140025882715904 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.177485942840576, loss=1.5242400169372559
I0130 02:57:52.426919 140025891108608 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.016423463821411, loss=1.4769943952560425
I0130 02:58:26.380424 140025882715904 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.1718802452087402, loss=1.552138090133667
I0130 02:59:00.355026 140025891108608 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.9136502742767334, loss=1.5212135314941406
I0130 02:59:34.329139 140025882715904 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.22861647605896, loss=1.4754750728607178
I0130 03:00:08.356708 140025891108608 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.128685712814331, loss=1.5188809633255005
I0130 03:00:42.300822 140025882715904 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.0412983894348145, loss=1.5991764068603516
I0130 03:01:16.239466 140025891108608 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.2726573944091797, loss=1.487321376800537
I0130 03:01:50.175610 140025882715904 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.0981359481811523, loss=1.5033901929855347
I0130 03:02:24.161273 140025891108608 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.487668752670288, loss=1.432997465133667
I0130 03:02:58.110723 140025882715904 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.5046212673187256, loss=1.441277265548706
I0130 03:03:32.008221 140025891108608 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.082737445831299, loss=1.4983853101730347
I0130 03:04:05.962332 140025882715904 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.9455376863479614, loss=1.4319484233856201
I0130 03:04:39.901288 140025891108608 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.1655349731445312, loss=1.536670446395874
I0130 03:04:45.480499 140187804313408 spec.py:321] Evaluating on the training split.
I0130 03:04:51.768304 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 03:05:00.318818 140187804313408 spec.py:349] Evaluating on the test split.
I0130 03:05:03.050758 140187804313408 submission_runner.py:408] Time since start: 26981.25s, 	Step: 76618, 	{'train/accuracy': 0.7401546239852905, 'train/loss': 0.9870557188987732, 'validation/accuracy': 0.6706399917602539, 'validation/loss': 1.3400790691375732, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.035409450531006, 'test/num_examples': 10000, 'score': 26046.512956619263, 'total_duration': 26981.249529123306, 'accumulated_submission_time': 26046.512956619263, 'accumulated_eval_time': 929.9996762275696, 'accumulated_logging_time': 1.908890724182129}
I0130 03:05:03.083055 140025891108608 logging_writer.py:48] [76618] accumulated_eval_time=929.999676, accumulated_logging_time=1.908891, accumulated_submission_time=26046.512957, global_step=76618, preemption_count=0, score=26046.512957, test/accuracy=0.547400, test/loss=2.035409, test/num_examples=10000, total_duration=26981.249529, train/accuracy=0.740155, train/loss=0.987056, validation/accuracy=0.670640, validation/loss=1.340079, validation/num_examples=50000
I0130 03:05:31.193233 140026050483968 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.1537256240844727, loss=1.5056465864181519
I0130 03:06:05.085921 140025891108608 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.080246925354004, loss=1.4537601470947266
I0130 03:06:39.057074 140026050483968 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.069439172744751, loss=1.4293339252471924
I0130 03:07:12.960815 140025891108608 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1524548530578613, loss=1.582079291343689
I0130 03:07:46.904838 140026050483968 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.270573377609253, loss=1.4902868270874023
I0130 03:08:20.894435 140025891108608 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.114466667175293, loss=1.500030279159546
I0130 03:08:54.841264 140026050483968 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.033879280090332, loss=1.5079420804977417
I0130 03:09:28.752467 140025891108608 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.3344218730926514, loss=1.5120666027069092
I0130 03:10:02.689464 140026050483968 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.261354684829712, loss=1.4368669986724854
I0130 03:10:36.584179 140025891108608 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.4420342445373535, loss=1.4773194789886475
I0130 03:11:10.544686 140026050483968 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.274977922439575, loss=1.391861915588379
I0130 03:11:44.468456 140025891108608 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.161151647567749, loss=1.4926085472106934
I0130 03:12:18.377327 140026050483968 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.2445225715637207, loss=1.5343860387802124
I0130 03:12:52.444882 140025891108608 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.9430286884307861, loss=1.3624482154846191
I0130 03:13:26.370962 140026050483968 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.1842310428619385, loss=1.3965952396392822
I0130 03:13:33.314086 140187804313408 spec.py:321] Evaluating on the training split.
I0130 03:13:39.633422 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 03:13:48.507967 140187804313408 spec.py:349] Evaluating on the test split.
I0130 03:13:51.185002 140187804313408 submission_runner.py:408] Time since start: 27509.38s, 	Step: 78122, 	{'train/accuracy': 0.7384207248687744, 'train/loss': 1.0084189176559448, 'validation/accuracy': 0.665340006351471, 'validation/loss': 1.3739938735961914, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.11980938911438, 'test/num_examples': 10000, 'score': 26556.6778280735, 'total_duration': 27509.383778572083, 'accumulated_submission_time': 26556.6778280735, 'accumulated_eval_time': 947.8705537319183, 'accumulated_logging_time': 1.9521074295043945}
I0130 03:13:51.215350 140025891108608 logging_writer.py:48] [78122] accumulated_eval_time=947.870554, accumulated_logging_time=1.952107, accumulated_submission_time=26556.677828, global_step=78122, preemption_count=0, score=26556.677828, test/accuracy=0.539200, test/loss=2.119809, test/num_examples=10000, total_duration=27509.383779, train/accuracy=0.738421, train/loss=1.008419, validation/accuracy=0.665340, validation/loss=1.373994, validation/num_examples=50000
I0130 03:14:17.955430 140026033698560 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.2203493118286133, loss=1.5445494651794434
I0130 03:14:51.851226 140025891108608 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.209019422531128, loss=1.4477733373641968
I0130 03:15:25.761916 140026033698560 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.364840030670166, loss=1.4650908708572388
I0130 03:15:59.666340 140025891108608 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.3748812675476074, loss=1.5480042695999146
I0130 03:16:33.598045 140026033698560 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.3646087646484375, loss=1.5185073614120483
I0130 03:17:07.509271 140025891108608 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.299464702606201, loss=1.3787860870361328
I0130 03:17:41.430850 140026033698560 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.38667368888855, loss=1.4593077898025513
I0130 03:18:15.368413 140025891108608 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.0721426010131836, loss=1.424949049949646
I0130 03:18:49.329651 140026033698560 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.0993332862854004, loss=1.510956048965454
I0130 03:19:23.264550 140025891108608 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.0660688877105713, loss=1.4630786180496216
I0130 03:19:57.329796 140026033698560 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.185969114303589, loss=1.454593539237976
I0130 03:20:31.274625 140025891108608 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.3302032947540283, loss=1.412095308303833
I0130 03:21:05.197680 140026033698560 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.0873053073883057, loss=1.490602731704712
I0130 03:21:39.133202 140025891108608 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.079536199569702, loss=1.4943881034851074
I0130 03:22:13.059774 140026033698560 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.249450445175171, loss=1.4641789197921753
I0130 03:22:21.354520 140187804313408 spec.py:321] Evaluating on the training split.
I0130 03:22:27.682577 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 03:22:36.319609 140187804313408 spec.py:349] Evaluating on the test split.
I0130 03:22:38.999157 140187804313408 submission_runner.py:408] Time since start: 28037.20s, 	Step: 79626, 	{'train/accuracy': 0.767578125, 'train/loss': 0.8716039061546326, 'validation/accuracy': 0.6688399910926819, 'validation/loss': 1.3541743755340576, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.0731024742126465, 'test/num_examples': 10000, 'score': 27066.751952409744, 'total_duration': 28037.197903633118, 'accumulated_submission_time': 27066.751952409744, 'accumulated_eval_time': 965.5151259899139, 'accumulated_logging_time': 1.991746425628662}
I0130 03:22:39.034024 140026025305856 logging_writer.py:48] [79626] accumulated_eval_time=965.515126, accumulated_logging_time=1.991746, accumulated_submission_time=27066.751952, global_step=79626, preemption_count=0, score=27066.751952, test/accuracy=0.544500, test/loss=2.073102, test/num_examples=10000, total_duration=28037.197904, train/accuracy=0.767578, train/loss=0.871604, validation/accuracy=0.668840, validation/loss=1.354174, validation/num_examples=50000
I0130 03:23:04.424000 140026050483968 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.1586363315582275, loss=1.5325922966003418
I0130 03:23:38.267744 140026025305856 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.247621774673462, loss=1.4083356857299805
I0130 03:24:12.184742 140026050483968 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.134925127029419, loss=1.3522295951843262
I0130 03:24:46.103167 140026025305856 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.5074307918548584, loss=1.3365941047668457
I0130 03:25:20.038423 140026050483968 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.295454740524292, loss=1.5095648765563965
I0130 03:25:53.955185 140026025305856 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.080496311187744, loss=1.462677240371704
I0130 03:26:28.088229 140026050483968 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.1724257469177246, loss=1.4356563091278076
I0130 03:27:01.972514 140026025305856 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.076995849609375, loss=1.434043288230896
I0130 03:27:35.922285 140026050483968 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.390620470046997, loss=1.461024522781372
I0130 03:28:09.861040 140026025305856 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.192335367202759, loss=1.566660761833191
I0130 03:28:43.772695 140026050483968 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.4018423557281494, loss=1.4980924129486084
I0130 03:29:17.732121 140026025305856 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.2845468521118164, loss=1.4777470827102661
I0130 03:29:51.693468 140026050483968 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.1433496475219727, loss=1.3661830425262451
I0130 03:30:25.606102 140026025305856 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.1400134563446045, loss=1.3792235851287842
I0130 03:30:59.545410 140026050483968 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1777970790863037, loss=1.3073551654815674
I0130 03:31:09.209822 140187804313408 spec.py:321] Evaluating on the training split.
I0130 03:31:15.706773 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 03:31:24.445474 140187804313408 spec.py:349] Evaluating on the test split.
I0130 03:31:27.153561 140187804313408 submission_runner.py:408] Time since start: 28565.35s, 	Step: 81130, 	{'train/accuracy': 0.7565369606018066, 'train/loss': 0.9142917394638062, 'validation/accuracy': 0.6692000031471252, 'validation/loss': 1.3499724864959717, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.085909128189087, 'test/num_examples': 10000, 'score': 27576.86024737358, 'total_duration': 28565.352335691452, 'accumulated_submission_time': 27576.86024737358, 'accumulated_eval_time': 983.458841085434, 'accumulated_logging_time': 2.0363407135009766}
I0130 03:31:27.185195 140025882715904 logging_writer.py:48] [81130] accumulated_eval_time=983.458841, accumulated_logging_time=2.036341, accumulated_submission_time=27576.860247, global_step=81130, preemption_count=0, score=27576.860247, test/accuracy=0.532400, test/loss=2.085909, test/num_examples=10000, total_duration=28565.352336, train/accuracy=0.756537, train/loss=0.914292, validation/accuracy=0.669200, validation/loss=1.349972, validation/num_examples=50000
I0130 03:31:51.251046 140025891108608 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.0522408485412598, loss=1.3117411136627197
I0130 03:32:25.143622 140025882715904 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.0634429454803467, loss=1.3345792293548584
I0130 03:32:59.179072 140025891108608 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.1946511268615723, loss=1.4538958072662354
I0130 03:33:33.097930 140025882715904 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.378375768661499, loss=1.483100175857544
I0130 03:34:07.058397 140025891108608 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.158918857574463, loss=1.4135961532592773
I0130 03:34:41.002681 140025882715904 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.383516550064087, loss=1.4664548635482788
I0130 03:35:14.968924 140025891108608 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.080594778060913, loss=1.434685230255127
I0130 03:35:48.925758 140025882715904 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.141423225402832, loss=1.4451839923858643
I0130 03:36:22.878595 140025891108608 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.9454538822174072, loss=1.417649269104004
I0130 03:36:56.839533 140025882715904 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.201712131500244, loss=1.3789393901824951
I0130 03:37:30.795000 140025891108608 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.1954996585845947, loss=1.4777390956878662
I0130 03:38:04.740525 140025882715904 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.3594868183135986, loss=1.4665976762771606
I0130 03:38:38.689156 140025891108608 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.5183613300323486, loss=1.491317629814148
I0130 03:39:12.636953 140025882715904 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2461018562316895, loss=1.4197595119476318
I0130 03:39:46.650676 140025891108608 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.055520534515381, loss=1.3980776071548462
I0130 03:39:57.326803 140187804313408 spec.py:321] Evaluating on the training split.
I0130 03:40:03.743849 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 03:40:12.557486 140187804313408 spec.py:349] Evaluating on the test split.
I0130 03:40:15.269155 140187804313408 submission_runner.py:408] Time since start: 29093.47s, 	Step: 82633, 	{'train/accuracy': 0.7384008169174194, 'train/loss': 1.0000159740447998, 'validation/accuracy': 0.6617599725723267, 'validation/loss': 1.3959338665008545, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.1417131423950195, 'test/num_examples': 10000, 'score': 28086.937801122665, 'total_duration': 29093.46788740158, 'accumulated_submission_time': 28086.937801122665, 'accumulated_eval_time': 1001.4011144638062, 'accumulated_logging_time': 2.0771901607513428}
I0130 03:40:15.319949 140025882715904 logging_writer.py:48] [82633] accumulated_eval_time=1001.401114, accumulated_logging_time=2.077190, accumulated_submission_time=28086.937801, global_step=82633, preemption_count=0, score=28086.937801, test/accuracy=0.534300, test/loss=2.141713, test/num_examples=10000, total_duration=29093.467887, train/accuracy=0.738401, train/loss=1.000016, validation/accuracy=0.661760, validation/loss=1.395934, validation/num_examples=50000
I0130 03:40:38.314353 140025891108608 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.228933811187744, loss=1.4931585788726807
I0130 03:41:12.226678 140025882715904 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.8956800699234009, loss=1.3837555646896362
I0130 03:41:46.164092 140025891108608 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.2660601139068604, loss=1.462680459022522
I0130 03:42:20.098559 140025882715904 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.3436155319213867, loss=1.4952527284622192
I0130 03:42:54.051229 140025891108608 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.0638601779937744, loss=1.3926759958267212
I0130 03:43:27.988629 140025882715904 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.465651273727417, loss=1.4534515142440796
I0130 03:44:01.926155 140025891108608 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.152768611907959, loss=1.4283092021942139
I0130 03:44:35.848618 140025882715904 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.302579402923584, loss=1.4739876985549927
I0130 03:45:09.790940 140025891108608 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.1514272689819336, loss=1.4632761478424072
I0130 03:45:43.720728 140025882715904 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.1958680152893066, loss=1.479943871498108
I0130 03:46:17.737476 140025891108608 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.149869680404663, loss=1.4371498823165894
I0130 03:46:51.704608 140025882715904 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.4412124156951904, loss=1.445389747619629
I0130 03:47:25.612317 140025891108608 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.093125820159912, loss=1.373087763786316
I0130 03:47:59.548495 140025882715904 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.477376937866211, loss=1.436702013015747
I0130 03:48:33.446648 140025891108608 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.331455945968628, loss=1.4760457277297974
I0130 03:48:45.460804 140187804313408 spec.py:321] Evaluating on the training split.
I0130 03:48:51.853947 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 03:49:00.475623 140187804313408 spec.py:349] Evaluating on the test split.
I0130 03:49:03.195920 140187804313408 submission_runner.py:408] Time since start: 29621.39s, 	Step: 84137, 	{'train/accuracy': 0.7523317933082581, 'train/loss': 0.9355828166007996, 'validation/accuracy': 0.6721799969673157, 'validation/loss': 1.334437608718872, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.071115255355835, 'test/num_examples': 10000, 'score': 28597.013434410095, 'total_duration': 29621.394691944122, 'accumulated_submission_time': 28597.013434410095, 'accumulated_eval_time': 1019.1362066268921, 'accumulated_logging_time': 2.137763500213623}
I0130 03:49:03.231456 140026016913152 logging_writer.py:48] [84137] accumulated_eval_time=1019.136207, accumulated_logging_time=2.137764, accumulated_submission_time=28597.013434, global_step=84137, preemption_count=0, score=28597.013434, test/accuracy=0.541100, test/loss=2.071115, test/num_examples=10000, total_duration=29621.394692, train/accuracy=0.752332, train/loss=0.935583, validation/accuracy=0.672180, validation/loss=1.334438, validation/num_examples=50000
I0130 03:49:24.924193 140026025305856 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.073011875152588, loss=1.332646369934082
I0130 03:49:58.795686 140026016913152 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.0718841552734375, loss=1.5089083909988403
I0130 03:50:32.729911 140026025305856 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.4728267192840576, loss=1.4100717306137085
I0130 03:51:06.662063 140026016913152 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.2362751960754395, loss=1.4528520107269287
I0130 03:51:40.564153 140026025305856 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.2177555561065674, loss=1.5256519317626953
I0130 03:52:14.498879 140026016913152 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.2893471717834473, loss=1.4477481842041016
I0130 03:52:48.650257 140026025305856 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.4227938652038574, loss=1.443297028541565
I0130 03:53:22.556489 140026016913152 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.375715494155884, loss=1.3607282638549805
I0130 03:53:56.519848 140026025305856 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.3653690814971924, loss=1.4470655918121338
I0130 03:54:30.454900 140026016913152 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.202291965484619, loss=1.433333396911621
I0130 03:55:04.359344 140026025305856 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.1378965377807617, loss=1.3888927698135376
I0130 03:55:38.317239 140026016913152 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.1350462436676025, loss=1.436445713043213
I0130 03:56:12.279237 140026025305856 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.251011371612549, loss=1.4092156887054443
I0130 03:56:46.239615 140026016913152 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.3525965213775635, loss=1.4556386470794678
I0130 03:57:20.188967 140026025305856 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.1506288051605225, loss=1.4090359210968018
I0130 03:57:33.221777 140187804313408 spec.py:321] Evaluating on the training split.
I0130 03:57:39.458364 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 03:57:48.118378 140187804313408 spec.py:349] Evaluating on the test split.
I0130 03:57:50.821671 140187804313408 submission_runner.py:408] Time since start: 30149.02s, 	Step: 85640, 	{'train/accuracy': 0.7469307780265808, 'train/loss': 0.961391270160675, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.3483177423477173, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.091202735900879, 'test/num_examples': 10000, 'score': 29106.938084840775, 'total_duration': 30149.020438194275, 'accumulated_submission_time': 29106.938084840775, 'accumulated_eval_time': 1036.7360591888428, 'accumulated_logging_time': 2.182596445083618}
I0130 03:57:50.854887 140026058876672 logging_writer.py:48] [85640] accumulated_eval_time=1036.736059, accumulated_logging_time=2.182596, accumulated_submission_time=29106.938085, global_step=85640, preemption_count=0, score=29106.938085, test/accuracy=0.549400, test/loss=2.091203, test/num_examples=10000, total_duration=30149.020438, train/accuracy=0.746931, train/loss=0.961391, validation/accuracy=0.669760, validation/loss=1.348318, validation/num_examples=50000
I0130 03:58:11.514768 140026067269376 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.1203112602233887, loss=1.4822344779968262
I0130 03:58:45.364756 140026058876672 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.374760389328003, loss=1.394275426864624
I0130 03:59:19.384872 140026067269376 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.2995455265045166, loss=1.4715023040771484
I0130 03:59:53.317097 140026058876672 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.2619876861572266, loss=1.4336055517196655
I0130 04:00:27.214044 140026067269376 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.14980149269104, loss=1.4249979257583618
I0130 04:01:01.141839 140026058876672 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.2736291885375977, loss=1.4858318567276
I0130 04:01:35.095361 140026067269376 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.273099899291992, loss=1.3990049362182617
I0130 04:02:09.007574 140026058876672 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.254497766494751, loss=1.4819267988204956
I0130 04:02:42.913481 140026067269376 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.5452256202697754, loss=1.438246488571167
I0130 04:03:16.849755 140026058876672 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.2892496585845947, loss=1.5164382457733154
I0130 04:03:50.779881 140026067269376 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.3634836673736572, loss=1.5674262046813965
I0130 04:04:24.706940 140026058876672 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.414053440093994, loss=1.4629307985305786
I0130 04:04:58.644098 140026067269376 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.2588648796081543, loss=1.3782405853271484
I0130 04:05:32.580029 140026058876672 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.0881125926971436, loss=1.3662724494934082
I0130 04:06:06.715700 140026067269376 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.380244255065918, loss=1.477571964263916
I0130 04:06:21.108378 140187804313408 spec.py:321] Evaluating on the training split.
I0130 04:06:27.404464 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 04:06:36.077380 140187804313408 spec.py:349] Evaluating on the test split.
I0130 04:06:38.676108 140187804313408 submission_runner.py:408] Time since start: 30676.87s, 	Step: 87144, 	{'train/accuracy': 0.7379224896430969, 'train/loss': 0.9985449314117432, 'validation/accuracy': 0.6692799925804138, 'validation/loss': 1.3523105382919312, 'validation/num_examples': 50000, 'test/accuracy': 0.534600019454956, 'test/loss': 2.0925774574279785, 'test/num_examples': 10000, 'score': 29617.127873182297, 'total_duration': 30676.874872922897, 'accumulated_submission_time': 29617.127873182297, 'accumulated_eval_time': 1054.3037416934967, 'accumulated_logging_time': 2.2248997688293457}
I0130 04:06:38.713261 140026025305856 logging_writer.py:48] [87144] accumulated_eval_time=1054.303742, accumulated_logging_time=2.224900, accumulated_submission_time=29617.127873, global_step=87144, preemption_count=0, score=29617.127873, test/accuracy=0.534600, test/loss=2.092577, test/num_examples=10000, total_duration=30676.874873, train/accuracy=0.737922, train/loss=0.998545, validation/accuracy=0.669280, validation/loss=1.352311, validation/num_examples=50000
I0130 04:06:58.033777 140026033698560 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.1459767818450928, loss=1.3959379196166992
I0130 04:07:31.943607 140026025305856 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.351672410964966, loss=1.3991731405258179
I0130 04:08:05.891420 140026033698560 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.281750202178955, loss=1.4867078065872192
I0130 04:08:39.835904 140026025305856 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.436471939086914, loss=1.45703125
I0130 04:09:13.788775 140026033698560 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.2332258224487305, loss=1.4204853773117065
I0130 04:09:47.736563 140026025305856 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.2364561557769775, loss=1.4766546487808228
I0130 04:10:21.698956 140026033698560 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.3156797885894775, loss=1.4453009366989136
I0130 04:10:55.615648 140026025305856 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.27103590965271, loss=1.4449474811553955
I0130 04:11:29.566869 140026033698560 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.233870267868042, loss=1.4777013063430786
I0130 04:12:03.481359 140026025305856 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.1888041496276855, loss=1.4090532064437866
I0130 04:12:37.504324 140026033698560 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.427429437637329, loss=1.5192699432373047
I0130 04:13:11.435005 140026025305856 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.2756636142730713, loss=1.3441113233566284
I0130 04:13:45.406375 140026033698560 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.3359594345092773, loss=1.472625970840454
I0130 04:14:19.374339 140026025305856 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.4289283752441406, loss=1.5084697008132935
I0130 04:14:53.324591 140026033698560 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.4707982540130615, loss=1.3809337615966797
I0130 04:15:08.717377 140187804313408 spec.py:321] Evaluating on the training split.
I0130 04:15:14.984055 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 04:15:23.858136 140187804313408 spec.py:349] Evaluating on the test split.
I0130 04:15:26.516713 140187804313408 submission_runner.py:408] Time since start: 31204.72s, 	Step: 88647, 	{'train/accuracy': 0.7837810516357422, 'train/loss': 0.8073683381080627, 'validation/accuracy': 0.6779199838638306, 'validation/loss': 1.3172043561935425, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.068551778793335, 'test/num_examples': 10000, 'score': 30127.066581964493, 'total_duration': 31204.71547460556, 'accumulated_submission_time': 30127.066581964493, 'accumulated_eval_time': 1072.1030399799347, 'accumulated_logging_time': 2.2718443870544434}
I0130 04:15:26.552896 140025882715904 logging_writer.py:48] [88647] accumulated_eval_time=1072.103040, accumulated_logging_time=2.271844, accumulated_submission_time=30127.066582, global_step=88647, preemption_count=0, score=30127.066582, test/accuracy=0.542400, test/loss=2.068552, test/num_examples=10000, total_duration=31204.715475, train/accuracy=0.783781, train/loss=0.807368, validation/accuracy=0.677920, validation/loss=1.317204, validation/num_examples=50000
I0130 04:15:44.847015 140025891108608 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.1401748657226562, loss=1.3085740804672241
I0130 04:16:18.729911 140025882715904 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.1915953159332275, loss=1.478866696357727
I0130 04:16:52.652833 140025891108608 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.1414711475372314, loss=1.3954877853393555
I0130 04:17:26.578258 140025882715904 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.1667802333831787, loss=1.3843083381652832
I0130 04:18:00.527728 140025891108608 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.2312448024749756, loss=1.3594934940338135
I0130 04:18:34.469543 140025882715904 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.1861398220062256, loss=1.4331492185592651
I0130 04:19:08.599761 140025891108608 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.270444869995117, loss=1.3991990089416504
I0130 04:19:42.480659 140025882715904 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.3833296298980713, loss=1.4947888851165771
I0130 04:20:16.436530 140025891108608 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.398066759109497, loss=1.4890402555465698
I0130 04:20:50.320482 140025882715904 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.2597012519836426, loss=1.5155415534973145
I0130 04:21:24.252187 140025891108608 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.2060811519622803, loss=1.322051763534546
I0130 04:21:58.162022 140025882715904 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.317599296569824, loss=1.4679303169250488
I0130 04:22:32.102368 140025891108608 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.4774117469787598, loss=1.4988648891448975
I0130 04:23:05.992015 140025882715904 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.40094256401062, loss=1.534916639328003
I0130 04:23:39.923985 140025891108608 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.573183059692383, loss=1.4362986087799072
I0130 04:23:56.687559 140187804313408 spec.py:321] Evaluating on the training split.
I0130 04:24:03.189438 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 04:24:11.964092 140187804313408 spec.py:349] Evaluating on the test split.
I0130 04:24:14.681539 140187804313408 submission_runner.py:408] Time since start: 31732.88s, 	Step: 90151, 	{'train/accuracy': 0.7757493257522583, 'train/loss': 0.8434444069862366, 'validation/accuracy': 0.6840199828147888, 'validation/loss': 1.2816094160079956, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.0205748081207275, 'test/num_examples': 10000, 'score': 30637.134697914124, 'total_duration': 31732.880301237106, 'accumulated_submission_time': 30637.134697914124, 'accumulated_eval_time': 1090.0969746112823, 'accumulated_logging_time': 2.317918062210083}
I0130 04:24:14.719782 140026050483968 logging_writer.py:48] [90151] accumulated_eval_time=1090.096975, accumulated_logging_time=2.317918, accumulated_submission_time=30637.134698, global_step=90151, preemption_count=0, score=30637.134698, test/accuracy=0.553900, test/loss=2.020575, test/num_examples=10000, total_duration=31732.880301, train/accuracy=0.775749, train/loss=0.843444, validation/accuracy=0.684020, validation/loss=1.281609, validation/num_examples=50000
I0130 04:24:31.685749 140026058876672 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.39556884765625, loss=1.4505155086517334
I0130 04:25:05.543897 140026050483968 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.1321065425872803, loss=1.4871571063995361
I0130 04:25:39.594970 140026058876672 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.382995128631592, loss=1.3179770708084106
I0130 04:26:13.498718 140026050483968 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.1666088104248047, loss=1.3858652114868164
I0130 04:26:47.430682 140026058876672 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.3636550903320312, loss=1.3689476251602173
I0130 04:27:21.331218 140026050483968 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.1408743858337402, loss=1.489753007888794
I0130 04:27:55.279654 140026058876672 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.237884521484375, loss=1.3435066938400269
I0130 04:28:29.192731 140026050483968 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.2454638481140137, loss=1.40619695186615
I0130 04:29:03.144704 140026058876672 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.2789626121520996, loss=1.327444076538086
I0130 04:29:37.090648 140026050483968 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.2026021480560303, loss=1.4205557107925415
I0130 04:30:10.983926 140026058876672 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.2938427925109863, loss=1.401855230331421
I0130 04:30:44.917371 140026050483968 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.4401068687438965, loss=1.4385555982589722
I0130 04:31:18.829337 140026058876672 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.339477300643921, loss=1.3733174800872803
I0130 04:31:52.763701 140026050483968 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.272707223892212, loss=1.3717565536499023
I0130 04:32:26.763003 140026058876672 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.2939000129699707, loss=1.4773626327514648
I0130 04:32:44.912838 140187804313408 spec.py:321] Evaluating on the training split.
I0130 04:32:51.411632 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 04:32:59.862864 140187804313408 spec.py:349] Evaluating on the test split.
I0130 04:33:02.567229 140187804313408 submission_runner.py:408] Time since start: 32260.77s, 	Step: 91655, 	{'train/accuracy': 0.7606425285339355, 'train/loss': 0.8926047086715698, 'validation/accuracy': 0.6730200052261353, 'validation/loss': 1.342376470565796, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.098851203918457, 'test/num_examples': 10000, 'score': 31147.257095575333, 'total_duration': 32260.765964984894, 'accumulated_submission_time': 31147.257095575333, 'accumulated_eval_time': 1107.751292705536, 'accumulated_logging_time': 2.369476079940796}
I0130 04:33:02.607873 140026016913152 logging_writer.py:48] [91655] accumulated_eval_time=1107.751293, accumulated_logging_time=2.369476, accumulated_submission_time=31147.257096, global_step=91655, preemption_count=0, score=31147.257096, test/accuracy=0.548100, test/loss=2.098851, test/num_examples=10000, total_duration=32260.765965, train/accuracy=0.760643, train/loss=0.892605, validation/accuracy=0.673020, validation/loss=1.342376, validation/num_examples=50000
I0130 04:33:18.208381 140026025305856 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.498075008392334, loss=1.4637212753295898
I0130 04:33:52.087742 140026016913152 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.447047472000122, loss=1.4444563388824463
I0130 04:34:25.982229 140026025305856 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.2809901237487793, loss=1.4296048879623413
I0130 04:34:59.880938 140026016913152 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.1955597400665283, loss=1.423049807548523
I0130 04:35:33.791696 140026025305856 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.228085517883301, loss=1.3621716499328613
I0130 04:36:07.707682 140026016913152 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.3548953533172607, loss=1.3496112823486328
I0130 04:36:41.639504 140026025305856 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.219881534576416, loss=1.3489006757736206
I0130 04:37:15.555311 140026016913152 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.3616771697998047, loss=1.3875517845153809
I0130 04:37:49.463610 140026025305856 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.067969799041748, loss=1.342323660850525
I0130 04:38:23.381376 140026016913152 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.2838246822357178, loss=1.2582308053970337
I0130 04:38:57.478873 140026025305856 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.475274085998535, loss=1.3808709383010864
I0130 04:39:31.388177 140026016913152 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.7254891395568848, loss=1.2972036600112915
I0130 04:40:05.310553 140026025305856 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.5693628787994385, loss=1.4493721723556519
I0130 04:40:39.197414 140026016913152 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.3421108722686768, loss=1.4512251615524292
I0130 04:41:13.126199 140026025305856 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.131824254989624, loss=1.2794474363327026
I0130 04:41:32.614103 140187804313408 spec.py:321] Evaluating on the training split.
I0130 04:41:39.028193 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 04:41:47.671062 140187804313408 spec.py:349] Evaluating on the test split.
I0130 04:41:50.373460 140187804313408 submission_runner.py:408] Time since start: 32788.57s, 	Step: 93159, 	{'train/accuracy': 0.7567561864852905, 'train/loss': 0.9086245894432068, 'validation/accuracy': 0.6795399785041809, 'validation/loss': 1.3091254234313965, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 2.040935516357422, 'test/num_examples': 10000, 'score': 31657.1976583004, 'total_duration': 32788.57222819328, 'accumulated_submission_time': 31657.1976583004, 'accumulated_eval_time': 1125.5106115341187, 'accumulated_logging_time': 2.420135974884033}
I0130 04:41:50.407646 140026025305856 logging_writer.py:48] [93159] accumulated_eval_time=1125.510612, accumulated_logging_time=2.420136, accumulated_submission_time=31657.197658, global_step=93159, preemption_count=0, score=31657.197658, test/accuracy=0.551100, test/loss=2.040936, test/num_examples=10000, total_duration=32788.572228, train/accuracy=0.756756, train/loss=0.908625, validation/accuracy=0.679540, validation/loss=1.309125, validation/num_examples=50000
I0130 04:42:04.631018 140026042091264 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.3286261558532715, loss=1.3918863534927368
I0130 04:42:38.481421 140026025305856 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.2431423664093018, loss=1.2713594436645508
I0130 04:43:12.416314 140026042091264 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.4284703731536865, loss=1.5470696687698364
I0130 04:43:46.288180 140026025305856 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.4429242610931396, loss=1.3118890523910522
I0130 04:44:20.231285 140026042091264 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.2914602756500244, loss=1.379267692565918
I0130 04:44:54.151387 140026025305856 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.355379104614258, loss=1.3532373905181885
I0130 04:45:28.218674 140026042091264 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.1915929317474365, loss=1.3564999103546143
I0130 04:46:02.135810 140026025305856 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.2401814460754395, loss=1.2660026550292969
I0130 04:46:36.045484 140026042091264 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.5270087718963623, loss=1.4873132705688477
I0130 04:47:09.985801 140026025305856 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.333418130874634, loss=1.3917715549468994
I0130 04:47:43.938528 140026042091264 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.500328540802002, loss=1.400889277458191
I0130 04:48:17.900959 140026025305856 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.4290571212768555, loss=1.290116548538208
I0130 04:48:51.811385 140026042091264 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.5048885345458984, loss=1.2745319604873657
I0130 04:49:25.737279 140026025305856 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.3119630813598633, loss=1.4064264297485352
I0130 04:49:59.688653 140026042091264 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.347846269607544, loss=1.3956224918365479
I0130 04:50:20.536098 140187804313408 spec.py:321] Evaluating on the training split.
I0130 04:50:26.822903 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 04:50:35.693364 140187804313408 spec.py:349] Evaluating on the test split.
I0130 04:50:38.380132 140187804313408 submission_runner.py:408] Time since start: 33316.58s, 	Step: 94663, 	{'train/accuracy': 0.7587292790412903, 'train/loss': 0.9139228463172913, 'validation/accuracy': 0.6759399771690369, 'validation/loss': 1.32182776927948, 'validation/num_examples': 50000, 'test/accuracy': 0.5546000003814697, 'test/loss': 2.026935338973999, 'test/num_examples': 10000, 'score': 32167.262050628662, 'total_duration': 33316.578904390335, 'accumulated_submission_time': 32167.262050628662, 'accumulated_eval_time': 1143.3546307086945, 'accumulated_logging_time': 2.4641125202178955}
I0130 04:50:38.420889 140026016913152 logging_writer.py:48] [94663] accumulated_eval_time=1143.354631, accumulated_logging_time=2.464113, accumulated_submission_time=32167.262051, global_step=94663, preemption_count=0, score=32167.262051, test/accuracy=0.554600, test/loss=2.026935, test/num_examples=10000, total_duration=33316.578904, train/accuracy=0.758729, train/loss=0.913923, validation/accuracy=0.675940, validation/loss=1.321828, validation/num_examples=50000
I0130 04:50:51.307841 140026025305856 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.321333646774292, loss=1.3066744804382324
I0130 04:51:25.158197 140026016913152 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.5399980545043945, loss=1.496248722076416
I0130 04:51:59.141302 140026025305856 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.558795928955078, loss=1.3089113235473633
I0130 04:52:33.053992 140026016913152 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.486598014831543, loss=1.431522250175476
I0130 04:53:06.984496 140026025305856 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.677325963973999, loss=1.4239885807037354
I0130 04:53:40.900999 140026016913152 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.135922908782959, loss=1.2516310214996338
I0130 04:54:14.823689 140026025305856 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.3655924797058105, loss=1.394777536392212
I0130 04:54:48.799988 140026016913152 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.340644121170044, loss=1.3476307392120361
I0130 04:55:22.749616 140026025305856 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.5207138061523438, loss=1.3381184339523315
I0130 04:55:56.646665 140026016913152 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.5196170806884766, loss=1.3349740505218506
I0130 04:56:30.591746 140026025305856 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.3785452842712402, loss=1.3783522844314575
I0130 04:57:04.498332 140026016913152 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.407627820968628, loss=1.4155726432800293
I0130 04:57:38.437899 140026025305856 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.61711049079895, loss=1.3084756135940552
I0130 04:58:12.351272 140026016913152 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.393024206161499, loss=1.5454094409942627
I0130 04:58:46.469287 140026025305856 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.339322566986084, loss=1.532477617263794
I0130 04:59:08.651185 140187804313408 spec.py:321] Evaluating on the training split.
I0130 04:59:14.962989 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 04:59:23.946503 140187804313408 spec.py:349] Evaluating on the test split.
I0130 04:59:26.645407 140187804313408 submission_runner.py:408] Time since start: 33844.84s, 	Step: 96167, 	{'train/accuracy': 0.7642498016357422, 'train/loss': 0.8854460120201111, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.2905325889587402, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.0234200954437256, 'test/num_examples': 10000, 'score': 32677.42819571495, 'total_duration': 33844.844178915024, 'accumulated_submission_time': 32677.42819571495, 'accumulated_eval_time': 1161.3488364219666, 'accumulated_logging_time': 2.514535427093506}
I0130 04:59:26.684031 140026042091264 logging_writer.py:48] [96167] accumulated_eval_time=1161.348836, accumulated_logging_time=2.514535, accumulated_submission_time=32677.428196, global_step=96167, preemption_count=0, score=32677.428196, test/accuracy=0.550800, test/loss=2.023420, test/num_examples=10000, total_duration=33844.844179, train/accuracy=0.764250, train/loss=0.885446, validation/accuracy=0.684560, validation/loss=1.290533, validation/num_examples=50000
I0130 04:59:38.227711 140026050483968 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.5496268272399902, loss=1.4392352104187012
I0130 05:00:12.505721 140026042091264 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.4880170822143555, loss=1.3789350986480713
I0130 05:00:46.468873 140026050483968 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.2571523189544678, loss=1.3182851076126099
I0130 05:01:20.417265 140026042091264 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.3935699462890625, loss=1.3946295976638794
I0130 05:01:54.359075 140026050483968 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.490692615509033, loss=1.4540539979934692
I0130 05:02:28.310445 140026042091264 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.474050521850586, loss=1.4314042329788208
I0130 05:03:02.267101 140026050483968 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.6982221603393555, loss=1.3219598531723022
I0130 05:03:36.192182 140026042091264 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.664856433868408, loss=1.3585432767868042
I0130 05:04:10.162949 140026050483968 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.4001216888427734, loss=1.3537474870681763
I0130 05:04:44.126459 140026042091264 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.6514315605163574, loss=1.3449419736862183
I0130 05:05:18.261209 140026050483968 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.2793662548065186, loss=1.3463290929794312
I0130 05:05:52.232332 140026042091264 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.366518259048462, loss=1.321480631828308
I0130 05:06:26.173015 140026050483968 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.437925100326538, loss=1.3502521514892578
I0130 05:07:00.127733 140026042091264 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.384497880935669, loss=1.217527151107788
I0130 05:07:34.088962 140026050483968 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.414062738418579, loss=1.4364615678787231
I0130 05:07:56.953153 140187804313408 spec.py:321] Evaluating on the training split.
I0130 05:08:04.068329 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 05:08:12.993527 140187804313408 spec.py:349] Evaluating on the test split.
I0130 05:08:15.737038 140187804313408 submission_runner.py:408] Time since start: 34373.94s, 	Step: 97669, 	{'train/accuracy': 0.8039301633834839, 'train/loss': 0.7246366143226624, 'validation/accuracy': 0.6893399953842163, 'validation/loss': 1.2651199102401733, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.0260353088378906, 'test/num_examples': 10000, 'score': 33187.63098335266, 'total_duration': 34373.93581676483, 'accumulated_submission_time': 33187.63098335266, 'accumulated_eval_time': 1180.1326916217804, 'accumulated_logging_time': 2.562276601791382}
I0130 05:08:15.770676 140025891108608 logging_writer.py:48] [97669] accumulated_eval_time=1180.132692, accumulated_logging_time=2.562277, accumulated_submission_time=33187.630983, global_step=97669, preemption_count=0, score=33187.630983, test/accuracy=0.552800, test/loss=2.026035, test/num_examples=10000, total_duration=34373.935817, train/accuracy=0.803930, train/loss=0.724637, validation/accuracy=0.689340, validation/loss=1.265120, validation/num_examples=50000
I0130 05:08:26.635712 140026016913152 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.4520342350006104, loss=1.3664493560791016
I0130 05:09:00.544398 140025891108608 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.496882200241089, loss=1.4462220668792725
I0130 05:09:34.435845 140026016913152 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.526639461517334, loss=1.3114527463912964
I0130 05:10:08.367663 140025891108608 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.3227388858795166, loss=1.4162176847457886
I0130 05:10:42.308369 140026016913152 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.395979166030884, loss=1.3285995721817017
I0130 05:11:16.256785 140025891108608 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.373967170715332, loss=1.4552193880081177
I0130 05:11:50.310116 140026016913152 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.485755681991577, loss=1.3520950078964233
I0130 05:12:24.265052 140025891108608 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.229407787322998, loss=1.2380858659744263
I0130 05:12:58.184468 140026016913152 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.522012710571289, loss=1.3985466957092285
I0130 05:13:32.092109 140025891108608 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.6074039936065674, loss=1.4048181772232056
I0130 05:14:06.025195 140026016913152 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.355929136276245, loss=1.360518217086792
I0130 05:14:39.933430 140025891108608 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.422767162322998, loss=1.469551682472229
I0130 05:15:13.881183 140026016913152 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.543365001678467, loss=1.411118745803833
I0130 05:15:47.798517 140025891108608 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.3015942573547363, loss=1.2763659954071045
I0130 05:16:21.744008 140026016913152 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.330650568008423, loss=1.2051395177841187
I0130 05:16:45.994680 140187804313408 spec.py:321] Evaluating on the training split.
I0130 05:16:52.325008 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 05:17:00.997960 140187804313408 spec.py:349] Evaluating on the test split.
I0130 05:17:03.918759 140187804313408 submission_runner.py:408] Time since start: 34902.12s, 	Step: 99173, 	{'train/accuracy': 0.7836814522743225, 'train/loss': 0.7890745997428894, 'validation/accuracy': 0.6879400014877319, 'validation/loss': 1.2637015581130981, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.98188054561615, 'test/num_examples': 10000, 'score': 33697.79170894623, 'total_duration': 34902.117525577545, 'accumulated_submission_time': 33697.79170894623, 'accumulated_eval_time': 1198.0567321777344, 'accumulated_logging_time': 2.6048731803894043}
I0130 05:17:03.954886 140026050483968 logging_writer.py:48] [99173] accumulated_eval_time=1198.056732, accumulated_logging_time=2.604873, accumulated_submission_time=33697.791709, global_step=99173, preemption_count=0, score=33697.791709, test/accuracy=0.565400, test/loss=1.981881, test/num_examples=10000, total_duration=34902.117526, train/accuracy=0.783681, train/loss=0.789075, validation/accuracy=0.687940, validation/loss=1.263702, validation/num_examples=50000
I0130 05:17:13.461828 140026058876672 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.532177686691284, loss=1.3313288688659668
I0130 05:17:47.346702 140026050483968 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.754624128341675, loss=1.485398530960083
I0130 05:18:21.326631 140026058876672 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.7315433025360107, loss=1.3367764949798584
I0130 05:18:55.271509 140026050483968 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.5970184803009033, loss=1.3965402841567993
I0130 05:19:29.201321 140026058876672 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.5105254650115967, loss=1.3278638124465942
I0130 05:20:03.167330 140026050483968 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.524336338043213, loss=1.4230793714523315
I0130 05:20:37.126976 140026058876672 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.5618231296539307, loss=1.3079545497894287
I0130 05:21:11.075955 140026050483968 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.5355989933013916, loss=1.3109850883483887
I0130 05:21:45.038373 140026058876672 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.331977367401123, loss=1.3488092422485352
I0130 05:22:18.979066 140026050483968 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.6111185550689697, loss=1.3573343753814697
I0130 05:22:52.907520 140026058876672 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.4350857734680176, loss=1.2546038627624512
I0130 05:23:26.857302 140026050483968 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.4468994140625, loss=1.321762204170227
I0130 05:24:00.825937 140026058876672 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.4848380088806152, loss=1.2834502458572388
I0130 05:24:34.789646 140026050483968 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.4144930839538574, loss=1.3429760932922363
I0130 05:25:08.819101 140026058876672 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.4507718086242676, loss=1.2833061218261719
I0130 05:25:34.101164 140187804313408 spec.py:321] Evaluating on the training split.
I0130 05:25:40.372944 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 05:25:49.311012 140187804313408 spec.py:349] Evaluating on the test split.
I0130 05:25:52.008877 140187804313408 submission_runner.py:408] Time since start: 35430.21s, 	Step: 100676, 	{'train/accuracy': 0.7801538705825806, 'train/loss': 0.8223650455474854, 'validation/accuracy': 0.689579963684082, 'validation/loss': 1.2677134275436401, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 1.9989219903945923, 'test/num_examples': 10000, 'score': 34207.87293791771, 'total_duration': 35430.207654953, 'accumulated_submission_time': 34207.87293791771, 'accumulated_eval_time': 1215.964411497116, 'accumulated_logging_time': 2.652205467224121}
I0130 05:25:52.045674 140026025305856 logging_writer.py:48] [100676] accumulated_eval_time=1215.964411, accumulated_logging_time=2.652205, accumulated_submission_time=34207.872938, global_step=100676, preemption_count=0, score=34207.872938, test/accuracy=0.560100, test/loss=1.998922, test/num_examples=10000, total_duration=35430.207655, train/accuracy=0.780154, train/loss=0.822365, validation/accuracy=0.689580, validation/loss=1.267713, validation/num_examples=50000
I0130 05:26:00.541574 140026033698560 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.248992443084717, loss=1.3041647672653198
I0130 05:26:34.399237 140026025305856 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.4778058528900146, loss=1.3562989234924316
I0130 05:27:08.318115 140026033698560 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.6017119884490967, loss=1.4003334045410156
I0130 05:27:42.237718 140026025305856 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.417781114578247, loss=1.3541059494018555
I0130 05:28:16.184812 140026033698560 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.5759758949279785, loss=1.4275224208831787
I0130 05:28:50.090653 140026025305856 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.423173427581787, loss=1.2286944389343262
I0130 05:29:24.033067 140026033698560 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.5352447032928467, loss=1.3495399951934814
I0130 05:29:57.980171 140026025305856 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.787269353866577, loss=1.3864825963974
I0130 05:30:31.893833 140026033698560 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.477137804031372, loss=1.2711706161499023
I0130 05:31:05.822851 140026025305856 logging_writer.py:48] [101600] global_step=101600, grad_norm=3.091884136199951, loss=1.2901129722595215
I0130 05:31:39.850828 140026033698560 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.4864883422851562, loss=1.4398270845413208
I0130 05:32:13.778387 140026025305856 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.7935972213745117, loss=1.3056485652923584
I0130 05:32:47.730297 140026033698560 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.650485038757324, loss=1.4383081197738647
I0130 05:33:21.650679 140026025305856 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.3921139240264893, loss=1.4001585245132446
I0130 05:33:55.604257 140026033698560 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.8012373447418213, loss=1.262209415435791
I0130 05:34:22.206788 140187804313408 spec.py:321] Evaluating on the training split.
I0130 05:34:28.604445 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 05:34:37.162733 140187804313408 spec.py:349] Evaluating on the test split.
I0130 05:34:39.863435 140187804313408 submission_runner.py:408] Time since start: 35958.06s, 	Step: 102180, 	{'train/accuracy': 0.78226637840271, 'train/loss': 0.813494086265564, 'validation/accuracy': 0.6924200057983398, 'validation/loss': 1.249699354171753, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9510797262191772, 'test/num_examples': 10000, 'score': 34717.97048306465, 'total_duration': 35958.062203884125, 'accumulated_submission_time': 34717.97048306465, 'accumulated_eval_time': 1233.6210358142853, 'accumulated_logging_time': 2.6980385780334473}
I0130 05:34:39.899246 140026050483968 logging_writer.py:48] [102180] accumulated_eval_time=1233.621036, accumulated_logging_time=2.698039, accumulated_submission_time=34717.970483, global_step=102180, preemption_count=0, score=34717.970483, test/accuracy=0.567800, test/loss=1.951080, test/num_examples=10000, total_duration=35958.062204, train/accuracy=0.782266, train/loss=0.813494, validation/accuracy=0.692420, validation/loss=1.249699, validation/num_examples=50000
I0130 05:34:47.041698 140026058876672 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.4695017337799072, loss=1.4175496101379395
I0130 05:35:20.909590 140026050483968 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.478219985961914, loss=1.3719090223312378
I0130 05:35:54.871649 140026058876672 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.398846387863159, loss=1.1609867811203003
I0130 05:36:28.806148 140026050483968 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.662475109100342, loss=1.2878063917160034
I0130 05:37:02.685856 140026058876672 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.493016004562378, loss=1.2110230922698975
I0130 05:37:36.661833 140026050483968 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.5901451110839844, loss=1.1654635667800903
I0130 05:38:10.832868 140026058876672 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.4719088077545166, loss=1.4109411239624023
I0130 05:38:44.794480 140026050483968 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.782104969024658, loss=1.4213531017303467
I0130 05:39:18.728633 140026058876672 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.4609053134918213, loss=1.2728157043457031
I0130 05:39:52.676914 140026050483968 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.457838773727417, loss=1.3185807466506958
I0130 05:40:26.595075 140026058876672 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.499238967895508, loss=1.283911108970642
I0130 05:41:00.558573 140026050483968 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.8793649673461914, loss=1.2554863691329956
I0130 05:41:34.512260 140026058876672 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.393172025680542, loss=1.183220386505127
I0130 05:42:08.420498 140026050483968 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.6669747829437256, loss=1.456009864807129
I0130 05:42:42.371474 140026058876672 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.607128858566284, loss=1.2889481782913208
I0130 05:43:10.019940 140187804313408 spec.py:321] Evaluating on the training split.
I0130 05:43:16.313381 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 05:43:24.958058 140187804313408 spec.py:349] Evaluating on the test split.
I0130 05:43:27.673757 140187804313408 submission_runner.py:408] Time since start: 36485.87s, 	Step: 103683, 	{'train/accuracy': 0.7759087681770325, 'train/loss': 0.8256220817565918, 'validation/accuracy': 0.6915599703788757, 'validation/loss': 1.2632023096084595, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 1.9927477836608887, 'test/num_examples': 10000, 'score': 35228.02629613876, 'total_duration': 36485.87252473831, 'accumulated_submission_time': 35228.02629613876, 'accumulated_eval_time': 1251.2748274803162, 'accumulated_logging_time': 2.7438771724700928}
I0130 05:43:27.714225 140026016913152 logging_writer.py:48] [103683] accumulated_eval_time=1251.274827, accumulated_logging_time=2.743877, accumulated_submission_time=35228.026296, global_step=103683, preemption_count=0, score=35228.026296, test/accuracy=0.562800, test/loss=1.992748, test/num_examples=10000, total_duration=36485.872525, train/accuracy=0.775909, train/loss=0.825622, validation/accuracy=0.691560, validation/loss=1.263202, validation/num_examples=50000
I0130 05:43:33.796160 140026025305856 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.439481019973755, loss=1.3526737689971924
I0130 05:44:07.683347 140026016913152 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.448211193084717, loss=1.200967788696289
I0130 05:44:41.733534 140026025305856 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.722353219985962, loss=1.3578715324401855
I0130 05:45:15.651430 140026016913152 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.5914976596832275, loss=1.3476979732513428
I0130 05:45:49.605844 140026025305856 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.4525389671325684, loss=1.325493574142456
I0130 05:46:23.558514 140026016913152 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.5152549743652344, loss=1.2464048862457275
I0130 05:46:57.476676 140026025305856 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.762932300567627, loss=1.246748685836792
I0130 05:47:31.417378 140026016913152 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.718287229537964, loss=1.3329781293869019
I0130 05:48:05.364282 140026025305856 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.475613832473755, loss=1.3518502712249756
I0130 05:48:39.286436 140026016913152 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.3849151134490967, loss=1.272019386291504
I0130 05:49:13.241697 140026025305856 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.8016159534454346, loss=1.351479411125183
I0130 05:49:47.159119 140026016913152 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.712847948074341, loss=1.2905519008636475
I0130 05:50:21.078651 140026025305856 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.459304094314575, loss=1.2716808319091797
I0130 05:50:54.981826 140026016913152 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.6883301734924316, loss=1.3102167844772339
I0130 05:51:29.075603 140026025305856 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.4588072299957275, loss=1.3612093925476074
I0130 05:51:57.732969 140187804313408 spec.py:321] Evaluating on the training split.
I0130 05:52:04.198275 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 05:52:12.772630 140187804313408 spec.py:349] Evaluating on the test split.
I0130 05:52:15.582013 140187804313408 submission_runner.py:408] Time since start: 37013.78s, 	Step: 105186, 	{'train/accuracy': 0.7785993218421936, 'train/loss': 0.8219675421714783, 'validation/accuracy': 0.6947000026702881, 'validation/loss': 1.2501887083053589, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 1.9662601947784424, 'test/num_examples': 10000, 'score': 35737.980461120605, 'total_duration': 37013.7807905674, 'accumulated_submission_time': 35737.980461120605, 'accumulated_eval_time': 1269.1238374710083, 'accumulated_logging_time': 2.7935187816619873}
I0130 05:52:15.614602 140026050483968 logging_writer.py:48] [105186] accumulated_eval_time=1269.123837, accumulated_logging_time=2.793519, accumulated_submission_time=35737.980461, global_step=105186, preemption_count=0, score=35737.980461, test/accuracy=0.564400, test/loss=1.966260, test/num_examples=10000, total_duration=37013.780791, train/accuracy=0.778599, train/loss=0.821968, validation/accuracy=0.694700, validation/loss=1.250189, validation/num_examples=50000
I0130 05:52:20.713939 140026058876672 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.613689422607422, loss=1.3172714710235596
I0130 05:52:54.553812 140026050483968 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.584012031555176, loss=1.4325275421142578
I0130 05:53:28.434523 140026058876672 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.563692808151245, loss=1.271959900856018
I0130 05:54:02.375149 140026050483968 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.437129497528076, loss=1.1952780485153198
I0130 05:54:36.307294 140026058876672 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.5690512657165527, loss=1.3076127767562866
I0130 05:55:10.243986 140026050483968 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.589263677597046, loss=1.293900728225708
I0130 05:55:44.179396 140026058876672 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.5020601749420166, loss=1.3011136054992676
I0130 05:56:18.162370 140026050483968 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.678257942199707, loss=1.3216328620910645
I0130 05:56:52.109199 140026058876672 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.9371132850646973, loss=1.3034015893936157
I0130 05:57:26.082829 140026050483968 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.4380900859832764, loss=1.254543423652649
I0130 05:58:00.123726 140026058876672 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.6612277030944824, loss=1.2943066358566284
I0130 05:58:34.092270 140026050483968 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.586684226989746, loss=1.422095537185669
I0130 05:59:08.032849 140026058876672 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.8250679969787598, loss=1.3136262893676758
I0130 05:59:41.937376 140026050483968 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.6842386722564697, loss=1.2587119340896606
I0130 06:00:15.879119 140026058876672 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.4722702503204346, loss=1.3182926177978516
I0130 06:00:45.914613 140187804313408 spec.py:321] Evaluating on the training split.
I0130 06:00:52.276786 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 06:01:01.129513 140187804313408 spec.py:349] Evaluating on the test split.
I0130 06:01:04.019589 140187804313408 submission_runner.py:408] Time since start: 37542.22s, 	Step: 106690, 	{'train/accuracy': 0.8191565275192261, 'train/loss': 0.6560265421867371, 'validation/accuracy': 0.6922399997711182, 'validation/loss': 1.2624542713165283, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.02057147026062, 'test/num_examples': 10000, 'score': 36248.21604681015, 'total_duration': 37542.218354701996, 'accumulated_submission_time': 36248.21604681015, 'accumulated_eval_time': 1287.228770017624, 'accumulated_logging_time': 2.8367748260498047}
I0130 06:01:04.059233 140026025305856 logging_writer.py:48] [106690] accumulated_eval_time=1287.228770, accumulated_logging_time=2.836775, accumulated_submission_time=36248.216047, global_step=106690, preemption_count=0, score=36248.216047, test/accuracy=0.568500, test/loss=2.020571, test/num_examples=10000, total_duration=37542.218355, train/accuracy=0.819157, train/loss=0.656027, validation/accuracy=0.692240, validation/loss=1.262454, validation/num_examples=50000
I0130 06:01:07.792891 140026033698560 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.6997809410095215, loss=1.3289217948913574
I0130 06:01:41.663040 140026025305856 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.742727756500244, loss=1.31502366065979
I0130 06:02:15.558307 140026033698560 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.504145860671997, loss=1.3007748126983643
I0130 06:02:49.512140 140026025305856 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.5440783500671387, loss=1.3283134698867798
I0130 06:03:23.467953 140026033698560 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.7158617973327637, loss=1.2888836860656738
I0130 06:03:57.358531 140026025305856 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.802492141723633, loss=1.4155950546264648
I0130 06:04:31.405019 140026033698560 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.7497074604034424, loss=1.256601333618164
I0130 06:05:05.340631 140026025305856 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.6328957080841064, loss=1.1989245414733887
I0130 06:05:39.293858 140026033698560 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.6285295486450195, loss=1.2672587633132935
I0130 06:06:13.186723 140026025305856 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.600069761276245, loss=1.3043076992034912
I0130 06:06:47.128475 140026033698560 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.7005741596221924, loss=1.3002318143844604
I0130 06:07:21.066937 140026025305856 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.6208584308624268, loss=1.3507499694824219
I0130 06:07:55.016098 140026033698560 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.6337454319000244, loss=1.1959986686706543
I0130 06:08:28.965405 140026025305856 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.82621693611145, loss=1.4178321361541748
I0130 06:09:02.917130 140026033698560 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.554738759994507, loss=1.242372989654541
I0130 06:09:34.276122 140187804313408 spec.py:321] Evaluating on the training split.
I0130 06:09:40.594151 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 06:09:49.251794 140187804313408 spec.py:349] Evaluating on the test split.
I0130 06:09:51.937610 140187804313408 submission_runner.py:408] Time since start: 38070.14s, 	Step: 108194, 	{'train/accuracy': 0.7940050959587097, 'train/loss': 0.7586848735809326, 'validation/accuracy': 0.6922000050544739, 'validation/loss': 1.26578688621521, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.0142979621887207, 'test/num_examples': 10000, 'score': 36758.368527173996, 'total_duration': 38070.13637447357, 'accumulated_submission_time': 36758.368527173996, 'accumulated_eval_time': 1304.8902144432068, 'accumulated_logging_time': 2.8855910301208496}
I0130 06:09:51.974694 140025891108608 logging_writer.py:48] [108194] accumulated_eval_time=1304.890214, accumulated_logging_time=2.885591, accumulated_submission_time=36758.368527, global_step=108194, preemption_count=0, score=36758.368527, test/accuracy=0.556600, test/loss=2.014298, test/num_examples=10000, total_duration=38070.136374, train/accuracy=0.794005, train/loss=0.758685, validation/accuracy=0.692200, validation/loss=1.265787, validation/num_examples=50000
I0130 06:09:54.352950 140026016913152 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.7380714416503906, loss=1.2438374757766724
I0130 06:10:28.229428 140025891108608 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.5917880535125732, loss=1.3623900413513184
I0130 06:11:02.173022 140026016913152 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.49186372756958, loss=1.3208378553390503
I0130 06:11:36.146951 140025891108608 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.7214725017547607, loss=1.2479217052459717
I0130 06:12:10.098289 140026016913152 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.534806489944458, loss=1.202810287475586
I0130 06:12:44.039512 140025891108608 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.593414306640625, loss=1.2785141468048096
I0130 06:13:17.945871 140026016913152 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.6334962844848633, loss=1.3323873281478882
I0130 06:13:51.888561 140025891108608 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.7279109954833984, loss=1.3023695945739746
I0130 06:14:25.828268 140026016913152 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.796069622039795, loss=1.261406660079956
I0130 06:14:59.738133 140025891108608 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.60465407371521, loss=1.304000735282898
I0130 06:15:33.683459 140026016913152 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.6154627799987793, loss=1.2905855178833008
I0130 06:16:07.651466 140025891108608 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.7135283946990967, loss=1.267448902130127
I0130 06:16:41.600306 140026016913152 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.8930118083953857, loss=1.3061953783035278
I0130 06:17:15.562936 140025891108608 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.6305675506591797, loss=1.2775864601135254
I0130 06:17:49.571258 140026016913152 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.3805220127105713, loss=1.2193821668624878
I0130 06:18:22.274653 140187804313408 spec.py:321] Evaluating on the training split.
I0130 06:18:28.565424 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 06:18:37.427724 140187804313408 spec.py:349] Evaluating on the test split.
I0130 06:18:40.111003 140187804313408 submission_runner.py:408] Time since start: 38598.31s, 	Step: 109698, 	{'train/accuracy': 0.7897400856018066, 'train/loss': 0.7790143489837646, 'validation/accuracy': 0.6975199580192566, 'validation/loss': 1.232077956199646, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 1.950202465057373, 'test/num_examples': 10000, 'score': 37268.601791381836, 'total_duration': 38598.309775829315, 'accumulated_submission_time': 37268.601791381836, 'accumulated_eval_time': 1322.726529121399, 'accumulated_logging_time': 2.933062791824341}
I0130 06:18:40.148455 140026033698560 logging_writer.py:48] [109698] accumulated_eval_time=1322.726529, accumulated_logging_time=2.933063, accumulated_submission_time=37268.601791, global_step=109698, preemption_count=0, score=37268.601791, test/accuracy=0.565300, test/loss=1.950202, test/num_examples=10000, total_duration=38598.309776, train/accuracy=0.789740, train/loss=0.779014, validation/accuracy=0.697520, validation/loss=1.232078, validation/num_examples=50000
I0130 06:18:41.180855 140026058876672 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.7244820594787598, loss=1.3327144384384155
I0130 06:19:15.020312 140026033698560 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.767921209335327, loss=1.279792308807373
I0130 06:19:48.903383 140026058876672 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.7296924591064453, loss=1.2462589740753174
I0130 06:20:22.829096 140026033698560 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.7605154514312744, loss=1.3072400093078613
I0130 06:20:56.772982 140026058876672 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.6032636165618896, loss=1.228564977645874
I0130 06:21:30.686240 140026033698560 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.747649908065796, loss=1.226781964302063
I0130 06:22:04.651383 140026058876672 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.7580204010009766, loss=1.2539345026016235
I0130 06:22:38.583015 140026033698560 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.768641233444214, loss=1.3474836349487305
I0130 06:23:12.528143 140026058876672 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.882884979248047, loss=1.2751784324645996
I0130 06:23:46.460928 140026033698560 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.7596473693847656, loss=1.339475154876709
I0130 06:24:20.460932 140026058876672 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.8875420093536377, loss=1.308774709701538
I0130 06:24:54.385084 140026033698560 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.883633852005005, loss=1.3079421520233154
I0130 06:25:28.330386 140026058876672 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.7338547706604004, loss=1.3038103580474854
I0130 06:26:02.230129 140026033698560 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.561884880065918, loss=1.2114540338516235
I0130 06:26:36.472213 140026058876672 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.6982078552246094, loss=1.2922167778015137
I0130 06:27:10.398854 140026033698560 logging_writer.py:48] [111200] global_step=111200, grad_norm=3.3191795349121094, loss=1.4012504816055298
I0130 06:27:10.406697 140187804313408 spec.py:321] Evaluating on the training split.
I0130 06:27:16.727019 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 06:27:25.618312 140187804313408 spec.py:349] Evaluating on the test split.
I0130 06:27:28.303852 140187804313408 submission_runner.py:408] Time since start: 39126.50s, 	Step: 111201, 	{'train/accuracy': 0.7818080186843872, 'train/loss': 0.8038931488990784, 'validation/accuracy': 0.6892600059509277, 'validation/loss': 1.272587776184082, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.025722026824951, 'test/num_examples': 10000, 'score': 37778.793501615524, 'total_duration': 39126.50262880325, 'accumulated_submission_time': 37778.793501615524, 'accumulated_eval_time': 1340.6236248016357, 'accumulated_logging_time': 2.9807047843933105}
I0130 06:27:28.344175 140026025305856 logging_writer.py:48] [111201] accumulated_eval_time=1340.623625, accumulated_logging_time=2.980705, accumulated_submission_time=37778.793502, global_step=111201, preemption_count=0, score=37778.793502, test/accuracy=0.559100, test/loss=2.025722, test/num_examples=10000, total_duration=39126.502629, train/accuracy=0.781808, train/loss=0.803893, validation/accuracy=0.689260, validation/loss=1.272588, validation/num_examples=50000
I0130 06:28:02.253238 140026042091264 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.690196990966797, loss=1.2385015487670898
I0130 06:28:36.161999 140026025305856 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.621544361114502, loss=1.1386852264404297
I0130 06:29:10.121724 140026042091264 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.404639959335327, loss=1.1693096160888672
I0130 06:29:44.046387 140026025305856 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.6712071895599365, loss=1.2936863899230957
I0130 06:30:17.998469 140026042091264 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.626737356185913, loss=1.1896032094955444
I0130 06:30:51.935355 140026025305856 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.642698049545288, loss=1.241968035697937
I0130 06:31:25.898248 140026042091264 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.760305643081665, loss=1.2697628736495972
I0130 06:31:59.828655 140026025305856 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.8513121604919434, loss=1.3428922891616821
I0130 06:32:33.786543 140026042091264 logging_writer.py:48] [112100] global_step=112100, grad_norm=3.0789639949798584, loss=1.3306727409362793
I0130 06:33:07.712009 140026025305856 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.5204007625579834, loss=1.1163434982299805
I0130 06:33:41.661471 140026042091264 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.8468940258026123, loss=1.2374320030212402
I0130 06:34:15.583412 140026025305856 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.754793167114258, loss=1.3828556537628174
I0130 06:34:49.488999 140026042091264 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.8146417140960693, loss=1.280200719833374
I0130 06:35:23.431032 140026025305856 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.945410966873169, loss=1.2544444799423218
I0130 06:35:57.350557 140026042091264 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.580160140991211, loss=1.2141273021697998
I0130 06:35:58.513631 140187804313408 spec.py:321] Evaluating on the training split.
I0130 06:36:04.966960 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 06:36:13.891409 140187804313408 spec.py:349] Evaluating on the test split.
I0130 06:36:16.758794 140187804313408 submission_runner.py:408] Time since start: 39654.96s, 	Step: 112705, 	{'train/accuracy': 0.7928690910339355, 'train/loss': 0.7587718367576599, 'validation/accuracy': 0.699400007724762, 'validation/loss': 1.2161651849746704, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.9524030685424805, 'test/num_examples': 10000, 'score': 38288.8953294754, 'total_duration': 39654.957559108734, 'accumulated_submission_time': 38288.8953294754, 'accumulated_eval_time': 1358.8687388896942, 'accumulated_logging_time': 3.033480167388916}
I0130 06:36:16.796845 140026025305856 logging_writer.py:48] [112705] accumulated_eval_time=1358.868739, accumulated_logging_time=3.033480, accumulated_submission_time=38288.895329, global_step=112705, preemption_count=0, score=38288.895329, test/accuracy=0.569900, test/loss=1.952403, test/num_examples=10000, total_duration=39654.957559, train/accuracy=0.792869, train/loss=0.758772, validation/accuracy=0.699400, validation/loss=1.216165, validation/num_examples=50000
I0130 06:36:49.309402 140026033698560 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.6566271781921387, loss=1.285765290260315
I0130 06:37:23.200602 140026025305856 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.9261205196380615, loss=1.334699034690857
I0130 06:37:57.238507 140026033698560 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.8966901302337646, loss=1.2742140293121338
I0130 06:38:31.114470 140026025305856 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.944568634033203, loss=1.301465630531311
I0130 06:39:04.999976 140026033698560 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.819467306137085, loss=1.189802885055542
I0130 06:39:38.948086 140026025305856 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.939464569091797, loss=1.1895478963851929
I0130 06:40:12.874789 140026033698560 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.8203952312469482, loss=1.2813503742218018
I0130 06:40:46.823922 140026025305856 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.9775044918060303, loss=1.3801628351211548
I0130 06:41:20.743309 140026033698560 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.6949076652526855, loss=1.1227667331695557
I0130 06:41:54.710791 140026025305856 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.678480863571167, loss=1.269232988357544
I0130 06:42:28.594859 140026033698560 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.7560763359069824, loss=1.1853159666061401
I0130 06:43:02.541366 140026025305856 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.8546793460845947, loss=1.2388594150543213
I0130 06:43:36.453534 140026033698560 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.890075206756592, loss=1.1642764806747437
I0130 06:44:10.496038 140026025305856 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.793727397918701, loss=1.2040143013000488
I0130 06:44:44.395023 140026033698560 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.9052846431732178, loss=1.3904165029525757
I0130 06:44:46.932895 140187804313408 spec.py:321] Evaluating on the training split.
I0130 06:44:53.425173 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 06:45:02.406624 140187804313408 spec.py:349] Evaluating on the test split.
I0130 06:45:05.164494 140187804313408 submission_runner.py:408] Time since start: 40183.36s, 	Step: 114209, 	{'train/accuracy': 0.7909358739852905, 'train/loss': 0.765129566192627, 'validation/accuracy': 0.6998800039291382, 'validation/loss': 1.2264665365219116, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.9734641313552856, 'test/num_examples': 10000, 'score': 38798.96592998505, 'total_duration': 40183.36325955391, 'accumulated_submission_time': 38798.96592998505, 'accumulated_eval_time': 1377.1002910137177, 'accumulated_logging_time': 3.080017566680908}
I0130 06:45:05.201288 140026050483968 logging_writer.py:48] [114209] accumulated_eval_time=1377.100291, accumulated_logging_time=3.080018, accumulated_submission_time=38798.965930, global_step=114209, preemption_count=0, score=38798.965930, test/accuracy=0.570900, test/loss=1.973464, test/num_examples=10000, total_duration=40183.363260, train/accuracy=0.790936, train/loss=0.765130, validation/accuracy=0.699880, validation/loss=1.226467, validation/num_examples=50000
I0130 06:45:36.377627 140026058876672 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.9079174995422363, loss=1.1813709735870361
I0130 06:46:10.282079 140026050483968 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.6606552600860596, loss=1.2323319911956787
I0130 06:46:44.223181 140026058876672 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.6440422534942627, loss=1.2193738222122192
I0130 06:47:18.171605 140026050483968 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.879432439804077, loss=1.2692276239395142
I0130 06:47:52.127702 140026058876672 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.9279541969299316, loss=1.278369426727295
I0130 06:48:26.065943 140026050483968 logging_writer.py:48] [114800] global_step=114800, grad_norm=3.056048631668091, loss=1.3189756870269775
I0130 06:48:59.990940 140026058876672 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.783538341522217, loss=1.1802260875701904
I0130 06:49:33.952007 140026050483968 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.8914637565612793, loss=1.2606496810913086
I0130 06:50:07.889510 140026058876672 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.7414989471435547, loss=1.1327933073043823
I0130 06:50:41.921570 140026050483968 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.83758544921875, loss=1.2132353782653809
I0130 06:51:15.864921 140026058876672 logging_writer.py:48] [115300] global_step=115300, grad_norm=3.1212432384490967, loss=1.2526252269744873
I0130 06:51:49.815097 140026050483968 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.803692579269409, loss=1.2044909000396729
I0130 06:52:23.726603 140026058876672 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.760824680328369, loss=1.2234294414520264
I0130 06:52:57.650208 140026050483968 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.940861701965332, loss=1.2025856971740723
I0130 06:53:31.599039 140026058876672 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.9776217937469482, loss=1.2979656457901
I0130 06:53:35.478527 140187804313408 spec.py:321] Evaluating on the training split.
I0130 06:53:41.782574 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 06:53:50.593300 140187804313408 spec.py:349] Evaluating on the test split.
I0130 06:53:53.389747 140187804313408 submission_runner.py:408] Time since start: 40711.59s, 	Step: 115713, 	{'train/accuracy': 0.8435705900192261, 'train/loss': 0.576335072517395, 'validation/accuracy': 0.7015399932861328, 'validation/loss': 1.2031558752059937, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.942007064819336, 'test/num_examples': 10000, 'score': 39309.17578649521, 'total_duration': 40711.58852005005, 'accumulated_submission_time': 39309.17578649521, 'accumulated_eval_time': 1395.0114710330963, 'accumulated_logging_time': 3.126986265182495}
I0130 06:53:53.432586 140025891108608 logging_writer.py:48] [115713] accumulated_eval_time=1395.011471, accumulated_logging_time=3.126986, accumulated_submission_time=39309.175786, global_step=115713, preemption_count=0, score=39309.175786, test/accuracy=0.571100, test/loss=1.942007, test/num_examples=10000, total_duration=40711.588520, train/accuracy=0.843571, train/loss=0.576335, validation/accuracy=0.701540, validation/loss=1.203156, validation/num_examples=50000
I0130 06:54:23.265801 140026016913152 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.6517958641052246, loss=1.1852688789367676
I0130 06:54:57.188283 140025891108608 logging_writer.py:48] [115900] global_step=115900, grad_norm=3.1106150150299072, loss=1.204811692237854
I0130 06:55:31.092176 140026016913152 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.8656556606292725, loss=1.2377662658691406
I0130 06:56:05.038955 140025891108608 logging_writer.py:48] [116100] global_step=116100, grad_norm=3.450904369354248, loss=1.3046528100967407
I0130 06:56:38.972337 140026016913152 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.82551908493042, loss=1.2090424299240112
I0130 06:57:12.871596 140025891108608 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.8998677730560303, loss=1.3175957202911377
I0130 06:57:46.895224 140026016913152 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.6054842472076416, loss=1.134359359741211
I0130 06:58:20.816630 140025891108608 logging_writer.py:48] [116500] global_step=116500, grad_norm=3.021632194519043, loss=1.374661922454834
I0130 06:58:54.755354 140026016913152 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.7708747386932373, loss=1.3223919868469238
I0130 06:59:28.700538 140025891108608 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.6783013343811035, loss=1.2288422584533691
I0130 07:00:02.636657 140026016913152 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.8323731422424316, loss=1.2188003063201904
I0130 07:00:36.562173 140025891108608 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.9031291007995605, loss=1.2360994815826416
I0130 07:01:10.495047 140026016913152 logging_writer.py:48] [117000] global_step=117000, grad_norm=3.082543134689331, loss=1.2486627101898193
I0130 07:01:44.442082 140025891108608 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.7877426147460938, loss=1.2776520252227783
I0130 07:02:18.389078 140026016913152 logging_writer.py:48] [117200] global_step=117200, grad_norm=3.186711072921753, loss=1.263999342918396
I0130 07:02:23.643479 140187804313408 spec.py:321] Evaluating on the training split.
I0130 07:02:29.950587 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 07:02:38.627910 140187804313408 spec.py:349] Evaluating on the test split.
I0130 07:02:41.217086 140187804313408 submission_runner.py:408] Time since start: 41239.42s, 	Step: 117217, 	{'train/accuracy': 0.8229233026504517, 'train/loss': 0.6437040567398071, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.194665551185608, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.9088903665542603, 'test/num_examples': 10000, 'score': 39819.32137942314, 'total_duration': 41239.41584944725, 'accumulated_submission_time': 39819.32137942314, 'accumulated_eval_time': 1412.585030078888, 'accumulated_logging_time': 3.1796674728393555}
I0130 07:02:41.260206 140026050483968 logging_writer.py:48] [117217] accumulated_eval_time=1412.585030, accumulated_logging_time=3.179667, accumulated_submission_time=39819.321379, global_step=117217, preemption_count=0, score=39819.321379, test/accuracy=0.575500, test/loss=1.908890, test/num_examples=10000, total_duration=41239.415849, train/accuracy=0.822923, train/loss=0.643704, validation/accuracy=0.707380, validation/loss=1.194666, validation/num_examples=50000
I0130 07:03:09.691495 140026058876672 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.912853479385376, loss=1.2695295810699463
I0130 07:03:43.625743 140026050483968 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.699303388595581, loss=1.2158613204956055
I0130 07:04:17.638783 140026058876672 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.8100574016571045, loss=1.2153156995773315
I0130 07:04:51.583305 140026050483968 logging_writer.py:48] [117600] global_step=117600, grad_norm=3.1227173805236816, loss=1.2572394609451294
I0130 07:05:25.550309 140026058876672 logging_writer.py:48] [117700] global_step=117700, grad_norm=3.2771575450897217, loss=1.1639044284820557
I0130 07:05:59.459632 140026050483968 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.785550117492676, loss=1.242056965827942
I0130 07:06:33.396091 140026058876672 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.820758819580078, loss=1.2073566913604736
I0130 07:07:07.315322 140026050483968 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.0052335262298584, loss=1.1771214008331299
I0130 07:07:41.312040 140026058876672 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.932676315307617, loss=1.1778626441955566
I0130 07:08:15.246603 140026050483968 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.956563711166382, loss=1.2397059202194214
I0130 07:08:49.172210 140026058876672 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.611863374710083, loss=1.1015485525131226
I0130 07:09:23.096689 140026050483968 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.9054667949676514, loss=1.199502944946289
I0130 07:09:57.043164 140026058876672 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.888410806655884, loss=1.3271691799163818
I0130 07:10:31.086073 140026050483968 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.016028881072998, loss=1.1728554964065552
I0130 07:11:05.013383 140026058876672 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.7309725284576416, loss=1.1235898733139038
I0130 07:11:11.296393 140187804313408 spec.py:321] Evaluating on the training split.
I0130 07:11:17.584222 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 07:11:26.580558 140187804313408 spec.py:349] Evaluating on the test split.
I0130 07:11:29.337594 140187804313408 submission_runner.py:408] Time since start: 41767.54s, 	Step: 118720, 	{'train/accuracy': 0.8165457248687744, 'train/loss': 0.6733990907669067, 'validation/accuracy': 0.7057200074195862, 'validation/loss': 1.2064604759216309, 'validation/num_examples': 50000, 'test/accuracy': 0.5786000490188599, 'test/loss': 1.9398770332336426, 'test/num_examples': 10000, 'score': 40329.29307627678, 'total_duration': 41767.536363363266, 'accumulated_submission_time': 40329.29307627678, 'accumulated_eval_time': 1430.6261870861053, 'accumulated_logging_time': 3.2323155403137207}
I0130 07:11:29.376333 140026025305856 logging_writer.py:48] [118720] accumulated_eval_time=1430.626187, accumulated_logging_time=3.232316, accumulated_submission_time=40329.293076, global_step=118720, preemption_count=0, score=40329.293076, test/accuracy=0.578600, test/loss=1.939877, test/num_examples=10000, total_duration=41767.536363, train/accuracy=0.816546, train/loss=0.673399, validation/accuracy=0.705720, validation/loss=1.206460, validation/num_examples=50000
I0130 07:11:56.775645 140026033698560 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.2445216178894043, loss=1.2866640090942383
I0130 07:12:30.686491 140026025305856 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.9405643939971924, loss=1.1872813701629639
I0130 07:13:04.597638 140026033698560 logging_writer.py:48] [119000] global_step=119000, grad_norm=3.1152780055999756, loss=1.2490980625152588
I0130 07:13:38.562855 140026025305856 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.8837828636169434, loss=1.2382813692092896
I0130 07:14:12.510432 140026033698560 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.9353413581848145, loss=1.2090721130371094
I0130 07:14:46.426742 140026025305856 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.974724769592285, loss=1.1761789321899414
I0130 07:15:20.360144 140026033698560 logging_writer.py:48] [119400] global_step=119400, grad_norm=3.2155368328094482, loss=1.2422397136688232
I0130 07:15:54.325601 140026025305856 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.79009747505188, loss=1.1340769529342651
I0130 07:16:28.258205 140026033698560 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.874575138092041, loss=1.2591636180877686
I0130 07:17:02.405609 140026025305856 logging_writer.py:48] [119700] global_step=119700, grad_norm=3.132215976715088, loss=1.240525722503662
I0130 07:17:36.346852 140026033698560 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.9533801078796387, loss=1.1624042987823486
I0130 07:18:10.307800 140026025305856 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.9294729232788086, loss=1.2084380388259888
I0130 07:18:44.267820 140026033698560 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.648841381072998, loss=1.1180803775787354
I0130 07:19:18.163909 140026025305856 logging_writer.py:48] [120100] global_step=120100, grad_norm=3.090336322784424, loss=1.173750638961792
I0130 07:19:52.117414 140026033698560 logging_writer.py:48] [120200] global_step=120200, grad_norm=3.064872980117798, loss=1.220517635345459
I0130 07:19:59.367170 140187804313408 spec.py:321] Evaluating on the training split.
I0130 07:20:05.835298 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 07:20:14.567848 140187804313408 spec.py:349] Evaluating on the test split.
I0130 07:20:17.981631 140187804313408 submission_runner.py:408] Time since start: 42296.18s, 	Step: 120223, 	{'train/accuracy': 0.8181002736091614, 'train/loss': 0.6516917943954468, 'validation/accuracy': 0.7089999914169312, 'validation/loss': 1.1775261163711548, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.913909673690796, 'test/num_examples': 10000, 'score': 40839.21898150444, 'total_duration': 42296.18041563034, 'accumulated_submission_time': 40839.21898150444, 'accumulated_eval_time': 1449.2406451702118, 'accumulated_logging_time': 3.280336380004883}
I0130 07:20:18.016546 140026016913152 logging_writer.py:48] [120223] accumulated_eval_time=1449.240645, accumulated_logging_time=3.280336, accumulated_submission_time=40839.218982, global_step=120223, preemption_count=0, score=40839.218982, test/accuracy=0.580000, test/loss=1.913910, test/num_examples=10000, total_duration=42296.180416, train/accuracy=0.818100, train/loss=0.651692, validation/accuracy=0.709000, validation/loss=1.177526, validation/num_examples=50000
I0130 07:20:44.417962 140026050483968 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.9941980838775635, loss=1.2252483367919922
I0130 07:21:18.321702 140026016913152 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.896702766418457, loss=1.2984333038330078
I0130 07:21:52.272427 140026050483968 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.006328582763672, loss=1.1582766771316528
I0130 07:22:26.229080 140026016913152 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.9292409420013428, loss=1.0835621356964111
I0130 07:23:00.132975 140026050483968 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.836061716079712, loss=1.279430866241455
I0130 07:23:34.078846 140026016913152 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.8432276248931885, loss=1.1358463764190674
I0130 07:24:08.157253 140026050483968 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.990575075149536, loss=1.185784101486206
I0130 07:24:42.087342 140026016913152 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.8198084831237793, loss=1.1527408361434937
I0130 07:25:16.038476 140026050483968 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.8370749950408936, loss=1.132851004600525
I0130 07:25:49.985478 140026016913152 logging_writer.py:48] [121200] global_step=121200, grad_norm=3.2151217460632324, loss=1.227101445198059
I0130 07:26:23.934513 140026050483968 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.072092294692993, loss=1.211846947669983
I0130 07:26:57.863537 140026016913152 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.0954272747039795, loss=1.152694821357727
I0130 07:27:31.829152 140026050483968 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.907118558883667, loss=1.0654513835906982
I0130 07:28:05.798625 140026016913152 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.0299031734466553, loss=1.222089409828186
I0130 07:28:39.752783 140026050483968 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.9144887924194336, loss=1.1649622917175293
I0130 07:28:48.033255 140187804313408 spec.py:321] Evaluating on the training split.
I0130 07:28:54.328350 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 07:29:03.431211 140187804313408 spec.py:349] Evaluating on the test split.
I0130 07:29:06.104315 140187804313408 submission_runner.py:408] Time since start: 42824.30s, 	Step: 121726, 	{'train/accuracy': 0.8082947731018066, 'train/loss': 0.6896772384643555, 'validation/accuracy': 0.7066999673843384, 'validation/loss': 1.1927813291549683, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 1.9429177045822144, 'test/num_examples': 10000, 'score': 41349.17365002632, 'total_duration': 42824.30308508873, 'accumulated_submission_time': 41349.17365002632, 'accumulated_eval_time': 1467.3116641044617, 'accumulated_logging_time': 3.323556900024414}
I0130 07:29:06.148071 140025891108608 logging_writer.py:48] [121726] accumulated_eval_time=1467.311664, accumulated_logging_time=3.323557, accumulated_submission_time=41349.173650, global_step=121726, preemption_count=0, score=41349.173650, test/accuracy=0.575900, test/loss=1.942918, test/num_examples=10000, total_duration=42824.303085, train/accuracy=0.808295, train/loss=0.689677, validation/accuracy=0.706700, validation/loss=1.192781, validation/num_examples=50000
I0130 07:29:31.552892 140026016913152 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.7907347679138184, loss=1.1066607236862183
I0130 07:30:05.456938 140025891108608 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.8878235816955566, loss=1.1743766069412231
I0130 07:30:39.440955 140026016913152 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.319103956222534, loss=1.2157747745513916
I0130 07:31:13.311369 140025891108608 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.060248851776123, loss=1.1191819906234741
I0130 07:31:47.246162 140026016913152 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.136706829071045, loss=1.2420246601104736
I0130 07:32:21.151378 140025891108608 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.1802167892456055, loss=1.1328506469726562
I0130 07:32:55.070990 140026016913152 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.9551990032196045, loss=1.1881606578826904
I0130 07:33:28.992768 140025891108608 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.4016354084014893, loss=1.1225345134735107
I0130 07:34:02.921617 140026016913152 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.9193100929260254, loss=1.1859666109085083
I0130 07:34:36.858872 140025891108608 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.89040470123291, loss=1.0999675989151
I0130 07:35:10.780478 140026016913152 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.028590679168701, loss=1.1482467651367188
I0130 07:35:44.753101 140025891108608 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.219238042831421, loss=1.1912989616394043
I0130 07:36:18.628458 140026016913152 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.7903599739074707, loss=1.102400779724121
I0130 07:36:52.592027 140025891108608 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.915111780166626, loss=1.189548134803772
I0130 07:37:26.530566 140026016913152 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.008362293243408, loss=1.1371495723724365
I0130 07:37:36.189676 140187804313408 spec.py:321] Evaluating on the training split.
I0130 07:37:42.523784 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 07:37:51.425836 140187804313408 spec.py:349] Evaluating on the test split.
I0130 07:37:54.124452 140187804313408 submission_runner.py:408] Time since start: 43352.32s, 	Step: 123230, 	{'train/accuracy': 0.8172034025192261, 'train/loss': 0.6480165719985962, 'validation/accuracy': 0.7120800018310547, 'validation/loss': 1.1730077266693115, 'validation/num_examples': 50000, 'test/accuracy': 0.5870000123977661, 'test/loss': 1.930961012840271, 'test/num_examples': 10000, 'score': 41859.14821410179, 'total_duration': 43352.32322573662, 'accumulated_submission_time': 41859.14821410179, 'accumulated_eval_time': 1485.2464039325714, 'accumulated_logging_time': 3.3781511783599854}
I0130 07:37:54.163664 140026042091264 logging_writer.py:48] [123230] accumulated_eval_time=1485.246404, accumulated_logging_time=3.378151, accumulated_submission_time=41859.148214, global_step=123230, preemption_count=0, score=41859.148214, test/accuracy=0.587000, test/loss=1.930961, test/num_examples=10000, total_duration=43352.323226, train/accuracy=0.817203, train/loss=0.648017, validation/accuracy=0.712080, validation/loss=1.173008, validation/num_examples=50000
I0130 07:38:18.212709 140026050483968 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.049394369125366, loss=1.2639844417572021
I0130 07:38:52.116413 140026042091264 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.2485225200653076, loss=1.1728646755218506
I0130 07:39:25.986556 140026050483968 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.86213755607605, loss=1.1903711557388306
I0130 07:39:59.935836 140026042091264 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.8815226554870605, loss=1.227249264717102
I0130 07:40:33.840808 140026050483968 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.2094955444335938, loss=1.311946988105774
I0130 07:41:07.766170 140026042091264 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.418158769607544, loss=1.1100807189941406
I0130 07:41:41.744966 140026050483968 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.1170318126678467, loss=1.2292089462280273
I0130 07:42:15.694632 140026042091264 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.912813663482666, loss=1.125187635421753
I0130 07:42:49.595609 140026050483968 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.132915735244751, loss=1.1580299139022827
I0130 07:43:23.534812 140026042091264 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.1555495262145996, loss=1.1702994108200073
I0130 07:43:57.572832 140026050483968 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.9444546699523926, loss=1.1636097431182861
I0130 07:44:31.507914 140026042091264 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.125959873199463, loss=1.1383380889892578
I0130 07:45:05.457562 140026050483968 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.9075164794921875, loss=1.0438177585601807
I0130 07:45:39.437208 140026042091264 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.988272190093994, loss=1.1175731420516968
I0130 07:46:13.326011 140026050483968 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.049473524093628, loss=1.1279352903366089
I0130 07:46:24.338654 140187804313408 spec.py:321] Evaluating on the training split.
I0130 07:46:30.618901 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 07:46:39.320688 140187804313408 spec.py:349] Evaluating on the test split.
I0130 07:46:41.978706 140187804313408 submission_runner.py:408] Time since start: 43880.18s, 	Step: 124734, 	{'train/accuracy': 0.8444275856018066, 'train/loss': 0.5557413101196289, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.1623338460922241, 'validation/num_examples': 50000, 'test/accuracy': 0.5822000503540039, 'test/loss': 1.8956857919692993, 'test/num_examples': 10000, 'score': 42369.25692343712, 'total_duration': 43880.17747545242, 'accumulated_submission_time': 42369.25692343712, 'accumulated_eval_time': 1502.8864228725433, 'accumulated_logging_time': 3.428584575653076}
I0130 07:46:42.019266 140026025305856 logging_writer.py:48] [124734] accumulated_eval_time=1502.886423, accumulated_logging_time=3.428585, accumulated_submission_time=42369.256923, global_step=124734, preemption_count=0, score=42369.256923, test/accuracy=0.582200, test/loss=1.895686, test/num_examples=10000, total_duration=43880.177475, train/accuracy=0.844428, train/loss=0.555741, validation/accuracy=0.713520, validation/loss=1.162334, validation/num_examples=50000
I0130 07:47:04.732887 140026033698560 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.1470448970794678, loss=1.0626059770584106
I0130 07:47:38.612844 140026025305856 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.3150899410247803, loss=1.2202693223953247
I0130 07:48:12.488277 140026033698560 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.0137147903442383, loss=1.1420425176620483
I0130 07:48:46.414786 140026025305856 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.989776849746704, loss=1.1206787824630737
I0130 07:49:20.340961 140026033698560 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.1306211948394775, loss=1.1258331537246704
I0130 07:49:54.258955 140026025305856 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.1680827140808105, loss=1.1672604084014893
I0130 07:50:28.418929 140026033698560 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.2199909687042236, loss=1.1769920587539673
I0130 07:51:02.361691 140026025305856 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.232974052429199, loss=1.1676591634750366
I0130 07:51:36.324327 140026033698560 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.453946828842163, loss=1.164279818534851
I0130 07:52:10.244384 140026025305856 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.068493604660034, loss=1.1030272245407104
I0130 07:52:44.162628 140026033698560 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.423630952835083, loss=1.1481432914733887
I0130 07:53:18.051342 140026025305856 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.21813702583313, loss=1.11464524269104
I0130 07:53:51.965527 140026033698560 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.3034462928771973, loss=1.16712486743927
I0130 07:54:25.906350 140026025305856 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.2062108516693115, loss=1.0899200439453125
I0130 07:54:59.796488 140026033698560 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.2405920028686523, loss=1.0955092906951904
I0130 07:55:12.155379 140187804313408 spec.py:321] Evaluating on the training split.
I0130 07:55:18.532969 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 07:55:27.259391 140187804313408 spec.py:349] Evaluating on the test split.
I0130 07:55:29.972533 140187804313408 submission_runner.py:408] Time since start: 44408.17s, 	Step: 126238, 	{'train/accuracy': 0.8463608026504517, 'train/loss': 0.5420656800270081, 'validation/accuracy': 0.7162599563598633, 'validation/loss': 1.1581156253814697, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.887725830078125, 'test/num_examples': 10000, 'score': 42879.326265096664, 'total_duration': 44408.17130422592, 'accumulated_submission_time': 42879.326265096664, 'accumulated_eval_time': 1520.7035658359528, 'accumulated_logging_time': 3.4789483547210693}
I0130 07:55:30.016681 140026016913152 logging_writer.py:48] [126238] accumulated_eval_time=1520.703566, accumulated_logging_time=3.478948, accumulated_submission_time=42879.326265, global_step=126238, preemption_count=0, score=42879.326265, test/accuracy=0.590200, test/loss=1.887726, test/num_examples=10000, total_duration=44408.171304, train/accuracy=0.846361, train/loss=0.542066, validation/accuracy=0.716260, validation/loss=1.158116, validation/num_examples=50000
I0130 07:55:51.324479 140026025305856 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.033113479614258, loss=1.071965217590332
I0130 07:56:25.242383 140026016913152 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.095684766769409, loss=1.0925055742263794
I0130 07:56:59.279769 140026025305856 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.998303174972534, loss=1.0358891487121582
I0130 07:57:33.232254 140026016913152 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.286421537399292, loss=1.139162302017212
I0130 07:58:07.204376 140026025305856 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.0036966800689697, loss=1.1873265504837036
I0130 07:58:41.183130 140026016913152 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.559957265853882, loss=1.2058380842208862
I0130 07:59:15.072945 140026025305856 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.0064220428466797, loss=1.079866647720337
I0130 07:59:49.023706 140026016913152 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.3364882469177246, loss=1.1029808521270752
I0130 08:00:22.960855 140026025305856 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.043060541152954, loss=1.11056649684906
I0130 08:00:56.896220 140026016913152 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.181269645690918, loss=1.0213525295257568
I0130 08:01:30.849609 140026025305856 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.048208236694336, loss=1.1571943759918213
I0130 08:02:04.804109 140026016913152 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.0911035537719727, loss=1.0968459844589233
I0130 08:02:38.713415 140026025305856 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.2865331172943115, loss=1.0897157192230225
I0130 08:03:12.759946 140026016913152 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.236499547958374, loss=1.0719497203826904
I0130 08:03:46.688839 140026025305856 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.1146297454833984, loss=1.019310474395752
I0130 08:04:00.080299 140187804313408 spec.py:321] Evaluating on the training split.
I0130 08:04:06.514082 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 08:04:15.045387 140187804313408 spec.py:349] Evaluating on the test split.
I0130 08:04:17.727562 140187804313408 submission_runner.py:408] Time since start: 44935.93s, 	Step: 127741, 	{'train/accuracy': 0.8425143361091614, 'train/loss': 0.5580957531929016, 'validation/accuracy': 0.7177199721336365, 'validation/loss': 1.1547913551330566, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.8950108289718628, 'test/num_examples': 10000, 'score': 43389.325770139694, 'total_duration': 44935.92632341385, 'accumulated_submission_time': 43389.325770139694, 'accumulated_eval_time': 1538.350778579712, 'accumulated_logging_time': 3.5324387550354004}
I0130 08:04:17.770904 140026050483968 logging_writer.py:48] [127741] accumulated_eval_time=1538.350779, accumulated_logging_time=3.532439, accumulated_submission_time=43389.325770, global_step=127741, preemption_count=0, score=43389.325770, test/accuracy=0.589800, test/loss=1.895011, test/num_examples=10000, total_duration=44935.926323, train/accuracy=0.842514, train/loss=0.558096, validation/accuracy=0.717720, validation/loss=1.154791, validation/num_examples=50000
I0130 08:04:38.109527 140026058876672 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.354505777359009, loss=1.1926236152648926
I0130 08:05:11.966078 140026050483968 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.9694950580596924, loss=1.0214046239852905
I0130 08:05:45.867950 140026058876672 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.8992555141448975, loss=1.1647381782531738
I0130 08:06:19.804049 140026050483968 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.0463948249816895, loss=1.1045650243759155
I0130 08:06:53.730220 140026058876672 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.37934947013855, loss=1.1298080682754517
I0130 08:07:27.612406 140026050483968 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.4997096061706543, loss=1.1306549310684204
I0130 08:08:01.550702 140026058876672 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.0923454761505127, loss=1.0185679197311401
I0130 08:08:35.445234 140026050483968 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.2482385635375977, loss=1.1600522994995117
I0130 08:09:09.373076 140026058876672 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.0296730995178223, loss=1.0610706806182861
I0130 08:09:43.338828 140026050483968 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.533400297164917, loss=1.0687114000320435
I0130 08:10:17.238239 140026058876672 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.366527557373047, loss=1.2235479354858398
I0130 08:10:51.171576 140026050483968 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.3389360904693604, loss=1.1451665163040161
I0130 08:11:25.110102 140026058876672 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.2488062381744385, loss=1.1268796920776367
I0130 08:11:59.025580 140026050483968 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.630683183670044, loss=1.165438175201416
I0130 08:12:32.936275 140026058876672 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.3061981201171875, loss=1.054749608039856
I0130 08:12:48.021526 140187804313408 spec.py:321] Evaluating on the training split.
I0130 08:12:54.384422 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 08:13:03.389994 140187804313408 spec.py:349] Evaluating on the test split.
I0130 08:13:06.062050 140187804313408 submission_runner.py:408] Time since start: 45464.26s, 	Step: 129246, 	{'train/accuracy': 0.8390465378761292, 'train/loss': 0.5831946134567261, 'validation/accuracy': 0.7163999676704407, 'validation/loss': 1.167493462562561, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.9175639152526855, 'test/num_examples': 10000, 'score': 43899.508662223816, 'total_duration': 45464.26081061363, 'accumulated_submission_time': 43899.508662223816, 'accumulated_eval_time': 1556.3912541866302, 'accumulated_logging_time': 3.587545394897461}
I0130 08:13:06.108269 140025882715904 logging_writer.py:48] [129246] accumulated_eval_time=1556.391254, accumulated_logging_time=3.587545, accumulated_submission_time=43899.508662, global_step=129246, preemption_count=0, score=43899.508662, test/accuracy=0.591000, test/loss=1.917564, test/num_examples=10000, total_duration=45464.260811, train/accuracy=0.839047, train/loss=0.583195, validation/accuracy=0.716400, validation/loss=1.167493, validation/num_examples=50000
I0130 08:13:24.729879 140025891108608 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.728607177734375, loss=1.1821582317352295
I0130 08:13:58.575670 140025882715904 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.479578971862793, loss=1.0981358289718628
I0130 08:14:32.498998 140025891108608 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.278507709503174, loss=1.1789368391036987
I0130 08:15:06.395196 140025882715904 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.1367223262786865, loss=1.1046655178070068
I0130 08:15:40.324118 140025891108608 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.1435976028442383, loss=1.0325782299041748
I0130 08:16:14.243928 140025882715904 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.439621686935425, loss=1.1364028453826904
I0130 08:16:48.267726 140025891108608 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.645526647567749, loss=1.0243202447891235
I0130 08:17:22.208638 140025882715904 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.177471876144409, loss=1.125138759613037
I0130 08:17:56.125833 140025891108608 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.619816541671753, loss=1.0768731832504272
I0130 08:18:30.048527 140025882715904 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.417146921157837, loss=1.023112416267395
I0130 08:19:03.990704 140025891108608 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.4230403900146484, loss=1.1222339868545532
I0130 08:19:37.911685 140025882715904 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.555222749710083, loss=1.084324598312378
I0130 08:20:11.847972 140025891108608 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.476043939590454, loss=1.173356294631958
I0130 08:20:45.804052 140025882715904 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.408182382583618, loss=1.1928735971450806
I0130 08:21:19.718147 140025891108608 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.263361930847168, loss=1.0188335180282593
I0130 08:21:36.152575 140187804313408 spec.py:321] Evaluating on the training split.
I0130 08:21:42.516670 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 08:21:51.498452 140187804313408 spec.py:349] Evaluating on the test split.
I0130 08:21:54.172931 140187804313408 submission_runner.py:408] Time since start: 45992.37s, 	Step: 130750, 	{'train/accuracy': 0.8318319320678711, 'train/loss': 0.5993456840515137, 'validation/accuracy': 0.715939998626709, 'validation/loss': 1.1581324338912964, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8844317197799683, 'test/num_examples': 10000, 'score': 44409.489119291306, 'total_duration': 45992.3717007637, 'accumulated_submission_time': 44409.489119291306, 'accumulated_eval_time': 1574.4115755558014, 'accumulated_logging_time': 3.6436238288879395}
I0130 08:21:54.213159 140026042091264 logging_writer.py:48] [130750] accumulated_eval_time=1574.411576, accumulated_logging_time=3.643624, accumulated_submission_time=44409.489119, global_step=130750, preemption_count=0, score=44409.489119, test/accuracy=0.590900, test/loss=1.884432, test/num_examples=10000, total_duration=45992.371701, train/accuracy=0.831832, train/loss=0.599346, validation/accuracy=0.715940, validation/loss=1.158132, validation/num_examples=50000
I0130 08:22:11.492370 140026050483968 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.166348934173584, loss=1.0530925989151
I0130 08:22:45.347884 140026042091264 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.204139232635498, loss=1.101698637008667
I0130 08:23:19.353058 140026050483968 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.2102949619293213, loss=1.0331717729568481
I0130 08:23:53.262457 140026042091264 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.429246664047241, loss=1.1349601745605469
I0130 08:24:27.192256 140026050483968 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.0720510482788086, loss=1.041394829750061
I0130 08:25:01.085550 140026042091264 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.104578971862793, loss=1.0528345108032227
I0130 08:25:35.018785 140026050483968 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.2621216773986816, loss=1.114148736000061
I0130 08:26:08.954160 140026042091264 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.383867025375366, loss=1.0025663375854492
I0130 08:26:42.856812 140026050483968 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.0890746116638184, loss=1.0926072597503662
I0130 08:27:16.795524 140026042091264 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.276564836502075, loss=1.0228208303451538
I0130 08:27:50.729328 140026050483968 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.074336528778076, loss=0.9634712338447571
I0130 08:28:24.661827 140026042091264 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.3398241996765137, loss=1.0668179988861084
I0130 08:28:58.585803 140026050483968 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.3448965549468994, loss=1.1009206771850586
I0130 08:29:32.497525 140026042091264 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.4117801189422607, loss=1.1441808938980103
I0130 08:30:06.470448 140026050483968 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.1817431449890137, loss=1.0732665061950684
I0130 08:30:24.254405 140187804313408 spec.py:321] Evaluating on the training split.
I0130 08:30:30.538347 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 08:30:39.145490 140187804313408 spec.py:349] Evaluating on the test split.
I0130 08:30:41.874588 140187804313408 submission_runner.py:408] Time since start: 46520.07s, 	Step: 132254, 	{'train/accuracy': 0.8413584232330322, 'train/loss': 0.5591051578521729, 'validation/accuracy': 0.7198799848556519, 'validation/loss': 1.1507633924484253, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.8953807353973389, 'test/num_examples': 10000, 'score': 44919.46421599388, 'total_duration': 46520.07335758209, 'accumulated_submission_time': 44919.46421599388, 'accumulated_eval_time': 1592.0317244529724, 'accumulated_logging_time': 3.695392608642578}
I0130 08:30:41.918370 140026016913152 logging_writer.py:48] [132254] accumulated_eval_time=1592.031724, accumulated_logging_time=3.695393, accumulated_submission_time=44919.464216, global_step=132254, preemption_count=0, score=44919.464216, test/accuracy=0.595200, test/loss=1.895381, test/num_examples=10000, total_duration=46520.073358, train/accuracy=0.841358, train/loss=0.559105, validation/accuracy=0.719880, validation/loss=1.150763, validation/num_examples=50000
I0130 08:30:57.847493 140026025305856 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.292207717895508, loss=1.1414083242416382
I0130 08:31:31.737709 140026016913152 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.375648021697998, loss=1.0426920652389526
I0130 08:32:05.679878 140026025305856 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.457965135574341, loss=1.120840072631836
I0130 08:32:39.615235 140026016913152 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.2860331535339355, loss=1.017351508140564
I0130 08:33:13.544810 140026025305856 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.295227289199829, loss=1.036569595336914
I0130 08:33:47.474802 140026016913152 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.4977102279663086, loss=1.0852731466293335
I0130 08:34:21.391322 140026025305856 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.3235275745391846, loss=1.0950638055801392
I0130 08:34:55.295492 140026016913152 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.3714020252227783, loss=1.1280262470245361
I0130 08:35:29.244117 140026025305856 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.5878026485443115, loss=1.0466949939727783
I0130 08:36:03.162242 140026016913152 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.2784781455993652, loss=1.1382484436035156
I0130 08:36:37.193181 140026025305856 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.3858816623687744, loss=0.9975409507751465
I0130 08:37:11.128493 140026016913152 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.234233856201172, loss=1.0616248846054077
I0130 08:37:45.069191 140026025305856 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.4324381351470947, loss=1.043391227722168
I0130 08:38:19.003931 140026016913152 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.349881172180176, loss=1.0079702138900757
I0130 08:38:52.921714 140026025305856 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.449681282043457, loss=1.0113551616668701
I0130 08:39:12.061641 140187804313408 spec.py:321] Evaluating on the training split.
I0130 08:39:18.410237 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 08:39:27.288327 140187804313408 spec.py:349] Evaluating on the test split.
I0130 08:39:29.984481 140187804313408 submission_runner.py:408] Time since start: 47048.18s, 	Step: 133758, 	{'train/accuracy': 0.8465999364852905, 'train/loss': 0.5491646528244019, 'validation/accuracy': 0.7141599655151367, 'validation/loss': 1.1705526113510132, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.9156215190887451, 'test/num_examples': 10000, 'score': 45429.54328536987, 'total_duration': 47048.18325304985, 'accumulated_submission_time': 45429.54328536987, 'accumulated_eval_time': 1609.954525232315, 'accumulated_logging_time': 3.7486047744750977}
I0130 08:39:30.024806 140026016913152 logging_writer.py:48] [133758] accumulated_eval_time=1609.954525, accumulated_logging_time=3.748605, accumulated_submission_time=45429.543285, global_step=133758, preemption_count=0, score=45429.543285, test/accuracy=0.590300, test/loss=1.915622, test/num_examples=10000, total_duration=47048.183253, train/accuracy=0.846600, train/loss=0.549165, validation/accuracy=0.714160, validation/loss=1.170553, validation/num_examples=50000
I0130 08:39:44.561699 140026042091264 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.581103563308716, loss=1.1489301919937134
I0130 08:40:18.446821 140026016913152 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.6747119426727295, loss=1.0515751838684082
I0130 08:40:52.365260 140026042091264 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.5165724754333496, loss=1.125286340713501
I0130 08:41:26.302992 140026016913152 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.554912805557251, loss=1.098815679550171
I0130 08:42:00.215394 140026042091264 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.66166615486145, loss=1.100499153137207
I0130 08:42:34.142031 140026016913152 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.2238337993621826, loss=1.0614948272705078
I0130 08:43:08.129103 140026042091264 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.2932560443878174, loss=0.9810147285461426
I0130 08:43:42.034329 140026016913152 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.401154041290283, loss=1.0160753726959229
I0130 08:44:15.940573 140026042091264 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.3857908248901367, loss=0.9904347062110901
I0130 08:44:49.874054 140026016913152 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.619110345840454, loss=1.1001670360565186
I0130 08:45:23.788159 140026042091264 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.4520599842071533, loss=1.05045747756958
I0130 08:45:57.724288 140026016913152 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.502915143966675, loss=1.1087896823883057
I0130 08:46:31.693353 140026042091264 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.5833888053894043, loss=0.981003999710083
I0130 08:47:05.624643 140026016913152 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.737560987472534, loss=1.0971839427947998
I0130 08:47:39.510794 140026042091264 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.6262502670288086, loss=1.092573642730713
I0130 08:47:59.995511 140187804313408 spec.py:321] Evaluating on the training split.
I0130 08:48:07.299205 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 08:48:15.845427 140187804313408 spec.py:349] Evaluating on the test split.
I0130 08:48:18.556094 140187804313408 submission_runner.py:408] Time since start: 47576.75s, 	Step: 135262, 	{'train/accuracy': 0.8697385191917419, 'train/loss': 0.4575299024581909, 'validation/accuracy': 0.7210599780082703, 'validation/loss': 1.1391361951828003, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.874566674232483, 'test/num_examples': 10000, 'score': 45939.4492623806, 'total_duration': 47576.754868507385, 'accumulated_submission_time': 45939.4492623806, 'accumulated_eval_time': 1628.5150740146637, 'accumulated_logging_time': 3.798022508621216}
I0130 08:48:18.596889 140026033698560 logging_writer.py:48] [135262] accumulated_eval_time=1628.515074, accumulated_logging_time=3.798023, accumulated_submission_time=45939.449262, global_step=135262, preemption_count=0, score=45939.449262, test/accuracy=0.594300, test/loss=1.874567, test/num_examples=10000, total_duration=47576.754869, train/accuracy=0.869739, train/loss=0.457530, validation/accuracy=0.721060, validation/loss=1.139136, validation/num_examples=50000
I0130 08:48:31.840006 140026058876672 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.6646060943603516, loss=1.0679150819778442
I0130 08:49:05.689197 140026033698560 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.5313892364501953, loss=1.0607779026031494
I0130 08:49:39.732368 140026058876672 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.31992769241333, loss=1.0058438777923584
I0130 08:50:13.683621 140026033698560 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.8355770111083984, loss=1.1328222751617432
I0130 08:50:47.612615 140026058876672 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.3831636905670166, loss=1.0230008363723755
I0130 08:51:21.549416 140026033698560 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.6647934913635254, loss=1.0345313549041748
I0130 08:51:55.475792 140026058876672 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.153609037399292, loss=0.9847835302352905
I0130 08:52:29.428402 140026033698560 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.4040231704711914, loss=1.0323264598846436
I0130 08:53:03.363903 140026058876672 logging_writer.py:48] [136100] global_step=136100, grad_norm=4.0201735496521, loss=1.0349009037017822
I0130 08:53:37.311473 140026033698560 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.3912513256073, loss=0.9547950029373169
I0130 08:54:11.260067 140026058876672 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.6812939643859863, loss=1.0245720148086548
I0130 08:54:45.176541 140026033698560 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.8460216522216797, loss=1.026318073272705
I0130 08:55:19.145013 140026058876672 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.8256189823150635, loss=1.096229910850525
I0130 08:55:53.108356 140026033698560 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.88690185546875, loss=1.0367333889007568
I0130 08:56:27.115311 140026058876672 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.359419345855713, loss=0.9943723678588867
I0130 08:56:48.651674 140187804313408 spec.py:321] Evaluating on the training split.
I0130 08:56:54.943572 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 08:57:03.873342 140187804313408 spec.py:349] Evaluating on the test split.
I0130 08:57:06.581846 140187804313408 submission_runner.py:408] Time since start: 48104.78s, 	Step: 136765, 	{'train/accuracy': 0.8618462681770325, 'train/loss': 0.48523062467575073, 'validation/accuracy': 0.7225599884986877, 'validation/loss': 1.1408337354660034, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.8832614421844482, 'test/num_examples': 10000, 'score': 46449.43899035454, 'total_duration': 48104.78061199188, 'accumulated_submission_time': 46449.43899035454, 'accumulated_eval_time': 1646.445203781128, 'accumulated_logging_time': 3.8481035232543945}
I0130 08:57:06.631435 140026016913152 logging_writer.py:48] [136765] accumulated_eval_time=1646.445204, accumulated_logging_time=3.848104, accumulated_submission_time=46449.438990, global_step=136765, preemption_count=0, score=46449.438990, test/accuracy=0.600300, test/loss=1.883261, test/num_examples=10000, total_duration=48104.780612, train/accuracy=0.861846, train/loss=0.485231, validation/accuracy=0.722560, validation/loss=1.140834, validation/num_examples=50000
I0130 08:57:18.852472 140026025305856 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.373502731323242, loss=0.9722197651863098
I0130 08:57:52.676640 140026016913152 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.0683770179748535, loss=0.8753202557563782
I0130 08:58:26.583186 140026025305856 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.4457054138183594, loss=1.1285637617111206
I0130 08:59:00.506911 140026016913152 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.5106823444366455, loss=1.0322446823120117
I0130 08:59:34.448281 140026025305856 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.802511215209961, loss=1.0075091123580933
I0130 09:00:08.383170 140026016913152 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.303560256958008, loss=0.9241158962249756
I0130 09:00:42.311046 140026025305856 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.411126136779785, loss=1.0591611862182617
I0130 09:01:16.260511 140026016913152 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.4203226566314697, loss=0.8935225605964661
I0130 09:01:50.204998 140026025305856 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2688002586364746, loss=1.0246021747589111
I0130 09:02:24.156546 140026016913152 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.7187681198120117, loss=1.0211901664733887
I0130 09:02:58.287978 140026025305856 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.622044801712036, loss=1.0394244194030762
I0130 09:03:32.177507 140026016913152 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.390375852584839, loss=1.0256634950637817
I0130 09:04:06.100317 140026025305856 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.3208789825439453, loss=0.940830409526825
I0130 09:04:40.060153 140026016913152 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.695232391357422, loss=1.0517841577529907
I0130 09:05:14.002575 140026025305856 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.526139497756958, loss=0.9978013634681702
I0130 09:05:36.894262 140187804313408 spec.py:321] Evaluating on the training split.
I0130 09:05:43.141044 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 09:05:51.982100 140187804313408 spec.py:349] Evaluating on the test split.
I0130 09:05:54.641333 140187804313408 submission_runner.py:408] Time since start: 48632.84s, 	Step: 138269, 	{'train/accuracy': 0.867586076259613, 'train/loss': 0.4603193998336792, 'validation/accuracy': 0.731939971446991, 'validation/loss': 1.108080506324768, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.865692138671875, 'test/num_examples': 10000, 'score': 46959.6368894577, 'total_duration': 48632.8400952816, 'accumulated_submission_time': 46959.6368894577, 'accumulated_eval_time': 1664.1922266483307, 'accumulated_logging_time': 3.9070560932159424}
I0130 09:05:54.684944 140026033698560 logging_writer.py:48] [138269] accumulated_eval_time=1664.192227, accumulated_logging_time=3.907056, accumulated_submission_time=46959.636889, global_step=138269, preemption_count=0, score=46959.636889, test/accuracy=0.604600, test/loss=1.865692, test/num_examples=10000, total_duration=48632.840095, train/accuracy=0.867586, train/loss=0.460319, validation/accuracy=0.731940, validation/loss=1.108081, validation/num_examples=50000
I0130 09:06:05.510644 140026058876672 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.671541452407837, loss=1.0643820762634277
I0130 09:06:39.415280 140026033698560 logging_writer.py:48] [138400] global_step=138400, grad_norm=4.016096115112305, loss=0.9816513061523438
I0130 09:07:13.344835 140026058876672 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.3951659202575684, loss=0.9689088463783264
I0130 09:07:47.281654 140026033698560 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.6632909774780273, loss=0.973612368106842
I0130 09:08:21.230084 140026058876672 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.761094570159912, loss=0.9962659478187561
I0130 09:08:55.150034 140026033698560 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.7174251079559326, loss=1.027614951133728
I0130 09:09:29.190350 140026058876672 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.573845386505127, loss=1.0051699876785278
I0130 09:10:03.094373 140026033698560 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.7742862701416016, loss=1.085675597190857
I0130 09:10:37.042844 140026058876672 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.664257049560547, loss=0.9851902723312378
I0130 09:11:10.952633 140026033698560 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.47515606880188, loss=0.9503118991851807
I0130 09:11:44.888253 140026058876672 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.500091075897217, loss=1.001394510269165
I0130 09:12:18.836280 140026033698560 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.5394318103790283, loss=0.9691428542137146
I0130 09:12:52.777892 140026058876672 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.623185634613037, loss=0.9640179872512817
I0130 09:13:26.700945 140026033698560 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.765993595123291, loss=1.038189172744751
I0130 09:14:00.649182 140026058876672 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.860363483428955, loss=0.9705395698547363
I0130 09:14:24.899564 140187804313408 spec.py:321] Evaluating on the training split.
I0130 09:14:31.142497 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 09:14:39.814075 140187804313408 spec.py:349] Evaluating on the test split.
I0130 09:14:42.538176 140187804313408 submission_runner.py:408] Time since start: 49160.74s, 	Step: 139773, 	{'train/accuracy': 0.8621252775192261, 'train/loss': 0.47670185565948486, 'validation/accuracy': 0.7252799868583679, 'validation/loss': 1.134071946144104, 'validation/num_examples': 50000, 'test/accuracy': 0.5985000133514404, 'test/loss': 1.8925247192382812, 'test/num_examples': 10000, 'score': 47469.786170721054, 'total_duration': 49160.736943006516, 'accumulated_submission_time': 47469.786170721054, 'accumulated_eval_time': 1681.8307964801788, 'accumulated_logging_time': 3.9598209857940674}
I0130 09:14:42.579782 140026025305856 logging_writer.py:48] [139773] accumulated_eval_time=1681.830796, accumulated_logging_time=3.959821, accumulated_submission_time=47469.786171, global_step=139773, preemption_count=0, score=47469.786171, test/accuracy=0.598500, test/loss=1.892525, test/num_examples=10000, total_duration=49160.736943, train/accuracy=0.862125, train/loss=0.476702, validation/accuracy=0.725280, validation/loss=1.134072, validation/num_examples=50000
I0130 09:14:52.072815 140026042091264 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.65437650680542, loss=1.0247211456298828
I0130 09:15:25.972834 140026025305856 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.5974175930023193, loss=0.9991468191146851
I0130 09:15:59.960706 140026042091264 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.824342966079712, loss=1.0245568752288818
I0130 09:16:33.870079 140026025305856 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.8903250694274902, loss=0.9432076215744019
I0130 09:17:07.778768 140026042091264 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.576235055923462, loss=0.9709640145301819
I0130 09:17:41.725931 140026025305856 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.730830669403076, loss=0.9732308983802795
I0130 09:18:15.648180 140026042091264 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.7189931869506836, loss=0.9873161315917969
I0130 09:18:49.612969 140026025305856 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.6627302169799805, loss=0.9964497089385986
I0130 09:19:23.557462 140026042091264 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.3245716094970703, loss=0.9924284219741821
I0130 09:19:57.469618 140026025305856 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.406921863555908, loss=0.9549976587295532
I0130 09:20:31.410752 140026042091264 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.7155494689941406, loss=0.9593865871429443
I0130 09:21:05.298143 140026025305856 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.707150459289551, loss=1.0107008218765259
I0130 09:21:39.232987 140026042091264 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.84828519821167, loss=0.9922136664390564
I0130 09:22:13.188257 140026025305856 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.8808045387268066, loss=0.9737567901611328
I0130 09:22:47.185780 140026042091264 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.605588674545288, loss=0.9629846811294556
I0130 09:23:12.784992 140187804313408 spec.py:321] Evaluating on the training split.
I0130 09:23:19.135286 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 09:23:27.804218 140187804313408 spec.py:349] Evaluating on the test split.
I0130 09:23:30.504695 140187804313408 submission_runner.py:408] Time since start: 49688.70s, 	Step: 141277, 	{'train/accuracy': 0.8630420565605164, 'train/loss': 0.47475898265838623, 'validation/accuracy': 0.726419985294342, 'validation/loss': 1.1304347515106201, 'validation/num_examples': 50000, 'test/accuracy': 0.6062000393867493, 'test/loss': 1.8601926565170288, 'test/num_examples': 10000, 'score': 47979.92598223686, 'total_duration': 49688.70345711708, 'accumulated_submission_time': 47979.92598223686, 'accumulated_eval_time': 1699.5504500865936, 'accumulated_logging_time': 4.012192010879517}
I0130 09:23:30.550796 140026016913152 logging_writer.py:48] [141277] accumulated_eval_time=1699.550450, accumulated_logging_time=4.012192, accumulated_submission_time=47979.925982, global_step=141277, preemption_count=0, score=47979.925982, test/accuracy=0.606200, test/loss=1.860193, test/num_examples=10000, total_duration=49688.703457, train/accuracy=0.863042, train/loss=0.474759, validation/accuracy=0.726420, validation/loss=1.130435, validation/num_examples=50000
I0130 09:23:38.685357 140026025305856 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.742863655090332, loss=0.9507397413253784
I0130 09:24:12.574432 140026016913152 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.597625494003296, loss=0.9482570290565491
I0130 09:24:46.476291 140026025305856 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.578094482421875, loss=0.9496711492538452
I0130 09:25:20.425242 140026016913152 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.919421434402466, loss=0.9088295698165894
I0130 09:25:54.321481 140026025305856 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.8848626613616943, loss=1.0277540683746338
I0130 09:26:28.271765 140026016913152 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.683197498321533, loss=1.00081205368042
I0130 09:27:02.235281 140026025305856 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.390678882598877, loss=0.9017361998558044
I0130 09:27:36.170043 140026016913152 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.757889747619629, loss=0.8874672055244446
I0130 09:28:10.108328 140026025305856 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.506176710128784, loss=0.9108490943908691
I0130 09:28:44.038523 140026016913152 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.414222478866577, loss=0.9260143041610718
I0130 09:29:18.058447 140026025305856 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.8000686168670654, loss=1.0168954133987427
I0130 09:29:52.007695 140026016913152 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.5186777114868164, loss=0.9332785606384277
I0130 09:30:25.904278 140026025305856 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.8193585872650146, loss=0.9871068596839905
I0130 09:30:59.853173 140026016913152 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.5666003227233887, loss=0.8884372711181641
I0130 09:31:33.799384 140026025305856 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.701753854751587, loss=0.9500048756599426
I0130 09:32:00.756799 140187804313408 spec.py:321] Evaluating on the training split.
I0130 09:32:07.253325 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 09:32:15.784270 140187804313408 spec.py:349] Evaluating on the test split.
I0130 09:32:18.438378 140187804313408 submission_runner.py:408] Time since start: 50216.64s, 	Step: 142781, 	{'train/accuracy': 0.8639389276504517, 'train/loss': 0.46813029050827026, 'validation/accuracy': 0.7273600101470947, 'validation/loss': 1.1400127410888672, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9035028219223022, 'test/num_examples': 10000, 'score': 48490.05924510956, 'total_duration': 50216.637149333954, 'accumulated_submission_time': 48490.05924510956, 'accumulated_eval_time': 1717.2319912910461, 'accumulated_logging_time': 4.073648452758789}
I0130 09:32:18.481589 140026042091264 logging_writer.py:48] [142781] accumulated_eval_time=1717.231991, accumulated_logging_time=4.073648, accumulated_submission_time=48490.059245, global_step=142781, preemption_count=0, score=48490.059245, test/accuracy=0.596300, test/loss=1.903503, test/num_examples=10000, total_duration=50216.637149, train/accuracy=0.863939, train/loss=0.468130, validation/accuracy=0.727360, validation/loss=1.140013, validation/num_examples=50000
I0130 09:32:25.277413 140026050483968 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.6338109970092773, loss=0.9451789855957031
I0130 09:32:59.133471 140026042091264 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.8003289699554443, loss=1.049994707107544
I0130 09:33:33.051393 140026050483968 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.972602367401123, loss=0.9776080846786499
I0130 09:34:06.975288 140026042091264 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.8088197708129883, loss=0.9309573173522949
I0130 09:34:40.902714 140026050483968 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.5008320808410645, loss=0.8308301568031311
I0130 09:35:14.820888 140026042091264 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.662818670272827, loss=1.009061336517334
I0130 09:35:48.835582 140026050483968 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.83713960647583, loss=0.9431937336921692
I0130 09:36:22.773336 140026042091264 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.6982481479644775, loss=0.9285195469856262
I0130 09:36:56.721506 140026050483968 logging_writer.py:48] [143600] global_step=143600, grad_norm=4.101797103881836, loss=0.967573881149292
I0130 09:37:30.646097 140026042091264 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.8659069538116455, loss=0.9445794820785522
I0130 09:38:04.600564 140026050483968 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.629730701446533, loss=0.9377163052558899
I0130 09:38:38.553537 140026042091264 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.857114315032959, loss=0.9209779500961304
I0130 09:39:12.471576 140026050483968 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.6566758155822754, loss=0.96410071849823
I0130 09:39:46.408247 140026042091264 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.661271572113037, loss=1.0005815029144287
I0130 09:40:20.312600 140026050483968 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.810150384902954, loss=0.9505953788757324
I0130 09:40:48.611212 140187804313408 spec.py:321] Evaluating on the training split.
I0130 09:40:54.904076 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 09:41:03.962346 140187804313408 spec.py:349] Evaluating on the test split.
I0130 09:41:06.637830 140187804313408 submission_runner.py:408] Time since start: 50744.84s, 	Step: 144285, 	{'train/accuracy': 0.8989357352256775, 'train/loss': 0.352335661649704, 'validation/accuracy': 0.7356399893760681, 'validation/loss': 1.1080385446548462, 'validation/num_examples': 50000, 'test/accuracy': 0.6078000068664551, 'test/loss': 1.842755913734436, 'test/num_examples': 10000, 'score': 49000.12387108803, 'total_duration': 50744.836602687836, 'accumulated_submission_time': 49000.12387108803, 'accumulated_eval_time': 1735.258573770523, 'accumulated_logging_time': 4.126613616943359}
I0130 09:41:06.684674 140026016913152 logging_writer.py:48] [144285] accumulated_eval_time=1735.258574, accumulated_logging_time=4.126614, accumulated_submission_time=49000.123871, global_step=144285, preemption_count=0, score=49000.123871, test/accuracy=0.607800, test/loss=1.842756, test/num_examples=10000, total_duration=50744.836603, train/accuracy=0.898936, train/loss=0.352336, validation/accuracy=0.735640, validation/loss=1.108039, validation/num_examples=50000
I0130 09:41:12.120069 140026025305856 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.8004982471466064, loss=0.9450092315673828
I0130 09:41:45.996206 140026016913152 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.718981981277466, loss=0.9083557724952698
I0130 09:42:19.988164 140026025305856 logging_writer.py:48] [144500] global_step=144500, grad_norm=4.102127552032471, loss=0.9694247245788574
I0130 09:42:53.940795 140026016913152 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.403118848800659, loss=0.9111056327819824
I0130 09:43:27.885260 140026025305856 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.8993375301361084, loss=1.0190248489379883
I0130 09:44:01.826104 140026016913152 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.9383111000061035, loss=0.9292860627174377
I0130 09:44:35.782393 140026025305856 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.730661392211914, loss=0.9502730369567871
I0130 09:45:09.719932 140026016913152 logging_writer.py:48] [145000] global_step=145000, grad_norm=4.029911994934082, loss=0.9433789253234863
I0130 09:45:43.672550 140026025305856 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.699465274810791, loss=0.9013019800186157
I0130 09:46:17.588025 140026016913152 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.715592622756958, loss=0.9356544017791748
I0130 09:46:51.489999 140026025305856 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.7642970085144043, loss=0.9483531713485718
I0130 09:47:25.436576 140026016913152 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.5864450931549072, loss=0.8161032795906067
I0130 09:47:59.351204 140026025305856 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.8852758407592773, loss=0.8918799161911011
I0130 09:48:33.318654 140026016913152 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.748933792114258, loss=0.8354357481002808
I0130 09:49:07.303533 140026025305856 logging_writer.py:48] [145700] global_step=145700, grad_norm=4.120216369628906, loss=0.9020079374313354
I0130 09:49:36.663712 140187804313408 spec.py:321] Evaluating on the training split.
I0130 09:49:42.967438 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 09:49:51.668479 140187804313408 spec.py:349] Evaluating on the test split.
I0130 09:49:54.362997 140187804313408 submission_runner.py:408] Time since start: 51272.56s, 	Step: 145788, 	{'train/accuracy': 0.8931760191917419, 'train/loss': 0.36942258477211, 'validation/accuracy': 0.7378199696540833, 'validation/loss': 1.0927354097366333, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.842835783958435, 'test/num_examples': 10000, 'score': 49510.038204193115, 'total_duration': 51272.56175875664, 'accumulated_submission_time': 49510.038204193115, 'accumulated_eval_time': 1752.9578087329865, 'accumulated_logging_time': 4.1841514110565186}
I0130 09:49:54.407384 140026016913152 logging_writer.py:48] [145788] accumulated_eval_time=1752.957809, accumulated_logging_time=4.184151, accumulated_submission_time=49510.038204, global_step=145788, preemption_count=0, score=49510.038204, test/accuracy=0.613200, test/loss=1.842836, test/num_examples=10000, total_duration=51272.561759, train/accuracy=0.893176, train/loss=0.369423, validation/accuracy=0.737820, validation/loss=1.092735, validation/num_examples=50000
I0130 09:49:58.841828 140026042091264 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.912811517715454, loss=1.020305871963501
I0130 09:50:32.742317 140026016913152 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.6116206645965576, loss=0.8879970908164978
I0130 09:51:06.641487 140026042091264 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.978562355041504, loss=0.92705237865448
I0130 09:51:40.590545 140026016913152 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.5987725257873535, loss=0.850915789604187
I0130 09:52:14.542873 140026042091264 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.9386682510375977, loss=0.9083040952682495
I0130 09:52:48.444886 140026016913152 logging_writer.py:48] [146300] global_step=146300, grad_norm=4.08683967590332, loss=0.8837958574295044
I0130 09:53:22.396062 140026042091264 logging_writer.py:48] [146400] global_step=146400, grad_norm=4.01815938949585, loss=0.9236322045326233
I0130 09:53:56.769102 140026016913152 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.878732442855835, loss=0.9202451705932617
I0130 09:54:30.721915 140026042091264 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.6657021045684814, loss=0.8961467146873474
I0130 09:55:04.650913 140026016913152 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.970059394836426, loss=0.9053735733032227
I0130 09:55:38.748753 140026042091264 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.246482849121094, loss=0.8729350566864014
I0130 09:56:12.686220 140026016913152 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.716836929321289, loss=0.9609588980674744
I0130 09:56:46.637083 140026042091264 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.9230003356933594, loss=0.9550180435180664
I0130 09:57:20.562599 140026016913152 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.968820333480835, loss=0.9242407083511353
I0130 09:57:54.528730 140026042091264 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.9108386039733887, loss=0.8952425718307495
I0130 09:58:24.515043 140187804313408 spec.py:321] Evaluating on the training split.
I0130 09:58:30.854379 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 09:58:39.752764 140187804313408 spec.py:349] Evaluating on the test split.
I0130 09:58:42.436643 140187804313408 submission_runner.py:408] Time since start: 51800.64s, 	Step: 147290, 	{'train/accuracy': 0.8905253410339355, 'train/loss': 0.3740461766719818, 'validation/accuracy': 0.7361399531364441, 'validation/loss': 1.08772873878479, 'validation/num_examples': 50000, 'test/accuracy': 0.6048000454902649, 'test/loss': 1.8468471765518188, 'test/num_examples': 10000, 'score': 50020.07937192917, 'total_duration': 51800.63540673256, 'accumulated_submission_time': 50020.07937192917, 'accumulated_eval_time': 1770.8793761730194, 'accumulated_logging_time': 4.238028526306152}
I0130 09:58:42.483564 140025891108608 logging_writer.py:48] [147290] accumulated_eval_time=1770.879376, accumulated_logging_time=4.238029, accumulated_submission_time=50020.079372, global_step=147290, preemption_count=0, score=50020.079372, test/accuracy=0.604800, test/loss=1.846847, test/num_examples=10000, total_duration=51800.635407, train/accuracy=0.890525, train/loss=0.374046, validation/accuracy=0.736140, validation/loss=1.087729, validation/num_examples=50000
I0130 09:58:46.226007 140026016913152 logging_writer.py:48] [147300] global_step=147300, grad_norm=4.03134298324585, loss=0.8225104808807373
I0130 09:59:20.099138 140025891108608 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.805917978286743, loss=0.8239774107933044
I0130 09:59:54.013850 140026016913152 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.7215659618377686, loss=0.8957561254501343
I0130 10:00:27.966026 140025891108608 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.8363254070281982, loss=0.8823500871658325
I0130 10:01:01.909538 140026016913152 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.8320770263671875, loss=0.9071284532546997
I0130 10:01:35.835614 140025891108608 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.326447486877441, loss=0.9121553897857666
I0130 10:02:09.911382 140026016913152 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.147067546844482, loss=1.0014798641204834
I0130 10:02:43.837317 140025891108608 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.023883819580078, loss=0.9111208915710449
I0130 10:03:17.774438 140026016913152 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.805103063583374, loss=0.8329947590827942
I0130 10:03:51.728868 140025891108608 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.688946008682251, loss=0.8557913303375244
I0130 10:04:25.691100 140026016913152 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.881833076477051, loss=0.8032400012016296
I0130 10:04:59.592226 140025891108608 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.7343902587890625, loss=0.8405343890190125
I0130 10:05:33.553387 140026016913152 logging_writer.py:48] [148500] global_step=148500, grad_norm=4.2461113929748535, loss=0.9363638758659363
I0130 10:06:07.511882 140025891108608 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.954968214035034, loss=0.8849101066589355
I0130 10:06:41.438982 140026016913152 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.9045331478118896, loss=0.893612802028656
I0130 10:07:12.457015 140187804313408 spec.py:321] Evaluating on the training split.
I0130 10:07:18.735153 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 10:07:27.652706 140187804313408 spec.py:349] Evaluating on the test split.
I0130 10:07:30.346314 140187804313408 submission_runner.py:408] Time since start: 52328.55s, 	Step: 148793, 	{'train/accuracy': 0.8932557106018066, 'train/loss': 0.3641088008880615, 'validation/accuracy': 0.7346799969673157, 'validation/loss': 1.1118369102478027, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.8756835460662842, 'test/num_examples': 10000, 'score': 50529.98758006096, 'total_duration': 52328.545087337494, 'accumulated_submission_time': 50529.98758006096, 'accumulated_eval_time': 1788.7686505317688, 'accumulated_logging_time': 4.294397830963135}
I0130 10:07:30.391077 140026042091264 logging_writer.py:48] [148793] accumulated_eval_time=1788.768651, accumulated_logging_time=4.294398, accumulated_submission_time=50529.987580, global_step=148793, preemption_count=0, score=50529.987580, test/accuracy=0.607300, test/loss=1.875684, test/num_examples=10000, total_duration=52328.545087, train/accuracy=0.893256, train/loss=0.364109, validation/accuracy=0.734680, validation/loss=1.111837, validation/num_examples=50000
I0130 10:07:33.117429 140026050483968 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.8675038814544678, loss=0.8324533700942993
I0130 10:08:07.034223 140026042091264 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.8332009315490723, loss=0.8321044445037842
I0130 10:08:40.928613 140026050483968 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.364491939544678, loss=0.9368129372596741
I0130 10:09:15.104026 140026042091264 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.8275399208068848, loss=0.8954670429229736
I0130 10:09:49.027880 140026050483968 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.951932191848755, loss=0.851989209651947
I0130 10:10:22.936982 140026042091264 logging_writer.py:48] [149300] global_step=149300, grad_norm=4.1512227058410645, loss=0.8975885510444641
I0130 10:10:56.894237 140026050483968 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.9117181301116943, loss=0.8503260612487793
I0130 10:11:30.855537 140026042091264 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.442240238189697, loss=0.8914255499839783
I0130 10:12:04.759914 140026050483968 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.769549608230591, loss=0.8681402802467346
I0130 10:12:38.724350 140026042091264 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.865915298461914, loss=0.8624532222747803
I0130 10:13:12.677721 140026050483968 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.000083923339844, loss=0.8898487091064453
I0130 10:13:46.630533 140026042091264 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.211828231811523, loss=0.8893753290176392
I0130 10:14:20.547021 140026050483968 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.099770545959473, loss=0.8390698432922363
I0130 10:14:54.496962 140026042091264 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.031057357788086, loss=0.888894259929657
I0130 10:15:28.510814 140026050483968 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.790659189224243, loss=0.7705476880073547
I0130 10:16:00.552857 140187804313408 spec.py:321] Evaluating on the training split.
I0130 10:16:06.961896 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 10:16:15.628207 140187804313408 spec.py:349] Evaluating on the test split.
I0130 10:16:18.338587 140187804313408 submission_runner.py:408] Time since start: 52856.54s, 	Step: 150296, 	{'train/accuracy': 0.8932358026504517, 'train/loss': 0.36360234022140503, 'validation/accuracy': 0.7393400073051453, 'validation/loss': 1.103121042251587, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.858383297920227, 'test/num_examples': 10000, 'score': 51040.08410692215, 'total_duration': 52856.537358284, 'accumulated_submission_time': 51040.08410692215, 'accumulated_eval_time': 1806.554343700409, 'accumulated_logging_time': 4.349008321762085}
I0130 10:16:18.385764 140026016913152 logging_writer.py:48] [150296] accumulated_eval_time=1806.554344, accumulated_logging_time=4.349008, accumulated_submission_time=51040.084107, global_step=150296, preemption_count=0, score=51040.084107, test/accuracy=0.616000, test/loss=1.858383, test/num_examples=10000, total_duration=52856.537358, train/accuracy=0.893236, train/loss=0.363602, validation/accuracy=0.739340, validation/loss=1.103121, validation/num_examples=50000
I0130 10:16:20.080965 140026025305856 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.9084086418151855, loss=0.8489187359809875
I0130 10:16:53.955674 140026016913152 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.8248751163482666, loss=0.7968541979789734
I0130 10:17:27.848835 140026025305856 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.7542364597320557, loss=0.8931216597557068
I0130 10:18:01.755796 140026016913152 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.106807231903076, loss=0.9080759286880493
I0130 10:18:35.707853 140026025305856 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.662095785140991, loss=0.8005401492118835
I0130 10:19:09.661315 140026016913152 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.698540687561035, loss=0.8305339217185974
I0130 10:19:43.611823 140026025305856 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.211924076080322, loss=0.9882556200027466
I0130 10:20:17.554881 140026016913152 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.827436923980713, loss=0.80368971824646
I0130 10:20:51.494299 140026025305856 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.823718309402466, loss=0.9468512535095215
I0130 10:21:25.475437 140026016913152 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.152624607086182, loss=0.8714194297790527
I0130 10:21:59.502923 140026025305856 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.6970980167388916, loss=0.8380047082901001
I0130 10:22:33.456196 140026016913152 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.9378461837768555, loss=0.8535041213035583
I0130 10:23:07.415689 140026025305856 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.06797981262207, loss=0.8880646824836731
I0130 10:23:41.384635 140026016913152 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.06467866897583, loss=0.8588759899139404
I0130 10:24:15.328740 140026025305856 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.086287498474121, loss=0.8347681760787964
I0130 10:24:48.423983 140187804313408 spec.py:321] Evaluating on the training split.
I0130 10:24:54.757641 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 10:25:03.864255 140187804313408 spec.py:349] Evaluating on the test split.
I0130 10:25:06.583597 140187804313408 submission_runner.py:408] Time since start: 53384.78s, 	Step: 151799, 	{'train/accuracy': 0.8980388641357422, 'train/loss': 0.35135653614997864, 'validation/accuracy': 0.7346799969673157, 'validation/loss': 1.1100929975509644, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.8680452108383179, 'test/num_examples': 10000, 'score': 51550.05728435516, 'total_duration': 53384.782229185104, 'accumulated_submission_time': 51550.05728435516, 'accumulated_eval_time': 1824.7137916088104, 'accumulated_logging_time': 4.405257701873779}
I0130 10:25:06.630353 140026058876672 logging_writer.py:48] [151799] accumulated_eval_time=1824.713792, accumulated_logging_time=4.405258, accumulated_submission_time=51550.057284, global_step=151799, preemption_count=0, score=51550.057284, test/accuracy=0.609900, test/loss=1.868045, test/num_examples=10000, total_duration=53384.782229, train/accuracy=0.898039, train/loss=0.351357, validation/accuracy=0.734680, validation/loss=1.110093, validation/num_examples=50000
I0130 10:25:07.317972 140026067269376 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.1881561279296875, loss=0.8436710834503174
I0130 10:25:41.173198 140026058876672 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.389618873596191, loss=0.926002025604248
I0130 10:26:15.113301 140026067269376 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.827486991882324, loss=0.9009323120117188
I0130 10:26:49.025616 140026058876672 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.092340469360352, loss=0.8484600782394409
I0130 10:27:22.946429 140026067269376 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.950676918029785, loss=0.8148780465126038
I0130 10:27:56.880880 140026058876672 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.9740653038024902, loss=0.8726339936256409
I0130 10:28:30.800821 140026067269376 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.0765156745910645, loss=0.811663031578064
I0130 10:29:04.805011 140026058876672 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.961594581604004, loss=0.8068481683731079
I0130 10:29:38.741197 140026067269376 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.011520862579346, loss=0.9101853966712952
I0130 10:30:12.666286 140026058876672 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.9611527919769287, loss=0.7739570140838623
I0130 10:30:46.593839 140026067269376 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.7466225624084473, loss=0.8110716342926025
I0130 10:31:20.517505 140026058876672 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.129103183746338, loss=0.8520114421844482
I0130 10:31:54.481548 140026067269376 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.110418796539307, loss=0.8479581475257874
I0130 10:32:28.412290 140026058876672 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.063630104064941, loss=0.8297019004821777
I0130 10:33:02.328814 140026067269376 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.081508159637451, loss=0.7753943204879761
I0130 10:33:36.312987 140026058876672 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.428371906280518, loss=0.8642963171005249
I0130 10:33:36.809849 140187804313408 spec.py:321] Evaluating on the training split.
I0130 10:33:43.102594 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 10:33:51.969931 140187804313408 spec.py:349] Evaluating on the test split.
I0130 10:33:54.633432 140187804313408 submission_runner.py:408] Time since start: 53912.83s, 	Step: 153303, 	{'train/accuracy': 0.9187459945678711, 'train/loss': 0.28570467233657837, 'validation/accuracy': 0.7361999750137329, 'validation/loss': 1.1064831018447876, 'validation/num_examples': 50000, 'test/accuracy': 0.6100000143051147, 'test/loss': 1.8766512870788574, 'test/num_examples': 10000, 'score': 52060.17003774643, 'total_duration': 53912.83220410347, 'accumulated_submission_time': 52060.17003774643, 'accumulated_eval_time': 1842.5373332500458, 'accumulated_logging_time': 4.462253093719482}
I0130 10:33:54.677176 140026033698560 logging_writer.py:48] [153303] accumulated_eval_time=1842.537333, accumulated_logging_time=4.462253, accumulated_submission_time=52060.170038, global_step=153303, preemption_count=0, score=52060.170038, test/accuracy=0.610000, test/loss=1.876651, test/num_examples=10000, total_duration=53912.832204, train/accuracy=0.918746, train/loss=0.285705, validation/accuracy=0.736200, validation/loss=1.106483, validation/num_examples=50000
I0130 10:34:27.904442 140026042091264 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.30440092086792, loss=0.9705067873001099
I0130 10:35:01.813346 140026033698560 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.012359142303467, loss=0.8835445642471313
I0130 10:35:35.825380 140026042091264 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.910186290740967, loss=0.845003604888916
I0130 10:36:09.780383 140026033698560 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.780120849609375, loss=0.7784790992736816
I0130 10:36:43.718732 140026042091264 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.9506311416625977, loss=0.7830916047096252
I0130 10:37:17.655165 140026033698560 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.6984710693359375, loss=0.7711213827133179
I0130 10:37:51.617318 140026042091264 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.29649019241333, loss=0.8104386925697327
I0130 10:38:25.549350 140026033698560 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.087879180908203, loss=0.8837658166885376
I0130 10:38:59.483389 140026042091264 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.9359049797058105, loss=0.7280019521713257
I0130 10:39:33.403211 140026033698560 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.380361080169678, loss=0.851631760597229
I0130 10:40:07.351705 140026042091264 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.917813777923584, loss=0.7382307052612305
I0130 10:40:41.285607 140026033698560 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.131374359130859, loss=0.890685498714447
I0130 10:41:15.230515 140026042091264 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.726170301437378, loss=0.726449191570282
I0130 10:41:49.230926 140026033698560 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.8138961791992188, loss=0.7750535607337952
I0130 10:42:23.173068 140026042091264 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.265891075134277, loss=0.8850601315498352
I0130 10:42:24.676877 140187804313408 spec.py:321] Evaluating on the training split.
I0130 10:42:30.980024 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 10:42:39.972229 140187804313408 spec.py:349] Evaluating on the test split.
I0130 10:42:42.715283 140187804313408 submission_runner.py:408] Time since start: 54440.91s, 	Step: 154806, 	{'train/accuracy': 0.9205396771430969, 'train/loss': 0.2786844074726105, 'validation/accuracy': 0.7440399527549744, 'validation/loss': 1.0820631980895996, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.84649658203125, 'test/num_examples': 10000, 'score': 52570.1029856205, 'total_duration': 54440.91394495964, 'accumulated_submission_time': 52570.1029856205, 'accumulated_eval_time': 1860.575585603714, 'accumulated_logging_time': 4.516483306884766}
I0130 10:42:42.762904 140025891108608 logging_writer.py:48] [154806] accumulated_eval_time=1860.575586, accumulated_logging_time=4.516483, accumulated_submission_time=52570.102986, global_step=154806, preemption_count=0, score=52570.102986, test/accuracy=0.616600, test/loss=1.846497, test/num_examples=10000, total_duration=54440.913945, train/accuracy=0.920540, train/loss=0.278684, validation/accuracy=0.744040, validation/loss=1.082063, validation/num_examples=50000
I0130 10:43:14.946770 140026016913152 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.086610317230225, loss=0.7267025709152222
I0130 10:43:48.853494 140025891108608 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.1923346519470215, loss=0.8437729477882385
I0130 10:44:22.747672 140026016913152 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.146712303161621, loss=0.7834782600402832
I0130 10:44:56.696836 140025891108608 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.465802192687988, loss=0.8458768129348755
I0130 10:45:30.621364 140026016913152 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.202315330505371, loss=0.8793281316757202
I0130 10:46:04.545991 140025891108608 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.271827697753906, loss=0.8283186554908752
I0130 10:46:38.482256 140026016913152 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.052781581878662, loss=0.7472153306007385
I0130 10:47:12.399616 140025891108608 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.982909679412842, loss=0.8184470534324646
I0130 10:47:46.351727 140026016913152 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.193408966064453, loss=0.8577131628990173
I0130 10:48:20.304424 140025891108608 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.480224609375, loss=0.8029050827026367
I0130 10:48:54.327461 140026016913152 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.475282192230225, loss=0.8546789288520813
I0130 10:49:28.275709 140025891108608 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.102090835571289, loss=0.8016262650489807
I0130 10:50:02.198376 140026016913152 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.360492706298828, loss=0.7546676993370056
I0130 10:50:36.180380 140025891108608 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.09920597076416, loss=0.8611545562744141
I0130 10:51:10.140505 140026016913152 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.306307315826416, loss=0.8083624839782715
I0130 10:51:13.003545 140187804313408 spec.py:321] Evaluating on the training split.
I0130 10:51:19.386114 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 10:51:28.198814 140187804313408 spec.py:349] Evaluating on the test split.
I0130 10:51:30.886657 140187804313408 submission_runner.py:408] Time since start: 54969.09s, 	Step: 156310, 	{'train/accuracy': 0.9159757494926453, 'train/loss': 0.2899870276451111, 'validation/accuracy': 0.7417399883270264, 'validation/loss': 1.092580795288086, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.8602204322814941, 'test/num_examples': 10000, 'score': 53080.279076099396, 'total_duration': 54969.0854177475, 'accumulated_submission_time': 53080.279076099396, 'accumulated_eval_time': 1878.458645582199, 'accumulated_logging_time': 4.5743114948272705}
I0130 10:51:30.930352 140025891108608 logging_writer.py:48] [156310] accumulated_eval_time=1878.458646, accumulated_logging_time=4.574311, accumulated_submission_time=53080.279076, global_step=156310, preemption_count=0, score=53080.279076, test/accuracy=0.615900, test/loss=1.860220, test/num_examples=10000, total_duration=54969.085418, train/accuracy=0.915976, train/loss=0.289987, validation/accuracy=0.741740, validation/loss=1.092581, validation/num_examples=50000
I0130 10:52:01.739561 140026042091264 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.242042541503906, loss=0.805077314376831
I0130 10:52:35.641666 140025891108608 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.419969081878662, loss=0.769869327545166
I0130 10:53:09.571556 140026042091264 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.940553903579712, loss=0.8244855999946594
I0130 10:53:43.527176 140025891108608 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.8701119422912598, loss=0.7929900884628296
I0130 10:54:17.519950 140026042091264 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.429073333740234, loss=0.754462480545044
I0130 10:54:51.472564 140025891108608 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.094237327575684, loss=0.7487782835960388
I0130 10:55:25.489454 140026042091264 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.133402347564697, loss=0.7983089089393616
I0130 10:55:59.416193 140025891108608 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.30172872543335, loss=0.8052017688751221
I0130 10:56:33.360333 140026042091264 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.9141526222229004, loss=0.7390211224555969
I0130 10:57:07.300120 140025891108608 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.341584205627441, loss=0.7958060503005981
I0130 10:57:41.276056 140026042091264 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.362577438354492, loss=0.806041955947876
I0130 10:58:15.238131 140025891108608 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.042393207550049, loss=0.7297207117080688
I0130 10:58:49.159420 140026042091264 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.112826347351074, loss=0.7897986173629761
I0130 10:59:23.113972 140025891108608 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.46950626373291, loss=0.8095224499702454
I0130 10:59:57.079565 140026042091264 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.221804141998291, loss=0.7611865401268005
I0130 11:00:00.960768 140187804313408 spec.py:321] Evaluating on the training split.
I0130 11:00:07.453024 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 11:00:16.098707 140187804313408 spec.py:349] Evaluating on the test split.
I0130 11:00:18.789546 140187804313408 submission_runner.py:408] Time since start: 55496.99s, 	Step: 157813, 	{'train/accuracy': 0.9194834232330322, 'train/loss': 0.27230069041252136, 'validation/accuracy': 0.7447599768638611, 'validation/loss': 1.0868667364120483, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.8521299362182617, 'test/num_examples': 10000, 'score': 53590.2423658371, 'total_duration': 55496.98831796646, 'accumulated_submission_time': 53590.2423658371, 'accumulated_eval_time': 1896.2873928546906, 'accumulated_logging_time': 4.627979040145874}
I0130 11:00:18.834805 140026025305856 logging_writer.py:48] [157813] accumulated_eval_time=1896.287393, accumulated_logging_time=4.627979, accumulated_submission_time=53590.242366, global_step=157813, preemption_count=0, score=53590.242366, test/accuracy=0.617400, test/loss=1.852130, test/num_examples=10000, total_duration=55496.988318, train/accuracy=0.919483, train/loss=0.272301, validation/accuracy=0.744760, validation/loss=1.086867, validation/num_examples=50000
I0130 11:00:48.645886 140026033698560 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.592637062072754, loss=0.874574601650238
I0130 11:01:22.574579 140026025305856 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.9767813682556152, loss=0.7173193097114563
I0130 11:01:56.628099 140026033698560 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.08474588394165, loss=0.7537708878517151
I0130 11:02:30.569255 140026025305856 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.282278537750244, loss=0.861910879611969
I0130 11:03:04.486683 140026033698560 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.29829216003418, loss=0.8354070782661438
I0130 11:03:38.439932 140026025305856 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.090198516845703, loss=0.8488731384277344
I0130 11:04:12.381549 140026033698560 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.676873207092285, loss=0.879554033279419
I0130 11:04:46.308800 140026025305856 logging_writer.py:48] [158600] global_step=158600, grad_norm=3.9956674575805664, loss=0.7359058856964111
I0130 11:05:20.257770 140026033698560 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.167384147644043, loss=0.7929730415344238
I0130 11:05:54.220884 140026025305856 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.27477502822876, loss=0.855780839920044
I0130 11:06:28.172410 140026033698560 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.213902473449707, loss=0.8221836090087891
I0130 11:07:02.125857 140026025305856 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.0943121910095215, loss=0.761408269405365
I0130 11:07:36.058609 140026033698560 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.5380988121032715, loss=0.7783751487731934
I0130 11:08:10.117815 140026025305856 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.553060531616211, loss=0.8129655718803406
I0130 11:08:44.054645 140026033698560 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.8860950469970703, loss=0.7935217022895813
I0130 11:08:48.964967 140187804313408 spec.py:321] Evaluating on the training split.
I0130 11:08:55.230858 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 11:09:04.323690 140187804313408 spec.py:349] Evaluating on the test split.
I0130 11:09:07.032729 140187804313408 submission_runner.py:408] Time since start: 56025.23s, 	Step: 159316, 	{'train/accuracy': 0.9231106042861938, 'train/loss': 0.2618570923805237, 'validation/accuracy': 0.746239960193634, 'validation/loss': 1.0792582035064697, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.8350087404251099, 'test/num_examples': 10000, 'score': 54100.30746936798, 'total_duration': 56025.2314991951, 'accumulated_submission_time': 54100.30746936798, 'accumulated_eval_time': 1914.3551132678986, 'accumulated_logging_time': 4.6828649044036865}
I0130 11:09:07.079202 140026058876672 logging_writer.py:48] [159316] accumulated_eval_time=1914.355113, accumulated_logging_time=4.682865, accumulated_submission_time=54100.307469, global_step=159316, preemption_count=0, score=54100.307469, test/accuracy=0.622300, test/loss=1.835009, test/num_examples=10000, total_duration=56025.231499, train/accuracy=0.923111, train/loss=0.261857, validation/accuracy=0.746240, validation/loss=1.079258, validation/num_examples=50000
I0130 11:09:35.892060 140026067269376 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.131347179412842, loss=0.7886602878570557
I0130 11:10:09.816982 140026058876672 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.538443565368652, loss=0.8328930735588074
I0130 11:10:43.747386 140026067269376 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.5228962898254395, loss=0.7600840330123901
I0130 11:11:17.713995 140026058876672 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.315317153930664, loss=0.784622073173523
I0130 11:11:51.657849 140026067269376 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.371408939361572, loss=0.774564266204834
I0130 11:12:25.604308 140026058876672 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.265938758850098, loss=0.8131582140922546
I0130 11:12:59.529493 140026067269376 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.553658485412598, loss=0.8547003865242004
I0130 11:13:33.482864 140026058876672 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.325829982757568, loss=0.7575029134750366
I0130 11:14:07.436210 140026067269376 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.145772933959961, loss=0.7244063019752502
I0130 11:14:41.472453 140026058876672 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.409366130828857, loss=0.8773477077484131
I0130 11:15:15.440279 140026067269376 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.52706241607666, loss=0.7878803014755249
I0130 11:15:49.398671 140026058876672 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.0544843673706055, loss=0.7726364731788635
I0130 11:16:23.362134 140026067269376 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.03800630569458, loss=0.6921149492263794
I0130 11:16:57.304145 140026058876672 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.486453056335449, loss=0.7096649408340454
I0130 11:17:31.234911 140026067269376 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.217241287231445, loss=0.7444822788238525
I0130 11:17:37.143385 140187804313408 spec.py:321] Evaluating on the training split.
I0130 11:17:43.436324 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 11:17:52.242576 140187804313408 spec.py:349] Evaluating on the test split.
I0130 11:17:54.907630 140187804313408 submission_runner.py:408] Time since start: 56553.11s, 	Step: 160819, 	{'train/accuracy': 0.9272759556770325, 'train/loss': 0.25060129165649414, 'validation/accuracy': 0.7461400032043457, 'validation/loss': 1.0742896795272827, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.841195821762085, 'test/num_examples': 10000, 'score': 54610.303926706314, 'total_duration': 56553.10640120506, 'accumulated_submission_time': 54610.303926706314, 'accumulated_eval_time': 1932.1193158626556, 'accumulated_logging_time': 4.740338563919067}
I0130 11:17:54.953098 140025882715904 logging_writer.py:48] [160819] accumulated_eval_time=1932.119316, accumulated_logging_time=4.740339, accumulated_submission_time=54610.303927, global_step=160819, preemption_count=0, score=54610.303927, test/accuracy=0.616500, test/loss=1.841196, test/num_examples=10000, total_duration=56553.106401, train/accuracy=0.927276, train/loss=0.250601, validation/accuracy=0.746140, validation/loss=1.074290, validation/num_examples=50000
I0130 11:18:22.749292 140025891108608 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.122922420501709, loss=0.7096086144447327
I0130 11:18:56.660583 140025882715904 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.177313804626465, loss=0.7332479953765869
I0130 11:19:30.596436 140025891108608 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.075604438781738, loss=0.6818748712539673
I0130 11:20:04.500248 140025882715904 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.737951755523682, loss=0.7790836691856384
I0130 11:20:38.453016 140025891108608 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.364772796630859, loss=0.8128220438957214
I0130 11:21:12.353847 140025882715904 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.737295150756836, loss=0.743674635887146
I0130 11:21:46.484263 140025891108608 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.2015790939331055, loss=0.6810874342918396
I0130 11:22:20.428237 140025882715904 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.092203617095947, loss=0.6935080885887146
I0130 11:22:54.368317 140025891108608 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.312067031860352, loss=0.7486346960067749
I0130 11:23:28.328325 140025882715904 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.17728853225708, loss=0.7207891345024109
I0130 11:24:02.286598 140025891108608 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.470311641693115, loss=0.7123143076896667
I0130 11:24:36.198010 140025882715904 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.819591045379639, loss=0.7695772647857666
I0130 11:25:10.130462 140025891108608 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.265821933746338, loss=0.7609573602676392
I0130 11:25:44.098454 140025882715904 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.746347904205322, loss=0.831628143787384
I0130 11:26:18.059224 140025891108608 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.813007831573486, loss=0.8000668883323669
I0130 11:26:24.976258 140187804313408 spec.py:321] Evaluating on the training split.
I0130 11:26:31.246050 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 11:26:39.851012 140187804313408 spec.py:349] Evaluating on the test split.
I0130 11:26:42.523685 140187804313408 submission_runner.py:408] Time since start: 57080.72s, 	Step: 162322, 	{'train/accuracy': 0.9462890625, 'train/loss': 0.1946813315153122, 'validation/accuracy': 0.7486400008201599, 'validation/loss': 1.0698437690734863, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.8549610376358032, 'test/num_examples': 10000, 'score': 55120.26196694374, 'total_duration': 57080.722338199615, 'accumulated_submission_time': 55120.26196694374, 'accumulated_eval_time': 1949.6665840148926, 'accumulated_logging_time': 4.796144008636475}
I0130 11:26:42.573207 140025891108608 logging_writer.py:48] [162322] accumulated_eval_time=1949.666584, accumulated_logging_time=4.796144, accumulated_submission_time=55120.261967, global_step=162322, preemption_count=0, score=55120.261967, test/accuracy=0.617200, test/loss=1.854961, test/num_examples=10000, total_duration=57080.722338, train/accuracy=0.946289, train/loss=0.194681, validation/accuracy=0.748640, validation/loss=1.069844, validation/num_examples=50000
I0130 11:27:09.379665 140026016913152 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.27919864654541, loss=0.7371627688407898
I0130 11:27:43.293931 140025891108608 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.109646797180176, loss=0.6793325543403625
I0130 11:28:17.376995 140026016913152 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.420037746429443, loss=0.7008877396583557
I0130 11:28:51.316335 140025891108608 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.39752721786499, loss=0.7186356782913208
I0130 11:29:25.271744 140026016913152 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.392375469207764, loss=0.7091320753097534
I0130 11:29:59.242015 140025891108608 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.366483211517334, loss=0.7217091917991638
I0130 11:30:33.190546 140026016913152 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.353550910949707, loss=0.7148872017860413
I0130 11:31:07.128254 140025891108608 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.434034824371338, loss=0.7492650151252747
I0130 11:31:41.091740 140026016913152 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.390560150146484, loss=0.7106068730354309
I0130 11:32:15.039177 140025891108608 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.424604892730713, loss=0.785667896270752
I0130 11:32:49.033792 140026016913152 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.1037678718566895, loss=0.7168907523155212
I0130 11:33:22.950715 140025891108608 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.210501194000244, loss=0.6955685615539551
I0130 11:33:56.918807 140026016913152 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.285160541534424, loss=0.7285475134849548
I0130 11:34:30.933609 140025891108608 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.373396396636963, loss=0.6916754841804504
I0130 11:35:04.881594 140026016913152 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.391366481781006, loss=0.7075615525245667
I0130 11:35:12.832341 140187804313408 spec.py:321] Evaluating on the training split.
I0130 11:35:19.163579 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 11:35:27.929224 140187804313408 spec.py:349] Evaluating on the test split.
I0130 11:35:30.680076 140187804313408 submission_runner.py:408] Time since start: 57608.88s, 	Step: 163825, 	{'train/accuracy': 0.9412069320678711, 'train/loss': 0.20645396411418915, 'validation/accuracy': 0.7478599548339844, 'validation/loss': 1.0675770044326782, 'validation/num_examples': 50000, 'test/accuracy': 0.6202000379562378, 'test/loss': 1.8455793857574463, 'test/num_examples': 10000, 'score': 55630.45647478104, 'total_duration': 57608.87885069847, 'accumulated_submission_time': 55630.45647478104, 'accumulated_eval_time': 1967.5142793655396, 'accumulated_logging_time': 4.854479789733887}
I0130 11:35:30.727701 140026058876672 logging_writer.py:48] [163825] accumulated_eval_time=1967.514279, accumulated_logging_time=4.854480, accumulated_submission_time=55630.456475, global_step=163825, preemption_count=0, score=55630.456475, test/accuracy=0.620200, test/loss=1.845579, test/num_examples=10000, total_duration=57608.878851, train/accuracy=0.941207, train/loss=0.206454, validation/accuracy=0.747860, validation/loss=1.067577, validation/num_examples=50000
I0130 11:35:56.473623 140026067269376 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.228371620178223, loss=0.7059246301651001
I0130 11:36:30.365044 140026058876672 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.147658348083496, loss=0.6690192222595215
I0130 11:37:04.290345 140026067269376 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.380987167358398, loss=0.706241250038147
I0130 11:37:38.190575 140026058876672 logging_writer.py:48] [164200] global_step=164200, grad_norm=3.992744207382202, loss=0.7073162794113159
I0130 11:38:12.134494 140026067269376 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.575537204742432, loss=0.7597779631614685
I0130 11:38:46.028457 140026058876672 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.580239295959473, loss=0.7306525707244873
I0130 11:39:19.972523 140026067269376 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.339812755584717, loss=0.7023195028305054
I0130 11:39:53.925274 140026058876672 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.154926300048828, loss=0.7010742425918579
I0130 11:40:27.858777 140026067269376 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.412323951721191, loss=0.6732290983200073
I0130 11:41:01.892068 140026058876672 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.429029941558838, loss=0.722973108291626
I0130 11:41:35.849331 140026067269376 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.582183361053467, loss=0.7927812933921814
I0130 11:42:09.770952 140026058876672 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.3806538581848145, loss=0.708543062210083
I0130 11:42:43.714329 140026067269376 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.5846757888793945, loss=0.7550879716873169
I0130 11:43:17.672537 140026058876672 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.000744342803955, loss=0.5804426074028015
I0130 11:43:51.620144 140026067269376 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.742900371551514, loss=0.7861775755882263
I0130 11:44:00.919985 140187804313408 spec.py:321] Evaluating on the training split.
I0130 11:44:07.404398 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 11:44:16.218148 140187804313408 spec.py:349] Evaluating on the test split.
I0130 11:44:18.885503 140187804313408 submission_runner.py:408] Time since start: 58137.08s, 	Step: 165329, 	{'train/accuracy': 0.9415258169174194, 'train/loss': 0.20436236262321472, 'validation/accuracy': 0.7491599917411804, 'validation/loss': 1.068067193031311, 'validation/num_examples': 50000, 'test/accuracy': 0.6196000576019287, 'test/loss': 1.8581254482269287, 'test/num_examples': 10000, 'score': 56140.58478808403, 'total_duration': 58137.0842730999, 'accumulated_submission_time': 56140.58478808403, 'accumulated_eval_time': 1985.4797623157501, 'accumulated_logging_time': 4.911173105239868}
I0130 11:44:18.936438 140026025305856 logging_writer.py:48] [165329] accumulated_eval_time=1985.479762, accumulated_logging_time=4.911173, accumulated_submission_time=56140.584788, global_step=165329, preemption_count=0, score=56140.584788, test/accuracy=0.619600, test/loss=1.858125, test/num_examples=10000, total_duration=58137.084273, train/accuracy=0.941526, train/loss=0.204362, validation/accuracy=0.749160, validation/loss=1.068067, validation/num_examples=50000
I0130 11:44:43.357267 140026033698560 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.131954193115234, loss=0.6682511568069458
I0130 11:45:17.254239 140026025305856 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.467309951782227, loss=0.6813753247261047
I0130 11:45:51.231040 140026033698560 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.487898349761963, loss=0.7512912154197693
I0130 11:46:25.191620 140026025305856 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.262387752532959, loss=0.7109116911888123
I0130 11:46:59.109628 140026033698560 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.607316017150879, loss=0.6967166662216187
I0130 11:47:33.047704 140026025305856 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.383069038391113, loss=0.7298112511634827
I0130 11:48:07.075652 140026033698560 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.364511966705322, loss=0.7129532694816589
I0130 11:48:40.998648 140026025305856 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.12360143661499, loss=0.6204545497894287
I0130 11:49:14.978518 140026033698560 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.333133697509766, loss=0.6822454333305359
I0130 11:49:48.946301 140026025305856 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.8099236488342285, loss=0.7845649123191833
I0130 11:50:22.915743 140026033698560 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.6230058670043945, loss=0.7153184413909912
I0130 11:50:56.905187 140026025305856 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.596371173858643, loss=0.6978604197502136
I0130 11:51:30.825811 140026033698560 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.363752365112305, loss=0.7704868316650391
I0130 11:52:04.811583 140026025305856 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.230770111083984, loss=0.6987916231155396
I0130 11:52:38.769331 140026033698560 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.591547966003418, loss=0.7217708826065063
I0130 11:52:49.124331 140187804313408 spec.py:321] Evaluating on the training split.
I0130 11:52:55.421855 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 11:53:04.491984 140187804313408 spec.py:349] Evaluating on the test split.
I0130 11:53:07.162597 140187804313408 submission_runner.py:408] Time since start: 58665.36s, 	Step: 166832, 	{'train/accuracy': 0.9414859414100647, 'train/loss': 0.20662200450897217, 'validation/accuracy': 0.7490400075912476, 'validation/loss': 1.0683197975158691, 'validation/num_examples': 50000, 'test/accuracy': 0.6226000189781189, 'test/loss': 1.8363534212112427, 'test/num_examples': 10000, 'score': 56650.707070589066, 'total_duration': 58665.36136960983, 'accumulated_submission_time': 56650.707070589066, 'accumulated_eval_time': 2003.517992734909, 'accumulated_logging_time': 4.971322536468506}
I0130 11:53:07.209756 140025891108608 logging_writer.py:48] [166832] accumulated_eval_time=2003.517993, accumulated_logging_time=4.971323, accumulated_submission_time=56650.707071, global_step=166832, preemption_count=0, score=56650.707071, test/accuracy=0.622600, test/loss=1.836353, test/num_examples=10000, total_duration=58665.361370, train/accuracy=0.941486, train/loss=0.206622, validation/accuracy=0.749040, validation/loss=1.068320, validation/num_examples=50000
I0130 11:53:30.586309 140026050483968 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.466360569000244, loss=0.736466109752655
I0130 11:54:04.474684 140025891108608 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.240091323852539, loss=0.6986699104309082
I0130 11:54:38.510982 140026050483968 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.300500392913818, loss=0.6407466530799866
I0130 11:55:12.438844 140025891108608 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.276172161102295, loss=0.6622911691665649
I0130 11:55:46.392814 140026050483968 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.657865524291992, loss=0.6925188302993774
I0130 11:56:20.297012 140025891108608 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.467053413391113, loss=0.734268844127655
I0130 11:56:54.232510 140026050483968 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.677985191345215, loss=0.7536008954048157
I0130 11:57:28.193219 140025891108608 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.546933174133301, loss=0.6650292873382568
I0130 11:58:02.153102 140026050483968 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.726864814758301, loss=0.7592318654060364
I0130 11:58:36.107564 140025891108608 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.374034404754639, loss=0.6938923597335815
I0130 11:59:10.086369 140026050483968 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.109882831573486, loss=0.7571017742156982
I0130 11:59:44.046952 140025891108608 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.207937717437744, loss=0.6543752551078796
I0130 12:00:17.996882 140026050483968 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.801544666290283, loss=0.7787395715713501
I0130 12:00:52.050996 140025891108608 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.546691417694092, loss=0.6682902574539185
I0130 12:01:25.998149 140026050483968 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.729840278625488, loss=0.7719622850418091
I0130 12:01:37.347959 140187804313408 spec.py:321] Evaluating on the training split.
I0130 12:01:43.657232 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 12:01:52.350289 140187804313408 spec.py:349] Evaluating on the test split.
I0130 12:01:55.037239 140187804313408 submission_runner.py:408] Time since start: 59193.24s, 	Step: 168335, 	{'train/accuracy': 0.9429408311843872, 'train/loss': 0.1995099037885666, 'validation/accuracy': 0.7509599924087524, 'validation/loss': 1.0619680881500244, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.8424190282821655, 'test/num_examples': 10000, 'score': 57160.779010772705, 'total_duration': 59193.236011981964, 'accumulated_submission_time': 57160.779010772705, 'accumulated_eval_time': 2021.207232952118, 'accumulated_logging_time': 5.02895712852478}
I0130 12:01:55.084272 140026016913152 logging_writer.py:48] [168335] accumulated_eval_time=2021.207233, accumulated_logging_time=5.028957, accumulated_submission_time=57160.779011, global_step=168335, preemption_count=0, score=57160.779011, test/accuracy=0.626100, test/loss=1.842419, test/num_examples=10000, total_duration=59193.236012, train/accuracy=0.942941, train/loss=0.199510, validation/accuracy=0.750960, validation/loss=1.061968, validation/num_examples=50000
I0130 12:02:17.432192 140026025305856 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.80794620513916, loss=0.7007116079330444
I0130 12:02:51.338181 140026016913152 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.380363464355469, loss=0.6952865123748779
I0130 12:03:25.307363 140026025305856 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.692152976989746, loss=0.6955401301383972
I0130 12:03:59.270512 140026016913152 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.302470684051514, loss=0.7497127056121826
I0130 12:04:33.184294 140026025305856 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.75972318649292, loss=0.6883726119995117
I0130 12:05:07.155801 140026016913152 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.640013217926025, loss=0.7117714285850525
I0130 12:05:41.126884 140026025305856 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.244030475616455, loss=0.6935087442398071
I0130 12:06:15.048659 140026016913152 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.751038551330566, loss=0.7025766968727112
I0130 12:06:48.986035 140026025305856 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.128870487213135, loss=0.6093921661376953
I0130 12:07:22.945738 140026016913152 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.070508003234863, loss=0.6907280087471008
I0130 12:07:56.963016 140026025305856 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.188095569610596, loss=0.6605699062347412
I0130 12:08:30.921195 140026016913152 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.497845649719238, loss=0.7213768362998962
I0130 12:09:04.895010 140026025305856 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.377100944519043, loss=0.6431207656860352
I0130 12:09:38.855088 140026016913152 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.324048042297363, loss=0.7313083410263062
I0130 12:10:12.767190 140026025305856 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.571333885192871, loss=0.6937921047210693
I0130 12:10:25.152456 140187804313408 spec.py:321] Evaluating on the training split.
I0130 12:10:31.398298 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 12:10:40.008145 140187804313408 spec.py:349] Evaluating on the test split.
I0130 12:10:42.692228 140187804313408 submission_runner.py:408] Time since start: 59720.89s, 	Step: 169838, 	{'train/accuracy': 0.949238657951355, 'train/loss': 0.18085843324661255, 'validation/accuracy': 0.7513399720191956, 'validation/loss': 1.060373306274414, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.8413702249526978, 'test/num_examples': 10000, 'score': 57670.78219342232, 'total_duration': 59720.89051890373, 'accumulated_submission_time': 57670.78219342232, 'accumulated_eval_time': 2038.746482372284, 'accumulated_logging_time': 5.086165189743042}
I0130 12:10:42.742055 140025882715904 logging_writer.py:48] [169838] accumulated_eval_time=2038.746482, accumulated_logging_time=5.086165, accumulated_submission_time=57670.782193, global_step=169838, preemption_count=0, score=57670.782193, test/accuracy=0.624000, test/loss=1.841370, test/num_examples=10000, total_duration=59720.890519, train/accuracy=0.949239, train/loss=0.180858, validation/accuracy=0.751340, validation/loss=1.060373, validation/num_examples=50000
I0130 12:11:04.098994 140026042091264 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.107296943664551, loss=0.6317044496536255
I0130 12:11:37.959042 140025882715904 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.232758045196533, loss=0.7294427156448364
I0130 12:12:11.899968 140026042091264 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.140666484832764, loss=0.6398448944091797
I0130 12:12:45.814945 140025882715904 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.526887893676758, loss=0.608625054359436
I0130 12:13:19.753816 140026042091264 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.463752269744873, loss=0.6744718551635742
I0130 12:13:53.684193 140025882715904 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.167089939117432, loss=0.634661078453064
I0130 12:14:27.733084 140026042091264 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.835292339324951, loss=0.7351570725440979
I0130 12:15:01.650460 140025882715904 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.666426658630371, loss=0.6235671043395996
I0130 12:15:35.618947 140026042091264 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.408116340637207, loss=0.6979942321777344
I0130 12:16:09.588343 140025882715904 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.363412380218506, loss=0.6197899580001831
I0130 12:16:43.514309 140026042091264 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.3205437660217285, loss=0.630250096321106
I0130 12:17:17.439990 140025882715904 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.535398960113525, loss=0.661378026008606
I0130 12:17:51.394287 140026042091264 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.340378761291504, loss=0.6403166651725769
I0130 12:18:25.346437 140025882715904 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.619087219238281, loss=0.6601189970970154
I0130 12:18:59.279824 140026042091264 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.914887428283691, loss=0.7234287261962891
I0130 12:19:12.989647 140187804313408 spec.py:321] Evaluating on the training split.
I0130 12:19:19.421487 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 12:19:28.099952 140187804313408 spec.py:349] Evaluating on the test split.
I0130 12:19:30.820233 140187804313408 submission_runner.py:408] Time since start: 60249.02s, 	Step: 171342, 	{'train/accuracy': 0.9575095176696777, 'train/loss': 0.15615008771419525, 'validation/accuracy': 0.7531999945640564, 'validation/loss': 1.0549283027648926, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.8321973085403442, 'test/num_examples': 10000, 'score': 58180.96397686005, 'total_duration': 60249.019006729126, 'accumulated_submission_time': 58180.96397686005, 'accumulated_eval_time': 2056.577041864395, 'accumulated_logging_time': 5.145231485366821}
I0130 12:19:30.870904 140025882715904 logging_writer.py:48] [171342] accumulated_eval_time=2056.577042, accumulated_logging_time=5.145231, accumulated_submission_time=58180.963977, global_step=171342, preemption_count=0, score=58180.963977, test/accuracy=0.626400, test/loss=1.832197, test/num_examples=10000, total_duration=60249.019007, train/accuracy=0.957510, train/loss=0.156150, validation/accuracy=0.753200, validation/loss=1.054928, validation/num_examples=50000
I0130 12:19:50.877552 140025891108608 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.485450267791748, loss=0.6753836870193481
I0130 12:20:24.775700 140025882715904 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.644359111785889, loss=0.6342430114746094
I0130 12:20:58.780773 140025891108608 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.221564769744873, loss=0.6304251551628113
I0130 12:21:32.747015 140025882715904 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.635717868804932, loss=0.6557080149650574
I0130 12:22:06.690256 140025891108608 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.6592698097229, loss=0.5456376075744629
I0130 12:22:40.627864 140025882715904 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.339142322540283, loss=0.6511848568916321
I0130 12:23:14.572850 140025891108608 logging_writer.py:48] [172000] global_step=172000, grad_norm=5.009279251098633, loss=0.7638218998908997
I0130 12:23:48.536193 140025882715904 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.479156494140625, loss=0.6377371549606323
I0130 12:24:22.478768 140025891108608 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.142635345458984, loss=0.6270007491111755
I0130 12:24:56.416526 140025882715904 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.10659122467041, loss=0.6359602808952332
I0130 12:25:30.373167 140025891108608 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.537055015563965, loss=0.6714129447937012
I0130 12:26:04.320852 140025882715904 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.758276462554932, loss=0.6546058058738708
I0130 12:26:38.271449 140025891108608 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.687445163726807, loss=0.6077805757522583
I0130 12:27:12.323018 140025882715904 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.654620170593262, loss=0.6456941962242126
I0130 12:27:46.294184 140025891108608 logging_writer.py:48] [172800] global_step=172800, grad_norm=5.184676647186279, loss=0.7403703927993774
I0130 12:28:01.063713 140187804313408 spec.py:321] Evaluating on the training split.
I0130 12:28:07.543505 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 12:28:16.205409 140187804313408 spec.py:349] Evaluating on the test split.
I0130 12:28:18.835614 140187804313408 submission_runner.py:408] Time since start: 60777.03s, 	Step: 172845, 	{'train/accuracy': 0.9551578164100647, 'train/loss': 0.1624404788017273, 'validation/accuracy': 0.7537599802017212, 'validation/loss': 1.0578937530517578, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8417407274246216, 'test/num_examples': 10000, 'score': 58691.0938334465, 'total_duration': 60777.034376859665, 'accumulated_submission_time': 58691.0938334465, 'accumulated_eval_time': 2074.348899126053, 'accumulated_logging_time': 5.2050487995147705}
I0130 12:28:18.886247 140026050483968 logging_writer.py:48] [172845] accumulated_eval_time=2074.348899, accumulated_logging_time=5.205049, accumulated_submission_time=58691.093833, global_step=172845, preemption_count=0, score=58691.093833, test/accuracy=0.627400, test/loss=1.841741, test/num_examples=10000, total_duration=60777.034377, train/accuracy=0.955158, train/loss=0.162440, validation/accuracy=0.753760, validation/loss=1.057894, validation/num_examples=50000
I0130 12:28:37.883413 140026058876672 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.552740097045898, loss=0.6882666349411011
I0130 12:29:11.787900 140026050483968 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.701327323913574, loss=0.6156537532806396
I0130 12:29:45.723916 140026058876672 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.882460117340088, loss=0.6709367632865906
I0130 12:30:19.641115 140026050483968 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.430417537689209, loss=0.6554515361785889
I0130 12:30:53.600047 140026058876672 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.967024803161621, loss=0.6880109310150146
I0130 12:31:27.548311 140026050483968 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.447051525115967, loss=0.6508327126502991
I0130 12:32:01.478180 140026058876672 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.8876543045043945, loss=0.6900376081466675
I0130 12:32:35.417334 140026050483968 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.493315696716309, loss=0.6897181868553162
I0130 12:33:09.369215 140026058876672 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.690701961517334, loss=0.7100976705551147
I0130 12:33:43.305205 140026050483968 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.323721885681152, loss=0.6356932520866394
I0130 12:34:17.336547 140026058876672 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.658712387084961, loss=0.7133092880249023
I0130 12:34:51.280576 140026050483968 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.530672073364258, loss=0.6032925844192505
I0130 12:35:25.221441 140026058876672 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.749698162078857, loss=0.6849310398101807
I0130 12:35:59.165427 140026050483968 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.2847161293029785, loss=0.7119327783584595
I0130 12:36:33.123087 140026058876672 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.726811408996582, loss=0.6461691856384277
I0130 12:36:48.892307 140187804313408 spec.py:321] Evaluating on the training split.
I0130 12:36:55.891830 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 12:37:04.734135 140187804313408 spec.py:349] Evaluating on the test split.
I0130 12:37:07.407370 140187804313408 submission_runner.py:408] Time since start: 61305.61s, 	Step: 174348, 	{'train/accuracy': 0.9562938213348389, 'train/loss': 0.16008029878139496, 'validation/accuracy': 0.7548199892044067, 'validation/loss': 1.0536705255508423, 'validation/num_examples': 50000, 'test/accuracy': 0.6251000165939331, 'test/loss': 1.828043818473816, 'test/num_examples': 10000, 'score': 59201.03386545181, 'total_duration': 61305.606140851974, 'accumulated_submission_time': 59201.03386545181, 'accumulated_eval_time': 2092.863926887512, 'accumulated_logging_time': 5.264968156814575}
I0130 12:37:07.455705 140026016913152 logging_writer.py:48] [174348] accumulated_eval_time=2092.863927, accumulated_logging_time=5.264968, accumulated_submission_time=59201.033865, global_step=174348, preemption_count=0, score=59201.033865, test/accuracy=0.625100, test/loss=1.828044, test/num_examples=10000, total_duration=61305.606141, train/accuracy=0.956294, train/loss=0.160080, validation/accuracy=0.754820, validation/loss=1.053671, validation/num_examples=50000
I0130 12:37:25.415606 140026025305856 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.348964691162109, loss=0.5846433639526367
I0130 12:37:59.348261 140026016913152 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.4176506996154785, loss=0.6092145442962646
I0130 12:38:33.323047 140026025305856 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.317137718200684, loss=0.6291986703872681
I0130 12:39:07.280430 140026016913152 logging_writer.py:48] [174700] global_step=174700, grad_norm=3.9985127449035645, loss=0.5498360395431519
I0130 12:39:41.198247 140026025305856 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.269328594207764, loss=0.5626881718635559
I0130 12:40:15.169279 140026016913152 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.270788669586182, loss=0.6276395916938782
I0130 12:40:49.232027 140026025305856 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.453269004821777, loss=0.6490452289581299
I0130 12:41:23.201412 140026016913152 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.725427150726318, loss=0.7042542099952698
I0130 12:41:57.166610 140026025305856 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.570084571838379, loss=0.6073024272918701
I0130 12:42:31.137797 140026016913152 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.123055458068848, loss=0.5625777840614319
I0130 12:43:05.093322 140026025305856 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.433549880981445, loss=0.6360874176025391
I0130 12:43:39.074358 140026016913152 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.642962455749512, loss=0.6183580160140991
I0130 12:44:13.018297 140026025305856 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.231269836425781, loss=0.6547369956970215
I0130 12:44:46.999389 140026016913152 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.649458885192871, loss=0.6207783818244934
I0130 12:45:20.958614 140026025305856 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.513749122619629, loss=0.6532573699951172
I0130 12:45:37.424005 140187804313408 spec.py:321] Evaluating on the training split.
I0130 12:45:43.692185 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 12:45:52.750674 140187804313408 spec.py:349] Evaluating on the test split.
I0130 12:45:55.347874 140187804313408 submission_runner.py:408] Time since start: 61833.55s, 	Step: 175850, 	{'train/accuracy': 0.9552574753761292, 'train/loss': 0.16187043488025665, 'validation/accuracy': 0.753879964351654, 'validation/loss': 1.0522363185882568, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8285508155822754, 'test/num_examples': 10000, 'score': 59710.93507862091, 'total_duration': 61833.54664039612, 'accumulated_submission_time': 59710.93507862091, 'accumulated_eval_time': 2110.787750482559, 'accumulated_logging_time': 5.325360298156738}
I0130 12:45:55.396997 140026042091264 logging_writer.py:48] [175850] accumulated_eval_time=2110.787750, accumulated_logging_time=5.325360, accumulated_submission_time=59710.935079, global_step=175850, preemption_count=0, score=59710.935079, test/accuracy=0.628000, test/loss=1.828551, test/num_examples=10000, total_duration=61833.546640, train/accuracy=0.955257, train/loss=0.161870, validation/accuracy=0.753880, validation/loss=1.052236, validation/num_examples=50000
I0130 12:46:12.699892 140026050483968 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.619441986083984, loss=0.6164233684539795
I0130 12:46:46.627978 140026042091264 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.545316696166992, loss=0.659449577331543
I0130 12:47:20.655126 140026050483968 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.444076061248779, loss=0.6236532330513
I0130 12:47:54.620950 140026042091264 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.531540870666504, loss=0.6466150283813477
I0130 12:48:28.590086 140026050483968 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.814996242523193, loss=0.6077619791030884
I0130 12:49:02.505118 140026042091264 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.014827251434326, loss=0.5888925194740295
I0130 12:49:36.471465 140026050483968 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.43978214263916, loss=0.6048012375831604
I0130 12:50:10.417012 140026042091264 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.470873832702637, loss=0.6410275101661682
I0130 12:50:44.336252 140026050483968 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.547436237335205, loss=0.6259188652038574
I0130 12:51:18.295428 140026042091264 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.543537139892578, loss=0.6725542545318604
I0130 12:51:52.236042 140026050483968 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.475057125091553, loss=0.5780135989189148
I0130 12:52:26.190977 140026042091264 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.55480432510376, loss=0.6309146881103516
I0130 12:53:00.103358 140026050483968 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.886172771453857, loss=0.6393946409225464
I0130 12:53:34.059955 140026042091264 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.786263465881348, loss=0.7105021476745605
I0130 12:54:08.063186 140026050483968 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.942039489746094, loss=0.6039720773696899
I0130 12:54:25.525798 140187804313408 spec.py:321] Evaluating on the training split.
I0130 12:54:31.823556 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 12:54:40.691993 140187804313408 spec.py:349] Evaluating on the test split.
I0130 12:54:43.401475 140187804313408 submission_runner.py:408] Time since start: 62361.60s, 	Step: 177353, 	{'train/accuracy': 0.9591039419174194, 'train/loss': 0.15503987669944763, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0516177415847778, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.8207694292068481, 'test/num_examples': 10000, 'score': 60220.999058008194, 'total_duration': 62361.60022234917, 'accumulated_submission_time': 60220.999058008194, 'accumulated_eval_time': 2128.663361310959, 'accumulated_logging_time': 5.3837199211120605}
I0130 12:54:43.475579 140025891108608 logging_writer.py:48] [177353] accumulated_eval_time=2128.663361, accumulated_logging_time=5.383720, accumulated_submission_time=60220.999058, global_step=177353, preemption_count=0, score=60220.999058, test/accuracy=0.629400, test/loss=1.820769, test/num_examples=10000, total_duration=62361.600222, train/accuracy=0.959104, train/loss=0.155040, validation/accuracy=0.755580, validation/loss=1.051618, validation/num_examples=50000
I0130 12:54:59.745524 140026016913152 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.3879618644714355, loss=0.6328867673873901
I0130 12:55:33.664637 140025891108608 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.189380168914795, loss=0.6308016777038574
I0130 12:56:07.594415 140026016913152 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.836048126220703, loss=0.6308857798576355
I0130 12:56:41.558746 140025891108608 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.367558002471924, loss=0.5945249795913696
I0130 12:57:15.531646 140026016913152 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.666929244995117, loss=0.605498194694519
I0130 12:57:49.499385 140025891108608 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.71759557723999, loss=0.620887041091919
I0130 12:58:23.453927 140026016913152 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.656583786010742, loss=0.6920940279960632
I0130 12:58:57.393332 140025891108608 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.504641532897949, loss=0.6218072175979614
I0130 12:59:31.356753 140026016913152 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.9264116287231445, loss=0.689386785030365
I0130 13:00:05.310184 140025891108608 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.677552223205566, loss=0.680342435836792
I0130 13:00:39.350352 140026016913152 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.468532085418701, loss=0.6384385824203491
I0130 13:01:13.310505 140025891108608 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.294090270996094, loss=0.6727108955383301
I0130 13:01:47.284254 140026016913152 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.555825233459473, loss=0.6596450209617615
I0130 13:02:21.232679 140025891108608 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.414422988891602, loss=0.5799724459648132
I0130 13:02:55.156453 140026016913152 logging_writer.py:48] [178800] global_step=178800, grad_norm=5.153641223907471, loss=0.6195536851882935
I0130 13:03:13.634719 140187804313408 spec.py:321] Evaluating on the training split.
I0130 13:03:19.904848 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 13:03:28.506439 140187804313408 spec.py:349] Evaluating on the test split.
I0130 13:03:31.219288 140187804313408 submission_runner.py:408] Time since start: 62889.42s, 	Step: 178856, 	{'train/accuracy': 0.9585060477256775, 'train/loss': 0.1536491960287094, 'validation/accuracy': 0.7564199566841125, 'validation/loss': 1.0479017496109009, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.821165680885315, 'test/num_examples': 10000, 'score': 60731.09321784973, 'total_duration': 62889.418065071106, 'accumulated_submission_time': 60731.09321784973, 'accumulated_eval_time': 2146.2478954792023, 'accumulated_logging_time': 5.467687129974365}
I0130 13:03:31.271986 140026050483968 logging_writer.py:48] [178856] accumulated_eval_time=2146.247895, accumulated_logging_time=5.467687, accumulated_submission_time=60731.093218, global_step=178856, preemption_count=0, score=60731.093218, test/accuracy=0.629500, test/loss=1.821166, test/num_examples=10000, total_duration=62889.418065, train/accuracy=0.958506, train/loss=0.153649, validation/accuracy=0.756420, validation/loss=1.047902, validation/num_examples=50000
I0130 13:03:46.530911 140026058876672 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.6821746826171875, loss=0.6954527497291565
I0130 13:04:20.384059 140026050483968 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.542905330657959, loss=0.7154136300086975
I0130 13:04:54.318611 140026058876672 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.450821876525879, loss=0.6357672214508057
I0130 13:05:28.239121 140026050483968 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.732532501220703, loss=0.6338256001472473
I0130 13:06:02.176358 140026058876672 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.919622421264648, loss=0.6737886071205139
I0130 13:06:36.135921 140026050483968 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.296517848968506, loss=0.642188549041748
I0130 13:07:10.283979 140026058876672 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.910374164581299, loss=0.6859410405158997
I0130 13:07:44.181093 140026050483968 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.725435256958008, loss=0.6560375094413757
I0130 13:08:18.114539 140026058876672 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.27288818359375, loss=0.5952715873718262
I0130 13:08:52.098791 140026050483968 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.546712875366211, loss=0.6367809176445007
I0130 13:09:26.053747 140026058876672 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.338839530944824, loss=0.6471195816993713
I0130 13:09:59.967325 140026050483968 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.608270168304443, loss=0.6239129304885864
I0130 13:10:33.913560 140026058876672 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.203003406524658, loss=0.6391400098800659
I0130 13:11:07.846761 140026050483968 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.783381462097168, loss=0.658828854560852
I0130 13:11:41.772042 140026058876672 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.913914203643799, loss=0.5871587991714478
I0130 13:12:01.265502 140187804313408 spec.py:321] Evaluating on the training split.
I0130 13:12:07.758513 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 13:12:16.383006 140187804313408 spec.py:349] Evaluating on the test split.
I0130 13:12:19.060111 140187804313408 submission_runner.py:408] Time since start: 63417.26s, 	Step: 180359, 	{'train/accuracy': 0.9599609375, 'train/loss': 0.1484406441450119, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0471597909927368, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8269197940826416, 'test/num_examples': 10000, 'score': 61241.01958680153, 'total_duration': 63417.25886678696, 'accumulated_submission_time': 61241.01958680153, 'accumulated_eval_time': 2164.042452096939, 'accumulated_logging_time': 5.532188653945923}
I0130 13:12:19.115712 140025891108608 logging_writer.py:48] [180359] accumulated_eval_time=2164.042452, accumulated_logging_time=5.532189, accumulated_submission_time=61241.019587, global_step=180359, preemption_count=0, score=61241.019587, test/accuracy=0.631400, test/loss=1.826920, test/num_examples=10000, total_duration=63417.258867, train/accuracy=0.959961, train/loss=0.148441, validation/accuracy=0.755680, validation/loss=1.047160, validation/num_examples=50000
I0130 13:12:33.356390 140026016913152 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.5331830978393555, loss=0.6284253001213074
I0130 13:13:07.266830 140025891108608 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.574392318725586, loss=0.6736647486686707
I0130 13:13:41.332455 140026016913152 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.607141017913818, loss=0.6904040575027466
I0130 13:14:15.284943 140025891108608 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.41162633895874, loss=0.617440938949585
I0130 13:14:49.215944 140026016913152 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.317668437957764, loss=0.6093015074729919
I0130 13:15:23.166273 140025891108608 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.3219380378723145, loss=0.6309179663658142
I0130 13:15:57.144600 140026016913152 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.392659664154053, loss=0.5649346113204956
I0130 13:16:31.085615 140025891108608 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.567825794219971, loss=0.649027943611145
I0130 13:17:05.048702 140026016913152 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.6178741455078125, loss=0.6220678091049194
I0130 13:17:39.007160 140025891108608 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.87615966796875, loss=0.6489298939704895
I0130 13:18:12.982412 140026016913152 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.492257118225098, loss=0.6365363001823425
I0130 13:18:46.903897 140025891108608 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.799635887145996, loss=0.6697242856025696
I0130 13:19:20.852688 140026016913152 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.6046552658081055, loss=0.6765819787979126
I0130 13:19:54.813882 140025891108608 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.693966865539551, loss=0.6113370656967163
I0130 13:20:28.854733 140026016913152 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.476284503936768, loss=0.5916349291801453
I0130 13:20:49.370165 140187804313408 spec.py:321] Evaluating on the training split.
I0130 13:20:55.707339 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 13:21:04.470942 140187804313408 spec.py:349] Evaluating on the test split.
I0130 13:21:07.159806 140187804313408 submission_runner.py:408] Time since start: 63945.36s, 	Step: 181862, 	{'train/accuracy': 0.961355984210968, 'train/loss': 0.14692814648151398, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.046109676361084, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8241914510726929, 'test/num_examples': 10000, 'score': 61751.206904411316, 'total_duration': 63945.358550071716, 'accumulated_submission_time': 61751.206904411316, 'accumulated_eval_time': 2181.832026720047, 'accumulated_logging_time': 5.597205638885498}
I0130 13:21:07.228145 140025891108608 logging_writer.py:48] [181862] accumulated_eval_time=2181.832027, accumulated_logging_time=5.597206, accumulated_submission_time=61751.206904, global_step=181862, preemption_count=0, score=61751.206904, test/accuracy=0.630300, test/loss=1.824191, test/num_examples=10000, total_duration=63945.358550, train/accuracy=0.961356, train/loss=0.146928, validation/accuracy=0.755780, validation/loss=1.046110, validation/num_examples=50000
I0130 13:21:20.448870 140026042091264 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.584424018859863, loss=0.6299231648445129
I0130 13:21:54.311244 140025891108608 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.8311662673950195, loss=0.6332154273986816
I0130 13:22:28.257865 140026042091264 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.395991802215576, loss=0.6402652859687805
I0130 13:23:02.153910 140025891108608 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.832477569580078, loss=0.6912075877189636
I0130 13:23:36.113490 140026042091264 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.217496871948242, loss=0.5960395336151123
I0130 13:24:10.050477 140025891108608 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.872061729431152, loss=0.6364648342132568
I0130 13:24:44.008592 140026042091264 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.339905261993408, loss=0.5842087268829346
I0130 13:25:17.982687 140025891108608 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.795414924621582, loss=0.6497725248336792
I0130 13:25:51.931911 140026042091264 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.4339752197265625, loss=0.6121317744255066
I0130 13:26:25.836538 140025891108608 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.631410598754883, loss=0.7081211805343628
I0130 13:26:59.845163 140026042091264 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.3963623046875, loss=0.638222336769104
I0130 13:27:33.805853 140025891108608 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.35211181640625, loss=0.6903859376907349
I0130 13:28:07.743382 140026042091264 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.629440784454346, loss=0.6452673673629761
I0130 13:28:41.666994 140025891108608 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.957558631896973, loss=0.6680967211723328
I0130 13:29:15.613660 140026042091264 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.520843982696533, loss=0.6056002974510193
I0130 13:29:37.487108 140187804313408 spec.py:321] Evaluating on the training split.
I0130 13:29:43.776626 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 13:29:52.392824 140187804313408 spec.py:349] Evaluating on the test split.
I0130 13:29:55.063242 140187804313408 submission_runner.py:408] Time since start: 64473.26s, 	Step: 183366, 	{'train/accuracy': 0.9613958597183228, 'train/loss': 0.1471957266330719, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0465097427368164, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8238227367401123, 'test/num_examples': 10000, 'score': 62261.4028301239, 'total_duration': 64473.26201725006, 'accumulated_submission_time': 62261.4028301239, 'accumulated_eval_time': 2199.4081242084503, 'accumulated_logging_time': 5.6746907234191895}
I0130 13:29:55.114922 140026033698560 logging_writer.py:48] [183366] accumulated_eval_time=2199.408124, accumulated_logging_time=5.674691, accumulated_submission_time=62261.402830, global_step=183366, preemption_count=0, score=62261.402830, test/accuracy=0.631200, test/loss=1.823823, test/num_examples=10000, total_duration=64473.262017, train/accuracy=0.961396, train/loss=0.147196, validation/accuracy=0.755720, validation/loss=1.046510, validation/num_examples=50000
I0130 13:30:07.022143 140026050483968 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.083427429199219, loss=0.5756307244300842
I0130 13:30:40.908326 140026033698560 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.46860933303833, loss=0.6746070384979248
I0130 13:31:14.869253 140026050483968 logging_writer.py:48] [183600] global_step=183600, grad_norm=5.062007427215576, loss=0.6302974820137024
I0130 13:31:48.829348 140026033698560 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.604502201080322, loss=0.6916400194168091
I0130 13:32:22.763913 140026050483968 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.632259845733643, loss=0.6569226980209351
I0130 13:32:56.710314 140026033698560 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.675717353820801, loss=0.7317283153533936
I0130 13:33:30.883334 140026050483968 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.948824405670166, loss=0.6313909888267517
I0130 13:34:04.803941 140026033698560 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.204619407653809, loss=0.5339694619178772
I0130 13:34:38.764726 140026050483968 logging_writer.py:48] [184200] global_step=184200, grad_norm=5.062843322753906, loss=0.6720148921012878
I0130 13:35:12.744277 140026033698560 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.611778736114502, loss=0.6599946618080139
I0130 13:35:46.682880 140026050483968 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.539718151092529, loss=0.592192530632019
I0130 13:36:20.632901 140026033698560 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.480991840362549, loss=0.5916741490364075
I0130 13:36:54.588687 140026050483968 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.11292839050293, loss=0.6133835315704346
I0130 13:37:28.551743 140026033698560 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.394138813018799, loss=0.629272997379303
I0130 13:38:02.515188 140026050483968 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.33481502532959, loss=0.6084906458854675
I0130 13:38:25.067289 140187804313408 spec.py:321] Evaluating on the training split.
I0130 13:38:31.326637 140187804313408 spec.py:333] Evaluating on the validation split.
I0130 13:38:40.113166 140187804313408 spec.py:349] Evaluating on the test split.
I0130 13:38:42.815844 140187804313408 submission_runner.py:408] Time since start: 65001.01s, 	Step: 184868, 	{'train/accuracy': 0.959980845451355, 'train/loss': 0.14590847492218018, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0453925132751465, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8226069211959839, 'test/num_examples': 10000, 'score': 62771.28944039345, 'total_duration': 65001.01459479332, 'accumulated_submission_time': 62771.28944039345, 'accumulated_eval_time': 2217.1566207408905, 'accumulated_logging_time': 5.737559795379639}
I0130 13:38:42.872451 140026025305856 logging_writer.py:48] [184868] accumulated_eval_time=2217.156621, accumulated_logging_time=5.737560, accumulated_submission_time=62771.289440, global_step=184868, preemption_count=0, score=62771.289440, test/accuracy=0.631200, test/loss=1.822607, test/num_examples=10000, total_duration=65001.014595, train/accuracy=0.959981, train/loss=0.145908, validation/accuracy=0.756200, validation/loss=1.045393, validation/num_examples=50000
I0130 13:38:54.052799 140026042091264 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.548070430755615, loss=0.6527472734451294
I0130 13:39:27.974135 140026025305856 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.5830206871032715, loss=0.6179855465888977
I0130 13:40:01.975312 140026042091264 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.63563346862793, loss=0.5944182872772217
I0130 13:40:35.944424 140026025305856 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.430079936981201, loss=0.5575675964355469
I0130 13:41:09.905904 140026042091264 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.5743913650512695, loss=0.6602115631103516
I0130 13:41:43.819327 140026025305856 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.498904228210449, loss=0.605371356010437
I0130 13:42:17.796781 140026042091264 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.6476945877075195, loss=0.6361095905303955
I0130 13:42:39.703105 140026025305856 logging_writer.py:48] [185566] global_step=185566, preemption_count=0, score=63008.051584
I0130 13:42:40.132827 140187804313408 checkpoints.py:490] Saving checkpoint at step: 185566
I0130 13:42:41.225010 140187804313408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_5/checkpoint_185566
I0130 13:42:41.249472 140187804313408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_resnet_jax/trial_5/checkpoint_185566.
I0130 13:42:41.996590 140187804313408 submission_runner.py:583] Tuning trial 5/5
I0130 13:42:41.996796 140187804313408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0130 13:42:42.008173 140187804313408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010363520123064518, 'train/loss': 6.912950038909912, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913174629211426, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.9125494956970215, 'test/num_examples': 10000, 'score': 32.45261359214783, 'total_duration': 50.37277865409851, 'accumulated_submission_time': 32.45261359214783, 'accumulated_eval_time': 17.920067310333252, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1497, {'train/accuracy': 0.16388313472270966, 'train/loss': 4.294137477874756, 'validation/accuracy': 0.14711999893188477, 'validation/loss': 4.409202575683594, 'validation/num_examples': 50000, 'test/accuracy': 0.10910000652074814, 'test/loss': 4.887209415435791, 'test/num_examples': 10000, 'score': 542.546288728714, 'total_duration': 578.2290046215057, 'accumulated_submission_time': 542.546288728714, 'accumulated_eval_time': 35.60792398452759, 'accumulated_logging_time': 0.02004528045654297, 'global_step': 1497, 'preemption_count': 0}), (2994, {'train/accuracy': 0.3364756107330322, 'train/loss': 3.0776820182800293, 'validation/accuracy': 0.310839980840683, 'validation/loss': 3.213662624359131, 'validation/num_examples': 50000, 'test/accuracy': 0.24000000953674316, 'test/loss': 3.8412790298461914, 'test/num_examples': 10000, 'score': 1052.6925213336945, 'total_duration': 1106.3321468830109, 'accumulated_submission_time': 1052.6925213336945, 'accumulated_eval_time': 53.482457876205444, 'accumulated_logging_time': 0.04800295829772949, 'global_step': 2994, 'preemption_count': 0}), (4492, {'train/accuracy': 0.5123365521430969, 'train/loss': 2.0910186767578125, 'validation/accuracy': 0.4371799826622009, 'validation/loss': 2.503953695297241, 'validation/num_examples': 50000, 'test/accuracy': 0.33550000190734863, 'test/loss': 3.185488224029541, 'test/num_examples': 10000, 'score': 1562.6216881275177, 'total_duration': 1634.1957335472107, 'accumulated_submission_time': 1562.6216881275177, 'accumulated_eval_time': 71.33141088485718, 'accumulated_logging_time': 0.07839679718017578, 'global_step': 4492, 'preemption_count': 0}), (5992, {'train/accuracy': 0.5540696382522583, 'train/loss': 1.8846989870071411, 'validation/accuracy': 0.5013599991798401, 'validation/loss': 2.165916919708252, 'validation/num_examples': 50000, 'test/accuracy': 0.38910001516342163, 'test/loss': 2.906574010848999, 'test/num_examples': 10000, 'score': 2072.839183807373, 'total_duration': 2162.2559444904327, 'accumulated_submission_time': 2072.839183807373, 'accumulated_eval_time': 89.09158182144165, 'accumulated_logging_time': 0.10589408874511719, 'global_step': 5992, 'preemption_count': 0}), (7492, {'train/accuracy': 0.5963010191917419, 'train/loss': 1.6671967506408691, 'validation/accuracy': 0.5468400120735168, 'validation/loss': 1.9419997930526733, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.6873884201049805, 'test/num_examples': 10000, 'score': 2582.7628107070923, 'total_duration': 2690.230018377304, 'accumulated_submission_time': 2582.7628107070923, 'accumulated_eval_time': 107.05718922615051, 'accumulated_logging_time': 0.13532543182373047, 'global_step': 7492, 'preemption_count': 0}), (8993, {'train/accuracy': 0.5882493257522583, 'train/loss': 1.7096151113510132, 'validation/accuracy': 0.5422999858856201, 'validation/loss': 1.9613946676254272, 'validation/num_examples': 50000, 'test/accuracy': 0.4240000247955322, 'test/loss': 2.697845935821533, 'test/num_examples': 10000, 'score': 3092.9980075359344, 'total_duration': 3218.3233294487, 'accumulated_submission_time': 3092.9980075359344, 'accumulated_eval_time': 124.83120155334473, 'accumulated_logging_time': 0.16447710990905762, 'global_step': 8993, 'preemption_count': 0}), (10494, {'train/accuracy': 0.6168686151504517, 'train/loss': 1.563010334968567, 'validation/accuracy': 0.5707399845123291, 'validation/loss': 1.820184588432312, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.5981123447418213, 'test/num_examples': 10000, 'score': 3603.048814535141, 'total_duration': 3746.226803779602, 'accumulated_submission_time': 3603.048814535141, 'accumulated_eval_time': 142.59801578521729, 'accumulated_logging_time': 0.19438743591308594, 'global_step': 10494, 'preemption_count': 0}), (11995, {'train/accuracy': 0.6272122263908386, 'train/loss': 1.5147466659545898, 'validation/accuracy': 0.5814799666404724, 'validation/loss': 1.769814372062683, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.5150883197784424, 'test/num_examples': 10000, 'score': 4112.968335390091, 'total_duration': 4274.098203659058, 'accumulated_submission_time': 4112.968335390091, 'accumulated_eval_time': 160.46470522880554, 'accumulated_logging_time': 0.22560787200927734, 'global_step': 11995, 'preemption_count': 0}), (13496, {'train/accuracy': 0.6594387888908386, 'train/loss': 1.3796418905258179, 'validation/accuracy': 0.5920199751853943, 'validation/loss': 1.711396336555481, 'validation/num_examples': 50000, 'test/accuracy': 0.46400001645088196, 'test/loss': 2.4521992206573486, 'test/num_examples': 10000, 'score': 4623.030744314194, 'total_duration': 4802.1791207790375, 'accumulated_submission_time': 4623.030744314194, 'accumulated_eval_time': 178.39834880828857, 'accumulated_logging_time': 0.25389599800109863, 'global_step': 13496, 'preemption_count': 0}), (14997, {'train/accuracy': 0.6680684089660645, 'train/loss': 1.324404001235962, 'validation/accuracy': 0.5925399661064148, 'validation/loss': 1.721736192703247, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.463068723678589, 'test/num_examples': 10000, 'score': 5133.006846666336, 'total_duration': 5330.253460884094, 'accumulated_submission_time': 5133.006846666336, 'accumulated_eval_time': 196.40883922576904, 'accumulated_logging_time': 0.28588294982910156, 'global_step': 14997, 'preemption_count': 0}), (16499, {'train/accuracy': 0.6616111397743225, 'train/loss': 1.3371883630752563, 'validation/accuracy': 0.6007199883460999, 'validation/loss': 1.6780246496200562, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.453063726425171, 'test/num_examples': 10000, 'score': 5643.204411029816, 'total_duration': 5858.414639472961, 'accumulated_submission_time': 5643.204411029816, 'accumulated_eval_time': 214.28442406654358, 'accumulated_logging_time': 0.31774473190307617, 'global_step': 16499, 'preemption_count': 0}), (18001, {'train/accuracy': 0.6668726205825806, 'train/loss': 1.3255809545516968, 'validation/accuracy': 0.6147199869155884, 'validation/loss': 1.603179693222046, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3434627056121826, 'test/num_examples': 10000, 'score': 6153.446529865265, 'total_duration': 6386.974694013596, 'accumulated_submission_time': 6153.446529865265, 'accumulated_eval_time': 232.5098798274994, 'accumulated_logging_time': 0.35366272926330566, 'global_step': 18001, 'preemption_count': 0}), (19503, {'train/accuracy': 0.6614915132522583, 'train/loss': 1.3690069913864136, 'validation/accuracy': 0.6030600070953369, 'validation/loss': 1.656613826751709, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.4080591201782227, 'test/num_examples': 10000, 'score': 6663.575809955597, 'total_duration': 6915.123049736023, 'accumulated_submission_time': 6663.575809955597, 'accumulated_eval_time': 250.44188237190247, 'accumulated_logging_time': 0.3847367763519287, 'global_step': 19503, 'preemption_count': 0}), (21006, {'train/accuracy': 0.6614915132522583, 'train/loss': 1.3474102020263672, 'validation/accuracy': 0.6093999743461609, 'validation/loss': 1.626532793045044, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.356985330581665, 'test/num_examples': 10000, 'score': 7173.793117523193, 'total_duration': 7443.621410608292, 'accumulated_submission_time': 7173.793117523193, 'accumulated_eval_time': 268.6344575881958, 'accumulated_logging_time': 0.41891908645629883, 'global_step': 21006, 'preemption_count': 0}), (22508, {'train/accuracy': 0.6671914458274841, 'train/loss': 1.326819896697998, 'validation/accuracy': 0.6113399863243103, 'validation/loss': 1.6232869625091553, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.3702409267425537, 'test/num_examples': 10000, 'score': 7683.747024536133, 'total_duration': 7971.509313106537, 'accumulated_submission_time': 7683.747024536133, 'accumulated_eval_time': 286.4828236103058, 'accumulated_logging_time': 0.4501662254333496, 'global_step': 22508, 'preemption_count': 0}), (24011, {'train/accuracy': 0.6983218789100647, 'train/loss': 1.184372901916504, 'validation/accuracy': 0.6136800050735474, 'validation/loss': 1.602954387664795, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.310030937194824, 'test/num_examples': 10000, 'score': 8193.990616083145, 'total_duration': 8499.679381370544, 'accumulated_submission_time': 8193.990616083145, 'accumulated_eval_time': 304.32325291633606, 'accumulated_logging_time': 0.48122096061706543, 'global_step': 24011, 'preemption_count': 0}), (25513, {'train/accuracy': 0.6822385191917419, 'train/loss': 1.2491592168807983, 'validation/accuracy': 0.6146399974822998, 'validation/loss': 1.6218963861465454, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.3448243141174316, 'test/num_examples': 10000, 'score': 8703.985595703125, 'total_duration': 9027.862311124802, 'accumulated_submission_time': 8703.985595703125, 'accumulated_eval_time': 322.419335603714, 'accumulated_logging_time': 0.5180819034576416, 'global_step': 25513, 'preemption_count': 0}), (27016, {'train/accuracy': 0.6840720772743225, 'train/loss': 1.2421938180923462, 'validation/accuracy': 0.6232399940490723, 'validation/loss': 1.559043526649475, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.2529296875, 'test/num_examples': 10000, 'score': 9214.018256664276, 'total_duration': 9555.59879374504, 'accumulated_submission_time': 9214.018256664276, 'accumulated_eval_time': 340.03698801994324, 'accumulated_logging_time': 0.5498857498168945, 'global_step': 27016, 'preemption_count': 0}), (28518, {'train/accuracy': 0.6774553656578064, 'train/loss': 1.2737376689910889, 'validation/accuracy': 0.6197400093078613, 'validation/loss': 1.5842479467391968, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.3093740940093994, 'test/num_examples': 10000, 'score': 9724.112269163132, 'total_duration': 10083.73843884468, 'accumulated_submission_time': 9724.112269163132, 'accumulated_eval_time': 357.99346256256104, 'accumulated_logging_time': 0.5839834213256836, 'global_step': 28518, 'preemption_count': 0}), (30021, {'train/accuracy': 0.6818000674247742, 'train/loss': 1.2576472759246826, 'validation/accuracy': 0.6284199953079224, 'validation/loss': 1.5533267259597778, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.2645206451416016, 'test/num_examples': 10000, 'score': 10234.21173286438, 'total_duration': 10612.092364549637, 'accumulated_submission_time': 10234.21173286438, 'accumulated_eval_time': 376.1608896255493, 'accumulated_logging_time': 0.6163861751556396, 'global_step': 30021, 'preemption_count': 0}), (31523, {'train/accuracy': 0.6839724183082581, 'train/loss': 1.2390429973602295, 'validation/accuracy': 0.6254400014877319, 'validation/loss': 1.5558955669403076, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.269998788833618, 'test/num_examples': 10000, 'score': 10744.19499206543, 'total_duration': 11140.10380768776, 'accumulated_submission_time': 10744.19499206543, 'accumulated_eval_time': 394.0955708026886, 'accumulated_logging_time': 0.6547572612762451, 'global_step': 31523, 'preemption_count': 0}), (33025, {'train/accuracy': 0.7113161683082581, 'train/loss': 1.1245791912078857, 'validation/accuracy': 0.6225999593734741, 'validation/loss': 1.5649104118347168, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.3233165740966797, 'test/num_examples': 10000, 'score': 11254.327833890915, 'total_duration': 11667.965344667435, 'accumulated_submission_time': 11254.327833890915, 'accumulated_eval_time': 411.72698998451233, 'accumulated_logging_time': 0.6967041492462158, 'global_step': 33025, 'preemption_count': 0}), (34527, {'train/accuracy': 0.6932995915412903, 'train/loss': 1.201788306236267, 'validation/accuracy': 0.624459981918335, 'validation/loss': 1.5534310340881348, 'validation/num_examples': 50000, 'test/accuracy': 0.4936000108718872, 'test/loss': 2.312537670135498, 'test/num_examples': 10000, 'score': 11764.347550868988, 'total_duration': 12196.06828379631, 'accumulated_submission_time': 11764.347550868988, 'accumulated_eval_time': 429.72024512290955, 'accumulated_logging_time': 0.731212854385376, 'global_step': 34527, 'preemption_count': 0}), (36030, {'train/accuracy': 0.6974449753761292, 'train/loss': 1.1905690431594849, 'validation/accuracy': 0.6278799772262573, 'validation/loss': 1.5417182445526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.2813334465026855, 'test/num_examples': 10000, 'score': 12274.329077005386, 'total_duration': 12723.898445367813, 'accumulated_submission_time': 12274.329077005386, 'accumulated_eval_time': 447.4782257080078, 'accumulated_logging_time': 0.7672967910766602, 'global_step': 36030, 'preemption_count': 0}), (37533, {'train/accuracy': 0.7012914419174194, 'train/loss': 1.1706738471984863, 'validation/accuracy': 0.6375399827957153, 'validation/loss': 1.4896409511566162, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.2257673740386963, 'test/num_examples': 10000, 'score': 12784.488025665283, 'total_duration': 13252.32969903946, 'accumulated_submission_time': 12784.488025665283, 'accumulated_eval_time': 465.655154466629, 'accumulated_logging_time': 0.8066210746765137, 'global_step': 37533, 'preemption_count': 0}), (39037, {'train/accuracy': 0.6754822731018066, 'train/loss': 1.2862061262130737, 'validation/accuracy': 0.6221799850463867, 'validation/loss': 1.5718142986297607, 'validation/num_examples': 50000, 'test/accuracy': 0.49310001730918884, 'test/loss': 2.3207204341888428, 'test/num_examples': 10000, 'score': 13294.657634973526, 'total_duration': 13780.520778179169, 'accumulated_submission_time': 13294.657634973526, 'accumulated_eval_time': 483.5669913291931, 'accumulated_logging_time': 0.8619179725646973, 'global_step': 39037, 'preemption_count': 0}), (40539, {'train/accuracy': 0.7030452489852905, 'train/loss': 1.1629226207733154, 'validation/accuracy': 0.6412999629974365, 'validation/loss': 1.4571741819381714, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.200263738632202, 'test/num_examples': 10000, 'score': 13804.57847905159, 'total_duration': 14308.516256809235, 'accumulated_submission_time': 13804.57847905159, 'accumulated_eval_time': 501.5477590560913, 'accumulated_logging_time': 0.9012980461120605, 'global_step': 40539, 'preemption_count': 0}), (42041, {'train/accuracy': 0.7258250713348389, 'train/loss': 1.0648127794265747, 'validation/accuracy': 0.6235199570655823, 'validation/loss': 1.5412627458572388, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.3147168159484863, 'test/num_examples': 10000, 'score': 14314.496087789536, 'total_duration': 14836.219805955887, 'accumulated_submission_time': 14314.496087789536, 'accumulated_eval_time': 519.2402155399323, 'accumulated_logging_time': 0.9375874996185303, 'global_step': 42041, 'preemption_count': 0}), (43544, {'train/accuracy': 0.7102598547935486, 'train/loss': 1.1271930932998657, 'validation/accuracy': 0.6373999714851379, 'validation/loss': 1.4905970096588135, 'validation/num_examples': 50000, 'test/accuracy': 0.5039000511169434, 'test/loss': 2.2426929473876953, 'test/num_examples': 10000, 'score': 14824.710835456848, 'total_duration': 15364.400060892105, 'accumulated_submission_time': 14824.710835456848, 'accumulated_eval_time': 537.1130058765411, 'accumulated_logging_time': 0.9754059314727783, 'global_step': 43544, 'preemption_count': 0}), (45048, {'train/accuracy': 0.7119140625, 'train/loss': 1.117753505706787, 'validation/accuracy': 0.6432799696922302, 'validation/loss': 1.4690630435943604, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.19850754737854, 'test/num_examples': 10000, 'score': 15334.921788215637, 'total_duration': 15892.65364933014, 'accumulated_submission_time': 15334.921788215637, 'accumulated_eval_time': 555.0589742660522, 'accumulated_logging_time': 1.0168430805206299, 'global_step': 45048, 'preemption_count': 0}), (46549, {'train/accuracy': 0.6991389989852905, 'train/loss': 1.1732345819473267, 'validation/accuracy': 0.6344599723815918, 'validation/loss': 1.5220282077789307, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2613413333892822, 'test/num_examples': 10000, 'score': 15844.842143058777, 'total_duration': 16420.54642868042, 'accumulated_submission_time': 15844.842143058777, 'accumulated_eval_time': 572.9238193035126, 'accumulated_logging_time': 1.068709373474121, 'global_step': 46549, 'preemption_count': 0}), (48053, {'train/accuracy': 0.7051379084587097, 'train/loss': 1.1431678533554077, 'validation/accuracy': 0.6401799917221069, 'validation/loss': 1.4874356985092163, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.180363893508911, 'test/num_examples': 10000, 'score': 16355.0407371521, 'total_duration': 16948.419927835464, 'accumulated_submission_time': 16355.0407371521, 'accumulated_eval_time': 590.5062322616577, 'accumulated_logging_time': 1.105797290802002, 'global_step': 48053, 'preemption_count': 0}), (49556, {'train/accuracy': 0.6994977593421936, 'train/loss': 1.168281078338623, 'validation/accuracy': 0.638759970664978, 'validation/loss': 1.4861197471618652, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2292864322662354, 'test/num_examples': 10000, 'score': 16865.1335606575, 'total_duration': 17476.397280454636, 'accumulated_submission_time': 16865.1335606575, 'accumulated_eval_time': 608.299302816391, 'accumulated_logging_time': 1.1421661376953125, 'global_step': 49556, 'preemption_count': 0}), (51060, {'train/accuracy': 0.7434629797935486, 'train/loss': 0.9698396325111389, 'validation/accuracy': 0.6350199580192566, 'validation/loss': 1.5050801038742065, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.2390754222869873, 'test/num_examples': 10000, 'score': 17375.330893039703, 'total_duration': 18004.835172891617, 'accumulated_submission_time': 17375.330893039703, 'accumulated_eval_time': 626.4494802951813, 'accumulated_logging_time': 1.178267478942871, 'global_step': 51060, 'preemption_count': 0}), (52563, {'train/accuracy': 0.7270607352256775, 'train/loss': 1.0618703365325928, 'validation/accuracy': 0.6416199803352356, 'validation/loss': 1.4836870431900024, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.173795223236084, 'test/num_examples': 10000, 'score': 17885.269745588303, 'total_duration': 18532.783682346344, 'accumulated_submission_time': 17885.269745588303, 'accumulated_eval_time': 644.3647968769073, 'accumulated_logging_time': 1.2180454730987549, 'global_step': 52563, 'preemption_count': 0}), (54067, {'train/accuracy': 0.70316481590271, 'train/loss': 1.1438255310058594, 'validation/accuracy': 0.6356399655342102, 'validation/loss': 1.5182785987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.5090000033378601, 'test/loss': 2.266648769378662, 'test/num_examples': 10000, 'score': 18395.33567595482, 'total_duration': 19060.81605863571, 'accumulated_submission_time': 18395.33567595482, 'accumulated_eval_time': 662.2336344718933, 'accumulated_logging_time': 1.2604291439056396, 'global_step': 54067, 'preemption_count': 0}), (55570, {'train/accuracy': 0.7121531963348389, 'train/loss': 1.1044635772705078, 'validation/accuracy': 0.6428200006484985, 'validation/loss': 1.4710193872451782, 'validation/num_examples': 50000, 'test/accuracy': 0.5169000029563904, 'test/loss': 2.2187652587890625, 'test/num_examples': 10000, 'score': 18905.46354651451, 'total_duration': 19589.064910411835, 'accumulated_submission_time': 18905.46354651451, 'accumulated_eval_time': 680.2520830631256, 'accumulated_logging_time': 1.3046739101409912, 'global_step': 55570, 'preemption_count': 0}), (57074, {'train/accuracy': 0.7025071382522583, 'train/loss': 1.1650789976119995, 'validation/accuracy': 0.6407999992370605, 'validation/loss': 1.4790819883346558, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.253450393676758, 'test/num_examples': 10000, 'score': 19415.575752735138, 'total_duration': 20116.90283894539, 'accumulated_submission_time': 19415.575752735138, 'accumulated_eval_time': 697.8852643966675, 'accumulated_logging_time': 1.3421645164489746, 'global_step': 57074, 'preemption_count': 0}), (58577, {'train/accuracy': 0.7065728306770325, 'train/loss': 1.1429468393325806, 'validation/accuracy': 0.6433799862861633, 'validation/loss': 1.4730268716812134, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.2345850467681885, 'test/num_examples': 10000, 'score': 19925.48170900345, 'total_duration': 20645.44976592064, 'accumulated_submission_time': 19925.48170900345, 'accumulated_eval_time': 716.4295771121979, 'accumulated_logging_time': 1.383310317993164, 'global_step': 58577, 'preemption_count': 0}), (60081, {'train/accuracy': 0.7592673897743225, 'train/loss': 0.929104745388031, 'validation/accuracy': 0.658079981803894, 'validation/loss': 1.402864933013916, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.1215386390686035, 'test/num_examples': 10000, 'score': 20435.709728956223, 'total_duration': 21173.62443089485, 'accumulated_submission_time': 20435.709728956223, 'accumulated_eval_time': 734.2747831344604, 'accumulated_logging_time': 1.4285414218902588, 'global_step': 60081, 'preemption_count': 0}), (61584, {'train/accuracy': 0.73636794090271, 'train/loss': 1.0095868110656738, 'validation/accuracy': 0.6518799662590027, 'validation/loss': 1.4322763681411743, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.153595447540283, 'test/num_examples': 10000, 'score': 20945.643161058426, 'total_duration': 21701.610381364822, 'accumulated_submission_time': 20945.643161058426, 'accumulated_eval_time': 752.2365992069244, 'accumulated_logging_time': 1.4622957706451416, 'global_step': 61584, 'preemption_count': 0}), (63088, {'train/accuracy': 0.7388990521430969, 'train/loss': 0.9919620156288147, 'validation/accuracy': 0.6604399681091309, 'validation/loss': 1.3843040466308594, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.1002607345581055, 'test/num_examples': 10000, 'score': 21455.754417657852, 'total_duration': 22229.440237522125, 'accumulated_submission_time': 21455.754417657852, 'accumulated_eval_time': 769.8576102256775, 'accumulated_logging_time': 1.5023293495178223, 'global_step': 63088, 'preemption_count': 0}), (64591, {'train/accuracy': 0.7138074040412903, 'train/loss': 1.106141448020935, 'validation/accuracy': 0.6453199982643127, 'validation/loss': 1.4593579769134521, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.223273992538452, 'test/num_examples': 10000, 'score': 21965.739110708237, 'total_duration': 22757.195997476578, 'accumulated_submission_time': 21965.739110708237, 'accumulated_eval_time': 787.5250680446625, 'accumulated_logging_time': 1.551323413848877, 'global_step': 64591, 'preemption_count': 0}), (66095, {'train/accuracy': 0.7254065275192261, 'train/loss': 1.0538322925567627, 'validation/accuracy': 0.6597200036048889, 'validation/loss': 1.3996851444244385, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.1329658031463623, 'test/num_examples': 10000, 'score': 22475.68429350853, 'total_duration': 23285.028936624527, 'accumulated_submission_time': 22475.68429350853, 'accumulated_eval_time': 805.305394411087, 'accumulated_logging_time': 1.60355544090271, 'global_step': 66095, 'preemption_count': 0}), (67598, {'train/accuracy': 0.7191087007522583, 'train/loss': 1.0847852230072021, 'validation/accuracy': 0.6524999737739563, 'validation/loss': 1.4338654279708862, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.1947619915008545, 'test/num_examples': 10000, 'score': 22985.69870376587, 'total_duration': 23813.08985543251, 'accumulated_submission_time': 22985.69870376587, 'accumulated_eval_time': 823.2509181499481, 'accumulated_logging_time': 1.6490552425384521, 'global_step': 67598, 'preemption_count': 0}), (69101, {'train/accuracy': 0.7221181392669678, 'train/loss': 1.0768852233886719, 'validation/accuracy': 0.6430599689483643, 'validation/loss': 1.46754789352417, 'validation/num_examples': 50000, 'test/accuracy': 0.5143000483512878, 'test/loss': 2.2291743755340576, 'test/num_examples': 10000, 'score': 23495.98047399521, 'total_duration': 24341.536016464233, 'accumulated_submission_time': 23495.98047399521, 'accumulated_eval_time': 841.3075633049011, 'accumulated_logging_time': 1.700188159942627, 'global_step': 69101, 'preemption_count': 0}), (70605, {'train/accuracy': 0.7581911683082581, 'train/loss': 0.9060986638069153, 'validation/accuracy': 0.6655600070953369, 'validation/loss': 1.3723647594451904, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.08642315864563, 'test/num_examples': 10000, 'score': 24006.12465786934, 'total_duration': 24869.49318599701, 'accumulated_submission_time': 24006.12465786934, 'accumulated_eval_time': 859.0242967605591, 'accumulated_logging_time': 1.7394630908966064, 'global_step': 70605, 'preemption_count': 0}), (72109, {'train/accuracy': 0.7310466766357422, 'train/loss': 1.0297045707702637, 'validation/accuracy': 0.6527999639511108, 'validation/loss': 1.4410232305526733, 'validation/num_examples': 50000, 'test/accuracy': 0.5228000283241272, 'test/loss': 2.215155601501465, 'test/num_examples': 10000, 'score': 24516.316374063492, 'total_duration': 25397.422772169113, 'accumulated_submission_time': 24516.316374063492, 'accumulated_eval_time': 876.6616532802582, 'accumulated_logging_time': 1.7846426963806152, 'global_step': 72109, 'preemption_count': 0}), (73611, {'train/accuracy': 0.7375039458274841, 'train/loss': 0.9993064999580383, 'validation/accuracy': 0.6623799800872803, 'validation/loss': 1.3905607461929321, 'validation/num_examples': 50000, 'test/accuracy': 0.5325000286102295, 'test/loss': 2.110114812850952, 'test/num_examples': 10000, 'score': 25026.270292043686, 'total_duration': 25925.412529945374, 'accumulated_submission_time': 25026.270292043686, 'accumulated_eval_time': 894.5995259284973, 'accumulated_logging_time': 1.8255927562713623, 'global_step': 73611, 'preemption_count': 0}), (75115, {'train/accuracy': 0.7437619566917419, 'train/loss': 0.9866275191307068, 'validation/accuracy': 0.6665399670600891, 'validation/loss': 1.3468725681304932, 'validation/num_examples': 50000, 'test/accuracy': 0.5391000509262085, 'test/loss': 2.0707874298095703, 'test/num_examples': 10000, 'score': 25536.4892745018, 'total_duration': 26453.562220811844, 'accumulated_submission_time': 25536.4892745018, 'accumulated_eval_time': 912.429455280304, 'accumulated_logging_time': 1.8706903457641602, 'global_step': 75115, 'preemption_count': 0}), (76618, {'train/accuracy': 0.7401546239852905, 'train/loss': 0.9870557188987732, 'validation/accuracy': 0.6706399917602539, 'validation/loss': 1.3400790691375732, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.035409450531006, 'test/num_examples': 10000, 'score': 26046.512956619263, 'total_duration': 26981.249529123306, 'accumulated_submission_time': 26046.512956619263, 'accumulated_eval_time': 929.9996762275696, 'accumulated_logging_time': 1.908890724182129, 'global_step': 76618, 'preemption_count': 0}), (78122, {'train/accuracy': 0.7384207248687744, 'train/loss': 1.0084189176559448, 'validation/accuracy': 0.665340006351471, 'validation/loss': 1.3739938735961914, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.11980938911438, 'test/num_examples': 10000, 'score': 26556.6778280735, 'total_duration': 27509.383778572083, 'accumulated_submission_time': 26556.6778280735, 'accumulated_eval_time': 947.8705537319183, 'accumulated_logging_time': 1.9521074295043945, 'global_step': 78122, 'preemption_count': 0}), (79626, {'train/accuracy': 0.767578125, 'train/loss': 0.8716039061546326, 'validation/accuracy': 0.6688399910926819, 'validation/loss': 1.3541743755340576, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.0731024742126465, 'test/num_examples': 10000, 'score': 27066.751952409744, 'total_duration': 28037.197903633118, 'accumulated_submission_time': 27066.751952409744, 'accumulated_eval_time': 965.5151259899139, 'accumulated_logging_time': 1.991746425628662, 'global_step': 79626, 'preemption_count': 0}), (81130, {'train/accuracy': 0.7565369606018066, 'train/loss': 0.9142917394638062, 'validation/accuracy': 0.6692000031471252, 'validation/loss': 1.3499724864959717, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.085909128189087, 'test/num_examples': 10000, 'score': 27576.86024737358, 'total_duration': 28565.352335691452, 'accumulated_submission_time': 27576.86024737358, 'accumulated_eval_time': 983.458841085434, 'accumulated_logging_time': 2.0363407135009766, 'global_step': 81130, 'preemption_count': 0}), (82633, {'train/accuracy': 0.7384008169174194, 'train/loss': 1.0000159740447998, 'validation/accuracy': 0.6617599725723267, 'validation/loss': 1.3959338665008545, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.1417131423950195, 'test/num_examples': 10000, 'score': 28086.937801122665, 'total_duration': 29093.46788740158, 'accumulated_submission_time': 28086.937801122665, 'accumulated_eval_time': 1001.4011144638062, 'accumulated_logging_time': 2.0771901607513428, 'global_step': 82633, 'preemption_count': 0}), (84137, {'train/accuracy': 0.7523317933082581, 'train/loss': 0.9355828166007996, 'validation/accuracy': 0.6721799969673157, 'validation/loss': 1.334437608718872, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.071115255355835, 'test/num_examples': 10000, 'score': 28597.013434410095, 'total_duration': 29621.394691944122, 'accumulated_submission_time': 28597.013434410095, 'accumulated_eval_time': 1019.1362066268921, 'accumulated_logging_time': 2.137763500213623, 'global_step': 84137, 'preemption_count': 0}), (85640, {'train/accuracy': 0.7469307780265808, 'train/loss': 0.961391270160675, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.3483177423477173, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.091202735900879, 'test/num_examples': 10000, 'score': 29106.938084840775, 'total_duration': 30149.020438194275, 'accumulated_submission_time': 29106.938084840775, 'accumulated_eval_time': 1036.7360591888428, 'accumulated_logging_time': 2.182596445083618, 'global_step': 85640, 'preemption_count': 0}), (87144, {'train/accuracy': 0.7379224896430969, 'train/loss': 0.9985449314117432, 'validation/accuracy': 0.6692799925804138, 'validation/loss': 1.3523105382919312, 'validation/num_examples': 50000, 'test/accuracy': 0.534600019454956, 'test/loss': 2.0925774574279785, 'test/num_examples': 10000, 'score': 29617.127873182297, 'total_duration': 30676.874872922897, 'accumulated_submission_time': 29617.127873182297, 'accumulated_eval_time': 1054.3037416934967, 'accumulated_logging_time': 2.2248997688293457, 'global_step': 87144, 'preemption_count': 0}), (88647, {'train/accuracy': 0.7837810516357422, 'train/loss': 0.8073683381080627, 'validation/accuracy': 0.6779199838638306, 'validation/loss': 1.3172043561935425, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.068551778793335, 'test/num_examples': 10000, 'score': 30127.066581964493, 'total_duration': 31204.71547460556, 'accumulated_submission_time': 30127.066581964493, 'accumulated_eval_time': 1072.1030399799347, 'accumulated_logging_time': 2.2718443870544434, 'global_step': 88647, 'preemption_count': 0}), (90151, {'train/accuracy': 0.7757493257522583, 'train/loss': 0.8434444069862366, 'validation/accuracy': 0.6840199828147888, 'validation/loss': 1.2816094160079956, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.0205748081207275, 'test/num_examples': 10000, 'score': 30637.134697914124, 'total_duration': 31732.880301237106, 'accumulated_submission_time': 30637.134697914124, 'accumulated_eval_time': 1090.0969746112823, 'accumulated_logging_time': 2.317918062210083, 'global_step': 90151, 'preemption_count': 0}), (91655, {'train/accuracy': 0.7606425285339355, 'train/loss': 0.8926047086715698, 'validation/accuracy': 0.6730200052261353, 'validation/loss': 1.342376470565796, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.098851203918457, 'test/num_examples': 10000, 'score': 31147.257095575333, 'total_duration': 32260.765964984894, 'accumulated_submission_time': 31147.257095575333, 'accumulated_eval_time': 1107.751292705536, 'accumulated_logging_time': 2.369476079940796, 'global_step': 91655, 'preemption_count': 0}), (93159, {'train/accuracy': 0.7567561864852905, 'train/loss': 0.9086245894432068, 'validation/accuracy': 0.6795399785041809, 'validation/loss': 1.3091254234313965, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 2.040935516357422, 'test/num_examples': 10000, 'score': 31657.1976583004, 'total_duration': 32788.57222819328, 'accumulated_submission_time': 31657.1976583004, 'accumulated_eval_time': 1125.5106115341187, 'accumulated_logging_time': 2.420135974884033, 'global_step': 93159, 'preemption_count': 0}), (94663, {'train/accuracy': 0.7587292790412903, 'train/loss': 0.9139228463172913, 'validation/accuracy': 0.6759399771690369, 'validation/loss': 1.32182776927948, 'validation/num_examples': 50000, 'test/accuracy': 0.5546000003814697, 'test/loss': 2.026935338973999, 'test/num_examples': 10000, 'score': 32167.262050628662, 'total_duration': 33316.578904390335, 'accumulated_submission_time': 32167.262050628662, 'accumulated_eval_time': 1143.3546307086945, 'accumulated_logging_time': 2.4641125202178955, 'global_step': 94663, 'preemption_count': 0}), (96167, {'train/accuracy': 0.7642498016357422, 'train/loss': 0.8854460120201111, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.2905325889587402, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.0234200954437256, 'test/num_examples': 10000, 'score': 32677.42819571495, 'total_duration': 33844.844178915024, 'accumulated_submission_time': 32677.42819571495, 'accumulated_eval_time': 1161.3488364219666, 'accumulated_logging_time': 2.514535427093506, 'global_step': 96167, 'preemption_count': 0}), (97669, {'train/accuracy': 0.8039301633834839, 'train/loss': 0.7246366143226624, 'validation/accuracy': 0.6893399953842163, 'validation/loss': 1.2651199102401733, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.0260353088378906, 'test/num_examples': 10000, 'score': 33187.63098335266, 'total_duration': 34373.93581676483, 'accumulated_submission_time': 33187.63098335266, 'accumulated_eval_time': 1180.1326916217804, 'accumulated_logging_time': 2.562276601791382, 'global_step': 97669, 'preemption_count': 0}), (99173, {'train/accuracy': 0.7836814522743225, 'train/loss': 0.7890745997428894, 'validation/accuracy': 0.6879400014877319, 'validation/loss': 1.2637015581130981, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.98188054561615, 'test/num_examples': 10000, 'score': 33697.79170894623, 'total_duration': 34902.117525577545, 'accumulated_submission_time': 33697.79170894623, 'accumulated_eval_time': 1198.0567321777344, 'accumulated_logging_time': 2.6048731803894043, 'global_step': 99173, 'preemption_count': 0}), (100676, {'train/accuracy': 0.7801538705825806, 'train/loss': 0.8223650455474854, 'validation/accuracy': 0.689579963684082, 'validation/loss': 1.2677134275436401, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 1.9989219903945923, 'test/num_examples': 10000, 'score': 34207.87293791771, 'total_duration': 35430.207654953, 'accumulated_submission_time': 34207.87293791771, 'accumulated_eval_time': 1215.964411497116, 'accumulated_logging_time': 2.652205467224121, 'global_step': 100676, 'preemption_count': 0}), (102180, {'train/accuracy': 0.78226637840271, 'train/loss': 0.813494086265564, 'validation/accuracy': 0.6924200057983398, 'validation/loss': 1.249699354171753, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9510797262191772, 'test/num_examples': 10000, 'score': 34717.97048306465, 'total_duration': 35958.062203884125, 'accumulated_submission_time': 34717.97048306465, 'accumulated_eval_time': 1233.6210358142853, 'accumulated_logging_time': 2.6980385780334473, 'global_step': 102180, 'preemption_count': 0}), (103683, {'train/accuracy': 0.7759087681770325, 'train/loss': 0.8256220817565918, 'validation/accuracy': 0.6915599703788757, 'validation/loss': 1.2632023096084595, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 1.9927477836608887, 'test/num_examples': 10000, 'score': 35228.02629613876, 'total_duration': 36485.87252473831, 'accumulated_submission_time': 35228.02629613876, 'accumulated_eval_time': 1251.2748274803162, 'accumulated_logging_time': 2.7438771724700928, 'global_step': 103683, 'preemption_count': 0}), (105186, {'train/accuracy': 0.7785993218421936, 'train/loss': 0.8219675421714783, 'validation/accuracy': 0.6947000026702881, 'validation/loss': 1.2501887083053589, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 1.9662601947784424, 'test/num_examples': 10000, 'score': 35737.980461120605, 'total_duration': 37013.7807905674, 'accumulated_submission_time': 35737.980461120605, 'accumulated_eval_time': 1269.1238374710083, 'accumulated_logging_time': 2.7935187816619873, 'global_step': 105186, 'preemption_count': 0}), (106690, {'train/accuracy': 0.8191565275192261, 'train/loss': 0.6560265421867371, 'validation/accuracy': 0.6922399997711182, 'validation/loss': 1.2624542713165283, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.02057147026062, 'test/num_examples': 10000, 'score': 36248.21604681015, 'total_duration': 37542.218354701996, 'accumulated_submission_time': 36248.21604681015, 'accumulated_eval_time': 1287.228770017624, 'accumulated_logging_time': 2.8367748260498047, 'global_step': 106690, 'preemption_count': 0}), (108194, {'train/accuracy': 0.7940050959587097, 'train/loss': 0.7586848735809326, 'validation/accuracy': 0.6922000050544739, 'validation/loss': 1.26578688621521, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.0142979621887207, 'test/num_examples': 10000, 'score': 36758.368527173996, 'total_duration': 38070.13637447357, 'accumulated_submission_time': 36758.368527173996, 'accumulated_eval_time': 1304.8902144432068, 'accumulated_logging_time': 2.8855910301208496, 'global_step': 108194, 'preemption_count': 0}), (109698, {'train/accuracy': 0.7897400856018066, 'train/loss': 0.7790143489837646, 'validation/accuracy': 0.6975199580192566, 'validation/loss': 1.232077956199646, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 1.950202465057373, 'test/num_examples': 10000, 'score': 37268.601791381836, 'total_duration': 38598.309775829315, 'accumulated_submission_time': 37268.601791381836, 'accumulated_eval_time': 1322.726529121399, 'accumulated_logging_time': 2.933062791824341, 'global_step': 109698, 'preemption_count': 0}), (111201, {'train/accuracy': 0.7818080186843872, 'train/loss': 0.8038931488990784, 'validation/accuracy': 0.6892600059509277, 'validation/loss': 1.272587776184082, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.025722026824951, 'test/num_examples': 10000, 'score': 37778.793501615524, 'total_duration': 39126.50262880325, 'accumulated_submission_time': 37778.793501615524, 'accumulated_eval_time': 1340.6236248016357, 'accumulated_logging_time': 2.9807047843933105, 'global_step': 111201, 'preemption_count': 0}), (112705, {'train/accuracy': 0.7928690910339355, 'train/loss': 0.7587718367576599, 'validation/accuracy': 0.699400007724762, 'validation/loss': 1.2161651849746704, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.9524030685424805, 'test/num_examples': 10000, 'score': 38288.8953294754, 'total_duration': 39654.957559108734, 'accumulated_submission_time': 38288.8953294754, 'accumulated_eval_time': 1358.8687388896942, 'accumulated_logging_time': 3.033480167388916, 'global_step': 112705, 'preemption_count': 0}), (114209, {'train/accuracy': 0.7909358739852905, 'train/loss': 0.765129566192627, 'validation/accuracy': 0.6998800039291382, 'validation/loss': 1.2264665365219116, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.9734641313552856, 'test/num_examples': 10000, 'score': 38798.96592998505, 'total_duration': 40183.36325955391, 'accumulated_submission_time': 38798.96592998505, 'accumulated_eval_time': 1377.1002910137177, 'accumulated_logging_time': 3.080017566680908, 'global_step': 114209, 'preemption_count': 0}), (115713, {'train/accuracy': 0.8435705900192261, 'train/loss': 0.576335072517395, 'validation/accuracy': 0.7015399932861328, 'validation/loss': 1.2031558752059937, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.942007064819336, 'test/num_examples': 10000, 'score': 39309.17578649521, 'total_duration': 40711.58852005005, 'accumulated_submission_time': 39309.17578649521, 'accumulated_eval_time': 1395.0114710330963, 'accumulated_logging_time': 3.126986265182495, 'global_step': 115713, 'preemption_count': 0}), (117217, {'train/accuracy': 0.8229233026504517, 'train/loss': 0.6437040567398071, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.194665551185608, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.9088903665542603, 'test/num_examples': 10000, 'score': 39819.32137942314, 'total_duration': 41239.41584944725, 'accumulated_submission_time': 39819.32137942314, 'accumulated_eval_time': 1412.585030078888, 'accumulated_logging_time': 3.1796674728393555, 'global_step': 117217, 'preemption_count': 0}), (118720, {'train/accuracy': 0.8165457248687744, 'train/loss': 0.6733990907669067, 'validation/accuracy': 0.7057200074195862, 'validation/loss': 1.2064604759216309, 'validation/num_examples': 50000, 'test/accuracy': 0.5786000490188599, 'test/loss': 1.9398770332336426, 'test/num_examples': 10000, 'score': 40329.29307627678, 'total_duration': 41767.536363363266, 'accumulated_submission_time': 40329.29307627678, 'accumulated_eval_time': 1430.6261870861053, 'accumulated_logging_time': 3.2323155403137207, 'global_step': 118720, 'preemption_count': 0}), (120223, {'train/accuracy': 0.8181002736091614, 'train/loss': 0.6516917943954468, 'validation/accuracy': 0.7089999914169312, 'validation/loss': 1.1775261163711548, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.913909673690796, 'test/num_examples': 10000, 'score': 40839.21898150444, 'total_duration': 42296.18041563034, 'accumulated_submission_time': 40839.21898150444, 'accumulated_eval_time': 1449.2406451702118, 'accumulated_logging_time': 3.280336380004883, 'global_step': 120223, 'preemption_count': 0}), (121726, {'train/accuracy': 0.8082947731018066, 'train/loss': 0.6896772384643555, 'validation/accuracy': 0.7066999673843384, 'validation/loss': 1.1927813291549683, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 1.9429177045822144, 'test/num_examples': 10000, 'score': 41349.17365002632, 'total_duration': 42824.30308508873, 'accumulated_submission_time': 41349.17365002632, 'accumulated_eval_time': 1467.3116641044617, 'accumulated_logging_time': 3.323556900024414, 'global_step': 121726, 'preemption_count': 0}), (123230, {'train/accuracy': 0.8172034025192261, 'train/loss': 0.6480165719985962, 'validation/accuracy': 0.7120800018310547, 'validation/loss': 1.1730077266693115, 'validation/num_examples': 50000, 'test/accuracy': 0.5870000123977661, 'test/loss': 1.930961012840271, 'test/num_examples': 10000, 'score': 41859.14821410179, 'total_duration': 43352.32322573662, 'accumulated_submission_time': 41859.14821410179, 'accumulated_eval_time': 1485.2464039325714, 'accumulated_logging_time': 3.3781511783599854, 'global_step': 123230, 'preemption_count': 0}), (124734, {'train/accuracy': 0.8444275856018066, 'train/loss': 0.5557413101196289, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.1623338460922241, 'validation/num_examples': 50000, 'test/accuracy': 0.5822000503540039, 'test/loss': 1.8956857919692993, 'test/num_examples': 10000, 'score': 42369.25692343712, 'total_duration': 43880.17747545242, 'accumulated_submission_time': 42369.25692343712, 'accumulated_eval_time': 1502.8864228725433, 'accumulated_logging_time': 3.428584575653076, 'global_step': 124734, 'preemption_count': 0}), (126238, {'train/accuracy': 0.8463608026504517, 'train/loss': 0.5420656800270081, 'validation/accuracy': 0.7162599563598633, 'validation/loss': 1.1581156253814697, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.887725830078125, 'test/num_examples': 10000, 'score': 42879.326265096664, 'total_duration': 44408.17130422592, 'accumulated_submission_time': 42879.326265096664, 'accumulated_eval_time': 1520.7035658359528, 'accumulated_logging_time': 3.4789483547210693, 'global_step': 126238, 'preemption_count': 0}), (127741, {'train/accuracy': 0.8425143361091614, 'train/loss': 0.5580957531929016, 'validation/accuracy': 0.7177199721336365, 'validation/loss': 1.1547913551330566, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.8950108289718628, 'test/num_examples': 10000, 'score': 43389.325770139694, 'total_duration': 44935.92632341385, 'accumulated_submission_time': 43389.325770139694, 'accumulated_eval_time': 1538.350778579712, 'accumulated_logging_time': 3.5324387550354004, 'global_step': 127741, 'preemption_count': 0}), (129246, {'train/accuracy': 0.8390465378761292, 'train/loss': 0.5831946134567261, 'validation/accuracy': 0.7163999676704407, 'validation/loss': 1.167493462562561, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.9175639152526855, 'test/num_examples': 10000, 'score': 43899.508662223816, 'total_duration': 45464.26081061363, 'accumulated_submission_time': 43899.508662223816, 'accumulated_eval_time': 1556.3912541866302, 'accumulated_logging_time': 3.587545394897461, 'global_step': 129246, 'preemption_count': 0}), (130750, {'train/accuracy': 0.8318319320678711, 'train/loss': 0.5993456840515137, 'validation/accuracy': 0.715939998626709, 'validation/loss': 1.1581324338912964, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8844317197799683, 'test/num_examples': 10000, 'score': 44409.489119291306, 'total_duration': 45992.3717007637, 'accumulated_submission_time': 44409.489119291306, 'accumulated_eval_time': 1574.4115755558014, 'accumulated_logging_time': 3.6436238288879395, 'global_step': 130750, 'preemption_count': 0}), (132254, {'train/accuracy': 0.8413584232330322, 'train/loss': 0.5591051578521729, 'validation/accuracy': 0.7198799848556519, 'validation/loss': 1.1507633924484253, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.8953807353973389, 'test/num_examples': 10000, 'score': 44919.46421599388, 'total_duration': 46520.07335758209, 'accumulated_submission_time': 44919.46421599388, 'accumulated_eval_time': 1592.0317244529724, 'accumulated_logging_time': 3.695392608642578, 'global_step': 132254, 'preemption_count': 0}), (133758, {'train/accuracy': 0.8465999364852905, 'train/loss': 0.5491646528244019, 'validation/accuracy': 0.7141599655151367, 'validation/loss': 1.1705526113510132, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.9156215190887451, 'test/num_examples': 10000, 'score': 45429.54328536987, 'total_duration': 47048.18325304985, 'accumulated_submission_time': 45429.54328536987, 'accumulated_eval_time': 1609.954525232315, 'accumulated_logging_time': 3.7486047744750977, 'global_step': 133758, 'preemption_count': 0}), (135262, {'train/accuracy': 0.8697385191917419, 'train/loss': 0.4575299024581909, 'validation/accuracy': 0.7210599780082703, 'validation/loss': 1.1391361951828003, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.874566674232483, 'test/num_examples': 10000, 'score': 45939.4492623806, 'total_duration': 47576.754868507385, 'accumulated_submission_time': 45939.4492623806, 'accumulated_eval_time': 1628.5150740146637, 'accumulated_logging_time': 3.798022508621216, 'global_step': 135262, 'preemption_count': 0}), (136765, {'train/accuracy': 0.8618462681770325, 'train/loss': 0.48523062467575073, 'validation/accuracy': 0.7225599884986877, 'validation/loss': 1.1408337354660034, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.8832614421844482, 'test/num_examples': 10000, 'score': 46449.43899035454, 'total_duration': 48104.78061199188, 'accumulated_submission_time': 46449.43899035454, 'accumulated_eval_time': 1646.445203781128, 'accumulated_logging_time': 3.8481035232543945, 'global_step': 136765, 'preemption_count': 0}), (138269, {'train/accuracy': 0.867586076259613, 'train/loss': 0.4603193998336792, 'validation/accuracy': 0.731939971446991, 'validation/loss': 1.108080506324768, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.865692138671875, 'test/num_examples': 10000, 'score': 46959.6368894577, 'total_duration': 48632.8400952816, 'accumulated_submission_time': 46959.6368894577, 'accumulated_eval_time': 1664.1922266483307, 'accumulated_logging_time': 3.9070560932159424, 'global_step': 138269, 'preemption_count': 0}), (139773, {'train/accuracy': 0.8621252775192261, 'train/loss': 0.47670185565948486, 'validation/accuracy': 0.7252799868583679, 'validation/loss': 1.134071946144104, 'validation/num_examples': 50000, 'test/accuracy': 0.5985000133514404, 'test/loss': 1.8925247192382812, 'test/num_examples': 10000, 'score': 47469.786170721054, 'total_duration': 49160.736943006516, 'accumulated_submission_time': 47469.786170721054, 'accumulated_eval_time': 1681.8307964801788, 'accumulated_logging_time': 3.9598209857940674, 'global_step': 139773, 'preemption_count': 0}), (141277, {'train/accuracy': 0.8630420565605164, 'train/loss': 0.47475898265838623, 'validation/accuracy': 0.726419985294342, 'validation/loss': 1.1304347515106201, 'validation/num_examples': 50000, 'test/accuracy': 0.6062000393867493, 'test/loss': 1.8601926565170288, 'test/num_examples': 10000, 'score': 47979.92598223686, 'total_duration': 49688.70345711708, 'accumulated_submission_time': 47979.92598223686, 'accumulated_eval_time': 1699.5504500865936, 'accumulated_logging_time': 4.012192010879517, 'global_step': 141277, 'preemption_count': 0}), (142781, {'train/accuracy': 0.8639389276504517, 'train/loss': 0.46813029050827026, 'validation/accuracy': 0.7273600101470947, 'validation/loss': 1.1400127410888672, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9035028219223022, 'test/num_examples': 10000, 'score': 48490.05924510956, 'total_duration': 50216.637149333954, 'accumulated_submission_time': 48490.05924510956, 'accumulated_eval_time': 1717.2319912910461, 'accumulated_logging_time': 4.073648452758789, 'global_step': 142781, 'preemption_count': 0}), (144285, {'train/accuracy': 0.8989357352256775, 'train/loss': 0.352335661649704, 'validation/accuracy': 0.7356399893760681, 'validation/loss': 1.1080385446548462, 'validation/num_examples': 50000, 'test/accuracy': 0.6078000068664551, 'test/loss': 1.842755913734436, 'test/num_examples': 10000, 'score': 49000.12387108803, 'total_duration': 50744.836602687836, 'accumulated_submission_time': 49000.12387108803, 'accumulated_eval_time': 1735.258573770523, 'accumulated_logging_time': 4.126613616943359, 'global_step': 144285, 'preemption_count': 0}), (145788, {'train/accuracy': 0.8931760191917419, 'train/loss': 0.36942258477211, 'validation/accuracy': 0.7378199696540833, 'validation/loss': 1.0927354097366333, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.842835783958435, 'test/num_examples': 10000, 'score': 49510.038204193115, 'total_duration': 51272.56175875664, 'accumulated_submission_time': 49510.038204193115, 'accumulated_eval_time': 1752.9578087329865, 'accumulated_logging_time': 4.1841514110565186, 'global_step': 145788, 'preemption_count': 0}), (147290, {'train/accuracy': 0.8905253410339355, 'train/loss': 0.3740461766719818, 'validation/accuracy': 0.7361399531364441, 'validation/loss': 1.08772873878479, 'validation/num_examples': 50000, 'test/accuracy': 0.6048000454902649, 'test/loss': 1.8468471765518188, 'test/num_examples': 10000, 'score': 50020.07937192917, 'total_duration': 51800.63540673256, 'accumulated_submission_time': 50020.07937192917, 'accumulated_eval_time': 1770.8793761730194, 'accumulated_logging_time': 4.238028526306152, 'global_step': 147290, 'preemption_count': 0}), (148793, {'train/accuracy': 0.8932557106018066, 'train/loss': 0.3641088008880615, 'validation/accuracy': 0.7346799969673157, 'validation/loss': 1.1118369102478027, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.8756835460662842, 'test/num_examples': 10000, 'score': 50529.98758006096, 'total_duration': 52328.545087337494, 'accumulated_submission_time': 50529.98758006096, 'accumulated_eval_time': 1788.7686505317688, 'accumulated_logging_time': 4.294397830963135, 'global_step': 148793, 'preemption_count': 0}), (150296, {'train/accuracy': 0.8932358026504517, 'train/loss': 0.36360234022140503, 'validation/accuracy': 0.7393400073051453, 'validation/loss': 1.103121042251587, 'validation/num_examples': 50000, 'test/accuracy': 0.6160000562667847, 'test/loss': 1.858383297920227, 'test/num_examples': 10000, 'score': 51040.08410692215, 'total_duration': 52856.537358284, 'accumulated_submission_time': 51040.08410692215, 'accumulated_eval_time': 1806.554343700409, 'accumulated_logging_time': 4.349008321762085, 'global_step': 150296, 'preemption_count': 0}), (151799, {'train/accuracy': 0.8980388641357422, 'train/loss': 0.35135653614997864, 'validation/accuracy': 0.7346799969673157, 'validation/loss': 1.1100929975509644, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.8680452108383179, 'test/num_examples': 10000, 'score': 51550.05728435516, 'total_duration': 53384.782229185104, 'accumulated_submission_time': 51550.05728435516, 'accumulated_eval_time': 1824.7137916088104, 'accumulated_logging_time': 4.405257701873779, 'global_step': 151799, 'preemption_count': 0}), (153303, {'train/accuracy': 0.9187459945678711, 'train/loss': 0.28570467233657837, 'validation/accuracy': 0.7361999750137329, 'validation/loss': 1.1064831018447876, 'validation/num_examples': 50000, 'test/accuracy': 0.6100000143051147, 'test/loss': 1.8766512870788574, 'test/num_examples': 10000, 'score': 52060.17003774643, 'total_duration': 53912.83220410347, 'accumulated_submission_time': 52060.17003774643, 'accumulated_eval_time': 1842.5373332500458, 'accumulated_logging_time': 4.462253093719482, 'global_step': 153303, 'preemption_count': 0}), (154806, {'train/accuracy': 0.9205396771430969, 'train/loss': 0.2786844074726105, 'validation/accuracy': 0.7440399527549744, 'validation/loss': 1.0820631980895996, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.84649658203125, 'test/num_examples': 10000, 'score': 52570.1029856205, 'total_duration': 54440.91394495964, 'accumulated_submission_time': 52570.1029856205, 'accumulated_eval_time': 1860.575585603714, 'accumulated_logging_time': 4.516483306884766, 'global_step': 154806, 'preemption_count': 0}), (156310, {'train/accuracy': 0.9159757494926453, 'train/loss': 0.2899870276451111, 'validation/accuracy': 0.7417399883270264, 'validation/loss': 1.092580795288086, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.8602204322814941, 'test/num_examples': 10000, 'score': 53080.279076099396, 'total_duration': 54969.0854177475, 'accumulated_submission_time': 53080.279076099396, 'accumulated_eval_time': 1878.458645582199, 'accumulated_logging_time': 4.5743114948272705, 'global_step': 156310, 'preemption_count': 0}), (157813, {'train/accuracy': 0.9194834232330322, 'train/loss': 0.27230069041252136, 'validation/accuracy': 0.7447599768638611, 'validation/loss': 1.0868667364120483, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.8521299362182617, 'test/num_examples': 10000, 'score': 53590.2423658371, 'total_duration': 55496.98831796646, 'accumulated_submission_time': 53590.2423658371, 'accumulated_eval_time': 1896.2873928546906, 'accumulated_logging_time': 4.627979040145874, 'global_step': 157813, 'preemption_count': 0}), (159316, {'train/accuracy': 0.9231106042861938, 'train/loss': 0.2618570923805237, 'validation/accuracy': 0.746239960193634, 'validation/loss': 1.0792582035064697, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.8350087404251099, 'test/num_examples': 10000, 'score': 54100.30746936798, 'total_duration': 56025.2314991951, 'accumulated_submission_time': 54100.30746936798, 'accumulated_eval_time': 1914.3551132678986, 'accumulated_logging_time': 4.6828649044036865, 'global_step': 159316, 'preemption_count': 0}), (160819, {'train/accuracy': 0.9272759556770325, 'train/loss': 0.25060129165649414, 'validation/accuracy': 0.7461400032043457, 'validation/loss': 1.0742896795272827, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.841195821762085, 'test/num_examples': 10000, 'score': 54610.303926706314, 'total_duration': 56553.10640120506, 'accumulated_submission_time': 54610.303926706314, 'accumulated_eval_time': 1932.1193158626556, 'accumulated_logging_time': 4.740338563919067, 'global_step': 160819, 'preemption_count': 0}), (162322, {'train/accuracy': 0.9462890625, 'train/loss': 0.1946813315153122, 'validation/accuracy': 0.7486400008201599, 'validation/loss': 1.0698437690734863, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.8549610376358032, 'test/num_examples': 10000, 'score': 55120.26196694374, 'total_duration': 57080.722338199615, 'accumulated_submission_time': 55120.26196694374, 'accumulated_eval_time': 1949.6665840148926, 'accumulated_logging_time': 4.796144008636475, 'global_step': 162322, 'preemption_count': 0}), (163825, {'train/accuracy': 0.9412069320678711, 'train/loss': 0.20645396411418915, 'validation/accuracy': 0.7478599548339844, 'validation/loss': 1.0675770044326782, 'validation/num_examples': 50000, 'test/accuracy': 0.6202000379562378, 'test/loss': 1.8455793857574463, 'test/num_examples': 10000, 'score': 55630.45647478104, 'total_duration': 57608.87885069847, 'accumulated_submission_time': 55630.45647478104, 'accumulated_eval_time': 1967.5142793655396, 'accumulated_logging_time': 4.854479789733887, 'global_step': 163825, 'preemption_count': 0}), (165329, {'train/accuracy': 0.9415258169174194, 'train/loss': 0.20436236262321472, 'validation/accuracy': 0.7491599917411804, 'validation/loss': 1.068067193031311, 'validation/num_examples': 50000, 'test/accuracy': 0.6196000576019287, 'test/loss': 1.8581254482269287, 'test/num_examples': 10000, 'score': 56140.58478808403, 'total_duration': 58137.0842730999, 'accumulated_submission_time': 56140.58478808403, 'accumulated_eval_time': 1985.4797623157501, 'accumulated_logging_time': 4.911173105239868, 'global_step': 165329, 'preemption_count': 0}), (166832, {'train/accuracy': 0.9414859414100647, 'train/loss': 0.20662200450897217, 'validation/accuracy': 0.7490400075912476, 'validation/loss': 1.0683197975158691, 'validation/num_examples': 50000, 'test/accuracy': 0.6226000189781189, 'test/loss': 1.8363534212112427, 'test/num_examples': 10000, 'score': 56650.707070589066, 'total_duration': 58665.36136960983, 'accumulated_submission_time': 56650.707070589066, 'accumulated_eval_time': 2003.517992734909, 'accumulated_logging_time': 4.971322536468506, 'global_step': 166832, 'preemption_count': 0}), (168335, {'train/accuracy': 0.9429408311843872, 'train/loss': 0.1995099037885666, 'validation/accuracy': 0.7509599924087524, 'validation/loss': 1.0619680881500244, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.8424190282821655, 'test/num_examples': 10000, 'score': 57160.779010772705, 'total_duration': 59193.236011981964, 'accumulated_submission_time': 57160.779010772705, 'accumulated_eval_time': 2021.207232952118, 'accumulated_logging_time': 5.02895712852478, 'global_step': 168335, 'preemption_count': 0}), (169838, {'train/accuracy': 0.949238657951355, 'train/loss': 0.18085843324661255, 'validation/accuracy': 0.7513399720191956, 'validation/loss': 1.060373306274414, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.8413702249526978, 'test/num_examples': 10000, 'score': 57670.78219342232, 'total_duration': 59720.89051890373, 'accumulated_submission_time': 57670.78219342232, 'accumulated_eval_time': 2038.746482372284, 'accumulated_logging_time': 5.086165189743042, 'global_step': 169838, 'preemption_count': 0}), (171342, {'train/accuracy': 0.9575095176696777, 'train/loss': 0.15615008771419525, 'validation/accuracy': 0.7531999945640564, 'validation/loss': 1.0549283027648926, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.8321973085403442, 'test/num_examples': 10000, 'score': 58180.96397686005, 'total_duration': 60249.019006729126, 'accumulated_submission_time': 58180.96397686005, 'accumulated_eval_time': 2056.577041864395, 'accumulated_logging_time': 5.145231485366821, 'global_step': 171342, 'preemption_count': 0}), (172845, {'train/accuracy': 0.9551578164100647, 'train/loss': 0.1624404788017273, 'validation/accuracy': 0.7537599802017212, 'validation/loss': 1.0578937530517578, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8417407274246216, 'test/num_examples': 10000, 'score': 58691.0938334465, 'total_duration': 60777.034376859665, 'accumulated_submission_time': 58691.0938334465, 'accumulated_eval_time': 2074.348899126053, 'accumulated_logging_time': 5.2050487995147705, 'global_step': 172845, 'preemption_count': 0}), (174348, {'train/accuracy': 0.9562938213348389, 'train/loss': 0.16008029878139496, 'validation/accuracy': 0.7548199892044067, 'validation/loss': 1.0536705255508423, 'validation/num_examples': 50000, 'test/accuracy': 0.6251000165939331, 'test/loss': 1.828043818473816, 'test/num_examples': 10000, 'score': 59201.03386545181, 'total_duration': 61305.606140851974, 'accumulated_submission_time': 59201.03386545181, 'accumulated_eval_time': 2092.863926887512, 'accumulated_logging_time': 5.264968156814575, 'global_step': 174348, 'preemption_count': 0}), (175850, {'train/accuracy': 0.9552574753761292, 'train/loss': 0.16187043488025665, 'validation/accuracy': 0.753879964351654, 'validation/loss': 1.0522363185882568, 'validation/num_examples': 50000, 'test/accuracy': 0.628000020980835, 'test/loss': 1.8285508155822754, 'test/num_examples': 10000, 'score': 59710.93507862091, 'total_duration': 61833.54664039612, 'accumulated_submission_time': 59710.93507862091, 'accumulated_eval_time': 2110.787750482559, 'accumulated_logging_time': 5.325360298156738, 'global_step': 175850, 'preemption_count': 0}), (177353, {'train/accuracy': 0.9591039419174194, 'train/loss': 0.15503987669944763, 'validation/accuracy': 0.7555800080299377, 'validation/loss': 1.0516177415847778, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.8207694292068481, 'test/num_examples': 10000, 'score': 60220.999058008194, 'total_duration': 62361.60022234917, 'accumulated_submission_time': 60220.999058008194, 'accumulated_eval_time': 2128.663361310959, 'accumulated_logging_time': 5.3837199211120605, 'global_step': 177353, 'preemption_count': 0}), (178856, {'train/accuracy': 0.9585060477256775, 'train/loss': 0.1536491960287094, 'validation/accuracy': 0.7564199566841125, 'validation/loss': 1.0479017496109009, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.821165680885315, 'test/num_examples': 10000, 'score': 60731.09321784973, 'total_duration': 62889.418065071106, 'accumulated_submission_time': 60731.09321784973, 'accumulated_eval_time': 2146.2478954792023, 'accumulated_logging_time': 5.467687129974365, 'global_step': 178856, 'preemption_count': 0}), (180359, {'train/accuracy': 0.9599609375, 'train/loss': 0.1484406441450119, 'validation/accuracy': 0.7556799650192261, 'validation/loss': 1.0471597909927368, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8269197940826416, 'test/num_examples': 10000, 'score': 61241.01958680153, 'total_duration': 63417.25886678696, 'accumulated_submission_time': 61241.01958680153, 'accumulated_eval_time': 2164.042452096939, 'accumulated_logging_time': 5.532188653945923, 'global_step': 180359, 'preemption_count': 0}), (181862, {'train/accuracy': 0.961355984210968, 'train/loss': 0.14692814648151398, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.046109676361084, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.8241914510726929, 'test/num_examples': 10000, 'score': 61751.206904411316, 'total_duration': 63945.358550071716, 'accumulated_submission_time': 61751.206904411316, 'accumulated_eval_time': 2181.832026720047, 'accumulated_logging_time': 5.597205638885498, 'global_step': 181862, 'preemption_count': 0}), (183366, {'train/accuracy': 0.9613958597183228, 'train/loss': 0.1471957266330719, 'validation/accuracy': 0.7557199597358704, 'validation/loss': 1.0465097427368164, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8238227367401123, 'test/num_examples': 10000, 'score': 62261.4028301239, 'total_duration': 64473.26201725006, 'accumulated_submission_time': 62261.4028301239, 'accumulated_eval_time': 2199.4081242084503, 'accumulated_logging_time': 5.6746907234191895, 'global_step': 183366, 'preemption_count': 0}), (184868, {'train/accuracy': 0.959980845451355, 'train/loss': 0.14590847492218018, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0453925132751465, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8226069211959839, 'test/num_examples': 10000, 'score': 62771.28944039345, 'total_duration': 65001.01459479332, 'accumulated_submission_time': 62771.28944039345, 'accumulated_eval_time': 2217.1566207408905, 'accumulated_logging_time': 5.737559795379639, 'global_step': 184868, 'preemption_count': 0})], 'global_step': 185566}
I0130 13:42:42.008568 140187804313408 submission_runner.py:586] Timing: 63008.05158352852
I0130 13:42:42.008641 140187804313408 submission_runner.py:588] Total number of evals: 124
I0130 13:42:42.008700 140187804313408 submission_runner.py:589] ====================
I0130 13:42:42.010142 140187804313408 submission_runner.py:673] Final imagenet_resnet score: 63008.05158352852
