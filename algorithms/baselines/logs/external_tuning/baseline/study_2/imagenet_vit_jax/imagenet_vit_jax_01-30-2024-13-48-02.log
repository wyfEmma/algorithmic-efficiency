python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_2 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=2064292405 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_01-30-2024-13-48-02.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0130 13:48:22.687041 139936116377408 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax.
I0130 13:48:23.675494 139936116377408 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0130 13:48:23.676198 139936116377408 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0130 13:48:23.676348 139936116377408 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0130 13:48:23.677331 139936116377408 submission_runner.py:542] Using RNG seed 2064292405
I0130 13:48:24.742400 139936116377408 submission_runner.py:551] --- Tuning run 1/5 ---
I0130 13:48:24.742605 139936116377408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_1.
I0130 13:48:24.742886 139936116377408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_1/hparams.json.
I0130 13:48:24.924158 139936116377408 submission_runner.py:206] Initializing dataset.
I0130 13:48:24.940316 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:48:24.950530 139936116377408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:48:25.324513 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:48:34.513033 139936116377408 submission_runner.py:213] Initializing model.
I0130 13:48:44.414196 139936116377408 submission_runner.py:255] Initializing optimizer.
I0130 13:48:45.449509 139936116377408 submission_runner.py:262] Initializing metrics bundle.
I0130 13:48:45.449758 139936116377408 submission_runner.py:280] Initializing checkpoint and logger.
I0130 13:48:45.451239 139936116377408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0130 13:48:45.451391 139936116377408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0130 13:48:45.818353 139936116377408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0130 13:48:46.169906 139936116377408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_1/flags_0.json.
I0130 13:48:46.180156 139936116377408 submission_runner.py:314] Starting training loop.
I0130 13:49:28.618366 139774241273600 logging_writer.py:48] [0] global_step=0, grad_norm=0.3286792039871216, loss=6.907756805419922
I0130 13:49:28.635400 139936116377408 spec.py:321] Evaluating on the training split.
I0130 13:49:28.644416 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:49:28.653532 139936116377408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:49:28.738049 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:49:45.790259 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 13:49:45.800988 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:49:45.820966 139936116377408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 13:49:45.899092 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 13:50:03.299891 139936116377408 spec.py:349] Evaluating on the test split.
I0130 13:50:03.306435 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 13:50:03.313532 139936116377408 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0130 13:50:03.363344 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 13:50:08.698066 139936116377408 submission_runner.py:408] Time since start: 82.52s, 	Step: 1, 	{'train/accuracy': 0.0010351561941206455, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.45515441894531, 'total_duration': 82.5178644657135, 'accumulated_submission_time': 42.45515441894531, 'accumulated_eval_time': 40.06261396408081, 'accumulated_logging_time': 0}
I0130 13:50:08.716047 139741752174336 logging_writer.py:48] [1] accumulated_eval_time=40.062614, accumulated_logging_time=0, accumulated_submission_time=42.455154, global_step=1, preemption_count=0, score=42.455154, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=82.517864, train/accuracy=0.001035, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0130 13:51:10.292100 139772246845184 logging_writer.py:48] [100] global_step=100, grad_norm=0.34918680787086487, loss=6.906249046325684
I0130 13:51:54.075622 139772255237888 logging_writer.py:48] [200] global_step=200, grad_norm=0.43331021070480347, loss=6.894942283630371
I0130 13:52:39.415931 139772246845184 logging_writer.py:48] [300] global_step=300, grad_norm=0.6003257036209106, loss=6.858579158782959
I0130 13:53:24.679045 139772255237888 logging_writer.py:48] [400] global_step=400, grad_norm=0.6580072641372681, loss=6.82118558883667
I0130 13:54:09.993439 139772246845184 logging_writer.py:48] [500] global_step=500, grad_norm=0.9131408333778381, loss=6.835170269012451
I0130 13:54:55.077675 139772255237888 logging_writer.py:48] [600] global_step=600, grad_norm=0.9715859889984131, loss=6.757582187652588
I0130 13:55:40.173753 139772246845184 logging_writer.py:48] [700] global_step=700, grad_norm=1.7296198606491089, loss=6.665935039520264
I0130 13:56:25.432221 139772255237888 logging_writer.py:48] [800] global_step=800, grad_norm=0.9966913461685181, loss=6.733123779296875
I0130 13:57:08.937826 139936116377408 spec.py:321] Evaluating on the training split.
I0130 13:57:20.939442 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 13:57:29.163796 139936116377408 spec.py:349] Evaluating on the test split.
I0130 13:57:30.816429 139936116377408 submission_runner.py:408] Time since start: 524.64s, 	Step: 898, 	{'train/accuracy': 0.013769530691206455, 'train/loss': 6.404803276062012, 'validation/accuracy': 0.013580000028014183, 'validation/loss': 6.413737773895264, 'validation/num_examples': 50000, 'test/accuracy': 0.01080000028014183, 'test/loss': 6.456206321716309, 'test/num_examples': 10000, 'score': 462.6178922653198, 'total_duration': 524.6362130641937, 'accumulated_submission_time': 462.6178922653198, 'accumulated_eval_time': 61.94122099876404, 'accumulated_logging_time': 0.02863001823425293}
I0130 13:57:30.834515 139741760567040 logging_writer.py:48] [898] accumulated_eval_time=61.941221, accumulated_logging_time=0.028630, accumulated_submission_time=462.617892, global_step=898, preemption_count=0, score=462.617892, test/accuracy=0.010800, test/loss=6.456206, test/num_examples=10000, total_duration=524.636213, train/accuracy=0.013770, train/loss=6.404803, validation/accuracy=0.013580, validation/loss=6.413738, validation/num_examples=50000
I0130 13:57:32.061286 139741768959744 logging_writer.py:48] [900] global_step=900, grad_norm=1.5052900314331055, loss=6.696053981781006
I0130 13:58:11.892600 139741760567040 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.650515079498291, loss=6.542445182800293
I0130 13:58:56.930809 139741768959744 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.7018461227416992, loss=6.765303134918213
I0130 13:59:42.302449 139741760567040 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.298552989959717, loss=6.467925548553467
I0130 14:00:27.492378 139741768959744 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.5578536987304688, loss=6.742947578430176
I0130 14:01:12.551294 139741760567040 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.5815385580062866, loss=6.481950759887695
I0130 14:01:57.737624 139741768959744 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.5045583248138428, loss=6.494715213775635
I0130 14:02:42.956502 139741760567040 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.6224985122680664, loss=6.272870063781738
I0130 14:03:28.093772 139741768959744 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.1008453369140625, loss=6.232130527496338
I0130 14:04:13.223056 139741760567040 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.5846893787384033, loss=6.261770248413086
I0130 14:04:31.269587 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:04:43.058644 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:04:51.279431 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:04:52.903045 139936116377408 submission_runner.py:408] Time since start: 966.72s, 	Step: 1842, 	{'train/accuracy': 0.04136718809604645, 'train/loss': 5.8381195068359375, 'validation/accuracy': 0.03776000067591667, 'validation/loss': 5.869866847991943, 'validation/num_examples': 50000, 'test/accuracy': 0.03060000203549862, 'test/loss': 5.982430934906006, 'test/num_examples': 10000, 'score': 882.9875965118408, 'total_duration': 966.72283244133, 'accumulated_submission_time': 882.9875965118408, 'accumulated_eval_time': 83.57465052604675, 'accumulated_logging_time': 0.06095552444458008}
I0130 14:04:52.919842 139741768959744 logging_writer.py:48] [1842] accumulated_eval_time=83.574651, accumulated_logging_time=0.060956, accumulated_submission_time=882.987597, global_step=1842, preemption_count=0, score=882.987597, test/accuracy=0.030600, test/loss=5.982431, test/num_examples=10000, total_duration=966.722832, train/accuracy=0.041367, train/loss=5.838120, validation/accuracy=0.037760, validation/loss=5.869867, validation/num_examples=50000
I0130 14:05:16.373246 139741760567040 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.7387218475341797, loss=6.5428361892700195
I0130 14:05:59.264531 139741768959744 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.743584156036377, loss=6.637528896331787
I0130 14:06:44.444137 139741760567040 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.9850839376449585, loss=6.144577503204346
I0130 14:07:29.519951 139741768959744 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.5393717288970947, loss=6.4177141189575195
I0130 14:08:14.432499 139741760567040 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.9603855609893799, loss=6.129576683044434
I0130 14:08:59.377514 139741768959744 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.8304433822631836, loss=6.042595386505127
I0130 14:09:44.354745 139741760567040 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2487843036651611, loss=6.551854133605957
I0130 14:10:29.298455 139741768959744 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.7542860507965088, loss=6.364750862121582
I0130 14:11:14.285190 139741760567040 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2047022581100464, loss=6.196743965148926
I0130 14:11:53.198285 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:12:05.279661 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:12:13.434509 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:12:15.054708 139936116377408 submission_runner.py:408] Time since start: 1408.87s, 	Step: 2788, 	{'train/accuracy': 0.07021484524011612, 'train/loss': 5.4130401611328125, 'validation/accuracy': 0.06375999748706818, 'validation/loss': 5.468223571777344, 'validation/num_examples': 50000, 'test/accuracy': 0.05180000141263008, 'test/loss': 5.644834041595459, 'test/num_examples': 10000, 'score': 1303.2035462856293, 'total_duration': 1408.8744959831238, 'accumulated_submission_time': 1303.2035462856293, 'accumulated_eval_time': 105.43107485771179, 'accumulated_logging_time': 0.08992552757263184}
I0130 14:12:15.072398 139741768959744 logging_writer.py:48] [2788] accumulated_eval_time=105.431075, accumulated_logging_time=0.089926, accumulated_submission_time=1303.203546, global_step=2788, preemption_count=0, score=1303.203546, test/accuracy=0.051800, test/loss=5.644834, test/num_examples=10000, total_duration=1408.874496, train/accuracy=0.070215, train/loss=5.413040, validation/accuracy=0.063760, validation/loss=5.468224, validation/num_examples=50000
I0130 14:12:20.271743 139741760567040 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.956847906112671, loss=6.033196926116943
I0130 14:13:00.717562 139741768959744 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.9357738494873047, loss=6.011171817779541
I0130 14:13:45.789550 139741760567040 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.7649513483047485, loss=5.926673889160156
I0130 14:14:30.921372 139741768959744 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.3825181722640991, loss=5.8831915855407715
I0130 14:15:16.006845 139741760567040 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.4217485189437866, loss=6.631226539611816
I0130 14:16:00.902239 139741768959744 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.6542344093322754, loss=5.89897346496582
I0130 14:16:46.008729 139741760567040 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.2877212762832642, loss=6.151626110076904
I0130 14:17:31.285691 139741768959744 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.656225562095642, loss=5.8411478996276855
I0130 14:18:16.385035 139741760567040 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.7618433237075806, loss=5.9018025398254395
I0130 14:19:01.202839 139741768959744 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.059375762939453, loss=5.788522720336914
I0130 14:19:15.294959 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:19:27.197944 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:19:35.401630 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:19:37.035677 139936116377408 submission_runner.py:408] Time since start: 1850.86s, 	Step: 3733, 	{'train/accuracy': 0.10271484404802322, 'train/loss': 5.1039581298828125, 'validation/accuracy': 0.09443999826908112, 'validation/loss': 5.144573211669922, 'validation/num_examples': 50000, 'test/accuracy': 0.07150000333786011, 'test/loss': 5.371340274810791, 'test/num_examples': 10000, 'score': 1723.3643689155579, 'total_duration': 1850.8554532527924, 'accumulated_submission_time': 1723.3643689155579, 'accumulated_eval_time': 127.17176485061646, 'accumulated_logging_time': 0.11867594718933105}
I0130 14:19:37.052663 139741760567040 logging_writer.py:48] [3733] accumulated_eval_time=127.171765, accumulated_logging_time=0.118676, accumulated_submission_time=1723.364369, global_step=3733, preemption_count=0, score=1723.364369, test/accuracy=0.071500, test/loss=5.371340, test/num_examples=10000, total_duration=1850.855453, train/accuracy=0.102715, train/loss=5.103958, validation/accuracy=0.094440, validation/loss=5.144573, validation/num_examples=50000
I0130 14:20:04.086924 139741768959744 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.557178258895874, loss=5.804905891418457
I0130 14:20:48.154743 139741760567040 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.7640923261642456, loss=5.7201385498046875
I0130 14:21:33.429643 139741768959744 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.357598066329956, loss=6.538231372833252
I0130 14:22:18.716171 139741760567040 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.432373285293579, loss=6.515148639678955
I0130 14:23:04.040784 139741768959744 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.4955580234527588, loss=5.794456958770752
I0130 14:23:48.903130 139741760567040 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.2876421213150024, loss=6.510432243347168
I0130 14:24:34.383830 139741768959744 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.7652639150619507, loss=5.626123905181885
I0130 14:25:19.790097 139741760567040 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.2792888879776, loss=6.289567947387695
I0130 14:26:04.981732 139741768959744 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.763120412826538, loss=5.535326957702637
I0130 14:26:37.366470 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:26:49.121522 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:26:57.560768 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:26:59.168998 139936116377408 submission_runner.py:408] Time since start: 2292.99s, 	Step: 4673, 	{'train/accuracy': 0.13539062440395355, 'train/loss': 4.738257884979248, 'validation/accuracy': 0.12799999117851257, 'validation/loss': 4.791199207305908, 'validation/num_examples': 50000, 'test/accuracy': 0.09760000556707382, 'test/loss': 5.071451663970947, 'test/num_examples': 10000, 'score': 2143.616794347763, 'total_duration': 2292.988788843155, 'accumulated_submission_time': 2143.616794347763, 'accumulated_eval_time': 148.97430968284607, 'accumulated_logging_time': 0.14718222618103027}
I0130 14:26:59.186668 139741760567040 logging_writer.py:48] [4673] accumulated_eval_time=148.974310, accumulated_logging_time=0.147182, accumulated_submission_time=2143.616794, global_step=4673, preemption_count=0, score=2143.616794, test/accuracy=0.097600, test/loss=5.071452, test/num_examples=10000, total_duration=2292.988789, train/accuracy=0.135391, train/loss=4.738258, validation/accuracy=0.128000, validation/loss=4.791199, validation/num_examples=50000
I0130 14:27:10.374476 139741768959744 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.6352256536483765, loss=5.639646530151367
I0130 14:27:51.858141 139741760567040 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.5143948793411255, loss=5.446117401123047
I0130 14:28:37.028525 139741768959744 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.6944825649261475, loss=6.0450944900512695
I0130 14:29:23.036768 139741760567040 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.5045932531356812, loss=5.928247928619385
I0130 14:30:08.941924 139741768959744 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.5830668210983276, loss=5.614555835723877
I0130 14:30:54.775308 139741760567040 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.561802625656128, loss=5.487425804138184
I0130 14:31:40.559224 139741768959744 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.7955149412155151, loss=5.415988445281982
I0130 14:32:25.807564 139741760567040 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.5797743797302246, loss=5.315223217010498
I0130 14:33:11.117716 139741768959744 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.6490964889526367, loss=5.355499744415283
I0130 14:33:56.170511 139741760567040 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.171875238418579, loss=6.354504585266113
I0130 14:33:59.512906 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:34:11.641788 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:34:19.796397 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:34:21.417642 139936116377408 submission_runner.py:408] Time since start: 2735.24s, 	Step: 5609, 	{'train/accuracy': 0.1873828023672104, 'train/loss': 4.345278739929199, 'validation/accuracy': 0.1711599975824356, 'validation/loss': 4.43392276763916, 'validation/num_examples': 50000, 'test/accuracy': 0.12640000879764557, 'test/loss': 4.761846542358398, 'test/num_examples': 10000, 'score': 2563.884337425232, 'total_duration': 2735.237434864044, 'accumulated_submission_time': 2563.884337425232, 'accumulated_eval_time': 170.8790421485901, 'accumulated_logging_time': 0.1756596565246582}
I0130 14:34:21.436359 139741768959744 logging_writer.py:48] [5609] accumulated_eval_time=170.879042, accumulated_logging_time=0.175660, accumulated_submission_time=2563.884337, global_step=5609, preemption_count=0, score=2563.884337, test/accuracy=0.126400, test/loss=4.761847, test/num_examples=10000, total_duration=2735.237435, train/accuracy=0.187383, train/loss=4.345279, validation/accuracy=0.171160, validation/loss=4.433923, validation/num_examples=50000
I0130 14:34:58.101537 139741760567040 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.1174432039260864, loss=6.414262294769287
I0130 14:35:42.930815 139741768959744 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.4455318450927734, loss=5.394261837005615
I0130 14:36:28.336874 139741760567040 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.6759750843048096, loss=5.1558837890625
I0130 14:37:13.716446 139741768959744 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.4524495601654053, loss=5.233353614807129
I0130 14:37:58.776806 139741760567040 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.8172513246536255, loss=5.213385105133057
I0130 14:38:44.371011 139741768959744 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.6185715198516846, loss=5.457916259765625
I0130 14:39:30.648303 139741760567040 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.2367314100265503, loss=6.351158142089844
I0130 14:40:16.235753 139741768959744 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.213525414466858, loss=5.855071067810059
I0130 14:41:01.802320 139741760567040 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.9450174570083618, loss=5.256561279296875
I0130 14:41:21.834011 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:41:33.884166 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:41:45.225507 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:41:46.844934 139936116377408 submission_runner.py:408] Time since start: 3180.66s, 	Step: 6546, 	{'train/accuracy': 0.23087890446186066, 'train/loss': 3.9954845905303955, 'validation/accuracy': 0.21111999452114105, 'validation/loss': 4.086441993713379, 'validation/num_examples': 50000, 'test/accuracy': 0.15630000829696655, 'test/loss': 4.464807987213135, 'test/num_examples': 10000, 'score': 2984.2211933135986, 'total_duration': 3180.6647255420685, 'accumulated_submission_time': 2984.2211933135986, 'accumulated_eval_time': 195.88995552062988, 'accumulated_logging_time': 0.2058713436126709}
I0130 14:41:46.867095 139741768959744 logging_writer.py:48] [6546] accumulated_eval_time=195.889956, accumulated_logging_time=0.205871, accumulated_submission_time=2984.221193, global_step=6546, preemption_count=0, score=2984.221193, test/accuracy=0.156300, test/loss=4.464808, test/num_examples=10000, total_duration=3180.664726, train/accuracy=0.230879, train/loss=3.995485, validation/accuracy=0.211120, validation/loss=4.086442, validation/num_examples=50000
I0130 14:42:08.768338 139741760567040 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.6446846723556519, loss=5.201990127563477
I0130 14:42:51.885360 139741768959744 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.4615428447723389, loss=5.379145622253418
I0130 14:43:37.197134 139741760567040 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.476068377494812, loss=5.206360340118408
I0130 14:44:22.421543 139741768959744 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.7748156785964966, loss=4.985867023468018
I0130 14:45:07.561709 139741760567040 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.5776673555374146, loss=5.021708011627197
I0130 14:45:52.521939 139741768959744 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.0037128925323486, loss=4.913602352142334
I0130 14:46:37.823933 139741760567040 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.7263649702072144, loss=4.995832443237305
I0130 14:47:23.067355 139741768959744 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.1851682662963867, loss=6.309526443481445
I0130 14:48:08.631721 139741760567040 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.9657135009765625, loss=4.900970935821533
I0130 14:48:47.234944 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:48:59.367982 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:49:11.855348 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:49:13.463283 139936116377408 submission_runner.py:408] Time since start: 3627.28s, 	Step: 7486, 	{'train/accuracy': 0.2678515613079071, 'train/loss': 3.742283821105957, 'validation/accuracy': 0.24859999120235443, 'validation/loss': 3.837204694747925, 'validation/num_examples': 50000, 'test/accuracy': 0.18870000541210175, 'test/loss': 4.262177467346191, 'test/num_examples': 10000, 'score': 3404.525137901306, 'total_duration': 3627.2830555438995, 'accumulated_submission_time': 3404.525137901306, 'accumulated_eval_time': 222.11826848983765, 'accumulated_logging_time': 0.23824262619018555}
I0130 14:49:13.487788 139741768959744 logging_writer.py:48] [7486] accumulated_eval_time=222.118268, accumulated_logging_time=0.238243, accumulated_submission_time=3404.525138, global_step=7486, preemption_count=0, score=3404.525138, test/accuracy=0.188700, test/loss=4.262177, test/num_examples=10000, total_duration=3627.283056, train/accuracy=0.267852, train/loss=3.742284, validation/accuracy=0.248600, validation/loss=3.837205, validation/num_examples=50000
I0130 14:49:19.472937 139741760567040 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.6636217832565308, loss=4.769843101501465
I0130 14:50:00.396029 139741768959744 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.4910914897918701, loss=5.091312885284424
I0130 14:50:45.471517 139741760567040 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.6612563133239746, loss=4.818792819976807
I0130 14:51:30.869476 139741768959744 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.682180643081665, loss=4.884369373321533
I0130 14:52:16.312540 139741760567040 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.01019024848938, loss=5.1667680740356445
I0130 14:53:01.390752 139741768959744 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.3780807256698608, loss=6.16922664642334
I0130 14:53:46.706958 139741760567040 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.6807790994644165, loss=4.856742858886719
I0130 14:54:31.783433 139741768959744 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.315602421760559, loss=5.534570693969727
I0130 14:55:16.988910 139741760567040 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.7860560417175293, loss=4.826779365539551
I0130 14:56:02.147086 139741768959744 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.5731655359268188, loss=4.662837982177734
I0130 14:56:13.874782 139936116377408 spec.py:321] Evaluating on the training split.
I0130 14:56:25.850920 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 14:56:43.361909 139936116377408 spec.py:349] Evaluating on the test split.
I0130 14:56:44.982839 139936116377408 submission_runner.py:408] Time since start: 4078.80s, 	Step: 8427, 	{'train/accuracy': 0.3073437511920929, 'train/loss': 3.5018374919891357, 'validation/accuracy': 0.28248000144958496, 'validation/loss': 3.621398448944092, 'validation/num_examples': 50000, 'test/accuracy': 0.2143000066280365, 'test/loss': 4.080821514129639, 'test/num_examples': 10000, 'score': 3824.85044836998, 'total_duration': 4078.8026208877563, 'accumulated_submission_time': 3824.85044836998, 'accumulated_eval_time': 253.22631406784058, 'accumulated_logging_time': 0.2754535675048828}
I0130 14:56:45.001589 139741760567040 logging_writer.py:48] [8427] accumulated_eval_time=253.226314, accumulated_logging_time=0.275454, accumulated_submission_time=3824.850448, global_step=8427, preemption_count=0, score=3824.850448, test/accuracy=0.214300, test/loss=4.080822, test/num_examples=10000, total_duration=4078.802621, train/accuracy=0.307344, train/loss=3.501837, validation/accuracy=0.282480, validation/loss=3.621398, validation/num_examples=50000
I0130 14:57:14.443655 139741768959744 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.8224469423294067, loss=4.72355318069458
I0130 14:57:58.687461 139741760567040 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.4958105087280273, loss=5.406982421875
I0130 14:58:43.951335 139741768959744 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.1203980445861816, loss=6.009604454040527
I0130 14:59:29.421345 139741760567040 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.7965362071990967, loss=4.588654518127441
I0130 15:00:14.634171 139741768959744 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.2199009656906128, loss=5.842665195465088
I0130 15:00:59.656565 139741760567040 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.4874722957611084, loss=4.933285713195801
I0130 15:01:45.204166 139741768959744 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.8032910823822021, loss=4.583003044128418
I0130 15:02:30.609094 139741760567040 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.6840686798095703, loss=4.520804405212402
I0130 15:03:15.811976 139741768959744 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.2059276103973389, loss=5.644423484802246
I0130 15:03:45.215244 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:03:57.852091 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:04:09.274807 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:04:10.932988 139936116377408 submission_runner.py:408] Time since start: 4524.75s, 	Step: 9367, 	{'train/accuracy': 0.353339821100235, 'train/loss': 3.1838603019714355, 'validation/accuracy': 0.315420001745224, 'validation/loss': 3.375602960586548, 'validation/num_examples': 50000, 'test/accuracy': 0.242000013589859, 'test/loss': 3.8549141883850098, 'test/num_examples': 10000, 'score': 4245.004207611084, 'total_duration': 4524.752751588821, 'accumulated_submission_time': 4245.004207611084, 'accumulated_eval_time': 278.9440326690674, 'accumulated_logging_time': 0.30495738983154297}
I0130 15:04:10.959729 139741760567040 logging_writer.py:48] [9367] accumulated_eval_time=278.944033, accumulated_logging_time=0.304957, accumulated_submission_time=4245.004208, global_step=9367, preemption_count=0, score=4245.004208, test/accuracy=0.242000, test/loss=3.854914, test/num_examples=10000, total_duration=4524.752752, train/accuracy=0.353340, train/loss=3.183860, validation/accuracy=0.315420, validation/loss=3.375603, validation/num_examples=50000
I0130 15:04:24.519939 139741768959744 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.7881317138671875, loss=4.460725784301758
I0130 15:05:06.268830 139741760567040 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.2700763940811157, loss=6.1924614906311035
I0130 15:05:51.668293 139741768959744 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.4239256381988525, loss=5.949153900146484
I0130 15:06:37.360335 139741760567040 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.151298999786377, loss=6.043115615844727
I0130 15:07:22.638553 139741768959744 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.5978387594223022, loss=4.4552459716796875
I0130 15:08:08.124691 139741760567040 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.1169373989105225, loss=6.097369194030762
I0130 15:08:53.344506 139741768959744 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.923211693763733, loss=4.378747463226318
I0130 15:09:38.507842 139741760567040 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.0659624338150024, loss=6.089102268218994
I0130 15:10:23.981745 139741768959744 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.4592726230621338, loss=5.520472049713135
I0130 15:11:09.297356 139741760567040 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.1690868139266968, loss=5.4197869300842285
I0130 15:11:11.330918 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:11:23.340116 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:11:38.375141 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:11:40.003430 139936116377408 submission_runner.py:408] Time since start: 4973.82s, 	Step: 10306, 	{'train/accuracy': 0.37101560831069946, 'train/loss': 3.0405492782592773, 'validation/accuracy': 0.34401997923851013, 'validation/loss': 3.174036741256714, 'validation/num_examples': 50000, 'test/accuracy': 0.26600000262260437, 'test/loss': 3.685535430908203, 'test/num_examples': 10000, 'score': 4665.314122676849, 'total_duration': 4973.823203802109, 'accumulated_submission_time': 4665.314122676849, 'accumulated_eval_time': 307.616507768631, 'accumulated_logging_time': 0.3441452980041504}
I0130 15:11:40.022779 139741768959744 logging_writer.py:48] [10306] accumulated_eval_time=307.616508, accumulated_logging_time=0.344145, accumulated_submission_time=4665.314123, global_step=10306, preemption_count=0, score=4665.314123, test/accuracy=0.266000, test/loss=3.685535, test/num_examples=10000, total_duration=4973.823204, train/accuracy=0.371016, train/loss=3.040549, validation/accuracy=0.344020, validation/loss=3.174037, validation/num_examples=50000
I0130 15:12:17.939646 139741760567040 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.4706628322601318, loss=5.0122175216674805
I0130 15:13:03.611842 139741768959744 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.7354456186294556, loss=4.997095108032227
I0130 15:13:49.131325 139741760567040 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.5101662874221802, loss=4.38527774810791
I0130 15:14:34.412189 139741768959744 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.7639055252075195, loss=4.413652420043945
I0130 15:15:19.866997 139741760567040 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.447096347808838, loss=4.369044303894043
I0130 15:16:05.550800 139741768959744 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.139418363571167, loss=5.306131362915039
I0130 15:16:51.016538 139741760567040 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.5757734775543213, loss=4.271786689758301
I0130 15:17:36.208663 139741768959744 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.5169947147369385, loss=4.3066301345825195
I0130 15:18:21.627853 139741760567040 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.496589183807373, loss=4.436069011688232
I0130 15:18:40.285035 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:18:53.781486 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:19:06.336083 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:19:07.948442 139936116377408 submission_runner.py:408] Time since start: 5421.77s, 	Step: 11243, 	{'train/accuracy': 0.3888476490974426, 'train/loss': 2.948613166809082, 'validation/accuracy': 0.36027997732162476, 'validation/loss': 3.093329668045044, 'validation/num_examples': 50000, 'test/accuracy': 0.27720001339912415, 'test/loss': 3.618096351623535, 'test/num_examples': 10000, 'score': 5085.516779184341, 'total_duration': 5421.768236398697, 'accumulated_submission_time': 5085.516779184341, 'accumulated_eval_time': 335.2799074649811, 'accumulated_logging_time': 0.3747379779815674}
I0130 15:19:07.969686 139741768959744 logging_writer.py:48] [11243] accumulated_eval_time=335.279907, accumulated_logging_time=0.374738, accumulated_submission_time=5085.516779, global_step=11243, preemption_count=0, score=5085.516779, test/accuracy=0.277200, test/loss=3.618096, test/num_examples=10000, total_duration=5421.768236, train/accuracy=0.388848, train/loss=2.948613, validation/accuracy=0.360280, validation/loss=3.093330, validation/num_examples=50000
I0130 15:19:31.065657 139741760567040 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.3768397569656372, loss=4.357163429260254
I0130 15:20:15.335408 139741768959744 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.1241627931594849, loss=5.935842514038086
I0130 15:21:00.515199 139741760567040 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.3704191446304321, loss=4.8967814445495605
I0130 15:21:46.202569 139741768959744 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.8054312467575073, loss=4.293083667755127
I0130 15:22:31.487880 139741760567040 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.1390453577041626, loss=5.39995002746582
I0130 15:23:16.635072 139741768959744 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.6318106651306152, loss=4.220737457275391
I0130 15:24:01.583739 139741760567040 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0817549228668213, loss=5.955562114715576
I0130 15:24:46.879842 139741768959744 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.6022518873214722, loss=4.056201457977295
I0130 15:25:32.125205 139741760567040 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.4730206727981567, loss=4.669613838195801
I0130 15:26:08.108730 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:26:21.107660 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:26:38.214434 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:26:39.843838 139936116377408 submission_runner.py:408] Time since start: 5873.66s, 	Step: 12180, 	{'train/accuracy': 0.4136328101158142, 'train/loss': 2.8385469913482666, 'validation/accuracy': 0.37389999628067017, 'validation/loss': 3.021648406982422, 'validation/num_examples': 50000, 'test/accuracy': 0.28790000081062317, 'test/loss': 3.568051338195801, 'test/num_examples': 10000, 'score': 5505.593339443207, 'total_duration': 5873.66360616684, 'accumulated_submission_time': 5505.593339443207, 'accumulated_eval_time': 367.0149767398834, 'accumulated_logging_time': 0.4094424247741699}
I0130 15:26:39.866011 139741768959744 logging_writer.py:48] [12180] accumulated_eval_time=367.014977, accumulated_logging_time=0.409442, accumulated_submission_time=5505.593339, global_step=12180, preemption_count=0, score=5505.593339, test/accuracy=0.287900, test/loss=3.568051, test/num_examples=10000, total_duration=5873.663606, train/accuracy=0.413633, train/loss=2.838547, validation/accuracy=0.373900, validation/loss=3.021648, validation/num_examples=50000
I0130 15:26:48.217604 139741760567040 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.2776784896850586, loss=4.304296493530273
I0130 15:27:29.108757 139741768959744 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.7131478786468506, loss=4.126785755157471
I0130 15:28:15.045664 139741760567040 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.5979684591293335, loss=4.2983293533325195
I0130 15:29:00.823234 139741768959744 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.9700018167495728, loss=4.136612892150879
I0130 15:29:46.535611 139741760567040 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.3084913492202759, loss=4.713408470153809
I0130 15:30:32.043944 139741768959744 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.403967022895813, loss=4.163977146148682
I0130 15:31:17.709169 139741760567040 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.1788197755813599, loss=5.3377532958984375
I0130 15:32:03.520147 139741768959744 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.342603325843811, loss=4.146725654602051
I0130 15:32:48.961685 139741760567040 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.3902970552444458, loss=4.101583957672119
I0130 15:33:34.815429 139741768959744 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.3730872869491577, loss=4.229142665863037
I0130 15:33:40.009785 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:33:53.613778 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:34:11.737906 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:34:13.349639 139936116377408 submission_runner.py:408] Time since start: 6327.17s, 	Step: 13113, 	{'train/accuracy': 0.4274218678474426, 'train/loss': 2.7390637397766113, 'validation/accuracy': 0.39733999967575073, 'validation/loss': 2.8888654708862305, 'validation/num_examples': 50000, 'test/accuracy': 0.3093000054359436, 'test/loss': 3.4295434951782227, 'test/num_examples': 10000, 'score': 5925.677830457687, 'total_duration': 6327.169443368912, 'accumulated_submission_time': 5925.677830457687, 'accumulated_eval_time': 400.35482573509216, 'accumulated_logging_time': 0.441788911819458}
I0130 15:34:13.366474 139741760567040 logging_writer.py:48] [13113] accumulated_eval_time=400.354826, accumulated_logging_time=0.441789, accumulated_submission_time=5925.677830, global_step=13113, preemption_count=0, score=5925.677830, test/accuracy=0.309300, test/loss=3.429543, test/num_examples=10000, total_duration=6327.169443, train/accuracy=0.427422, train/loss=2.739064, validation/accuracy=0.397340, validation/loss=2.888865, validation/num_examples=50000
I0130 15:34:48.332757 139741768959744 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.719050645828247, loss=4.107598304748535
I0130 15:35:32.260094 139741760567040 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4494153261184692, loss=4.078512668609619
I0130 15:36:17.979633 139741768959744 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9878590106964111, loss=6.054774761199951
I0130 15:37:03.695518 139741760567040 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3848507404327393, loss=4.1127214431762695
I0130 15:37:49.581797 139741768959744 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.369407057762146, loss=5.415308475494385
I0130 15:38:35.256218 139741760567040 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0953524112701416, loss=5.330229759216309
I0130 15:39:20.909597 139741768959744 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1432790756225586, loss=4.954446792602539
I0130 15:40:06.744469 139741760567040 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9169301390647888, loss=5.723358631134033
I0130 15:40:52.089977 139741768959744 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.6359279155731201, loss=4.074666500091553
I0130 15:41:13.619480 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:41:27.710176 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:41:48.815574 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:41:50.421696 139936116377408 submission_runner.py:408] Time since start: 6784.24s, 	Step: 14049, 	{'train/accuracy': 0.44376951456069946, 'train/loss': 2.6229381561279297, 'validation/accuracy': 0.41033998131752014, 'validation/loss': 2.784433364868164, 'validation/num_examples': 50000, 'test/accuracy': 0.3188000023365021, 'test/loss': 3.3676393032073975, 'test/num_examples': 10000, 'score': 6345.869318246841, 'total_duration': 6784.241494894028, 'accumulated_submission_time': 6345.869318246841, 'accumulated_eval_time': 437.1570258140564, 'accumulated_logging_time': 0.47129106521606445}
I0130 15:41:50.441715 139741760567040 logging_writer.py:48] [14049] accumulated_eval_time=437.157026, accumulated_logging_time=0.471291, accumulated_submission_time=6345.869318, global_step=14049, preemption_count=0, score=6345.869318, test/accuracy=0.318800, test/loss=3.367639, test/num_examples=10000, total_duration=6784.241495, train/accuracy=0.443770, train/loss=2.622938, validation/accuracy=0.410340, validation/loss=2.784433, validation/num_examples=50000
I0130 15:42:11.079167 139741768959744 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.4783611297607422, loss=4.038317680358887
I0130 15:42:53.409754 139741760567040 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.6438761949539185, loss=4.094578266143799
I0130 15:43:39.100436 139741768959744 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.040342926979065, loss=5.214268207550049
I0130 15:44:24.585142 139741760567040 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.5043588876724243, loss=4.153425216674805
I0130 15:45:09.996903 139741768959744 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4801243543624878, loss=3.9621007442474365
I0130 15:45:55.000275 139741760567040 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.3483151197433472, loss=4.078253269195557
I0130 15:46:40.705778 139741768959744 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.4424864053726196, loss=3.9575581550598145
I0130 15:47:26.661092 139741760567040 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.3654576539993286, loss=4.084779739379883
I0130 15:48:12.352752 139741768959744 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0136103630065918, loss=6.073407173156738
I0130 15:48:50.422312 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:49:05.385875 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:49:25.220427 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:49:26.833984 139936116377408 submission_runner.py:408] Time since start: 7240.65s, 	Step: 14985, 	{'train/accuracy': 0.4652148187160492, 'train/loss': 2.516399383544922, 'validation/accuracy': 0.4221400022506714, 'validation/loss': 2.725985288619995, 'validation/num_examples': 50000, 'test/accuracy': 0.32660001516342163, 'test/loss': 3.2856686115264893, 'test/num_examples': 10000, 'score': 6765.7920479774475, 'total_duration': 7240.653775691986, 'accumulated_submission_time': 6765.7920479774475, 'accumulated_eval_time': 473.5686767101288, 'accumulated_logging_time': 0.5005159378051758}
I0130 15:49:26.854135 139741760567040 logging_writer.py:48] [14985] accumulated_eval_time=473.568677, accumulated_logging_time=0.500516, accumulated_submission_time=6765.792048, global_step=14985, preemption_count=0, score=6765.792048, test/accuracy=0.326600, test/loss=3.285669, test/num_examples=10000, total_duration=7240.653776, train/accuracy=0.465215, train/loss=2.516399, validation/accuracy=0.422140, validation/loss=2.725985, validation/num_examples=50000
I0130 15:49:33.220345 139741768959744 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.2760416269302368, loss=4.769414901733398
I0130 15:50:14.661623 139741760567040 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.4831547737121582, loss=4.051008224487305
I0130 15:51:00.269247 139741768959744 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.3265115022659302, loss=4.458902359008789
I0130 15:51:46.331423 139741760567040 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.5147571563720703, loss=4.32001256942749
I0130 15:52:32.294795 139741768959744 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.3316445350646973, loss=3.916020154953003
I0130 15:53:18.129844 139741760567040 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9367555975914001, loss=5.651531219482422
I0130 15:54:03.698321 139741768959744 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.2364698648452759, loss=4.909947872161865
I0130 15:54:49.631340 139741760567040 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.2933769226074219, loss=3.934527635574341
I0130 15:55:35.322114 139741768959744 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.8705866932868958, loss=5.901880264282227
I0130 15:56:21.169542 139741760567040 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.2686561346054077, loss=4.238343715667725
I0130 15:56:27.247164 139936116377408 spec.py:321] Evaluating on the training split.
I0130 15:56:41.764033 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 15:56:58.383030 139936116377408 spec.py:349] Evaluating on the test split.
I0130 15:56:59.989907 139936116377408 submission_runner.py:408] Time since start: 7693.81s, 	Step: 15915, 	{'train/accuracy': 0.4888085722923279, 'train/loss': 2.4176840782165527, 'validation/accuracy': 0.43361997604370117, 'validation/loss': 2.675327777862549, 'validation/num_examples': 50000, 'test/accuracy': 0.33230000734329224, 'test/loss': 3.2542564868927, 'test/num_examples': 10000, 'score': 7186.127139806747, 'total_duration': 7693.80969619751, 'accumulated_submission_time': 7186.127139806747, 'accumulated_eval_time': 506.31141448020935, 'accumulated_logging_time': 0.5309438705444336}
I0130 15:57:00.010103 139741768959744 logging_writer.py:48] [15915] accumulated_eval_time=506.311414, accumulated_logging_time=0.530944, accumulated_submission_time=7186.127140, global_step=15915, preemption_count=0, score=7186.127140, test/accuracy=0.332300, test/loss=3.254256, test/num_examples=10000, total_duration=7693.809696, train/accuracy=0.488809, train/loss=2.417684, validation/accuracy=0.433620, validation/loss=2.675328, validation/num_examples=50000
I0130 15:57:35.870454 139741760567040 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.2693049907684326, loss=4.672481060028076
I0130 15:58:21.730890 139741768959744 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.9704217314720154, loss=5.841692924499512
I0130 15:59:07.352319 139741760567040 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.5933533906936646, loss=3.997211456298828
I0130 15:59:53.239578 139741768959744 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.4166265726089478, loss=4.061657428741455
I0130 16:00:38.986149 139741760567040 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.3749676942825317, loss=4.013110637664795
I0130 16:01:25.099405 139741768959744 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.2934701442718506, loss=3.8671674728393555
I0130 16:02:11.394112 139741760567040 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.5004644393920898, loss=4.033512115478516
I0130 16:02:57.114247 139741768959744 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.3756238222122192, loss=3.9248499870300293
I0130 16:03:43.133840 139741760567040 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.4708693027496338, loss=4.037065505981445
I0130 16:04:00.213716 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:04:15.177437 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:04:36.407778 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:04:38.023504 139936116377408 submission_runner.py:408] Time since start: 8151.84s, 	Step: 16839, 	{'train/accuracy': 0.4630078077316284, 'train/loss': 2.538491725921631, 'validation/accuracy': 0.43459999561309814, 'validation/loss': 2.679887056350708, 'validation/num_examples': 50000, 'test/accuracy': 0.33740001916885376, 'test/loss': 3.2630906105041504, 'test/num_examples': 10000, 'score': 7606.272791385651, 'total_duration': 8151.8433039188385, 'accumulated_submission_time': 7606.272791385651, 'accumulated_eval_time': 544.1211948394775, 'accumulated_logging_time': 0.5615954399108887}
I0130 16:04:38.041732 139741768959744 logging_writer.py:48] [16839] accumulated_eval_time=544.121195, accumulated_logging_time=0.561595, accumulated_submission_time=7606.272791, global_step=16839, preemption_count=0, score=7606.272791, test/accuracy=0.337400, test/loss=3.263091, test/num_examples=10000, total_duration=8151.843304, train/accuracy=0.463008, train/loss=2.538492, validation/accuracy=0.434600, validation/loss=2.679887, validation/num_examples=50000
I0130 16:05:02.679413 139741760567040 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.0725430250167847, loss=5.026483535766602
I0130 16:05:46.410020 139741768959744 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.5270644426345825, loss=4.021817684173584
I0130 16:06:32.197967 139741760567040 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4549287557601929, loss=3.861456871032715
I0130 16:07:17.954252 139741768959744 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.5885839462280273, loss=4.0249552726745605
I0130 16:08:04.031234 139741760567040 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.3983619213104248, loss=3.8491358757019043
I0130 16:08:50.053516 139741768959744 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.2354528903961182, loss=4.615017890930176
I0130 16:09:36.197313 139741760567040 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.6048232316970825, loss=3.986241102218628
I0130 16:10:21.873360 139741768959744 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.340892791748047, loss=3.9257984161376953
I0130 16:11:07.710419 139741760567040 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.3337092399597168, loss=3.9227499961853027
I0130 16:11:38.489117 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:11:53.204263 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:12:12.157298 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:12:13.764564 139936116377408 submission_runner.py:408] Time since start: 8607.58s, 	Step: 17769, 	{'train/accuracy': 0.4804101586341858, 'train/loss': 2.426698684692383, 'validation/accuracy': 0.44652000069618225, 'validation/loss': 2.5907909870147705, 'validation/num_examples': 50000, 'test/accuracy': 0.34170001745224, 'test/loss': 3.178121328353882, 'test/num_examples': 10000, 'score': 8026.660900354385, 'total_duration': 8607.584362506866, 'accumulated_submission_time': 8026.660900354385, 'accumulated_eval_time': 579.3966374397278, 'accumulated_logging_time': 0.5898916721343994}
I0130 16:12:13.782375 139741768959744 logging_writer.py:48] [17769] accumulated_eval_time=579.396637, accumulated_logging_time=0.589892, accumulated_submission_time=8026.660900, global_step=17769, preemption_count=0, score=8026.660900, test/accuracy=0.341700, test/loss=3.178121, test/num_examples=10000, total_duration=8607.584363, train/accuracy=0.480410, train/loss=2.426699, validation/accuracy=0.446520, validation/loss=2.590791, validation/num_examples=50000
I0130 16:12:26.492890 139741760567040 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.3102383613586426, loss=4.109201431274414
I0130 16:13:08.420400 139741768959744 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.497939109802246, loss=3.951738119125366
I0130 16:13:53.924164 139741760567040 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.6227710247039795, loss=3.9690351486206055
I0130 16:14:39.889598 139741768959744 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9336596727371216, loss=5.850613117218018
I0130 16:15:25.358244 139741760567040 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.4521262645721436, loss=3.8747501373291016
I0130 16:16:11.079308 139741768959744 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.5881537199020386, loss=4.026778697967529
I0130 16:16:56.598717 139741760567040 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8679644465446472, loss=5.847249984741211
I0130 16:17:42.250703 139741768959744 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.344365119934082, loss=3.936944007873535
I0130 16:18:28.047822 139741760567040 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.3627561330795288, loss=3.836024761199951
I0130 16:19:13.767447 139741768959744 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.3260340690612793, loss=3.919829845428467
I0130 16:19:13.783824 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:19:28.418980 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:19:46.651232 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:19:48.267922 139936116377408 submission_runner.py:408] Time since start: 9062.09s, 	Step: 18701, 	{'train/accuracy': 0.5080273151397705, 'train/loss': 2.2943875789642334, 'validation/accuracy': 0.4573400020599365, 'validation/loss': 2.531662702560425, 'validation/num_examples': 50000, 'test/accuracy': 0.3521000146865845, 'test/loss': 3.1127684116363525, 'test/num_examples': 10000, 'score': 8446.603140592575, 'total_duration': 9062.087691783905, 'accumulated_submission_time': 8446.603140592575, 'accumulated_eval_time': 613.8807055950165, 'accumulated_logging_time': 0.6182742118835449}
I0130 16:19:48.289728 139741760567040 logging_writer.py:48] [18701] accumulated_eval_time=613.880706, accumulated_logging_time=0.618274, accumulated_submission_time=8446.603141, global_step=18701, preemption_count=0, score=8446.603141, test/accuracy=0.352100, test/loss=3.112768, test/num_examples=10000, total_duration=9062.087692, train/accuracy=0.508027, train/loss=2.294388, validation/accuracy=0.457340, validation/loss=2.531663, validation/num_examples=50000
I0130 16:20:29.652569 139741768959744 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.3982720375061035, loss=3.837881088256836
I0130 16:21:14.856382 139741760567040 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.386696457862854, loss=3.817986488342285
I0130 16:22:00.346137 139741768959744 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.5315526723861694, loss=3.8139946460723877
I0130 16:22:46.072394 139741760567040 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.2672059535980225, loss=5.325793266296387
I0130 16:23:32.159147 139741768959744 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.36058509349823, loss=4.023496627807617
I0130 16:24:18.074688 139741760567040 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.2687300443649292, loss=3.8810336589813232
I0130 16:25:04.223600 139741768959744 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.2128479480743408, loss=4.124999523162842
I0130 16:25:49.961442 139741760567040 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.560595154762268, loss=3.805326461791992
I0130 16:26:35.788479 139741768959744 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.3101940155029297, loss=3.8887925148010254
I0130 16:26:48.658695 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:27:03.412987 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:27:24.138495 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:27:25.750135 139936116377408 submission_runner.py:408] Time since start: 9519.57s, 	Step: 19630, 	{'train/accuracy': 0.49162107706069946, 'train/loss': 2.3779008388519287, 'validation/accuracy': 0.4596799910068512, 'validation/loss': 2.5362136363983154, 'validation/num_examples': 50000, 'test/accuracy': 0.36320000886917114, 'test/loss': 3.1231794357299805, 'test/num_examples': 10000, 'score': 8866.9126598835, 'total_duration': 9519.569938898087, 'accumulated_submission_time': 8866.9126598835, 'accumulated_eval_time': 650.9721372127533, 'accumulated_logging_time': 0.6507935523986816}
I0130 16:27:25.767921 139741760567040 logging_writer.py:48] [19630] accumulated_eval_time=650.972137, accumulated_logging_time=0.650794, accumulated_submission_time=8866.912660, global_step=19630, preemption_count=0, score=8866.912660, test/accuracy=0.363200, test/loss=3.123179, test/num_examples=10000, total_duration=9519.569939, train/accuracy=0.491621, train/loss=2.377901, validation/accuracy=0.459680, validation/loss=2.536214, validation/num_examples=50000
I0130 16:27:53.976781 139741768959744 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.3141659498214722, loss=3.8044800758361816
I0130 16:28:39.170036 139741760567040 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1860144138336182, loss=4.078652858734131
I0130 16:29:24.872535 139741768959744 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.2584009170532227, loss=4.076190948486328
I0130 16:30:10.708181 139741760567040 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.3074852228164673, loss=3.85984468460083
I0130 16:30:56.647804 139741768959744 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.1571398973464966, loss=4.733482360839844
I0130 16:31:42.426687 139741760567040 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8228750824928284, loss=5.86992073059082
I0130 16:32:28.139584 139741768959744 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0540975332260132, loss=4.477332592010498
I0130 16:33:14.140836 139741760567040 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.942706823348999, loss=5.796263217926025
I0130 16:33:59.908109 139741768959744 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9039198160171509, loss=5.826507568359375
I0130 16:34:25.883388 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:34:40.908197 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:35:00.402642 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:35:02.016553 139936116377408 submission_runner.py:408] Time since start: 9975.84s, 	Step: 20558, 	{'train/accuracy': 0.5131444931030273, 'train/loss': 2.2290053367614746, 'validation/accuracy': 0.4773799777030945, 'validation/loss': 2.3989546298980713, 'validation/num_examples': 50000, 'test/accuracy': 0.36740002036094666, 'test/loss': 2.994854211807251, 'test/num_examples': 10000, 'score': 9286.968410253525, 'total_duration': 9975.836344718933, 'accumulated_submission_time': 9286.968410253525, 'accumulated_eval_time': 687.1052870750427, 'accumulated_logging_time': 0.6784398555755615}
I0130 16:35:02.038954 139741760567040 logging_writer.py:48] [20558] accumulated_eval_time=687.105287, accumulated_logging_time=0.678440, accumulated_submission_time=9286.968410, global_step=20558, preemption_count=0, score=9286.968410, test/accuracy=0.367400, test/loss=2.994854, test/num_examples=10000, total_duration=9975.836345, train/accuracy=0.513144, train/loss=2.229005, validation/accuracy=0.477380, validation/loss=2.398955, validation/num_examples=50000
I0130 16:35:19.138686 139741768959744 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.3948441743850708, loss=3.8731749057769775
I0130 16:36:02.202426 139741760567040 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.3984744548797607, loss=3.7716410160064697
I0130 16:36:48.357626 139741768959744 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.3437271118164062, loss=5.328827857971191
I0130 16:37:34.617998 139741760567040 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1299556493759155, loss=4.487828254699707
I0130 16:38:20.727464 139741768959744 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9522876143455505, loss=5.244108200073242
I0130 16:39:06.779179 139741760567040 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.2465778589248657, loss=3.826275587081909
I0130 16:39:53.185335 139741768959744 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.4406424760818481, loss=3.95023512840271
I0130 16:40:39.477098 139741760567040 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8010690808296204, loss=5.817972183227539
I0130 16:41:25.349166 139741768959744 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.1822850704193115, loss=4.394797325134277
I0130 16:42:02.444569 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:42:17.836705 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:42:33.563335 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:42:35.180716 139936116377408 submission_runner.py:408] Time since start: 10429.00s, 	Step: 21482, 	{'train/accuracy': 0.5301367044448853, 'train/loss': 2.1761655807495117, 'validation/accuracy': 0.48363998532295227, 'validation/loss': 2.3937957286834717, 'validation/num_examples': 50000, 'test/accuracy': 0.37720000743865967, 'test/loss': 3.007101535797119, 'test/num_examples': 10000, 'score': 9707.312128067017, 'total_duration': 10429.00049996376, 'accumulated_submission_time': 9707.312128067017, 'accumulated_eval_time': 719.8414082527161, 'accumulated_logging_time': 0.7146031856536865}
I0130 16:42:35.204151 139741760567040 logging_writer.py:48] [21482] accumulated_eval_time=719.841408, accumulated_logging_time=0.714603, accumulated_submission_time=9707.312128, global_step=21482, preemption_count=0, score=9707.312128, test/accuracy=0.377200, test/loss=3.007102, test/num_examples=10000, total_duration=10429.000500, train/accuracy=0.530137, train/loss=2.176166, validation/accuracy=0.483640, validation/loss=2.393796, validation/num_examples=50000
I0130 16:42:42.783346 139741768959744 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8753533363342285, loss=5.756853103637695
I0130 16:43:30.072521 139741760567040 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.853537380695343, loss=5.687702178955078
I0130 16:44:19.100820 139741768959744 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8319389224052429, loss=5.544749736785889
I0130 16:45:08.619863 139741760567040 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2513765096664429, loss=3.7932486534118652
I0130 16:45:58.135344 139741768959744 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.5444650650024414, loss=3.8573381900787354
I0130 16:46:47.703448 139741760567040 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.3049691915512085, loss=3.7670130729675293
I0130 16:47:36.988790 139741768959744 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1879253387451172, loss=3.901007890701294
I0130 16:48:26.549639 139741760567040 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2393594980239868, loss=4.625145435333252
I0130 16:49:16.428941 139741768959744 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.2684111595153809, loss=3.7156853675842285
I0130 16:49:35.538576 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:49:47.984279 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:50:10.451508 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:50:12.060473 139936116377408 submission_runner.py:408] Time since start: 10885.88s, 	Step: 22340, 	{'train/accuracy': 0.5262304544448853, 'train/loss': 2.1738510131835938, 'validation/accuracy': 0.49164000153541565, 'validation/loss': 2.3356337547302246, 'validation/num_examples': 50000, 'test/accuracy': 0.3882000148296356, 'test/loss': 2.9433727264404297, 'test/num_examples': 10000, 'score': 10127.587964057922, 'total_duration': 10885.880244970322, 'accumulated_submission_time': 10127.587964057922, 'accumulated_eval_time': 756.3632752895355, 'accumulated_logging_time': 0.7525720596313477}
I0130 16:50:12.079305 139741760567040 logging_writer.py:48] [22340] accumulated_eval_time=756.363275, accumulated_logging_time=0.752572, accumulated_submission_time=10127.587964, global_step=22340, preemption_count=0, score=10127.587964, test/accuracy=0.388200, test/loss=2.943373, test/num_examples=10000, total_duration=10885.880245, train/accuracy=0.526230, train/loss=2.173851, validation/accuracy=0.491640, validation/loss=2.335634, validation/num_examples=50000
I0130 16:50:36.336652 139741768959744 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.9638262987136841, loss=5.414488315582275
I0130 16:51:20.184758 139741760567040 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.158381700515747, loss=4.325053691864014
I0130 16:52:06.023051 139741768959744 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.1178196668624878, loss=4.109893798828125
I0130 16:52:52.027835 139741760567040 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.5758025646209717, loss=3.755181312561035
I0130 16:53:37.969707 139741768959744 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.2079139947891235, loss=4.167370319366455
I0130 16:54:23.970001 139741760567040 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.4014790058135986, loss=3.927443265914917
I0130 16:55:09.701951 139741768959744 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.3121428489685059, loss=3.7811288833618164
I0130 16:55:55.639264 139741760567040 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.4024900197982788, loss=3.6953682899475098
I0130 16:56:41.510576 139741768959744 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.308166742324829, loss=3.707350492477417
I0130 16:57:12.368458 139936116377408 spec.py:321] Evaluating on the training split.
I0130 16:57:23.799164 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 16:57:46.771811 139936116377408 spec.py:349] Evaluating on the test split.
I0130 16:57:48.381802 139936116377408 submission_runner.py:408] Time since start: 11342.20s, 	Step: 23269, 	{'train/accuracy': 0.5324609279632568, 'train/loss': 2.129218816757202, 'validation/accuracy': 0.4975399971008301, 'validation/loss': 2.299388885498047, 'validation/num_examples': 50000, 'test/accuracy': 0.38840001821517944, 'test/loss': 2.9122867584228516, 'test/num_examples': 10000, 'score': 10547.819781303406, 'total_duration': 11342.201602220535, 'accumulated_submission_time': 10547.819781303406, 'accumulated_eval_time': 792.3766157627106, 'accumulated_logging_time': 0.7804503440856934}
I0130 16:57:48.403803 139741760567040 logging_writer.py:48] [23269] accumulated_eval_time=792.376616, accumulated_logging_time=0.780450, accumulated_submission_time=10547.819781, global_step=23269, preemption_count=0, score=10547.819781, test/accuracy=0.388400, test/loss=2.912287, test/num_examples=10000, total_duration=11342.201602, train/accuracy=0.532461, train/loss=2.129219, validation/accuracy=0.497540, validation/loss=2.299389, validation/num_examples=50000
I0130 16:58:01.120017 139741768959744 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.0891423225402832, loss=4.730091094970703
I0130 16:58:43.217965 139741760567040 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.3348937034606934, loss=3.7373204231262207
I0130 16:59:29.011623 139741768959744 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9685273766517639, loss=5.556176662445068
I0130 17:00:14.998566 139741760567040 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.3625383377075195, loss=4.287515640258789
I0130 17:01:00.325063 139741768959744 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.430145502090454, loss=3.616443395614624
I0130 17:01:46.451060 139741760567040 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.25447678565979, loss=4.023900985717773
I0130 17:02:32.281477 139741768959744 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.3518035411834717, loss=3.7960128784179688
I0130 17:03:17.868688 139741760567040 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2004833221435547, loss=4.696959972381592
I0130 17:04:03.890008 139741768959744 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.2310792207717896, loss=3.998351573944092
I0130 17:04:48.409276 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:05:00.184563 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:05:23.581381 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:05:25.195105 139936116377408 submission_runner.py:408] Time since start: 11799.01s, 	Step: 24199, 	{'train/accuracy': 0.5442578196525574, 'train/loss': 2.104851245880127, 'validation/accuracy': 0.498339980840683, 'validation/loss': 2.3141963481903076, 'validation/num_examples': 50000, 'test/accuracy': 0.3945000171661377, 'test/loss': 2.9152963161468506, 'test/num_examples': 10000, 'score': 10967.767451047897, 'total_duration': 11799.014899253845, 'accumulated_submission_time': 10967.767451047897, 'accumulated_eval_time': 829.1624467372894, 'accumulated_logging_time': 0.8119678497314453}
I0130 17:05:25.216955 139741760567040 logging_writer.py:48] [24199] accumulated_eval_time=829.162447, accumulated_logging_time=0.811968, accumulated_submission_time=10967.767451, global_step=24199, preemption_count=0, score=10967.767451, test/accuracy=0.394500, test/loss=2.915296, test/num_examples=10000, total_duration=11799.014899, train/accuracy=0.544258, train/loss=2.104851, validation/accuracy=0.498340, validation/loss=2.314196, validation/num_examples=50000
I0130 17:05:26.023182 139741768959744 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.1120587587356567, loss=4.443454742431641
I0130 17:06:06.865924 139741760567040 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.4269603490829468, loss=3.7041685581207275
I0130 17:06:52.464917 139741768959744 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0782536268234253, loss=5.14367151260376
I0130 17:07:38.398818 139741760567040 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.2571358680725098, loss=3.7649049758911133
I0130 17:08:24.150425 139741768959744 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.2564092874526978, loss=3.682513475418091
I0130 17:09:10.176964 139741760567040 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.1706292629241943, loss=4.788079261779785
I0130 17:09:55.874538 139741768959744 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9244620203971863, loss=5.679656028747559
I0130 17:10:41.548614 139741760567040 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.153996229171753, loss=5.455413818359375
I0130 17:11:27.433269 139741768959744 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0336384773254395, loss=5.838314056396484
I0130 17:12:13.563961 139741760567040 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.2332541942596436, loss=4.149996280670166
I0130 17:12:25.550469 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:12:37.466222 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:13:00.318261 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:13:01.923337 139936116377408 submission_runner.py:408] Time since start: 12255.74s, 	Step: 25128, 	{'train/accuracy': 0.5494335889816284, 'train/loss': 2.1070497035980225, 'validation/accuracy': 0.5054199695587158, 'validation/loss': 2.3007595539093018, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.9255411624908447, 'test/num_examples': 10000, 'score': 11388.04214978218, 'total_duration': 12255.743125915527, 'accumulated_submission_time': 11388.04214978218, 'accumulated_eval_time': 865.5352845191956, 'accumulated_logging_time': 0.8451135158538818}
I0130 17:13:01.950789 139741768959744 logging_writer.py:48] [25128] accumulated_eval_time=865.535285, accumulated_logging_time=0.845114, accumulated_submission_time=11388.042150, global_step=25128, preemption_count=0, score=11388.042150, test/accuracy=0.397600, test/loss=2.925541, test/num_examples=10000, total_duration=12255.743126, train/accuracy=0.549434, train/loss=2.107050, validation/accuracy=0.505420, validation/loss=2.300760, validation/num_examples=50000
I0130 17:13:30.970845 139741760567040 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0361422300338745, loss=4.745290756225586
I0130 17:14:16.182221 139741768959744 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9812695980072021, loss=5.451280117034912
I0130 17:15:04.502151 139741760567040 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.2405409812927246, loss=3.623426914215088
I0130 17:15:51.410738 139741768959744 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.323865532875061, loss=3.6927387714385986
I0130 17:16:37.352927 139741760567040 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0493083000183105, loss=4.895125865936279
I0130 17:17:23.452629 139741768959744 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.2916103601455688, loss=3.8876841068267822
I0130 17:18:09.276633 139741760567040 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.4004032611846924, loss=3.667644500732422
I0130 17:18:55.304922 139741768959744 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2581015825271606, loss=3.862563371658325
I0130 17:19:41.149151 139741760567040 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.397640347480774, loss=3.555752754211426
I0130 17:20:01.979774 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:20:13.636923 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:20:36.111781 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:20:37.716938 139936116377408 submission_runner.py:408] Time since start: 12711.54s, 	Step: 26047, 	{'train/accuracy': 0.5480077862739563, 'train/loss': 2.075366735458374, 'validation/accuracy': 0.5156199932098389, 'validation/loss': 2.2329912185668945, 'validation/num_examples': 50000, 'test/accuracy': 0.4003000259399414, 'test/loss': 2.8565263748168945, 'test/num_examples': 10000, 'score': 11808.012126922607, 'total_duration': 12711.5367333889, 'accumulated_submission_time': 11808.012126922607, 'accumulated_eval_time': 901.2724430561066, 'accumulated_logging_time': 0.883490800857544}
I0130 17:20:37.744078 139741768959744 logging_writer.py:48] [26047] accumulated_eval_time=901.272443, accumulated_logging_time=0.883491, accumulated_submission_time=11808.012127, global_step=26047, preemption_count=0, score=11808.012127, test/accuracy=0.400300, test/loss=2.856526, test/num_examples=10000, total_duration=12711.536733, train/accuracy=0.548008, train/loss=2.075367, validation/accuracy=0.515620, validation/loss=2.232991, validation/num_examples=50000
I0130 17:20:59.215717 139741760567040 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.0986378192901611, loss=5.730767250061035
I0130 17:21:43.098959 139741768959744 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.2770380973815918, loss=3.921492576599121
I0130 17:22:28.969155 139741760567040 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.4793636798858643, loss=3.58249831199646
I0130 17:23:15.170345 139741768959744 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.1223313808441162, loss=4.368575096130371
I0130 17:24:00.876875 139741760567040 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.4929481744766235, loss=3.60318660736084
I0130 17:24:47.020642 139741768959744 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.4287294149398804, loss=3.64963436126709
I0130 17:25:32.756129 139741760567040 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.335708737373352, loss=3.7388575077056885
I0130 17:26:18.677377 139741768959744 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.0041837692260742, loss=5.448441505432129
I0130 17:27:04.570431 139741760567040 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.0742602348327637, loss=4.839416980743408
I0130 17:27:37.793145 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:27:49.660528 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:28:11.655894 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:28:13.273687 139936116377408 submission_runner.py:408] Time since start: 13167.09s, 	Step: 26974, 	{'train/accuracy': 0.5512499809265137, 'train/loss': 2.059241533279419, 'validation/accuracy': 0.510919988155365, 'validation/loss': 2.2513058185577393, 'validation/num_examples': 50000, 'test/accuracy': 0.39570000767707825, 'test/loss': 2.879225015640259, 'test/num_examples': 10000, 'score': 12228.003628730774, 'total_duration': 13167.093488931656, 'accumulated_submission_time': 12228.003628730774, 'accumulated_eval_time': 936.7529728412628, 'accumulated_logging_time': 0.9200353622436523}
I0130 17:28:13.294579 139741768959744 logging_writer.py:48] [26974] accumulated_eval_time=936.752973, accumulated_logging_time=0.920035, accumulated_submission_time=12228.003629, global_step=26974, preemption_count=0, score=12228.003629, test/accuracy=0.395700, test/loss=2.879225, test/num_examples=10000, total_duration=13167.093489, train/accuracy=0.551250, train/loss=2.059242, validation/accuracy=0.510920, validation/loss=2.251306, validation/num_examples=50000
I0130 17:28:24.037963 139741760567040 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.3955276012420654, loss=3.5024452209472656
I0130 17:29:06.618805 139741768959744 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.181014895439148, loss=3.851137638092041
I0130 17:29:52.111370 139741760567040 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.426478385925293, loss=3.727515697479248
I0130 17:30:38.366652 139741768959744 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.3489899635314941, loss=3.5234553813934326
I0130 17:31:23.909323 139741760567040 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.205273985862732, loss=4.90787410736084
I0130 17:32:09.704940 139741768959744 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1270579099655151, loss=3.734928846359253
I0130 17:32:56.142458 139741760567040 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.0166840553283691, loss=5.604180335998535
I0130 17:33:42.018645 139741768959744 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.0322405099868774, loss=4.812196731567383
I0130 17:34:28.135992 139741760567040 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.9932807683944702, loss=5.737479209899902
I0130 17:35:13.425916 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:35:25.710421 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:35:47.628511 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:35:49.239493 139936116377408 submission_runner.py:408] Time since start: 13623.06s, 	Step: 27900, 	{'train/accuracy': 0.5764257907867432, 'train/loss': 1.9356085062026978, 'validation/accuracy': 0.5181599855422974, 'validation/loss': 2.198256015777588, 'validation/num_examples': 50000, 'test/accuracy': 0.40390002727508545, 'test/loss': 2.825509786605835, 'test/num_examples': 10000, 'score': 12648.078053474426, 'total_duration': 13623.059293746948, 'accumulated_submission_time': 12648.078053474426, 'accumulated_eval_time': 972.5665490627289, 'accumulated_logging_time': 0.9505248069763184}
I0130 17:35:49.260253 139741768959744 logging_writer.py:48] [27900] accumulated_eval_time=972.566549, accumulated_logging_time=0.950525, accumulated_submission_time=12648.078053, global_step=27900, preemption_count=0, score=12648.078053, test/accuracy=0.403900, test/loss=2.825510, test/num_examples=10000, total_duration=13623.059294, train/accuracy=0.576426, train/loss=1.935609, validation/accuracy=0.518160, validation/loss=2.198256, validation/num_examples=50000
I0130 17:35:49.655881 139741760567040 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.296062707901001, loss=3.669295310974121
I0130 17:36:30.798517 139741768959744 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.4802664518356323, loss=3.563030958175659
I0130 17:37:16.317037 139741760567040 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3728084564208984, loss=3.751821994781494
I0130 17:38:02.639193 139741768959744 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.237789273262024, loss=4.379944801330566
I0130 17:38:48.587849 139741760567040 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.5140280723571777, loss=3.6140079498291016
I0130 17:39:34.699508 139741768959744 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.2994537353515625, loss=3.5550923347473145
I0130 17:40:20.599411 139741760567040 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.0876353979110718, loss=3.9869303703308105
I0130 17:41:06.650044 139741768959744 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.1931687593460083, loss=3.7612719535827637
I0130 17:41:52.526512 139741760567040 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.3101732730865479, loss=4.806695938110352
I0130 17:42:38.732205 139741768959744 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.0730133056640625, loss=4.976094722747803
I0130 17:42:49.459069 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:43:01.373159 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:43:23.364135 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:43:24.961279 139936116377408 submission_runner.py:408] Time since start: 14078.78s, 	Step: 28825, 	{'train/accuracy': 0.5618749856948853, 'train/loss': 1.990774154663086, 'validation/accuracy': 0.5294600129127502, 'validation/loss': 2.153543472290039, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.7733428478240967, 'test/num_examples': 10000, 'score': 13068.219451665878, 'total_duration': 14078.781080007553, 'accumulated_submission_time': 13068.219451665878, 'accumulated_eval_time': 1008.0687556266785, 'accumulated_logging_time': 0.9811530113220215}
I0130 17:43:24.984550 139741760567040 logging_writer.py:48] [28825] accumulated_eval_time=1008.068756, accumulated_logging_time=0.981153, accumulated_submission_time=13068.219452, global_step=28825, preemption_count=0, score=13068.219452, test/accuracy=0.415400, test/loss=2.773343, test/num_examples=10000, total_duration=14078.781080, train/accuracy=0.561875, train/loss=1.990774, validation/accuracy=0.529460, validation/loss=2.153543, validation/num_examples=50000
I0130 17:43:55.200836 139741768959744 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.499777913093567, loss=4.069689750671387
I0130 17:44:40.648032 139741760567040 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.3145499229431152, loss=3.648695468902588
I0130 17:45:26.999821 139741768959744 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.062333106994629, loss=4.834097862243652
I0130 17:46:13.036516 139741760567040 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.3091551065444946, loss=4.030813217163086
I0130 17:46:58.884297 139741768959744 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.3382775783538818, loss=3.8333497047424316
I0130 17:47:44.836460 139741760567040 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.2729064226150513, loss=3.70335054397583
I0130 17:48:30.722245 139741768959744 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.269273281097412, loss=3.6803810596466064
I0130 17:49:16.391618 139741760567040 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.3578877449035645, loss=3.542609214782715
I0130 17:50:02.003287 139741768959744 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.364384412765503, loss=3.5313165187835693
I0130 17:50:25.345815 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:50:37.143399 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:50:59.515506 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:51:01.124537 139936116377408 submission_runner.py:408] Time since start: 14534.94s, 	Step: 29752, 	{'train/accuracy': 0.5666210651397705, 'train/loss': 1.9570411443710327, 'validation/accuracy': 0.5254200100898743, 'validation/loss': 2.154048442840576, 'validation/num_examples': 50000, 'test/accuracy': 0.414900004863739, 'test/loss': 2.78692626953125, 'test/num_examples': 10000, 'score': 13488.522999286652, 'total_duration': 14534.944336652756, 'accumulated_submission_time': 13488.522999286652, 'accumulated_eval_time': 1043.8474705219269, 'accumulated_logging_time': 1.0138554573059082}
I0130 17:51:01.143064 139741760567040 logging_writer.py:48] [29752] accumulated_eval_time=1043.847471, accumulated_logging_time=1.013855, accumulated_submission_time=13488.522999, global_step=29752, preemption_count=0, score=13488.522999, test/accuracy=0.414900, test/loss=2.786926, test/num_examples=10000, total_duration=14534.944337, train/accuracy=0.566621, train/loss=1.957041, validation/accuracy=0.525420, validation/loss=2.154048, validation/num_examples=50000
I0130 17:51:20.647707 139741768959744 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.4275908470153809, loss=3.547421932220459
I0130 17:52:04.435103 139741760567040 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.398733377456665, loss=3.533944845199585
I0130 17:52:49.886898 139741768959744 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.5866906642913818, loss=3.543093681335449
I0130 17:53:36.062660 139741760567040 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.4256447553634644, loss=3.971482276916504
I0130 17:54:21.509893 139741768959744 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.1672123670578003, loss=4.391115188598633
I0130 17:55:07.285704 139741760567040 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.3340736627578735, loss=3.6177468299865723
I0130 17:55:53.123375 139741768959744 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.0924978256225586, loss=5.415821075439453
I0130 17:56:38.837163 139741760567040 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3028544187545776, loss=3.8769869804382324
I0130 17:57:24.637133 139741768959744 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.630711555480957, loss=3.5851030349731445
I0130 17:58:01.249821 139936116377408 spec.py:321] Evaluating on the training split.
I0130 17:58:13.476195 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 17:58:35.336964 139936116377408 spec.py:349] Evaluating on the test split.
I0130 17:58:36.948887 139936116377408 submission_runner.py:408] Time since start: 14990.77s, 	Step: 30682, 	{'train/accuracy': 0.5687890648841858, 'train/loss': 2.034207344055176, 'validation/accuracy': 0.5227400064468384, 'validation/loss': 2.254662036895752, 'validation/num_examples': 50000, 'test/accuracy': 0.41610002517700195, 'test/loss': 2.8487892150878906, 'test/num_examples': 10000, 'score': 13908.572232484818, 'total_duration': 14990.768680334091, 'accumulated_submission_time': 13908.572232484818, 'accumulated_eval_time': 1079.5465333461761, 'accumulated_logging_time': 1.0415377616882324}
I0130 17:58:36.968030 139741760567040 logging_writer.py:48] [30682] accumulated_eval_time=1079.546533, accumulated_logging_time=1.041538, accumulated_submission_time=13908.572232, global_step=30682, preemption_count=0, score=13908.572232, test/accuracy=0.416100, test/loss=2.848789, test/num_examples=10000, total_duration=14990.768680, train/accuracy=0.568789, train/loss=2.034207, validation/accuracy=0.522740, validation/loss=2.254662, validation/num_examples=50000
I0130 17:58:44.521628 139741768959744 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.170828104019165, loss=4.135777950286865
I0130 17:59:26.555285 139741760567040 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.158128261566162, loss=5.13964319229126
I0130 18:00:12.223433 139741768959744 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.2309612035751343, loss=4.325620174407959
I0130 18:00:58.206719 139741760567040 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.4240328073501587, loss=3.466688394546509
I0130 18:01:44.195075 139741768959744 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.449106216430664, loss=3.6077463626861572
I0130 18:02:30.074687 139741760567040 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.106892704963684, loss=4.867712497711182
I0130 18:03:16.213779 139741768959744 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.1434180736541748, loss=4.707542419433594
I0130 18:04:02.072698 139741760567040 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.3704551458358765, loss=3.523522138595581
I0130 18:04:47.582420 139741768959744 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1950031518936157, loss=4.053246974945068
I0130 18:05:33.373148 139741760567040 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.410502314567566, loss=3.4845404624938965
I0130 18:05:37.217345 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:05:49.093038 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:06:10.612550 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:06:12.213936 139936116377408 submission_runner.py:408] Time since start: 15446.03s, 	Step: 31610, 	{'train/accuracy': 0.5677539110183716, 'train/loss': 1.991437315940857, 'validation/accuracy': 0.531279981136322, 'validation/loss': 2.1594009399414062, 'validation/num_examples': 50000, 'test/accuracy': 0.4204000234603882, 'test/loss': 2.7725627422332764, 'test/num_examples': 10000, 'score': 14328.762746572495, 'total_duration': 15446.033729076385, 'accumulated_submission_time': 14328.762746572495, 'accumulated_eval_time': 1114.5431122779846, 'accumulated_logging_time': 1.0704760551452637}
I0130 18:06:12.236718 139741768959744 logging_writer.py:48] [31610] accumulated_eval_time=1114.543112, accumulated_logging_time=1.070476, accumulated_submission_time=14328.762747, global_step=31610, preemption_count=0, score=14328.762747, test/accuracy=0.420400, test/loss=2.772563, test/num_examples=10000, total_duration=15446.033729, train/accuracy=0.567754, train/loss=1.991437, validation/accuracy=0.531280, validation/loss=2.159401, validation/num_examples=50000
I0130 18:06:48.899952 139741760567040 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.3027002811431885, loss=3.7004995346069336
I0130 18:07:34.539547 139741768959744 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.33570396900177, loss=3.6314196586608887
I0130 18:08:20.594476 139741760567040 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.3068854808807373, loss=3.630784749984741
I0130 18:09:06.353156 139741768959744 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.3204550743103027, loss=3.571338653564453
I0130 18:09:51.857207 139741760567040 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.187639594078064, loss=5.697493553161621
I0130 18:10:37.788818 139741768959744 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.2371050119400024, loss=5.7206010818481445
I0130 18:11:24.110051 139741760567040 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.297165870666504, loss=3.764284372329712
I0130 18:12:09.689776 139741768959744 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.507338523864746, loss=3.6589372158050537
I0130 18:12:55.223931 139741760567040 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.9501277804374695, loss=5.58231782913208
I0130 18:13:12.597008 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:13:24.350387 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:13:46.332241 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:13:47.939938 139936116377408 submission_runner.py:408] Time since start: 15901.76s, 	Step: 32539, 	{'train/accuracy': 0.5756444931030273, 'train/loss': 1.9299191236495972, 'validation/accuracy': 0.5379799604415894, 'validation/loss': 2.113074541091919, 'validation/num_examples': 50000, 'test/accuracy': 0.42490002512931824, 'test/loss': 2.7316830158233643, 'test/num_examples': 10000, 'score': 14749.06371474266, 'total_duration': 15901.75974059105, 'accumulated_submission_time': 14749.06371474266, 'accumulated_eval_time': 1149.8860309123993, 'accumulated_logging_time': 1.1039931774139404}
I0130 18:13:47.964030 139741768959744 logging_writer.py:48] [32539] accumulated_eval_time=1149.886031, accumulated_logging_time=1.103993, accumulated_submission_time=14749.063715, global_step=32539, preemption_count=0, score=14749.063715, test/accuracy=0.424900, test/loss=2.731683, test/num_examples=10000, total_duration=15901.759741, train/accuracy=0.575644, train/loss=1.929919, validation/accuracy=0.537980, validation/loss=2.113075, validation/num_examples=50000
I0130 18:14:12.615865 139741760567040 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.4098198413848877, loss=3.4997565746307373
I0130 18:14:56.960011 139741768959744 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.3818130493164062, loss=3.5106043815612793
I0130 18:15:43.130751 139741760567040 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.3796652555465698, loss=3.390916585922241
I0130 18:16:29.173089 139741768959744 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.385864019393921, loss=3.551340341567993
I0130 18:17:14.925171 139741760567040 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0544079542160034, loss=4.646872520446777
I0130 18:18:00.790871 139741768959744 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.4175429344177246, loss=3.445406913757324
I0130 18:18:46.620131 139741760567040 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.4094353914260864, loss=3.575622797012329
I0130 18:19:32.660338 139741768959744 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.4839922189712524, loss=3.531900405883789
I0130 18:20:18.422109 139741760567040 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.2877681255340576, loss=4.792230606079102
I0130 18:20:47.956066 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:20:59.912585 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:21:23.106205 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:21:24.709218 139936116377408 submission_runner.py:408] Time since start: 16358.53s, 	Step: 33466, 	{'train/accuracy': 0.5826953053474426, 'train/loss': 1.9023802280426025, 'validation/accuracy': 0.5382999777793884, 'validation/loss': 2.1023356914520264, 'validation/num_examples': 50000, 'test/accuracy': 0.4214000105857849, 'test/loss': 2.7348814010620117, 'test/num_examples': 10000, 'score': 15168.989698171616, 'total_duration': 16358.529019117355, 'accumulated_submission_time': 15168.989698171616, 'accumulated_eval_time': 1186.6391808986664, 'accumulated_logging_time': 1.1376256942749023}
I0130 18:21:24.730209 139741768959744 logging_writer.py:48] [33466] accumulated_eval_time=1186.639181, accumulated_logging_time=1.137626, accumulated_submission_time=15168.989698, global_step=33466, preemption_count=0, score=15168.989698, test/accuracy=0.421400, test/loss=2.734881, test/num_examples=10000, total_duration=16358.529019, train/accuracy=0.582695, train/loss=1.902380, validation/accuracy=0.538300, validation/loss=2.102336, validation/num_examples=50000
I0130 18:21:38.650597 139741760567040 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.0296485424041748, loss=4.956490993499756
I0130 18:22:21.759046 139741768959744 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.4240903854370117, loss=3.533669948577881
I0130 18:23:07.627578 139741760567040 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.3231711387634277, loss=3.5600340366363525
I0130 18:23:53.725502 139741768959744 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.323419451713562, loss=3.6174612045288086
I0130 18:24:39.578041 139741760567040 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.3713291883468628, loss=3.5469582080841064
I0130 18:25:25.290753 139741768959744 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.4852079153060913, loss=3.567474603652954
I0130 18:26:11.114095 139741760567040 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.5041853189468384, loss=3.5651352405548096
I0130 18:26:56.719170 139741768959744 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.3926187753677368, loss=3.6544201374053955
I0130 18:27:42.703203 139741760567040 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.1392933130264282, loss=5.545693874359131
I0130 18:28:25.033025 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:28:37.051871 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:28:59.769331 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:29:01.371158 139936116377408 submission_runner.py:408] Time since start: 16815.19s, 	Step: 34393, 	{'train/accuracy': 0.5888671875, 'train/loss': 1.8822269439697266, 'validation/accuracy': 0.5423799753189087, 'validation/loss': 2.0869996547698975, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.7067244052886963, 'test/num_examples': 10000, 'score': 15589.234191179276, 'total_duration': 16815.190942525864, 'accumulated_submission_time': 15589.234191179276, 'accumulated_eval_time': 1222.977279663086, 'accumulated_logging_time': 1.1691491603851318}
I0130 18:29:01.395271 139741768959744 logging_writer.py:48] [34393] accumulated_eval_time=1222.977280, accumulated_logging_time=1.169149, accumulated_submission_time=15589.234191, global_step=34393, preemption_count=0, score=15589.234191, test/accuracy=0.431300, test/loss=2.706724, test/num_examples=10000, total_duration=16815.190943, train/accuracy=0.588867, train/loss=1.882227, validation/accuracy=0.542380, validation/loss=2.087000, validation/num_examples=50000
I0130 18:29:04.578121 139741760567040 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.409414529800415, loss=3.5529305934906006
I0130 18:29:45.884281 139741768959744 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.6672453880310059, loss=3.564206123352051
I0130 18:30:31.540970 139741760567040 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.4864840507507324, loss=3.352822780609131
I0130 18:31:17.718038 139741768959744 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.4098763465881348, loss=3.4547414779663086
I0130 18:32:03.711555 139741760567040 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.0483736991882324, loss=4.63029670715332
I0130 18:32:49.534715 139741768959744 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.2222356796264648, loss=4.563464641571045
I0130 18:33:35.403632 139741760567040 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.1251229047775269, loss=5.529773712158203
I0130 18:34:21.557687 139741768959744 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.0609283447265625, loss=5.233485698699951
I0130 18:35:07.507579 139741760567040 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.4073861837387085, loss=3.491588830947876
I0130 18:35:53.222553 139741768959744 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.314794898033142, loss=3.8102831840515137
I0130 18:36:01.645400 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:36:13.540515 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:36:36.101293 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:36:37.718751 139936116377408 submission_runner.py:408] Time since start: 17271.54s, 	Step: 35320, 	{'train/accuracy': 0.5822656154632568, 'train/loss': 1.9264488220214844, 'validation/accuracy': 0.5397199988365173, 'validation/loss': 2.1193225383758545, 'validation/num_examples': 50000, 'test/accuracy': 0.4256000220775604, 'test/loss': 2.749296188354492, 'test/num_examples': 10000, 'score': 16009.42619729042, 'total_duration': 17271.53852891922, 'accumulated_submission_time': 16009.42619729042, 'accumulated_eval_time': 1259.0506103038788, 'accumulated_logging_time': 1.20351243019104}
I0130 18:36:37.745872 139741760567040 logging_writer.py:48] [35320] accumulated_eval_time=1259.050610, accumulated_logging_time=1.203512, accumulated_submission_time=16009.426197, global_step=35320, preemption_count=0, score=16009.426197, test/accuracy=0.425600, test/loss=2.749296, test/num_examples=10000, total_duration=17271.538529, train/accuracy=0.582266, train/loss=1.926449, validation/accuracy=0.539720, validation/loss=2.119323, validation/num_examples=50000
I0130 18:37:10.037063 139741768959744 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.4339916706085205, loss=3.445673942565918
I0130 18:37:55.453616 139741760567040 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.4431401491165161, loss=3.427576780319214
I0130 18:38:41.318996 139741768959744 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.2241207361221313, loss=5.131340026855469
I0130 18:39:27.090918 139741760567040 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.4133234024047852, loss=3.579254150390625
I0130 18:40:12.834245 139741768959744 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.13131844997406, loss=5.644035339355469
I0130 18:40:58.585344 139741760567040 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.597164273262024, loss=3.6223437786102295
I0130 18:41:44.559898 139741768959744 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.4989559650421143, loss=3.4025931358337402
I0130 18:42:30.760705 139741760567040 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.4910329580307007, loss=3.4155378341674805
I0130 18:43:16.089921 139741768959744 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.3323488235473633, loss=3.4527180194854736
I0130 18:43:38.094917 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:43:50.039884 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:44:12.009036 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:44:13.614376 139936116377408 submission_runner.py:408] Time since start: 17727.43s, 	Step: 36250, 	{'train/accuracy': 0.5885351300239563, 'train/loss': 1.876665472984314, 'validation/accuracy': 0.5481399893760681, 'validation/loss': 2.0634071826934814, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.680048704147339, 'test/num_examples': 10000, 'score': 16429.708421945572, 'total_duration': 17727.43417429924, 'accumulated_submission_time': 16429.708421945572, 'accumulated_eval_time': 1294.5700707435608, 'accumulated_logging_time': 1.2412841320037842}
I0130 18:44:13.640386 139741760567040 logging_writer.py:48] [36250] accumulated_eval_time=1294.570071, accumulated_logging_time=1.241284, accumulated_submission_time=16429.708422, global_step=36250, preemption_count=0, score=16429.708422, test/accuracy=0.431000, test/loss=2.680049, test/num_examples=10000, total_duration=17727.434174, train/accuracy=0.588535, train/loss=1.876665, validation/accuracy=0.548140, validation/loss=2.063407, validation/num_examples=50000
I0130 18:44:33.926629 139741768959744 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.4129546880722046, loss=3.486538887023926
I0130 18:45:17.794698 139741760567040 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.3613742589950562, loss=3.776808261871338
I0130 18:46:03.829010 139741768959744 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.373968482017517, loss=4.3680739402771
I0130 18:46:49.740533 139741760567040 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.3958302736282349, loss=3.4424548149108887
I0130 18:47:35.414064 139741768959744 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.4504461288452148, loss=3.4414212703704834
I0130 18:48:21.246161 139741760567040 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.5281100273132324, loss=3.5283608436584473
I0130 18:49:07.076507 139741768959744 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1915034055709839, loss=5.044661521911621
I0130 18:49:52.739216 139741760567040 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.581824779510498, loss=3.497204542160034
I0130 18:50:38.605808 139741768959744 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.4013922214508057, loss=3.5104053020477295
I0130 18:51:14.040452 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:51:25.899802 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:51:49.200959 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:51:50.808526 139936116377408 submission_runner.py:408] Time since start: 18184.63s, 	Step: 37179, 	{'train/accuracy': 0.6117382645606995, 'train/loss': 1.750945806503296, 'validation/accuracy': 0.5479400157928467, 'validation/loss': 2.0413665771484375, 'validation/num_examples': 50000, 'test/accuracy': 0.4376000165939331, 'test/loss': 2.6541614532470703, 'test/num_examples': 10000, 'score': 16850.04904961586, 'total_duration': 18184.6283288002, 'accumulated_submission_time': 16850.04904961586, 'accumulated_eval_time': 1331.338150024414, 'accumulated_logging_time': 1.278425931930542}
I0130 18:51:50.830936 139741760567040 logging_writer.py:48] [37179] accumulated_eval_time=1331.338150, accumulated_logging_time=1.278426, accumulated_submission_time=16850.049050, global_step=37179, preemption_count=0, score=16850.049050, test/accuracy=0.437600, test/loss=2.654161, test/num_examples=10000, total_duration=18184.628329, train/accuracy=0.611738, train/loss=1.750946, validation/accuracy=0.547940, validation/loss=2.041367, validation/num_examples=50000
I0130 18:51:59.583043 139741768959744 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.3204877376556396, loss=3.8116543292999268
I0130 18:52:41.622702 139741760567040 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.3549952507019043, loss=3.308823585510254
I0130 18:53:27.381146 139741768959744 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.4474668502807617, loss=3.315753936767578
I0130 18:54:13.629974 139741760567040 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.4669326543807983, loss=3.610858201980591
I0130 18:54:59.858575 139741768959744 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.7295441627502441, loss=3.4950742721557617
I0130 18:55:45.916833 139741760567040 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.2904123067855835, loss=3.702180862426758
I0130 18:56:31.785778 139741768959744 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.2206413745880127, loss=4.588807582855225
I0130 18:57:17.832367 139741760567040 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1725016832351685, loss=4.701391220092773
I0130 18:58:03.759242 139741768959744 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.4823894500732422, loss=3.3722918033599854
I0130 18:58:49.806442 139741760567040 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0447301864624023, loss=5.671311378479004
I0130 18:58:50.837047 139936116377408 spec.py:321] Evaluating on the training split.
I0130 18:59:02.757073 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 18:59:25.523162 139936116377408 spec.py:349] Evaluating on the test split.
I0130 18:59:27.137182 139936116377408 submission_runner.py:408] Time since start: 18640.96s, 	Step: 38104, 	{'train/accuracy': 0.5925390720367432, 'train/loss': 1.8541145324707031, 'validation/accuracy': 0.5517799854278564, 'validation/loss': 2.030083656311035, 'validation/num_examples': 50000, 'test/accuracy': 0.4384000301361084, 'test/loss': 2.6467978954315186, 'test/num_examples': 10000, 'score': 17269.996902942657, 'total_duration': 18640.95697760582, 'accumulated_submission_time': 17269.996902942657, 'accumulated_eval_time': 1367.6382720470428, 'accumulated_logging_time': 1.311347246170044}
I0130 18:59:27.158858 139741768959744 logging_writer.py:48] [38104] accumulated_eval_time=1367.638272, accumulated_logging_time=1.311347, accumulated_submission_time=17269.996903, global_step=38104, preemption_count=0, score=17269.996903, test/accuracy=0.438400, test/loss=2.646798, test/num_examples=10000, total_duration=18640.956978, train/accuracy=0.592539, train/loss=1.854115, validation/accuracy=0.551780, validation/loss=2.030084, validation/num_examples=50000
I0130 19:00:06.715504 139741760567040 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.4221217632293701, loss=3.9952497482299805
I0130 19:00:52.374364 139741768959744 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.6298458576202393, loss=3.5403616428375244
I0130 19:01:38.698268 139741760567040 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.215030312538147, loss=4.345987796783447
I0130 19:02:24.670656 139741768959744 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.3623417615890503, loss=3.4216561317443848
I0130 19:03:10.671725 139741760567040 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.5685619115829468, loss=3.483677387237549
I0130 19:03:56.251117 139741768959744 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.2558162212371826, loss=4.465843677520752
I0130 19:04:42.283723 139741760567040 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.5014392137527466, loss=3.4452099800109863
I0130 19:05:28.413376 139741768959744 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.6838488578796387, loss=3.3697140216827393
I0130 19:06:14.144294 139741760567040 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.3772152662277222, loss=3.867023468017578
I0130 19:06:27.176549 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:06:39.164510 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 19:07:01.911026 139936116377408 spec.py:349] Evaluating on the test split.
I0130 19:07:03.529335 139936116377408 submission_runner.py:408] Time since start: 19097.35s, 	Step: 39030, 	{'train/accuracy': 0.5927343368530273, 'train/loss': 1.8353626728057861, 'validation/accuracy': 0.5510199666023254, 'validation/loss': 2.0330088138580322, 'validation/num_examples': 50000, 'test/accuracy': 0.4350000321865082, 'test/loss': 2.658357620239258, 'test/num_examples': 10000, 'score': 17689.955061912537, 'total_duration': 19097.34910964966, 'accumulated_submission_time': 17689.955061912537, 'accumulated_eval_time': 1403.9910144805908, 'accumulated_logging_time': 1.3433549404144287}
I0130 19:07:03.553390 139741768959744 logging_writer.py:48] [39030] accumulated_eval_time=1403.991014, accumulated_logging_time=1.343355, accumulated_submission_time=17689.955062, global_step=39030, preemption_count=0, score=17689.955062, test/accuracy=0.435000, test/loss=2.658358, test/num_examples=10000, total_duration=19097.349110, train/accuracy=0.592734, train/loss=1.835363, validation/accuracy=0.551020, validation/loss=2.033009, validation/num_examples=50000
I0130 19:07:31.773768 139741760567040 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.4605441093444824, loss=3.6529011726379395
I0130 19:08:16.796642 139741768959744 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.3375613689422607, loss=3.873896598815918
I0130 19:09:02.970142 139741760567040 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.4160020351409912, loss=4.303236961364746
I0130 19:09:48.888972 139741768959744 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.1644192934036255, loss=4.234302520751953
I0130 19:10:34.854952 139741760567040 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.500996470451355, loss=3.4904515743255615
I0130 19:11:20.572882 139741768959744 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.463533878326416, loss=3.4005379676818848
I0130 19:12:06.657448 139741760567040 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.6271315813064575, loss=3.5978870391845703
I0130 19:12:52.496353 139741768959744 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.508894681930542, loss=3.3872787952423096
I0130 19:13:38.608764 139741760567040 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.3777103424072266, loss=3.429283857345581
I0130 19:14:03.972067 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:14:15.790964 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 19:14:38.988197 139936116377408 spec.py:349] Evaluating on the test split.
I0130 19:14:40.597184 139936116377408 submission_runner.py:408] Time since start: 19554.42s, 	Step: 39957, 	{'train/accuracy': 0.5999609231948853, 'train/loss': 1.784259557723999, 'validation/accuracy': 0.5545600056648254, 'validation/loss': 2.0094738006591797, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.6230082511901855, 'test/num_examples': 10000, 'score': 18110.31490755081, 'total_duration': 19554.41698360443, 'accumulated_submission_time': 18110.31490755081, 'accumulated_eval_time': 1440.6161260604858, 'accumulated_logging_time': 1.377694845199585}
I0130 19:14:40.617814 139741768959744 logging_writer.py:48] [39957] accumulated_eval_time=1440.616126, accumulated_logging_time=1.377695, accumulated_submission_time=18110.314908, global_step=39957, preemption_count=0, score=18110.314908, test/accuracy=0.446300, test/loss=2.623008, test/num_examples=10000, total_duration=19554.416984, train/accuracy=0.599961, train/loss=1.784260, validation/accuracy=0.554560, validation/loss=2.009474, validation/num_examples=50000
I0130 19:14:58.119491 139741760567040 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.0730421543121338, loss=5.10196590423584
I0130 19:15:41.605991 139741768959744 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.6068757772445679, loss=3.4180562496185303
I0130 19:16:27.522298 139741760567040 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.671497106552124, loss=3.436258316040039
I0130 19:17:13.751873 139741768959744 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.3407237529754639, loss=3.3550896644592285
I0130 19:17:59.661590 139741760567040 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.3378164768218994, loss=3.3511054515838623
I0130 19:18:45.741596 139741768959744 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.515974998474121, loss=3.3625335693359375
I0130 19:19:31.703378 139741760567040 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.2029753923416138, loss=4.128592491149902
I0130 19:20:17.790560 139741768959744 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.5182973146438599, loss=3.4648048877716064
I0130 19:21:03.650579 139741760567040 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.3751296997070312, loss=3.636691093444824
I0130 19:21:40.661873 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:21:52.569800 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 19:22:13.958289 139936116377408 spec.py:349] Evaluating on the test split.
I0130 19:22:15.579546 139936116377408 submission_runner.py:408] Time since start: 20009.40s, 	Step: 40882, 	{'train/accuracy': 0.5937694907188416, 'train/loss': 1.8420950174331665, 'validation/accuracy': 0.5575799942016602, 'validation/loss': 2.006721258163452, 'validation/num_examples': 50000, 'test/accuracy': 0.4439000189304352, 'test/loss': 2.64489483833313, 'test/num_examples': 10000, 'score': 18530.302005290985, 'total_duration': 20009.399348258972, 'accumulated_submission_time': 18530.302005290985, 'accumulated_eval_time': 1475.5338282585144, 'accumulated_logging_time': 1.4079561233520508}
I0130 19:22:15.602824 139741768959744 logging_writer.py:48] [40882] accumulated_eval_time=1475.533828, accumulated_logging_time=1.407956, accumulated_submission_time=18530.302005, global_step=40882, preemption_count=0, score=18530.302005, test/accuracy=0.443900, test/loss=2.644895, test/num_examples=10000, total_duration=20009.399348, train/accuracy=0.593769, train/loss=1.842095, validation/accuracy=0.557580, validation/loss=2.006721, validation/num_examples=50000
I0130 19:22:23.160620 139741760567040 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.0028773546218872, loss=5.520220756530762
I0130 19:23:04.991576 139741768959744 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.4492270946502686, loss=3.8818469047546387
I0130 19:23:50.941815 139741760567040 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.3786873817443848, loss=3.5147743225097656
I0130 19:24:36.765452 139741768959744 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.444685697555542, loss=3.457263708114624
I0130 19:25:22.827212 139741760567040 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.5904054641723633, loss=3.740628719329834
I0130 19:26:08.788622 139741768959744 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.3582180738449097, loss=3.340623140335083
I0130 19:26:54.548575 139741760567040 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.1355386972427368, loss=4.928720474243164
I0130 19:27:40.470339 139741768959744 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1924855709075928, loss=5.049332141876221
I0130 19:28:26.115522 139741760567040 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1719286441802979, loss=5.598661422729492
I0130 19:29:12.071462 139741768959744 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.6035287380218506, loss=3.3809964656829834
I0130 19:29:15.981175 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:29:27.658863 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 19:29:50.093766 139936116377408 spec.py:349] Evaluating on the test split.
I0130 19:29:51.701164 139936116377408 submission_runner.py:408] Time since start: 20465.52s, 	Step: 41810, 	{'train/accuracy': 0.6000195145606995, 'train/loss': 1.8037185668945312, 'validation/accuracy': 0.5605800151824951, 'validation/loss': 1.9919346570968628, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.6278748512268066, 'test/num_examples': 10000, 'score': 18950.62165570259, 'total_duration': 20465.520961999893, 'accumulated_submission_time': 18950.62165570259, 'accumulated_eval_time': 1511.2537944316864, 'accumulated_logging_time': 1.4423680305480957}
I0130 19:29:51.722897 139741760567040 logging_writer.py:48] [41810] accumulated_eval_time=1511.253794, accumulated_logging_time=1.442368, accumulated_submission_time=18950.621656, global_step=41810, preemption_count=0, score=18950.621656, test/accuracy=0.442200, test/loss=2.627875, test/num_examples=10000, total_duration=20465.520962, train/accuracy=0.600020, train/loss=1.803719, validation/accuracy=0.560580, validation/loss=1.991935, validation/num_examples=50000
I0130 19:30:28.463947 139741768959744 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.0182881355285645, loss=5.15239143371582
I0130 19:31:14.330214 139741760567040 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.3441005945205688, loss=3.4273972511291504
I0130 19:32:00.348807 139741768959744 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.2304365634918213, loss=4.730759620666504
I0130 19:32:46.429844 139741760567040 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.5632781982421875, loss=3.433441400527954
I0130 19:33:32.617937 139741768959744 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.427388072013855, loss=3.4069957733154297
I0130 19:34:18.440717 139741760567040 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.288819432258606, loss=3.890937328338623
I0130 19:35:04.126225 139741768959744 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.4368839263916016, loss=3.5163793563842773
I0130 19:35:50.305842 139741760567040 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.1803032159805298, loss=4.430387496948242
I0130 19:36:36.179821 139741768959744 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.5110609531402588, loss=3.392524242401123
I0130 19:36:51.787055 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:37:03.902283 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 19:37:25.894667 139936116377408 spec.py:349] Evaluating on the test split.
I0130 19:37:27.504960 139936116377408 submission_runner.py:408] Time since start: 20921.32s, 	Step: 42736, 	{'train/accuracy': 0.6067187190055847, 'train/loss': 1.7677797079086304, 'validation/accuracy': 0.5633800029754639, 'validation/loss': 1.9798634052276611, 'validation/num_examples': 50000, 'test/accuracy': 0.44680002331733704, 'test/loss': 2.621208667755127, 'test/num_examples': 10000, 'score': 19370.62746167183, 'total_duration': 20921.324761152267, 'accumulated_submission_time': 19370.62746167183, 'accumulated_eval_time': 1546.9716968536377, 'accumulated_logging_time': 1.474189043045044}
I0130 19:37:27.525782 139741760567040 logging_writer.py:48] [42736] accumulated_eval_time=1546.971697, accumulated_logging_time=1.474189, accumulated_submission_time=19370.627462, global_step=42736, preemption_count=0, score=19370.627462, test/accuracy=0.446800, test/loss=2.621209, test/num_examples=10000, total_duration=20921.324761, train/accuracy=0.606719, train/loss=1.767780, validation/accuracy=0.563380, validation/loss=1.979863, validation/num_examples=50000
I0130 19:37:53.362337 139741768959744 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.355266809463501, loss=3.3522744178771973
I0130 19:38:37.915446 139741760567040 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.846934199333191, loss=3.431806802749634
I0130 19:39:23.991888 139741768959744 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.5071440935134888, loss=3.547914505004883
I0130 19:40:10.023916 139741760567040 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.5443445444107056, loss=3.4197704792022705
I0130 19:40:55.843510 139741768959744 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.434476375579834, loss=3.425682544708252
I0130 19:41:41.794017 139741760567040 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.4821064472198486, loss=3.6636388301849365
I0130 19:42:27.390795 139741768959744 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.3460900783538818, loss=3.3817455768585205
I0130 19:43:13.366139 139741760567040 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.489746332168579, loss=3.9553205966949463
I0130 19:43:59.057025 139741768959744 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.3689321279525757, loss=3.5098559856414795
I0130 19:44:27.795030 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:44:39.621584 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 19:45:00.680764 139936116377408 spec.py:349] Evaluating on the test split.
I0130 19:45:02.305494 139936116377408 submission_runner.py:408] Time since start: 21376.13s, 	Step: 43664, 	{'train/accuracy': 0.6116601228713989, 'train/loss': 1.7431319952011108, 'validation/accuracy': 0.5555999875068665, 'validation/loss': 1.9900355339050293, 'validation/num_examples': 50000, 'test/accuracy': 0.44040003418922424, 'test/loss': 2.6423449516296387, 'test/num_examples': 10000, 'score': 19790.838992118835, 'total_duration': 21376.125276327133, 'accumulated_submission_time': 19790.838992118835, 'accumulated_eval_time': 1581.4821391105652, 'accumulated_logging_time': 1.5045185089111328}
I0130 19:45:02.329161 139741760567040 logging_writer.py:48] [43664] accumulated_eval_time=1581.482139, accumulated_logging_time=1.504519, accumulated_submission_time=19790.838992, global_step=43664, preemption_count=0, score=19790.838992, test/accuracy=0.440400, test/loss=2.642345, test/num_examples=10000, total_duration=21376.125276, train/accuracy=0.611660, train/loss=1.743132, validation/accuracy=0.555600, validation/loss=1.990036, validation/num_examples=50000
I0130 19:45:17.040993 139741768959744 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.263270616531372, loss=3.993849992752075
I0130 19:46:00.223124 139741760567040 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.2191658020019531, loss=4.627868175506592
I0130 19:46:46.145035 139741768959744 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2351601123809814, loss=5.062844753265381
I0130 19:47:32.343986 139741760567040 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.5510157346725464, loss=3.3635764122009277
I0130 19:48:18.331323 139741768959744 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.3732551336288452, loss=3.391530990600586
I0130 19:49:04.448574 139741760567040 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.4103643894195557, loss=3.6120729446411133
I0130 19:49:50.489444 139741768959744 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.4524564743041992, loss=3.484900712966919
I0130 19:50:36.524158 139741760567040 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.4992035627365112, loss=3.5250022411346436
I0130 19:51:22.411883 139741768959744 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.4719815254211426, loss=3.470773220062256
I0130 19:52:02.579235 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:52:14.517025 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 19:52:36.614346 139936116377408 spec.py:349] Evaluating on the test split.
I0130 19:52:38.227226 139936116377408 submission_runner.py:408] Time since start: 21832.05s, 	Step: 44589, 	{'train/accuracy': 0.6066015362739563, 'train/loss': 1.761168122291565, 'validation/accuracy': 0.562720000743866, 'validation/loss': 1.9542808532714844, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.59022855758667, 'test/num_examples': 10000, 'score': 20211.031491041183, 'total_duration': 21832.046981096268, 'accumulated_submission_time': 20211.031491041183, 'accumulated_eval_time': 1617.130084991455, 'accumulated_logging_time': 1.5374326705932617}
I0130 19:52:38.258117 139741760567040 logging_writer.py:48] [44589] accumulated_eval_time=1617.130085, accumulated_logging_time=1.537433, accumulated_submission_time=20211.031491, global_step=44589, preemption_count=0, score=20211.031491, test/accuracy=0.447800, test/loss=2.590229, test/num_examples=10000, total_duration=21832.046981, train/accuracy=0.606602, train/loss=1.761168, validation/accuracy=0.562720, validation/loss=1.954281, validation/num_examples=50000
I0130 19:52:43.039312 139741768959744 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.623579502105713, loss=3.6080543994903564
I0130 19:53:24.792171 139741760567040 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.5969922542572021, loss=3.4222075939178467
I0130 19:54:11.073427 139741768959744 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.2940796613693237, loss=3.780813455581665
I0130 19:54:57.224618 139741760567040 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.4220895767211914, loss=3.499100685119629
I0130 19:55:42.825217 139741768959744 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.411244511604309, loss=3.6688499450683594
I0130 19:56:29.240812 139741760567040 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.4348093271255493, loss=3.415907859802246
I0130 19:57:15.288952 139741768959744 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.3204412460327148, loss=5.458733558654785
I0130 19:58:01.269753 139741760567040 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.6477866172790527, loss=3.3871850967407227
I0130 19:58:47.601889 139741768959744 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.5797966718673706, loss=3.41756534576416
I0130 19:59:34.128004 139741760567040 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.413051724433899, loss=3.3615055084228516
I0130 19:59:38.402567 139936116377408 spec.py:321] Evaluating on the training split.
I0130 19:59:50.161653 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:00:12.535030 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:00:14.158969 139936116377408 submission_runner.py:408] Time since start: 22287.98s, 	Step: 45511, 	{'train/accuracy': 0.6109179258346558, 'train/loss': 1.7445123195648193, 'validation/accuracy': 0.5663999915122986, 'validation/loss': 1.957970380783081, 'validation/num_examples': 50000, 'test/accuracy': 0.4520000219345093, 'test/loss': 2.569453239440918, 'test/num_examples': 10000, 'score': 20631.116145849228, 'total_duration': 22287.97873067856, 'accumulated_submission_time': 20631.116145849228, 'accumulated_eval_time': 1652.8864409923553, 'accumulated_logging_time': 1.5798923969268799}
I0130 20:00:14.184862 139741768959744 logging_writer.py:48] [45511] accumulated_eval_time=1652.886441, accumulated_logging_time=1.579892, accumulated_submission_time=20631.116146, global_step=45511, preemption_count=0, score=20631.116146, test/accuracy=0.452000, test/loss=2.569453, test/num_examples=10000, total_duration=22287.978731, train/accuracy=0.610918, train/loss=1.744512, validation/accuracy=0.566400, validation/loss=1.957970, validation/num_examples=50000
I0130 20:00:50.108355 139741760567040 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.3717997074127197, loss=3.9535226821899414
I0130 20:01:35.945561 139741768959744 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.3906594514846802, loss=3.43184757232666
I0130 20:02:22.296518 139741760567040 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.4485417604446411, loss=3.474428653717041
I0130 20:03:08.059009 139741768959744 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.4883512258529663, loss=3.9411258697509766
I0130 20:03:53.644819 139741760567040 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.310239315032959, loss=3.721052885055542
I0130 20:04:39.276295 139741768959744 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.4023200273513794, loss=3.827017307281494
I0130 20:05:25.426377 139741760567040 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.1154197454452515, loss=4.525116920471191
I0130 20:06:11.585414 139741768959744 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.4205238819122314, loss=3.411651134490967
I0130 20:06:57.303115 139741760567040 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.2727974653244019, loss=4.3916015625
I0130 20:07:14.572377 139936116377408 spec.py:321] Evaluating on the training split.
I0130 20:07:26.917982 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:07:48.318333 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:07:49.931899 139936116377408 submission_runner.py:408] Time since start: 22743.75s, 	Step: 46439, 	{'train/accuracy': 0.6299023032188416, 'train/loss': 1.6438137292861938, 'validation/accuracy': 0.5723599791526794, 'validation/loss': 1.911130428314209, 'validation/num_examples': 50000, 'test/accuracy': 0.45250001549720764, 'test/loss': 2.548551559448242, 'test/num_examples': 10000, 'score': 21051.443721055984, 'total_duration': 22743.75170326233, 'accumulated_submission_time': 21051.443721055984, 'accumulated_eval_time': 1688.2459979057312, 'accumulated_logging_time': 1.6172716617584229}
I0130 20:07:49.954490 139741768959744 logging_writer.py:48] [46439] accumulated_eval_time=1688.245998, accumulated_logging_time=1.617272, accumulated_submission_time=21051.443721, global_step=46439, preemption_count=0, score=21051.443721, test/accuracy=0.452500, test/loss=2.548552, test/num_examples=10000, total_duration=22743.751703, train/accuracy=0.629902, train/loss=1.643814, validation/accuracy=0.572360, validation/loss=1.911130, validation/num_examples=50000
I0130 20:08:14.599797 139741760567040 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.3943184614181519, loss=3.8728275299072266
I0130 20:08:58.803165 139741768959744 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.685896396636963, loss=3.3739562034606934
I0130 20:09:44.964548 139741760567040 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.5005594491958618, loss=3.408750534057617
I0130 20:10:30.914703 139741768959744 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.4778692722320557, loss=3.489410877227783
I0130 20:11:16.783473 139741760567040 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.4539552927017212, loss=5.218703269958496
I0130 20:12:02.949416 139741768959744 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.3939785957336426, loss=3.3206241130828857
I0130 20:12:48.761466 139741760567040 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.491754174232483, loss=3.3132643699645996
I0130 20:13:34.674083 139741768959744 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.1475059986114502, loss=5.171700477600098
I0130 20:14:20.821920 139741760567040 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.5186753273010254, loss=3.3798699378967285
I0130 20:14:50.396003 139936116377408 spec.py:321] Evaluating on the training split.
I0130 20:15:02.509186 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:15:25.497370 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:15:27.112375 139936116377408 submission_runner.py:408] Time since start: 23200.93s, 	Step: 47366, 	{'train/accuracy': 0.6089648008346558, 'train/loss': 1.7728519439697266, 'validation/accuracy': 0.5699399709701538, 'validation/loss': 1.9609678983688354, 'validation/num_examples': 50000, 'test/accuracy': 0.4547000229358673, 'test/loss': 2.593113422393799, 'test/num_examples': 10000, 'score': 21471.823915719986, 'total_duration': 23200.93217253685, 'accumulated_submission_time': 21471.823915719986, 'accumulated_eval_time': 1724.9623546600342, 'accumulated_logging_time': 1.65169358253479}
I0130 20:15:27.136163 139741768959744 logging_writer.py:48] [47366] accumulated_eval_time=1724.962355, accumulated_logging_time=1.651694, accumulated_submission_time=21471.823916, global_step=47366, preemption_count=0, score=21471.823916, test/accuracy=0.454700, test/loss=2.593113, test/num_examples=10000, total_duration=23200.932173, train/accuracy=0.608965, train/loss=1.772852, validation/accuracy=0.569940, validation/loss=1.960968, validation/num_examples=50000
I0130 20:15:41.066006 139741760567040 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.3676859140396118, loss=4.009522438049316
I0130 20:16:23.840457 139741768959744 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.6056920289993286, loss=3.4495913982391357
I0130 20:17:09.730924 139741760567040 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.4786498546600342, loss=3.397202491760254
I0130 20:17:55.860024 139741768959744 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.4966330528259277, loss=3.3159141540527344
I0130 20:18:42.038409 139741760567040 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.1628985404968262, loss=4.842611789703369
I0130 20:19:27.720653 139741768959744 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7221730947494507, loss=3.3833189010620117
I0130 20:20:13.802196 139741760567040 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.1885071992874146, loss=4.276000022888184
I0130 20:20:59.679943 139741768959744 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.2185616493225098, loss=4.804901599884033
I0130 20:21:45.933933 139741760567040 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.5511658191680908, loss=3.439516544342041
I0130 20:22:27.418457 139936116377408 spec.py:321] Evaluating on the training split.
I0130 20:22:39.410575 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:23:02.845201 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:23:04.455007 139936116377408 submission_runner.py:408] Time since start: 23658.27s, 	Step: 48292, 	{'train/accuracy': 0.6068750023841858, 'train/loss': 1.7780888080596924, 'validation/accuracy': 0.5670199990272522, 'validation/loss': 1.9639018774032593, 'validation/num_examples': 50000, 'test/accuracy': 0.4553000330924988, 'test/loss': 2.6022982597351074, 'test/num_examples': 10000, 'score': 21892.04855132103, 'total_duration': 23658.274804115295, 'accumulated_submission_time': 21892.04855132103, 'accumulated_eval_time': 1761.9988887310028, 'accumulated_logging_time': 1.6848242282867432}
I0130 20:23:04.481457 139741768959744 logging_writer.py:48] [48292] accumulated_eval_time=1761.998889, accumulated_logging_time=1.684824, accumulated_submission_time=21892.048551, global_step=48292, preemption_count=0, score=21892.048551, test/accuracy=0.455300, test/loss=2.602298, test/num_examples=10000, total_duration=23658.274804, train/accuracy=0.606875, train/loss=1.778089, validation/accuracy=0.567020, validation/loss=1.963902, validation/num_examples=50000
I0130 20:23:08.068654 139741760567040 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.3949304819107056, loss=3.5111215114593506
I0130 20:23:49.230887 139741768959744 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.3729804754257202, loss=3.3486053943634033
I0130 20:24:35.365279 139741760567040 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.5247310400009155, loss=3.419407367706299
I0130 20:25:21.989976 139741768959744 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.4834973812103271, loss=3.2388312816619873
I0130 20:26:07.872614 139741760567040 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.5213384628295898, loss=3.3434479236602783
I0130 20:26:54.037332 139741768959744 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.4359548091888428, loss=3.273001194000244
I0130 20:27:40.196620 139741760567040 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.5484225749969482, loss=3.283302068710327
I0130 20:28:26.160170 139741768959744 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.3048738241195679, loss=5.161161422729492
I0130 20:29:12.237920 139741760567040 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.121849536895752, loss=5.591568946838379
I0130 20:29:58.125505 139741768959744 logging_writer.py:48] [49200] global_step=49200, grad_norm=2.0901780128479004, loss=3.358726978302002
I0130 20:30:04.790513 139936116377408 spec.py:321] Evaluating on the training split.
I0130 20:30:16.573321 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:30:38.351571 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:30:39.961838 139936116377408 submission_runner.py:408] Time since start: 24113.78s, 	Step: 49216, 	{'train/accuracy': 0.6234374642372131, 'train/loss': 1.6764730215072632, 'validation/accuracy': 0.5714200139045715, 'validation/loss': 1.9154047966003418, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.5392799377441406, 'test/num_examples': 10000, 'score': 22312.300507068634, 'total_duration': 24113.78163957596, 'accumulated_submission_time': 22312.300507068634, 'accumulated_eval_time': 1797.1702196598053, 'accumulated_logging_time': 1.7208774089813232}
I0130 20:30:39.983849 139741760567040 logging_writer.py:48] [49216] accumulated_eval_time=1797.170220, accumulated_logging_time=1.720877, accumulated_submission_time=22312.300507, global_step=49216, preemption_count=0, score=22312.300507, test/accuracy=0.458000, test/loss=2.539280, test/num_examples=10000, total_duration=24113.781640, train/accuracy=0.623437, train/loss=1.676473, validation/accuracy=0.571420, validation/loss=1.915405, validation/num_examples=50000
I0130 20:31:14.481171 139741768959744 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.3403321504592896, loss=3.69864821434021
I0130 20:32:00.411794 139741760567040 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.7064992189407349, loss=3.383955955505371
I0130 20:32:46.884830 139741768959744 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.4331785440444946, loss=3.7838711738586426
I0130 20:33:32.989220 139741760567040 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.5720694065093994, loss=3.2988104820251465
I0130 20:34:19.060128 139741768959744 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.5393023490905762, loss=3.359623432159424
I0130 20:35:04.929813 139741760567040 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.2285555601119995, loss=5.527309417724609
I0130 20:35:51.034089 139741768959744 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.1958889961242676, loss=4.9067583084106445
I0130 20:36:37.099348 139741760567040 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.6748378276824951, loss=3.3383126258850098
I0130 20:37:23.188307 139741768959744 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.1990845203399658, loss=4.5231122970581055
I0130 20:37:40.237015 139936116377408 spec.py:321] Evaluating on the training split.
I0130 20:37:52.044812 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:38:13.189389 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:38:14.798869 139936116377408 submission_runner.py:408] Time since start: 24568.62s, 	Step: 50139, 	{'train/accuracy': 0.6090039014816284, 'train/loss': 1.7651121616363525, 'validation/accuracy': 0.569819986820221, 'validation/loss': 1.935936689376831, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.5697062015533447, 'test/num_examples': 10000, 'score': 22732.495416641235, 'total_duration': 24568.618636846542, 'accumulated_submission_time': 22732.495416641235, 'accumulated_eval_time': 1831.73202419281, 'accumulated_logging_time': 1.7526824474334717}
I0130 20:38:14.829035 139741760567040 logging_writer.py:48] [50139] accumulated_eval_time=1831.732024, accumulated_logging_time=1.752682, accumulated_submission_time=22732.495417, global_step=50139, preemption_count=0, score=22732.495417, test/accuracy=0.455700, test/loss=2.569706, test/num_examples=10000, total_duration=24568.618637, train/accuracy=0.609004, train/loss=1.765112, validation/accuracy=0.569820, validation/loss=1.935937, validation/num_examples=50000
I0130 20:38:39.479900 139741768959744 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.4063074588775635, loss=3.482977867126465
I0130 20:39:23.852533 139741760567040 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.2194808721542358, loss=4.526981353759766
I0130 20:40:09.792416 139741768959744 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.4403116703033447, loss=3.9052252769470215
I0130 20:40:55.585656 139741760567040 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.4400609731674194, loss=3.3432788848876953
I0130 20:41:41.813493 139741768959744 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.0412237644195557, loss=5.466181755065918
I0130 20:42:27.961407 139741760567040 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.4281343221664429, loss=3.926236152648926
I0130 20:43:14.011467 139741768959744 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.3650060892105103, loss=3.288546323776245
I0130 20:43:59.934626 139741760567040 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.466785192489624, loss=3.368220329284668
I0130 20:44:45.983875 139741768959744 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.6116536855697632, loss=3.8679051399230957
I0130 20:45:15.121708 139936116377408 spec.py:321] Evaluating on the training split.
I0130 20:45:27.386174 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:45:50.146810 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:45:51.755066 139936116377408 submission_runner.py:408] Time since start: 25025.57s, 	Step: 51065, 	{'train/accuracy': 0.6101757884025574, 'train/loss': 1.7590184211730957, 'validation/accuracy': 0.5707600116729736, 'validation/loss': 1.9382047653198242, 'validation/num_examples': 50000, 'test/accuracy': 0.45210000872612, 'test/loss': 2.5792510509490967, 'test/num_examples': 10000, 'score': 23152.728211402893, 'total_duration': 25025.574861764908, 'accumulated_submission_time': 23152.728211402893, 'accumulated_eval_time': 1868.3653919696808, 'accumulated_logging_time': 1.794832706451416}
I0130 20:45:51.778012 139741760567040 logging_writer.py:48] [51065] accumulated_eval_time=1868.365392, accumulated_logging_time=1.794833, accumulated_submission_time=23152.728211, global_step=51065, preemption_count=0, score=23152.728211, test/accuracy=0.452100, test/loss=2.579251, test/num_examples=10000, total_duration=25025.574862, train/accuracy=0.610176, train/loss=1.759018, validation/accuracy=0.570760, validation/loss=1.938205, validation/num_examples=50000
I0130 20:46:06.096412 139741768959744 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.434589147567749, loss=3.4030921459198
I0130 20:46:48.410511 139741760567040 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.58231520652771, loss=3.2797439098358154
I0130 20:47:34.335139 139741768959744 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.5484110116958618, loss=3.3388590812683105
I0130 20:48:20.616412 139741760567040 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.5637871026992798, loss=3.7211804389953613
I0130 20:49:06.885779 139741768959744 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.571280837059021, loss=3.45371413230896
I0130 20:49:52.691970 139741760567040 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2809340953826904, loss=4.80690860748291
I0130 20:50:38.644920 139741768959744 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.6183050870895386, loss=3.3327932357788086
I0130 20:51:24.575369 139741760567040 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.3419787883758545, loss=4.145470142364502
I0130 20:52:10.568424 139741768959744 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.3113418817520142, loss=3.9990732669830322
I0130 20:52:52.207474 139936116377408 spec.py:321] Evaluating on the training split.
I0130 20:53:04.293105 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 20:53:27.901401 139936116377408 spec.py:349] Evaluating on the test split.
I0130 20:53:29.511975 139936116377408 submission_runner.py:408] Time since start: 25483.33s, 	Step: 51992, 	{'train/accuracy': 0.6263476610183716, 'train/loss': 1.6484354734420776, 'validation/accuracy': 0.5796999931335449, 'validation/loss': 1.8653367757797241, 'validation/num_examples': 50000, 'test/accuracy': 0.46560001373291016, 'test/loss': 2.487163782119751, 'test/num_examples': 10000, 'score': 23573.097232103348, 'total_duration': 25483.33177614212, 'accumulated_submission_time': 23573.097232103348, 'accumulated_eval_time': 1905.6698813438416, 'accumulated_logging_time': 1.828599452972412}
I0130 20:53:29.542086 139741760567040 logging_writer.py:48] [51992] accumulated_eval_time=1905.669881, accumulated_logging_time=1.828599, accumulated_submission_time=23573.097232, global_step=51992, preemption_count=0, score=23573.097232, test/accuracy=0.465600, test/loss=2.487164, test/num_examples=10000, total_duration=25483.331776, train/accuracy=0.626348, train/loss=1.648435, validation/accuracy=0.579700, validation/loss=1.865337, validation/num_examples=50000
I0130 20:53:33.122255 139741768959744 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.5094331502914429, loss=3.7004613876342773
I0130 20:54:14.298329 139741760567040 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.2197091579437256, loss=4.322305679321289
I0130 20:55:00.094923 139741768959744 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.630163311958313, loss=3.4995219707489014
I0130 20:55:46.357013 139741760567040 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.4159986972808838, loss=3.6361091136932373
I0130 20:56:32.002430 139741768959744 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.463487148284912, loss=3.466646671295166
I0130 20:57:17.977655 139741760567040 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.568637490272522, loss=3.330756902694702
I0130 20:58:04.239204 139741768959744 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.4345967769622803, loss=3.864635467529297
I0130 20:58:49.775375 139741760567040 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.5149011611938477, loss=3.3186659812927246
I0130 20:59:35.837813 139741768959744 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.511704683303833, loss=3.3936820030212402
I0130 21:00:21.694468 139741760567040 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.288220763206482, loss=4.479917049407959
I0130 21:00:29.566167 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:00:41.431097 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:01:05.792312 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:01:07.399046 139936116377408 submission_runner.py:408] Time since start: 25941.22s, 	Step: 52919, 	{'train/accuracy': 0.6298437118530273, 'train/loss': 1.6609742641448975, 'validation/accuracy': 0.5778399705886841, 'validation/loss': 1.9005619287490845, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.5275325775146484, 'test/num_examples': 10000, 'score': 23993.060639619827, 'total_duration': 25941.218841791153, 'accumulated_submission_time': 23993.060639619827, 'accumulated_eval_time': 1943.5027811527252, 'accumulated_logging_time': 1.8709113597869873}
I0130 21:01:07.422580 139741768959744 logging_writer.py:48] [52919] accumulated_eval_time=1943.502781, accumulated_logging_time=1.870911, accumulated_submission_time=23993.060640, global_step=52919, preemption_count=0, score=23993.060640, test/accuracy=0.464900, test/loss=2.527533, test/num_examples=10000, total_duration=25941.218842, train/accuracy=0.629844, train/loss=1.660974, validation/accuracy=0.577840, validation/loss=1.900562, validation/num_examples=50000
I0130 21:01:40.024351 139741760567040 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.3090826272964478, loss=5.307700157165527
I0130 21:02:26.081538 139741768959744 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.705439805984497, loss=3.298776865005493
I0130 21:03:12.134738 139741760567040 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.6671843528747559, loss=3.3521859645843506
I0130 21:03:57.908929 139741768959744 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.3525172472000122, loss=5.429901123046875
I0130 21:04:43.817408 139741760567040 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.4884072542190552, loss=3.438276529312134
I0130 21:05:30.022146 139741768959744 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.4344173669815063, loss=3.617511510848999
I0130 21:06:15.819549 139741760567040 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.722029447555542, loss=3.306429386138916
I0130 21:07:01.491035 139741768959744 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.3390882015228271, loss=3.8679416179656982
I0130 21:07:47.555588 139741760567040 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.3391145467758179, loss=3.9568281173706055
I0130 21:08:07.617536 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:08:19.587880 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:08:42.451520 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:08:44.065997 139936116377408 submission_runner.py:408] Time since start: 26397.89s, 	Step: 53845, 	{'train/accuracy': 0.6171679496765137, 'train/loss': 1.7618088722229004, 'validation/accuracy': 0.5772799849510193, 'validation/loss': 1.9457567930221558, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.5673742294311523, 'test/num_examples': 10000, 'score': 24413.198468208313, 'total_duration': 26397.88577604294, 'accumulated_submission_time': 24413.198468208313, 'accumulated_eval_time': 1979.9512028694153, 'accumulated_logging_time': 1.9040093421936035}
I0130 21:08:44.093625 139741768959744 logging_writer.py:48] [53845] accumulated_eval_time=1979.951203, accumulated_logging_time=1.904009, accumulated_submission_time=24413.198468, global_step=53845, preemption_count=0, score=24413.198468, test/accuracy=0.457800, test/loss=2.567374, test/num_examples=10000, total_duration=26397.885776, train/accuracy=0.617168, train/loss=1.761809, validation/accuracy=0.577280, validation/loss=1.945757, validation/num_examples=50000
I0130 21:09:06.344470 139741760567040 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.2533615827560425, loss=4.750953674316406
I0130 21:09:50.400606 139741768959744 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.1088799238204956, loss=4.668279647827148
I0130 21:10:36.585501 139741760567040 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.2529287338256836, loss=3.999014377593994
I0130 21:11:22.757855 139741768959744 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.4616285562515259, loss=3.3328537940979004
I0130 21:12:08.745553 139741760567040 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.379723072052002, loss=4.227346420288086
I0130 21:12:54.669631 139741768959744 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.5042139291763306, loss=3.3194079399108887
I0130 21:13:40.562000 139741760567040 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.5503343343734741, loss=3.2816691398620605
I0130 21:14:26.564356 139741768959744 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.4598487615585327, loss=3.8176074028015137
I0130 21:15:12.683333 139741760567040 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.5836176872253418, loss=3.3768250942230225
I0130 21:15:44.492424 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:15:56.221268 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:16:19.804153 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:16:21.412282 139936116377408 submission_runner.py:408] Time since start: 26855.23s, 	Step: 54771, 	{'train/accuracy': 0.6280664205551147, 'train/loss': 1.6353827714920044, 'validation/accuracy': 0.581820011138916, 'validation/loss': 1.8503868579864502, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.465291976928711, 'test/num_examples': 10000, 'score': 24833.537580490112, 'total_duration': 26855.232084035873, 'accumulated_submission_time': 24833.537580490112, 'accumulated_eval_time': 2016.8710873126984, 'accumulated_logging_time': 1.9429829120635986}
I0130 21:16:21.438885 139741768959744 logging_writer.py:48] [54771] accumulated_eval_time=2016.871087, accumulated_logging_time=1.942983, accumulated_submission_time=24833.537580, global_step=54771, preemption_count=0, score=24833.537580, test/accuracy=0.465300, test/loss=2.465292, test/num_examples=10000, total_duration=26855.232084, train/accuracy=0.628066, train/loss=1.635383, validation/accuracy=0.581820, validation/loss=1.850387, validation/num_examples=50000
I0130 21:16:33.373086 139741760567040 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.1628165245056152, loss=4.7134318351745605
I0130 21:17:15.763140 139741768959744 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.671900749206543, loss=3.270857572555542
I0130 21:18:01.327937 139741760567040 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.6813567876815796, loss=3.3791937828063965
I0130 21:18:47.416820 139741760567040 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.4660041332244873, loss=3.228722095489502
I0130 21:19:33.263744 139741768959744 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.5489115715026855, loss=3.4447102546691895
I0130 21:20:19.154294 139741760567040 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.5594435930252075, loss=3.3703103065490723
I0130 21:21:04.870566 139741768959744 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.4924674034118652, loss=3.196457862854004
I0130 21:21:50.578395 139741760567040 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.4378113746643066, loss=3.3588008880615234
I0130 21:22:36.617679 139741768959744 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.5449836254119873, loss=3.3716533184051514
I0130 21:23:21.555178 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:23:33.384667 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:23:55.790394 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:23:57.400770 139936116377408 submission_runner.py:408] Time since start: 27311.22s, 	Step: 55700, 	{'train/accuracy': 0.6409375071525574, 'train/loss': 1.605479121208191, 'validation/accuracy': 0.5806399583816528, 'validation/loss': 1.8812267780303955, 'validation/num_examples': 50000, 'test/accuracy': 0.46140003204345703, 'test/loss': 2.5292446613311768, 'test/num_examples': 10000, 'score': 25253.59513092041, 'total_duration': 27311.22057056427, 'accumulated_submission_time': 25253.59513092041, 'accumulated_eval_time': 2052.716703414917, 'accumulated_logging_time': 1.9796583652496338}
I0130 21:23:57.424896 139741760567040 logging_writer.py:48] [55700] accumulated_eval_time=2052.716703, accumulated_logging_time=1.979658, accumulated_submission_time=25253.595131, global_step=55700, preemption_count=0, score=25253.595131, test/accuracy=0.461400, test/loss=2.529245, test/num_examples=10000, total_duration=27311.220571, train/accuracy=0.640938, train/loss=1.605479, validation/accuracy=0.580640, validation/loss=1.881227, validation/num_examples=50000
I0130 21:23:57.825206 139741768959744 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.5941616296768188, loss=3.3236753940582275
I0130 21:24:38.683740 139741760567040 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.5835306644439697, loss=3.344381093978882
I0130 21:25:24.540480 139741768959744 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.6737314462661743, loss=3.350754737854004
I0130 21:26:10.804136 139741760567040 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.6998450756072998, loss=3.5626797676086426
I0130 21:26:56.457278 139741768959744 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.5883102416992188, loss=3.6970722675323486
I0130 21:27:42.303790 139741760567040 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.5525202751159668, loss=3.328143358230591
I0130 21:28:28.394261 139741768959744 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.498451590538025, loss=3.2951371669769287
I0130 21:29:14.120940 139741760567040 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.660414457321167, loss=3.415353298187256
I0130 21:30:00.012625 139741768959744 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8354889154434204, loss=3.4270219802856445
I0130 21:30:46.017564 139741760567040 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.2869200706481934, loss=5.55357027053833
I0130 21:30:57.517228 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:31:09.606777 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:31:31.953548 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:31:33.568536 139936116377408 submission_runner.py:408] Time since start: 27767.39s, 	Step: 56627, 	{'train/accuracy': 0.6262499690055847, 'train/loss': 1.657572865486145, 'validation/accuracy': 0.5814999938011169, 'validation/loss': 1.8544203042984009, 'validation/num_examples': 50000, 'test/accuracy': 0.4660000205039978, 'test/loss': 2.484644651412964, 'test/num_examples': 10000, 'score': 25673.629153251648, 'total_duration': 27767.3883125782, 'accumulated_submission_time': 25673.629153251648, 'accumulated_eval_time': 2088.7680180072784, 'accumulated_logging_time': 2.0139429569244385}
I0130 21:31:33.595560 139741768959744 logging_writer.py:48] [56627] accumulated_eval_time=2088.768018, accumulated_logging_time=2.013943, accumulated_submission_time=25673.629153, global_step=56627, preemption_count=0, score=25673.629153, test/accuracy=0.466000, test/loss=2.484645, test/num_examples=10000, total_duration=27767.388313, train/accuracy=0.626250, train/loss=1.657573, validation/accuracy=0.581500, validation/loss=1.854420, validation/num_examples=50000
I0130 21:32:03.033444 139741760567040 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.5514187812805176, loss=3.247760534286499
I0130 21:32:48.272430 139741768959744 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2224794626235962, loss=4.962167739868164
I0130 21:33:34.482716 139741760567040 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.2254910469055176, loss=4.963934898376465
I0130 21:34:20.206218 139741768959744 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1374646425247192, loss=4.976729869842529
I0130 21:35:06.094546 139741760567040 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0845866203308105, loss=4.958518981933594
I0130 21:35:51.775062 139741768959744 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.438480257987976, loss=4.866322040557861
I0130 21:36:37.559001 139741760567040 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.6729644536972046, loss=3.894543409347534
I0130 21:37:23.550664 139741768959744 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.583970069885254, loss=3.217977523803711
I0130 21:38:09.426540 139741760567040 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.5653417110443115, loss=3.478935718536377
I0130 21:38:33.920136 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:38:45.752160 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:39:09.748006 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:39:11.345266 139936116377408 submission_runner.py:408] Time since start: 28225.17s, 	Step: 57555, 	{'train/accuracy': 0.6287499666213989, 'train/loss': 1.6723737716674805, 'validation/accuracy': 0.5836600065231323, 'validation/loss': 1.8777090311050415, 'validation/num_examples': 50000, 'test/accuracy': 0.4666000306606293, 'test/loss': 2.5032131671905518, 'test/num_examples': 10000, 'score': 26093.89505290985, 'total_duration': 28225.165060520172, 'accumulated_submission_time': 26093.89505290985, 'accumulated_eval_time': 2126.1931478977203, 'accumulated_logging_time': 2.051152229309082}
I0130 21:39:11.369096 139741768959744 logging_writer.py:48] [57555] accumulated_eval_time=2126.193148, accumulated_logging_time=2.051152, accumulated_submission_time=26093.895053, global_step=57555, preemption_count=0, score=26093.895053, test/accuracy=0.466600, test/loss=2.503213, test/num_examples=10000, total_duration=28225.165061, train/accuracy=0.628750, train/loss=1.672374, validation/accuracy=0.583660, validation/loss=1.877709, validation/num_examples=50000
I0130 21:39:29.655841 139741760567040 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.3478378057479858, loss=4.128739356994629
I0130 21:40:13.034292 139741768959744 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.6451479196548462, loss=3.3611724376678467
I0130 21:40:58.859405 139741760567040 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.4996792078018188, loss=3.814601182937622
I0130 21:41:45.061125 139741768959744 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.6837533712387085, loss=3.38234281539917
I0130 21:42:31.142032 139741760567040 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.535127878189087, loss=3.2773568630218506
I0130 21:43:17.238388 139741768959744 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.3995736837387085, loss=4.554544925689697
I0130 21:44:03.083625 139741760567040 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.3197026252746582, loss=4.722010612487793
I0130 21:44:48.936952 139741768959744 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.517386555671692, loss=3.260410785675049
I0130 21:45:34.865551 139741760567040 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.6222803592681885, loss=3.3198108673095703
I0130 21:46:11.449790 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:46:23.369770 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:46:48.468989 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:46:50.076850 139936116377408 submission_runner.py:408] Time since start: 28683.90s, 	Step: 58481, 	{'train/accuracy': 0.6403515338897705, 'train/loss': 1.61244797706604, 'validation/accuracy': 0.582859992980957, 'validation/loss': 1.8621419668197632, 'validation/num_examples': 50000, 'test/accuracy': 0.4684000313282013, 'test/loss': 2.4887142181396484, 'test/num_examples': 10000, 'score': 26513.911805152893, 'total_duration': 28683.89664721489, 'accumulated_submission_time': 26513.911805152893, 'accumulated_eval_time': 2164.8202011585236, 'accumulated_logging_time': 2.0870184898376465}
I0130 21:46:50.105030 139741768959744 logging_writer.py:48] [58481] accumulated_eval_time=2164.820201, accumulated_logging_time=2.087018, accumulated_submission_time=26513.911805, global_step=58481, preemption_count=0, score=26513.911805, test/accuracy=0.468400, test/loss=2.488714, test/num_examples=10000, total_duration=28683.896647, train/accuracy=0.640352, train/loss=1.612448, validation/accuracy=0.582860, validation/loss=1.862142, validation/num_examples=50000
I0130 21:46:58.053854 139741760567040 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.298341155052185, loss=4.256979942321777
I0130 21:47:40.052600 139741768959744 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.4950194358825684, loss=3.6029770374298096
I0130 21:48:25.823869 139741760567040 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.6429240703582764, loss=3.332357883453369
I0130 21:49:12.133144 139741768959744 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.4904308319091797, loss=3.3859875202178955
I0130 21:49:58.086175 139741760567040 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.170517086982727, loss=4.45646858215332
I0130 21:50:43.754052 139741768959744 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.552565336227417, loss=3.291424512863159
I0130 21:51:29.883648 139741760567040 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.4673378467559814, loss=3.8551135063171387
I0130 21:52:16.043566 139741768959744 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.7851063013076782, loss=3.2274467945098877
I0130 21:53:02.003628 139741760567040 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.1194260120391846, loss=5.405107021331787
I0130 21:53:48.035753 139741768959744 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.2525655031204224, loss=5.563337326049805
I0130 21:53:50.399001 139936116377408 spec.py:321] Evaluating on the training split.
I0130 21:54:02.440927 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 21:54:27.465471 139936116377408 spec.py:349] Evaluating on the test split.
I0130 21:54:29.075289 139936116377408 submission_runner.py:408] Time since start: 29142.90s, 	Step: 59407, 	{'train/accuracy': 0.6268945336341858, 'train/loss': 1.6685230731964111, 'validation/accuracy': 0.5874199867248535, 'validation/loss': 1.8534951210021973, 'validation/num_examples': 50000, 'test/accuracy': 0.47380003333091736, 'test/loss': 2.4757091999053955, 'test/num_examples': 10000, 'score': 26934.146147489548, 'total_duration': 29142.89506626129, 'accumulated_submission_time': 26934.146147489548, 'accumulated_eval_time': 2203.4964802265167, 'accumulated_logging_time': 2.1264965534210205}
I0130 21:54:29.102958 139741760567040 logging_writer.py:48] [59407] accumulated_eval_time=2203.496480, accumulated_logging_time=2.126497, accumulated_submission_time=26934.146147, global_step=59407, preemption_count=0, score=26934.146147, test/accuracy=0.473800, test/loss=2.475709, test/num_examples=10000, total_duration=29142.895066, train/accuracy=0.626895, train/loss=1.668523, validation/accuracy=0.587420, validation/loss=1.853495, validation/num_examples=50000
I0130 21:55:07.440152 139741768959744 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.3542917966842651, loss=5.434773921966553
I0130 21:55:53.053925 139741760567040 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.715915322303772, loss=3.296308994293213
I0130 21:56:39.185077 139741768959744 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.179505467414856, loss=5.193676948547363
I0130 21:57:25.062959 139741760567040 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.498504400253296, loss=3.527756690979004
I0130 21:58:11.026576 139741768959744 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.6337299346923828, loss=3.260694742202759
I0130 21:58:56.613092 139741760567040 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.303815245628357, loss=4.051119804382324
I0130 21:59:42.561904 139741768959744 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.7048717737197876, loss=3.3117237091064453
I0130 22:00:28.671109 139741760567040 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.5950978994369507, loss=3.3028993606567383
I0130 22:01:14.571640 139741768959744 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.4477384090423584, loss=4.46200704574585
I0130 22:01:29.392728 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:01:41.424116 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:02:05.201415 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:02:06.817109 139936116377408 submission_runner.py:408] Time since start: 29600.64s, 	Step: 60334, 	{'train/accuracy': 0.6327733993530273, 'train/loss': 1.6337242126464844, 'validation/accuracy': 0.5888599753379822, 'validation/loss': 1.8365858793258667, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.4529848098754883, 'test/num_examples': 10000, 'score': 27354.376095294952, 'total_duration': 29600.636869430542, 'accumulated_submission_time': 27354.376095294952, 'accumulated_eval_time': 2240.920811891556, 'accumulated_logging_time': 2.165325164794922}
I0130 22:02:06.845131 139741760567040 logging_writer.py:48] [60334] accumulated_eval_time=2240.920812, accumulated_logging_time=2.165325, accumulated_submission_time=27354.376095, global_step=60334, preemption_count=0, score=27354.376095, test/accuracy=0.476200, test/loss=2.452985, test/num_examples=10000, total_duration=29600.636869, train/accuracy=0.632773, train/loss=1.633724, validation/accuracy=0.588860, validation/loss=1.836586, validation/num_examples=50000
I0130 22:02:33.485724 139741768959744 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.6056900024414062, loss=3.287381172180176
I0130 22:03:18.390409 139741760567040 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.943931221961975, loss=3.380147933959961
I0130 22:04:04.631569 139741768959744 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.4641417264938354, loss=3.609096050262451
I0130 22:04:50.565428 139741760567040 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.5558234453201294, loss=3.308786153793335
I0130 22:05:36.462061 139741768959744 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.534223198890686, loss=3.398320436477661
I0130 22:06:22.253565 139741760567040 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.4645692110061646, loss=4.188255786895752
I0130 22:07:08.363606 139741768959744 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.3695498704910278, loss=4.814197540283203
I0130 22:07:54.028251 139741760567040 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.580778956413269, loss=3.422959804534912
I0130 22:08:39.675014 139741768959744 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.1966629028320312, loss=4.500712871551514
I0130 22:09:06.908104 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:09:18.806367 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:09:38.953878 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:09:40.576385 139936116377408 submission_runner.py:408] Time since start: 30054.40s, 	Step: 61261, 	{'train/accuracy': 0.6378515362739563, 'train/loss': 1.6398062705993652, 'validation/accuracy': 0.5875999927520752, 'validation/loss': 1.8620257377624512, 'validation/num_examples': 50000, 'test/accuracy': 0.4675000309944153, 'test/loss': 2.50374174118042, 'test/num_examples': 10000, 'score': 27774.38081717491, 'total_duration': 30054.39617562294, 'accumulated_submission_time': 27774.38081717491, 'accumulated_eval_time': 2274.5890777111053, 'accumulated_logging_time': 2.203279733657837}
I0130 22:09:40.605855 139741760567040 logging_writer.py:48] [61261] accumulated_eval_time=2274.589078, accumulated_logging_time=2.203280, accumulated_submission_time=27774.380817, global_step=61261, preemption_count=0, score=27774.380817, test/accuracy=0.467500, test/loss=2.503742, test/num_examples=10000, total_duration=30054.396176, train/accuracy=0.637852, train/loss=1.639806, validation/accuracy=0.587600, validation/loss=1.862026, validation/num_examples=50000
I0130 22:09:56.524677 139741768959744 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.565219759941101, loss=3.2485365867614746
I0130 22:10:40.410203 139741760567040 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.592590570449829, loss=3.2320024967193604
I0130 22:11:26.331300 139741768959744 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.9371150732040405, loss=3.284363269805908
I0130 22:12:12.800590 139741760567040 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.5766526460647583, loss=3.561976909637451
I0130 22:12:58.693444 139741768959744 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.4312409162521362, loss=4.483053207397461
I0130 22:13:44.737834 139741760567040 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.49746572971344, loss=3.3080978393554688
I0130 22:14:30.750502 139741768959744 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.7070159912109375, loss=3.273646354675293
I0130 22:15:16.547057 139741760567040 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.6324397325515747, loss=3.3330674171447754
I0130 22:16:02.450317 139741768959744 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.7219219207763672, loss=3.5784595012664795
I0130 22:16:40.659976 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:16:52.627008 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:17:14.930529 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:17:16.545762 139936116377408 submission_runner.py:408] Time since start: 30510.37s, 	Step: 62185, 	{'train/accuracy': 0.6422656178474426, 'train/loss': 1.6118204593658447, 'validation/accuracy': 0.5854799747467041, 'validation/loss': 1.8537278175354004, 'validation/num_examples': 50000, 'test/accuracy': 0.4650000333786011, 'test/loss': 2.4965875148773193, 'test/num_examples': 10000, 'score': 28194.3759431839, 'total_duration': 30510.36556315422, 'accumulated_submission_time': 28194.3759431839, 'accumulated_eval_time': 2310.4748668670654, 'accumulated_logging_time': 2.243015766143799}
I0130 22:17:16.573960 139741760567040 logging_writer.py:48] [62185] accumulated_eval_time=2310.474867, accumulated_logging_time=2.243016, accumulated_submission_time=28194.375943, global_step=62185, preemption_count=0, score=28194.375943, test/accuracy=0.465000, test/loss=2.496588, test/num_examples=10000, total_duration=30510.365563, train/accuracy=0.642266, train/loss=1.611820, validation/accuracy=0.585480, validation/loss=1.853728, validation/num_examples=50000
I0130 22:17:22.938296 139741768959744 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.6104059219360352, loss=3.2569096088409424
I0130 22:18:04.602493 139741760567040 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.4464118480682373, loss=5.454934597015381
I0130 22:18:50.437327 139741768959744 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.511523962020874, loss=3.33901309967041
I0130 22:19:36.267542 139741760567040 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.726594090461731, loss=3.778395652770996
I0130 22:20:22.402645 139741768959744 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.4524437189102173, loss=3.3341116905212402
I0130 22:21:08.321375 139741760567040 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.2926615476608276, loss=4.589843273162842
I0130 22:21:54.208500 139741768959744 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.2820760011672974, loss=5.147732257843018
I0130 22:22:40.257330 139741760567040 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.7086455821990967, loss=3.2439258098602295
I0130 22:23:26.389513 139741768959744 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.683678150177002, loss=3.256554365158081
I0130 22:24:12.393034 139741760567040 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.4770756959915161, loss=3.222606897354126
I0130 22:24:16.624816 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:24:28.338746 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:24:51.254697 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:24:52.865942 139936116377408 submission_runner.py:408] Time since start: 30966.69s, 	Step: 63111, 	{'train/accuracy': 0.6295117139816284, 'train/loss': 1.700981855392456, 'validation/accuracy': 0.5878599882125854, 'validation/loss': 1.883209466934204, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.5159428119659424, 'test/num_examples': 10000, 'score': 28614.368557929993, 'total_duration': 30966.68574333191, 'accumulated_submission_time': 28614.368557929993, 'accumulated_eval_time': 2346.715988636017, 'accumulated_logging_time': 2.2811131477355957}
I0130 22:24:52.892889 139741768959744 logging_writer.py:48] [63111] accumulated_eval_time=2346.715989, accumulated_logging_time=2.281113, accumulated_submission_time=28614.368558, global_step=63111, preemption_count=0, score=28614.368558, test/accuracy=0.472600, test/loss=2.515943, test/num_examples=10000, total_duration=30966.685743, train/accuracy=0.629512, train/loss=1.700982, validation/accuracy=0.587860, validation/loss=1.883209, validation/num_examples=50000
I0130 22:25:28.832340 139741760567040 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.30983567237854, loss=4.775147438049316
I0130 22:26:14.923619 139741768959744 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.414122462272644, loss=3.598803758621216
I0130 22:27:00.987054 139741760567040 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.1637805700302124, loss=5.41022253036499
I0130 22:27:46.807492 139741768959744 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.5779457092285156, loss=4.0463762283325195
I0130 22:28:32.744852 139741760567040 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.2158942222595215, loss=4.818108081817627
I0130 22:29:18.467147 139741768959744 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.5266703367233276, loss=3.304694890975952
I0130 22:30:04.657113 139741760567040 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.2268037796020508, loss=4.775081634521484
I0130 22:30:50.480821 139741768959744 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.5716949701309204, loss=4.968616008758545
I0130 22:31:36.823425 139741760567040 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.5534406900405884, loss=3.255980968475342
I0130 22:31:53.122630 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:32:05.108503 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:32:27.980995 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:32:29.597646 139936116377408 submission_runner.py:408] Time since start: 31423.42s, 	Step: 64038, 	{'train/accuracy': 0.6376757621765137, 'train/loss': 1.6427497863769531, 'validation/accuracy': 0.5946999788284302, 'validation/loss': 1.8466299772262573, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.4646663665771484, 'test/num_examples': 10000, 'score': 29034.539268016815, 'total_duration': 31423.417449235916, 'accumulated_submission_time': 29034.539268016815, 'accumulated_eval_time': 2383.1910014152527, 'accumulated_logging_time': 2.3187735080718994}
I0130 22:32:29.622111 139741768959744 logging_writer.py:48] [64038] accumulated_eval_time=2383.191001, accumulated_logging_time=2.318774, accumulated_submission_time=29034.539268, global_step=64038, preemption_count=0, score=29034.539268, test/accuracy=0.477100, test/loss=2.464666, test/num_examples=10000, total_duration=31423.417449, train/accuracy=0.637676, train/loss=1.642750, validation/accuracy=0.594700, validation/loss=1.846630, validation/num_examples=50000
I0130 22:32:54.666956 139741760567040 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.7525935173034668, loss=3.453376531600952
I0130 22:33:38.965351 139741768959744 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.2369695901870728, loss=5.3618059158325195
I0130 22:34:25.177346 139741760567040 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.532539963722229, loss=3.2095141410827637
I0130 22:35:11.300394 139741768959744 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.303794264793396, loss=5.204959392547607
I0130 22:35:57.202054 139741760567040 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.7966821193695068, loss=3.2842588424682617
I0130 22:36:43.059283 139741768959744 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.4715889692306519, loss=3.5371880531311035
I0130 22:37:28.980704 139741760567040 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.6254982948303223, loss=3.2928900718688965
I0130 22:38:15.043729 139741768959744 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.5313761234283447, loss=3.5892183780670166
I0130 22:39:00.887430 139741760567040 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.820676565170288, loss=4.05560827255249
I0130 22:39:29.780687 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:39:41.754118 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:40:05.762460 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:40:07.372600 139936116377408 submission_runner.py:408] Time since start: 31881.19s, 	Step: 64965, 	{'train/accuracy': 0.6559374928474426, 'train/loss': 1.5458028316497803, 'validation/accuracy': 0.5884599685668945, 'validation/loss': 1.832724690437317, 'validation/num_examples': 50000, 'test/accuracy': 0.4723000228404999, 'test/loss': 2.4600064754486084, 'test/num_examples': 10000, 'score': 29454.639416217804, 'total_duration': 31881.192403554916, 'accumulated_submission_time': 29454.639416217804, 'accumulated_eval_time': 2420.782904148102, 'accumulated_logging_time': 2.3532867431640625}
I0130 22:40:07.397164 139741768959744 logging_writer.py:48] [64965] accumulated_eval_time=2420.782904, accumulated_logging_time=2.353287, accumulated_submission_time=29454.639416, global_step=64965, preemption_count=0, score=29454.639416, test/accuracy=0.472300, test/loss=2.460006, test/num_examples=10000, total_duration=31881.192404, train/accuracy=0.655937, train/loss=1.545803, validation/accuracy=0.588460, validation/loss=1.832725, validation/num_examples=50000
I0130 22:40:21.721171 139741760567040 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.482325553894043, loss=5.079877853393555
I0130 22:41:04.524510 139741768959744 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.3340097665786743, loss=4.0894293785095215
I0130 22:41:50.027483 139741760567040 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.6281251907348633, loss=3.341587543487549
I0130 22:42:36.057718 139741768959744 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.5532375574111938, loss=5.433506965637207
I0130 22:43:21.793807 139741760567040 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.6629005670547485, loss=3.3224289417266846
I0130 22:44:07.756660 139741768959744 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.5420210361480713, loss=3.3707849979400635
I0130 22:44:53.533395 139741760567040 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.6132553815841675, loss=3.4879751205444336
I0130 22:45:39.446737 139741768959744 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.7689934968948364, loss=3.2769973278045654
I0130 22:46:25.523923 139741760567040 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.6402220726013184, loss=3.22635817527771
I0130 22:47:07.520229 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:47:19.334412 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:47:42.640522 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:47:44.245863 139936116377408 submission_runner.py:408] Time since start: 32338.07s, 	Step: 65893, 	{'train/accuracy': 0.6297070384025574, 'train/loss': 1.6710429191589355, 'validation/accuracy': 0.5890399813652039, 'validation/loss': 1.8539769649505615, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.482262134552002, 'test/num_examples': 10000, 'score': 29874.705132722855, 'total_duration': 32338.065663814545, 'accumulated_submission_time': 29874.705132722855, 'accumulated_eval_time': 2457.5085434913635, 'accumulated_logging_time': 2.387108564376831}
I0130 22:47:44.275788 139741768959744 logging_writer.py:48] [65893] accumulated_eval_time=2457.508543, accumulated_logging_time=2.387109, accumulated_submission_time=29874.705133, global_step=65893, preemption_count=0, score=29874.705133, test/accuracy=0.475900, test/loss=2.482262, test/num_examples=10000, total_duration=32338.065664, train/accuracy=0.629707, train/loss=1.671043, validation/accuracy=0.589040, validation/loss=1.853977, validation/num_examples=50000
I0130 22:47:47.459326 139741760567040 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.6626852750778198, loss=3.7472779750823975
I0130 22:48:28.753956 139741768959744 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.6999601125717163, loss=3.198122978210449
I0130 22:49:14.822380 139741760567040 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.210986614227295, loss=4.857369899749756
I0130 22:50:01.119558 139741768959744 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.496443510055542, loss=3.472773790359497
I0130 22:50:47.652146 139741760567040 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.6271049976348877, loss=3.2394466400146484
I0130 22:51:33.673144 139741768959744 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.7047405242919922, loss=3.5817854404449463
I0130 22:52:20.091478 139741760567040 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.689095139503479, loss=3.17349910736084
I0130 22:53:06.020212 139741768959744 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.5429387092590332, loss=3.3182590007781982
I0130 22:53:52.034119 139741760567040 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.500985026359558, loss=5.350062370300293
I0130 22:54:38.556092 139741768959744 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.6042176485061646, loss=3.6917192935943604
I0130 22:54:44.254662 139936116377408 spec.py:321] Evaluating on the training split.
I0130 22:54:56.120162 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 22:55:20.957458 139936116377408 spec.py:349] Evaluating on the test split.
I0130 22:55:22.573558 139936116377408 submission_runner.py:408] Time since start: 32796.39s, 	Step: 66814, 	{'train/accuracy': 0.6425976157188416, 'train/loss': 1.6025398969650269, 'validation/accuracy': 0.5962600111961365, 'validation/loss': 1.8162277936935425, 'validation/num_examples': 50000, 'test/accuracy': 0.47050002217292786, 'test/loss': 2.4594082832336426, 'test/num_examples': 10000, 'score': 30294.62602829933, 'total_duration': 32796.39333939552, 'accumulated_submission_time': 30294.62602829933, 'accumulated_eval_time': 2495.827398777008, 'accumulated_logging_time': 2.426995038986206}
I0130 22:55:22.607103 139741760567040 logging_writer.py:48] [66814] accumulated_eval_time=2495.827399, accumulated_logging_time=2.426995, accumulated_submission_time=30294.626028, global_step=66814, preemption_count=0, score=30294.626028, test/accuracy=0.470500, test/loss=2.459408, test/num_examples=10000, total_duration=32796.393339, train/accuracy=0.642598, train/loss=1.602540, validation/accuracy=0.596260, validation/loss=1.816228, validation/num_examples=50000
I0130 22:55:57.444909 139741768959744 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3712825775146484, loss=4.577697277069092
I0130 22:56:43.435539 139741760567040 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.2845568656921387, loss=5.219566822052002
I0130 22:57:29.476685 139741768959744 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.648176670074463, loss=3.2668237686157227
I0130 22:58:15.352282 139741760567040 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.2936639785766602, loss=4.416524887084961
I0130 22:59:01.570911 139741768959744 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.5680733919143677, loss=4.52918004989624
I0130 22:59:47.585108 139741760567040 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.7999132871627808, loss=3.5691978931427
I0130 23:00:33.842158 139741768959744 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.7498263120651245, loss=3.308422327041626
I0130 23:01:20.272655 139741760567040 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.7678098678588867, loss=3.4416279792785645
I0130 23:02:06.301256 139741768959744 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.596877098083496, loss=3.2487826347351074
I0130 23:02:22.968838 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:02:34.866977 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:03:00.971182 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:03:02.582323 139936116377408 submission_runner.py:408] Time since start: 33256.40s, 	Step: 67738, 	{'train/accuracy': 0.6525781154632568, 'train/loss': 1.570324182510376, 'validation/accuracy': 0.6031599640846252, 'validation/loss': 1.8069418668746948, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.4569497108459473, 'test/num_examples': 10000, 'score': 30714.93037724495, 'total_duration': 33256.4021191597, 'accumulated_submission_time': 30714.93037724495, 'accumulated_eval_time': 2535.44087266922, 'accumulated_logging_time': 2.4705276489257812}
I0130 23:03:02.609492 139741760567040 logging_writer.py:48] [67738] accumulated_eval_time=2535.440873, accumulated_logging_time=2.470528, accumulated_submission_time=30714.930377, global_step=67738, preemption_count=0, score=30714.930377, test/accuracy=0.478000, test/loss=2.456950, test/num_examples=10000, total_duration=33256.402119, train/accuracy=0.652578, train/loss=1.570324, validation/accuracy=0.603160, validation/loss=1.806942, validation/num_examples=50000
I0130 23:03:27.817103 139741768959744 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.548094630241394, loss=3.224039077758789
I0130 23:04:12.277319 139741760567040 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.647061824798584, loss=5.455694198608398
I0130 23:04:58.456095 139741768959744 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.433458924293518, loss=3.807478904724121
I0130 23:05:44.534937 139741760567040 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.576060175895691, loss=3.7196006774902344
I0130 23:06:30.561348 139741768959744 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.49972403049469, loss=3.5921988487243652
I0130 23:07:16.595974 139741760567040 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.6469123363494873, loss=3.4653944969177246
I0130 23:08:02.714268 139741768959744 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3962533473968506, loss=4.136295795440674
I0130 23:08:48.554911 139741760567040 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.6919734477996826, loss=3.331490993499756
I0130 23:09:34.593032 139741768959744 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.7495718002319336, loss=3.248262882232666
I0130 23:10:02.831770 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:10:14.735097 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:10:41.413200 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:10:43.021075 139936116377408 submission_runner.py:408] Time since start: 33716.84s, 	Step: 68663, 	{'train/accuracy': 0.6356640458106995, 'train/loss': 1.6274088621139526, 'validation/accuracy': 0.5971199870109558, 'validation/loss': 1.8149526119232178, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.4471421241760254, 'test/num_examples': 10000, 'score': 31135.09487080574, 'total_duration': 33716.840874910355, 'accumulated_submission_time': 31135.09487080574, 'accumulated_eval_time': 2575.6301860809326, 'accumulated_logging_time': 2.5075490474700928}
I0130 23:10:43.049131 139741760567040 logging_writer.py:48] [68663] accumulated_eval_time=2575.630186, accumulated_logging_time=2.507549, accumulated_submission_time=31135.094871, global_step=68663, preemption_count=0, score=31135.094871, test/accuracy=0.475400, test/loss=2.447142, test/num_examples=10000, total_duration=33716.840875, train/accuracy=0.635664, train/loss=1.627409, validation/accuracy=0.597120, validation/loss=1.814953, validation/num_examples=50000
I0130 23:10:58.370812 139741768959744 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.5928609371185303, loss=3.103911876678467
I0130 23:11:42.088057 139741760567040 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2623666524887085, loss=5.31343936920166
I0130 23:12:27.795728 139741768959744 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.7144896984100342, loss=3.3336009979248047
I0130 23:13:13.868903 139741760567040 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.5801265239715576, loss=3.1692519187927246
I0130 23:13:59.702578 139741768959744 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.5721880197525024, loss=3.1489317417144775
I0130 23:14:45.767345 139741760567040 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.558255910873413, loss=3.158900022506714
I0130 23:15:31.629587 139741768959744 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.645581603050232, loss=3.2386670112609863
I0130 23:16:17.481366 139741760567040 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.6276155710220337, loss=3.212179660797119
I0130 23:17:03.731553 139741768959744 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.8217004537582397, loss=3.300656795501709
I0130 23:17:43.392304 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:17:55.513905 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:18:20.443455 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:18:22.051080 139936116377408 submission_runner.py:408] Time since start: 34175.87s, 	Step: 69587, 	{'train/accuracy': 0.6425390243530273, 'train/loss': 1.6058720350265503, 'validation/accuracy': 0.6004999876022339, 'validation/loss': 1.7994102239608765, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.4608607292175293, 'test/num_examples': 10000, 'score': 31555.165743112564, 'total_duration': 34175.87082648277, 'accumulated_submission_time': 31555.165743112564, 'accumulated_eval_time': 2614.288892507553, 'accumulated_logging_time': 2.7598090171813965}
I0130 23:18:22.081268 139741760567040 logging_writer.py:48] [69587] accumulated_eval_time=2614.288893, accumulated_logging_time=2.759809, accumulated_submission_time=31555.165743, global_step=69587, preemption_count=0, score=31555.165743, test/accuracy=0.476100, test/loss=2.460861, test/num_examples=10000, total_duration=34175.870826, train/accuracy=0.642539, train/loss=1.605872, validation/accuracy=0.600500, validation/loss=1.799410, validation/num_examples=50000
I0130 23:18:27.643691 139741768959744 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.5288259983062744, loss=3.2180583477020264
I0130 23:19:09.407177 139741760567040 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.7075430154800415, loss=3.1590499877929688
I0130 23:19:55.427929 139741768959744 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.443816900253296, loss=4.110730171203613
I0130 23:20:42.219993 139741760567040 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.6802362203598022, loss=3.2547714710235596
I0130 23:21:28.210772 139741768959744 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.744248867034912, loss=3.21885347366333
I0130 23:22:14.818193 139741760567040 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.5995606184005737, loss=3.288609027862549
I0130 23:23:00.793678 139741768959744 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.6279830932617188, loss=3.549708366394043
I0130 23:23:46.918450 139741760567040 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.477137565612793, loss=4.218610763549805
I0130 23:24:33.007888 139741768959744 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.516103744506836, loss=3.360281229019165
I0130 23:25:19.179366 139741760567040 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.7381986379623413, loss=3.213622570037842
I0130 23:25:22.060394 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:25:33.855099 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:26:00.196599 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:26:01.805099 139936116377408 submission_runner.py:408] Time since start: 34635.62s, 	Step: 70508, 	{'train/accuracy': 0.6518359184265137, 'train/loss': 1.5789217948913574, 'validation/accuracy': 0.5998600125312805, 'validation/loss': 1.8045138120651245, 'validation/num_examples': 50000, 'test/accuracy': 0.4789000153541565, 'test/loss': 2.4507243633270264, 'test/num_examples': 10000, 'score': 31975.086597681046, 'total_duration': 34635.624881505966, 'accumulated_submission_time': 31975.086597681046, 'accumulated_eval_time': 2654.033591747284, 'accumulated_logging_time': 2.801017999649048}
I0130 23:26:01.841146 139741768959744 logging_writer.py:48] [70508] accumulated_eval_time=2654.033592, accumulated_logging_time=2.801018, accumulated_submission_time=31975.086598, global_step=70508, preemption_count=0, score=31975.086598, test/accuracy=0.478900, test/loss=2.450724, test/num_examples=10000, total_duration=34635.624882, train/accuracy=0.651836, train/loss=1.578922, validation/accuracy=0.599860, validation/loss=1.804514, validation/num_examples=50000
I0130 23:26:39.740395 139741760567040 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.6981011629104614, loss=3.254451274871826
I0130 23:27:25.672316 139741768959744 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.706921935081482, loss=3.2660152912139893
I0130 23:28:11.894461 139741760567040 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.7342348098754883, loss=3.1804544925689697
I0130 23:28:57.815089 139741768959744 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.2924822568893433, loss=4.365906238555908
I0130 23:29:44.045490 139741760567040 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3968992233276367, loss=5.407196044921875
I0130 23:30:30.108670 139741768959744 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.706619143486023, loss=3.2507848739624023
I0130 23:31:16.173918 139741760567040 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.6853182315826416, loss=3.344278335571289
I0130 23:32:02.495866 139741768959744 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.7606619596481323, loss=3.4331769943237305
I0130 23:32:48.678654 139741760567040 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.29198157787323, loss=4.177173614501953
I0130 23:33:02.331037 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:33:14.078184 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:33:39.355236 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:33:40.970186 139936116377408 submission_runner.py:408] Time since start: 35094.79s, 	Step: 71431, 	{'train/accuracy': 0.6530663967132568, 'train/loss': 1.5456774234771729, 'validation/accuracy': 0.6032800078392029, 'validation/loss': 1.7672827243804932, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.4054059982299805, 'test/num_examples': 10000, 'score': 32395.517565488815, 'total_duration': 35094.78996706009, 'accumulated_submission_time': 32395.517565488815, 'accumulated_eval_time': 2692.672725915909, 'accumulated_logging_time': 2.847104072570801}
I0130 23:33:40.999796 139741768959744 logging_writer.py:48] [71431] accumulated_eval_time=2692.672726, accumulated_logging_time=2.847104, accumulated_submission_time=32395.517565, global_step=71431, preemption_count=0, score=32395.517565, test/accuracy=0.484000, test/loss=2.405406, test/num_examples=10000, total_duration=35094.789967, train/accuracy=0.653066, train/loss=1.545677, validation/accuracy=0.603280, validation/loss=1.767283, validation/num_examples=50000
I0130 23:34:08.843939 139741760567040 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.9696773290634155, loss=3.486013412475586
I0130 23:34:53.672986 139741768959744 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.3967498540878296, loss=4.1875104904174805
I0130 23:35:40.289146 139741760567040 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.8034480810165405, loss=3.130631923675537
I0130 23:36:26.627855 139741768959744 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.311349868774414, loss=4.73241662979126
I0130 23:37:12.862556 139741760567040 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.7028627395629883, loss=3.1894569396972656
I0130 23:37:59.153486 139741768959744 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.6301344633102417, loss=3.522540330886841
I0130 23:38:45.189009 139741760567040 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.4487359523773193, loss=5.30341911315918
I0130 23:39:31.214229 139741768959744 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.6683954000473022, loss=3.286975622177124
I0130 23:40:17.625122 139741760567040 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3463813066482544, loss=3.732715368270874
I0130 23:40:41.202070 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:40:53.067642 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:41:16.336550 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:41:17.943396 139936116377408 submission_runner.py:408] Time since start: 35551.76s, 	Step: 72353, 	{'train/accuracy': 0.6449413895606995, 'train/loss': 1.5734144449234009, 'validation/accuracy': 0.6037600040435791, 'validation/loss': 1.765405535697937, 'validation/num_examples': 50000, 'test/accuracy': 0.48420003056526184, 'test/loss': 2.39375638961792, 'test/num_examples': 10000, 'score': 32815.659552812576, 'total_duration': 35551.76320028305, 'accumulated_submission_time': 32815.659552812576, 'accumulated_eval_time': 2729.4140496253967, 'accumulated_logging_time': 2.888112783432007}
I0130 23:41:17.969349 139741768959744 logging_writer.py:48] [72353] accumulated_eval_time=2729.414050, accumulated_logging_time=2.888113, accumulated_submission_time=32815.659553, global_step=72353, preemption_count=0, score=32815.659553, test/accuracy=0.484200, test/loss=2.393756, test/num_examples=10000, total_duration=35551.763200, train/accuracy=0.644941, train/loss=1.573414, validation/accuracy=0.603760, validation/loss=1.765406, validation/num_examples=50000
I0130 23:41:37.109393 139741760567040 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.2550946474075317, loss=4.661667346954346
I0130 23:42:20.889554 139741768959744 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.7353476285934448, loss=3.271113395690918
I0130 23:43:06.937235 139741760567040 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.5466418266296387, loss=3.9988231658935547
I0130 23:43:52.957358 139741768959744 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.5774531364440918, loss=3.353240966796875
I0130 23:44:39.118595 139741760567040 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.2461870908737183, loss=4.968425273895264
I0130 23:45:25.388761 139741768959744 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.4002503156661987, loss=5.414728164672852
I0130 23:46:11.466391 139741760567040 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.5571647882461548, loss=4.375890731811523
I0130 23:46:57.257936 139741768959744 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.8154031038284302, loss=3.20028018951416
I0130 23:47:43.388480 139741760567040 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.2873209714889526, loss=5.372230052947998
I0130 23:48:18.053455 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:48:29.907819 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:48:53.928117 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:48:55.545599 139936116377408 submission_runner.py:408] Time since start: 36009.37s, 	Step: 73277, 	{'train/accuracy': 0.6486523151397705, 'train/loss': 1.5562613010406494, 'validation/accuracy': 0.6032199859619141, 'validation/loss': 1.7581266164779663, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.3881523609161377, 'test/num_examples': 10000, 'score': 33235.68685173988, 'total_duration': 36009.36540389061, 'accumulated_submission_time': 33235.68685173988, 'accumulated_eval_time': 2766.906188249588, 'accumulated_logging_time': 2.9232640266418457}
I0130 23:48:55.575626 139741768959744 logging_writer.py:48] [73277] accumulated_eval_time=2766.906188, accumulated_logging_time=2.923264, accumulated_submission_time=33235.686852, global_step=73277, preemption_count=0, score=33235.686852, test/accuracy=0.484000, test/loss=2.388152, test/num_examples=10000, total_duration=36009.365404, train/accuracy=0.648652, train/loss=1.556261, validation/accuracy=0.603220, validation/loss=1.758127, validation/num_examples=50000
I0130 23:49:05.115919 139741760567040 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.760428547859192, loss=3.387514114379883
I0130 23:49:47.066241 139741768959744 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.588932752609253, loss=3.3389267921447754
I0130 23:50:33.125996 139741760567040 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.6929302215576172, loss=3.322723627090454
I0130 23:51:18.871330 139741768959744 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.8133076429367065, loss=3.245232105255127
I0130 23:52:04.649938 139741760567040 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.5045788288116455, loss=3.459348201751709
I0130 23:52:50.440454 139741768959744 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.7364445924758911, loss=3.2028868198394775
I0130 23:53:36.637837 139741760567040 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.360519289970398, loss=5.274910926818848
I0130 23:54:22.793275 139741768959744 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.7425518035888672, loss=3.1254348754882812
I0130 23:55:08.913847 139741760567040 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.5445096492767334, loss=5.471441268920898
I0130 23:55:54.866494 139741768959744 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.7644027471542358, loss=3.2574965953826904
I0130 23:55:55.942214 139936116377408 spec.py:321] Evaluating on the training split.
I0130 23:56:08.026270 139936116377408 spec.py:333] Evaluating on the validation split.
I0130 23:56:34.166194 139936116377408 spec.py:349] Evaluating on the test split.
I0130 23:56:35.769075 139936116377408 submission_runner.py:408] Time since start: 36469.59s, 	Step: 74204, 	{'train/accuracy': 0.6731249690055847, 'train/loss': 1.4761346578598022, 'validation/accuracy': 0.6058200001716614, 'validation/loss': 1.766079068183899, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.4043872356414795, 'test/num_examples': 10000, 'score': 33655.99447274208, 'total_duration': 36469.588874578476, 'accumulated_submission_time': 33655.99447274208, 'accumulated_eval_time': 2806.733047246933, 'accumulated_logging_time': 2.963426113128662}
I0130 23:56:35.798159 139741760567040 logging_writer.py:48] [74204] accumulated_eval_time=2806.733047, accumulated_logging_time=2.963426, accumulated_submission_time=33655.994473, global_step=74204, preemption_count=0, score=33655.994473, test/accuracy=0.480000, test/loss=2.404387, test/num_examples=10000, total_duration=36469.588875, train/accuracy=0.673125, train/loss=1.476135, validation/accuracy=0.605820, validation/loss=1.766079, validation/num_examples=50000
I0130 23:57:15.570540 139741768959744 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.7402796745300293, loss=3.19051456451416
I0130 23:58:01.506641 139741760567040 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.601944088935852, loss=4.392080783843994
I0130 23:58:47.825648 139741768959744 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.6274590492248535, loss=3.1930463314056396
I0130 23:59:33.695877 139741760567040 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.5191080570220947, loss=3.678344964981079
I0131 00:00:19.726438 139741768959744 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.6090677976608276, loss=5.378730297088623
I0131 00:01:05.734819 139741760567040 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.7561496496200562, loss=3.193014621734619
I0131 00:01:51.772906 139741768959744 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.357352614402771, loss=4.552568435668945
I0131 00:02:37.785811 139741760567040 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.2976467609405518, loss=5.066181659698486
I0131 00:03:24.224944 139741768959744 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.7560497522354126, loss=3.334066867828369
I0131 00:03:36.160280 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:03:48.089634 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:04:14.953884 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:04:16.577525 139936116377408 submission_runner.py:408] Time since start: 36930.40s, 	Step: 75128, 	{'train/accuracy': 0.6450976133346558, 'train/loss': 1.5667901039123535, 'validation/accuracy': 0.6061399579048157, 'validation/loss': 1.760995626449585, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.39943790435791, 'test/num_examples': 10000, 'score': 34076.297404289246, 'total_duration': 36930.39732980728, 'accumulated_submission_time': 34076.297404289246, 'accumulated_eval_time': 2847.1502919197083, 'accumulated_logging_time': 3.0034356117248535}
I0131 00:04:16.604318 139741760567040 logging_writer.py:48] [75128] accumulated_eval_time=2847.150292, accumulated_logging_time=3.003436, accumulated_submission_time=34076.297404, global_step=75128, preemption_count=0, score=34076.297404, test/accuracy=0.486700, test/loss=2.399438, test/num_examples=10000, total_duration=36930.397330, train/accuracy=0.645098, train/loss=1.566790, validation/accuracy=0.606140, validation/loss=1.760996, validation/num_examples=50000
I0131 00:04:45.612088 139741768959744 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.893622636795044, loss=3.101743698120117
I0131 00:05:31.029355 139741760567040 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.7368805408477783, loss=3.2491250038146973
I0131 00:06:17.204848 139741768959744 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.8807377815246582, loss=3.2707839012145996
I0131 00:07:03.195198 139741760567040 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.581409215927124, loss=3.2799949645996094
I0131 00:07:48.953483 139741768959744 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.5069210529327393, loss=3.6936402320861816
I0131 00:08:35.015568 139741760567040 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.4388030767440796, loss=4.194555282592773
I0131 00:09:21.041914 139741768959744 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.8365010023117065, loss=3.367577075958252
I0131 00:10:07.317708 139741760567040 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.8152118921279907, loss=3.1531338691711426
I0131 00:10:53.225835 139741768959744 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.7336217164993286, loss=3.4521803855895996
I0131 00:11:16.825304 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:11:28.696759 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:11:55.221338 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:11:56.836079 139936116377408 submission_runner.py:408] Time since start: 37390.66s, 	Step: 76053, 	{'train/accuracy': 0.6499999761581421, 'train/loss': 1.5527905225753784, 'validation/accuracy': 0.6094799637794495, 'validation/loss': 1.7591296434402466, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.395556688308716, 'test/num_examples': 10000, 'score': 34496.459659576416, 'total_duration': 37390.65586042404, 'accumulated_submission_time': 34496.459659576416, 'accumulated_eval_time': 2887.1610465049744, 'accumulated_logging_time': 3.0404083728790283}
I0131 00:11:56.865118 139741760567040 logging_writer.py:48] [76053] accumulated_eval_time=2887.161047, accumulated_logging_time=3.040408, accumulated_submission_time=34496.459660, global_step=76053, preemption_count=0, score=34496.459660, test/accuracy=0.492700, test/loss=2.395557, test/num_examples=10000, total_duration=37390.655860, train/accuracy=0.650000, train/loss=1.552791, validation/accuracy=0.609480, validation/loss=1.759130, validation/num_examples=50000
I0131 00:12:15.948479 139741768959744 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.633887767791748, loss=3.043391704559326
I0131 00:12:59.651787 139741760567040 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.9787980318069458, loss=3.231053352355957
I0131 00:13:45.501174 139741768959744 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.6993470191955566, loss=3.2741804122924805
I0131 00:14:32.228274 139741760567040 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.4301819801330566, loss=4.950627326965332
I0131 00:15:18.329194 139741768959744 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.415168046951294, loss=4.487695693969727
I0131 00:16:04.480902 139741760567040 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.5269763469696045, loss=5.2983479499816895
I0131 00:16:50.593407 139741768959744 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.8150748014450073, loss=3.30961012840271
I0131 00:17:36.672534 139741760567040 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.3412617444992065, loss=4.376684665679932
I0131 00:18:22.713416 139741768959744 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.3902781009674072, loss=4.932516098022461
I0131 00:18:57.016373 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:19:09.118320 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:19:32.689187 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:19:34.299923 139936116377408 submission_runner.py:408] Time since start: 37848.12s, 	Step: 76976, 	{'train/accuracy': 0.6608788967132568, 'train/loss': 1.5342339277267456, 'validation/accuracy': 0.6050800085067749, 'validation/loss': 1.7775715589523315, 'validation/num_examples': 50000, 'test/accuracy': 0.48820000886917114, 'test/loss': 2.4163527488708496, 'test/num_examples': 10000, 'score': 34916.55329370499, 'total_duration': 37848.119691848755, 'accumulated_submission_time': 34916.55329370499, 'accumulated_eval_time': 2924.444561958313, 'accumulated_logging_time': 3.0794270038604736}
I0131 00:19:34.334582 139741760567040 logging_writer.py:48] [76976] accumulated_eval_time=2924.444562, accumulated_logging_time=3.079427, accumulated_submission_time=34916.553294, global_step=76976, preemption_count=0, score=34916.553294, test/accuracy=0.488200, test/loss=2.416353, test/num_examples=10000, total_duration=37848.119692, train/accuracy=0.660879, train/loss=1.534234, validation/accuracy=0.605080, validation/loss=1.777572, validation/num_examples=50000
I0131 00:19:44.284762 139741768959744 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.9564287662506104, loss=3.159813642501831
I0131 00:20:26.410878 139741760567040 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.6584645509719849, loss=3.0981035232543945
I0131 00:21:12.145131 139741768959744 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.7672117948532104, loss=3.1838059425354004
I0131 00:21:58.153627 139741760567040 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.7935149669647217, loss=3.2179574966430664
I0131 00:22:44.163084 139741768959744 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.5748088359832764, loss=3.3481452465057373
I0131 00:23:30.296168 139741760567040 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.9288212060928345, loss=3.1323294639587402
I0131 00:24:16.359547 139741768959744 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.5997635126113892, loss=3.4702506065368652
I0131 00:25:02.175579 139741760567040 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.775221347808838, loss=3.279183864593506
I0131 00:25:48.209763 139741768959744 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.6079789400100708, loss=5.250421524047852
I0131 00:26:34.076798 139741760567040 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.5914835929870605, loss=3.547438144683838
I0131 00:26:34.673417 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:26:46.582211 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:27:12.207684 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:27:13.821770 139936116377408 submission_runner.py:408] Time since start: 38307.64s, 	Step: 77903, 	{'train/accuracy': 0.6502929329872131, 'train/loss': 1.5727944374084473, 'validation/accuracy': 0.6092599630355835, 'validation/loss': 1.7659096717834473, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.4130568504333496, 'test/num_examples': 10000, 'score': 35336.832350969315, 'total_duration': 38307.64156937599, 'accumulated_submission_time': 35336.832350969315, 'accumulated_eval_time': 2963.59290099144, 'accumulated_logging_time': 3.1252198219299316}
I0131 00:27:13.848291 139741768959744 logging_writer.py:48] [77903] accumulated_eval_time=2963.592901, accumulated_logging_time=3.125220, accumulated_submission_time=35336.832351, global_step=77903, preemption_count=0, score=35336.832351, test/accuracy=0.490500, test/loss=2.413057, test/num_examples=10000, total_duration=38307.641569, train/accuracy=0.650293, train/loss=1.572794, validation/accuracy=0.609260, validation/loss=1.765910, validation/num_examples=50000
I0131 00:27:53.631051 139741760567040 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.7507964372634888, loss=3.1394295692443848
I0131 00:28:39.442035 139741768959744 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.6691999435424805, loss=3.2178280353546143
I0131 00:29:25.574000 139741760567040 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.8072704076766968, loss=3.2253177165985107
I0131 00:30:11.346830 139741768959744 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.3829549551010132, loss=4.487414360046387
I0131 00:30:57.288357 139741760567040 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.7205195426940918, loss=3.161677837371826
I0131 00:31:43.395380 139741768959744 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.6702637672424316, loss=3.2127585411071777
I0131 00:32:29.358947 139741760567040 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.4874143600463867, loss=3.6849782466888428
I0131 00:33:15.486316 139741768959744 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.7817301750183105, loss=3.191145896911621
I0131 00:34:01.183718 139741760567040 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.4631414413452148, loss=5.083888530731201
I0131 00:34:13.962918 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:34:25.835884 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:34:51.766943 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:34:53.371367 139936116377408 submission_runner.py:408] Time since start: 38767.19s, 	Step: 78829, 	{'train/accuracy': 0.6518749594688416, 'train/loss': 1.5727999210357666, 'validation/accuracy': 0.6112599968910217, 'validation/loss': 1.7607871294021606, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.392758369445801, 'test/num_examples': 10000, 'score': 35756.88972687721, 'total_duration': 38767.19116520882, 'accumulated_submission_time': 35756.88972687721, 'accumulated_eval_time': 3003.001363515854, 'accumulated_logging_time': 3.161072254180908}
I0131 00:34:53.400410 139741768959744 logging_writer.py:48] [78829] accumulated_eval_time=3003.001364, accumulated_logging_time=3.161072, accumulated_submission_time=35756.889727, global_step=78829, preemption_count=0, score=35756.889727, test/accuracy=0.486500, test/loss=2.392758, test/num_examples=10000, total_duration=38767.191165, train/accuracy=0.651875, train/loss=1.572800, validation/accuracy=0.611260, validation/loss=1.760787, validation/num_examples=50000
I0131 00:35:22.014786 139741760567040 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.3775049448013306, loss=5.3243513107299805
I0131 00:36:07.386578 139741768959744 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.5186759233474731, loss=4.359102249145508
I0131 00:36:53.453259 139741760567040 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.0418241024017334, loss=3.2284233570098877
I0131 00:37:39.615519 139741768959744 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.3855215311050415, loss=5.268198013305664
I0131 00:38:25.457008 139741760567040 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.9662843942642212, loss=3.277625560760498
I0131 00:39:11.355856 139741768959744 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.7455899715423584, loss=3.185478925704956
I0131 00:39:56.876075 139741760567040 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.558556079864502, loss=3.146150588989258
I0131 00:40:43.241608 139741768959744 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.366613507270813, loss=4.677069664001465
I0131 00:41:29.439851 139741760567040 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.3717188835144043, loss=5.28814172744751
I0131 00:41:53.574398 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:42:05.582862 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:42:31.415174 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:42:33.018400 139936116377408 submission_runner.py:408] Time since start: 39226.84s, 	Step: 79754, 	{'train/accuracy': 0.6651171445846558, 'train/loss': 1.4895260334014893, 'validation/accuracy': 0.6138399839401245, 'validation/loss': 1.7199392318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.49550002813339233, 'test/loss': 2.342292547225952, 'test/num_examples': 10000, 'score': 36177.00625920296, 'total_duration': 39226.83818221092, 'accumulated_submission_time': 36177.00625920296, 'accumulated_eval_time': 3042.4453415870667, 'accumulated_logging_time': 3.1995127201080322}
I0131 00:42:33.051984 139741768959744 logging_writer.py:48] [79754] accumulated_eval_time=3042.445342, accumulated_logging_time=3.199513, accumulated_submission_time=36177.006259, global_step=79754, preemption_count=0, score=36177.006259, test/accuracy=0.495500, test/loss=2.342293, test/num_examples=10000, total_duration=39226.838182, train/accuracy=0.665117, train/loss=1.489526, validation/accuracy=0.613840, validation/loss=1.719939, validation/num_examples=50000
I0131 00:42:51.726710 139741760567040 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.056392192840576, loss=3.3094797134399414
I0131 00:43:35.482166 139741768959744 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.828320860862732, loss=3.2163236141204834
I0131 00:44:21.593533 139741760567040 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.6318988800048828, loss=3.074950933456421
I0131 00:45:08.136368 139741768959744 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.7078802585601807, loss=3.346574544906616
I0131 00:45:54.163646 139741760567040 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.481553316116333, loss=3.9006967544555664
I0131 00:46:40.389879 139741768959744 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.7446682453155518, loss=3.17223858833313
I0131 00:47:26.562054 139741760567040 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.4091452360153198, loss=4.510695934295654
I0131 00:48:12.633706 139741768959744 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.3951882123947144, loss=5.158534049987793
I0131 00:48:58.561190 139741760567040 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.9184945821762085, loss=3.2071025371551514
I0131 00:49:33.192534 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:49:45.243428 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:50:12.239596 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:50:13.855269 139936116377408 submission_runner.py:408] Time since start: 39687.68s, 	Step: 80677, 	{'train/accuracy': 0.6646288633346558, 'train/loss': 1.4810144901275635, 'validation/accuracy': 0.6114999651908875, 'validation/loss': 1.7160485982894897, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.342613458633423, 'test/num_examples': 10000, 'score': 36597.088631391525, 'total_duration': 39687.675043821335, 'accumulated_submission_time': 36597.088631391525, 'accumulated_eval_time': 3083.1080589294434, 'accumulated_logging_time': 3.2435245513916016}
I0131 00:50:13.890983 139741768959744 logging_writer.py:48] [80677] accumulated_eval_time=3083.108059, accumulated_logging_time=3.243525, accumulated_submission_time=36597.088631, global_step=80677, preemption_count=0, score=36597.088631, test/accuracy=0.493800, test/loss=2.342613, test/num_examples=10000, total_duration=39687.675044, train/accuracy=0.664629, train/loss=1.481014, validation/accuracy=0.611500, validation/loss=1.716049, validation/num_examples=50000
I0131 00:50:23.429182 139741760567040 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.6755918264389038, loss=3.2353618144989014
I0131 00:51:05.959032 139741768959744 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.5585458278656006, loss=4.236922264099121
I0131 00:51:51.843864 139741760567040 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.751584529876709, loss=3.0736212730407715
I0131 00:52:37.939183 139741768959744 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.710915207862854, loss=3.477985143661499
I0131 00:53:23.870131 139741760567040 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.7922697067260742, loss=3.0188241004943848
I0131 00:54:10.020130 139741768959744 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.4378702640533447, loss=5.155369758605957
I0131 00:54:55.742736 139741760567040 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.9026741981506348, loss=3.204620838165283
I0131 00:55:41.737861 139741768959744 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.7655045986175537, loss=3.1684975624084473
I0131 00:56:27.650394 139741760567040 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.6510628461837769, loss=3.3045976161956787
I0131 00:57:13.628413 139741768959744 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.5645676851272583, loss=4.768588542938232
I0131 00:57:14.222408 139936116377408 spec.py:321] Evaluating on the training split.
I0131 00:57:26.300121 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 00:57:54.015850 139936116377408 spec.py:349] Evaluating on the test split.
I0131 00:57:55.632910 139936116377408 submission_runner.py:408] Time since start: 40149.45s, 	Step: 81603, 	{'train/accuracy': 0.6604687571525574, 'train/loss': 1.5106688737869263, 'validation/accuracy': 0.6142599582672119, 'validation/loss': 1.7114691734313965, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.352527379989624, 'test/num_examples': 10000, 'score': 37017.36079597473, 'total_duration': 40149.45268511772, 'accumulated_submission_time': 37017.36079597473, 'accumulated_eval_time': 3124.51851439476, 'accumulated_logging_time': 3.2905027866363525}
I0131 00:57:55.667706 139741760567040 logging_writer.py:48] [81603] accumulated_eval_time=3124.518514, accumulated_logging_time=3.290503, accumulated_submission_time=37017.360796, global_step=81603, preemption_count=0, score=37017.360796, test/accuracy=0.497400, test/loss=2.352527, test/num_examples=10000, total_duration=40149.452685, train/accuracy=0.660469, train/loss=1.510669, validation/accuracy=0.614260, validation/loss=1.711469, validation/num_examples=50000
I0131 00:58:35.615763 139741768959744 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.4960211515426636, loss=5.284489154815674
I0131 00:59:21.506177 139741760567040 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.4011790752410889, loss=5.299644470214844
I0131 01:00:07.694542 139741768959744 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.883326768875122, loss=3.2004971504211426
I0131 01:00:53.435623 139741760567040 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.5652248859405518, loss=4.7614264488220215
I0131 01:01:39.490316 139741768959744 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.6980341672897339, loss=3.2164573669433594
I0131 01:02:25.278306 139741760567040 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.5161210298538208, loss=5.339248180389404
I0131 01:03:11.231733 139741768959744 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.6501833200454712, loss=3.1972031593322754
I0131 01:03:57.167868 139741760567040 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.7541530132293701, loss=3.0894265174865723
I0131 01:04:43.052060 139741768959744 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.521766185760498, loss=3.930802345275879
I0131 01:04:56.065816 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:05:08.083915 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:05:33.727482 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:05:35.333006 139936116377408 submission_runner.py:408] Time since start: 40609.15s, 	Step: 82530, 	{'train/accuracy': 0.6646093726158142, 'train/loss': 1.5200191736221313, 'validation/accuracy': 0.6138399839401245, 'validation/loss': 1.7434662580490112, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.370671510696411, 'test/num_examples': 10000, 'score': 37437.699897289276, 'total_duration': 40609.15281009674, 'accumulated_submission_time': 37437.699897289276, 'accumulated_eval_time': 3163.785692691803, 'accumulated_logging_time': 3.336221933364868}
I0131 01:05:35.363430 139741760567040 logging_writer.py:48] [82530] accumulated_eval_time=3163.785693, accumulated_logging_time=3.336222, accumulated_submission_time=37437.699897, global_step=82530, preemption_count=0, score=37437.699897, test/accuracy=0.493900, test/loss=2.370672, test/num_examples=10000, total_duration=40609.152810, train/accuracy=0.664609, train/loss=1.520019, validation/accuracy=0.613840, validation/loss=1.743466, validation/num_examples=50000
I0131 01:06:03.594646 139741768959744 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.3616610765457153, loss=4.927918910980225
I0131 01:06:48.785143 139741760567040 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.7115638256072998, loss=3.098545551300049
I0131 01:07:35.075785 139741768959744 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.7050199508666992, loss=3.2250404357910156
I0131 01:08:21.145104 139741760567040 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.7187389135360718, loss=3.08518123626709
I0131 01:09:07.255250 139741768959744 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.6872406005859375, loss=4.152834415435791
I0131 01:09:53.031164 139741760567040 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.8461471796035767, loss=3.1648709774017334
I0131 01:10:39.110841 139741768959744 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.8680449724197388, loss=3.325148820877075
I0131 01:11:25.252066 139741760567040 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.6497759819030762, loss=5.133896827697754
I0131 01:12:11.568463 139741768959744 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.8596618175506592, loss=5.266632556915283
I0131 01:12:35.652504 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:12:47.474308 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:13:13.447412 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:13:15.051334 139936116377408 submission_runner.py:408] Time since start: 41068.87s, 	Step: 83454, 	{'train/accuracy': 0.6852148175239563, 'train/loss': 1.3989347219467163, 'validation/accuracy': 0.6177399754524231, 'validation/loss': 1.7047220468521118, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3427956104278564, 'test/num_examples': 10000, 'score': 37857.93138933182, 'total_duration': 41068.871118307114, 'accumulated_submission_time': 37857.93138933182, 'accumulated_eval_time': 3203.184502363205, 'accumulated_logging_time': 3.375791311264038}
I0131 01:13:15.078896 139741760567040 logging_writer.py:48] [83454] accumulated_eval_time=3203.184502, accumulated_logging_time=3.375791, accumulated_submission_time=37857.931389, global_step=83454, preemption_count=0, score=37857.931389, test/accuracy=0.500400, test/loss=2.342796, test/num_examples=10000, total_duration=41068.871118, train/accuracy=0.685215, train/loss=1.398935, validation/accuracy=0.617740, validation/loss=1.704722, validation/num_examples=50000
I0131 01:13:33.775498 139741768959744 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.8633211851119995, loss=3.0863420963287354
I0131 01:14:17.280702 139741760567040 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.5512840747833252, loss=3.8496310710906982
I0131 01:15:03.375767 139741768959744 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.5863611698150635, loss=4.789248466491699
I0131 01:15:49.494256 139741760567040 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.6601605415344238, loss=3.3847789764404297
I0131 01:16:35.935099 139741768959744 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.948004961013794, loss=3.116245985031128
I0131 01:17:21.918362 139741760567040 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.8295668363571167, loss=3.4877936840057373
I0131 01:18:08.369463 139741768959744 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.8335299491882324, loss=3.143282651901245
I0131 01:18:54.399005 139741760567040 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.7904216051101685, loss=3.094780683517456
I0131 01:19:40.589518 139741768959744 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.5298504829406738, loss=4.177771091461182
I0131 01:20:15.404988 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:20:27.220313 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:20:53.459833 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:20:55.062575 139936116377408 submission_runner.py:408] Time since start: 41528.88s, 	Step: 84377, 	{'train/accuracy': 0.6599413752555847, 'train/loss': 1.5086867809295654, 'validation/accuracy': 0.6137599945068359, 'validation/loss': 1.7188671827316284, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.361658811569214, 'test/num_examples': 10000, 'score': 38278.19797229767, 'total_duration': 41528.8823723793, 'accumulated_submission_time': 38278.19797229767, 'accumulated_eval_time': 3242.8420696258545, 'accumulated_logging_time': 3.4148974418640137}
I0131 01:20:55.091978 139741760567040 logging_writer.py:48] [84377] accumulated_eval_time=3242.842070, accumulated_logging_time=3.414897, accumulated_submission_time=38278.197972, global_step=84377, preemption_count=0, score=38278.197972, test/accuracy=0.492700, test/loss=2.361659, test/num_examples=10000, total_duration=41528.882372, train/accuracy=0.659941, train/loss=1.508687, validation/accuracy=0.613760, validation/loss=1.718867, validation/num_examples=50000
I0131 01:21:04.619385 139741768959744 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.9653117656707764, loss=3.1740329265594482
I0131 01:21:46.981389 139741760567040 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.7545475959777832, loss=3.264049768447876
I0131 01:22:32.985293 139741768959744 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.6938111782073975, loss=3.351078987121582
I0131 01:23:19.149764 139741760567040 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.091536521911621, loss=3.2017769813537598
I0131 01:24:05.092112 139741768959744 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.965969443321228, loss=3.1395716667175293
I0131 01:24:51.257681 139741760567040 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.593376874923706, loss=5.259899139404297
I0131 01:25:37.251969 139741768959744 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.7020673751831055, loss=3.3756418228149414
I0131 01:26:23.392199 139741760567040 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.8004802465438843, loss=3.1716885566711426
I0131 01:27:09.514017 139741768959744 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.8868756294250488, loss=3.0985002517700195
I0131 01:27:55.439178 139741760567040 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.5035032033920288, loss=5.253616809844971
I0131 01:27:55.453919 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:28:07.682706 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:28:35.560305 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:28:37.170040 139936116377408 submission_runner.py:408] Time since start: 41990.99s, 	Step: 85301, 	{'train/accuracy': 0.6687890291213989, 'train/loss': 1.4672608375549316, 'validation/accuracy': 0.6248399615287781, 'validation/loss': 1.679897427558899, 'validation/num_examples': 50000, 'test/accuracy': 0.5024999976158142, 'test/loss': 2.314694881439209, 'test/num_examples': 10000, 'score': 38698.49995803833, 'total_duration': 41990.98984336853, 'accumulated_submission_time': 38698.49995803833, 'accumulated_eval_time': 3284.55818772316, 'accumulated_logging_time': 3.455544948577881}
I0131 01:28:37.200822 139741768959744 logging_writer.py:48] [85301] accumulated_eval_time=3284.558188, accumulated_logging_time=3.455545, accumulated_submission_time=38698.499958, global_step=85301, preemption_count=0, score=38698.499958, test/accuracy=0.502500, test/loss=2.314695, test/num_examples=10000, total_duration=41990.989843, train/accuracy=0.668789, train/loss=1.467261, validation/accuracy=0.624840, validation/loss=1.679897, validation/num_examples=50000
I0131 01:29:17.917520 139741760567040 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.8073126077651978, loss=3.131300926208496
I0131 01:30:03.432581 139741768959744 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.821909785270691, loss=3.239100456237793
I0131 01:30:49.702711 139741760567040 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.8776310682296753, loss=3.201136350631714
I0131 01:31:35.566205 139741768959744 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.798674464225769, loss=3.1822257041931152
I0131 01:32:21.889075 139741760567040 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.6539337635040283, loss=3.34710431098938
I0131 01:33:07.758824 139741768959744 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.5977997779846191, loss=3.147736072540283
I0131 01:33:53.578755 139741760567040 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.7497243881225586, loss=3.1930084228515625
I0131 01:34:39.726725 139741768959744 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.6379846334457397, loss=3.3457400798797607
I0131 01:35:25.785157 139741760567040 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.5618319511413574, loss=5.146417140960693
I0131 01:35:37.324095 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:35:49.371498 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:36:12.293098 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:36:13.909019 139936116377408 submission_runner.py:408] Time since start: 42447.73s, 	Step: 86227, 	{'train/accuracy': 0.682421863079071, 'train/loss': 1.420057773590088, 'validation/accuracy': 0.6224600076675415, 'validation/loss': 1.6837719678878784, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.308379650115967, 'test/num_examples': 10000, 'score': 39118.564464092255, 'total_duration': 42447.72882437706, 'accumulated_submission_time': 39118.564464092255, 'accumulated_eval_time': 3321.14311671257, 'accumulated_logging_time': 3.497130870819092}
I0131 01:36:13.936342 139741768959744 logging_writer.py:48] [86227] accumulated_eval_time=3321.143117, accumulated_logging_time=3.497131, accumulated_submission_time=39118.564464, global_step=86227, preemption_count=0, score=39118.564464, test/accuracy=0.503500, test/loss=2.308380, test/num_examples=10000, total_duration=42447.728824, train/accuracy=0.682422, train/loss=1.420058, validation/accuracy=0.622460, validation/loss=1.683772, validation/num_examples=50000
I0131 01:36:43.347326 139741760567040 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.971184492111206, loss=4.812784194946289
I0131 01:37:28.423345 139741768959744 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.6442428827285767, loss=5.1282734870910645
I0131 01:38:14.296058 139741760567040 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.7422033548355103, loss=3.0105531215667725
I0131 01:39:00.331093 139741768959744 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.5697729587554932, loss=3.981658697128296
I0131 01:39:46.157919 139741760567040 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.7655894756317139, loss=3.065671920776367
I0131 01:40:32.297308 139741768959744 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.5192925930023193, loss=3.5519371032714844
I0131 01:41:18.161034 139741760567040 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5990819931030273, loss=3.84659481048584
I0131 01:42:04.349526 139741768959744 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.661550760269165, loss=3.218010902404785
I0131 01:42:50.027817 139741760567040 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.7712581157684326, loss=3.1582822799682617
I0131 01:43:13.988519 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:43:26.035940 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:43:52.849871 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:43:54.459888 139936116377408 submission_runner.py:408] Time since start: 42908.28s, 	Step: 87154, 	{'train/accuracy': 0.6678515672683716, 'train/loss': 1.4901386499404907, 'validation/accuracy': 0.624019980430603, 'validation/loss': 1.6841695308685303, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.3107705116271973, 'test/num_examples': 10000, 'score': 39538.55706048012, 'total_duration': 42908.27966308594, 'accumulated_submission_time': 39538.55706048012, 'accumulated_eval_time': 3361.6144444942474, 'accumulated_logging_time': 3.5359609127044678}
I0131 01:43:54.492356 139741768959744 logging_writer.py:48] [87154] accumulated_eval_time=3361.614444, accumulated_logging_time=3.535961, accumulated_submission_time=39538.557060, global_step=87154, preemption_count=0, score=39538.557060, test/accuracy=0.503400, test/loss=2.310771, test/num_examples=10000, total_duration=42908.279663, train/accuracy=0.667852, train/loss=1.490139, validation/accuracy=0.624020, validation/loss=1.684170, validation/num_examples=50000
I0131 01:44:13.163191 139741760567040 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.7861604690551758, loss=3.0914618968963623
I0131 01:44:56.747351 139741768959744 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.6233607530593872, loss=3.373958110809326
I0131 01:45:42.692814 139741760567040 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.8326910734176636, loss=3.159196376800537
I0131 01:46:28.676248 139741768959744 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.9958404302597046, loss=3.165282726287842
I0131 01:47:14.789554 139741760567040 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.5592865943908691, loss=4.255619049072266
I0131 01:48:00.466010 139741768959744 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.5886893272399902, loss=4.2287187576293945
I0131 01:48:46.588339 139741760567040 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.4798023700714111, loss=4.547367095947266
I0131 01:49:32.381990 139741768959744 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.6752889156341553, loss=3.126992702484131
I0131 01:50:18.438942 139741760567040 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.5899224281311035, loss=4.463545799255371
I0131 01:50:54.768302 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:51:06.941613 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:51:30.629528 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:51:32.246351 139936116377408 submission_runner.py:408] Time since start: 43366.07s, 	Step: 88081, 	{'train/accuracy': 0.6660546660423279, 'train/loss': 1.5081756114959717, 'validation/accuracy': 0.6201399564743042, 'validation/loss': 1.717879295349121, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.3405404090881348, 'test/num_examples': 10000, 'score': 39958.77399516106, 'total_duration': 43366.066150188446, 'accumulated_submission_time': 39958.77399516106, 'accumulated_eval_time': 3399.0924847126007, 'accumulated_logging_time': 3.5792877674102783}
I0131 01:51:32.283274 139741768959744 logging_writer.py:48] [88081] accumulated_eval_time=3399.092485, accumulated_logging_time=3.579288, accumulated_submission_time=39958.773995, global_step=88081, preemption_count=0, score=39958.773995, test/accuracy=0.500900, test/loss=2.340540, test/num_examples=10000, total_duration=43366.066150, train/accuracy=0.666055, train/loss=1.508176, validation/accuracy=0.620140, validation/loss=1.717879, validation/num_examples=50000
I0131 01:51:40.236515 139741760567040 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.9548770189285278, loss=3.1520402431488037
I0131 01:52:22.104619 139741768959744 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.6495732069015503, loss=4.947704792022705
I0131 01:53:08.088962 139741760567040 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.8568042516708374, loss=3.0859289169311523
I0131 01:53:54.098502 139741768959744 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.5074061155319214, loss=5.087944507598877
I0131 01:54:39.783826 139741760567040 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.618210792541504, loss=4.021491527557373
I0131 01:55:26.123388 139741768959744 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.9232025146484375, loss=3.167914867401123
I0131 01:56:11.813040 139741760567040 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.8813927173614502, loss=3.0850841999053955
I0131 01:56:57.687740 139741768959744 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.7930747270584106, loss=3.2243497371673584
I0131 01:57:44.028958 139741760567040 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.6729662418365479, loss=3.411501884460449
I0131 01:58:30.124243 139741768959744 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.8098264932632446, loss=3.0654892921447754
I0131 01:58:32.510140 139936116377408 spec.py:321] Evaluating on the training split.
I0131 01:58:44.540301 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 01:59:08.664371 139936116377408 spec.py:349] Evaluating on the test split.
I0131 01:59:10.269026 139936116377408 submission_runner.py:408] Time since start: 43824.09s, 	Step: 89007, 	{'train/accuracy': 0.6788476705551147, 'train/loss': 1.4367852210998535, 'validation/accuracy': 0.6260199546813965, 'validation/loss': 1.682494044303894, 'validation/num_examples': 50000, 'test/accuracy': 0.506600022315979, 'test/loss': 2.3096532821655273, 'test/num_examples': 10000, 'score': 40378.939730882645, 'total_duration': 43824.08883070946, 'accumulated_submission_time': 40378.939730882645, 'accumulated_eval_time': 3436.8513662815094, 'accumulated_logging_time': 3.6288912296295166}
I0131 01:59:10.296819 139741760567040 logging_writer.py:48] [89007] accumulated_eval_time=3436.851366, accumulated_logging_time=3.628891, accumulated_submission_time=40378.939731, global_step=89007, preemption_count=0, score=40378.939731, test/accuracy=0.506600, test/loss=2.309653, test/num_examples=10000, total_duration=43824.088831, train/accuracy=0.678848, train/loss=1.436785, validation/accuracy=0.626020, validation/loss=1.682494, validation/num_examples=50000
I0131 01:59:48.363724 139741768959744 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.0483314990997314, loss=3.228846788406372
I0131 02:00:34.428384 139741760567040 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.7086950540542603, loss=3.3763251304626465
I0131 02:01:20.890298 139741768959744 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.7124370336532593, loss=3.4449079036712646
I0131 02:02:07.334874 139741760567040 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.6440526247024536, loss=4.751515865325928
I0131 02:02:53.257458 139741768959744 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.4479963779449463, loss=4.104257583618164
I0131 02:03:39.483402 139741760567040 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.8013312816619873, loss=3.1304657459259033
I0131 02:04:25.658657 139741768959744 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.099097967147827, loss=3.4668190479278564
I0131 02:05:11.929139 139741760567040 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.789969801902771, loss=5.0878753662109375
I0131 02:05:57.807648 139741768959744 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.7404230833053589, loss=3.024078369140625
I0131 02:06:10.412936 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:06:22.199858 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 02:06:46.193111 139936116377408 spec.py:349] Evaluating on the test split.
I0131 02:06:47.794627 139936116377408 submission_runner.py:408] Time since start: 44281.61s, 	Step: 89929, 	{'train/accuracy': 0.6749609112739563, 'train/loss': 1.4869822263717651, 'validation/accuracy': 0.6248999834060669, 'validation/loss': 1.7146426439285278, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.3380980491638184, 'test/num_examples': 10000, 'score': 40798.99773478508, 'total_duration': 44281.61442565918, 'accumulated_submission_time': 40798.99773478508, 'accumulated_eval_time': 3474.23304772377, 'accumulated_logging_time': 3.6661694049835205}
I0131 02:06:47.822895 139741760567040 logging_writer.py:48] [89929] accumulated_eval_time=3474.233048, accumulated_logging_time=3.666169, accumulated_submission_time=40798.997735, global_step=89929, preemption_count=0, score=40798.997735, test/accuracy=0.506500, test/loss=2.338098, test/num_examples=10000, total_duration=44281.614426, train/accuracy=0.674961, train/loss=1.486982, validation/accuracy=0.624900, validation/loss=1.714643, validation/num_examples=50000
I0131 02:07:16.426920 139741768959744 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.9876059293746948, loss=3.112644672393799
I0131 02:08:01.543870 139741760567040 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.5024222135543823, loss=5.241654396057129
I0131 02:08:47.461345 139741768959744 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.5721162557601929, loss=3.795159101486206
I0131 02:09:33.723735 139741760567040 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.6838912963867188, loss=3.62355637550354
I0131 02:10:19.740129 139741768959744 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.162830352783203, loss=3.09674072265625
I0131 02:11:05.743935 139741760567040 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.7725259065628052, loss=3.107783555984497
I0131 02:11:51.857594 139741768959744 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.4807779788970947, loss=4.2852253913879395
I0131 02:12:37.667878 139741760567040 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.276923656463623, loss=3.0947189331054688
I0131 02:13:23.910267 139741768959744 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.7509945631027222, loss=3.549119234085083
I0131 02:13:48.214395 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:14:00.192466 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 02:14:25.184371 139936116377408 spec.py:349] Evaluating on the test split.
I0131 02:14:26.784721 139936116377408 submission_runner.py:408] Time since start: 44740.60s, 	Step: 90855, 	{'train/accuracy': 0.6749218702316284, 'train/loss': 1.44557523727417, 'validation/accuracy': 0.6292200088500977, 'validation/loss': 1.6595513820648193, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.2997994422912598, 'test/num_examples': 10000, 'score': 41219.33115816116, 'total_duration': 44740.604519844055, 'accumulated_submission_time': 41219.33115816116, 'accumulated_eval_time': 3512.803384065628, 'accumulated_logging_time': 3.704136610031128}
I0131 02:14:26.814118 139741760567040 logging_writer.py:48] [90855] accumulated_eval_time=3512.803384, accumulated_logging_time=3.704137, accumulated_submission_time=41219.331158, global_step=90855, preemption_count=0, score=41219.331158, test/accuracy=0.506900, test/loss=2.299799, test/num_examples=10000, total_duration=44740.604520, train/accuracy=0.674922, train/loss=1.445575, validation/accuracy=0.629220, validation/loss=1.659551, validation/num_examples=50000
I0131 02:14:45.093081 139741768959744 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.846292495727539, loss=3.241666793823242
I0131 02:15:28.708839 139741760567040 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.830965280532837, loss=3.8592424392700195
I0131 02:16:14.697004 139741768959744 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.832631230354309, loss=3.130648612976074
I0131 02:17:00.672645 139741760567040 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.9832985401153564, loss=3.0033040046691895
I0131 02:17:46.642438 139741768959744 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.6870839595794678, loss=3.404531478881836
I0131 02:18:32.844295 139741760567040 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.8409463167190552, loss=3.0074234008789062
I0131 02:19:18.829771 139741768959744 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.5898606777191162, loss=4.954170227050781
I0131 02:20:04.895728 139741760567040 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.7788742780685425, loss=3.0461435317993164
I0131 02:20:50.864456 139741768959744 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.8378150463104248, loss=3.098376512527466
I0131 02:21:26.942268 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:21:39.034490 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 02:22:03.166267 139936116377408 spec.py:349] Evaluating on the test split.
I0131 02:22:04.771186 139936116377408 submission_runner.py:408] Time since start: 45198.59s, 	Step: 91780, 	{'train/accuracy': 0.6800000071525574, 'train/loss': 1.466551423072815, 'validation/accuracy': 0.6314399838447571, 'validation/loss': 1.6920958757400513, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.329899787902832, 'test/num_examples': 10000, 'score': 41639.402092933655, 'total_duration': 45198.59097504616, 'accumulated_submission_time': 41639.402092933655, 'accumulated_eval_time': 3550.6322796344757, 'accumulated_logging_time': 3.7427337169647217}
I0131 02:22:04.800684 139741760567040 logging_writer.py:48] [91780] accumulated_eval_time=3550.632280, accumulated_logging_time=3.742734, accumulated_submission_time=41639.402093, global_step=91780, preemption_count=0, score=41639.402093, test/accuracy=0.505700, test/loss=2.329900, test/num_examples=10000, total_duration=45198.590975, train/accuracy=0.680000, train/loss=1.466551, validation/accuracy=0.631440, validation/loss=1.692096, validation/num_examples=50000
I0131 02:22:13.146601 139741768959744 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.5222251415252686, loss=4.965782642364502
I0131 02:22:54.964823 139741760567040 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.7047830820083618, loss=3.3385696411132812
I0131 02:23:41.564023 139741768959744 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.796000361442566, loss=3.1711721420288086
I0131 02:24:28.371454 139741760567040 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.550411343574524, loss=4.9958696365356445
I0131 02:25:14.308637 139741768959744 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.8581875562667847, loss=3.0822806358337402
I0131 02:26:00.314248 139741760567040 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.586266279220581, loss=5.158376216888428
I0131 02:26:46.635689 139741768959744 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.5273140668869019, loss=4.551017761230469
I0131 02:27:32.759227 139741760567040 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.5048134326934814, loss=4.235459327697754
I0131 02:28:18.857672 139741768959744 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.1132564544677734, loss=3.1135501861572266
I0131 02:29:04.909192 139741760567040 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.5091724395751953, loss=4.831927299499512
I0131 02:29:04.923478 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:29:16.741057 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 02:29:43.302943 139936116377408 spec.py:349] Evaluating on the test split.
I0131 02:29:44.923155 139936116377408 submission_runner.py:408] Time since start: 45658.74s, 	Step: 92701, 	{'train/accuracy': 0.69873046875, 'train/loss': 1.3424489498138428, 'validation/accuracy': 0.6325399875640869, 'validation/loss': 1.649196982383728, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2850232124328613, 'test/num_examples': 10000, 'score': 42059.464174985886, 'total_duration': 45658.742933273315, 'accumulated_submission_time': 42059.464174985886, 'accumulated_eval_time': 3590.631927251816, 'accumulated_logging_time': 3.7838430404663086}
I0131 02:29:44.952993 139741768959744 logging_writer.py:48] [92701] accumulated_eval_time=3590.631927, accumulated_logging_time=3.783843, accumulated_submission_time=42059.464175, global_step=92701, preemption_count=0, score=42059.464175, test/accuracy=0.512100, test/loss=2.285023, test/num_examples=10000, total_duration=45658.742933, train/accuracy=0.698730, train/loss=1.342449, validation/accuracy=0.632540, validation/loss=1.649197, validation/num_examples=50000
I0131 02:30:25.677337 139741760567040 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.8615922927856445, loss=3.1711912155151367
I0131 02:31:11.705462 139741768959744 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.6516669988632202, loss=4.082616329193115
I0131 02:31:58.313073 139741760567040 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.6796640157699585, loss=3.805997610092163
I0131 02:32:44.325813 139741768959744 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.8676631450653076, loss=3.0619852542877197
I0131 02:33:30.484354 139741760567040 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.729876160621643, loss=3.7739996910095215
I0131 02:34:16.329617 139741768959744 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7413063049316406, loss=3.1083531379699707
I0131 02:35:02.222809 139741760567040 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.743309736251831, loss=3.6783745288848877
I0131 02:35:48.032985 139741768959744 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.6541954278945923, loss=4.0028276443481445
I0131 02:36:34.150868 139741760567040 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.8460352420806885, loss=3.3073410987854004
I0131 02:36:45.287783 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:36:57.080367 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 02:37:21.620734 139936116377408 spec.py:349] Evaluating on the test split.
I0131 02:37:23.227354 139936116377408 submission_runner.py:408] Time since start: 46117.05s, 	Step: 93626, 	{'train/accuracy': 0.6816992163658142, 'train/loss': 1.4043458700180054, 'validation/accuracy': 0.637179970741272, 'validation/loss': 1.6172446012496948, 'validation/num_examples': 50000, 'test/accuracy': 0.5180000066757202, 'test/loss': 2.231893301010132, 'test/num_examples': 10000, 'score': 42479.74120259285, 'total_duration': 46117.04714727402, 'accumulated_submission_time': 42479.74120259285, 'accumulated_eval_time': 3628.571489095688, 'accumulated_logging_time': 3.8232078552246094}
I0131 02:37:23.256563 139741768959744 logging_writer.py:48] [93626] accumulated_eval_time=3628.571489, accumulated_logging_time=3.823208, accumulated_submission_time=42479.741203, global_step=93626, preemption_count=0, score=42479.741203, test/accuracy=0.518000, test/loss=2.231893, test/num_examples=10000, total_duration=46117.047147, train/accuracy=0.681699, train/loss=1.404346, validation/accuracy=0.637180, validation/loss=1.617245, validation/num_examples=50000
I0131 02:37:53.196885 139741760567040 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.754794955253601, loss=3.1355347633361816
I0131 02:38:38.092327 139741768959744 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.806141972541809, loss=3.1585311889648438
I0131 02:39:24.269440 139741760567040 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.613845944404602, loss=4.391348838806152
I0131 02:40:10.666613 139741768959744 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.9952707290649414, loss=2.9726362228393555
I0131 02:40:56.478434 139741760567040 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.5417001247406006, loss=5.082615852355957
I0131 02:41:42.690279 139741768959744 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.5749002695083618, loss=4.8679728507995605
I0131 02:42:29.026233 139741760567040 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.61312997341156, loss=4.955409526824951
I0131 02:43:15.081737 139741768959744 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.8047609329223633, loss=3.240962505340576
I0131 02:44:01.310996 139741760567040 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.6723642349243164, loss=3.401350975036621
I0131 02:44:23.603289 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:44:35.460670 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 02:44:59.137562 139936116377408 spec.py:349] Evaluating on the test split.
I0131 02:45:00.742407 139936116377408 submission_runner.py:408] Time since start: 46574.56s, 	Step: 94550, 	{'train/accuracy': 0.6850976347923279, 'train/loss': 1.3780698776245117, 'validation/accuracy': 0.6401599645614624, 'validation/loss': 1.591439127922058, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.23335862159729, 'test/num_examples': 10000, 'score': 42900.029477357864, 'total_duration': 46574.56220269203, 'accumulated_submission_time': 42900.029477357864, 'accumulated_eval_time': 3665.710597515106, 'accumulated_logging_time': 3.861924171447754}
I0131 02:45:00.771427 139741768959744 logging_writer.py:48] [94550] accumulated_eval_time=3665.710598, accumulated_logging_time=3.861924, accumulated_submission_time=42900.029477, global_step=94550, preemption_count=0, score=42900.029477, test/accuracy=0.515400, test/loss=2.233359, test/num_examples=10000, total_duration=46574.562203, train/accuracy=0.685098, train/loss=1.378070, validation/accuracy=0.640160, validation/loss=1.591439, validation/num_examples=50000
I0131 02:45:21.009841 139741760567040 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.8731621503829956, loss=3.063825845718384
I0131 02:46:04.738688 139741768959744 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.731131911277771, loss=3.3049986362457275
I0131 02:46:50.682968 139741760567040 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.8696495294570923, loss=3.223498821258545
I0131 02:47:36.793902 139741768959744 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.7672843933105469, loss=5.150559425354004
I0131 02:48:22.622770 139741760567040 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.5896021127700806, loss=4.317277908325195
I0131 02:49:09.027800 139741768959744 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.8447803258895874, loss=3.0049891471862793
I0131 02:49:55.034257 139741760567040 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.8920884132385254, loss=3.0845727920532227
I0131 02:50:40.976673 139741768959744 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.8720813989639282, loss=3.043733596801758
I0131 02:51:27.173693 139741760567040 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.9560847282409668, loss=4.863335132598877
I0131 02:52:00.876151 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:52:12.990514 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 02:52:37.066058 139936116377408 spec.py:349] Evaluating on the test split.
I0131 02:52:38.672801 139936116377408 submission_runner.py:408] Time since start: 47032.49s, 	Step: 95475, 	{'train/accuracy': 0.6938085556030273, 'train/loss': 1.3812767267227173, 'validation/accuracy': 0.6334999799728394, 'validation/loss': 1.646828293800354, 'validation/num_examples': 50000, 'test/accuracy': 0.5127000212669373, 'test/loss': 2.2695205211639404, 'test/num_examples': 10000, 'score': 43320.07567238808, 'total_duration': 47032.49260401726, 'accumulated_submission_time': 43320.07567238808, 'accumulated_eval_time': 3703.5072691440582, 'accumulated_logging_time': 3.901413679122925}
I0131 02:52:38.702732 139741768959744 logging_writer.py:48] [95475] accumulated_eval_time=3703.507269, accumulated_logging_time=3.901414, accumulated_submission_time=43320.075672, global_step=95475, preemption_count=0, score=43320.075672, test/accuracy=0.512700, test/loss=2.269521, test/num_examples=10000, total_duration=47032.492604, train/accuracy=0.693809, train/loss=1.381277, validation/accuracy=0.633500, validation/loss=1.646828, validation/num_examples=50000
I0131 02:52:49.028571 139741760567040 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.911057472229004, loss=5.216738700866699
I0131 02:53:31.403109 139741768959744 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.759682536125183, loss=3.2208468914031982
I0131 02:54:17.473105 139741760567040 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.77042818069458, loss=3.8291521072387695
I0131 02:55:03.805932 139741768959744 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.6152429580688477, loss=4.9643473625183105
I0131 02:55:49.717072 139741760567040 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.873847484588623, loss=3.263963460922241
I0131 02:56:35.921620 139741768959744 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.8753716945648193, loss=3.035118818283081
I0131 02:57:21.944370 139741760567040 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.5398051738739014, loss=5.139382839202881
I0131 02:58:07.879446 139741768959744 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.022465705871582, loss=3.3866825103759766
I0131 02:58:53.756328 139741760567040 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.882527470588684, loss=3.043783664703369
I0131 02:59:39.080527 139936116377408 spec.py:321] Evaluating on the training split.
I0131 02:59:50.918979 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:00:12.244696 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:00:13.852836 139936116377408 submission_runner.py:408] Time since start: 47487.67s, 	Step: 96400, 	{'train/accuracy': 0.6801366806030273, 'train/loss': 1.4493908882141113, 'validation/accuracy': 0.6377800107002258, 'validation/loss': 1.6380335092544556, 'validation/num_examples': 50000, 'test/accuracy': 0.5180000066757202, 'test/loss': 2.2730095386505127, 'test/num_examples': 10000, 'score': 43740.39475917816, 'total_duration': 47487.6726129055, 'accumulated_submission_time': 43740.39475917816, 'accumulated_eval_time': 3738.279561281204, 'accumulated_logging_time': 3.94172739982605}
I0131 03:00:13.891636 139741768959744 logging_writer.py:48] [96400] accumulated_eval_time=3738.279561, accumulated_logging_time=3.941727, accumulated_submission_time=43740.394759, global_step=96400, preemption_count=0, score=43740.394759, test/accuracy=0.518000, test/loss=2.273010, test/num_examples=10000, total_duration=47487.672613, train/accuracy=0.680137, train/loss=1.449391, validation/accuracy=0.637780, validation/loss=1.638034, validation/num_examples=50000
I0131 03:00:14.299790 139741760567040 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.5449985265731812, loss=5.194209098815918
I0131 03:00:54.942507 139741768959744 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.1668953895568848, loss=3.0649871826171875
I0131 03:01:40.884646 139741760567040 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.9543132781982422, loss=2.9848904609680176
I0131 03:02:27.218152 139741768959744 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.833853006362915, loss=2.9642460346221924
I0131 03:03:13.131785 139741760567040 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.098418951034546, loss=3.075345516204834
I0131 03:03:58.994941 139741768959744 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.118170738220215, loss=3.114666700363159
I0131 03:04:45.024626 139741760567040 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.7943658828735352, loss=3.213066816329956
I0131 03:05:30.935227 139741768959744 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.799556851387024, loss=5.070812702178955
I0131 03:06:16.942724 139741760567040 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.60247004032135, loss=3.6787965297698975
I0131 03:07:02.750362 139741768959744 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.6578295230865479, loss=4.7497334480285645
I0131 03:07:13.865264 139936116377408 spec.py:321] Evaluating on the training split.
I0131 03:07:25.790788 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:07:52.856535 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:07:54.462924 139936116377408 submission_runner.py:408] Time since start: 47948.28s, 	Step: 97326, 	{'train/accuracy': 0.6853125095367432, 'train/loss': 1.400512933731079, 'validation/accuracy': 0.6398599743843079, 'validation/loss': 1.6111207008361816, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2331061363220215, 'test/num_examples': 10000, 'score': 44160.30851793289, 'total_duration': 47948.28272128105, 'accumulated_submission_time': 44160.30851793289, 'accumulated_eval_time': 3778.8772070407867, 'accumulated_logging_time': 3.9924333095550537}
I0131 03:07:54.496221 139741760567040 logging_writer.py:48] [97326] accumulated_eval_time=3778.877207, accumulated_logging_time=3.992433, accumulated_submission_time=44160.308518, global_step=97326, preemption_count=0, score=44160.308518, test/accuracy=0.516300, test/loss=2.233106, test/num_examples=10000, total_duration=47948.282721, train/accuracy=0.685313, train/loss=1.400513, validation/accuracy=0.639860, validation/loss=1.611121, validation/num_examples=50000
I0131 03:08:24.253410 139741768959744 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.6875975131988525, loss=5.140527248382568
I0131 03:09:09.515178 139741760567040 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.6917815208435059, loss=3.2683141231536865
I0131 03:09:55.523076 139741768959744 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.8833311796188354, loss=3.202134132385254
I0131 03:10:41.793219 139741760567040 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.098989248275757, loss=3.3155407905578613
I0131 03:11:27.769273 139741768959744 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.6385490894317627, loss=4.225396633148193
I0131 03:12:13.924984 139741760567040 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.1163434982299805, loss=2.998310089111328
I0131 03:12:59.875883 139741768959744 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.8419774770736694, loss=2.9900553226470947
I0131 03:13:45.621527 139741760567040 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.943568468093872, loss=3.220306396484375
I0131 03:14:31.346814 139741768959744 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.0126242637634277, loss=2.97381854057312
I0131 03:14:54.700779 139936116377408 spec.py:321] Evaluating on the training split.
I0131 03:15:06.712990 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:15:32.518993 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:15:34.127298 139936116377408 submission_runner.py:408] Time since start: 48407.95s, 	Step: 98253, 	{'train/accuracy': 0.6956640481948853, 'train/loss': 1.3512005805969238, 'validation/accuracy': 0.6403399705886841, 'validation/loss': 1.6066975593566895, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.243485689163208, 'test/num_examples': 10000, 'score': 44580.45514702797, 'total_duration': 48407.94709467888, 'accumulated_submission_time': 44580.45514702797, 'accumulated_eval_time': 3818.3037271499634, 'accumulated_logging_time': 4.035202503204346}
I0131 03:15:34.157037 139741760567040 logging_writer.py:48] [98253] accumulated_eval_time=3818.303727, accumulated_logging_time=4.035203, accumulated_submission_time=44580.455147, global_step=98253, preemption_count=0, score=44580.455147, test/accuracy=0.517600, test/loss=2.243486, test/num_examples=10000, total_duration=48407.947095, train/accuracy=0.695664, train/loss=1.351201, validation/accuracy=0.640340, validation/loss=1.606698, validation/num_examples=50000
I0131 03:15:53.195416 139741768959744 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.9620685577392578, loss=3.175088882446289
I0131 03:16:36.461681 139741760567040 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.5608071088790894, loss=5.103854656219482
I0131 03:17:22.581776 139741768959744 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.0955753326416016, loss=5.19028902053833
I0131 03:18:08.440058 139741760567040 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.4767544269561768, loss=5.120360851287842
I0131 03:18:54.139881 139741768959744 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.097036838531494, loss=2.9533238410949707
I0131 03:19:40.056437 139741760567040 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.0478947162628174, loss=2.970268487930298
I0131 03:20:26.317179 139741768959744 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.9932186603546143, loss=3.149327516555786
I0131 03:21:12.366722 139741760567040 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.0340991020202637, loss=5.093334197998047
I0131 03:21:58.593194 139741768959744 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.040147304534912, loss=3.016939163208008
I0131 03:22:34.425344 139936116377408 spec.py:321] Evaluating on the training split.
I0131 03:22:46.222962 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:23:09.875655 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:23:11.483979 139936116377408 submission_runner.py:408] Time since start: 48865.30s, 	Step: 99180, 	{'train/accuracy': 0.6977148056030273, 'train/loss': 1.3450783491134644, 'validation/accuracy': 0.6491000056266785, 'validation/loss': 1.5536638498306274, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.1836001873016357, 'test/num_examples': 10000, 'score': 45000.66364359856, 'total_duration': 48865.30378293991, 'accumulated_submission_time': 45000.66364359856, 'accumulated_eval_time': 3855.362357854843, 'accumulated_logging_time': 4.076540231704712}
I0131 03:23:11.517016 139741760567040 logging_writer.py:48] [99180] accumulated_eval_time=3855.362358, accumulated_logging_time=4.076540, accumulated_submission_time=45000.663644, global_step=99180, preemption_count=0, score=45000.663644, test/accuracy=0.526200, test/loss=2.183600, test/num_examples=10000, total_duration=48865.303783, train/accuracy=0.697715, train/loss=1.345078, validation/accuracy=0.649100, validation/loss=1.553664, validation/num_examples=50000
I0131 03:23:19.856861 139741768959744 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.7959332466125488, loss=4.9432878494262695
I0131 03:24:01.979744 139741760567040 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.0139195919036865, loss=3.0949249267578125
I0131 03:24:47.989382 139741768959744 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.9605756998062134, loss=2.9518537521362305
I0131 03:25:34.609777 139741760567040 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.00888729095459, loss=5.013284206390381
I0131 03:26:20.233829 139741768959744 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.8779040575027466, loss=3.3896031379699707
I0131 03:27:06.497194 139741760567040 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.6672786474227905, loss=4.484711647033691
I0131 03:27:52.282032 139741768959744 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.7899693250656128, loss=3.897883892059326
I0131 03:28:38.752613 139741760567040 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.5280282497406006, loss=4.586856842041016
I0131 03:29:24.677295 139741768959744 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.8778053522109985, loss=2.8950986862182617
I0131 03:30:10.840729 139741760567040 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.720150351524353, loss=5.067898273468018
I0131 03:30:11.826704 139936116377408 spec.py:321] Evaluating on the training split.
I0131 03:30:23.594093 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:30:49.225543 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:30:50.829481 139936116377408 submission_runner.py:408] Time since start: 49324.65s, 	Step: 100104, 	{'train/accuracy': 0.6898828148841858, 'train/loss': 1.416399598121643, 'validation/accuracy': 0.6423400044441223, 'validation/loss': 1.6274499893188477, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.2531089782714844, 'test/num_examples': 10000, 'score': 45420.91577172279, 'total_duration': 49324.64928340912, 'accumulated_submission_time': 45420.91577172279, 'accumulated_eval_time': 3894.3651168346405, 'accumulated_logging_time': 4.119498014450073}
I0131 03:30:50.858772 139741768959744 logging_writer.py:48] [100104] accumulated_eval_time=3894.365117, accumulated_logging_time=4.119498, accumulated_submission_time=45420.915772, global_step=100104, preemption_count=0, score=45420.915772, test/accuracy=0.520700, test/loss=2.253109, test/num_examples=10000, total_duration=49324.649283, train/accuracy=0.689883, train/loss=1.416400, validation/accuracy=0.642340, validation/loss=1.627450, validation/num_examples=50000
I0131 03:31:30.301773 139741760567040 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.571908712387085, loss=4.837306499481201
I0131 03:32:16.194871 139741768959744 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.5713704824447632, loss=4.557147026062012
I0131 03:33:01.987636 139741760567040 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.027920961380005, loss=3.0921425819396973
I0131 03:33:47.718000 139741768959744 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.120840072631836, loss=2.9867639541625977
I0131 03:34:33.663112 139741760567040 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.666867971420288, loss=3.9877917766571045
I0131 03:35:19.616778 139741768959744 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.0706372261047363, loss=2.870612621307373
I0131 03:36:05.583793 139741760567040 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.089508533477783, loss=2.976682662963867
I0131 03:36:51.148314 139741768959744 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.686962604522705, loss=4.608253002166748
I0131 03:37:37.053392 139741760567040 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.9919331073760986, loss=2.8170909881591797
I0131 03:37:51.051523 139936116377408 spec.py:321] Evaluating on the training split.
I0131 03:38:03.169867 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:38:27.870831 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:38:29.495195 139936116377408 submission_runner.py:408] Time since start: 49783.31s, 	Step: 101032, 	{'train/accuracy': 0.6932812333106995, 'train/loss': 1.3365551233291626, 'validation/accuracy': 0.6417999863624573, 'validation/loss': 1.5768588781356812, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.190631628036499, 'test/num_examples': 10000, 'score': 45841.04868769646, 'total_duration': 49783.31498479843, 'accumulated_submission_time': 45841.04868769646, 'accumulated_eval_time': 3932.808772087097, 'accumulated_logging_time': 4.1595988273620605}
I0131 03:38:29.528249 139741768959744 logging_writer.py:48] [101032] accumulated_eval_time=3932.808772, accumulated_logging_time=4.159599, accumulated_submission_time=45841.048688, global_step=101032, preemption_count=0, score=45841.048688, test/accuracy=0.525100, test/loss=2.190632, test/num_examples=10000, total_duration=49783.314985, train/accuracy=0.693281, train/loss=1.336555, validation/accuracy=0.641800, validation/loss=1.576859, validation/num_examples=50000
I0131 03:38:56.937756 139741760567040 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.8121578693389893, loss=3.6074724197387695
I0131 03:39:41.551413 139741768959744 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.9802788496017456, loss=3.310926675796509
I0131 03:40:27.881519 139741760567040 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.034477472305298, loss=2.9440808296203613
I0131 03:41:14.465684 139741768959744 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.8640583753585815, loss=5.152844429016113
I0131 03:42:00.485439 139741760567040 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.0531699657440186, loss=2.993065595626831
I0131 03:42:46.327014 139741768959744 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.7306175231933594, loss=4.224236011505127
I0131 03:43:32.308460 139741760567040 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.8820064067840576, loss=3.159364700317383
I0131 03:44:18.221036 139741768959744 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.6791656017303467, loss=4.538590431213379
I0131 03:45:04.081635 139741760567040 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.865732192993164, loss=3.059643268585205
I0131 03:45:29.881759 139936116377408 spec.py:321] Evaluating on the training split.
I0131 03:45:41.817010 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:46:08.208603 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:46:09.806144 139936116377408 submission_runner.py:408] Time since start: 50243.63s, 	Step: 101958, 	{'train/accuracy': 0.7127343416213989, 'train/loss': 1.287361979484558, 'validation/accuracy': 0.642579972743988, 'validation/loss': 1.5980144739151, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.233988046646118, 'test/num_examples': 10000, 'score': 46261.34437060356, 'total_duration': 50243.625947237015, 'accumulated_submission_time': 46261.34437060356, 'accumulated_eval_time': 3972.7331693172455, 'accumulated_logging_time': 4.202354431152344}
I0131 03:46:09.836144 139741768959744 logging_writer.py:48] [101958] accumulated_eval_time=3972.733169, accumulated_logging_time=4.202354, accumulated_submission_time=46261.344371, global_step=101958, preemption_count=0, score=46261.344371, test/accuracy=0.515700, test/loss=2.233988, test/num_examples=10000, total_duration=50243.625947, train/accuracy=0.712734, train/loss=1.287362, validation/accuracy=0.642580, validation/loss=1.598014, validation/num_examples=50000
I0131 03:46:26.888353 139741760567040 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.866907000541687, loss=3.024583339691162
I0131 03:47:10.262601 139741768959744 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.7269762754440308, loss=3.9143214225769043
I0131 03:47:56.319197 139741760567040 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.7508792877197266, loss=3.787745952606201
I0131 03:48:42.583346 139741768959744 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.8461650609970093, loss=3.6341962814331055
I0131 03:49:28.493421 139741760567040 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.0201234817504883, loss=3.6390364170074463
I0131 03:50:14.458219 139741768959744 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.9038946628570557, loss=2.9370737075805664
I0131 03:51:00.730131 139741760567040 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.2592921257019043, loss=2.976626396179199
I0131 03:51:46.755171 139741768959744 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.8842041492462158, loss=3.267408847808838
I0131 03:52:32.662987 139741760567040 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.960252285003662, loss=5.143200397491455
I0131 03:53:10.170720 139936116377408 spec.py:321] Evaluating on the training split.
I0131 03:53:22.054028 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 03:53:48.762458 139936116377408 spec.py:349] Evaluating on the test split.
I0131 03:53:50.372996 139936116377408 submission_runner.py:408] Time since start: 50704.19s, 	Step: 102883, 	{'train/accuracy': 0.6974804401397705, 'train/loss': 1.3310356140136719, 'validation/accuracy': 0.6518599987030029, 'validation/loss': 1.5392245054244995, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.177493095397949, 'test/num_examples': 10000, 'score': 46681.6217648983, 'total_duration': 50704.19277739525, 'accumulated_submission_time': 46681.6217648983, 'accumulated_eval_time': 4012.935410261154, 'accumulated_logging_time': 4.241785049438477}
I0131 03:53:50.413236 139741768959744 logging_writer.py:48] [102883] accumulated_eval_time=4012.935410, accumulated_logging_time=4.241785, accumulated_submission_time=46681.621765, global_step=102883, preemption_count=0, score=46681.621765, test/accuracy=0.522300, test/loss=2.177493, test/num_examples=10000, total_duration=50704.192777, train/accuracy=0.697480, train/loss=1.331036, validation/accuracy=0.651860, validation/loss=1.539225, validation/num_examples=50000
I0131 03:53:57.560044 139741760567040 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.6949121952056885, loss=5.058338642120361
I0131 03:54:39.381905 139741768959744 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.9713761806488037, loss=4.949707984924316
I0131 03:55:25.353227 139741760567040 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.0353074073791504, loss=2.9053304195404053
I0131 03:56:11.736485 139741768959744 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.725608468055725, loss=4.18992805480957
I0131 03:56:57.392295 139741760567040 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.8364841938018799, loss=2.892721652984619
I0131 03:57:43.509070 139741768959744 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.9623057842254639, loss=3.0196173191070557
I0131 03:58:29.637520 139741760567040 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.8492112159729004, loss=3.740356683731079
I0131 03:59:15.445456 139741768959744 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.125229835510254, loss=2.8932104110717773
I0131 04:00:01.307090 139741760567040 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.9710497856140137, loss=2.922377109527588
I0131 04:00:47.120057 139741768959744 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.176536798477173, loss=4.950061798095703
I0131 04:00:50.381759 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:01:02.381400 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:01:30.123599 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:01:31.728659 139936116377408 submission_runner.py:408] Time since start: 51165.55s, 	Step: 103809, 	{'train/accuracy': 0.6997656226158142, 'train/loss': 1.369573950767517, 'validation/accuracy': 0.6473199725151062, 'validation/loss': 1.5862294435501099, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.2343835830688477, 'test/num_examples': 10000, 'score': 47101.53124284744, 'total_duration': 51165.548437833786, 'accumulated_submission_time': 47101.53124284744, 'accumulated_eval_time': 4054.28226852417, 'accumulated_logging_time': 4.292704820632935}
I0131 04:01:31.768179 139741760567040 logging_writer.py:48] [103809] accumulated_eval_time=4054.282269, accumulated_logging_time=4.292705, accumulated_submission_time=47101.531243, global_step=103809, preemption_count=0, score=47101.531243, test/accuracy=0.523600, test/loss=2.234384, test/num_examples=10000, total_duration=51165.548438, train/accuracy=0.699766, train/loss=1.369574, validation/accuracy=0.647320, validation/loss=1.586229, validation/num_examples=50000
I0131 04:02:09.493929 139741768959744 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.8391772508621216, loss=4.447103023529053
I0131 04:02:55.244472 139741760567040 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.1052474975585938, loss=2.8814520835876465
I0131 04:03:41.581795 139741768959744 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.9302972555160522, loss=3.1607420444488525
I0131 04:04:27.512970 139741760567040 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.15855073928833, loss=4.819146156311035
I0131 04:05:13.421892 139741768959744 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.115687370300293, loss=2.9350197315216064
I0131 04:05:59.279942 139741760567040 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.117633819580078, loss=2.798248767852783
I0131 04:06:45.378756 139741768959744 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.035083532333374, loss=2.86696195602417
I0131 04:07:31.354228 139741760567040 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.8332056999206543, loss=3.689589738845825
I0131 04:08:17.458405 139741768959744 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.065911293029785, loss=2.970154285430908
I0131 04:08:31.788768 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:08:43.647985 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:09:10.912940 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:09:12.514683 139936116377408 submission_runner.py:408] Time since start: 51626.33s, 	Step: 104733, 	{'train/accuracy': 0.7157617211341858, 'train/loss': 1.2719745635986328, 'validation/accuracy': 0.6475399732589722, 'validation/loss': 1.5650979280471802, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.189589262008667, 'test/num_examples': 10000, 'score': 47521.49310541153, 'total_duration': 51626.334483385086, 'accumulated_submission_time': 47521.49310541153, 'accumulated_eval_time': 4095.0082075595856, 'accumulated_logging_time': 4.342754125595093}
I0131 04:09:12.548197 139741760567040 logging_writer.py:48] [104733] accumulated_eval_time=4095.008208, accumulated_logging_time=4.342754, accumulated_submission_time=47521.493105, global_step=104733, preemption_count=0, score=47521.493105, test/accuracy=0.528700, test/loss=2.189589, test/num_examples=10000, total_duration=51626.334483, train/accuracy=0.715762, train/loss=1.271975, validation/accuracy=0.647540, validation/loss=1.565098, validation/num_examples=50000
I0131 04:09:39.543362 139741768959744 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.695600986480713, loss=3.8067073822021484
I0131 04:10:24.357172 139741760567040 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.9798039197921753, loss=2.9300014972686768
I0131 04:11:10.219351 139741768959744 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.1581156253814697, loss=2.9568090438842773
I0131 04:11:56.654201 139741760567040 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.0242233276367188, loss=2.9375410079956055
I0131 04:12:42.408807 139741768959744 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.7216331958770752, loss=4.502529144287109
I0131 04:13:28.607626 139741760567040 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.167501926422119, loss=2.9423699378967285
I0131 04:14:14.345511 139741768959744 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.9064491987228394, loss=3.6903510093688965
I0131 04:15:00.004522 139741760567040 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.0887787342071533, loss=2.978910446166992
I0131 04:15:45.930694 139741768959744 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.849622368812561, loss=4.463988304138184
I0131 04:16:12.756536 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:16:24.459405 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:16:48.156588 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:16:49.770752 139936116377408 submission_runner.py:408] Time since start: 52083.59s, 	Step: 105660, 	{'train/accuracy': 0.6998632550239563, 'train/loss': 1.3254988193511963, 'validation/accuracy': 0.649619996547699, 'validation/loss': 1.5387935638427734, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.172654867172241, 'test/num_examples': 10000, 'score': 47941.640141010284, 'total_duration': 52083.59052538872, 'accumulated_submission_time': 47941.640141010284, 'accumulated_eval_time': 4132.022391796112, 'accumulated_logging_time': 4.3900251388549805}
I0131 04:16:49.807066 139741760567040 logging_writer.py:48] [105660] accumulated_eval_time=4132.022392, accumulated_logging_time=4.390025, accumulated_submission_time=47941.640141, global_step=105660, preemption_count=0, score=47941.640141, test/accuracy=0.530900, test/loss=2.172655, test/num_examples=10000, total_duration=52083.590525, train/accuracy=0.699863, train/loss=1.325499, validation/accuracy=0.649620, validation/loss=1.538794, validation/num_examples=50000
I0131 04:17:06.088138 139741768959744 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.872401475906372, loss=3.1832423210144043
I0131 04:17:49.051916 139741760567040 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.9975522756576538, loss=2.911360263824463
I0131 04:18:34.792993 139741768959744 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.9162827730178833, loss=4.1470794677734375
I0131 04:19:20.736542 139741760567040 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.7979698181152344, loss=4.082692623138428
I0131 04:20:06.544491 139741768959744 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.3273794651031494, loss=3.213432788848877
I0131 04:20:52.550151 139741760567040 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.02964186668396, loss=2.871513843536377
I0131 04:21:38.121348 139741768959744 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.9678516387939453, loss=3.131742477416992
I0131 04:22:24.326658 139741760567040 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.181859016418457, loss=5.026622772216797
I0131 04:23:09.986700 139741768959744 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.2198116779327393, loss=2.981222152709961
I0131 04:23:50.001763 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:24:01.740617 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:24:28.517840 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:24:30.119457 139936116377408 submission_runner.py:408] Time since start: 52543.94s, 	Step: 106589, 	{'train/accuracy': 0.7017773389816284, 'train/loss': 1.3748812675476074, 'validation/accuracy': 0.6519799828529358, 'validation/loss': 1.600887656211853, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.2354602813720703, 'test/num_examples': 10000, 'score': 48361.77505850792, 'total_duration': 52543.93924975395, 'accumulated_submission_time': 48361.77505850792, 'accumulated_eval_time': 4172.140084028244, 'accumulated_logging_time': 4.437947511672974}
I0131 04:24:30.156701 139741760567040 logging_writer.py:48] [106589] accumulated_eval_time=4172.140084, accumulated_logging_time=4.437948, accumulated_submission_time=48361.775059, global_step=106589, preemption_count=0, score=48361.775059, test/accuracy=0.526000, test/loss=2.235460, test/num_examples=10000, total_duration=52543.939250, train/accuracy=0.701777, train/loss=1.374881, validation/accuracy=0.651980, validation/loss=1.600888, validation/num_examples=50000
I0131 04:24:34.916760 139741768959744 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.9999301433563232, loss=2.9731831550598145
I0131 04:25:16.413994 139741760567040 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.038285255432129, loss=2.958188772201538
I0131 04:26:02.199039 139741768959744 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.7794344425201416, loss=3.416975498199463
I0131 04:26:48.283088 139741760567040 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.9952750205993652, loss=4.99396276473999
I0131 04:27:34.066710 139741768959744 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.9109677076339722, loss=3.038804531097412
I0131 04:28:20.088887 139741760567040 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.792978286743164, loss=4.190412998199463
I0131 04:29:06.005564 139741768959744 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.0232884883880615, loss=3.8757052421569824
I0131 04:29:51.880971 139741760567040 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.7819756269454956, loss=4.74218225479126
I0131 04:30:37.821474 139741768959744 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.053157329559326, loss=3.1368534564971924
I0131 04:31:23.810237 139741760567040 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.1982431411743164, loss=2.9071078300476074
I0131 04:31:30.506103 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:31:42.505761 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:32:06.074283 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:32:07.679326 139936116377408 submission_runner.py:408] Time since start: 53001.50s, 	Step: 107516, 	{'train/accuracy': 0.7152148485183716, 'train/loss': 1.2641282081604004, 'validation/accuracy': 0.6518200039863586, 'validation/loss': 1.5298153162002563, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.170165777206421, 'test/num_examples': 10000, 'score': 48782.06770968437, 'total_duration': 53001.499126434326, 'accumulated_submission_time': 48782.06770968437, 'accumulated_eval_time': 4209.313284873962, 'accumulated_logging_time': 4.48431396484375}
I0131 04:32:07.714539 139741768959744 logging_writer.py:48] [107516] accumulated_eval_time=4209.313285, accumulated_logging_time=4.484314, accumulated_submission_time=48782.067710, global_step=107516, preemption_count=0, score=48782.067710, test/accuracy=0.526300, test/loss=2.170166, test/num_examples=10000, total_duration=53001.499126, train/accuracy=0.715215, train/loss=1.264128, validation/accuracy=0.651820, validation/loss=1.529815, validation/num_examples=50000
I0131 04:32:41.597382 139741760567040 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.0522000789642334, loss=3.0088255405426025
I0131 04:33:27.237897 139741768959744 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.824223518371582, loss=3.424683094024658
I0131 04:34:13.706326 139741760567040 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.2840981483459473, loss=5.180781364440918
I0131 04:34:59.664596 139741768959744 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.7966679334640503, loss=4.765140056610107
I0131 04:35:45.778250 139741760567040 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.180821418762207, loss=2.845064640045166
I0131 04:36:32.097292 139741768959744 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.2209219932556152, loss=3.5224738121032715
I0131 04:37:18.005624 139741760567040 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.27325439453125, loss=2.9403257369995117
I0131 04:38:04.163707 139741768959744 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.2120847702026367, loss=2.8756515979766846
I0131 04:38:50.243918 139741760567040 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.7890138626098633, loss=3.7066147327423096
I0131 04:39:08.111052 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:39:19.822141 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:39:44.974593 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:39:46.578423 139936116377408 submission_runner.py:408] Time since start: 53460.40s, 	Step: 108440, 	{'train/accuracy': 0.7088671922683716, 'train/loss': 1.30809485912323, 'validation/accuracy': 0.6545000076293945, 'validation/loss': 1.5428231954574585, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.1718273162841797, 'test/num_examples': 10000, 'score': 49202.40798187256, 'total_duration': 53460.39822816849, 'accumulated_submission_time': 49202.40798187256, 'accumulated_eval_time': 4247.780642032623, 'accumulated_logging_time': 4.5283708572387695}
I0131 04:39:46.609789 139741768959744 logging_writer.py:48] [108440] accumulated_eval_time=4247.780642, accumulated_logging_time=4.528371, accumulated_submission_time=49202.407982, global_step=108440, preemption_count=0, score=49202.407982, test/accuracy=0.529300, test/loss=2.171827, test/num_examples=10000, total_duration=53460.398228, train/accuracy=0.708867, train/loss=1.308095, validation/accuracy=0.654500, validation/loss=1.542823, validation/num_examples=50000
I0131 04:40:10.815615 139741760567040 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.1819045543670654, loss=3.1168527603149414
I0131 04:40:55.359405 139741768959744 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.159191370010376, loss=2.939453125
I0131 04:41:41.731585 139741760567040 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.1381118297576904, loss=2.881838798522949
I0131 04:42:28.112333 139741768959744 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.148294448852539, loss=2.8612747192382812
I0131 04:43:14.313457 139741760567040 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.2072761058807373, loss=3.151369571685791
I0131 04:44:00.269655 139741768959744 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.971328616142273, loss=3.1710119247436523
I0131 04:44:46.277455 139741760567040 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.092895269393921, loss=2.908520221710205
I0131 04:45:32.531384 139741768959744 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.0812644958496094, loss=2.848219871520996
I0131 04:46:18.521011 139741760567040 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.1006722450256348, loss=4.673169136047363
I0131 04:46:46.994619 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:46:58.823686 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:47:25.183439 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:47:26.775526 139936116377408 submission_runner.py:408] Time since start: 53920.60s, 	Step: 109364, 	{'train/accuracy': 0.7081640362739563, 'train/loss': 1.2886130809783936, 'validation/accuracy': 0.6595799922943115, 'validation/loss': 1.5078322887420654, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.130657434463501, 'test/num_examples': 10000, 'score': 49622.73654794693, 'total_duration': 53920.59532928467, 'accumulated_submission_time': 49622.73654794693, 'accumulated_eval_time': 4287.5615401268005, 'accumulated_logging_time': 4.569385528564453}
I0131 04:47:26.808277 139741768959744 logging_writer.py:48] [109364] accumulated_eval_time=4287.561540, accumulated_logging_time=4.569386, accumulated_submission_time=49622.736548, global_step=109364, preemption_count=0, score=49622.736548, test/accuracy=0.542500, test/loss=2.130657, test/num_examples=10000, total_duration=53920.595329, train/accuracy=0.708164, train/loss=1.288613, validation/accuracy=0.659580, validation/loss=1.507832, validation/num_examples=50000
I0131 04:47:41.495820 139741760567040 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.2785463333129883, loss=2.9987633228302
I0131 04:48:24.617477 139741768959744 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.0676071643829346, loss=2.835127830505371
I0131 04:49:10.622231 139741760567040 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.2731122970581055, loss=5.1114501953125
I0131 04:49:56.509696 139741768959744 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.7090728282928467, loss=3.7539565563201904
I0131 04:50:42.405184 139741760567040 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.3752546310424805, loss=2.9121336936950684
I0131 04:51:28.512678 139741768959744 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.0977225303649902, loss=3.2923011779785156
I0131 04:52:14.585078 139741760567040 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.10048508644104, loss=3.332253932952881
I0131 04:53:00.482763 139741768959744 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.117384433746338, loss=3.349578857421875
I0131 04:53:46.536632 139741760567040 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.1920835971832275, loss=3.208651304244995
I0131 04:54:26.987428 139936116377408 spec.py:321] Evaluating on the training split.
I0131 04:54:38.907602 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 04:55:03.352552 139936116377408 spec.py:349] Evaluating on the test split.
I0131 04:55:04.964716 139936116377408 submission_runner.py:408] Time since start: 54378.78s, 	Step: 110290, 	{'train/accuracy': 0.7182226181030273, 'train/loss': 1.246045470237732, 'validation/accuracy': 0.6599000096321106, 'validation/loss': 1.4922248125076294, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.11667799949646, 'test/num_examples': 10000, 'score': 50042.85763192177, 'total_duration': 54378.784519672394, 'accumulated_submission_time': 50042.85763192177, 'accumulated_eval_time': 4325.538824796677, 'accumulated_logging_time': 4.612484693527222}
I0131 04:55:05.000548 139741768959744 logging_writer.py:48] [110290] accumulated_eval_time=4325.538825, accumulated_logging_time=4.612485, accumulated_submission_time=50042.857632, global_step=110290, preemption_count=0, score=50042.857632, test/accuracy=0.538700, test/loss=2.116678, test/num_examples=10000, total_duration=54378.784520, train/accuracy=0.718223, train/loss=1.246045, validation/accuracy=0.659900, validation/loss=1.492225, validation/num_examples=50000
I0131 04:55:09.373902 139741760567040 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.3343307971954346, loss=2.928006172180176
I0131 04:55:50.678728 139741768959744 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.2433087825775146, loss=2.903780460357666
I0131 04:56:36.672864 139741760567040 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.9077764749526978, loss=4.665663242340088
I0131 04:57:22.987617 139741768959744 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.0672590732574463, loss=2.831979990005493
I0131 04:58:08.857448 139741760567040 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.02278733253479, loss=2.890320301055908
I0131 04:58:54.819527 139741768959744 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.0651047229766846, loss=3.1369969844818115
I0131 04:59:40.743706 139741760567040 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.0426132678985596, loss=5.058804988861084
I0131 05:00:26.976802 139741768959744 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.070847272872925, loss=3.2311463356018066
I0131 05:01:13.016502 139741760567040 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.8375747203826904, loss=3.94842529296875
I0131 05:01:58.918625 139741768959744 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.088592767715454, loss=4.208889961242676
I0131 05:02:05.015267 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:02:16.794417 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:02:40.374585 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:02:41.984580 139936116377408 submission_runner.py:408] Time since start: 54835.80s, 	Step: 111215, 	{'train/accuracy': 0.7352343797683716, 'train/loss': 1.1619881391525269, 'validation/accuracy': 0.6606599688529968, 'validation/loss': 1.4827560186386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.1034798622131348, 'test/num_examples': 10000, 'score': 50462.81347370148, 'total_duration': 54835.804351091385, 'accumulated_submission_time': 50462.81347370148, 'accumulated_eval_time': 4362.5080988407135, 'accumulated_logging_time': 4.6590001583099365}
I0131 05:02:42.024993 139741760567040 logging_writer.py:48] [111215] accumulated_eval_time=4362.508099, accumulated_logging_time=4.659000, accumulated_submission_time=50462.813474, global_step=111215, preemption_count=0, score=50462.813474, test/accuracy=0.532100, test/loss=2.103480, test/num_examples=10000, total_duration=54835.804351, train/accuracy=0.735234, train/loss=1.161988, validation/accuracy=0.660660, validation/loss=1.482756, validation/num_examples=50000
I0131 05:03:16.181456 139741768959744 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.9711204767227173, loss=2.8028321266174316
I0131 05:04:01.747638 139741760567040 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.8753318786621094, loss=4.442641258239746
I0131 05:04:47.769530 139741768959744 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.9932842254638672, loss=4.420891284942627
I0131 05:05:33.873484 139741760567040 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.1663036346435547, loss=2.9678523540496826
I0131 05:06:19.884282 139741768959744 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.076361894607544, loss=2.7765064239501953
I0131 05:07:05.948756 139741760567040 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.15423583984375, loss=3.024044990539551
I0131 05:07:51.709396 139741768959744 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.1490652561187744, loss=3.3289852142333984
I0131 05:08:37.727940 139741760567040 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.006850004196167, loss=4.654926300048828
I0131 05:09:23.634960 139741768959744 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.8639841079711914, loss=4.3285393714904785
I0131 05:09:42.464395 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:09:54.448322 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:10:16.610682 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:10:18.216459 139936116377408 submission_runner.py:408] Time since start: 55292.04s, 	Step: 112141, 	{'train/accuracy': 0.7141991853713989, 'train/loss': 1.2547168731689453, 'validation/accuracy': 0.6646999716758728, 'validation/loss': 1.48099946975708, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.13720703125, 'test/num_examples': 10000, 'score': 50883.195809841156, 'total_duration': 55292.03625512123, 'accumulated_submission_time': 50883.195809841156, 'accumulated_eval_time': 4398.260158777237, 'accumulated_logging_time': 4.708902835845947}
I0131 05:10:18.252172 139741760567040 logging_writer.py:48] [112141] accumulated_eval_time=4398.260159, accumulated_logging_time=4.708903, accumulated_submission_time=50883.195810, global_step=112141, preemption_count=0, score=50883.195810, test/accuracy=0.534000, test/loss=2.137207, test/num_examples=10000, total_duration=55292.036255, train/accuracy=0.714199, train/loss=1.254717, validation/accuracy=0.664700, validation/loss=1.480999, validation/num_examples=50000
I0131 05:10:42.067812 139741768959744 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.269745349884033, loss=2.8741707801818848
I0131 05:11:26.578500 139741760567040 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.0126585960388184, loss=3.311065673828125
I0131 05:12:13.021116 139741768959744 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.17223858833313, loss=2.942720413208008
I0131 05:12:58.863898 139741760567040 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.588568925857544, loss=2.8989949226379395
I0131 05:13:45.047084 139741768959744 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.9292927980422974, loss=3.5835518836975098
I0131 05:14:31.044348 139741760567040 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.982131838798523, loss=3.2247586250305176
I0131 05:15:17.254211 139741768959744 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.3449738025665283, loss=2.9028027057647705
I0131 05:16:03.393274 139741760567040 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.1537461280822754, loss=2.9964540004730225
I0131 05:16:49.676874 139741768959744 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.0241730213165283, loss=2.809462070465088
I0131 05:17:18.571181 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:17:30.365553 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:17:56.866624 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:17:58.469964 139936116377408 submission_runner.py:408] Time since start: 55752.29s, 	Step: 113064, 	{'train/accuracy': 0.7215819954872131, 'train/loss': 1.2358224391937256, 'validation/accuracy': 0.668940007686615, 'validation/loss': 1.475932002067566, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.1098098754882812, 'test/num_examples': 10000, 'score': 51303.45684719086, 'total_duration': 55752.28976273537, 'accumulated_submission_time': 51303.45684719086, 'accumulated_eval_time': 4438.158957719803, 'accumulated_logging_time': 4.753890514373779}
I0131 05:17:58.505095 139741760567040 logging_writer.py:48] [113064] accumulated_eval_time=4438.158958, accumulated_logging_time=4.753891, accumulated_submission_time=51303.456847, global_step=113064, preemption_count=0, score=51303.456847, test/accuracy=0.543900, test/loss=2.109810, test/num_examples=10000, total_duration=55752.289763, train/accuracy=0.721582, train/loss=1.235822, validation/accuracy=0.668940, validation/loss=1.475932, validation/num_examples=50000
I0131 05:18:13.190639 139741768959744 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.080547332763672, loss=2.8183157444000244
I0131 05:18:56.116418 139741760567040 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.1012609004974365, loss=2.795132637023926
I0131 05:19:42.184630 139741768959744 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.1305906772613525, loss=2.945037841796875
I0131 05:20:28.623763 139741760567040 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.969340205192566, loss=4.651368141174316
I0131 05:21:14.604512 139741768959744 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.9781767129898071, loss=3.287904977798462
I0131 05:22:00.719604 139741760567040 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.209444284439087, loss=3.038755178451538
I0131 05:22:46.700762 139741768959744 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.401231288909912, loss=2.904104232788086
I0131 05:23:32.323963 139741760567040 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.117398738861084, loss=2.8289506435394287
I0131 05:24:18.609070 139741768959744 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.272256374359131, loss=3.0165693759918213
I0131 05:24:58.498794 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:25:10.462179 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:25:34.839099 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:25:36.442345 139936116377408 submission_runner.py:408] Time since start: 56210.26s, 	Step: 113989, 	{'train/accuracy': 0.7289257645606995, 'train/loss': 1.1877706050872803, 'validation/accuracy': 0.660860002040863, 'validation/loss': 1.4783949851989746, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.090823173522949, 'test/num_examples': 10000, 'score': 51723.392731666565, 'total_duration': 56210.262149095535, 'accumulated_submission_time': 51723.392731666565, 'accumulated_eval_time': 4476.102520704269, 'accumulated_logging_time': 4.79875922203064}
I0131 05:25:36.477562 139741760567040 logging_writer.py:48] [113989] accumulated_eval_time=4476.102521, accumulated_logging_time=4.798759, accumulated_submission_time=51723.392732, global_step=113989, preemption_count=0, score=51723.392732, test/accuracy=0.543300, test/loss=2.090823, test/num_examples=10000, total_duration=56210.262149, train/accuracy=0.728926, train/loss=1.187771, validation/accuracy=0.660860, validation/loss=1.478395, validation/num_examples=50000
I0131 05:25:41.243388 139741768959744 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.9038861989974976, loss=3.7413408756256104
I0131 05:26:22.627366 139741760567040 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.3476755619049072, loss=2.8106741905212402
I0131 05:27:08.743530 139741768959744 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.0092074871063232, loss=2.806757926940918
I0131 05:27:55.035798 139741760567040 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.1020219326019287, loss=2.866194486618042
I0131 05:28:41.098304 139741768959744 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.0526342391967773, loss=3.427499294281006
I0131 05:29:27.299304 139741760567040 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.1174476146698, loss=2.897160530090332
I0131 05:30:13.244455 139741768959744 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.325594663619995, loss=3.07977557182312
I0131 05:30:59.023742 139741760567040 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.1991219520568848, loss=2.927825450897217
I0131 05:31:45.221785 139741768959744 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.354008436203003, loss=5.080622673034668
I0131 05:32:31.355887 139741760567040 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.307837963104248, loss=3.1508660316467285
I0131 05:32:36.518806 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:32:48.498592 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:33:12.681071 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:33:14.281029 139936116377408 submission_runner.py:408] Time since start: 56668.10s, 	Step: 114913, 	{'train/accuracy': 0.71644526720047, 'train/loss': 1.250329852104187, 'validation/accuracy': 0.6658799648284912, 'validation/loss': 1.4862215518951416, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.1070306301116943, 'test/num_examples': 10000, 'score': 52143.375336408615, 'total_duration': 56668.10083293915, 'accumulated_submission_time': 52143.375336408615, 'accumulated_eval_time': 4513.86473441124, 'accumulated_logging_time': 4.843943119049072}
I0131 05:33:14.316260 139741768959744 logging_writer.py:48] [114913] accumulated_eval_time=4513.864734, accumulated_logging_time=4.843943, accumulated_submission_time=52143.375336, global_step=114913, preemption_count=0, score=52143.375336, test/accuracy=0.543500, test/loss=2.107031, test/num_examples=10000, total_duration=56668.100833, train/accuracy=0.716445, train/loss=1.250330, validation/accuracy=0.665880, validation/loss=1.486222, validation/num_examples=50000
I0131 05:33:49.391859 139741760567040 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.274320125579834, loss=3.3849470615386963
I0131 05:34:35.219418 139741768959744 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.364598274230957, loss=2.785310983657837
I0131 05:35:21.434396 139741760567040 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.1314663887023926, loss=3.7648544311523438
I0131 05:36:08.187072 139741768959744 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.299600601196289, loss=2.8884036540985107
I0131 05:36:53.902292 139741760567040 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.249302864074707, loss=2.841762065887451
I0131 05:37:40.341888 139741768959744 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.438542604446411, loss=2.9276955127716064
I0131 05:38:26.289569 139741760567040 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.2846498489379883, loss=2.968778610229492
I0131 05:39:12.279377 139741768959744 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.8584492206573486, loss=3.7331886291503906
I0131 05:39:58.162968 139741760567040 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.4747018814086914, loss=2.8271005153656006
I0131 05:40:14.487436 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:40:26.535144 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:40:52.033010 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:40:53.638158 139936116377408 submission_runner.py:408] Time since start: 57127.46s, 	Step: 115837, 	{'train/accuracy': 0.7221484184265137, 'train/loss': 1.2188655138015747, 'validation/accuracy': 0.669439971446991, 'validation/loss': 1.451919674873352, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.074211597442627, 'test/num_examples': 10000, 'score': 52563.48961663246, 'total_duration': 57127.45796251297, 'accumulated_submission_time': 52563.48961663246, 'accumulated_eval_time': 4553.015449285507, 'accumulated_logging_time': 4.888232231140137}
I0131 05:40:53.674539 139741768959744 logging_writer.py:48] [115837] accumulated_eval_time=4553.015449, accumulated_logging_time=4.888232, accumulated_submission_time=52563.489617, global_step=115837, preemption_count=0, score=52563.489617, test/accuracy=0.545900, test/loss=2.074212, test/num_examples=10000, total_duration=57127.457963, train/accuracy=0.722148, train/loss=1.218866, validation/accuracy=0.669440, validation/loss=1.451920, validation/num_examples=50000
I0131 05:41:19.086888 139741760567040 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.266848564147949, loss=4.936178207397461
I0131 05:42:03.753979 139741768959744 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.3807482719421387, loss=2.867292881011963
I0131 05:42:49.969371 139741760567040 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.9763420820236206, loss=4.142353057861328
I0131 05:43:36.206856 139741768959744 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.5008444786071777, loss=2.9197306632995605
I0131 05:44:22.115226 139741760567040 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.1895580291748047, loss=2.886218309402466
I0131 05:45:08.171325 139741768959744 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.4477925300598145, loss=3.0589351654052734
I0131 05:45:53.951070 139741760567040 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.9003682136535645, loss=3.804870128631592
I0131 05:46:40.318479 139741768959744 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.2883315086364746, loss=2.857313871383667
I0131 05:47:26.276791 139741760567040 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.21401047706604, loss=2.8240480422973633
I0131 05:47:53.993947 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:48:06.143690 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:48:32.580415 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:48:34.188038 139936116377408 submission_runner.py:408] Time since start: 57588.01s, 	Step: 116762, 	{'train/accuracy': 0.7323827743530273, 'train/loss': 1.1630433797836304, 'validation/accuracy': 0.6712200045585632, 'validation/loss': 1.4364888668060303, 'validation/num_examples': 50000, 'test/accuracy': 0.5437999963760376, 'test/loss': 2.064565420150757, 'test/num_examples': 10000, 'score': 52983.74047589302, 'total_duration': 57588.00782227516, 'accumulated_submission_time': 52983.74047589302, 'accumulated_eval_time': 4593.209508657455, 'accumulated_logging_time': 4.9340245723724365}
I0131 05:48:34.220797 139741768959744 logging_writer.py:48] [116762] accumulated_eval_time=4593.209509, accumulated_logging_time=4.934025, accumulated_submission_time=52983.740476, global_step=116762, preemption_count=0, score=52983.740476, test/accuracy=0.543800, test/loss=2.064565, test/num_examples=10000, total_duration=57588.007822, train/accuracy=0.732383, train/loss=1.163043, validation/accuracy=0.671220, validation/loss=1.436489, validation/num_examples=50000
I0131 05:48:49.689164 139741760567040 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.3338887691497803, loss=2.844508647918701
I0131 05:49:33.103358 139741768959744 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.4021265506744385, loss=2.9290964603424072
I0131 05:50:19.140057 139741760567040 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.3094236850738525, loss=2.9553558826446533
I0131 05:51:05.318138 139741768959744 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.4042952060699463, loss=2.948380947113037
I0131 05:51:50.998570 139741760567040 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.2087786197662354, loss=3.663085460662842
I0131 05:52:36.920354 139741768959744 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.2172820568084717, loss=2.847452163696289
I0131 05:53:23.038231 139741760567040 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.13570237159729, loss=4.9870147705078125
I0131 05:54:08.760075 139741768959744 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.2281100749969482, loss=4.682807445526123
I0131 05:54:54.854794 139741760567040 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.9789992570877075, loss=4.606616020202637
I0131 05:55:34.489999 139936116377408 spec.py:321] Evaluating on the training split.
I0131 05:55:46.400589 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 05:56:13.696473 139936116377408 spec.py:349] Evaluating on the test split.
I0131 05:56:15.307852 139936116377408 submission_runner.py:408] Time since start: 58049.13s, 	Step: 117688, 	{'train/accuracy': 0.7284570336341858, 'train/loss': 1.1957414150238037, 'validation/accuracy': 0.6744399666786194, 'validation/loss': 1.430295467376709, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.056349277496338, 'test/num_examples': 10000, 'score': 53403.952806949615, 'total_duration': 58049.12762546539, 'accumulated_submission_time': 53403.952806949615, 'accumulated_eval_time': 4634.027329921722, 'accumulated_logging_time': 4.976112604141235}
I0131 05:56:15.346653 139741768959744 logging_writer.py:48] [117688] accumulated_eval_time=4634.027330, accumulated_logging_time=4.976113, accumulated_submission_time=53403.952807, global_step=117688, preemption_count=0, score=53403.952807, test/accuracy=0.552000, test/loss=2.056349, test/num_examples=10000, total_duration=58049.127625, train/accuracy=0.728457, train/loss=1.195741, validation/accuracy=0.674440, validation/loss=1.430295, validation/num_examples=50000
I0131 05:56:20.499361 139741760567040 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.4760196208953857, loss=2.760976552963257
I0131 05:57:02.142479 139741768959744 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.5453274250030518, loss=2.858837127685547
I0131 05:57:48.061210 139741760567040 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.9669294357299805, loss=4.623246192932129
I0131 05:58:34.466053 139741768959744 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.1919808387756348, loss=4.9988603591918945
I0131 05:59:20.481657 139741760567040 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.1327707767486572, loss=4.53539514541626
I0131 06:00:06.754129 139741768959744 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.0924196243286133, loss=3.7107958793640137
I0131 06:00:52.557134 139741760567040 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.1863906383514404, loss=3.545778751373291
I0131 06:01:38.633498 139741768959744 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.5330162048339844, loss=2.932863235473633
I0131 06:02:24.763987 139741760567040 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.02215576171875, loss=3.9783051013946533
I0131 06:03:10.860226 139741768959744 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.300766944885254, loss=3.1187191009521484
I0131 06:03:15.656304 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:03:27.541914 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:03:55.408547 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:03:57.009656 139936116377408 submission_runner.py:408] Time since start: 58510.83s, 	Step: 118612, 	{'train/accuracy': 0.724804699420929, 'train/loss': 1.209438681602478, 'validation/accuracy': 0.6693399548530579, 'validation/loss': 1.4543384313583374, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.073585271835327, 'test/num_examples': 10000, 'score': 53824.20411038399, 'total_duration': 58510.82946014404, 'accumulated_submission_time': 53824.20411038399, 'accumulated_eval_time': 4675.380663156509, 'accumulated_logging_time': 5.025782823562622}
I0131 06:03:57.044673 139741760567040 logging_writer.py:48] [118612] accumulated_eval_time=4675.380663, accumulated_logging_time=5.025783, accumulated_submission_time=53824.204110, global_step=118612, preemption_count=0, score=53824.204110, test/accuracy=0.551200, test/loss=2.073585, test/num_examples=10000, total_duration=58510.829460, train/accuracy=0.724805, train/loss=1.209439, validation/accuracy=0.669340, validation/loss=1.454338, validation/num_examples=50000
I0131 06:04:32.980518 139741768959744 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.2908732891082764, loss=2.842092990875244
I0131 06:05:18.662503 139741760567040 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.07735538482666, loss=3.0344719886779785
I0131 06:06:04.792127 139741768959744 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.2905709743499756, loss=2.7397255897521973
I0131 06:06:51.017739 139741760567040 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.5318002700805664, loss=3.069756031036377
I0131 06:07:37.065839 139741768959744 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.2642109394073486, loss=3.511181592941284
I0131 06:08:23.129409 139741760567040 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.090860605239868, loss=4.172199249267578
I0131 06:09:09.116690 139741768959744 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.353396415710449, loss=2.7627151012420654
I0131 06:09:55.130426 139741760567040 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.1844637393951416, loss=3.8600754737854004
I0131 06:10:41.168072 139741768959744 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.420193910598755, loss=2.995291233062744
I0131 06:10:57.380184 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:11:09.348175 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:11:36.520042 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:11:38.138103 139936116377408 submission_runner.py:408] Time since start: 58971.96s, 	Step: 119537, 	{'train/accuracy': 0.7267187237739563, 'train/loss': 1.2144628763198853, 'validation/accuracy': 0.670520007610321, 'validation/loss': 1.4620471000671387, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.072385549545288, 'test/num_examples': 10000, 'score': 54244.48342585564, 'total_duration': 58971.95789599419, 'accumulated_submission_time': 54244.48342585564, 'accumulated_eval_time': 4716.138568401337, 'accumulated_logging_time': 5.0698935985565186}
I0131 06:11:38.179766 139741760567040 logging_writer.py:48] [119537] accumulated_eval_time=4716.138568, accumulated_logging_time=5.069894, accumulated_submission_time=54244.483426, global_step=119537, preemption_count=0, score=54244.483426, test/accuracy=0.548100, test/loss=2.072386, test/num_examples=10000, total_duration=58971.957896, train/accuracy=0.726719, train/loss=1.214463, validation/accuracy=0.670520, validation/loss=1.462047, validation/num_examples=50000
I0131 06:12:03.590465 139741768959744 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.2646374702453613, loss=3.8965628147125244
I0131 06:12:48.373984 139741760567040 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.2619500160217285, loss=4.603934288024902
I0131 06:13:34.413644 139741768959744 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.197368621826172, loss=3.446563720703125
I0131 06:14:20.711808 139741760567040 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.5708281993865967, loss=2.7870707511901855
I0131 06:15:06.644126 139741768959744 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.606288194656372, loss=3.014610528945923
I0131 06:15:52.791097 139741760567040 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.175865650177002, loss=2.845569133758545
I0131 06:16:38.657094 139741768959744 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.515536069869995, loss=3.0227837562561035
I0131 06:17:24.647643 139741760567040 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.432755708694458, loss=3.340181350708008
I0131 06:18:10.617958 139741768959744 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.4562835693359375, loss=2.865858554840088
I0131 06:18:38.365376 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:18:50.157751 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:19:13.700937 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:19:15.305314 139936116377408 submission_runner.py:408] Time since start: 59429.13s, 	Step: 120462, 	{'train/accuracy': 0.7491210699081421, 'train/loss': 1.108149766921997, 'validation/accuracy': 0.6756599545478821, 'validation/loss': 1.4270612001419067, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 2.0518739223480225, 'test/num_examples': 10000, 'score': 54664.61124134064, 'total_duration': 59429.12511634827, 'accumulated_submission_time': 54664.61124134064, 'accumulated_eval_time': 4753.07851433754, 'accumulated_logging_time': 5.1212615966796875}
I0131 06:19:15.339663 139741760567040 logging_writer.py:48] [120462] accumulated_eval_time=4753.078514, accumulated_logging_time=5.121262, accumulated_submission_time=54664.611241, global_step=120462, preemption_count=0, score=54664.611241, test/accuracy=0.554000, test/loss=2.051874, test/num_examples=10000, total_duration=59429.125116, train/accuracy=0.749121, train/loss=1.108150, validation/accuracy=0.675660, validation/loss=1.427061, validation/num_examples=50000
I0131 06:19:30.814092 139741768959744 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.3183341026306152, loss=2.8681704998016357
I0131 06:20:14.012476 139741760567040 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.5339653491973877, loss=2.875570297241211
I0131 06:21:00.018446 139741768959744 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.409985065460205, loss=2.83553409576416
I0131 06:21:46.239774 139741760567040 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.2174630165100098, loss=3.7689788341522217
I0131 06:22:31.975942 139741768959744 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.381160020828247, loss=3.005280017852783
I0131 06:23:18.197342 139741760567040 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.3568482398986816, loss=4.425894737243652
I0131 06:24:03.848923 139741768959744 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.494049310684204, loss=2.8226451873779297
I0131 06:24:49.945225 139741760567040 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.213104009628296, loss=4.315746307373047
I0131 06:25:35.813808 139741768959744 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.1557745933532715, loss=3.2369747161865234
I0131 06:26:15.461784 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:26:27.308842 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:26:54.574575 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:26:56.180708 139936116377408 submission_runner.py:408] Time since start: 59890.00s, 	Step: 121388, 	{'train/accuracy': 0.7331249713897705, 'train/loss': 1.1577279567718506, 'validation/accuracy': 0.6822599768638611, 'validation/loss': 1.3907045125961304, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.0155694484710693, 'test/num_examples': 10000, 'score': 55084.6750433445, 'total_duration': 59890.00051164627, 'accumulated_submission_time': 55084.6750433445, 'accumulated_eval_time': 4793.797434568405, 'accumulated_logging_time': 5.165852785110474}
I0131 06:26:56.213828 139741760567040 logging_writer.py:48] [121388] accumulated_eval_time=4793.797435, accumulated_logging_time=5.165853, accumulated_submission_time=55084.675043, global_step=121388, preemption_count=0, score=55084.675043, test/accuracy=0.557300, test/loss=2.015569, test/num_examples=10000, total_duration=59890.000512, train/accuracy=0.733125, train/loss=1.157728, validation/accuracy=0.682260, validation/loss=1.390705, validation/num_examples=50000
I0131 06:27:01.358511 139741768959744 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.225391387939453, loss=4.215460777282715
I0131 06:27:43.047158 139741760567040 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.3610239028930664, loss=2.7738094329833984
I0131 06:28:28.979463 139741768959744 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.6376354694366455, loss=2.798959732055664
I0131 06:29:15.205558 139741760567040 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.35640549659729, loss=2.7947499752044678
I0131 06:30:01.002534 139741768959744 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.2271790504455566, loss=3.1537060737609863
I0131 06:30:46.965396 139741760567040 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.532243251800537, loss=2.8664186000823975
I0131 06:31:33.115908 139741768959744 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.3835315704345703, loss=2.850706100463867
I0131 06:32:19.302093 139741760567040 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.3826348781585693, loss=2.787320137023926
I0131 06:33:05.272158 139741768959744 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.1917428970336914, loss=3.171346664428711
I0131 06:33:51.055733 139741760567040 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.4307940006256104, loss=3.1683192253112793
I0131 06:33:56.214805 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:34:08.130844 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:34:32.756711 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:34:34.367230 139936116377408 submission_runner.py:408] Time since start: 60348.19s, 	Step: 122313, 	{'train/accuracy': 0.7346875071525574, 'train/loss': 1.1657814979553223, 'validation/accuracy': 0.675599992275238, 'validation/loss': 1.4246152639389038, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.026743173599243, 'test/num_examples': 10000, 'score': 55504.61960840225, 'total_duration': 60348.186989068985, 'accumulated_submission_time': 55504.61960840225, 'accumulated_eval_time': 4831.9498081207275, 'accumulated_logging_time': 5.2081685066223145}
I0131 06:34:34.409122 139741768959744 logging_writer.py:48] [122313] accumulated_eval_time=4831.949808, accumulated_logging_time=5.208169, accumulated_submission_time=55504.619608, global_step=122313, preemption_count=0, score=55504.619608, test/accuracy=0.559000, test/loss=2.026743, test/num_examples=10000, total_duration=60348.186989, train/accuracy=0.734688, train/loss=1.165781, validation/accuracy=0.675600, validation/loss=1.424615, validation/num_examples=50000
I0131 06:35:09.717213 139741760567040 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.4070522785186768, loss=4.387815475463867
I0131 06:35:55.061455 139741768959744 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.36684513092041, loss=2.8522186279296875
I0131 06:36:41.282490 139741760567040 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.2150580883026123, loss=2.745630979537964
I0131 06:37:27.397593 139741768959744 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.691337823867798, loss=2.8050544261932373
I0131 06:38:13.496794 139741760567040 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.6207094192504883, loss=2.8313918113708496
I0131 06:38:59.879054 139741768959744 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.1739537715911865, loss=2.9989216327667236
I0131 06:39:46.030671 139741760567040 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.2659459114074707, loss=3.3232412338256836
I0131 06:40:32.630652 139741768959744 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.9879958629608154, loss=3.7527782917022705
I0131 06:41:18.795405 139741760567040 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.3413872718811035, loss=2.8456037044525146
I0131 06:41:34.649590 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:41:46.767117 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:42:12.915913 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:42:14.515156 139936116377408 submission_runner.py:408] Time since start: 60808.33s, 	Step: 123236, 	{'train/accuracy': 0.7521093487739563, 'train/loss': 1.1196391582489014, 'validation/accuracy': 0.6818400025367737, 'validation/loss': 1.4134577512741089, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.019491672515869, 'test/num_examples': 10000, 'score': 55924.80131602287, 'total_duration': 60808.3349378109, 'accumulated_submission_time': 55924.80131602287, 'accumulated_eval_time': 4871.8153512477875, 'accumulated_logging_time': 5.260644197463989}
I0131 06:42:14.548433 139741768959744 logging_writer.py:48] [123236] accumulated_eval_time=4871.815351, accumulated_logging_time=5.260644, accumulated_submission_time=55924.801316, global_step=123236, preemption_count=0, score=55924.801316, test/accuracy=0.563000, test/loss=2.019492, test/num_examples=10000, total_duration=60808.334938, train/accuracy=0.752109, train/loss=1.119639, validation/accuracy=0.681840, validation/loss=1.413458, validation/num_examples=50000
I0131 06:42:40.345909 139741760567040 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.5067734718322754, loss=2.955921173095703
I0131 06:43:25.348692 139741768959744 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.496046304702759, loss=2.8162014484405518
I0131 06:44:11.586812 139741760567040 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.562767505645752, loss=2.91825270652771
I0131 06:44:57.616168 139741768959744 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.1782174110412598, loss=4.562877655029297
I0131 06:45:43.904968 139741760567040 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.3621156215667725, loss=2.801687717437744
I0131 06:46:29.936293 139741768959744 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.3054020404815674, loss=4.371302127838135
I0131 06:47:16.147404 139741760567040 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.185180902481079, loss=4.067119598388672
I0131 06:48:02.129628 139741768959744 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.430419445037842, loss=2.754716157913208
I0131 06:48:48.466559 139741760567040 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.379570245742798, loss=2.9666190147399902
I0131 06:49:14.917400 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:49:26.856338 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:49:54.260587 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:49:55.870784 139936116377408 submission_runner.py:408] Time since start: 61269.69s, 	Step: 124159, 	{'train/accuracy': 0.7383007407188416, 'train/loss': 1.1520472764968872, 'validation/accuracy': 0.6840800046920776, 'validation/loss': 1.3852416276931763, 'validation/num_examples': 50000, 'test/accuracy': 0.560200035572052, 'test/loss': 2.0003364086151123, 'test/num_examples': 10000, 'score': 56345.114104270935, 'total_duration': 61269.690590143204, 'accumulated_submission_time': 56345.114104270935, 'accumulated_eval_time': 4912.7687220573425, 'accumulated_logging_time': 5.302764654159546}
I0131 06:49:55.904004 139741768959744 logging_writer.py:48] [124159] accumulated_eval_time=4912.768722, accumulated_logging_time=5.302765, accumulated_submission_time=56345.114104, global_step=124159, preemption_count=0, score=56345.114104, test/accuracy=0.560200, test/loss=2.000336, test/num_examples=10000, total_duration=61269.690590, train/accuracy=0.738301, train/loss=1.152047, validation/accuracy=0.684080, validation/loss=1.385242, validation/num_examples=50000
I0131 06:50:12.569518 139741760567040 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.5000388622283936, loss=2.7874679565429688
I0131 06:50:55.790001 139741768959744 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.540064573287964, loss=2.8174428939819336
I0131 06:51:42.220410 139741760567040 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.3121931552886963, loss=3.6359689235687256
I0131 06:52:28.338343 139741768959744 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.136195182800293, loss=3.7970633506774902
I0131 06:53:14.213412 139741760567040 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.170104742050171, loss=3.412689447402954
I0131 06:54:00.058535 139741768959744 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.226268768310547, loss=3.8753063678741455
I0131 06:54:45.918663 139741760567040 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.3949882984161377, loss=2.71696400642395
I0131 06:55:31.797407 139741768959744 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.446816921234131, loss=2.721752166748047
I0131 06:56:17.685574 139741760567040 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.4199233055114746, loss=2.895660877227783
I0131 06:56:56.252955 139936116377408 spec.py:321] Evaluating on the training split.
I0131 06:57:07.876802 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 06:57:33.338012 139936116377408 spec.py:349] Evaluating on the test split.
I0131 06:57:34.956959 139936116377408 submission_runner.py:408] Time since start: 61728.78s, 	Step: 125086, 	{'train/accuracy': 0.7397655844688416, 'train/loss': 1.1573693752288818, 'validation/accuracy': 0.6845999956130981, 'validation/loss': 1.4100217819213867, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.0429952144622803, 'test/num_examples': 10000, 'score': 56765.40418744087, 'total_duration': 61728.776727199554, 'accumulated_submission_time': 56765.40418744087, 'accumulated_eval_time': 4951.472690105438, 'accumulated_logging_time': 5.346803903579712}
I0131 06:57:34.996871 139741768959744 logging_writer.py:48] [125086] accumulated_eval_time=4951.472690, accumulated_logging_time=5.346804, accumulated_submission_time=56765.404187, global_step=125086, preemption_count=0, score=56765.404187, test/accuracy=0.558900, test/loss=2.042995, test/num_examples=10000, total_duration=61728.776727, train/accuracy=0.739766, train/loss=1.157369, validation/accuracy=0.684600, validation/loss=1.410022, validation/num_examples=50000
I0131 06:57:40.947721 139741760567040 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.6662445068359375, loss=2.7168564796447754
I0131 06:58:22.619011 139741768959744 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.3434269428253174, loss=4.169229984283447
I0131 06:59:08.437722 139741760567040 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.3320441246032715, loss=2.8910069465637207
I0131 06:59:54.991447 139741768959744 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.7625350952148438, loss=2.688081741333008
I0131 07:00:40.793090 139741760567040 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.5521485805511475, loss=2.7379496097564697
I0131 07:01:26.985792 139741768959744 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.2759757041931152, loss=4.476016044616699
I0131 07:02:13.142879 139741760567040 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.218783140182495, loss=3.723170042037964
I0131 07:02:59.053948 139741768959744 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.420779228210449, loss=3.013073682785034
I0131 07:03:44.999282 139741760567040 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.4762930870056152, loss=4.835439682006836
I0131 07:04:30.941785 139741768959744 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.8542544841766357, loss=2.692936897277832
I0131 07:04:35.177067 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:04:47.108780 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:05:14.830542 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:05:16.446441 139936116377408 submission_runner.py:408] Time since start: 62190.27s, 	Step: 126011, 	{'train/accuracy': 0.7486132383346558, 'train/loss': 1.118463397026062, 'validation/accuracy': 0.6848799586296082, 'validation/loss': 1.392722725868225, 'validation/num_examples': 50000, 'test/accuracy': 0.5542000532150269, 'test/loss': 2.0321836471557617, 'test/num_examples': 10000, 'score': 57185.52679872513, 'total_duration': 62190.26622200012, 'accumulated_submission_time': 57185.52679872513, 'accumulated_eval_time': 4992.742039680481, 'accumulated_logging_time': 5.39652943611145}
I0131 07:05:16.485051 139741760567040 logging_writer.py:48] [126011] accumulated_eval_time=4992.742040, accumulated_logging_time=5.396529, accumulated_submission_time=57185.526799, global_step=126011, preemption_count=0, score=57185.526799, test/accuracy=0.554200, test/loss=2.032184, test/num_examples=10000, total_duration=62190.266222, train/accuracy=0.748613, train/loss=1.118463, validation/accuracy=0.684880, validation/loss=1.392723, validation/num_examples=50000
I0131 07:05:52.785893 139741768959744 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.5518908500671387, loss=3.0671350955963135
I0131 07:06:38.661514 139741760567040 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.6170504093170166, loss=3.1682260036468506
I0131 07:07:24.905982 139741768959744 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.4060544967651367, loss=4.83577823638916
I0131 07:08:11.449344 139741760567040 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.4830448627471924, loss=4.7383928298950195
I0131 07:08:57.143984 139741768959744 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.8070149421691895, loss=2.6761693954467773
I0131 07:09:43.464483 139741760567040 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.401977777481079, loss=4.452070236206055
I0131 07:10:29.355509 139741768959744 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.5606918334960938, loss=2.89654541015625
I0131 07:11:15.267056 139741760567040 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.41869854927063, loss=2.7815322875976562
I0131 07:12:01.462207 139741768959744 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.164393663406372, loss=3.7869460582733154
I0131 07:12:16.515541 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:12:28.399156 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:12:53.905558 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:12:55.510283 139936116377408 submission_runner.py:408] Time since start: 62649.33s, 	Step: 126934, 	{'train/accuracy': 0.7451757788658142, 'train/loss': 1.1258550882339478, 'validation/accuracy': 0.686739981174469, 'validation/loss': 1.3771331310272217, 'validation/num_examples': 50000, 'test/accuracy': 0.5667000412940979, 'test/loss': 1.988448977470398, 'test/num_examples': 10000, 'score': 57605.49915289879, 'total_duration': 62649.33008289337, 'accumulated_submission_time': 57605.49915289879, 'accumulated_eval_time': 5031.7367787361145, 'accumulated_logging_time': 5.445305347442627}
I0131 07:12:55.544682 139741760567040 logging_writer.py:48] [126934] accumulated_eval_time=5031.736779, accumulated_logging_time=5.445305, accumulated_submission_time=57605.499153, global_step=126934, preemption_count=0, score=57605.499153, test/accuracy=0.566700, test/loss=1.988449, test/num_examples=10000, total_duration=62649.330083, train/accuracy=0.745176, train/loss=1.125855, validation/accuracy=0.686740, validation/loss=1.377133, validation/num_examples=50000
I0131 07:13:22.139009 139741768959744 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.5605907440185547, loss=2.841886520385742
I0131 07:14:06.931153 139741760567040 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.6977014541625977, loss=2.663510799407959
I0131 07:14:52.883305 139741768959744 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.5683915615081787, loss=3.0382003784179688
I0131 07:15:39.154088 139741760567040 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.44844913482666, loss=2.8330650329589844
I0131 07:16:25.272365 139741768959744 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.401334047317505, loss=2.6324853897094727
I0131 07:17:11.405950 139741760567040 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.680675506591797, loss=2.747471809387207
I0131 07:17:57.145910 139741768959744 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.812086343765259, loss=2.8248372077941895
I0131 07:18:43.402431 139741760567040 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.4045169353485107, loss=2.9443342685699463
I0131 07:19:29.716992 139741768959744 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.6773288249969482, loss=2.806957483291626
I0131 07:19:55.839043 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:20:07.884008 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:20:35.134461 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:20:36.743478 139936116377408 submission_runner.py:408] Time since start: 63110.56s, 	Step: 127859, 	{'train/accuracy': 0.7473437190055847, 'train/loss': 1.1448750495910645, 'validation/accuracy': 0.6888999938964844, 'validation/loss': 1.3919219970703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.017214298248291, 'test/num_examples': 10000, 'score': 58025.73322200775, 'total_duration': 63110.56328248978, 'accumulated_submission_time': 58025.73322200775, 'accumulated_eval_time': 5072.641215085983, 'accumulated_logging_time': 5.489341497421265}
I0131 07:20:36.781475 139741760567040 logging_writer.py:48] [127859] accumulated_eval_time=5072.641215, accumulated_logging_time=5.489341, accumulated_submission_time=58025.733222, global_step=127859, preemption_count=0, score=58025.733222, test/accuracy=0.563700, test/loss=2.017214, test/num_examples=10000, total_duration=63110.563282, train/accuracy=0.747344, train/loss=1.144875, validation/accuracy=0.688900, validation/loss=1.391922, validation/num_examples=50000
I0131 07:20:53.445186 139741768959744 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.2935097217559814, loss=4.150960445404053
I0131 07:21:37.065295 139741760567040 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.6930131912231445, loss=2.699333906173706
I0131 07:22:23.191166 139741768959744 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.5842437744140625, loss=3.2533018589019775
I0131 07:23:09.500584 139741760567040 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.676987648010254, loss=2.8095877170562744
I0131 07:23:55.287975 139741768959744 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.503547191619873, loss=2.7914578914642334
I0131 07:24:41.266654 139741760567040 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.403794288635254, loss=4.730778694152832
I0131 07:25:27.479407 139741768959744 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.3056886196136475, loss=3.8978679180145264
I0131 07:26:13.645339 139741760567040 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.8920671939849854, loss=3.5129857063293457
I0131 07:26:59.768377 139741768959744 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.4459831714630127, loss=3.8768486976623535
I0131 07:27:36.832960 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:27:48.621802 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:28:16.506760 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:28:18.106960 139936116377408 submission_runner.py:408] Time since start: 63571.93s, 	Step: 128782, 	{'train/accuracy': 0.753710925579071, 'train/loss': 1.085994005203247, 'validation/accuracy': 0.6908999681472778, 'validation/loss': 1.3590346574783325, 'validation/num_examples': 50000, 'test/accuracy': 0.5646000504493713, 'test/loss': 1.9902604818344116, 'test/num_examples': 10000, 'score': 58445.72785902023, 'total_duration': 63571.92673802376, 'accumulated_submission_time': 58445.72785902023, 'accumulated_eval_time': 5113.915201663971, 'accumulated_logging_time': 5.5364158153533936}
I0131 07:28:18.145403 139741760567040 logging_writer.py:48] [128782] accumulated_eval_time=5113.915202, accumulated_logging_time=5.536416, accumulated_submission_time=58445.727859, global_step=128782, preemption_count=0, score=58445.727859, test/accuracy=0.564600, test/loss=1.990260, test/num_examples=10000, total_duration=63571.926738, train/accuracy=0.753711, train/loss=1.085994, validation/accuracy=0.690900, validation/loss=1.359035, validation/num_examples=50000
I0131 07:28:25.674756 139741768959744 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.8182485103607178, loss=2.737610101699829
I0131 07:29:08.061668 139741760567040 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.670902729034424, loss=2.9656119346618652
I0131 07:29:53.638163 139741768959744 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.4111807346343994, loss=3.7372121810913086
I0131 07:30:39.922529 139741760567040 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.467992067337036, loss=3.5291261672973633
I0131 07:31:26.090612 139741768959744 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.5413310527801514, loss=3.3326032161712646
I0131 07:32:12.164494 139741760567040 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.6247730255126953, loss=2.735501766204834
I0131 07:32:58.138939 139741768959744 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.9254331588745117, loss=2.689490795135498
I0131 07:33:44.038341 139741760567040 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.4822492599487305, loss=3.1574740409851074
I0131 07:34:30.044924 139741768959744 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.563037395477295, loss=2.857450485229492
I0131 07:35:15.893079 139741760567040 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.3526957035064697, loss=4.834211349487305
I0131 07:35:18.324169 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:35:30.207663 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:35:57.542388 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:35:59.148224 139936116377408 submission_runner.py:408] Time since start: 64032.97s, 	Step: 129707, 	{'train/accuracy': 0.7651562094688416, 'train/loss': 1.0557949542999268, 'validation/accuracy': 0.6894400119781494, 'validation/loss': 1.376419186592102, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 2.0006537437438965, 'test/num_examples': 10000, 'score': 58865.84973907471, 'total_duration': 64032.96802806854, 'accumulated_submission_time': 58865.84973907471, 'accumulated_eval_time': 5154.739242553711, 'accumulated_logging_time': 5.584019184112549}
I0131 07:35:59.183136 139741768959744 logging_writer.py:48] [129707] accumulated_eval_time=5154.739243, accumulated_logging_time=5.584019, accumulated_submission_time=58865.849739, global_step=129707, preemption_count=0, score=58865.849739, test/accuracy=0.562000, test/loss=2.000654, test/num_examples=10000, total_duration=64032.968028, train/accuracy=0.765156, train/loss=1.055795, validation/accuracy=0.689440, validation/loss=1.376419, validation/num_examples=50000
I0131 07:36:37.341187 139741760567040 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.501938819885254, loss=2.6575348377227783
I0131 07:37:23.380272 139741768959744 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.254936933517456, loss=4.846429824829102
I0131 07:38:09.954629 139741760567040 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.8461380004882812, loss=3.208106756210327
I0131 07:38:56.299864 139741768959744 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.355750322341919, loss=3.6538195610046387
I0131 07:39:42.586268 139741760567040 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.5159213542938232, loss=4.094751834869385
I0131 07:40:29.207434 139741768959744 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.841301679611206, loss=2.7292184829711914
I0131 07:41:15.455767 139741760567040 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.770038604736328, loss=4.27722692489624
I0131 07:42:01.605305 139741768959744 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.705486536026001, loss=2.6479263305664062
I0131 07:42:47.675194 139741760567040 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.557964324951172, loss=4.268294334411621
I0131 07:42:59.369675 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:43:11.453141 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:43:38.808628 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:43:40.416811 139936116377408 submission_runner.py:408] Time since start: 64494.24s, 	Step: 130627, 	{'train/accuracy': 0.7515429258346558, 'train/loss': 1.082430362701416, 'validation/accuracy': 0.6952399611473083, 'validation/loss': 1.3379631042480469, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 1.951003074645996, 'test/num_examples': 10000, 'score': 59285.97770404816, 'total_duration': 64494.23661541939, 'accumulated_submission_time': 59285.97770404816, 'accumulated_eval_time': 5195.786374568939, 'accumulated_logging_time': 5.630152940750122}
I0131 07:43:40.455062 139741768959744 logging_writer.py:48] [130627] accumulated_eval_time=5195.786375, accumulated_logging_time=5.630153, accumulated_submission_time=59285.977704, global_step=130627, preemption_count=0, score=59285.977704, test/accuracy=0.573800, test/loss=1.951003, test/num_examples=10000, total_duration=64494.236615, train/accuracy=0.751543, train/loss=1.082430, validation/accuracy=0.695240, validation/loss=1.337963, validation/num_examples=50000
I0131 07:44:09.847019 139741760567040 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.7522754669189453, loss=2.7188377380371094
I0131 07:44:55.135262 139741768959744 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.3593642711639404, loss=3.6619045734405518
I0131 07:45:41.644335 139741760567040 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.758333921432495, loss=4.334444522857666
I0131 07:46:27.643979 139741768959744 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.5056285858154297, loss=2.663597583770752
I0131 07:47:13.786747 139741760567040 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.469119071960449, loss=3.881117343902588
I0131 07:47:59.581952 139741768959744 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.02760910987854, loss=2.7863664627075195
I0131 07:48:45.470017 139741760567040 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.805457830429077, loss=4.437685012817383
I0131 07:49:31.705608 139741768959744 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.775951623916626, loss=3.910432815551758
I0131 07:50:18.231835 139741760567040 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.6034717559814453, loss=4.678900718688965
I0131 07:50:40.777795 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:50:52.603968 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:51:15.562935 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:51:17.160179 139936116377408 submission_runner.py:408] Time since start: 64950.98s, 	Step: 131551, 	{'train/accuracy': 0.7570898532867432, 'train/loss': 1.0904736518859863, 'validation/accuracy': 0.6944199800491333, 'validation/loss': 1.3673386573791504, 'validation/num_examples': 50000, 'test/accuracy': 0.5703999996185303, 'test/loss': 1.9871017932891846, 'test/num_examples': 10000, 'score': 59706.242832660675, 'total_duration': 64950.979976415634, 'accumulated_submission_time': 59706.242832660675, 'accumulated_eval_time': 5232.168748378754, 'accumulated_logging_time': 5.6784138679504395}
I0131 07:51:17.199261 139741768959744 logging_writer.py:48] [131551] accumulated_eval_time=5232.168748, accumulated_logging_time=5.678414, accumulated_submission_time=59706.242833, global_step=131551, preemption_count=0, score=59706.242833, test/accuracy=0.570400, test/loss=1.987102, test/num_examples=10000, total_duration=64950.979976, train/accuracy=0.757090, train/loss=1.090474, validation/accuracy=0.694420, validation/loss=1.367339, validation/num_examples=50000
I0131 07:51:37.053901 139741760567040 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.8402791023254395, loss=2.9899682998657227
I0131 07:52:20.966955 139741768959744 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.6728501319885254, loss=2.6910624504089355
I0131 07:53:07.211604 139741760567040 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.6090590953826904, loss=2.816655158996582
I0131 07:53:53.665938 139741768959744 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.5190556049346924, loss=2.6837964057922363
I0131 07:54:39.753082 139741760567040 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.976170778274536, loss=2.811056613922119
I0131 07:55:25.852070 139741768959744 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.7722277641296387, loss=2.7599802017211914
I0131 07:56:11.776593 139741760567040 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.7039742469787598, loss=4.490994453430176
I0131 07:56:57.602813 139741768959744 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.8344290256500244, loss=2.640014886856079
I0131 07:57:43.865753 139741760567040 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.624101161956787, loss=2.7993738651275635
I0131 07:58:17.592819 139936116377408 spec.py:321] Evaluating on the training split.
I0131 07:58:29.435076 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 07:58:56.463851 139936116377408 spec.py:349] Evaluating on the test split.
I0131 07:58:58.068133 139936116377408 submission_runner.py:408] Time since start: 65411.89s, 	Step: 132475, 	{'train/accuracy': 0.769238293170929, 'train/loss': 1.0069472789764404, 'validation/accuracy': 0.7003200054168701, 'validation/loss': 1.3125197887420654, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.9146699905395508, 'test/num_examples': 10000, 'score': 60126.579996824265, 'total_duration': 65411.887921094894, 'accumulated_submission_time': 60126.579996824265, 'accumulated_eval_time': 5272.644042253494, 'accumulated_logging_time': 5.7267396450042725}
I0131 07:58:58.104429 139741768959744 logging_writer.py:48] [132475] accumulated_eval_time=5272.644042, accumulated_logging_time=5.726740, accumulated_submission_time=60126.579997, global_step=132475, preemption_count=0, score=60126.579997, test/accuracy=0.576800, test/loss=1.914670, test/num_examples=10000, total_duration=65411.887921, train/accuracy=0.769238, train/loss=1.006947, validation/accuracy=0.700320, validation/loss=1.312520, validation/num_examples=50000
I0131 07:59:08.419837 139741760567040 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.8180158138275146, loss=2.896296977996826
I0131 07:59:50.934590 139741768959744 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.665348768234253, loss=2.603015184402466
I0131 08:00:36.705944 139741760567040 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.8036980628967285, loss=2.619845151901245
I0131 08:01:22.904802 139741768959744 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.7446835041046143, loss=3.8039121627807617
I0131 08:02:09.054235 139741760567040 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.774844169616699, loss=2.790346622467041
I0131 08:02:54.910332 139741768959744 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.6955859661102295, loss=2.938389301300049
I0131 08:03:40.864602 139741760567040 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.7668960094451904, loss=2.682080030441284
I0131 08:04:26.990114 139741768959744 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.9901161193847656, loss=2.638796329498291
I0131 08:05:13.125675 139741760567040 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.418731927871704, loss=3.6248409748077393
I0131 08:05:58.832176 139741768959744 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.961927652359009, loss=2.7643935680389404
I0131 08:05:58.844193 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:06:11.249169 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 08:06:38.960454 139936116377408 spec.py:349] Evaluating on the test split.
I0131 08:06:40.566830 139936116377408 submission_runner.py:408] Time since start: 65874.39s, 	Step: 133401, 	{'train/accuracy': 0.757519543170929, 'train/loss': 1.062558889389038, 'validation/accuracy': 0.6997599601745605, 'validation/loss': 1.3051460981369019, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.93559992313385, 'test/num_examples': 10000, 'score': 60547.2616622448, 'total_duration': 65874.38661026955, 'accumulated_submission_time': 60547.2616622448, 'accumulated_eval_time': 5314.366637706757, 'accumulated_logging_time': 5.773090362548828}
I0131 08:06:40.607082 139741760567040 logging_writer.py:48] [133401] accumulated_eval_time=5314.366638, accumulated_logging_time=5.773090, accumulated_submission_time=60547.261662, global_step=133401, preemption_count=0, score=60547.261662, test/accuracy=0.575500, test/loss=1.935600, test/num_examples=10000, total_duration=65874.386610, train/accuracy=0.757520, train/loss=1.062559, validation/accuracy=0.699760, validation/loss=1.305146, validation/num_examples=50000
I0131 08:07:21.250711 139741768959744 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.0967166423797607, loss=2.821085214614868
I0131 08:08:06.957201 139741760567040 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.792390823364258, loss=4.5159912109375
I0131 08:08:53.142543 139741768959744 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.677943706512451, loss=4.040702819824219
I0131 08:09:39.218770 139741760567040 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.077465534210205, loss=2.75107741355896
I0131 08:10:25.430811 139741768959744 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.771345615386963, loss=2.7716424465179443
I0131 08:11:11.441724 139741760567040 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.727998733520508, loss=3.074126720428467
I0131 08:11:57.492849 139741768959744 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.5504815578460693, loss=3.12054443359375
I0131 08:12:43.514818 139741760567040 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.5728960037231445, loss=3.4271602630615234
I0131 08:13:29.423828 139741768959744 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.0415823459625244, loss=4.67183780670166
I0131 08:13:41.051661 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:13:53.462629 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 08:14:16.082602 139936116377408 spec.py:349] Evaluating on the test split.
I0131 08:14:17.695835 139936116377408 submission_runner.py:408] Time since start: 66331.52s, 	Step: 134327, 	{'train/accuracy': 0.7621874809265137, 'train/loss': 1.0537630319595337, 'validation/accuracy': 0.700980007648468, 'validation/loss': 1.31785249710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9424078464508057, 'test/num_examples': 10000, 'score': 60967.648696899414, 'total_duration': 66331.51563692093, 'accumulated_submission_time': 60967.648696899414, 'accumulated_eval_time': 5351.010791301727, 'accumulated_logging_time': 5.823288679122925}
I0131 08:14:17.733191 139741760567040 logging_writer.py:48] [134327] accumulated_eval_time=5351.010791, accumulated_logging_time=5.823289, accumulated_submission_time=60967.648697, global_step=134327, preemption_count=0, score=60967.648697, test/accuracy=0.575100, test/loss=1.942408, test/num_examples=10000, total_duration=66331.515637, train/accuracy=0.762187, train/loss=1.053763, validation/accuracy=0.700980, validation/loss=1.317852, validation/num_examples=50000
I0131 08:14:47.130225 139741768959744 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.5551507472991943, loss=3.0055689811706543
I0131 08:15:32.481840 139741760567040 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.898705244064331, loss=2.849607467651367
I0131 08:16:18.577760 139741768959744 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.2345125675201416, loss=2.6460063457489014
I0131 08:17:04.571372 139741760567040 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.4383139610290527, loss=3.265443801879883
I0131 08:17:50.425014 139741768959744 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.4951131343841553, loss=3.486088991165161
I0131 08:18:36.688615 139741760567040 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.745649814605713, loss=2.8249740600585938
I0131 08:19:22.989954 139741768959744 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.7310338020324707, loss=3.2787604331970215
I0131 08:20:08.969111 139741760567040 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.7504537105560303, loss=2.6859824657440186
I0131 08:20:55.357096 139741768959744 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.810572624206543, loss=2.567824363708496
I0131 08:21:17.802873 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:21:29.732881 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 08:21:56.956796 139936116377408 spec.py:349] Evaluating on the test split.
I0131 08:21:58.574357 139936116377408 submission_runner.py:408] Time since start: 66792.39s, 	Step: 135250, 	{'train/accuracy': 0.7675390243530273, 'train/loss': 1.0237082242965698, 'validation/accuracy': 0.6987599730491638, 'validation/loss': 1.3225077390670776, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9518496990203857, 'test/num_examples': 10000, 'score': 61387.66164803505, 'total_duration': 66792.39415669441, 'accumulated_submission_time': 61387.66164803505, 'accumulated_eval_time': 5391.782269239426, 'accumulated_logging_time': 5.869398355484009}
I0131 08:21:58.612932 139741760567040 logging_writer.py:48] [135250] accumulated_eval_time=5391.782269, accumulated_logging_time=5.869398, accumulated_submission_time=61387.661648, global_step=135250, preemption_count=0, score=61387.661648, test/accuracy=0.571100, test/loss=1.951850, test/num_examples=10000, total_duration=66792.394157, train/accuracy=0.767539, train/loss=1.023708, validation/accuracy=0.698760, validation/loss=1.322508, validation/num_examples=50000
I0131 08:22:18.842422 139741768959744 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.888155698776245, loss=2.7686076164245605
I0131 08:23:02.910554 139741760567040 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.8689472675323486, loss=2.6285016536712646
I0131 08:23:49.118654 139741768959744 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.015296220779419, loss=2.7475075721740723
I0131 08:24:35.427198 139741760567040 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.0183162689208984, loss=2.75586199760437
I0131 08:25:21.356539 139741768959744 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.772731304168701, loss=2.9125702381134033
I0131 08:26:07.449012 139741760567040 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.796168565750122, loss=2.79945707321167
I0131 08:26:53.192942 139741768959744 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.907834768295288, loss=2.908798933029175
I0131 08:27:39.281179 139741760567040 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.8227148056030273, loss=3.1742889881134033
I0131 08:28:25.483708 139741768959744 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.897921323776245, loss=3.1701362133026123
I0131 08:28:58.871886 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:29:10.799779 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 08:29:35.912894 139936116377408 spec.py:349] Evaluating on the test split.
I0131 08:29:37.518731 139936116377408 submission_runner.py:408] Time since start: 67251.34s, 	Step: 136174, 	{'train/accuracy': 0.764843761920929, 'train/loss': 1.0483845472335815, 'validation/accuracy': 0.7023999691009521, 'validation/loss': 1.3212941884994507, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.929173469543457, 'test/num_examples': 10000, 'score': 61807.8627281189, 'total_duration': 67251.33851337433, 'accumulated_submission_time': 61807.8627281189, 'accumulated_eval_time': 5430.429076910019, 'accumulated_logging_time': 5.918476819992065}
I0131 08:29:37.561372 139741760567040 logging_writer.py:48] [136174] accumulated_eval_time=5430.429077, accumulated_logging_time=5.918477, accumulated_submission_time=61807.862728, global_step=136174, preemption_count=0, score=61807.862728, test/accuracy=0.578900, test/loss=1.929173, test/num_examples=10000, total_duration=67251.338513, train/accuracy=0.764844, train/loss=1.048385, validation/accuracy=0.702400, validation/loss=1.321294, validation/num_examples=50000
I0131 08:29:48.270850 139741768959744 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.9586236476898193, loss=2.6732916831970215
I0131 08:30:30.622719 139741760567040 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.5897891521453857, loss=3.611677408218384
I0131 08:31:16.481887 139741768959744 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.911611795425415, loss=2.631579875946045
I0131 08:32:02.824250 139741760567040 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.279096841812134, loss=2.7524914741516113
I0131 08:32:48.564400 139741768959744 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.411428928375244, loss=2.674553632736206
I0131 08:33:34.668115 139741760567040 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.7694039344787598, loss=2.640717029571533
I0131 08:34:20.417622 139741768959744 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.9526829719543457, loss=2.7233939170837402
I0131 08:35:06.462143 139741760567040 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.875774383544922, loss=2.6029725074768066
I0131 08:35:52.251870 139741768959744 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.806549072265625, loss=2.565077066421509
I0131 08:36:38.144902 139741760567040 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.7878293991088867, loss=4.046463489532471
I0131 08:36:38.157567 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:36:50.236119 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 08:37:15.759624 139936116377408 spec.py:349] Evaluating on the test split.
I0131 08:37:17.371321 139936116377408 submission_runner.py:408] Time since start: 67711.19s, 	Step: 137101, 	{'train/accuracy': 0.7681640386581421, 'train/loss': 1.0459219217300415, 'validation/accuracy': 0.7051399946212769, 'validation/loss': 1.3112800121307373, 'validation/num_examples': 50000, 'test/accuracy': 0.58160001039505, 'test/loss': 1.9345344305038452, 'test/num_examples': 10000, 'score': 62228.401629686356, 'total_duration': 67711.19112372398, 'accumulated_submission_time': 62228.401629686356, 'accumulated_eval_time': 5469.642811059952, 'accumulated_logging_time': 5.971429824829102}
I0131 08:37:17.407003 139741768959744 logging_writer.py:48] [137101] accumulated_eval_time=5469.642811, accumulated_logging_time=5.971430, accumulated_submission_time=62228.401630, global_step=137101, preemption_count=0, score=62228.401630, test/accuracy=0.581600, test/loss=1.934534, test/num_examples=10000, total_duration=67711.191124, train/accuracy=0.768164, train/loss=1.045922, validation/accuracy=0.705140, validation/loss=1.311280, validation/num_examples=50000
I0131 08:37:58.003676 139741760567040 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.92283034324646, loss=2.68886137008667
I0131 08:38:43.766840 139741768959744 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.877474784851074, loss=2.616154670715332
I0131 08:39:30.592731 139741760567040 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.1613049507141113, loss=2.675896644592285
I0131 08:40:16.592705 139741768959744 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.882197380065918, loss=4.35401725769043
I0131 08:41:02.593720 139741760567040 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.0825397968292236, loss=2.7359068393707275
I0131 08:41:49.126868 139741768959744 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.803317070007324, loss=2.6027257442474365
I0131 08:42:35.148903 139741760567040 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.130899429321289, loss=2.580009698867798
I0131 08:43:21.208149 139741768959744 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.8429362773895264, loss=2.6050186157226562
I0131 08:44:07.281218 139741760567040 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.9540185928344727, loss=2.6643102169036865
I0131 08:44:17.448828 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:44:29.491073 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 08:44:57.038120 139936116377408 spec.py:349] Evaluating on the test split.
I0131 08:44:58.650146 139936116377408 submission_runner.py:408] Time since start: 68172.47s, 	Step: 138024, 	{'train/accuracy': 0.7739452719688416, 'train/loss': 1.0145626068115234, 'validation/accuracy': 0.706559956073761, 'validation/loss': 1.2995346784591675, 'validation/num_examples': 50000, 'test/accuracy': 0.5817000269889832, 'test/loss': 1.9207830429077148, 'test/num_examples': 10000, 'score': 62648.38740777969, 'total_duration': 68172.46992921829, 'accumulated_submission_time': 62648.38740777969, 'accumulated_eval_time': 5510.844088315964, 'accumulated_logging_time': 6.016285419464111}
I0131 08:44:58.700046 139741768959744 logging_writer.py:48] [138024] accumulated_eval_time=5510.844088, accumulated_logging_time=6.016285, accumulated_submission_time=62648.387408, global_step=138024, preemption_count=0, score=62648.387408, test/accuracy=0.581700, test/loss=1.920783, test/num_examples=10000, total_duration=68172.469929, train/accuracy=0.773945, train/loss=1.014563, validation/accuracy=0.706560, validation/loss=1.299535, validation/num_examples=50000
I0131 08:45:29.440397 139741760567040 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.004624843597412, loss=3.6212868690490723
I0131 08:46:15.238272 139741768959744 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.887225389480591, loss=2.6658687591552734
I0131 08:47:01.204895 139741760567040 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.9719324111938477, loss=2.620777130126953
I0131 08:47:47.255442 139741768959744 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.0197606086730957, loss=4.38830041885376
I0131 08:48:33.267529 139741760567040 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.805750608444214, loss=2.624382257461548
I0131 08:49:19.462045 139741768959744 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.223900079727173, loss=2.6685099601745605
I0131 08:50:05.604338 139741760567040 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.9793436527252197, loss=2.5792078971862793
I0131 08:50:51.499425 139741768959744 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.885835886001587, loss=2.587803840637207
I0131 08:51:37.893300 139741760567040 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.1191391944885254, loss=2.668001174926758
I0131 08:51:58.866604 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:52:10.839187 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 08:52:38.478416 139936116377408 spec.py:349] Evaluating on the test split.
I0131 08:52:40.080645 139936116377408 submission_runner.py:408] Time since start: 68633.90s, 	Step: 138947, 	{'train/accuracy': 0.7826952934265137, 'train/loss': 0.9653533697128296, 'validation/accuracy': 0.7051399946212769, 'validation/loss': 1.3022921085357666, 'validation/num_examples': 50000, 'test/accuracy': 0.579200029373169, 'test/loss': 1.9257780313491821, 'test/num_examples': 10000, 'score': 63068.49566245079, 'total_duration': 68633.90044283867, 'accumulated_submission_time': 63068.49566245079, 'accumulated_eval_time': 5552.058122396469, 'accumulated_logging_time': 6.076954126358032}
I0131 08:52:40.115970 139741768959744 logging_writer.py:48] [138947] accumulated_eval_time=5552.058122, accumulated_logging_time=6.076954, accumulated_submission_time=63068.495662, global_step=138947, preemption_count=0, score=63068.495662, test/accuracy=0.579200, test/loss=1.925778, test/num_examples=10000, total_duration=68633.900443, train/accuracy=0.782695, train/loss=0.965353, validation/accuracy=0.705140, validation/loss=1.302292, validation/num_examples=50000
I0131 08:53:01.517017 139741760567040 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.082533597946167, loss=3.8903846740722656
I0131 08:53:45.646662 139741768959744 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.0760533809661865, loss=2.638660192489624
I0131 08:54:32.305562 139741760567040 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.0874903202056885, loss=2.650324821472168
I0131 08:55:18.802809 139741768959744 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.149240016937256, loss=2.912623882293701
I0131 08:56:05.065396 139741760567040 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.7507238388061523, loss=3.9284567832946777
I0131 08:56:51.254384 139741768959744 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.962881326675415, loss=4.3632097244262695
I0131 08:57:37.211233 139741760567040 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.2309253215789795, loss=2.6830506324768066
I0131 08:58:23.303021 139741768959744 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.3131189346313477, loss=2.5093014240264893
I0131 08:59:09.422001 139741760567040 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.8815813064575195, loss=3.7516608238220215
I0131 08:59:40.431908 139936116377408 spec.py:321] Evaluating on the training split.
I0131 08:59:52.339105 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:00:18.282759 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:00:19.884881 139936116377408 submission_runner.py:408] Time since start: 69093.70s, 	Step: 139869, 	{'train/accuracy': 0.7697656154632568, 'train/loss': 1.0047340393066406, 'validation/accuracy': 0.7098000049591064, 'validation/loss': 1.2667945623397827, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8732690811157227, 'test/num_examples': 10000, 'score': 63488.75289964676, 'total_duration': 69093.70464968681, 'accumulated_submission_time': 63488.75289964676, 'accumulated_eval_time': 5591.511062860489, 'accumulated_logging_time': 6.124045372009277}
I0131 09:00:19.927566 139741768959744 logging_writer.py:48] [139869] accumulated_eval_time=5591.511063, accumulated_logging_time=6.124045, accumulated_submission_time=63488.752900, global_step=139869, preemption_count=0, score=63488.752900, test/accuracy=0.589300, test/loss=1.873269, test/num_examples=10000, total_duration=69093.704650, train/accuracy=0.769766, train/loss=1.004734, validation/accuracy=0.709800, validation/loss=1.266795, validation/num_examples=50000
I0131 09:00:32.633744 139741760567040 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.8164408206939697, loss=2.9976601600646973
I0131 09:01:15.551294 139741768959744 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.7619988918304443, loss=3.664865493774414
I0131 09:02:01.125663 139741760567040 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.0134377479553223, loss=2.4057326316833496
I0131 09:02:47.337965 139741768959744 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.1708927154541016, loss=2.644496440887451
I0131 09:03:33.478029 139741760567040 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.887960910797119, loss=2.7120611667633057
I0131 09:04:19.461441 139741768959744 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.0391597747802734, loss=4.138660430908203
I0131 09:05:05.576866 139741760567040 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.849722385406494, loss=3.3381576538085938
I0131 09:05:51.078436 139741768959744 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.155277967453003, loss=2.6550779342651367
I0131 09:06:37.155318 139741760567040 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.2982547283172607, loss=2.6817915439605713
I0131 09:07:20.060596 139936116377408 spec.py:321] Evaluating on the training split.
I0131 09:07:31.807423 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:07:56.507055 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:07:58.115252 139936116377408 submission_runner.py:408] Time since start: 69551.94s, 	Step: 140795, 	{'train/accuracy': 0.7763866782188416, 'train/loss': 0.9946498870849609, 'validation/accuracy': 0.7087399959564209, 'validation/loss': 1.28446626663208, 'validation/num_examples': 50000, 'test/accuracy': 0.5814000368118286, 'test/loss': 1.9018096923828125, 'test/num_examples': 10000, 'score': 63908.82472872734, 'total_duration': 69551.93505644798, 'accumulated_submission_time': 63908.82472872734, 'accumulated_eval_time': 5629.565707683563, 'accumulated_logging_time': 6.17889142036438}
I0131 09:07:58.154830 139741768959744 logging_writer.py:48] [140795] accumulated_eval_time=5629.565708, accumulated_logging_time=6.178891, accumulated_submission_time=63908.824729, global_step=140795, preemption_count=0, score=63908.824729, test/accuracy=0.581400, test/loss=1.901810, test/num_examples=10000, total_duration=69551.935056, train/accuracy=0.776387, train/loss=0.994650, validation/accuracy=0.708740, validation/loss=1.284466, validation/num_examples=50000
I0131 09:08:00.536354 139741760567040 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.9384405612945557, loss=2.634448766708374
I0131 09:08:41.466788 139741768959744 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.33320951461792, loss=2.6307449340820312
I0131 09:09:27.282983 139741760567040 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.228313446044922, loss=4.338232040405273
I0131 09:10:13.558996 139741768959744 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.8621160984039307, loss=2.964431047439575
I0131 09:10:59.494971 139741760567040 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.9020769596099854, loss=2.6761739253997803
I0131 09:11:45.534403 139741768959744 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.1804635524749756, loss=2.560028076171875
I0131 09:12:32.142339 139741760567040 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.755272150039673, loss=2.64500093460083
I0131 09:13:18.217164 139741768959744 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.2664482593536377, loss=4.266983509063721
I0131 09:14:04.233609 139741760567040 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.053661346435547, loss=2.5676348209381104
I0131 09:14:50.630286 139741768959744 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.316911220550537, loss=2.6346542835235596
I0131 09:14:58.500653 139936116377408 spec.py:321] Evaluating on the training split.
I0131 09:15:10.555855 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:15:36.063676 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:15:37.668894 139936116377408 submission_runner.py:408] Time since start: 70011.49s, 	Step: 141719, 	{'train/accuracy': 0.7819921970367432, 'train/loss': 0.9503366351127625, 'validation/accuracy': 0.7135999798774719, 'validation/loss': 1.2518689632415771, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.8770052194595337, 'test/num_examples': 10000, 'score': 64329.11201548576, 'total_duration': 70011.48867511749, 'accumulated_submission_time': 64329.11201548576, 'accumulated_eval_time': 5668.733921766281, 'accumulated_logging_time': 6.22956919670105}
I0131 09:15:37.709256 139741760567040 logging_writer.py:48] [141719] accumulated_eval_time=5668.733922, accumulated_logging_time=6.229569, accumulated_submission_time=64329.112015, global_step=141719, preemption_count=0, score=64329.112015, test/accuracy=0.589600, test/loss=1.877005, test/num_examples=10000, total_duration=70011.488675, train/accuracy=0.781992, train/loss=0.950337, validation/accuracy=0.713600, validation/loss=1.251869, validation/num_examples=50000
I0131 09:16:10.449742 139741768959744 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.9695138931274414, loss=3.356261730194092
I0131 09:16:56.234159 139741760567040 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.8581717014312744, loss=2.611145496368408
I0131 09:17:42.851817 139741768959744 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.767887830734253, loss=3.924544334411621
I0131 09:18:28.990840 139741760567040 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.792266607284546, loss=3.8717129230499268
I0131 09:19:15.205169 139741768959744 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.077523708343506, loss=2.6529252529144287
I0131 09:20:00.953881 139741760567040 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.1527211666107178, loss=2.5761051177978516
I0131 09:20:47.288890 139741768959744 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.1125054359436035, loss=4.33022928237915
I0131 09:21:33.696600 139741760567040 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.0552680492401123, loss=3.6545283794403076
I0131 09:22:19.818570 139741768959744 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.260129690170288, loss=2.622441530227661
I0131 09:22:38.013006 139936116377408 spec.py:321] Evaluating on the training split.
I0131 09:22:49.888688 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:23:14.494394 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:23:16.111201 139936116377408 submission_runner.py:408] Time since start: 70469.93s, 	Step: 142641, 	{'train/accuracy': 0.7760937213897705, 'train/loss': 0.9692516326904297, 'validation/accuracy': 0.7108399868011475, 'validation/loss': 1.23958420753479, 'validation/num_examples': 50000, 'test/accuracy': 0.5865000486373901, 'test/loss': 1.8613325357437134, 'test/num_examples': 10000, 'score': 64749.359623909, 'total_duration': 70469.93096780777, 'accumulated_submission_time': 64749.359623909, 'accumulated_eval_time': 5706.832077026367, 'accumulated_logging_time': 6.278799533843994}
I0131 09:23:16.155255 139741760567040 logging_writer.py:48] [142641] accumulated_eval_time=5706.832077, accumulated_logging_time=6.278800, accumulated_submission_time=64749.359624, global_step=142641, preemption_count=0, score=64749.359624, test/accuracy=0.586500, test/loss=1.861333, test/num_examples=10000, total_duration=70469.930968, train/accuracy=0.776094, train/loss=0.969252, validation/accuracy=0.710840, validation/loss=1.239584, validation/num_examples=50000
I0131 09:23:39.963801 139741768959744 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.2179250717163086, loss=2.5669732093811035
I0131 09:24:24.171654 139741760567040 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.258697509765625, loss=3.762533187866211
I0131 09:25:10.232501 139741768959744 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.3024346828460693, loss=2.660144805908203
I0131 09:25:56.273020 139741760567040 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.4007272720336914, loss=4.298276424407959
I0131 09:26:42.358026 139741768959744 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.9342103004455566, loss=2.7350034713745117
I0131 09:27:28.497076 139741760567040 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.9608662128448486, loss=4.096885681152344
I0131 09:28:14.269684 139741768959744 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.1045422554016113, loss=2.8208930492401123
I0131 09:29:00.412417 139741760567040 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.2012810707092285, loss=2.6191635131835938
I0131 09:29:46.538884 139741768959744 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.74672269821167, loss=2.6372814178466797
I0131 09:30:16.214897 139936116377408 spec.py:321] Evaluating on the training split.
I0131 09:30:28.169015 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:30:55.344145 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:30:56.949894 139936116377408 submission_runner.py:408] Time since start: 70930.77s, 	Step: 143566, 	{'train/accuracy': 0.778613269329071, 'train/loss': 0.9984338283538818, 'validation/accuracy': 0.7130399942398071, 'validation/loss': 1.2804381847381592, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8944753408432007, 'test/num_examples': 10000, 'score': 65169.36147618294, 'total_duration': 70930.76969718933, 'accumulated_submission_time': 65169.36147618294, 'accumulated_eval_time': 5747.56706738472, 'accumulated_logging_time': 6.333415985107422}
I0131 09:30:56.986149 139741760567040 logging_writer.py:48] [143566] accumulated_eval_time=5747.567067, accumulated_logging_time=6.333416, accumulated_submission_time=65169.361476, global_step=143566, preemption_count=0, score=65169.361476, test/accuracy=0.587800, test/loss=1.894475, test/num_examples=10000, total_duration=70930.769697, train/accuracy=0.778613, train/loss=0.998434, validation/accuracy=0.713040, validation/loss=1.280438, validation/num_examples=50000
I0131 09:31:10.895163 139741768959744 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.085850238800049, loss=3.1505534648895264
I0131 09:31:53.736419 139741760567040 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.3661179542541504, loss=2.7804088592529297
I0131 09:32:39.357978 139741768959744 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.1479716300964355, loss=2.701107978820801
I0131 09:33:25.612078 139741760567040 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.0291829109191895, loss=2.5382180213928223
I0131 09:34:11.831503 139741768959744 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.152801513671875, loss=2.6611642837524414
I0131 09:34:57.623269 139741760567040 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.48149037361145, loss=3.9446635246276855
I0131 09:35:43.575356 139741768959744 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.1855595111846924, loss=3.0987682342529297
I0131 09:36:29.596890 139741760567040 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.18623685836792, loss=2.8879923820495605
I0131 09:37:15.836980 139741768959744 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.480783462524414, loss=2.511319875717163
I0131 09:37:57.121710 139936116377408 spec.py:321] Evaluating on the training split.
I0131 09:38:09.319528 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:38:37.086766 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:38:38.701359 139936116377408 submission_runner.py:408] Time since start: 71392.52s, 	Step: 144491, 	{'train/accuracy': 0.7850390672683716, 'train/loss': 0.9747884273529053, 'validation/accuracy': 0.7129600048065186, 'validation/loss': 1.2740776538848877, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.884268045425415, 'test/num_examples': 10000, 'score': 65589.43929386139, 'total_duration': 71392.52113199234, 'accumulated_submission_time': 65589.43929386139, 'accumulated_eval_time': 5789.146682500839, 'accumulated_logging_time': 6.380536079406738}
I0131 09:38:38.748896 139741760567040 logging_writer.py:48] [144491] accumulated_eval_time=5789.146683, accumulated_logging_time=6.380536, accumulated_submission_time=65589.439294, global_step=144491, preemption_count=0, score=65589.439294, test/accuracy=0.589800, test/loss=1.884268, test/num_examples=10000, total_duration=71392.521132, train/accuracy=0.785039, train/loss=0.974788, validation/accuracy=0.712960, validation/loss=1.274078, validation/num_examples=50000
I0131 09:38:42.719209 139741768959744 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.241058826446533, loss=2.6411149501800537
I0131 09:39:24.443951 139741760567040 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.104397773742676, loss=3.630216121673584
I0131 09:40:10.379953 139741768959744 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.028517484664917, loss=3.0740370750427246
I0131 09:40:56.612480 139741760567040 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.1081154346466064, loss=2.835379123687744
I0131 09:41:42.820070 139741768959744 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.7136335372924805, loss=3.4806268215179443
I0131 09:42:29.024352 139741760567040 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.560040235519409, loss=3.002072811126709
I0131 09:43:14.990699 139741768959744 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.329423666000366, loss=2.8527002334594727
I0131 09:44:01.112981 139741760567040 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.369607448577881, loss=2.660038471221924
I0131 09:44:47.270199 139741768959744 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.021270990371704, loss=2.764702320098877
I0131 09:45:33.311787 139741760567040 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.2005014419555664, loss=4.048150539398193
I0131 09:45:38.776186 139936116377408 spec.py:321] Evaluating on the training split.
I0131 09:45:50.749550 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:46:18.028814 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:46:19.642679 139936116377408 submission_runner.py:408] Time since start: 71853.46s, 	Step: 145414, 	{'train/accuracy': 0.77845698595047, 'train/loss': 0.9919044971466064, 'validation/accuracy': 0.7156599760055542, 'validation/loss': 1.2642072439193726, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8826606273651123, 'test/num_examples': 10000, 'score': 66009.40965223312, 'total_duration': 71853.4624812603, 'accumulated_submission_time': 66009.40965223312, 'accumulated_eval_time': 5830.013171672821, 'accumulated_logging_time': 6.438591480255127}
I0131 09:46:19.683745 139741768959744 logging_writer.py:48] [145414] accumulated_eval_time=5830.013172, accumulated_logging_time=6.438591, accumulated_submission_time=66009.409652, global_step=145414, preemption_count=0, score=66009.409652, test/accuracy=0.596600, test/loss=1.882661, test/num_examples=10000, total_duration=71853.462481, train/accuracy=0.778457, train/loss=0.991904, validation/accuracy=0.715660, validation/loss=1.264207, validation/num_examples=50000
I0131 09:46:54.585016 139741760567040 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.4834113121032715, loss=2.9262053966522217
I0131 09:47:40.490331 139741768959744 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.977287530899048, loss=4.560405731201172
I0131 09:48:26.638757 139741760567040 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.5444793701171875, loss=2.6749939918518066
I0131 09:49:12.600608 139741768959744 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.7201051712036133, loss=2.527427911758423
I0131 09:49:58.385807 139741760567040 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.9984798431396484, loss=2.5491578578948975
I0131 09:50:44.369914 139741768959744 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.5079691410064697, loss=2.573698043823242
I0131 09:51:30.094920 139741760567040 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.137408494949341, loss=3.525785446166992
I0131 09:52:16.428879 139741768959744 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.660095691680908, loss=4.446456432342529
I0131 09:53:02.022600 139741760567040 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.462332010269165, loss=2.3845391273498535
I0131 09:53:20.017435 139936116377408 spec.py:321] Evaluating on the training split.
I0131 09:53:31.885410 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 09:53:55.750377 139936116377408 spec.py:349] Evaluating on the test split.
I0131 09:53:57.362127 139936116377408 submission_runner.py:408] Time since start: 72311.18s, 	Step: 146341, 	{'train/accuracy': 0.7854687571525574, 'train/loss': 0.936896562576294, 'validation/accuracy': 0.7184799909591675, 'validation/loss': 1.2213993072509766, 'validation/num_examples': 50000, 'test/accuracy': 0.5951000452041626, 'test/loss': 1.8369319438934326, 'test/num_examples': 10000, 'score': 66429.68575835228, 'total_duration': 72311.18190431595, 'accumulated_submission_time': 66429.68575835228, 'accumulated_eval_time': 5867.35782957077, 'accumulated_logging_time': 6.490723133087158}
I0131 09:53:57.407103 139741768959744 logging_writer.py:48] [146341] accumulated_eval_time=5867.357830, accumulated_logging_time=6.490723, accumulated_submission_time=66429.685758, global_step=146341, preemption_count=0, score=66429.685758, test/accuracy=0.595100, test/loss=1.836932, test/num_examples=10000, total_duration=72311.181904, train/accuracy=0.785469, train/loss=0.936897, validation/accuracy=0.718480, validation/loss=1.221399, validation/num_examples=50000
I0131 09:54:21.229327 139741760567040 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.8244006633758545, loss=4.5666680335998535
I0131 09:55:05.567009 139741768959744 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.5450034141540527, loss=2.488766670227051
I0131 09:55:51.528889 139741760567040 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.6191978454589844, loss=4.372323513031006
I0131 09:56:37.579213 139741768959744 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.8017990589141846, loss=2.613450527191162
I0131 09:57:23.637272 139741760567040 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.285106897354126, loss=2.561232089996338
I0131 09:58:09.827331 139741768959744 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.715867519378662, loss=2.403890371322632
I0131 09:58:55.706527 139741760567040 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.4143664836883545, loss=2.747260570526123
I0131 09:59:41.963394 139741768959744 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.537712574005127, loss=2.5103023052215576
I0131 10:00:27.657298 139741760567040 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.643646717071533, loss=3.077657699584961
I0131 10:00:57.680494 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:01:09.768318 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:01:35.111011 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:01:36.728931 139936116377408 submission_runner.py:408] Time since start: 72770.55s, 	Step: 147267, 	{'train/accuracy': 0.791308581829071, 'train/loss': 0.9221143126487732, 'validation/accuracy': 0.7231599688529968, 'validation/loss': 1.2180984020233154, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.8256595134735107, 'test/num_examples': 10000, 'score': 66849.90239930153, 'total_duration': 72770.54873371124, 'accumulated_submission_time': 66849.90239930153, 'accumulated_eval_time': 5906.406289815903, 'accumulated_logging_time': 6.545153379440308}
I0131 10:01:36.770427 139741768959744 logging_writer.py:48] [147267] accumulated_eval_time=5906.406290, accumulated_logging_time=6.545153, accumulated_submission_time=66849.902399, global_step=147267, preemption_count=0, score=66849.902399, test/accuracy=0.602400, test/loss=1.825660, test/num_examples=10000, total_duration=72770.548734, train/accuracy=0.791309, train/loss=0.922114, validation/accuracy=0.723160, validation/loss=1.218098, validation/num_examples=50000
I0131 10:01:50.276069 139741760567040 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.4798357486724854, loss=2.495976686477661
I0131 10:02:32.740800 139741768959744 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.459355115890503, loss=3.3385345935821533
I0131 10:03:18.942413 139741760567040 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.958923816680908, loss=4.119162082672119
I0131 10:04:05.070537 139741768959744 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.009428024291992, loss=3.2861156463623047
I0131 10:04:51.162392 139741760567040 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.56084942817688, loss=2.9028007984161377
I0131 10:05:36.994576 139741768959744 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.4043846130371094, loss=2.584477424621582
I0131 10:06:23.035879 139741760567040 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.3130717277526855, loss=2.46480655670166
I0131 10:07:08.847194 139741768959744 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.7657828330993652, loss=2.7787413597106934
I0131 10:07:54.653621 139741760567040 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.6340394020080566, loss=4.0609130859375
I0131 10:08:36.755570 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:08:48.824842 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:09:16.002592 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:09:17.616818 139936116377408 submission_runner.py:408] Time since start: 73231.44s, 	Step: 148193, 	{'train/accuracy': 0.8017187118530273, 'train/loss': 0.8867132663726807, 'validation/accuracy': 0.7227199673652649, 'validation/loss': 1.2239965200424194, 'validation/num_examples': 50000, 'test/accuracy': 0.6012000441551208, 'test/loss': 1.8258556127548218, 'test/num_examples': 10000, 'score': 67269.83102846146, 'total_duration': 73231.43658638, 'accumulated_submission_time': 67269.83102846146, 'accumulated_eval_time': 5947.267498254776, 'accumulated_logging_time': 6.596628427505493}
I0131 10:09:17.657106 139741768959744 logging_writer.py:48] [148193] accumulated_eval_time=5947.267498, accumulated_logging_time=6.596628, accumulated_submission_time=67269.831028, global_step=148193, preemption_count=0, score=67269.831028, test/accuracy=0.601200, test/loss=1.825856, test/num_examples=10000, total_duration=73231.436586, train/accuracy=0.801719, train/loss=0.886713, validation/accuracy=0.722720, validation/loss=1.223997, validation/num_examples=50000
I0131 10:09:20.831796 139741760567040 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.648695707321167, loss=2.5210561752319336
I0131 10:10:01.967029 139741768959744 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.656160831451416, loss=2.612438440322876
I0131 10:10:47.856863 139741760567040 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.491100311279297, loss=2.5490405559539795
I0131 10:11:33.819050 139741768959744 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.5354504585266113, loss=4.311936378479004
I0131 10:12:20.025862 139741760567040 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.5288400650024414, loss=2.5923264026641846
I0131 10:13:06.328970 139741768959744 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.5588409900665283, loss=2.8653430938720703
I0131 10:13:51.632641 139741760567040 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.4714126586914062, loss=2.6159756183624268
I0131 10:14:37.718484 139741768959744 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.5060038566589355, loss=2.939373016357422
I0131 10:15:23.712397 139741760567040 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.329131126403809, loss=4.386219024658203
I0131 10:16:09.616099 139741768959744 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.6636836528778076, loss=2.5310089588165283
I0131 10:16:17.947720 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:16:29.802562 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:16:53.782331 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:16:55.386397 139936116377408 submission_runner.py:408] Time since start: 73689.21s, 	Step: 149120, 	{'train/accuracy': 0.7932226657867432, 'train/loss': 0.906248927116394, 'validation/accuracy': 0.7246599793434143, 'validation/loss': 1.2037746906280518, 'validation/num_examples': 50000, 'test/accuracy': 0.6017000079154968, 'test/loss': 1.801848292350769, 'test/num_examples': 10000, 'score': 67690.06477189064, 'total_duration': 73689.20619821548, 'accumulated_submission_time': 67690.06477189064, 'accumulated_eval_time': 5984.706177949905, 'accumulated_logging_time': 6.646602392196655}
I0131 10:16:55.423373 139741760567040 logging_writer.py:48] [149120] accumulated_eval_time=5984.706178, accumulated_logging_time=6.646602, accumulated_submission_time=67690.064772, global_step=149120, preemption_count=0, score=67690.064772, test/accuracy=0.601700, test/loss=1.801848, test/num_examples=10000, total_duration=73689.206198, train/accuracy=0.793223, train/loss=0.906249, validation/accuracy=0.724660, validation/loss=1.203775, validation/num_examples=50000
I0131 10:17:27.569861 139741768959744 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.6045291423797607, loss=2.726494789123535
I0131 10:18:13.118648 139741760567040 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.183488130569458, loss=3.2561590671539307
I0131 10:18:59.237993 139741768959744 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.172870397567749, loss=2.948970079421997
I0131 10:19:45.176093 139741760567040 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.9373841285705566, loss=4.190016269683838
I0131 10:20:31.329887 139741768959744 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.7187530994415283, loss=3.7829742431640625
I0131 10:21:17.234781 139741760567040 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.018608093261719, loss=2.6076512336730957
I0131 10:22:03.240812 139741768959744 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.540100336074829, loss=2.4239988327026367
I0131 10:22:49.402400 139741760567040 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.5731847286224365, loss=2.627368688583374
I0131 10:23:35.409949 139741768959744 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.0084638595581055, loss=2.646702289581299
I0131 10:23:55.452749 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:24:07.563750 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:24:32.914522 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:24:34.516946 139936116377408 submission_runner.py:408] Time since start: 74148.34s, 	Step: 150046, 	{'train/accuracy': 0.7907617092132568, 'train/loss': 0.9388483762741089, 'validation/accuracy': 0.7214599847793579, 'validation/loss': 1.2328137159347534, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.8345398902893066, 'test/num_examples': 10000, 'score': 68110.0372145176, 'total_duration': 74148.33674430847, 'accumulated_submission_time': 68110.0372145176, 'accumulated_eval_time': 6023.770362854004, 'accumulated_logging_time': 6.693514585494995}
I0131 10:24:34.557131 139741760567040 logging_writer.py:48] [150046] accumulated_eval_time=6023.770363, accumulated_logging_time=6.693515, accumulated_submission_time=68110.037215, global_step=150046, preemption_count=0, score=68110.037215, test/accuracy=0.602700, test/loss=1.834540, test/num_examples=10000, total_duration=74148.336744, train/accuracy=0.790762, train/loss=0.938848, validation/accuracy=0.721460, validation/loss=1.232814, validation/num_examples=50000
I0131 10:24:56.369024 139741768959744 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.763296365737915, loss=2.5323679447174072
I0131 10:25:40.729931 139741760567040 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.906973123550415, loss=4.079219818115234
I0131 10:26:26.794417 139741768959744 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.9361019134521484, loss=4.444845676422119
I0131 10:27:13.286270 139741760567040 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.8088555335998535, loss=2.5704801082611084
I0131 10:27:59.137014 139741768959744 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.8673439025878906, loss=2.5145697593688965
I0131 10:28:45.615872 139741760567040 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.2834928035736084, loss=2.6348204612731934
I0131 10:29:31.689663 139741768959744 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.8310818672180176, loss=4.055469036102295
I0131 10:30:17.822235 139741760567040 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.4628825187683105, loss=2.79374623298645
I0131 10:31:03.991385 139741768959744 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.0969085693359375, loss=3.993544101715088
I0131 10:31:34.533818 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:31:46.598830 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:32:15.633217 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:32:17.233635 139936116377408 submission_runner.py:408] Time since start: 74611.05s, 	Step: 150968, 	{'train/accuracy': 0.8061718344688416, 'train/loss': 0.8653810024261475, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.1898913383483887, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.8021141290664673, 'test/num_examples': 10000, 'score': 68529.95536541939, 'total_duration': 74611.0534362793, 'accumulated_submission_time': 68529.95536541939, 'accumulated_eval_time': 6066.470200300217, 'accumulated_logging_time': 6.744734525680542}
I0131 10:32:17.275654 139741760567040 logging_writer.py:48] [150968] accumulated_eval_time=6066.470200, accumulated_logging_time=6.744735, accumulated_submission_time=68529.955365, global_step=150968, preemption_count=0, score=68529.955365, test/accuracy=0.604300, test/loss=1.802114, test/num_examples=10000, total_duration=74611.053436, train/accuracy=0.806172, train/loss=0.865381, validation/accuracy=0.730120, validation/loss=1.189891, validation/num_examples=50000
I0131 10:32:30.370358 139741768959744 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.079762935638428, loss=2.5459699630737305
I0131 10:33:13.285244 139741760567040 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.858553409576416, loss=2.460451126098633
I0131 10:33:59.440695 139741768959744 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.6298704147338867, loss=2.54958176612854
I0131 10:34:45.711106 139741760567040 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.8214170932769775, loss=2.711796522140503
I0131 10:35:31.951529 139741768959744 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.000889778137207, loss=2.401426315307617
I0131 10:36:18.117837 139741760567040 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.711118221282959, loss=2.8052468299865723
I0131 10:37:04.241489 139741768959744 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.600886106491089, loss=3.113070249557495
I0131 10:37:50.127058 139741760567040 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.4367382526397705, loss=2.9976234436035156
I0131 10:38:36.312549 139741768959744 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.716796636581421, loss=2.663273334503174
I0131 10:39:17.452013 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:39:29.396006 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:39:57.420171 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:39:59.026371 139936116377408 submission_runner.py:408] Time since start: 75072.85s, 	Step: 151891, 	{'train/accuracy': 0.7955273389816284, 'train/loss': 0.8970678448677063, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.1849876642227173, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.778713345527649, 'test/num_examples': 10000, 'score': 68950.07535648346, 'total_duration': 75072.84614467621, 'accumulated_submission_time': 68950.07535648346, 'accumulated_eval_time': 6108.044515609741, 'accumulated_logging_time': 6.796485185623169}
I0131 10:39:59.073220 139741760567040 logging_writer.py:48] [151891] accumulated_eval_time=6108.044516, accumulated_logging_time=6.796485, accumulated_submission_time=68950.075356, global_step=151891, preemption_count=0, score=68950.075356, test/accuracy=0.610400, test/loss=1.778713, test/num_examples=10000, total_duration=75072.846145, train/accuracy=0.795527, train/loss=0.897068, validation/accuracy=0.730120, validation/loss=1.184988, validation/num_examples=50000
I0131 10:40:03.035524 139741768959744 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.6919939517974854, loss=3.7929065227508545
I0131 10:40:44.560005 139741760567040 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.738797903060913, loss=2.467391014099121
I0131 10:41:30.478550 139741768959744 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.5365633964538574, loss=3.252289295196533
I0131 10:42:16.927993 139741760567040 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.93869686126709, loss=3.9185431003570557
I0131 10:43:03.050860 139741768959744 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.681705951690674, loss=2.5426723957061768
I0131 10:43:49.162950 139741760567040 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.656090021133423, loss=2.3432111740112305
I0131 10:44:35.262728 139741768959744 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.249145030975342, loss=2.4615392684936523
I0131 10:45:21.131216 139741760567040 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.127778053283691, loss=2.4783945083618164
I0131 10:46:07.433296 139741768959744 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.084071636199951, loss=4.195085048675537
I0131 10:46:53.398020 139741760567040 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.3829421997070312, loss=3.0100033283233643
I0131 10:46:59.152298 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:47:11.021349 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:47:38.310709 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:47:39.912344 139936116377408 submission_runner.py:408] Time since start: 75533.73s, 	Step: 152814, 	{'train/accuracy': 0.8013476133346558, 'train/loss': 0.8696554899215698, 'validation/accuracy': 0.7305999994277954, 'validation/loss': 1.1746702194213867, 'validation/num_examples': 50000, 'test/accuracy': 0.6066000461578369, 'test/loss': 1.7887028455734253, 'test/num_examples': 10000, 'score': 69370.09718680382, 'total_duration': 75533.73214793205, 'accumulated_submission_time': 69370.09718680382, 'accumulated_eval_time': 6148.80455327034, 'accumulated_logging_time': 6.853741884231567}
I0131 10:47:39.950397 139741768959744 logging_writer.py:48] [152814] accumulated_eval_time=6148.804553, accumulated_logging_time=6.853742, accumulated_submission_time=69370.097187, global_step=152814, preemption_count=0, score=69370.097187, test/accuracy=0.606600, test/loss=1.788703, test/num_examples=10000, total_duration=75533.732148, train/accuracy=0.801348, train/loss=0.869655, validation/accuracy=0.730600, validation/loss=1.174670, validation/num_examples=50000
I0131 10:48:15.273287 139741760567040 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.11316442489624, loss=2.475059986114502
I0131 10:49:01.177954 139741768959744 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.8995912075042725, loss=3.0546927452087402
I0131 10:49:47.555473 139741760567040 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.199283599853516, loss=2.445831775665283
I0131 10:50:33.636122 139741768959744 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.9646451473236084, loss=2.4390170574188232
I0131 10:51:19.888422 139741760567040 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.911848306655884, loss=2.652163505554199
I0131 10:52:06.150360 139741768959744 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.326411724090576, loss=2.5755984783172607
I0131 10:52:52.183918 139741760567040 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.076658725738525, loss=2.4995384216308594
I0131 10:53:38.084636 139741768959744 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.416893005371094, loss=2.4429705142974854
I0131 10:54:24.182208 139741760567040 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.4561591148376465, loss=4.314422607421875
I0131 10:54:39.979698 139936116377408 spec.py:321] Evaluating on the training split.
I0131 10:54:51.714401 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 10:55:18.484682 139936116377408 spec.py:349] Evaluating on the test split.
I0131 10:55:20.102200 139936116377408 submission_runner.py:408] Time since start: 75993.92s, 	Step: 153736, 	{'train/accuracy': 0.8064843416213989, 'train/loss': 0.8482201099395752, 'validation/accuracy': 0.7335599660873413, 'validation/loss': 1.1666568517684937, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.7751833200454712, 'test/num_examples': 10000, 'score': 69790.06775164604, 'total_duration': 75993.92200398445, 'accumulated_submission_time': 69790.06775164604, 'accumulated_eval_time': 6188.927048921585, 'accumulated_logging_time': 6.903278827667236}
I0131 10:55:20.139543 139741768959744 logging_writer.py:48] [153736] accumulated_eval_time=6188.927049, accumulated_logging_time=6.903279, accumulated_submission_time=69790.067752, global_step=153736, preemption_count=0, score=69790.067752, test/accuracy=0.606500, test/loss=1.775183, test/num_examples=10000, total_duration=75993.922004, train/accuracy=0.806484, train/loss=0.848220, validation/accuracy=0.733560, validation/loss=1.166657, validation/num_examples=50000
I0131 10:55:45.936140 139741760567040 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.9537289142608643, loss=4.039530277252197
I0131 10:56:30.778635 139741768959744 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.203448295593262, loss=2.7349164485931396
I0131 10:57:16.938040 139741760567040 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.101180076599121, loss=2.460306406021118
I0131 10:58:02.933870 139741768959744 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.170845985412598, loss=2.481778621673584
I0131 10:58:48.813161 139741760567040 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.6296098232269287, loss=2.551370859146118
I0131 10:59:34.694528 139741768959744 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.104437351226807, loss=2.505673408508301
I0131 11:00:20.706533 139741760567040 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.624526023864746, loss=3.0989396572113037
I0131 11:01:06.480523 139741768959744 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.9212489128112793, loss=3.516021251678467
I0131 11:01:52.563291 139741760567040 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.5493903160095215, loss=3.6957199573516846
I0131 11:02:20.145725 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:02:32.027792 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:02:59.516520 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:03:01.125196 139936116377408 submission_runner.py:408] Time since start: 76454.94s, 	Step: 154662, 	{'train/accuracy': 0.804492175579071, 'train/loss': 0.8526339530944824, 'validation/accuracy': 0.7369999885559082, 'validation/loss': 1.1511931419372559, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.75464928150177, 'test/num_examples': 10000, 'score': 70210.01730847359, 'total_duration': 76454.94499826431, 'accumulated_submission_time': 70210.01730847359, 'accumulated_eval_time': 6229.90651512146, 'accumulated_logging_time': 6.94967794418335}
I0131 11:03:01.164769 139741768959744 logging_writer.py:48] [154662] accumulated_eval_time=6229.906515, accumulated_logging_time=6.949678, accumulated_submission_time=70210.017308, global_step=154662, preemption_count=0, score=70210.017308, test/accuracy=0.617000, test/loss=1.754649, test/num_examples=10000, total_duration=76454.944998, train/accuracy=0.804492, train/loss=0.852634, validation/accuracy=0.737000, validation/loss=1.151193, validation/num_examples=50000
I0131 11:03:16.646164 139741760567040 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.7852416038513184, loss=2.3376054763793945
I0131 11:03:59.706564 139741768959744 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.7630157470703125, loss=2.707850456237793
I0131 11:04:45.754184 139741760567040 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.056440353393555, loss=2.551441192626953
I0131 11:05:32.193160 139741768959744 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.105482578277588, loss=2.4878220558166504
I0131 11:06:17.944857 139741760567040 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.002905368804932, loss=2.4181904792785645
I0131 11:07:04.106937 139741768959744 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.8169918060302734, loss=2.61398983001709
I0131 11:07:49.899353 139741760567040 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.345040798187256, loss=4.335409641265869
I0131 11:08:37.249080 139741768959744 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.0157246589660645, loss=2.3810620307922363
I0131 11:09:23.528004 139741760567040 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.444107532501221, loss=2.8895063400268555
I0131 11:10:01.461601 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:10:13.595504 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:10:41.474823 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:10:43.074799 139936116377408 submission_runner.py:408] Time since start: 76916.89s, 	Step: 155584, 	{'train/accuracy': 0.811328113079071, 'train/loss': 0.8368220329284668, 'validation/accuracy': 0.738860011100769, 'validation/loss': 1.1395392417907715, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.73326575756073, 'test/num_examples': 10000, 'score': 70630.25607657433, 'total_duration': 76916.89459323883, 'accumulated_submission_time': 70630.25607657433, 'accumulated_eval_time': 6271.519709348679, 'accumulated_logging_time': 6.999774217605591}
I0131 11:10:43.116043 139741768959744 logging_writer.py:48] [155584] accumulated_eval_time=6271.519709, accumulated_logging_time=6.999774, accumulated_submission_time=70630.256077, global_step=155584, preemption_count=0, score=70630.256077, test/accuracy=0.617500, test/loss=1.733266, test/num_examples=10000, total_duration=76916.894593, train/accuracy=0.811328, train/loss=0.836822, validation/accuracy=0.738860, validation/loss=1.139539, validation/num_examples=50000
I0131 11:10:49.864228 139741760567040 logging_writer.py:48] [155600] global_step=155600, grad_norm=5.305934906005859, loss=4.461625576019287
I0131 11:11:31.666679 139741768959744 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.6566803455352783, loss=3.2479631900787354
I0131 11:12:17.872828 139741760567040 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.8436808586120605, loss=3.9066379070281982
I0131 11:13:04.040234 139741768959744 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.23789119720459, loss=2.896578550338745
I0131 11:13:49.793980 139741760567040 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.4255690574646, loss=2.714942693710327
I0131 11:14:35.965147 139741768959744 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.055754661560059, loss=2.4598164558410645
I0131 11:15:22.094048 139741760567040 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.258218288421631, loss=2.30940580368042
I0131 11:16:08.295020 139741768959744 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.096556186676025, loss=2.3831675052642822
I0131 11:16:54.575086 139741760567040 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.279586315155029, loss=2.499387741088867
I0131 11:17:40.573715 139741768959744 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.17082405090332, loss=3.5077476501464844
I0131 11:17:43.479442 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:17:55.294815 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:18:19.996201 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:18:21.608130 139936116377408 submission_runner.py:408] Time since start: 77375.43s, 	Step: 156508, 	{'train/accuracy': 0.8138476610183716, 'train/loss': 0.8137789964675903, 'validation/accuracy': 0.7384200096130371, 'validation/loss': 1.1399335861206055, 'validation/num_examples': 50000, 'test/accuracy': 0.6134000420570374, 'test/loss': 1.7455615997314453, 'test/num_examples': 10000, 'score': 71050.56174874306, 'total_duration': 77375.42791485786, 'accumulated_submission_time': 71050.56174874306, 'accumulated_eval_time': 6309.6483726501465, 'accumulated_logging_time': 7.052521228790283}
I0131 11:18:21.651360 139741760567040 logging_writer.py:48] [156508] accumulated_eval_time=6309.648373, accumulated_logging_time=7.052521, accumulated_submission_time=71050.561749, global_step=156508, preemption_count=0, score=71050.561749, test/accuracy=0.613400, test/loss=1.745562, test/num_examples=10000, total_duration=77375.427915, train/accuracy=0.813848, train/loss=0.813779, validation/accuracy=0.738420, validation/loss=1.139934, validation/num_examples=50000
I0131 11:18:59.479018 139741768959744 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.359539031982422, loss=2.858372449874878
I0131 11:19:45.413303 139741760567040 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.361527442932129, loss=2.6990325450897217
I0131 11:20:31.807057 139741768959744 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.428271293640137, loss=4.283901214599609
I0131 11:21:17.920569 139741760567040 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.121919631958008, loss=2.397158145904541
I0131 11:22:04.164908 139741768959744 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.0694708824157715, loss=2.7472004890441895
I0131 11:22:50.032331 139741760567040 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.320922374725342, loss=2.4844629764556885
I0131 11:23:35.943416 139741768959744 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.9388961791992188, loss=3.1467413902282715
I0131 11:24:22.141296 139741760567040 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.221240997314453, loss=2.4100611209869385
I0131 11:25:08.308683 139741768959744 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.108593940734863, loss=3.159719705581665
I0131 11:25:21.829321 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:25:33.791441 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:25:59.584012 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:26:01.197749 139936116377408 submission_runner.py:408] Time since start: 77835.02s, 	Step: 157431, 	{'train/accuracy': 0.8206444978713989, 'train/loss': 0.7946438193321228, 'validation/accuracy': 0.7416599988937378, 'validation/loss': 1.1222891807556152, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.722597599029541, 'test/num_examples': 10000, 'score': 71470.68224668503, 'total_duration': 77835.01753425598, 'accumulated_submission_time': 71470.68224668503, 'accumulated_eval_time': 6349.01679110527, 'accumulated_logging_time': 7.105899810791016}
I0131 11:26:01.239837 139741760567040 logging_writer.py:48] [157431] accumulated_eval_time=6349.016791, accumulated_logging_time=7.105900, accumulated_submission_time=71470.682247, global_step=157431, preemption_count=0, score=71470.682247, test/accuracy=0.619100, test/loss=1.722598, test/num_examples=10000, total_duration=77835.017534, train/accuracy=0.820644, train/loss=0.794644, validation/accuracy=0.741660, validation/loss=1.122289, validation/num_examples=50000
I0131 11:26:29.013679 139741768959744 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.602550029754639, loss=2.4151320457458496
I0131 11:27:13.713140 139741760567040 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.525763988494873, loss=3.9275879859924316
I0131 11:28:16.784820 139741768959744 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.206340789794922, loss=2.410815954208374
I0131 11:29:18.024705 139741760567040 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.530459880828857, loss=2.494194984436035
I0131 11:30:04.736041 139741768959744 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.983595609664917, loss=2.2825610637664795
I0131 11:30:50.702926 139741760567040 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.4031758308410645, loss=2.3933372497558594
I0131 11:31:36.941048 139741768959744 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.031956672668457, loss=3.449378728866577
I0131 11:32:23.207965 139741760567040 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.849426507949829, loss=2.5490381717681885
I0131 11:33:01.541485 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:33:13.523687 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:33:41.683571 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:33:43.286309 139936116377408 submission_runner.py:408] Time since start: 78297.11s, 	Step: 158285, 	{'train/accuracy': 0.81201171875, 'train/loss': 0.8173814415931702, 'validation/accuracy': 0.7415399551391602, 'validation/loss': 1.131630539894104, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.7233086824417114, 'test/num_examples': 10000, 'score': 71890.93097662926, 'total_duration': 78297.10611104965, 'accumulated_submission_time': 71890.93097662926, 'accumulated_eval_time': 6390.761610031128, 'accumulated_logging_time': 7.158337116241455}
I0131 11:33:43.327448 139741768959744 logging_writer.py:48] [158285] accumulated_eval_time=6390.761610, accumulated_logging_time=7.158337, accumulated_submission_time=71890.930977, global_step=158285, preemption_count=0, score=71890.930977, test/accuracy=0.620300, test/loss=1.723309, test/num_examples=10000, total_duration=78297.106111, train/accuracy=0.812012, train/loss=0.817381, validation/accuracy=0.741540, validation/loss=1.131631, validation/num_examples=50000
I0131 11:33:49.679995 139741760567040 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.818326234817505, loss=2.3625431060791016
I0131 11:34:31.690691 139741768959744 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.675299644470215, loss=2.381337881088257
I0131 11:35:17.878061 139741760567040 logging_writer.py:48] [158500] global_step=158500, grad_norm=5.126248359680176, loss=4.275540351867676
I0131 11:36:04.185356 139741768959744 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.021653652191162, loss=2.610393524169922
I0131 11:36:50.033277 139741760567040 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.5750508308410645, loss=4.0347723960876465
I0131 11:37:36.088754 139741768959744 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.463932037353516, loss=2.8567771911621094
I0131 11:38:22.442536 139741760567040 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.5015974044799805, loss=2.3590261936187744
I0131 11:39:08.371500 139741768959744 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.177829265594482, loss=3.370253324508667
I0131 11:39:54.546965 139741760567040 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.2864556312561035, loss=3.71822452545166
I0131 11:40:40.785260 139741768959744 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.415660381317139, loss=2.5364060401916504
I0131 11:40:43.441638 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:40:55.286910 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:41:18.196316 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:41:19.808883 139936116377408 submission_runner.py:408] Time since start: 78753.63s, 	Step: 159207, 	{'train/accuracy': 0.8217968344688416, 'train/loss': 0.7972414493560791, 'validation/accuracy': 0.7410199642181396, 'validation/loss': 1.1312137842178345, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.7257004976272583, 'test/num_examples': 10000, 'score': 72310.98796439171, 'total_duration': 78753.62868118286, 'accumulated_submission_time': 72310.98796439171, 'accumulated_eval_time': 6427.128840446472, 'accumulated_logging_time': 7.209970235824585}
I0131 11:41:19.851789 139741760567040 logging_writer.py:48] [159207] accumulated_eval_time=6427.128840, accumulated_logging_time=7.209970, accumulated_submission_time=72310.987964, global_step=159207, preemption_count=0, score=72310.987964, test/accuracy=0.625700, test/loss=1.725700, test/num_examples=10000, total_duration=78753.628681, train/accuracy=0.821797, train/loss=0.797241, validation/accuracy=0.741020, validation/loss=1.131214, validation/num_examples=50000
I0131 11:41:58.234788 139741768959744 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.2302751541137695, loss=2.4866228103637695
I0131 11:42:44.387778 139741760567040 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.1695098876953125, loss=2.692453622817993
I0131 11:43:30.738639 139741768959744 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.38233757019043, loss=2.3508994579315186
I0131 11:44:16.818596 139741760567040 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.784768581390381, loss=2.3563101291656494
I0131 11:45:02.997725 139741768959744 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.403548717498779, loss=2.3670783042907715
I0131 11:45:49.166735 139741760567040 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.669744491577148, loss=2.485447406768799
I0131 11:46:35.315429 139741768959744 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.270205497741699, loss=3.5266518592834473
I0131 11:47:21.337839 139741760567040 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.4343485832214355, loss=3.1122727394104004
I0131 11:48:07.483748 139741768959744 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.606395721435547, loss=2.36930513381958
I0131 11:48:20.139567 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:48:32.050405 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:48:59.244988 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:49:00.860876 139936116377408 submission_runner.py:408] Time since start: 79214.68s, 	Step: 160129, 	{'train/accuracy': 0.8250585794448853, 'train/loss': 0.7695590257644653, 'validation/accuracy': 0.7425199747085571, 'validation/loss': 1.1162033081054688, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.7093664407730103, 'test/num_examples': 10000, 'score': 72731.21923279762, 'total_duration': 79214.68068003654, 'accumulated_submission_time': 72731.21923279762, 'accumulated_eval_time': 6467.8501443862915, 'accumulated_logging_time': 7.2619194984436035}
I0131 11:49:00.902260 139741760567040 logging_writer.py:48] [160129] accumulated_eval_time=6467.850144, accumulated_logging_time=7.261919, accumulated_submission_time=72731.219233, global_step=160129, preemption_count=0, score=72731.219233, test/accuracy=0.623800, test/loss=1.709366, test/num_examples=10000, total_duration=79214.680680, train/accuracy=0.825059, train/loss=0.769559, validation/accuracy=0.742520, validation/loss=1.116203, validation/num_examples=50000
I0131 11:49:29.465363 139741768959744 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.471478462219238, loss=2.821300983428955
I0131 11:50:14.647897 139741760567040 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.204908847808838, loss=3.731616973876953
I0131 11:51:01.075982 139741768959744 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.456357002258301, loss=2.3424148559570312
I0131 11:51:47.149884 139741760567040 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.088897228240967, loss=3.0116748809814453
I0131 11:52:33.096184 139741768959744 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.286914825439453, loss=3.114971399307251
I0131 11:53:19.237439 139741760567040 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.855698108673096, loss=3.5356950759887695
I0131 11:54:05.131732 139741768959744 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.32844877243042, loss=3.873276472091675
I0131 11:54:51.144451 139741760567040 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.458340167999268, loss=2.3506698608398438
I0131 11:55:37.013390 139741768959744 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.071206092834473, loss=3.1405181884765625
I0131 11:56:01.207665 139936116377408 spec.py:321] Evaluating on the training split.
I0131 11:56:13.275412 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 11:56:39.942636 139936116377408 spec.py:349] Evaluating on the test split.
I0131 11:56:41.543397 139936116377408 submission_runner.py:408] Time since start: 79675.36s, 	Step: 161054, 	{'train/accuracy': 0.8220117092132568, 'train/loss': 0.7986540794372559, 'validation/accuracy': 0.7463399767875671, 'validation/loss': 1.1236673593521118, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.7323718070983887, 'test/num_examples': 10000, 'score': 73151.4669148922, 'total_duration': 79675.36319732666, 'accumulated_submission_time': 73151.4669148922, 'accumulated_eval_time': 6508.185884714127, 'accumulated_logging_time': 7.314181804656982}
I0131 11:56:41.586853 139741760567040 logging_writer.py:48] [161054] accumulated_eval_time=6508.185885, accumulated_logging_time=7.314182, accumulated_submission_time=73151.466915, global_step=161054, preemption_count=0, score=73151.466915, test/accuracy=0.620900, test/loss=1.732372, test/num_examples=10000, total_duration=79675.363197, train/accuracy=0.822012, train/loss=0.798654, validation/accuracy=0.746340, validation/loss=1.123667, validation/num_examples=50000
I0131 11:57:00.234821 139741768959744 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.529212951660156, loss=2.448882579803467
I0131 11:57:43.704180 139741760567040 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.550463676452637, loss=2.329991579055786
I0131 11:58:29.575924 139741768959744 logging_writer.py:48] [161300] global_step=161300, grad_norm=5.356501579284668, loss=2.475174903869629
I0131 11:59:15.855396 139741760567040 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.055300235748291, loss=3.6812195777893066
I0131 12:00:01.894906 139741768959744 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.548767566680908, loss=2.304859161376953
I0131 12:00:47.988677 139741760567040 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.5111188888549805, loss=2.38553786277771
I0131 12:01:34.136766 139741768959744 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.346784591674805, loss=2.659179210662842
I0131 12:02:20.175707 139741760567040 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.5380682945251465, loss=3.8066930770874023
I0131 12:03:06.244034 139741768959744 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.5737762451171875, loss=2.377840995788574
I0131 12:03:41.797545 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:03:53.738420 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:04:19.052524 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:04:20.658164 139936116377408 submission_runner.py:408] Time since start: 80134.48s, 	Step: 161979, 	{'train/accuracy': 0.82289057970047, 'train/loss': 0.774276852607727, 'validation/accuracy': 0.7457799911499023, 'validation/loss': 1.096876859664917, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.6988554000854492, 'test/num_examples': 10000, 'score': 73571.6182346344, 'total_duration': 80134.4779574871, 'accumulated_submission_time': 73571.6182346344, 'accumulated_eval_time': 6547.046494960785, 'accumulated_logging_time': 7.370314836502075}
I0131 12:04:20.709859 139741760567040 logging_writer.py:48] [161979] accumulated_eval_time=6547.046495, accumulated_logging_time=7.370315, accumulated_submission_time=73571.618235, global_step=161979, preemption_count=0, score=73571.618235, test/accuracy=0.624000, test/loss=1.698855, test/num_examples=10000, total_duration=80134.477957, train/accuracy=0.822891, train/loss=0.774277, validation/accuracy=0.745780, validation/loss=1.096877, validation/num_examples=50000
I0131 12:04:29.436800 139741768959744 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.599302291870117, loss=3.128159761428833
I0131 12:05:11.645364 139741760567040 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.871201515197754, loss=2.305326223373413
I0131 12:05:57.681678 139741768959744 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.683084487915039, loss=2.3780336380004883
I0131 12:06:43.937861 139741760567040 logging_writer.py:48] [162300] global_step=162300, grad_norm=5.260918617248535, loss=4.165488243103027
I0131 12:07:29.852847 139741768959744 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.712433815002441, loss=2.4345219135284424
I0131 12:08:16.173613 139741760567040 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.55731725692749, loss=2.2824673652648926
I0131 12:09:02.280123 139741768959744 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.827246189117432, loss=2.5693156719207764
I0131 12:09:48.302270 139741760567040 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.348931789398193, loss=2.8025150299072266
I0131 12:10:34.346750 139741768959744 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.549857139587402, loss=4.402683734893799
I0131 12:11:20.558683 139741760567040 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.6437249183654785, loss=2.351776599884033
I0131 12:11:20.809909 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:11:32.730274 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:11:59.357640 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:12:00.960302 139936116377408 submission_runner.py:408] Time since start: 80594.78s, 	Step: 162902, 	{'train/accuracy': 0.82958984375, 'train/loss': 0.768218994140625, 'validation/accuracy': 0.7471399903297424, 'validation/loss': 1.1131218671798706, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.7115119695663452, 'test/num_examples': 10000, 'score': 73991.6619336605, 'total_duration': 80594.78010439873, 'accumulated_submission_time': 73991.6619336605, 'accumulated_eval_time': 6587.196877479553, 'accumulated_logging_time': 7.43206524848938}
I0131 12:12:00.999764 139741768959744 logging_writer.py:48] [162902] accumulated_eval_time=6587.196877, accumulated_logging_time=7.432065, accumulated_submission_time=73991.661934, global_step=162902, preemption_count=0, score=73991.661934, test/accuracy=0.627800, test/loss=1.711512, test/num_examples=10000, total_duration=80594.780104, train/accuracy=0.829590, train/loss=0.768219, validation/accuracy=0.747140, validation/loss=1.113122, validation/num_examples=50000
I0131 12:12:41.170123 139741760567040 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.903655529022217, loss=2.404125690460205
I0131 12:13:26.977157 139741768959744 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.440646171569824, loss=2.3177340030670166
I0131 12:14:13.295220 139741760567040 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.386372089385986, loss=2.5908939838409424
I0131 12:14:58.909250 139741768959744 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.346588611602783, loss=3.208228826522827
I0131 12:15:45.035036 139741760567040 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.2427778244018555, loss=2.665696144104004
I0131 12:16:30.889287 139741768959744 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.583412170410156, loss=2.238755702972412
I0131 12:17:17.192955 139741760567040 logging_writer.py:48] [163600] global_step=163600, grad_norm=5.4845991134643555, loss=4.228647708892822
I0131 12:18:03.158393 139741768959744 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.80646276473999, loss=2.3847439289093018
I0131 12:18:48.873531 139741760567040 logging_writer.py:48] [163800] global_step=163800, grad_norm=5.589351177215576, loss=2.3226070404052734
I0131 12:19:01.462159 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:19:13.569185 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:19:38.323226 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:19:39.928182 139936116377408 submission_runner.py:408] Time since start: 81053.75s, 	Step: 163829, 	{'train/accuracy': 0.8261523246765137, 'train/loss': 0.7748673558235168, 'validation/accuracy': 0.747219979763031, 'validation/loss': 1.0991054773330688, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.695138692855835, 'test/num_examples': 10000, 'score': 74412.06664991379, 'total_duration': 81053.74798631668, 'accumulated_submission_time': 74412.06664991379, 'accumulated_eval_time': 6625.662905454636, 'accumulated_logging_time': 7.482219219207764}
I0131 12:19:39.968691 139741768959744 logging_writer.py:48] [163829] accumulated_eval_time=6625.662905, accumulated_logging_time=7.482219, accumulated_submission_time=74412.066650, global_step=163829, preemption_count=0, score=74412.066650, test/accuracy=0.624500, test/loss=1.695139, test/num_examples=10000, total_duration=81053.747986, train/accuracy=0.826152, train/loss=0.774867, validation/accuracy=0.747220, validation/loss=1.099105, validation/num_examples=50000
I0131 12:20:08.808864 139741760567040 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.681860446929932, loss=2.3784236907958984
I0131 12:20:53.707029 139741768959744 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.504324436187744, loss=2.380831480026245
I0131 12:21:39.984814 139741760567040 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.415276527404785, loss=3.3470852375030518
I0131 12:22:26.045726 139741768959744 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.469690799713135, loss=2.438966989517212
I0131 12:23:12.147186 139741760567040 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.691882133483887, loss=2.323707103729248
I0131 12:23:58.021111 139741768959744 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.825070858001709, loss=3.4843063354492188
I0131 12:24:44.051250 139741760567040 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.653610706329346, loss=2.3735811710357666
I0131 12:25:29.991903 139741768959744 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.960951328277588, loss=2.3120369911193848
I0131 12:26:15.811447 139741760567040 logging_writer.py:48] [164700] global_step=164700, grad_norm=5.207300662994385, loss=2.349033832550049
I0131 12:26:39.958352 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:26:51.959285 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:27:17.901900 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:27:19.511358 139936116377408 submission_runner.py:408] Time since start: 81513.33s, 	Step: 164754, 	{'train/accuracy': 0.8268163800239563, 'train/loss': 0.7791228890419006, 'validation/accuracy': 0.7498199939727783, 'validation/loss': 1.0978347063064575, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.695865511894226, 'test/num_examples': 10000, 'score': 74831.999625206, 'total_duration': 81513.33116149902, 'accumulated_submission_time': 74831.999625206, 'accumulated_eval_time': 6665.215897798538, 'accumulated_logging_time': 7.531557321548462}
I0131 12:27:19.550445 139741768959744 logging_writer.py:48] [164754] accumulated_eval_time=6665.215898, accumulated_logging_time=7.531557, accumulated_submission_time=74831.999625, global_step=164754, preemption_count=0, score=74831.999625, test/accuracy=0.629200, test/loss=1.695866, test/num_examples=10000, total_duration=81513.331161, train/accuracy=0.826816, train/loss=0.779123, validation/accuracy=0.749820, validation/loss=1.097835, validation/num_examples=50000
I0131 12:27:38.204119 139741760567040 logging_writer.py:48] [164800] global_step=164800, grad_norm=5.048680305480957, loss=2.3232157230377197
I0131 12:28:22.016036 139741768959744 logging_writer.py:48] [164900] global_step=164900, grad_norm=5.05192232131958, loss=2.4317965507507324
I0131 12:29:07.987787 139741760567040 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.533421993255615, loss=2.376443862915039
I0131 12:29:54.024415 139741768959744 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.951350212097168, loss=2.398609161376953
I0131 12:30:40.210454 139741760567040 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.800879955291748, loss=2.374451160430908
I0131 12:31:26.309149 139741768959744 logging_writer.py:48] [165300] global_step=165300, grad_norm=6.426562786102295, loss=4.302764892578125
I0131 12:32:12.868000 139741760567040 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.938412666320801, loss=2.3129591941833496
I0131 12:32:59.019709 139741768959744 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.910811424255371, loss=2.682720184326172
I0131 12:33:45.376078 139741760567040 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.637269496917725, loss=2.399796724319458
I0131 12:34:19.761821 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:34:31.744048 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:34:55.651693 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:34:57.257839 139936116377408 submission_runner.py:408] Time since start: 81971.08s, 	Step: 165676, 	{'train/accuracy': 0.8290429711341858, 'train/loss': 0.7599804997444153, 'validation/accuracy': 0.749239981174469, 'validation/loss': 1.0925363302230835, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.6812580823898315, 'test/num_examples': 10000, 'score': 75252.15470719337, 'total_duration': 81971.07764077187, 'accumulated_submission_time': 75252.15470719337, 'accumulated_eval_time': 6702.711914539337, 'accumulated_logging_time': 7.5804502964019775}
I0131 12:34:57.302323 139741768959744 logging_writer.py:48] [165676] accumulated_eval_time=6702.711915, accumulated_logging_time=7.580450, accumulated_submission_time=75252.154707, global_step=165676, preemption_count=0, score=75252.154707, test/accuracy=0.630400, test/loss=1.681258, test/num_examples=10000, total_duration=81971.077641, train/accuracy=0.829043, train/loss=0.759980, validation/accuracy=0.749240, validation/loss=1.092536, validation/num_examples=50000
I0131 12:35:07.214715 139741760567040 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.046837329864502, loss=2.2463581562042236
I0131 12:35:49.305012 139741768959744 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.3499603271484375, loss=4.081335544586182
I0131 12:36:35.555731 139741760567040 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.67651891708374, loss=4.3200554847717285
I0131 12:37:21.896809 139741768959744 logging_writer.py:48] [166000] global_step=166000, grad_norm=5.091495513916016, loss=2.320167064666748
I0131 12:38:07.892249 139741760567040 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.609360694885254, loss=2.4294354915618896
I0131 12:38:53.997798 139741768959744 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.77024507522583, loss=2.263587236404419
I0131 12:39:39.754058 139741760567040 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.6214118003845215, loss=2.3838984966278076
I0131 12:40:25.933304 139741768959744 logging_writer.py:48] [166400] global_step=166400, grad_norm=5.478831768035889, loss=3.968888759613037
I0131 12:41:11.834046 139741760567040 logging_writer.py:48] [166500] global_step=166500, grad_norm=5.040825843811035, loss=2.4204652309417725
I0131 12:41:57.260764 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:42:09.193012 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:42:37.103340 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:42:38.707753 139936116377408 submission_runner.py:408] Time since start: 82432.53s, 	Step: 166600, 	{'train/accuracy': 0.8282421827316284, 'train/loss': 0.7497026920318604, 'validation/accuracy': 0.7534399628639221, 'validation/loss': 1.0718141794204712, 'validation/num_examples': 50000, 'test/accuracy': 0.6350000500679016, 'test/loss': 1.662460446357727, 'test/num_examples': 10000, 'score': 75672.05558896065, 'total_duration': 82432.52755188942, 'accumulated_submission_time': 75672.05558896065, 'accumulated_eval_time': 6744.158909320831, 'accumulated_logging_time': 7.63471245765686}
I0131 12:42:38.749471 139741768959744 logging_writer.py:48] [166600] accumulated_eval_time=6744.158909, accumulated_logging_time=7.634712, accumulated_submission_time=75672.055589, global_step=166600, preemption_count=0, score=75672.055589, test/accuracy=0.635000, test/loss=1.662460, test/num_examples=10000, total_duration=82432.527552, train/accuracy=0.828242, train/loss=0.749703, validation/accuracy=0.753440, validation/loss=1.071814, validation/num_examples=50000
I0131 12:42:39.146713 139741760567040 logging_writer.py:48] [166600] global_step=166600, grad_norm=6.464906215667725, loss=4.324336051940918
I0131 12:43:20.291651 139741768959744 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.317203521728516, loss=2.2194085121154785
I0131 12:44:06.178390 139741760567040 logging_writer.py:48] [166800] global_step=166800, grad_norm=5.215380668640137, loss=2.875434160232544
I0131 12:44:52.228009 139741768959744 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.439581394195557, loss=2.9570436477661133
I0131 12:45:38.193814 139741760567040 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.985898971557617, loss=2.3264334201812744
I0131 12:46:24.253309 139741768959744 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.342129230499268, loss=2.693105697631836
I0131 12:47:10.250749 139741760567040 logging_writer.py:48] [167200] global_step=167200, grad_norm=5.288568496704102, loss=2.3749942779541016
I0131 12:47:56.109233 139741768959744 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.227532863616943, loss=2.899791717529297
I0131 12:48:42.413877 139741760567040 logging_writer.py:48] [167400] global_step=167400, grad_norm=6.328924655914307, loss=3.1181414127349854
I0131 12:49:28.492428 139741768959744 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.957146167755127, loss=2.2388741970062256
I0131 12:49:39.070918 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:49:50.886785 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:50:17.017363 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:50:18.623616 139936116377408 submission_runner.py:408] Time since start: 82892.44s, 	Step: 167525, 	{'train/accuracy': 0.8338280916213989, 'train/loss': 0.7270570993423462, 'validation/accuracy': 0.7521399855613708, 'validation/loss': 1.0613549947738647, 'validation/num_examples': 50000, 'test/accuracy': 0.6344000101089478, 'test/loss': 1.6501274108886719, 'test/num_examples': 10000, 'score': 76092.31971812248, 'total_duration': 82892.44341945648, 'accumulated_submission_time': 76092.31971812248, 'accumulated_eval_time': 6783.7116050720215, 'accumulated_logging_time': 7.687152862548828}
I0131 12:50:18.670147 139741760567040 logging_writer.py:48] [167525] accumulated_eval_time=6783.711605, accumulated_logging_time=7.687153, accumulated_submission_time=76092.319718, global_step=167525, preemption_count=0, score=76092.319718, test/accuracy=0.634400, test/loss=1.650127, test/num_examples=10000, total_duration=82892.443419, train/accuracy=0.833828, train/loss=0.727057, validation/accuracy=0.752140, validation/loss=1.061355, validation/num_examples=50000
I0131 12:50:48.858309 139741768959744 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.381686210632324, loss=2.396760940551758
I0131 12:51:34.517293 139741760567040 logging_writer.py:48] [167700] global_step=167700, grad_norm=6.207218170166016, loss=4.260158538818359
I0131 12:52:20.708629 139741768959744 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.84618616104126, loss=2.2780284881591797
I0131 12:53:07.284604 139741760567040 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.806460857391357, loss=2.4463443756103516
I0131 12:53:53.345592 139741768959744 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.745864391326904, loss=2.2724621295928955
I0131 12:54:39.541641 139741760567040 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.331946849822998, loss=2.6108362674713135
I0131 12:55:25.495523 139741768959744 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.675297737121582, loss=2.405988931655884
I0131 12:56:11.808396 139741760567040 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.839433193206787, loss=2.237276554107666
I0131 12:56:57.779660 139741768959744 logging_writer.py:48] [168400] global_step=168400, grad_norm=5.377491474151611, loss=4.102245807647705
I0131 12:57:18.759382 139936116377408 spec.py:321] Evaluating on the training split.
I0131 12:57:30.749499 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 12:57:54.739491 139936116377408 spec.py:349] Evaluating on the test split.
I0131 12:57:56.345444 139936116377408 submission_runner.py:408] Time since start: 83350.17s, 	Step: 168447, 	{'train/accuracy': 0.8368163704872131, 'train/loss': 0.721832275390625, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0656956434249878, 'validation/num_examples': 50000, 'test/accuracy': 0.6376000046730042, 'test/loss': 1.6524871587753296, 'test/num_examples': 10000, 'score': 76512.35372972488, 'total_duration': 83350.1652495861, 'accumulated_submission_time': 76512.35372972488, 'accumulated_eval_time': 6821.297668457031, 'accumulated_logging_time': 7.742358684539795}
I0131 12:57:56.389359 139741760567040 logging_writer.py:48] [168447] accumulated_eval_time=6821.297668, accumulated_logging_time=7.742359, accumulated_submission_time=76512.353730, global_step=168447, preemption_count=0, score=76512.353730, test/accuracy=0.637600, test/loss=1.652487, test/num_examples=10000, total_duration=83350.165250, train/accuracy=0.836816, train/loss=0.721832, validation/accuracy=0.755840, validation/loss=1.065696, validation/num_examples=50000
I0131 12:58:17.831115 139741768959744 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.953656196594238, loss=2.408423900604248
I0131 12:59:01.910809 139741760567040 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.9410881996154785, loss=2.4331727027893066
I0131 12:59:48.184070 139741768959744 logging_writer.py:48] [168700] global_step=168700, grad_norm=5.225963115692139, loss=3.620210886001587
I0131 13:00:34.727287 139741760567040 logging_writer.py:48] [168800] global_step=168800, grad_norm=6.1085686683654785, loss=4.176998615264893
I0131 13:01:20.774402 139741768959744 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.962438106536865, loss=2.23781681060791
I0131 13:02:07.181067 139741760567040 logging_writer.py:48] [169000] global_step=169000, grad_norm=5.735518932342529, loss=4.049716949462891
I0131 13:02:53.191917 139741768959744 logging_writer.py:48] [169100] global_step=169100, grad_norm=6.163852214813232, loss=4.127035617828369
I0131 13:03:39.501314 139741760567040 logging_writer.py:48] [169200] global_step=169200, grad_norm=5.197423934936523, loss=2.265141725540161
I0131 13:04:25.717945 139741768959744 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.044521808624268, loss=2.3414196968078613
I0131 13:04:56.558052 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:05:08.678656 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:05:36.103554 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:05:37.713078 139936116377408 submission_runner.py:408] Time since start: 83811.53s, 	Step: 169369, 	{'train/accuracy': 0.8389843702316284, 'train/loss': 0.7196028828620911, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0582653284072876, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.6552094221115112, 'test/num_examples': 10000, 'score': 76932.46559858322, 'total_duration': 83811.53285884857, 'accumulated_submission_time': 76932.46559858322, 'accumulated_eval_time': 6862.45266699791, 'accumulated_logging_time': 7.796396732330322}
I0131 13:05:37.761953 139741760567040 logging_writer.py:48] [169369] accumulated_eval_time=6862.452667, accumulated_logging_time=7.796397, accumulated_submission_time=76932.465599, global_step=169369, preemption_count=0, score=76932.465599, test/accuracy=0.631800, test/loss=1.655209, test/num_examples=10000, total_duration=83811.532859, train/accuracy=0.838984, train/loss=0.719603, validation/accuracy=0.757040, validation/loss=1.058265, validation/num_examples=50000
I0131 13:05:50.450383 139741768959744 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.992339134216309, loss=2.277115821838379
I0131 13:06:33.446049 139741760567040 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.00510835647583, loss=3.307861328125
I0131 13:07:19.446944 139741768959744 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.0267181396484375, loss=2.225163698196411
I0131 13:08:05.898136 139741760567040 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.831604957580566, loss=2.2440850734710693
I0131 13:08:51.864242 139741768959744 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.8781890869140625, loss=2.2851476669311523
I0131 13:09:38.156163 139741760567040 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.792594909667969, loss=2.9387025833129883
I0131 13:10:24.200125 139741768959744 logging_writer.py:48] [170000] global_step=170000, grad_norm=5.029643535614014, loss=3.2587311267852783
I0131 13:11:10.142639 139741760567040 logging_writer.py:48] [170100] global_step=170100, grad_norm=5.380467414855957, loss=3.229461193084717
I0131 13:11:56.533718 139741768959744 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.938982009887695, loss=2.172924280166626
I0131 13:12:38.029011 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:12:49.907912 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:13:17.556008 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:13:19.164231 139936116377408 submission_runner.py:408] Time since start: 84272.98s, 	Step: 170291, 	{'train/accuracy': 0.8350781202316284, 'train/loss': 0.7398861646652222, 'validation/accuracy': 0.7601999640464783, 'validation/loss': 1.0654001235961914, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.660707950592041, 'test/num_examples': 10000, 'score': 77352.67522978783, 'total_duration': 84272.98402452469, 'accumulated_submission_time': 77352.67522978783, 'accumulated_eval_time': 6903.587861776352, 'accumulated_logging_time': 7.856116533279419}
I0131 13:13:19.210421 139741760567040 logging_writer.py:48] [170291] accumulated_eval_time=6903.587862, accumulated_logging_time=7.856117, accumulated_submission_time=77352.675230, global_step=170291, preemption_count=0, score=77352.675230, test/accuracy=0.637400, test/loss=1.660708, test/num_examples=10000, total_duration=84272.984025, train/accuracy=0.835078, train/loss=0.739886, validation/accuracy=0.760200, validation/loss=1.065400, validation/num_examples=50000
I0131 13:13:23.186640 139741768959744 logging_writer.py:48] [170300] global_step=170300, grad_norm=5.37992525100708, loss=2.4137630462646484
I0131 13:14:04.839914 139741760567040 logging_writer.py:48] [170400] global_step=170400, grad_norm=5.168771266937256, loss=2.249723196029663
I0131 13:14:50.895389 139741768959744 logging_writer.py:48] [170500] global_step=170500, grad_norm=5.115676403045654, loss=2.413041591644287
I0131 13:15:37.145112 139741760567040 logging_writer.py:48] [170600] global_step=170600, grad_norm=5.4443230628967285, loss=2.4511547088623047
I0131 13:16:06.894114 139741768959744 logging_writer.py:48] [170666] global_step=170666, preemption_count=0, score=77520.274422
I0131 13:16:07.632377 139936116377408 checkpoints.py:490] Saving checkpoint at step: 170666
I0131 13:16:08.993200 139936116377408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_1/checkpoint_170666
I0131 13:16:09.019243 139936116377408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_1/checkpoint_170666.
I0131 13:16:10.095204 139936116377408 submission_runner.py:583] Tuning trial 1/5
I0131 13:16:10.095437 139936116377408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0131 13:16:10.118161 139936116377408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010351561941206455, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 42.45515441894531, 'total_duration': 82.5178644657135, 'accumulated_submission_time': 42.45515441894531, 'accumulated_eval_time': 40.06261396408081, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (898, {'train/accuracy': 0.013769530691206455, 'train/loss': 6.404803276062012, 'validation/accuracy': 0.013580000028014183, 'validation/loss': 6.413737773895264, 'validation/num_examples': 50000, 'test/accuracy': 0.01080000028014183, 'test/loss': 6.456206321716309, 'test/num_examples': 10000, 'score': 462.6178922653198, 'total_duration': 524.6362130641937, 'accumulated_submission_time': 462.6178922653198, 'accumulated_eval_time': 61.94122099876404, 'accumulated_logging_time': 0.02863001823425293, 'global_step': 898, 'preemption_count': 0}), (1842, {'train/accuracy': 0.04136718809604645, 'train/loss': 5.8381195068359375, 'validation/accuracy': 0.03776000067591667, 'validation/loss': 5.869866847991943, 'validation/num_examples': 50000, 'test/accuracy': 0.03060000203549862, 'test/loss': 5.982430934906006, 'test/num_examples': 10000, 'score': 882.9875965118408, 'total_duration': 966.72283244133, 'accumulated_submission_time': 882.9875965118408, 'accumulated_eval_time': 83.57465052604675, 'accumulated_logging_time': 0.06095552444458008, 'global_step': 1842, 'preemption_count': 0}), (2788, {'train/accuracy': 0.07021484524011612, 'train/loss': 5.4130401611328125, 'validation/accuracy': 0.06375999748706818, 'validation/loss': 5.468223571777344, 'validation/num_examples': 50000, 'test/accuracy': 0.05180000141263008, 'test/loss': 5.644834041595459, 'test/num_examples': 10000, 'score': 1303.2035462856293, 'total_duration': 1408.8744959831238, 'accumulated_submission_time': 1303.2035462856293, 'accumulated_eval_time': 105.43107485771179, 'accumulated_logging_time': 0.08992552757263184, 'global_step': 2788, 'preemption_count': 0}), (3733, {'train/accuracy': 0.10271484404802322, 'train/loss': 5.1039581298828125, 'validation/accuracy': 0.09443999826908112, 'validation/loss': 5.144573211669922, 'validation/num_examples': 50000, 'test/accuracy': 0.07150000333786011, 'test/loss': 5.371340274810791, 'test/num_examples': 10000, 'score': 1723.3643689155579, 'total_duration': 1850.8554532527924, 'accumulated_submission_time': 1723.3643689155579, 'accumulated_eval_time': 127.17176485061646, 'accumulated_logging_time': 0.11867594718933105, 'global_step': 3733, 'preemption_count': 0}), (4673, {'train/accuracy': 0.13539062440395355, 'train/loss': 4.738257884979248, 'validation/accuracy': 0.12799999117851257, 'validation/loss': 4.791199207305908, 'validation/num_examples': 50000, 'test/accuracy': 0.09760000556707382, 'test/loss': 5.071451663970947, 'test/num_examples': 10000, 'score': 2143.616794347763, 'total_duration': 2292.988788843155, 'accumulated_submission_time': 2143.616794347763, 'accumulated_eval_time': 148.97430968284607, 'accumulated_logging_time': 0.14718222618103027, 'global_step': 4673, 'preemption_count': 0}), (5609, {'train/accuracy': 0.1873828023672104, 'train/loss': 4.345278739929199, 'validation/accuracy': 0.1711599975824356, 'validation/loss': 4.43392276763916, 'validation/num_examples': 50000, 'test/accuracy': 0.12640000879764557, 'test/loss': 4.761846542358398, 'test/num_examples': 10000, 'score': 2563.884337425232, 'total_duration': 2735.237434864044, 'accumulated_submission_time': 2563.884337425232, 'accumulated_eval_time': 170.8790421485901, 'accumulated_logging_time': 0.1756596565246582, 'global_step': 5609, 'preemption_count': 0}), (6546, {'train/accuracy': 0.23087890446186066, 'train/loss': 3.9954845905303955, 'validation/accuracy': 0.21111999452114105, 'validation/loss': 4.086441993713379, 'validation/num_examples': 50000, 'test/accuracy': 0.15630000829696655, 'test/loss': 4.464807987213135, 'test/num_examples': 10000, 'score': 2984.2211933135986, 'total_duration': 3180.6647255420685, 'accumulated_submission_time': 2984.2211933135986, 'accumulated_eval_time': 195.88995552062988, 'accumulated_logging_time': 0.2058713436126709, 'global_step': 6546, 'preemption_count': 0}), (7486, {'train/accuracy': 0.2678515613079071, 'train/loss': 3.742283821105957, 'validation/accuracy': 0.24859999120235443, 'validation/loss': 3.837204694747925, 'validation/num_examples': 50000, 'test/accuracy': 0.18870000541210175, 'test/loss': 4.262177467346191, 'test/num_examples': 10000, 'score': 3404.525137901306, 'total_duration': 3627.2830555438995, 'accumulated_submission_time': 3404.525137901306, 'accumulated_eval_time': 222.11826848983765, 'accumulated_logging_time': 0.23824262619018555, 'global_step': 7486, 'preemption_count': 0}), (8427, {'train/accuracy': 0.3073437511920929, 'train/loss': 3.5018374919891357, 'validation/accuracy': 0.28248000144958496, 'validation/loss': 3.621398448944092, 'validation/num_examples': 50000, 'test/accuracy': 0.2143000066280365, 'test/loss': 4.080821514129639, 'test/num_examples': 10000, 'score': 3824.85044836998, 'total_duration': 4078.8026208877563, 'accumulated_submission_time': 3824.85044836998, 'accumulated_eval_time': 253.22631406784058, 'accumulated_logging_time': 0.2754535675048828, 'global_step': 8427, 'preemption_count': 0}), (9367, {'train/accuracy': 0.353339821100235, 'train/loss': 3.1838603019714355, 'validation/accuracy': 0.315420001745224, 'validation/loss': 3.375602960586548, 'validation/num_examples': 50000, 'test/accuracy': 0.242000013589859, 'test/loss': 3.8549141883850098, 'test/num_examples': 10000, 'score': 4245.004207611084, 'total_duration': 4524.752751588821, 'accumulated_submission_time': 4245.004207611084, 'accumulated_eval_time': 278.9440326690674, 'accumulated_logging_time': 0.30495738983154297, 'global_step': 9367, 'preemption_count': 0}), (10306, {'train/accuracy': 0.37101560831069946, 'train/loss': 3.0405492782592773, 'validation/accuracy': 0.34401997923851013, 'validation/loss': 3.174036741256714, 'validation/num_examples': 50000, 'test/accuracy': 0.26600000262260437, 'test/loss': 3.685535430908203, 'test/num_examples': 10000, 'score': 4665.314122676849, 'total_duration': 4973.823203802109, 'accumulated_submission_time': 4665.314122676849, 'accumulated_eval_time': 307.616507768631, 'accumulated_logging_time': 0.3441452980041504, 'global_step': 10306, 'preemption_count': 0}), (11243, {'train/accuracy': 0.3888476490974426, 'train/loss': 2.948613166809082, 'validation/accuracy': 0.36027997732162476, 'validation/loss': 3.093329668045044, 'validation/num_examples': 50000, 'test/accuracy': 0.27720001339912415, 'test/loss': 3.618096351623535, 'test/num_examples': 10000, 'score': 5085.516779184341, 'total_duration': 5421.768236398697, 'accumulated_submission_time': 5085.516779184341, 'accumulated_eval_time': 335.2799074649811, 'accumulated_logging_time': 0.3747379779815674, 'global_step': 11243, 'preemption_count': 0}), (12180, {'train/accuracy': 0.4136328101158142, 'train/loss': 2.8385469913482666, 'validation/accuracy': 0.37389999628067017, 'validation/loss': 3.021648406982422, 'validation/num_examples': 50000, 'test/accuracy': 0.28790000081062317, 'test/loss': 3.568051338195801, 'test/num_examples': 10000, 'score': 5505.593339443207, 'total_duration': 5873.66360616684, 'accumulated_submission_time': 5505.593339443207, 'accumulated_eval_time': 367.0149767398834, 'accumulated_logging_time': 0.4094424247741699, 'global_step': 12180, 'preemption_count': 0}), (13113, {'train/accuracy': 0.4274218678474426, 'train/loss': 2.7390637397766113, 'validation/accuracy': 0.39733999967575073, 'validation/loss': 2.8888654708862305, 'validation/num_examples': 50000, 'test/accuracy': 0.3093000054359436, 'test/loss': 3.4295434951782227, 'test/num_examples': 10000, 'score': 5925.677830457687, 'total_duration': 6327.169443368912, 'accumulated_submission_time': 5925.677830457687, 'accumulated_eval_time': 400.35482573509216, 'accumulated_logging_time': 0.441788911819458, 'global_step': 13113, 'preemption_count': 0}), (14049, {'train/accuracy': 0.44376951456069946, 'train/loss': 2.6229381561279297, 'validation/accuracy': 0.41033998131752014, 'validation/loss': 2.784433364868164, 'validation/num_examples': 50000, 'test/accuracy': 0.3188000023365021, 'test/loss': 3.3676393032073975, 'test/num_examples': 10000, 'score': 6345.869318246841, 'total_duration': 6784.241494894028, 'accumulated_submission_time': 6345.869318246841, 'accumulated_eval_time': 437.1570258140564, 'accumulated_logging_time': 0.47129106521606445, 'global_step': 14049, 'preemption_count': 0}), (14985, {'train/accuracy': 0.4652148187160492, 'train/loss': 2.516399383544922, 'validation/accuracy': 0.4221400022506714, 'validation/loss': 2.725985288619995, 'validation/num_examples': 50000, 'test/accuracy': 0.32660001516342163, 'test/loss': 3.2856686115264893, 'test/num_examples': 10000, 'score': 6765.7920479774475, 'total_duration': 7240.653775691986, 'accumulated_submission_time': 6765.7920479774475, 'accumulated_eval_time': 473.5686767101288, 'accumulated_logging_time': 0.5005159378051758, 'global_step': 14985, 'preemption_count': 0}), (15915, {'train/accuracy': 0.4888085722923279, 'train/loss': 2.4176840782165527, 'validation/accuracy': 0.43361997604370117, 'validation/loss': 2.675327777862549, 'validation/num_examples': 50000, 'test/accuracy': 0.33230000734329224, 'test/loss': 3.2542564868927, 'test/num_examples': 10000, 'score': 7186.127139806747, 'total_duration': 7693.80969619751, 'accumulated_submission_time': 7186.127139806747, 'accumulated_eval_time': 506.31141448020935, 'accumulated_logging_time': 0.5309438705444336, 'global_step': 15915, 'preemption_count': 0}), (16839, {'train/accuracy': 0.4630078077316284, 'train/loss': 2.538491725921631, 'validation/accuracy': 0.43459999561309814, 'validation/loss': 2.679887056350708, 'validation/num_examples': 50000, 'test/accuracy': 0.33740001916885376, 'test/loss': 3.2630906105041504, 'test/num_examples': 10000, 'score': 7606.272791385651, 'total_duration': 8151.8433039188385, 'accumulated_submission_time': 7606.272791385651, 'accumulated_eval_time': 544.1211948394775, 'accumulated_logging_time': 0.5615954399108887, 'global_step': 16839, 'preemption_count': 0}), (17769, {'train/accuracy': 0.4804101586341858, 'train/loss': 2.426698684692383, 'validation/accuracy': 0.44652000069618225, 'validation/loss': 2.5907909870147705, 'validation/num_examples': 50000, 'test/accuracy': 0.34170001745224, 'test/loss': 3.178121328353882, 'test/num_examples': 10000, 'score': 8026.660900354385, 'total_duration': 8607.584362506866, 'accumulated_submission_time': 8026.660900354385, 'accumulated_eval_time': 579.3966374397278, 'accumulated_logging_time': 0.5898916721343994, 'global_step': 17769, 'preemption_count': 0}), (18701, {'train/accuracy': 0.5080273151397705, 'train/loss': 2.2943875789642334, 'validation/accuracy': 0.4573400020599365, 'validation/loss': 2.531662702560425, 'validation/num_examples': 50000, 'test/accuracy': 0.3521000146865845, 'test/loss': 3.1127684116363525, 'test/num_examples': 10000, 'score': 8446.603140592575, 'total_duration': 9062.087691783905, 'accumulated_submission_time': 8446.603140592575, 'accumulated_eval_time': 613.8807055950165, 'accumulated_logging_time': 0.6182742118835449, 'global_step': 18701, 'preemption_count': 0}), (19630, {'train/accuracy': 0.49162107706069946, 'train/loss': 2.3779008388519287, 'validation/accuracy': 0.4596799910068512, 'validation/loss': 2.5362136363983154, 'validation/num_examples': 50000, 'test/accuracy': 0.36320000886917114, 'test/loss': 3.1231794357299805, 'test/num_examples': 10000, 'score': 8866.9126598835, 'total_duration': 9519.569938898087, 'accumulated_submission_time': 8866.9126598835, 'accumulated_eval_time': 650.9721372127533, 'accumulated_logging_time': 0.6507935523986816, 'global_step': 19630, 'preemption_count': 0}), (20558, {'train/accuracy': 0.5131444931030273, 'train/loss': 2.2290053367614746, 'validation/accuracy': 0.4773799777030945, 'validation/loss': 2.3989546298980713, 'validation/num_examples': 50000, 'test/accuracy': 0.36740002036094666, 'test/loss': 2.994854211807251, 'test/num_examples': 10000, 'score': 9286.968410253525, 'total_duration': 9975.836344718933, 'accumulated_submission_time': 9286.968410253525, 'accumulated_eval_time': 687.1052870750427, 'accumulated_logging_time': 0.6784398555755615, 'global_step': 20558, 'preemption_count': 0}), (21482, {'train/accuracy': 0.5301367044448853, 'train/loss': 2.1761655807495117, 'validation/accuracy': 0.48363998532295227, 'validation/loss': 2.3937957286834717, 'validation/num_examples': 50000, 'test/accuracy': 0.37720000743865967, 'test/loss': 3.007101535797119, 'test/num_examples': 10000, 'score': 9707.312128067017, 'total_duration': 10429.00049996376, 'accumulated_submission_time': 9707.312128067017, 'accumulated_eval_time': 719.8414082527161, 'accumulated_logging_time': 0.7146031856536865, 'global_step': 21482, 'preemption_count': 0}), (22340, {'train/accuracy': 0.5262304544448853, 'train/loss': 2.1738510131835938, 'validation/accuracy': 0.49164000153541565, 'validation/loss': 2.3356337547302246, 'validation/num_examples': 50000, 'test/accuracy': 0.3882000148296356, 'test/loss': 2.9433727264404297, 'test/num_examples': 10000, 'score': 10127.587964057922, 'total_duration': 10885.880244970322, 'accumulated_submission_time': 10127.587964057922, 'accumulated_eval_time': 756.3632752895355, 'accumulated_logging_time': 0.7525720596313477, 'global_step': 22340, 'preemption_count': 0}), (23269, {'train/accuracy': 0.5324609279632568, 'train/loss': 2.129218816757202, 'validation/accuracy': 0.4975399971008301, 'validation/loss': 2.299388885498047, 'validation/num_examples': 50000, 'test/accuracy': 0.38840001821517944, 'test/loss': 2.9122867584228516, 'test/num_examples': 10000, 'score': 10547.819781303406, 'total_duration': 11342.201602220535, 'accumulated_submission_time': 10547.819781303406, 'accumulated_eval_time': 792.3766157627106, 'accumulated_logging_time': 0.7804503440856934, 'global_step': 23269, 'preemption_count': 0}), (24199, {'train/accuracy': 0.5442578196525574, 'train/loss': 2.104851245880127, 'validation/accuracy': 0.498339980840683, 'validation/loss': 2.3141963481903076, 'validation/num_examples': 50000, 'test/accuracy': 0.3945000171661377, 'test/loss': 2.9152963161468506, 'test/num_examples': 10000, 'score': 10967.767451047897, 'total_duration': 11799.014899253845, 'accumulated_submission_time': 10967.767451047897, 'accumulated_eval_time': 829.1624467372894, 'accumulated_logging_time': 0.8119678497314453, 'global_step': 24199, 'preemption_count': 0}), (25128, {'train/accuracy': 0.5494335889816284, 'train/loss': 2.1070497035980225, 'validation/accuracy': 0.5054199695587158, 'validation/loss': 2.3007595539093018, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.9255411624908447, 'test/num_examples': 10000, 'score': 11388.04214978218, 'total_duration': 12255.743125915527, 'accumulated_submission_time': 11388.04214978218, 'accumulated_eval_time': 865.5352845191956, 'accumulated_logging_time': 0.8451135158538818, 'global_step': 25128, 'preemption_count': 0}), (26047, {'train/accuracy': 0.5480077862739563, 'train/loss': 2.075366735458374, 'validation/accuracy': 0.5156199932098389, 'validation/loss': 2.2329912185668945, 'validation/num_examples': 50000, 'test/accuracy': 0.4003000259399414, 'test/loss': 2.8565263748168945, 'test/num_examples': 10000, 'score': 11808.012126922607, 'total_duration': 12711.5367333889, 'accumulated_submission_time': 11808.012126922607, 'accumulated_eval_time': 901.2724430561066, 'accumulated_logging_time': 0.883490800857544, 'global_step': 26047, 'preemption_count': 0}), (26974, {'train/accuracy': 0.5512499809265137, 'train/loss': 2.059241533279419, 'validation/accuracy': 0.510919988155365, 'validation/loss': 2.2513058185577393, 'validation/num_examples': 50000, 'test/accuracy': 0.39570000767707825, 'test/loss': 2.879225015640259, 'test/num_examples': 10000, 'score': 12228.003628730774, 'total_duration': 13167.093488931656, 'accumulated_submission_time': 12228.003628730774, 'accumulated_eval_time': 936.7529728412628, 'accumulated_logging_time': 0.9200353622436523, 'global_step': 26974, 'preemption_count': 0}), (27900, {'train/accuracy': 0.5764257907867432, 'train/loss': 1.9356085062026978, 'validation/accuracy': 0.5181599855422974, 'validation/loss': 2.198256015777588, 'validation/num_examples': 50000, 'test/accuracy': 0.40390002727508545, 'test/loss': 2.825509786605835, 'test/num_examples': 10000, 'score': 12648.078053474426, 'total_duration': 13623.059293746948, 'accumulated_submission_time': 12648.078053474426, 'accumulated_eval_time': 972.5665490627289, 'accumulated_logging_time': 0.9505248069763184, 'global_step': 27900, 'preemption_count': 0}), (28825, {'train/accuracy': 0.5618749856948853, 'train/loss': 1.990774154663086, 'validation/accuracy': 0.5294600129127502, 'validation/loss': 2.153543472290039, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.7733428478240967, 'test/num_examples': 10000, 'score': 13068.219451665878, 'total_duration': 14078.781080007553, 'accumulated_submission_time': 13068.219451665878, 'accumulated_eval_time': 1008.0687556266785, 'accumulated_logging_time': 0.9811530113220215, 'global_step': 28825, 'preemption_count': 0}), (29752, {'train/accuracy': 0.5666210651397705, 'train/loss': 1.9570411443710327, 'validation/accuracy': 0.5254200100898743, 'validation/loss': 2.154048442840576, 'validation/num_examples': 50000, 'test/accuracy': 0.414900004863739, 'test/loss': 2.78692626953125, 'test/num_examples': 10000, 'score': 13488.522999286652, 'total_duration': 14534.944336652756, 'accumulated_submission_time': 13488.522999286652, 'accumulated_eval_time': 1043.8474705219269, 'accumulated_logging_time': 1.0138554573059082, 'global_step': 29752, 'preemption_count': 0}), (30682, {'train/accuracy': 0.5687890648841858, 'train/loss': 2.034207344055176, 'validation/accuracy': 0.5227400064468384, 'validation/loss': 2.254662036895752, 'validation/num_examples': 50000, 'test/accuracy': 0.41610002517700195, 'test/loss': 2.8487892150878906, 'test/num_examples': 10000, 'score': 13908.572232484818, 'total_duration': 14990.768680334091, 'accumulated_submission_time': 13908.572232484818, 'accumulated_eval_time': 1079.5465333461761, 'accumulated_logging_time': 1.0415377616882324, 'global_step': 30682, 'preemption_count': 0}), (31610, {'train/accuracy': 0.5677539110183716, 'train/loss': 1.991437315940857, 'validation/accuracy': 0.531279981136322, 'validation/loss': 2.1594009399414062, 'validation/num_examples': 50000, 'test/accuracy': 0.4204000234603882, 'test/loss': 2.7725627422332764, 'test/num_examples': 10000, 'score': 14328.762746572495, 'total_duration': 15446.033729076385, 'accumulated_submission_time': 14328.762746572495, 'accumulated_eval_time': 1114.5431122779846, 'accumulated_logging_time': 1.0704760551452637, 'global_step': 31610, 'preemption_count': 0}), (32539, {'train/accuracy': 0.5756444931030273, 'train/loss': 1.9299191236495972, 'validation/accuracy': 0.5379799604415894, 'validation/loss': 2.113074541091919, 'validation/num_examples': 50000, 'test/accuracy': 0.42490002512931824, 'test/loss': 2.7316830158233643, 'test/num_examples': 10000, 'score': 14749.06371474266, 'total_duration': 15901.75974059105, 'accumulated_submission_time': 14749.06371474266, 'accumulated_eval_time': 1149.8860309123993, 'accumulated_logging_time': 1.1039931774139404, 'global_step': 32539, 'preemption_count': 0}), (33466, {'train/accuracy': 0.5826953053474426, 'train/loss': 1.9023802280426025, 'validation/accuracy': 0.5382999777793884, 'validation/loss': 2.1023356914520264, 'validation/num_examples': 50000, 'test/accuracy': 0.4214000105857849, 'test/loss': 2.7348814010620117, 'test/num_examples': 10000, 'score': 15168.989698171616, 'total_duration': 16358.529019117355, 'accumulated_submission_time': 15168.989698171616, 'accumulated_eval_time': 1186.6391808986664, 'accumulated_logging_time': 1.1376256942749023, 'global_step': 33466, 'preemption_count': 0}), (34393, {'train/accuracy': 0.5888671875, 'train/loss': 1.8822269439697266, 'validation/accuracy': 0.5423799753189087, 'validation/loss': 2.0869996547698975, 'validation/num_examples': 50000, 'test/accuracy': 0.43130001425743103, 'test/loss': 2.7067244052886963, 'test/num_examples': 10000, 'score': 15589.234191179276, 'total_duration': 16815.190942525864, 'accumulated_submission_time': 15589.234191179276, 'accumulated_eval_time': 1222.977279663086, 'accumulated_logging_time': 1.1691491603851318, 'global_step': 34393, 'preemption_count': 0}), (35320, {'train/accuracy': 0.5822656154632568, 'train/loss': 1.9264488220214844, 'validation/accuracy': 0.5397199988365173, 'validation/loss': 2.1193225383758545, 'validation/num_examples': 50000, 'test/accuracy': 0.4256000220775604, 'test/loss': 2.749296188354492, 'test/num_examples': 10000, 'score': 16009.42619729042, 'total_duration': 17271.53852891922, 'accumulated_submission_time': 16009.42619729042, 'accumulated_eval_time': 1259.0506103038788, 'accumulated_logging_time': 1.20351243019104, 'global_step': 35320, 'preemption_count': 0}), (36250, {'train/accuracy': 0.5885351300239563, 'train/loss': 1.876665472984314, 'validation/accuracy': 0.5481399893760681, 'validation/loss': 2.0634071826934814, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.680048704147339, 'test/num_examples': 10000, 'score': 16429.708421945572, 'total_duration': 17727.43417429924, 'accumulated_submission_time': 16429.708421945572, 'accumulated_eval_time': 1294.5700707435608, 'accumulated_logging_time': 1.2412841320037842, 'global_step': 36250, 'preemption_count': 0}), (37179, {'train/accuracy': 0.6117382645606995, 'train/loss': 1.750945806503296, 'validation/accuracy': 0.5479400157928467, 'validation/loss': 2.0413665771484375, 'validation/num_examples': 50000, 'test/accuracy': 0.4376000165939331, 'test/loss': 2.6541614532470703, 'test/num_examples': 10000, 'score': 16850.04904961586, 'total_duration': 18184.6283288002, 'accumulated_submission_time': 16850.04904961586, 'accumulated_eval_time': 1331.338150024414, 'accumulated_logging_time': 1.278425931930542, 'global_step': 37179, 'preemption_count': 0}), (38104, {'train/accuracy': 0.5925390720367432, 'train/loss': 1.8541145324707031, 'validation/accuracy': 0.5517799854278564, 'validation/loss': 2.030083656311035, 'validation/num_examples': 50000, 'test/accuracy': 0.4384000301361084, 'test/loss': 2.6467978954315186, 'test/num_examples': 10000, 'score': 17269.996902942657, 'total_duration': 18640.95697760582, 'accumulated_submission_time': 17269.996902942657, 'accumulated_eval_time': 1367.6382720470428, 'accumulated_logging_time': 1.311347246170044, 'global_step': 38104, 'preemption_count': 0}), (39030, {'train/accuracy': 0.5927343368530273, 'train/loss': 1.8353626728057861, 'validation/accuracy': 0.5510199666023254, 'validation/loss': 2.0330088138580322, 'validation/num_examples': 50000, 'test/accuracy': 0.4350000321865082, 'test/loss': 2.658357620239258, 'test/num_examples': 10000, 'score': 17689.955061912537, 'total_duration': 19097.34910964966, 'accumulated_submission_time': 17689.955061912537, 'accumulated_eval_time': 1403.9910144805908, 'accumulated_logging_time': 1.3433549404144287, 'global_step': 39030, 'preemption_count': 0}), (39957, {'train/accuracy': 0.5999609231948853, 'train/loss': 1.784259557723999, 'validation/accuracy': 0.5545600056648254, 'validation/loss': 2.0094738006591797, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.6230082511901855, 'test/num_examples': 10000, 'score': 18110.31490755081, 'total_duration': 19554.41698360443, 'accumulated_submission_time': 18110.31490755081, 'accumulated_eval_time': 1440.6161260604858, 'accumulated_logging_time': 1.377694845199585, 'global_step': 39957, 'preemption_count': 0}), (40882, {'train/accuracy': 0.5937694907188416, 'train/loss': 1.8420950174331665, 'validation/accuracy': 0.5575799942016602, 'validation/loss': 2.006721258163452, 'validation/num_examples': 50000, 'test/accuracy': 0.4439000189304352, 'test/loss': 2.64489483833313, 'test/num_examples': 10000, 'score': 18530.302005290985, 'total_duration': 20009.399348258972, 'accumulated_submission_time': 18530.302005290985, 'accumulated_eval_time': 1475.5338282585144, 'accumulated_logging_time': 1.4079561233520508, 'global_step': 40882, 'preemption_count': 0}), (41810, {'train/accuracy': 0.6000195145606995, 'train/loss': 1.8037185668945312, 'validation/accuracy': 0.5605800151824951, 'validation/loss': 1.9919346570968628, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.6278748512268066, 'test/num_examples': 10000, 'score': 18950.62165570259, 'total_duration': 20465.520961999893, 'accumulated_submission_time': 18950.62165570259, 'accumulated_eval_time': 1511.2537944316864, 'accumulated_logging_time': 1.4423680305480957, 'global_step': 41810, 'preemption_count': 0}), (42736, {'train/accuracy': 0.6067187190055847, 'train/loss': 1.7677797079086304, 'validation/accuracy': 0.5633800029754639, 'validation/loss': 1.9798634052276611, 'validation/num_examples': 50000, 'test/accuracy': 0.44680002331733704, 'test/loss': 2.621208667755127, 'test/num_examples': 10000, 'score': 19370.62746167183, 'total_duration': 20921.324761152267, 'accumulated_submission_time': 19370.62746167183, 'accumulated_eval_time': 1546.9716968536377, 'accumulated_logging_time': 1.474189043045044, 'global_step': 42736, 'preemption_count': 0}), (43664, {'train/accuracy': 0.6116601228713989, 'train/loss': 1.7431319952011108, 'validation/accuracy': 0.5555999875068665, 'validation/loss': 1.9900355339050293, 'validation/num_examples': 50000, 'test/accuracy': 0.44040003418922424, 'test/loss': 2.6423449516296387, 'test/num_examples': 10000, 'score': 19790.838992118835, 'total_duration': 21376.125276327133, 'accumulated_submission_time': 19790.838992118835, 'accumulated_eval_time': 1581.4821391105652, 'accumulated_logging_time': 1.5045185089111328, 'global_step': 43664, 'preemption_count': 0}), (44589, {'train/accuracy': 0.6066015362739563, 'train/loss': 1.761168122291565, 'validation/accuracy': 0.562720000743866, 'validation/loss': 1.9542808532714844, 'validation/num_examples': 50000, 'test/accuracy': 0.44780001044273376, 'test/loss': 2.59022855758667, 'test/num_examples': 10000, 'score': 20211.031491041183, 'total_duration': 21832.046981096268, 'accumulated_submission_time': 20211.031491041183, 'accumulated_eval_time': 1617.130084991455, 'accumulated_logging_time': 1.5374326705932617, 'global_step': 44589, 'preemption_count': 0}), (45511, {'train/accuracy': 0.6109179258346558, 'train/loss': 1.7445123195648193, 'validation/accuracy': 0.5663999915122986, 'validation/loss': 1.957970380783081, 'validation/num_examples': 50000, 'test/accuracy': 0.4520000219345093, 'test/loss': 2.569453239440918, 'test/num_examples': 10000, 'score': 20631.116145849228, 'total_duration': 22287.97873067856, 'accumulated_submission_time': 20631.116145849228, 'accumulated_eval_time': 1652.8864409923553, 'accumulated_logging_time': 1.5798923969268799, 'global_step': 45511, 'preemption_count': 0}), (46439, {'train/accuracy': 0.6299023032188416, 'train/loss': 1.6438137292861938, 'validation/accuracy': 0.5723599791526794, 'validation/loss': 1.911130428314209, 'validation/num_examples': 50000, 'test/accuracy': 0.45250001549720764, 'test/loss': 2.548551559448242, 'test/num_examples': 10000, 'score': 21051.443721055984, 'total_duration': 22743.75170326233, 'accumulated_submission_time': 21051.443721055984, 'accumulated_eval_time': 1688.2459979057312, 'accumulated_logging_time': 1.6172716617584229, 'global_step': 46439, 'preemption_count': 0}), (47366, {'train/accuracy': 0.6089648008346558, 'train/loss': 1.7728519439697266, 'validation/accuracy': 0.5699399709701538, 'validation/loss': 1.9609678983688354, 'validation/num_examples': 50000, 'test/accuracy': 0.4547000229358673, 'test/loss': 2.593113422393799, 'test/num_examples': 10000, 'score': 21471.823915719986, 'total_duration': 23200.93217253685, 'accumulated_submission_time': 21471.823915719986, 'accumulated_eval_time': 1724.9623546600342, 'accumulated_logging_time': 1.65169358253479, 'global_step': 47366, 'preemption_count': 0}), (48292, {'train/accuracy': 0.6068750023841858, 'train/loss': 1.7780888080596924, 'validation/accuracy': 0.5670199990272522, 'validation/loss': 1.9639018774032593, 'validation/num_examples': 50000, 'test/accuracy': 0.4553000330924988, 'test/loss': 2.6022982597351074, 'test/num_examples': 10000, 'score': 21892.04855132103, 'total_duration': 23658.274804115295, 'accumulated_submission_time': 21892.04855132103, 'accumulated_eval_time': 1761.9988887310028, 'accumulated_logging_time': 1.6848242282867432, 'global_step': 48292, 'preemption_count': 0}), (49216, {'train/accuracy': 0.6234374642372131, 'train/loss': 1.6764730215072632, 'validation/accuracy': 0.5714200139045715, 'validation/loss': 1.9154047966003418, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.5392799377441406, 'test/num_examples': 10000, 'score': 22312.300507068634, 'total_duration': 24113.78163957596, 'accumulated_submission_time': 22312.300507068634, 'accumulated_eval_time': 1797.1702196598053, 'accumulated_logging_time': 1.7208774089813232, 'global_step': 49216, 'preemption_count': 0}), (50139, {'train/accuracy': 0.6090039014816284, 'train/loss': 1.7651121616363525, 'validation/accuracy': 0.569819986820221, 'validation/loss': 1.935936689376831, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.5697062015533447, 'test/num_examples': 10000, 'score': 22732.495416641235, 'total_duration': 24568.618636846542, 'accumulated_submission_time': 22732.495416641235, 'accumulated_eval_time': 1831.73202419281, 'accumulated_logging_time': 1.7526824474334717, 'global_step': 50139, 'preemption_count': 0}), (51065, {'train/accuracy': 0.6101757884025574, 'train/loss': 1.7590184211730957, 'validation/accuracy': 0.5707600116729736, 'validation/loss': 1.9382047653198242, 'validation/num_examples': 50000, 'test/accuracy': 0.45210000872612, 'test/loss': 2.5792510509490967, 'test/num_examples': 10000, 'score': 23152.728211402893, 'total_duration': 25025.574861764908, 'accumulated_submission_time': 23152.728211402893, 'accumulated_eval_time': 1868.3653919696808, 'accumulated_logging_time': 1.794832706451416, 'global_step': 51065, 'preemption_count': 0}), (51992, {'train/accuracy': 0.6263476610183716, 'train/loss': 1.6484354734420776, 'validation/accuracy': 0.5796999931335449, 'validation/loss': 1.8653367757797241, 'validation/num_examples': 50000, 'test/accuracy': 0.46560001373291016, 'test/loss': 2.487163782119751, 'test/num_examples': 10000, 'score': 23573.097232103348, 'total_duration': 25483.33177614212, 'accumulated_submission_time': 23573.097232103348, 'accumulated_eval_time': 1905.6698813438416, 'accumulated_logging_time': 1.828599452972412, 'global_step': 51992, 'preemption_count': 0}), (52919, {'train/accuracy': 0.6298437118530273, 'train/loss': 1.6609742641448975, 'validation/accuracy': 0.5778399705886841, 'validation/loss': 1.9005619287490845, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.5275325775146484, 'test/num_examples': 10000, 'score': 23993.060639619827, 'total_duration': 25941.218841791153, 'accumulated_submission_time': 23993.060639619827, 'accumulated_eval_time': 1943.5027811527252, 'accumulated_logging_time': 1.8709113597869873, 'global_step': 52919, 'preemption_count': 0}), (53845, {'train/accuracy': 0.6171679496765137, 'train/loss': 1.7618088722229004, 'validation/accuracy': 0.5772799849510193, 'validation/loss': 1.9457567930221558, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.5673742294311523, 'test/num_examples': 10000, 'score': 24413.198468208313, 'total_duration': 26397.88577604294, 'accumulated_submission_time': 24413.198468208313, 'accumulated_eval_time': 1979.9512028694153, 'accumulated_logging_time': 1.9040093421936035, 'global_step': 53845, 'preemption_count': 0}), (54771, {'train/accuracy': 0.6280664205551147, 'train/loss': 1.6353827714920044, 'validation/accuracy': 0.581820011138916, 'validation/loss': 1.8503868579864502, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.465291976928711, 'test/num_examples': 10000, 'score': 24833.537580490112, 'total_duration': 26855.232084035873, 'accumulated_submission_time': 24833.537580490112, 'accumulated_eval_time': 2016.8710873126984, 'accumulated_logging_time': 1.9429829120635986, 'global_step': 54771, 'preemption_count': 0}), (55700, {'train/accuracy': 0.6409375071525574, 'train/loss': 1.605479121208191, 'validation/accuracy': 0.5806399583816528, 'validation/loss': 1.8812267780303955, 'validation/num_examples': 50000, 'test/accuracy': 0.46140003204345703, 'test/loss': 2.5292446613311768, 'test/num_examples': 10000, 'score': 25253.59513092041, 'total_duration': 27311.22057056427, 'accumulated_submission_time': 25253.59513092041, 'accumulated_eval_time': 2052.716703414917, 'accumulated_logging_time': 1.9796583652496338, 'global_step': 55700, 'preemption_count': 0}), (56627, {'train/accuracy': 0.6262499690055847, 'train/loss': 1.657572865486145, 'validation/accuracy': 0.5814999938011169, 'validation/loss': 1.8544203042984009, 'validation/num_examples': 50000, 'test/accuracy': 0.4660000205039978, 'test/loss': 2.484644651412964, 'test/num_examples': 10000, 'score': 25673.629153251648, 'total_duration': 27767.3883125782, 'accumulated_submission_time': 25673.629153251648, 'accumulated_eval_time': 2088.7680180072784, 'accumulated_logging_time': 2.0139429569244385, 'global_step': 56627, 'preemption_count': 0}), (57555, {'train/accuracy': 0.6287499666213989, 'train/loss': 1.6723737716674805, 'validation/accuracy': 0.5836600065231323, 'validation/loss': 1.8777090311050415, 'validation/num_examples': 50000, 'test/accuracy': 0.4666000306606293, 'test/loss': 2.5032131671905518, 'test/num_examples': 10000, 'score': 26093.89505290985, 'total_duration': 28225.165060520172, 'accumulated_submission_time': 26093.89505290985, 'accumulated_eval_time': 2126.1931478977203, 'accumulated_logging_time': 2.051152229309082, 'global_step': 57555, 'preemption_count': 0}), (58481, {'train/accuracy': 0.6403515338897705, 'train/loss': 1.61244797706604, 'validation/accuracy': 0.582859992980957, 'validation/loss': 1.8621419668197632, 'validation/num_examples': 50000, 'test/accuracy': 0.4684000313282013, 'test/loss': 2.4887142181396484, 'test/num_examples': 10000, 'score': 26513.911805152893, 'total_duration': 28683.89664721489, 'accumulated_submission_time': 26513.911805152893, 'accumulated_eval_time': 2164.8202011585236, 'accumulated_logging_time': 2.0870184898376465, 'global_step': 58481, 'preemption_count': 0}), (59407, {'train/accuracy': 0.6268945336341858, 'train/loss': 1.6685230731964111, 'validation/accuracy': 0.5874199867248535, 'validation/loss': 1.8534951210021973, 'validation/num_examples': 50000, 'test/accuracy': 0.47380003333091736, 'test/loss': 2.4757091999053955, 'test/num_examples': 10000, 'score': 26934.146147489548, 'total_duration': 29142.89506626129, 'accumulated_submission_time': 26934.146147489548, 'accumulated_eval_time': 2203.4964802265167, 'accumulated_logging_time': 2.1264965534210205, 'global_step': 59407, 'preemption_count': 0}), (60334, {'train/accuracy': 0.6327733993530273, 'train/loss': 1.6337242126464844, 'validation/accuracy': 0.5888599753379822, 'validation/loss': 1.8365858793258667, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.4529848098754883, 'test/num_examples': 10000, 'score': 27354.376095294952, 'total_duration': 29600.636869430542, 'accumulated_submission_time': 27354.376095294952, 'accumulated_eval_time': 2240.920811891556, 'accumulated_logging_time': 2.165325164794922, 'global_step': 60334, 'preemption_count': 0}), (61261, {'train/accuracy': 0.6378515362739563, 'train/loss': 1.6398062705993652, 'validation/accuracy': 0.5875999927520752, 'validation/loss': 1.8620257377624512, 'validation/num_examples': 50000, 'test/accuracy': 0.4675000309944153, 'test/loss': 2.50374174118042, 'test/num_examples': 10000, 'score': 27774.38081717491, 'total_duration': 30054.39617562294, 'accumulated_submission_time': 27774.38081717491, 'accumulated_eval_time': 2274.5890777111053, 'accumulated_logging_time': 2.203279733657837, 'global_step': 61261, 'preemption_count': 0}), (62185, {'train/accuracy': 0.6422656178474426, 'train/loss': 1.6118204593658447, 'validation/accuracy': 0.5854799747467041, 'validation/loss': 1.8537278175354004, 'validation/num_examples': 50000, 'test/accuracy': 0.4650000333786011, 'test/loss': 2.4965875148773193, 'test/num_examples': 10000, 'score': 28194.3759431839, 'total_duration': 30510.36556315422, 'accumulated_submission_time': 28194.3759431839, 'accumulated_eval_time': 2310.4748668670654, 'accumulated_logging_time': 2.243015766143799, 'global_step': 62185, 'preemption_count': 0}), (63111, {'train/accuracy': 0.6295117139816284, 'train/loss': 1.700981855392456, 'validation/accuracy': 0.5878599882125854, 'validation/loss': 1.883209466934204, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.5159428119659424, 'test/num_examples': 10000, 'score': 28614.368557929993, 'total_duration': 30966.68574333191, 'accumulated_submission_time': 28614.368557929993, 'accumulated_eval_time': 2346.715988636017, 'accumulated_logging_time': 2.2811131477355957, 'global_step': 63111, 'preemption_count': 0}), (64038, {'train/accuracy': 0.6376757621765137, 'train/loss': 1.6427497863769531, 'validation/accuracy': 0.5946999788284302, 'validation/loss': 1.8466299772262573, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.4646663665771484, 'test/num_examples': 10000, 'score': 29034.539268016815, 'total_duration': 31423.417449235916, 'accumulated_submission_time': 29034.539268016815, 'accumulated_eval_time': 2383.1910014152527, 'accumulated_logging_time': 2.3187735080718994, 'global_step': 64038, 'preemption_count': 0}), (64965, {'train/accuracy': 0.6559374928474426, 'train/loss': 1.5458028316497803, 'validation/accuracy': 0.5884599685668945, 'validation/loss': 1.832724690437317, 'validation/num_examples': 50000, 'test/accuracy': 0.4723000228404999, 'test/loss': 2.4600064754486084, 'test/num_examples': 10000, 'score': 29454.639416217804, 'total_duration': 31881.192403554916, 'accumulated_submission_time': 29454.639416217804, 'accumulated_eval_time': 2420.782904148102, 'accumulated_logging_time': 2.3532867431640625, 'global_step': 64965, 'preemption_count': 0}), (65893, {'train/accuracy': 0.6297070384025574, 'train/loss': 1.6710429191589355, 'validation/accuracy': 0.5890399813652039, 'validation/loss': 1.8539769649505615, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.482262134552002, 'test/num_examples': 10000, 'score': 29874.705132722855, 'total_duration': 32338.065663814545, 'accumulated_submission_time': 29874.705132722855, 'accumulated_eval_time': 2457.5085434913635, 'accumulated_logging_time': 2.387108564376831, 'global_step': 65893, 'preemption_count': 0}), (66814, {'train/accuracy': 0.6425976157188416, 'train/loss': 1.6025398969650269, 'validation/accuracy': 0.5962600111961365, 'validation/loss': 1.8162277936935425, 'validation/num_examples': 50000, 'test/accuracy': 0.47050002217292786, 'test/loss': 2.4594082832336426, 'test/num_examples': 10000, 'score': 30294.62602829933, 'total_duration': 32796.39333939552, 'accumulated_submission_time': 30294.62602829933, 'accumulated_eval_time': 2495.827398777008, 'accumulated_logging_time': 2.426995038986206, 'global_step': 66814, 'preemption_count': 0}), (67738, {'train/accuracy': 0.6525781154632568, 'train/loss': 1.570324182510376, 'validation/accuracy': 0.6031599640846252, 'validation/loss': 1.8069418668746948, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.4569497108459473, 'test/num_examples': 10000, 'score': 30714.93037724495, 'total_duration': 33256.4021191597, 'accumulated_submission_time': 30714.93037724495, 'accumulated_eval_time': 2535.44087266922, 'accumulated_logging_time': 2.4705276489257812, 'global_step': 67738, 'preemption_count': 0}), (68663, {'train/accuracy': 0.6356640458106995, 'train/loss': 1.6274088621139526, 'validation/accuracy': 0.5971199870109558, 'validation/loss': 1.8149526119232178, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.4471421241760254, 'test/num_examples': 10000, 'score': 31135.09487080574, 'total_duration': 33716.840874910355, 'accumulated_submission_time': 31135.09487080574, 'accumulated_eval_time': 2575.6301860809326, 'accumulated_logging_time': 2.5075490474700928, 'global_step': 68663, 'preemption_count': 0}), (69587, {'train/accuracy': 0.6425390243530273, 'train/loss': 1.6058720350265503, 'validation/accuracy': 0.6004999876022339, 'validation/loss': 1.7994102239608765, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.4608607292175293, 'test/num_examples': 10000, 'score': 31555.165743112564, 'total_duration': 34175.87082648277, 'accumulated_submission_time': 31555.165743112564, 'accumulated_eval_time': 2614.288892507553, 'accumulated_logging_time': 2.7598090171813965, 'global_step': 69587, 'preemption_count': 0}), (70508, {'train/accuracy': 0.6518359184265137, 'train/loss': 1.5789217948913574, 'validation/accuracy': 0.5998600125312805, 'validation/loss': 1.8045138120651245, 'validation/num_examples': 50000, 'test/accuracy': 0.4789000153541565, 'test/loss': 2.4507243633270264, 'test/num_examples': 10000, 'score': 31975.086597681046, 'total_duration': 34635.624881505966, 'accumulated_submission_time': 31975.086597681046, 'accumulated_eval_time': 2654.033591747284, 'accumulated_logging_time': 2.801017999649048, 'global_step': 70508, 'preemption_count': 0}), (71431, {'train/accuracy': 0.6530663967132568, 'train/loss': 1.5456774234771729, 'validation/accuracy': 0.6032800078392029, 'validation/loss': 1.7672827243804932, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.4054059982299805, 'test/num_examples': 10000, 'score': 32395.517565488815, 'total_duration': 35094.78996706009, 'accumulated_submission_time': 32395.517565488815, 'accumulated_eval_time': 2692.672725915909, 'accumulated_logging_time': 2.847104072570801, 'global_step': 71431, 'preemption_count': 0}), (72353, {'train/accuracy': 0.6449413895606995, 'train/loss': 1.5734144449234009, 'validation/accuracy': 0.6037600040435791, 'validation/loss': 1.765405535697937, 'validation/num_examples': 50000, 'test/accuracy': 0.48420003056526184, 'test/loss': 2.39375638961792, 'test/num_examples': 10000, 'score': 32815.659552812576, 'total_duration': 35551.76320028305, 'accumulated_submission_time': 32815.659552812576, 'accumulated_eval_time': 2729.4140496253967, 'accumulated_logging_time': 2.888112783432007, 'global_step': 72353, 'preemption_count': 0}), (73277, {'train/accuracy': 0.6486523151397705, 'train/loss': 1.5562613010406494, 'validation/accuracy': 0.6032199859619141, 'validation/loss': 1.7581266164779663, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.3881523609161377, 'test/num_examples': 10000, 'score': 33235.68685173988, 'total_duration': 36009.36540389061, 'accumulated_submission_time': 33235.68685173988, 'accumulated_eval_time': 2766.906188249588, 'accumulated_logging_time': 2.9232640266418457, 'global_step': 73277, 'preemption_count': 0}), (74204, {'train/accuracy': 0.6731249690055847, 'train/loss': 1.4761346578598022, 'validation/accuracy': 0.6058200001716614, 'validation/loss': 1.766079068183899, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.4043872356414795, 'test/num_examples': 10000, 'score': 33655.99447274208, 'total_duration': 36469.588874578476, 'accumulated_submission_time': 33655.99447274208, 'accumulated_eval_time': 2806.733047246933, 'accumulated_logging_time': 2.963426113128662, 'global_step': 74204, 'preemption_count': 0}), (75128, {'train/accuracy': 0.6450976133346558, 'train/loss': 1.5667901039123535, 'validation/accuracy': 0.6061399579048157, 'validation/loss': 1.760995626449585, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.39943790435791, 'test/num_examples': 10000, 'score': 34076.297404289246, 'total_duration': 36930.39732980728, 'accumulated_submission_time': 34076.297404289246, 'accumulated_eval_time': 2847.1502919197083, 'accumulated_logging_time': 3.0034356117248535, 'global_step': 75128, 'preemption_count': 0}), (76053, {'train/accuracy': 0.6499999761581421, 'train/loss': 1.5527905225753784, 'validation/accuracy': 0.6094799637794495, 'validation/loss': 1.7591296434402466, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.395556688308716, 'test/num_examples': 10000, 'score': 34496.459659576416, 'total_duration': 37390.65586042404, 'accumulated_submission_time': 34496.459659576416, 'accumulated_eval_time': 2887.1610465049744, 'accumulated_logging_time': 3.0404083728790283, 'global_step': 76053, 'preemption_count': 0}), (76976, {'train/accuracy': 0.6608788967132568, 'train/loss': 1.5342339277267456, 'validation/accuracy': 0.6050800085067749, 'validation/loss': 1.7775715589523315, 'validation/num_examples': 50000, 'test/accuracy': 0.48820000886917114, 'test/loss': 2.4163527488708496, 'test/num_examples': 10000, 'score': 34916.55329370499, 'total_duration': 37848.119691848755, 'accumulated_submission_time': 34916.55329370499, 'accumulated_eval_time': 2924.444561958313, 'accumulated_logging_time': 3.0794270038604736, 'global_step': 76976, 'preemption_count': 0}), (77903, {'train/accuracy': 0.6502929329872131, 'train/loss': 1.5727944374084473, 'validation/accuracy': 0.6092599630355835, 'validation/loss': 1.7659096717834473, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.4130568504333496, 'test/num_examples': 10000, 'score': 35336.832350969315, 'total_duration': 38307.64156937599, 'accumulated_submission_time': 35336.832350969315, 'accumulated_eval_time': 2963.59290099144, 'accumulated_logging_time': 3.1252198219299316, 'global_step': 77903, 'preemption_count': 0}), (78829, {'train/accuracy': 0.6518749594688416, 'train/loss': 1.5727999210357666, 'validation/accuracy': 0.6112599968910217, 'validation/loss': 1.7607871294021606, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.392758369445801, 'test/num_examples': 10000, 'score': 35756.88972687721, 'total_duration': 38767.19116520882, 'accumulated_submission_time': 35756.88972687721, 'accumulated_eval_time': 3003.001363515854, 'accumulated_logging_time': 3.161072254180908, 'global_step': 78829, 'preemption_count': 0}), (79754, {'train/accuracy': 0.6651171445846558, 'train/loss': 1.4895260334014893, 'validation/accuracy': 0.6138399839401245, 'validation/loss': 1.7199392318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.49550002813339233, 'test/loss': 2.342292547225952, 'test/num_examples': 10000, 'score': 36177.00625920296, 'total_duration': 39226.83818221092, 'accumulated_submission_time': 36177.00625920296, 'accumulated_eval_time': 3042.4453415870667, 'accumulated_logging_time': 3.1995127201080322, 'global_step': 79754, 'preemption_count': 0}), (80677, {'train/accuracy': 0.6646288633346558, 'train/loss': 1.4810144901275635, 'validation/accuracy': 0.6114999651908875, 'validation/loss': 1.7160485982894897, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.342613458633423, 'test/num_examples': 10000, 'score': 36597.088631391525, 'total_duration': 39687.675043821335, 'accumulated_submission_time': 36597.088631391525, 'accumulated_eval_time': 3083.1080589294434, 'accumulated_logging_time': 3.2435245513916016, 'global_step': 80677, 'preemption_count': 0}), (81603, {'train/accuracy': 0.6604687571525574, 'train/loss': 1.5106688737869263, 'validation/accuracy': 0.6142599582672119, 'validation/loss': 1.7114691734313965, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.352527379989624, 'test/num_examples': 10000, 'score': 37017.36079597473, 'total_duration': 40149.45268511772, 'accumulated_submission_time': 37017.36079597473, 'accumulated_eval_time': 3124.51851439476, 'accumulated_logging_time': 3.2905027866363525, 'global_step': 81603, 'preemption_count': 0}), (82530, {'train/accuracy': 0.6646093726158142, 'train/loss': 1.5200191736221313, 'validation/accuracy': 0.6138399839401245, 'validation/loss': 1.7434662580490112, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.370671510696411, 'test/num_examples': 10000, 'score': 37437.699897289276, 'total_duration': 40609.15281009674, 'accumulated_submission_time': 37437.699897289276, 'accumulated_eval_time': 3163.785692691803, 'accumulated_logging_time': 3.336221933364868, 'global_step': 82530, 'preemption_count': 0}), (83454, {'train/accuracy': 0.6852148175239563, 'train/loss': 1.3989347219467163, 'validation/accuracy': 0.6177399754524231, 'validation/loss': 1.7047220468521118, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3427956104278564, 'test/num_examples': 10000, 'score': 37857.93138933182, 'total_duration': 41068.871118307114, 'accumulated_submission_time': 37857.93138933182, 'accumulated_eval_time': 3203.184502363205, 'accumulated_logging_time': 3.375791311264038, 'global_step': 83454, 'preemption_count': 0}), (84377, {'train/accuracy': 0.6599413752555847, 'train/loss': 1.5086867809295654, 'validation/accuracy': 0.6137599945068359, 'validation/loss': 1.7188671827316284, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.361658811569214, 'test/num_examples': 10000, 'score': 38278.19797229767, 'total_duration': 41528.8823723793, 'accumulated_submission_time': 38278.19797229767, 'accumulated_eval_time': 3242.8420696258545, 'accumulated_logging_time': 3.4148974418640137, 'global_step': 84377, 'preemption_count': 0}), (85301, {'train/accuracy': 0.6687890291213989, 'train/loss': 1.4672608375549316, 'validation/accuracy': 0.6248399615287781, 'validation/loss': 1.679897427558899, 'validation/num_examples': 50000, 'test/accuracy': 0.5024999976158142, 'test/loss': 2.314694881439209, 'test/num_examples': 10000, 'score': 38698.49995803833, 'total_duration': 41990.98984336853, 'accumulated_submission_time': 38698.49995803833, 'accumulated_eval_time': 3284.55818772316, 'accumulated_logging_time': 3.455544948577881, 'global_step': 85301, 'preemption_count': 0}), (86227, {'train/accuracy': 0.682421863079071, 'train/loss': 1.420057773590088, 'validation/accuracy': 0.6224600076675415, 'validation/loss': 1.6837719678878784, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.308379650115967, 'test/num_examples': 10000, 'score': 39118.564464092255, 'total_duration': 42447.72882437706, 'accumulated_submission_time': 39118.564464092255, 'accumulated_eval_time': 3321.14311671257, 'accumulated_logging_time': 3.497130870819092, 'global_step': 86227, 'preemption_count': 0}), (87154, {'train/accuracy': 0.6678515672683716, 'train/loss': 1.4901386499404907, 'validation/accuracy': 0.624019980430603, 'validation/loss': 1.6841695308685303, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.3107705116271973, 'test/num_examples': 10000, 'score': 39538.55706048012, 'total_duration': 42908.27966308594, 'accumulated_submission_time': 39538.55706048012, 'accumulated_eval_time': 3361.6144444942474, 'accumulated_logging_time': 3.5359609127044678, 'global_step': 87154, 'preemption_count': 0}), (88081, {'train/accuracy': 0.6660546660423279, 'train/loss': 1.5081756114959717, 'validation/accuracy': 0.6201399564743042, 'validation/loss': 1.717879295349121, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.3405404090881348, 'test/num_examples': 10000, 'score': 39958.77399516106, 'total_duration': 43366.066150188446, 'accumulated_submission_time': 39958.77399516106, 'accumulated_eval_time': 3399.0924847126007, 'accumulated_logging_time': 3.5792877674102783, 'global_step': 88081, 'preemption_count': 0}), (89007, {'train/accuracy': 0.6788476705551147, 'train/loss': 1.4367852210998535, 'validation/accuracy': 0.6260199546813965, 'validation/loss': 1.682494044303894, 'validation/num_examples': 50000, 'test/accuracy': 0.506600022315979, 'test/loss': 2.3096532821655273, 'test/num_examples': 10000, 'score': 40378.939730882645, 'total_duration': 43824.08883070946, 'accumulated_submission_time': 40378.939730882645, 'accumulated_eval_time': 3436.8513662815094, 'accumulated_logging_time': 3.6288912296295166, 'global_step': 89007, 'preemption_count': 0}), (89929, {'train/accuracy': 0.6749609112739563, 'train/loss': 1.4869822263717651, 'validation/accuracy': 0.6248999834060669, 'validation/loss': 1.7146426439285278, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.3380980491638184, 'test/num_examples': 10000, 'score': 40798.99773478508, 'total_duration': 44281.61442565918, 'accumulated_submission_time': 40798.99773478508, 'accumulated_eval_time': 3474.23304772377, 'accumulated_logging_time': 3.6661694049835205, 'global_step': 89929, 'preemption_count': 0}), (90855, {'train/accuracy': 0.6749218702316284, 'train/loss': 1.44557523727417, 'validation/accuracy': 0.6292200088500977, 'validation/loss': 1.6595513820648193, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.2997994422912598, 'test/num_examples': 10000, 'score': 41219.33115816116, 'total_duration': 44740.604519844055, 'accumulated_submission_time': 41219.33115816116, 'accumulated_eval_time': 3512.803384065628, 'accumulated_logging_time': 3.704136610031128, 'global_step': 90855, 'preemption_count': 0}), (91780, {'train/accuracy': 0.6800000071525574, 'train/loss': 1.466551423072815, 'validation/accuracy': 0.6314399838447571, 'validation/loss': 1.6920958757400513, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.329899787902832, 'test/num_examples': 10000, 'score': 41639.402092933655, 'total_duration': 45198.59097504616, 'accumulated_submission_time': 41639.402092933655, 'accumulated_eval_time': 3550.6322796344757, 'accumulated_logging_time': 3.7427337169647217, 'global_step': 91780, 'preemption_count': 0}), (92701, {'train/accuracy': 0.69873046875, 'train/loss': 1.3424489498138428, 'validation/accuracy': 0.6325399875640869, 'validation/loss': 1.649196982383728, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2850232124328613, 'test/num_examples': 10000, 'score': 42059.464174985886, 'total_duration': 45658.742933273315, 'accumulated_submission_time': 42059.464174985886, 'accumulated_eval_time': 3590.631927251816, 'accumulated_logging_time': 3.7838430404663086, 'global_step': 92701, 'preemption_count': 0}), (93626, {'train/accuracy': 0.6816992163658142, 'train/loss': 1.4043458700180054, 'validation/accuracy': 0.637179970741272, 'validation/loss': 1.6172446012496948, 'validation/num_examples': 50000, 'test/accuracy': 0.5180000066757202, 'test/loss': 2.231893301010132, 'test/num_examples': 10000, 'score': 42479.74120259285, 'total_duration': 46117.04714727402, 'accumulated_submission_time': 42479.74120259285, 'accumulated_eval_time': 3628.571489095688, 'accumulated_logging_time': 3.8232078552246094, 'global_step': 93626, 'preemption_count': 0}), (94550, {'train/accuracy': 0.6850976347923279, 'train/loss': 1.3780698776245117, 'validation/accuracy': 0.6401599645614624, 'validation/loss': 1.591439127922058, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.23335862159729, 'test/num_examples': 10000, 'score': 42900.029477357864, 'total_duration': 46574.56220269203, 'accumulated_submission_time': 42900.029477357864, 'accumulated_eval_time': 3665.710597515106, 'accumulated_logging_time': 3.861924171447754, 'global_step': 94550, 'preemption_count': 0}), (95475, {'train/accuracy': 0.6938085556030273, 'train/loss': 1.3812767267227173, 'validation/accuracy': 0.6334999799728394, 'validation/loss': 1.646828293800354, 'validation/num_examples': 50000, 'test/accuracy': 0.5127000212669373, 'test/loss': 2.2695205211639404, 'test/num_examples': 10000, 'score': 43320.07567238808, 'total_duration': 47032.49260401726, 'accumulated_submission_time': 43320.07567238808, 'accumulated_eval_time': 3703.5072691440582, 'accumulated_logging_time': 3.901413679122925, 'global_step': 95475, 'preemption_count': 0}), (96400, {'train/accuracy': 0.6801366806030273, 'train/loss': 1.4493908882141113, 'validation/accuracy': 0.6377800107002258, 'validation/loss': 1.6380335092544556, 'validation/num_examples': 50000, 'test/accuracy': 0.5180000066757202, 'test/loss': 2.2730095386505127, 'test/num_examples': 10000, 'score': 43740.39475917816, 'total_duration': 47487.6726129055, 'accumulated_submission_time': 43740.39475917816, 'accumulated_eval_time': 3738.279561281204, 'accumulated_logging_time': 3.94172739982605, 'global_step': 96400, 'preemption_count': 0}), (97326, {'train/accuracy': 0.6853125095367432, 'train/loss': 1.400512933731079, 'validation/accuracy': 0.6398599743843079, 'validation/loss': 1.6111207008361816, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2331061363220215, 'test/num_examples': 10000, 'score': 44160.30851793289, 'total_duration': 47948.28272128105, 'accumulated_submission_time': 44160.30851793289, 'accumulated_eval_time': 3778.8772070407867, 'accumulated_logging_time': 3.9924333095550537, 'global_step': 97326, 'preemption_count': 0}), (98253, {'train/accuracy': 0.6956640481948853, 'train/loss': 1.3512005805969238, 'validation/accuracy': 0.6403399705886841, 'validation/loss': 1.6066975593566895, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.243485689163208, 'test/num_examples': 10000, 'score': 44580.45514702797, 'total_duration': 48407.94709467888, 'accumulated_submission_time': 44580.45514702797, 'accumulated_eval_time': 3818.3037271499634, 'accumulated_logging_time': 4.035202503204346, 'global_step': 98253, 'preemption_count': 0}), (99180, {'train/accuracy': 0.6977148056030273, 'train/loss': 1.3450783491134644, 'validation/accuracy': 0.6491000056266785, 'validation/loss': 1.5536638498306274, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.1836001873016357, 'test/num_examples': 10000, 'score': 45000.66364359856, 'total_duration': 48865.30378293991, 'accumulated_submission_time': 45000.66364359856, 'accumulated_eval_time': 3855.362357854843, 'accumulated_logging_time': 4.076540231704712, 'global_step': 99180, 'preemption_count': 0}), (100104, {'train/accuracy': 0.6898828148841858, 'train/loss': 1.416399598121643, 'validation/accuracy': 0.6423400044441223, 'validation/loss': 1.6274499893188477, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.2531089782714844, 'test/num_examples': 10000, 'score': 45420.91577172279, 'total_duration': 49324.64928340912, 'accumulated_submission_time': 45420.91577172279, 'accumulated_eval_time': 3894.3651168346405, 'accumulated_logging_time': 4.119498014450073, 'global_step': 100104, 'preemption_count': 0}), (101032, {'train/accuracy': 0.6932812333106995, 'train/loss': 1.3365551233291626, 'validation/accuracy': 0.6417999863624573, 'validation/loss': 1.5768588781356812, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.190631628036499, 'test/num_examples': 10000, 'score': 45841.04868769646, 'total_duration': 49783.31498479843, 'accumulated_submission_time': 45841.04868769646, 'accumulated_eval_time': 3932.808772087097, 'accumulated_logging_time': 4.1595988273620605, 'global_step': 101032, 'preemption_count': 0}), (101958, {'train/accuracy': 0.7127343416213989, 'train/loss': 1.287361979484558, 'validation/accuracy': 0.642579972743988, 'validation/loss': 1.5980144739151, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.233988046646118, 'test/num_examples': 10000, 'score': 46261.34437060356, 'total_duration': 50243.625947237015, 'accumulated_submission_time': 46261.34437060356, 'accumulated_eval_time': 3972.7331693172455, 'accumulated_logging_time': 4.202354431152344, 'global_step': 101958, 'preemption_count': 0}), (102883, {'train/accuracy': 0.6974804401397705, 'train/loss': 1.3310356140136719, 'validation/accuracy': 0.6518599987030029, 'validation/loss': 1.5392245054244995, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.177493095397949, 'test/num_examples': 10000, 'score': 46681.6217648983, 'total_duration': 50704.19277739525, 'accumulated_submission_time': 46681.6217648983, 'accumulated_eval_time': 4012.935410261154, 'accumulated_logging_time': 4.241785049438477, 'global_step': 102883, 'preemption_count': 0}), (103809, {'train/accuracy': 0.6997656226158142, 'train/loss': 1.369573950767517, 'validation/accuracy': 0.6473199725151062, 'validation/loss': 1.5862294435501099, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.2343835830688477, 'test/num_examples': 10000, 'score': 47101.53124284744, 'total_duration': 51165.548437833786, 'accumulated_submission_time': 47101.53124284744, 'accumulated_eval_time': 4054.28226852417, 'accumulated_logging_time': 4.292704820632935, 'global_step': 103809, 'preemption_count': 0}), (104733, {'train/accuracy': 0.7157617211341858, 'train/loss': 1.2719745635986328, 'validation/accuracy': 0.6475399732589722, 'validation/loss': 1.5650979280471802, 'validation/num_examples': 50000, 'test/accuracy': 0.528700053691864, 'test/loss': 2.189589262008667, 'test/num_examples': 10000, 'score': 47521.49310541153, 'total_duration': 51626.334483385086, 'accumulated_submission_time': 47521.49310541153, 'accumulated_eval_time': 4095.0082075595856, 'accumulated_logging_time': 4.342754125595093, 'global_step': 104733, 'preemption_count': 0}), (105660, {'train/accuracy': 0.6998632550239563, 'train/loss': 1.3254988193511963, 'validation/accuracy': 0.649619996547699, 'validation/loss': 1.5387935638427734, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.172654867172241, 'test/num_examples': 10000, 'score': 47941.640141010284, 'total_duration': 52083.59052538872, 'accumulated_submission_time': 47941.640141010284, 'accumulated_eval_time': 4132.022391796112, 'accumulated_logging_time': 4.3900251388549805, 'global_step': 105660, 'preemption_count': 0}), (106589, {'train/accuracy': 0.7017773389816284, 'train/loss': 1.3748812675476074, 'validation/accuracy': 0.6519799828529358, 'validation/loss': 1.600887656211853, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.2354602813720703, 'test/num_examples': 10000, 'score': 48361.77505850792, 'total_duration': 52543.93924975395, 'accumulated_submission_time': 48361.77505850792, 'accumulated_eval_time': 4172.140084028244, 'accumulated_logging_time': 4.437947511672974, 'global_step': 106589, 'preemption_count': 0}), (107516, {'train/accuracy': 0.7152148485183716, 'train/loss': 1.2641282081604004, 'validation/accuracy': 0.6518200039863586, 'validation/loss': 1.5298153162002563, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.170165777206421, 'test/num_examples': 10000, 'score': 48782.06770968437, 'total_duration': 53001.499126434326, 'accumulated_submission_time': 48782.06770968437, 'accumulated_eval_time': 4209.313284873962, 'accumulated_logging_time': 4.48431396484375, 'global_step': 107516, 'preemption_count': 0}), (108440, {'train/accuracy': 0.7088671922683716, 'train/loss': 1.30809485912323, 'validation/accuracy': 0.6545000076293945, 'validation/loss': 1.5428231954574585, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.1718273162841797, 'test/num_examples': 10000, 'score': 49202.40798187256, 'total_duration': 53460.39822816849, 'accumulated_submission_time': 49202.40798187256, 'accumulated_eval_time': 4247.780642032623, 'accumulated_logging_time': 4.5283708572387695, 'global_step': 108440, 'preemption_count': 0}), (109364, {'train/accuracy': 0.7081640362739563, 'train/loss': 1.2886130809783936, 'validation/accuracy': 0.6595799922943115, 'validation/loss': 1.5078322887420654, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.130657434463501, 'test/num_examples': 10000, 'score': 49622.73654794693, 'total_duration': 53920.59532928467, 'accumulated_submission_time': 49622.73654794693, 'accumulated_eval_time': 4287.5615401268005, 'accumulated_logging_time': 4.569385528564453, 'global_step': 109364, 'preemption_count': 0}), (110290, {'train/accuracy': 0.7182226181030273, 'train/loss': 1.246045470237732, 'validation/accuracy': 0.6599000096321106, 'validation/loss': 1.4922248125076294, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.11667799949646, 'test/num_examples': 10000, 'score': 50042.85763192177, 'total_duration': 54378.784519672394, 'accumulated_submission_time': 50042.85763192177, 'accumulated_eval_time': 4325.538824796677, 'accumulated_logging_time': 4.612484693527222, 'global_step': 110290, 'preemption_count': 0}), (111215, {'train/accuracy': 0.7352343797683716, 'train/loss': 1.1619881391525269, 'validation/accuracy': 0.6606599688529968, 'validation/loss': 1.4827560186386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.1034798622131348, 'test/num_examples': 10000, 'score': 50462.81347370148, 'total_duration': 54835.804351091385, 'accumulated_submission_time': 50462.81347370148, 'accumulated_eval_time': 4362.5080988407135, 'accumulated_logging_time': 4.6590001583099365, 'global_step': 111215, 'preemption_count': 0}), (112141, {'train/accuracy': 0.7141991853713989, 'train/loss': 1.2547168731689453, 'validation/accuracy': 0.6646999716758728, 'validation/loss': 1.48099946975708, 'validation/num_examples': 50000, 'test/accuracy': 0.534000039100647, 'test/loss': 2.13720703125, 'test/num_examples': 10000, 'score': 50883.195809841156, 'total_duration': 55292.03625512123, 'accumulated_submission_time': 50883.195809841156, 'accumulated_eval_time': 4398.260158777237, 'accumulated_logging_time': 4.708902835845947, 'global_step': 112141, 'preemption_count': 0}), (113064, {'train/accuracy': 0.7215819954872131, 'train/loss': 1.2358224391937256, 'validation/accuracy': 0.668940007686615, 'validation/loss': 1.475932002067566, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.1098098754882812, 'test/num_examples': 10000, 'score': 51303.45684719086, 'total_duration': 55752.28976273537, 'accumulated_submission_time': 51303.45684719086, 'accumulated_eval_time': 4438.158957719803, 'accumulated_logging_time': 4.753890514373779, 'global_step': 113064, 'preemption_count': 0}), (113989, {'train/accuracy': 0.7289257645606995, 'train/loss': 1.1877706050872803, 'validation/accuracy': 0.660860002040863, 'validation/loss': 1.4783949851989746, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.090823173522949, 'test/num_examples': 10000, 'score': 51723.392731666565, 'total_duration': 56210.262149095535, 'accumulated_submission_time': 51723.392731666565, 'accumulated_eval_time': 4476.102520704269, 'accumulated_logging_time': 4.79875922203064, 'global_step': 113989, 'preemption_count': 0}), (114913, {'train/accuracy': 0.71644526720047, 'train/loss': 1.250329852104187, 'validation/accuracy': 0.6658799648284912, 'validation/loss': 1.4862215518951416, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.1070306301116943, 'test/num_examples': 10000, 'score': 52143.375336408615, 'total_duration': 56668.10083293915, 'accumulated_submission_time': 52143.375336408615, 'accumulated_eval_time': 4513.86473441124, 'accumulated_logging_time': 4.843943119049072, 'global_step': 114913, 'preemption_count': 0}), (115837, {'train/accuracy': 0.7221484184265137, 'train/loss': 1.2188655138015747, 'validation/accuracy': 0.669439971446991, 'validation/loss': 1.451919674873352, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.074211597442627, 'test/num_examples': 10000, 'score': 52563.48961663246, 'total_duration': 57127.45796251297, 'accumulated_submission_time': 52563.48961663246, 'accumulated_eval_time': 4553.015449285507, 'accumulated_logging_time': 4.888232231140137, 'global_step': 115837, 'preemption_count': 0}), (116762, {'train/accuracy': 0.7323827743530273, 'train/loss': 1.1630433797836304, 'validation/accuracy': 0.6712200045585632, 'validation/loss': 1.4364888668060303, 'validation/num_examples': 50000, 'test/accuracy': 0.5437999963760376, 'test/loss': 2.064565420150757, 'test/num_examples': 10000, 'score': 52983.74047589302, 'total_duration': 57588.00782227516, 'accumulated_submission_time': 52983.74047589302, 'accumulated_eval_time': 4593.209508657455, 'accumulated_logging_time': 4.9340245723724365, 'global_step': 116762, 'preemption_count': 0}), (117688, {'train/accuracy': 0.7284570336341858, 'train/loss': 1.1957414150238037, 'validation/accuracy': 0.6744399666786194, 'validation/loss': 1.430295467376709, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.056349277496338, 'test/num_examples': 10000, 'score': 53403.952806949615, 'total_duration': 58049.12762546539, 'accumulated_submission_time': 53403.952806949615, 'accumulated_eval_time': 4634.027329921722, 'accumulated_logging_time': 4.976112604141235, 'global_step': 117688, 'preemption_count': 0}), (118612, {'train/accuracy': 0.724804699420929, 'train/loss': 1.209438681602478, 'validation/accuracy': 0.6693399548530579, 'validation/loss': 1.4543384313583374, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.073585271835327, 'test/num_examples': 10000, 'score': 53824.20411038399, 'total_duration': 58510.82946014404, 'accumulated_submission_time': 53824.20411038399, 'accumulated_eval_time': 4675.380663156509, 'accumulated_logging_time': 5.025782823562622, 'global_step': 118612, 'preemption_count': 0}), (119537, {'train/accuracy': 0.7267187237739563, 'train/loss': 1.2144628763198853, 'validation/accuracy': 0.670520007610321, 'validation/loss': 1.4620471000671387, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.072385549545288, 'test/num_examples': 10000, 'score': 54244.48342585564, 'total_duration': 58971.95789599419, 'accumulated_submission_time': 54244.48342585564, 'accumulated_eval_time': 4716.138568401337, 'accumulated_logging_time': 5.0698935985565186, 'global_step': 119537, 'preemption_count': 0}), (120462, {'train/accuracy': 0.7491210699081421, 'train/loss': 1.108149766921997, 'validation/accuracy': 0.6756599545478821, 'validation/loss': 1.4270612001419067, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 2.0518739223480225, 'test/num_examples': 10000, 'score': 54664.61124134064, 'total_duration': 59429.12511634827, 'accumulated_submission_time': 54664.61124134064, 'accumulated_eval_time': 4753.07851433754, 'accumulated_logging_time': 5.1212615966796875, 'global_step': 120462, 'preemption_count': 0}), (121388, {'train/accuracy': 0.7331249713897705, 'train/loss': 1.1577279567718506, 'validation/accuracy': 0.6822599768638611, 'validation/loss': 1.3907045125961304, 'validation/num_examples': 50000, 'test/accuracy': 0.5573000311851501, 'test/loss': 2.0155694484710693, 'test/num_examples': 10000, 'score': 55084.6750433445, 'total_duration': 59890.00051164627, 'accumulated_submission_time': 55084.6750433445, 'accumulated_eval_time': 4793.797434568405, 'accumulated_logging_time': 5.165852785110474, 'global_step': 121388, 'preemption_count': 0}), (122313, {'train/accuracy': 0.7346875071525574, 'train/loss': 1.1657814979553223, 'validation/accuracy': 0.675599992275238, 'validation/loss': 1.4246152639389038, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 2.026743173599243, 'test/num_examples': 10000, 'score': 55504.61960840225, 'total_duration': 60348.186989068985, 'accumulated_submission_time': 55504.61960840225, 'accumulated_eval_time': 4831.9498081207275, 'accumulated_logging_time': 5.2081685066223145, 'global_step': 122313, 'preemption_count': 0}), (123236, {'train/accuracy': 0.7521093487739563, 'train/loss': 1.1196391582489014, 'validation/accuracy': 0.6818400025367737, 'validation/loss': 1.4134577512741089, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.019491672515869, 'test/num_examples': 10000, 'score': 55924.80131602287, 'total_duration': 60808.3349378109, 'accumulated_submission_time': 55924.80131602287, 'accumulated_eval_time': 4871.8153512477875, 'accumulated_logging_time': 5.260644197463989, 'global_step': 123236, 'preemption_count': 0}), (124159, {'train/accuracy': 0.7383007407188416, 'train/loss': 1.1520472764968872, 'validation/accuracy': 0.6840800046920776, 'validation/loss': 1.3852416276931763, 'validation/num_examples': 50000, 'test/accuracy': 0.560200035572052, 'test/loss': 2.0003364086151123, 'test/num_examples': 10000, 'score': 56345.114104270935, 'total_duration': 61269.690590143204, 'accumulated_submission_time': 56345.114104270935, 'accumulated_eval_time': 4912.7687220573425, 'accumulated_logging_time': 5.302764654159546, 'global_step': 124159, 'preemption_count': 0}), (125086, {'train/accuracy': 0.7397655844688416, 'train/loss': 1.1573693752288818, 'validation/accuracy': 0.6845999956130981, 'validation/loss': 1.4100217819213867, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.0429952144622803, 'test/num_examples': 10000, 'score': 56765.40418744087, 'total_duration': 61728.776727199554, 'accumulated_submission_time': 56765.40418744087, 'accumulated_eval_time': 4951.472690105438, 'accumulated_logging_time': 5.346803903579712, 'global_step': 125086, 'preemption_count': 0}), (126011, {'train/accuracy': 0.7486132383346558, 'train/loss': 1.118463397026062, 'validation/accuracy': 0.6848799586296082, 'validation/loss': 1.392722725868225, 'validation/num_examples': 50000, 'test/accuracy': 0.5542000532150269, 'test/loss': 2.0321836471557617, 'test/num_examples': 10000, 'score': 57185.52679872513, 'total_duration': 62190.26622200012, 'accumulated_submission_time': 57185.52679872513, 'accumulated_eval_time': 4992.742039680481, 'accumulated_logging_time': 5.39652943611145, 'global_step': 126011, 'preemption_count': 0}), (126934, {'train/accuracy': 0.7451757788658142, 'train/loss': 1.1258550882339478, 'validation/accuracy': 0.686739981174469, 'validation/loss': 1.3771331310272217, 'validation/num_examples': 50000, 'test/accuracy': 0.5667000412940979, 'test/loss': 1.988448977470398, 'test/num_examples': 10000, 'score': 57605.49915289879, 'total_duration': 62649.33008289337, 'accumulated_submission_time': 57605.49915289879, 'accumulated_eval_time': 5031.7367787361145, 'accumulated_logging_time': 5.445305347442627, 'global_step': 126934, 'preemption_count': 0}), (127859, {'train/accuracy': 0.7473437190055847, 'train/loss': 1.1448750495910645, 'validation/accuracy': 0.6888999938964844, 'validation/loss': 1.3919219970703125, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.017214298248291, 'test/num_examples': 10000, 'score': 58025.73322200775, 'total_duration': 63110.56328248978, 'accumulated_submission_time': 58025.73322200775, 'accumulated_eval_time': 5072.641215085983, 'accumulated_logging_time': 5.489341497421265, 'global_step': 127859, 'preemption_count': 0}), (128782, {'train/accuracy': 0.753710925579071, 'train/loss': 1.085994005203247, 'validation/accuracy': 0.6908999681472778, 'validation/loss': 1.3590346574783325, 'validation/num_examples': 50000, 'test/accuracy': 0.5646000504493713, 'test/loss': 1.9902604818344116, 'test/num_examples': 10000, 'score': 58445.72785902023, 'total_duration': 63571.92673802376, 'accumulated_submission_time': 58445.72785902023, 'accumulated_eval_time': 5113.915201663971, 'accumulated_logging_time': 5.5364158153533936, 'global_step': 128782, 'preemption_count': 0}), (129707, {'train/accuracy': 0.7651562094688416, 'train/loss': 1.0557949542999268, 'validation/accuracy': 0.6894400119781494, 'validation/loss': 1.376419186592102, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 2.0006537437438965, 'test/num_examples': 10000, 'score': 58865.84973907471, 'total_duration': 64032.96802806854, 'accumulated_submission_time': 58865.84973907471, 'accumulated_eval_time': 5154.739242553711, 'accumulated_logging_time': 5.584019184112549, 'global_step': 129707, 'preemption_count': 0}), (130627, {'train/accuracy': 0.7515429258346558, 'train/loss': 1.082430362701416, 'validation/accuracy': 0.6952399611473083, 'validation/loss': 1.3379631042480469, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 1.951003074645996, 'test/num_examples': 10000, 'score': 59285.97770404816, 'total_duration': 64494.23661541939, 'accumulated_submission_time': 59285.97770404816, 'accumulated_eval_time': 5195.786374568939, 'accumulated_logging_time': 5.630152940750122, 'global_step': 130627, 'preemption_count': 0}), (131551, {'train/accuracy': 0.7570898532867432, 'train/loss': 1.0904736518859863, 'validation/accuracy': 0.6944199800491333, 'validation/loss': 1.3673386573791504, 'validation/num_examples': 50000, 'test/accuracy': 0.5703999996185303, 'test/loss': 1.9871017932891846, 'test/num_examples': 10000, 'score': 59706.242832660675, 'total_duration': 64950.979976415634, 'accumulated_submission_time': 59706.242832660675, 'accumulated_eval_time': 5232.168748378754, 'accumulated_logging_time': 5.6784138679504395, 'global_step': 131551, 'preemption_count': 0}), (132475, {'train/accuracy': 0.769238293170929, 'train/loss': 1.0069472789764404, 'validation/accuracy': 0.7003200054168701, 'validation/loss': 1.3125197887420654, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.9146699905395508, 'test/num_examples': 10000, 'score': 60126.579996824265, 'total_duration': 65411.887921094894, 'accumulated_submission_time': 60126.579996824265, 'accumulated_eval_time': 5272.644042253494, 'accumulated_logging_time': 5.7267396450042725, 'global_step': 132475, 'preemption_count': 0}), (133401, {'train/accuracy': 0.757519543170929, 'train/loss': 1.062558889389038, 'validation/accuracy': 0.6997599601745605, 'validation/loss': 1.3051460981369019, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.93559992313385, 'test/num_examples': 10000, 'score': 60547.2616622448, 'total_duration': 65874.38661026955, 'accumulated_submission_time': 60547.2616622448, 'accumulated_eval_time': 5314.366637706757, 'accumulated_logging_time': 5.773090362548828, 'global_step': 133401, 'preemption_count': 0}), (134327, {'train/accuracy': 0.7621874809265137, 'train/loss': 1.0537630319595337, 'validation/accuracy': 0.700980007648468, 'validation/loss': 1.31785249710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9424078464508057, 'test/num_examples': 10000, 'score': 60967.648696899414, 'total_duration': 66331.51563692093, 'accumulated_submission_time': 60967.648696899414, 'accumulated_eval_time': 5351.010791301727, 'accumulated_logging_time': 5.823288679122925, 'global_step': 134327, 'preemption_count': 0}), (135250, {'train/accuracy': 0.7675390243530273, 'train/loss': 1.0237082242965698, 'validation/accuracy': 0.6987599730491638, 'validation/loss': 1.3225077390670776, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9518496990203857, 'test/num_examples': 10000, 'score': 61387.66164803505, 'total_duration': 66792.39415669441, 'accumulated_submission_time': 61387.66164803505, 'accumulated_eval_time': 5391.782269239426, 'accumulated_logging_time': 5.869398355484009, 'global_step': 135250, 'preemption_count': 0}), (136174, {'train/accuracy': 0.764843761920929, 'train/loss': 1.0483845472335815, 'validation/accuracy': 0.7023999691009521, 'validation/loss': 1.3212941884994507, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.929173469543457, 'test/num_examples': 10000, 'score': 61807.8627281189, 'total_duration': 67251.33851337433, 'accumulated_submission_time': 61807.8627281189, 'accumulated_eval_time': 5430.429076910019, 'accumulated_logging_time': 5.918476819992065, 'global_step': 136174, 'preemption_count': 0}), (137101, {'train/accuracy': 0.7681640386581421, 'train/loss': 1.0459219217300415, 'validation/accuracy': 0.7051399946212769, 'validation/loss': 1.3112800121307373, 'validation/num_examples': 50000, 'test/accuracy': 0.58160001039505, 'test/loss': 1.9345344305038452, 'test/num_examples': 10000, 'score': 62228.401629686356, 'total_duration': 67711.19112372398, 'accumulated_submission_time': 62228.401629686356, 'accumulated_eval_time': 5469.642811059952, 'accumulated_logging_time': 5.971429824829102, 'global_step': 137101, 'preemption_count': 0}), (138024, {'train/accuracy': 0.7739452719688416, 'train/loss': 1.0145626068115234, 'validation/accuracy': 0.706559956073761, 'validation/loss': 1.2995346784591675, 'validation/num_examples': 50000, 'test/accuracy': 0.5817000269889832, 'test/loss': 1.9207830429077148, 'test/num_examples': 10000, 'score': 62648.38740777969, 'total_duration': 68172.46992921829, 'accumulated_submission_time': 62648.38740777969, 'accumulated_eval_time': 5510.844088315964, 'accumulated_logging_time': 6.016285419464111, 'global_step': 138024, 'preemption_count': 0}), (138947, {'train/accuracy': 0.7826952934265137, 'train/loss': 0.9653533697128296, 'validation/accuracy': 0.7051399946212769, 'validation/loss': 1.3022921085357666, 'validation/num_examples': 50000, 'test/accuracy': 0.579200029373169, 'test/loss': 1.9257780313491821, 'test/num_examples': 10000, 'score': 63068.49566245079, 'total_duration': 68633.90044283867, 'accumulated_submission_time': 63068.49566245079, 'accumulated_eval_time': 5552.058122396469, 'accumulated_logging_time': 6.076954126358032, 'global_step': 138947, 'preemption_count': 0}), (139869, {'train/accuracy': 0.7697656154632568, 'train/loss': 1.0047340393066406, 'validation/accuracy': 0.7098000049591064, 'validation/loss': 1.2667945623397827, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8732690811157227, 'test/num_examples': 10000, 'score': 63488.75289964676, 'total_duration': 69093.70464968681, 'accumulated_submission_time': 63488.75289964676, 'accumulated_eval_time': 5591.511062860489, 'accumulated_logging_time': 6.124045372009277, 'global_step': 139869, 'preemption_count': 0}), (140795, {'train/accuracy': 0.7763866782188416, 'train/loss': 0.9946498870849609, 'validation/accuracy': 0.7087399959564209, 'validation/loss': 1.28446626663208, 'validation/num_examples': 50000, 'test/accuracy': 0.5814000368118286, 'test/loss': 1.9018096923828125, 'test/num_examples': 10000, 'score': 63908.82472872734, 'total_duration': 69551.93505644798, 'accumulated_submission_time': 63908.82472872734, 'accumulated_eval_time': 5629.565707683563, 'accumulated_logging_time': 6.17889142036438, 'global_step': 140795, 'preemption_count': 0}), (141719, {'train/accuracy': 0.7819921970367432, 'train/loss': 0.9503366351127625, 'validation/accuracy': 0.7135999798774719, 'validation/loss': 1.2518689632415771, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.8770052194595337, 'test/num_examples': 10000, 'score': 64329.11201548576, 'total_duration': 70011.48867511749, 'accumulated_submission_time': 64329.11201548576, 'accumulated_eval_time': 5668.733921766281, 'accumulated_logging_time': 6.22956919670105, 'global_step': 141719, 'preemption_count': 0}), (142641, {'train/accuracy': 0.7760937213897705, 'train/loss': 0.9692516326904297, 'validation/accuracy': 0.7108399868011475, 'validation/loss': 1.23958420753479, 'validation/num_examples': 50000, 'test/accuracy': 0.5865000486373901, 'test/loss': 1.8613325357437134, 'test/num_examples': 10000, 'score': 64749.359623909, 'total_duration': 70469.93096780777, 'accumulated_submission_time': 64749.359623909, 'accumulated_eval_time': 5706.832077026367, 'accumulated_logging_time': 6.278799533843994, 'global_step': 142641, 'preemption_count': 0}), (143566, {'train/accuracy': 0.778613269329071, 'train/loss': 0.9984338283538818, 'validation/accuracy': 0.7130399942398071, 'validation/loss': 1.2804381847381592, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8944753408432007, 'test/num_examples': 10000, 'score': 65169.36147618294, 'total_duration': 70930.76969718933, 'accumulated_submission_time': 65169.36147618294, 'accumulated_eval_time': 5747.56706738472, 'accumulated_logging_time': 6.333415985107422, 'global_step': 143566, 'preemption_count': 0}), (144491, {'train/accuracy': 0.7850390672683716, 'train/loss': 0.9747884273529053, 'validation/accuracy': 0.7129600048065186, 'validation/loss': 1.2740776538848877, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.884268045425415, 'test/num_examples': 10000, 'score': 65589.43929386139, 'total_duration': 71392.52113199234, 'accumulated_submission_time': 65589.43929386139, 'accumulated_eval_time': 5789.146682500839, 'accumulated_logging_time': 6.380536079406738, 'global_step': 144491, 'preemption_count': 0}), (145414, {'train/accuracy': 0.77845698595047, 'train/loss': 0.9919044971466064, 'validation/accuracy': 0.7156599760055542, 'validation/loss': 1.2642072439193726, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8826606273651123, 'test/num_examples': 10000, 'score': 66009.40965223312, 'total_duration': 71853.4624812603, 'accumulated_submission_time': 66009.40965223312, 'accumulated_eval_time': 5830.013171672821, 'accumulated_logging_time': 6.438591480255127, 'global_step': 145414, 'preemption_count': 0}), (146341, {'train/accuracy': 0.7854687571525574, 'train/loss': 0.936896562576294, 'validation/accuracy': 0.7184799909591675, 'validation/loss': 1.2213993072509766, 'validation/num_examples': 50000, 'test/accuracy': 0.5951000452041626, 'test/loss': 1.8369319438934326, 'test/num_examples': 10000, 'score': 66429.68575835228, 'total_duration': 72311.18190431595, 'accumulated_submission_time': 66429.68575835228, 'accumulated_eval_time': 5867.35782957077, 'accumulated_logging_time': 6.490723133087158, 'global_step': 146341, 'preemption_count': 0}), (147267, {'train/accuracy': 0.791308581829071, 'train/loss': 0.9221143126487732, 'validation/accuracy': 0.7231599688529968, 'validation/loss': 1.2180984020233154, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.8256595134735107, 'test/num_examples': 10000, 'score': 66849.90239930153, 'total_duration': 72770.54873371124, 'accumulated_submission_time': 66849.90239930153, 'accumulated_eval_time': 5906.406289815903, 'accumulated_logging_time': 6.545153379440308, 'global_step': 147267, 'preemption_count': 0}), (148193, {'train/accuracy': 0.8017187118530273, 'train/loss': 0.8867132663726807, 'validation/accuracy': 0.7227199673652649, 'validation/loss': 1.2239965200424194, 'validation/num_examples': 50000, 'test/accuracy': 0.6012000441551208, 'test/loss': 1.8258556127548218, 'test/num_examples': 10000, 'score': 67269.83102846146, 'total_duration': 73231.43658638, 'accumulated_submission_time': 67269.83102846146, 'accumulated_eval_time': 5947.267498254776, 'accumulated_logging_time': 6.596628427505493, 'global_step': 148193, 'preemption_count': 0}), (149120, {'train/accuracy': 0.7932226657867432, 'train/loss': 0.906248927116394, 'validation/accuracy': 0.7246599793434143, 'validation/loss': 1.2037746906280518, 'validation/num_examples': 50000, 'test/accuracy': 0.6017000079154968, 'test/loss': 1.801848292350769, 'test/num_examples': 10000, 'score': 67690.06477189064, 'total_duration': 73689.20619821548, 'accumulated_submission_time': 67690.06477189064, 'accumulated_eval_time': 5984.706177949905, 'accumulated_logging_time': 6.646602392196655, 'global_step': 149120, 'preemption_count': 0}), (150046, {'train/accuracy': 0.7907617092132568, 'train/loss': 0.9388483762741089, 'validation/accuracy': 0.7214599847793579, 'validation/loss': 1.2328137159347534, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.8345398902893066, 'test/num_examples': 10000, 'score': 68110.0372145176, 'total_duration': 74148.33674430847, 'accumulated_submission_time': 68110.0372145176, 'accumulated_eval_time': 6023.770362854004, 'accumulated_logging_time': 6.693514585494995, 'global_step': 150046, 'preemption_count': 0}), (150968, {'train/accuracy': 0.8061718344688416, 'train/loss': 0.8653810024261475, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.1898913383483887, 'validation/num_examples': 50000, 'test/accuracy': 0.6043000221252441, 'test/loss': 1.8021141290664673, 'test/num_examples': 10000, 'score': 68529.95536541939, 'total_duration': 74611.0534362793, 'accumulated_submission_time': 68529.95536541939, 'accumulated_eval_time': 6066.470200300217, 'accumulated_logging_time': 6.744734525680542, 'global_step': 150968, 'preemption_count': 0}), (151891, {'train/accuracy': 0.7955273389816284, 'train/loss': 0.8970678448677063, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.1849876642227173, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.778713345527649, 'test/num_examples': 10000, 'score': 68950.07535648346, 'total_duration': 75072.84614467621, 'accumulated_submission_time': 68950.07535648346, 'accumulated_eval_time': 6108.044515609741, 'accumulated_logging_time': 6.796485185623169, 'global_step': 151891, 'preemption_count': 0}), (152814, {'train/accuracy': 0.8013476133346558, 'train/loss': 0.8696554899215698, 'validation/accuracy': 0.7305999994277954, 'validation/loss': 1.1746702194213867, 'validation/num_examples': 50000, 'test/accuracy': 0.6066000461578369, 'test/loss': 1.7887028455734253, 'test/num_examples': 10000, 'score': 69370.09718680382, 'total_duration': 75533.73214793205, 'accumulated_submission_time': 69370.09718680382, 'accumulated_eval_time': 6148.80455327034, 'accumulated_logging_time': 6.853741884231567, 'global_step': 152814, 'preemption_count': 0}), (153736, {'train/accuracy': 0.8064843416213989, 'train/loss': 0.8482201099395752, 'validation/accuracy': 0.7335599660873413, 'validation/loss': 1.1666568517684937, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.7751833200454712, 'test/num_examples': 10000, 'score': 69790.06775164604, 'total_duration': 75993.92200398445, 'accumulated_submission_time': 69790.06775164604, 'accumulated_eval_time': 6188.927048921585, 'accumulated_logging_time': 6.903278827667236, 'global_step': 153736, 'preemption_count': 0}), (154662, {'train/accuracy': 0.804492175579071, 'train/loss': 0.8526339530944824, 'validation/accuracy': 0.7369999885559082, 'validation/loss': 1.1511931419372559, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.75464928150177, 'test/num_examples': 10000, 'score': 70210.01730847359, 'total_duration': 76454.94499826431, 'accumulated_submission_time': 70210.01730847359, 'accumulated_eval_time': 6229.90651512146, 'accumulated_logging_time': 6.94967794418335, 'global_step': 154662, 'preemption_count': 0}), (155584, {'train/accuracy': 0.811328113079071, 'train/loss': 0.8368220329284668, 'validation/accuracy': 0.738860011100769, 'validation/loss': 1.1395392417907715, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.73326575756073, 'test/num_examples': 10000, 'score': 70630.25607657433, 'total_duration': 76916.89459323883, 'accumulated_submission_time': 70630.25607657433, 'accumulated_eval_time': 6271.519709348679, 'accumulated_logging_time': 6.999774217605591, 'global_step': 155584, 'preemption_count': 0}), (156508, {'train/accuracy': 0.8138476610183716, 'train/loss': 0.8137789964675903, 'validation/accuracy': 0.7384200096130371, 'validation/loss': 1.1399335861206055, 'validation/num_examples': 50000, 'test/accuracy': 0.6134000420570374, 'test/loss': 1.7455615997314453, 'test/num_examples': 10000, 'score': 71050.56174874306, 'total_duration': 77375.42791485786, 'accumulated_submission_time': 71050.56174874306, 'accumulated_eval_time': 6309.6483726501465, 'accumulated_logging_time': 7.052521228790283, 'global_step': 156508, 'preemption_count': 0}), (157431, {'train/accuracy': 0.8206444978713989, 'train/loss': 0.7946438193321228, 'validation/accuracy': 0.7416599988937378, 'validation/loss': 1.1222891807556152, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.722597599029541, 'test/num_examples': 10000, 'score': 71470.68224668503, 'total_duration': 77835.01753425598, 'accumulated_submission_time': 71470.68224668503, 'accumulated_eval_time': 6349.01679110527, 'accumulated_logging_time': 7.105899810791016, 'global_step': 157431, 'preemption_count': 0}), (158285, {'train/accuracy': 0.81201171875, 'train/loss': 0.8173814415931702, 'validation/accuracy': 0.7415399551391602, 'validation/loss': 1.131630539894104, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.7233086824417114, 'test/num_examples': 10000, 'score': 71890.93097662926, 'total_duration': 78297.10611104965, 'accumulated_submission_time': 71890.93097662926, 'accumulated_eval_time': 6390.761610031128, 'accumulated_logging_time': 7.158337116241455, 'global_step': 158285, 'preemption_count': 0}), (159207, {'train/accuracy': 0.8217968344688416, 'train/loss': 0.7972414493560791, 'validation/accuracy': 0.7410199642181396, 'validation/loss': 1.1312137842178345, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.7257004976272583, 'test/num_examples': 10000, 'score': 72310.98796439171, 'total_duration': 78753.62868118286, 'accumulated_submission_time': 72310.98796439171, 'accumulated_eval_time': 6427.128840446472, 'accumulated_logging_time': 7.209970235824585, 'global_step': 159207, 'preemption_count': 0}), (160129, {'train/accuracy': 0.8250585794448853, 'train/loss': 0.7695590257644653, 'validation/accuracy': 0.7425199747085571, 'validation/loss': 1.1162033081054688, 'validation/num_examples': 50000, 'test/accuracy': 0.6238000392913818, 'test/loss': 1.7093664407730103, 'test/num_examples': 10000, 'score': 72731.21923279762, 'total_duration': 79214.68068003654, 'accumulated_submission_time': 72731.21923279762, 'accumulated_eval_time': 6467.8501443862915, 'accumulated_logging_time': 7.2619194984436035, 'global_step': 160129, 'preemption_count': 0}), (161054, {'train/accuracy': 0.8220117092132568, 'train/loss': 0.7986540794372559, 'validation/accuracy': 0.7463399767875671, 'validation/loss': 1.1236673593521118, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.7323718070983887, 'test/num_examples': 10000, 'score': 73151.4669148922, 'total_duration': 79675.36319732666, 'accumulated_submission_time': 73151.4669148922, 'accumulated_eval_time': 6508.185884714127, 'accumulated_logging_time': 7.314181804656982, 'global_step': 161054, 'preemption_count': 0}), (161979, {'train/accuracy': 0.82289057970047, 'train/loss': 0.774276852607727, 'validation/accuracy': 0.7457799911499023, 'validation/loss': 1.096876859664917, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.6988554000854492, 'test/num_examples': 10000, 'score': 73571.6182346344, 'total_duration': 80134.4779574871, 'accumulated_submission_time': 73571.6182346344, 'accumulated_eval_time': 6547.046494960785, 'accumulated_logging_time': 7.370314836502075, 'global_step': 161979, 'preemption_count': 0}), (162902, {'train/accuracy': 0.82958984375, 'train/loss': 0.768218994140625, 'validation/accuracy': 0.7471399903297424, 'validation/loss': 1.1131218671798706, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.7115119695663452, 'test/num_examples': 10000, 'score': 73991.6619336605, 'total_duration': 80594.78010439873, 'accumulated_submission_time': 73991.6619336605, 'accumulated_eval_time': 6587.196877479553, 'accumulated_logging_time': 7.43206524848938, 'global_step': 162902, 'preemption_count': 0}), (163829, {'train/accuracy': 0.8261523246765137, 'train/loss': 0.7748673558235168, 'validation/accuracy': 0.747219979763031, 'validation/loss': 1.0991054773330688, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.695138692855835, 'test/num_examples': 10000, 'score': 74412.06664991379, 'total_duration': 81053.74798631668, 'accumulated_submission_time': 74412.06664991379, 'accumulated_eval_time': 6625.662905454636, 'accumulated_logging_time': 7.482219219207764, 'global_step': 163829, 'preemption_count': 0}), (164754, {'train/accuracy': 0.8268163800239563, 'train/loss': 0.7791228890419006, 'validation/accuracy': 0.7498199939727783, 'validation/loss': 1.0978347063064575, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.695865511894226, 'test/num_examples': 10000, 'score': 74831.999625206, 'total_duration': 81513.33116149902, 'accumulated_submission_time': 74831.999625206, 'accumulated_eval_time': 6665.215897798538, 'accumulated_logging_time': 7.531557321548462, 'global_step': 164754, 'preemption_count': 0}), (165676, {'train/accuracy': 0.8290429711341858, 'train/loss': 0.7599804997444153, 'validation/accuracy': 0.749239981174469, 'validation/loss': 1.0925363302230835, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.6812580823898315, 'test/num_examples': 10000, 'score': 75252.15470719337, 'total_duration': 81971.07764077187, 'accumulated_submission_time': 75252.15470719337, 'accumulated_eval_time': 6702.711914539337, 'accumulated_logging_time': 7.5804502964019775, 'global_step': 165676, 'preemption_count': 0}), (166600, {'train/accuracy': 0.8282421827316284, 'train/loss': 0.7497026920318604, 'validation/accuracy': 0.7534399628639221, 'validation/loss': 1.0718141794204712, 'validation/num_examples': 50000, 'test/accuracy': 0.6350000500679016, 'test/loss': 1.662460446357727, 'test/num_examples': 10000, 'score': 75672.05558896065, 'total_duration': 82432.52755188942, 'accumulated_submission_time': 75672.05558896065, 'accumulated_eval_time': 6744.158909320831, 'accumulated_logging_time': 7.63471245765686, 'global_step': 166600, 'preemption_count': 0}), (167525, {'train/accuracy': 0.8338280916213989, 'train/loss': 0.7270570993423462, 'validation/accuracy': 0.7521399855613708, 'validation/loss': 1.0613549947738647, 'validation/num_examples': 50000, 'test/accuracy': 0.6344000101089478, 'test/loss': 1.6501274108886719, 'test/num_examples': 10000, 'score': 76092.31971812248, 'total_duration': 82892.44341945648, 'accumulated_submission_time': 76092.31971812248, 'accumulated_eval_time': 6783.7116050720215, 'accumulated_logging_time': 7.687152862548828, 'global_step': 167525, 'preemption_count': 0}), (168447, {'train/accuracy': 0.8368163704872131, 'train/loss': 0.721832275390625, 'validation/accuracy': 0.755840003490448, 'validation/loss': 1.0656956434249878, 'validation/num_examples': 50000, 'test/accuracy': 0.6376000046730042, 'test/loss': 1.6524871587753296, 'test/num_examples': 10000, 'score': 76512.35372972488, 'total_duration': 83350.1652495861, 'accumulated_submission_time': 76512.35372972488, 'accumulated_eval_time': 6821.297668457031, 'accumulated_logging_time': 7.742358684539795, 'global_step': 168447, 'preemption_count': 0}), (169369, {'train/accuracy': 0.8389843702316284, 'train/loss': 0.7196028828620911, 'validation/accuracy': 0.7570399641990662, 'validation/loss': 1.0582653284072876, 'validation/num_examples': 50000, 'test/accuracy': 0.6318000555038452, 'test/loss': 1.6552094221115112, 'test/num_examples': 10000, 'score': 76932.46559858322, 'total_duration': 83811.53285884857, 'accumulated_submission_time': 76932.46559858322, 'accumulated_eval_time': 6862.45266699791, 'accumulated_logging_time': 7.796396732330322, 'global_step': 169369, 'preemption_count': 0}), (170291, {'train/accuracy': 0.8350781202316284, 'train/loss': 0.7398861646652222, 'validation/accuracy': 0.7601999640464783, 'validation/loss': 1.0654001235961914, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.660707950592041, 'test/num_examples': 10000, 'score': 77352.67522978783, 'total_duration': 84272.98402452469, 'accumulated_submission_time': 77352.67522978783, 'accumulated_eval_time': 6903.587861776352, 'accumulated_logging_time': 7.856116533279419, 'global_step': 170291, 'preemption_count': 0})], 'global_step': 170666}
I0131 13:16:10.120717 139936116377408 submission_runner.py:586] Timing: 77520.27442193031
I0131 13:16:10.120808 139936116377408 submission_runner.py:588] Total number of evals: 185
I0131 13:16:10.120852 139936116377408 submission_runner.py:589] ====================
I0131 13:16:10.120898 139936116377408 submission_runner.py:542] Using RNG seed 2064292405
I0131 13:16:10.122462 139936116377408 submission_runner.py:551] --- Tuning run 2/5 ---
I0131 13:16:10.122575 139936116377408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_2.
I0131 13:16:10.129538 139936116377408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_2/hparams.json.
I0131 13:16:10.131297 139936116377408 submission_runner.py:206] Initializing dataset.
I0131 13:16:10.145263 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0131 13:16:10.158680 139936116377408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0131 13:16:10.368821 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0131 13:16:14.580471 139936116377408 submission_runner.py:213] Initializing model.
I0131 13:16:21.236317 139936116377408 submission_runner.py:255] Initializing optimizer.
I0131 13:16:21.694750 139936116377408 submission_runner.py:262] Initializing metrics bundle.
I0131 13:16:21.694912 139936116377408 submission_runner.py:280] Initializing checkpoint and logger.
I0131 13:16:21.789676 139936116377408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_2 with prefix checkpoint_
I0131 13:16:21.789808 139936116377408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0131 13:16:37.634857 139936116377408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0131 13:16:53.129188 139936116377408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_2/flags_0.json.
I0131 13:16:53.141249 139936116377408 submission_runner.py:314] Starting training loop.
I0131 13:17:28.974236 139774392231680 logging_writer.py:48] [0] global_step=0, grad_norm=0.29237180948257446, loss=6.9077534675598145
I0131 13:17:28.984929 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:17:37.211447 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:17:54.099421 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:17:55.712600 139936116377408 submission_runner.py:408] Time since start: 62.57s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.84359121322632, 'total_duration': 62.57128691673279, 'accumulated_submission_time': 35.84359121322632, 'accumulated_eval_time': 26.727606058120728, 'accumulated_logging_time': 0}
I0131 13:17:55.722695 139774400624384 logging_writer.py:48] [1] accumulated_eval_time=26.727606, accumulated_logging_time=0, accumulated_submission_time=35.843591, global_step=1, preemption_count=0, score=35.843591, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=62.571287, train/accuracy=0.000977, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0131 13:19:03.259216 139774434195200 logging_writer.py:48] [100] global_step=100, grad_norm=0.3367437720298767, loss=6.904696464538574
I0131 13:19:50.310485 139774417409792 logging_writer.py:48] [200] global_step=200, grad_norm=0.4063701033592224, loss=6.885836601257324
I0131 13:20:37.687461 139774434195200 logging_writer.py:48] [300] global_step=300, grad_norm=0.4892681837081909, loss=6.84383487701416
I0131 13:21:25.469404 139774417409792 logging_writer.py:48] [400] global_step=400, grad_norm=0.4896983206272125, loss=6.815406322479248
I0131 13:22:12.806060 139774434195200 logging_writer.py:48] [500] global_step=500, grad_norm=0.6018588542938232, loss=6.825890064239502
I0131 13:23:00.250493 139774417409792 logging_writer.py:48] [600] global_step=600, grad_norm=0.6683430671691895, loss=6.748996734619141
I0131 13:23:48.063543 139774434195200 logging_writer.py:48] [700] global_step=700, grad_norm=1.2775777578353882, loss=6.678961277008057
I0131 13:24:35.609376 139774417409792 logging_writer.py:48] [800] global_step=800, grad_norm=0.666730523109436, loss=6.749059200286865
I0131 13:24:55.719081 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:25:08.898008 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:25:37.679702 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:25:39.287008 139936116377408 submission_runner.py:408] Time since start: 526.15s, 	Step: 844, 	{'train/accuracy': 0.0159765612334013, 'train/loss': 6.382034778594971, 'validation/accuracy': 0.01599999889731407, 'validation/loss': 6.395730972290039, 'validation/num_examples': 50000, 'test/accuracy': 0.01140000019222498, 'test/loss': 6.438459873199463, 'test/num_examples': 10000, 'score': 455.78622579574585, 'total_duration': 526.145667552948, 'accumulated_submission_time': 455.78622579574585, 'accumulated_eval_time': 70.29547381401062, 'accumulated_logging_time': 0.020416259765625}
I0131 13:25:39.307364 139774434195200 logging_writer.py:48] [844] accumulated_eval_time=70.295474, accumulated_logging_time=0.020416, accumulated_submission_time=455.786226, global_step=844, preemption_count=0, score=455.786226, test/accuracy=0.011400, test/loss=6.438460, test/num_examples=10000, total_duration=526.145668, train/accuracy=0.015977, train/loss=6.382035, validation/accuracy=0.016000, validation/loss=6.395731, validation/num_examples=50000
I0131 13:26:01.903610 139774417409792 logging_writer.py:48] [900] global_step=900, grad_norm=0.8556214570999146, loss=6.703063011169434
I0131 13:26:46.116742 139774434195200 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1817166805267334, loss=6.581580638885498
I0131 13:27:32.347404 139774417409792 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2813425064086914, loss=6.789424896240234
I0131 13:28:18.782488 139774434195200 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2563598155975342, loss=6.507691383361816
I0131 13:29:05.340394 139774417409792 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2946510314941406, loss=6.76340389251709
I0131 13:29:51.642524 139774434195200 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.046439290046692, loss=6.5238938331604
I0131 13:30:38.260176 139774417409792 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.036930799484253, loss=6.528967380523682
I0131 13:31:24.174007 139774434195200 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.0911989212036133, loss=6.35222864151001
I0131 13:32:10.852363 139774417409792 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.1451917886734009, loss=6.307199954986572
I0131 13:32:39.706925 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:32:53.092635 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:33:18.235341 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:33:19.844153 139936116377408 submission_runner.py:408] Time since start: 986.70s, 	Step: 1764, 	{'train/accuracy': 0.04486327990889549, 'train/loss': 5.828395366668701, 'validation/accuracy': 0.04315999895334244, 'validation/loss': 5.85699987411499, 'validation/num_examples': 50000, 'test/accuracy': 0.03350000083446503, 'test/loss': 5.981106758117676, 'test/num_examples': 10000, 'score': 876.1266677379608, 'total_duration': 986.7028484344482, 'accumulated_submission_time': 876.1266677379608, 'accumulated_eval_time': 110.43268370628357, 'accumulated_logging_time': 0.0518798828125}
I0131 13:33:19.858076 139774434195200 logging_writer.py:48] [1764] accumulated_eval_time=110.432684, accumulated_logging_time=0.051880, accumulated_submission_time=876.126668, global_step=1764, preemption_count=0, score=876.126668, test/accuracy=0.033500, test/loss=5.981107, test/num_examples=10000, total_duration=986.702848, train/accuracy=0.044863, train/loss=5.828395, validation/accuracy=0.043160, validation/loss=5.857000, validation/num_examples=50000
I0131 13:33:34.551656 139774417409792 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0708144903182983, loss=6.337923049926758
I0131 13:34:17.702289 139774434195200 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.495741844177246, loss=6.61985969543457
I0131 13:35:03.922334 139774417409792 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2915621995925903, loss=6.694345951080322
I0131 13:35:50.498833 139774434195200 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1788051128387451, loss=6.270634174346924
I0131 13:36:36.496245 139774417409792 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8637410998344421, loss=6.486799716949463
I0131 13:37:22.536716 139774434195200 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0256160497665405, loss=6.255561828613281
I0131 13:38:08.884242 139774417409792 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.5580543279647827, loss=6.192541122436523
I0131 13:38:55.202756 139774434195200 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6746224761009216, loss=6.609699726104736
I0131 13:39:41.603267 139774417409792 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.3786989450454712, loss=6.4596967697143555
I0131 13:40:20.057821 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:40:32.905310 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:41:02.399275 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:41:04.000434 139936116377408 submission_runner.py:408] Time since start: 1450.86s, 	Step: 2684, 	{'train/accuracy': 0.07359375059604645, 'train/loss': 5.448266983032227, 'validation/accuracy': 0.06747999787330627, 'validation/loss': 5.489231586456299, 'validation/num_examples': 50000, 'test/accuracy': 0.05020000413060188, 'test/loss': 5.668014049530029, 'test/num_examples': 10000, 'score': 1296.2641365528107, 'total_duration': 1450.8591334819794, 'accumulated_submission_time': 1296.2641365528107, 'accumulated_eval_time': 154.37527751922607, 'accumulated_logging_time': 0.07917928695678711}
I0131 13:41:04.015962 139774434195200 logging_writer.py:48] [2684] accumulated_eval_time=154.375278, accumulated_logging_time=0.079179, accumulated_submission_time=1296.264137, global_step=2684, preemption_count=0, score=1296.264137, test/accuracy=0.050200, test/loss=5.668014, test/num_examples=10000, total_duration=1450.859133, train/accuracy=0.073594, train/loss=5.448267, validation/accuracy=0.067480, validation/loss=5.489232, validation/num_examples=50000
I0131 13:41:10.753920 139774417409792 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9207566380500793, loss=6.307884693145752
I0131 13:41:53.074579 139774434195200 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.3201079368591309, loss=6.195891380310059
I0131 13:42:39.342480 139774417409792 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.92868572473526, loss=6.148087024688721
I0131 13:43:25.706755 139774434195200 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2900716066360474, loss=6.098473072052002
I0131 13:44:11.582063 139774417409792 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.0412907600402832, loss=6.0167083740234375
I0131 13:44:57.804138 139774434195200 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8665590882301331, loss=6.673191070556641
I0131 13:45:44.069132 139774417409792 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.2239973545074463, loss=6.05173397064209
I0131 13:46:30.243367 139774434195200 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9604957103729248, loss=6.2802414894104
I0131 13:47:16.812997 139774417409792 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.14837646484375, loss=6.01598596572876
I0131 13:48:03.016332 139774434195200 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.150696039199829, loss=6.047162055969238
I0131 13:48:04.111519 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:48:16.629955 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:48:46.899144 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:48:48.511400 139936116377408 submission_runner.py:408] Time since start: 1915.37s, 	Step: 3604, 	{'train/accuracy': 0.11046874523162842, 'train/loss': 5.068248271942139, 'validation/accuracy': 0.10171999782323837, 'validation/loss': 5.126224994659424, 'validation/num_examples': 50000, 'test/accuracy': 0.07670000195503235, 'test/loss': 5.356302261352539, 'test/num_examples': 10000, 'score': 1716.3032870292664, 'total_duration': 1915.3701028823853, 'accumulated_submission_time': 1716.3032870292664, 'accumulated_eval_time': 198.7751681804657, 'accumulated_logging_time': 0.10387563705444336}
I0131 13:48:48.525704 139774417409792 logging_writer.py:48] [3604] accumulated_eval_time=198.775168, accumulated_logging_time=0.103876, accumulated_submission_time=1716.303287, global_step=3604, preemption_count=0, score=1716.303287, test/accuracy=0.076700, test/loss=5.356302, test/num_examples=10000, total_duration=1915.370103, train/accuracy=0.110469, train/loss=5.068248, validation/accuracy=0.101720, validation/loss=5.126225, validation/num_examples=50000
I0131 13:49:28.322390 139774434195200 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1890612840652466, loss=5.95682954788208
I0131 13:50:14.885042 139774417409792 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.022048830986023, loss=5.935439109802246
I0131 13:51:01.003159 139774434195200 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.08446204662323, loss=5.871190071105957
I0131 13:51:47.676100 139774417409792 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9336323738098145, loss=6.594136714935303
I0131 13:52:34.012488 139774434195200 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.1681053638458252, loss=6.600436210632324
I0131 13:53:20.689993 139774417409792 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.036071538925171, loss=5.962074279785156
I0131 13:54:07.153116 139774434195200 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9179452061653137, loss=6.569419860839844
I0131 13:54:53.277837 139774417409792 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.0085870027542114, loss=5.774909019470215
I0131 13:55:39.535593 139774434195200 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9323210120201111, loss=6.3558807373046875
I0131 13:55:48.918216 139936116377408 spec.py:321] Evaluating on the training split.
I0131 13:56:01.607017 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 13:56:31.240339 139936116377408 spec.py:349] Evaluating on the test split.
I0131 13:56:32.846514 139936116377408 submission_runner.py:408] Time since start: 2379.71s, 	Step: 4522, 	{'train/accuracy': 0.1553320288658142, 'train/loss': 4.668953895568848, 'validation/accuracy': 0.13806000351905823, 'validation/loss': 4.754825592041016, 'validation/num_examples': 50000, 'test/accuracy': 0.09940000623464584, 'test/loss': 5.049302577972412, 'test/num_examples': 10000, 'score': 2136.6385333538055, 'total_duration': 2379.705213546753, 'accumulated_submission_time': 2136.6385333538055, 'accumulated_eval_time': 242.7034797668457, 'accumulated_logging_time': 0.12747573852539062}
I0131 13:56:32.861770 139774417409792 logging_writer.py:48] [4522] accumulated_eval_time=242.703480, accumulated_logging_time=0.127476, accumulated_submission_time=2136.638533, global_step=4522, preemption_count=0, score=2136.638533, test/accuracy=0.099400, test/loss=5.049303, test/num_examples=10000, total_duration=2379.705214, train/accuracy=0.155332, train/loss=4.668954, validation/accuracy=0.138060, validation/loss=4.754826, validation/num_examples=50000
I0131 13:57:04.729452 139774434195200 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.3031604290008545, loss=5.737040042877197
I0131 13:57:50.658181 139774417409792 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9964568018913269, loss=5.805342674255371
I0131 13:58:36.751344 139774434195200 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.284596562385559, loss=5.6620378494262695
I0131 13:59:22.858486 139774417409792 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.9833336472511292, loss=6.139472484588623
I0131 14:00:09.217495 139774434195200 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8047263622283936, loss=6.023842811584473
I0131 14:00:55.845206 139774417409792 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.154529333114624, loss=5.74703311920166
I0131 14:01:42.393715 139774434195200 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0859198570251465, loss=5.678483486175537
I0131 14:02:28.635689 139774417409792 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.2426356077194214, loss=5.601573944091797
I0131 14:03:14.881023 139774434195200 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.007133960723877, loss=5.569118022918701
I0131 14:03:32.940799 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:03:45.591737 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:04:11.628319 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:04:13.241283 139936116377408 submission_runner.py:408] Time since start: 2840.10s, 	Step: 5441, 	{'train/accuracy': 0.19896483421325684, 'train/loss': 4.309050559997559, 'validation/accuracy': 0.1846799999475479, 'validation/loss': 4.396907806396484, 'validation/num_examples': 50000, 'test/accuracy': 0.13460001349449158, 'test/loss': 4.739532470703125, 'test/num_examples': 10000, 'score': 2556.660418987274, 'total_duration': 2840.099986076355, 'accumulated_submission_time': 2556.660418987274, 'accumulated_eval_time': 283.0039734840393, 'accumulated_logging_time': 0.15241026878356934}
I0131 14:04:13.259269 139774417409792 logging_writer.py:48] [5441] accumulated_eval_time=283.003973, accumulated_logging_time=0.152410, accumulated_submission_time=2556.660419, global_step=5441, preemption_count=0, score=2556.660419, test/accuracy=0.134600, test/loss=4.739532, test/num_examples=10000, total_duration=2840.099986, train/accuracy=0.198965, train/loss=4.309051, validation/accuracy=0.184680, validation/loss=4.396908, validation/num_examples=50000
I0131 14:04:37.089775 139774434195200 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.1989299058914185, loss=5.5507001876831055
I0131 14:05:21.992159 139774417409792 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.861867368221283, loss=6.445501327514648
I0131 14:06:08.112699 139774434195200 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8330417275428772, loss=6.491645812988281
I0131 14:06:54.201517 139774417409792 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.1059186458587646, loss=5.597587585449219
I0131 14:07:40.423445 139774434195200 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.316179633140564, loss=5.406496524810791
I0131 14:08:26.736096 139774417409792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9892390966415405, loss=5.460799217224121
I0131 14:09:13.003409 139774434195200 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9370166063308716, loss=5.38486909866333
I0131 14:09:58.993174 139774417409792 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.1177237033843994, loss=5.671208381652832
I0131 14:10:45.437904 139774434195200 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8866536021232605, loss=6.446792125701904
I0131 14:11:13.331215 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:11:25.796354 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:11:53.894581 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:11:55.498032 139936116377408 submission_runner.py:408] Time since start: 3302.36s, 	Step: 6362, 	{'train/accuracy': 0.24464842677116394, 'train/loss': 3.974132537841797, 'validation/accuracy': 0.2258399873971939, 'validation/loss': 4.073367118835449, 'validation/num_examples': 50000, 'test/accuracy': 0.1720000058412552, 'test/loss': 4.464690208435059, 'test/num_examples': 10000, 'score': 2976.674590110779, 'total_duration': 3302.356697320938, 'accumulated_submission_time': 2976.674590110779, 'accumulated_eval_time': 325.17077374458313, 'accumulated_logging_time': 0.18027997016906738}
I0131 14:11:55.515916 139774417409792 logging_writer.py:48] [6362] accumulated_eval_time=325.170774, accumulated_logging_time=0.180280, accumulated_submission_time=2976.674590, global_step=6362, preemption_count=0, score=2976.674590, test/accuracy=0.172000, test/loss=4.464690, test/num_examples=10000, total_duration=3302.356697, train/accuracy=0.244648, train/loss=3.974133, validation/accuracy=0.225840, validation/loss=4.073367, validation/num_examples=50000
I0131 14:12:11.002349 139774434195200 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.839730441570282, loss=5.967426300048828
I0131 14:12:54.626116 139774417409792 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.3383994102478027, loss=5.458228588104248
I0131 14:13:40.812513 139774434195200 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9380530714988708, loss=5.364888668060303
I0131 14:14:27.013425 139774417409792 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8957403898239136, loss=5.578315734863281
I0131 14:15:13.002982 139774434195200 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9548847079277039, loss=5.439979076385498
I0131 14:15:58.943618 139774417409792 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.0292357206344604, loss=5.230770111083984
I0131 14:16:45.035641 139774434195200 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9817385077476501, loss=5.297751426696777
I0131 14:17:31.136201 139774417409792 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.476690649986267, loss=5.203582286834717
I0131 14:18:17.204050 139774434195200 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.1061909198760986, loss=5.23721981048584
I0131 14:18:55.837280 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:19:08.649171 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:19:35.121285 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:19:36.727991 139936116377408 submission_runner.py:408] Time since start: 3763.59s, 	Step: 7286, 	{'train/accuracy': 0.29001951217651367, 'train/loss': 3.6638667583465576, 'validation/accuracy': 0.2622399926185608, 'validation/loss': 3.812857151031494, 'validation/num_examples': 50000, 'test/accuracy': 0.1998000144958496, 'test/loss': 4.2360687255859375, 'test/num_examples': 10000, 'score': 3396.9373681545258, 'total_duration': 3763.5866770744324, 'accumulated_submission_time': 3396.9373681545258, 'accumulated_eval_time': 366.0614733695984, 'accumulated_logging_time': 0.20946073532104492}
I0131 14:19:36.743677 139774417409792 logging_writer.py:48] [7286] accumulated_eval_time=366.061473, accumulated_logging_time=0.209461, accumulated_submission_time=3396.937368, global_step=7286, preemption_count=0, score=3396.937368, test/accuracy=0.199800, test/loss=4.236069, test/num_examples=10000, total_duration=3763.586677, train/accuracy=0.290020, train/loss=3.663867, validation/accuracy=0.262240, validation/loss=3.812857, validation/num_examples=50000
I0131 14:19:42.712422 139774434195200 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9074705243110657, loss=6.432657241821289
I0131 14:20:24.699202 139774417409792 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.1805373430252075, loss=5.18623685836792
I0131 14:21:11.099984 139774434195200 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.0898748636245728, loss=5.025891304016113
I0131 14:21:57.407418 139774417409792 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.0321674346923828, loss=5.263045310974121
I0131 14:22:43.551206 139774434195200 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0262991189956665, loss=5.07974910736084
I0131 14:23:29.668021 139774434195200 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.1920701265335083, loss=5.1778645515441895
I0131 14:24:15.800426 139774417409792 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.0751644372940063, loss=5.423163890838623
I0131 14:25:01.591006 139774434195200 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9377591013908386, loss=6.300706386566162
I0131 14:25:47.807638 139774417409792 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.1142005920410156, loss=5.147862434387207
I0131 14:26:33.789139 139774434195200 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7948936223983765, loss=5.679685592651367
I0131 14:26:37.205128 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:26:49.862222 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:27:14.699619 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:27:16.313367 139936116377408 submission_runner.py:408] Time since start: 4223.17s, 	Step: 8209, 	{'train/accuracy': 0.3228124976158142, 'train/loss': 3.440293550491333, 'validation/accuracy': 0.30069997906684875, 'validation/loss': 3.549184560775757, 'validation/num_examples': 50000, 'test/accuracy': 0.233800008893013, 'test/loss': 4.013123989105225, 'test/num_examples': 10000, 'score': 3817.3410184383392, 'total_duration': 4223.172071218491, 'accumulated_submission_time': 3817.3410184383392, 'accumulated_eval_time': 405.1697099208832, 'accumulated_logging_time': 0.23502397537231445}
I0131 14:27:16.328612 139774417409792 logging_writer.py:48] [8209] accumulated_eval_time=405.169710, accumulated_logging_time=0.235024, accumulated_submission_time=3817.341018, global_step=8209, preemption_count=0, score=3817.341018, test/accuracy=0.233800, test/loss=4.013124, test/num_examples=10000, total_duration=4223.172071, train/accuracy=0.322812, train/loss=3.440294, validation/accuracy=0.300700, validation/loss=3.549185, validation/num_examples=50000
I0131 14:27:53.423310 139774434195200 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.085623860359192, loss=5.123925685882568
I0131 14:28:39.313871 139774417409792 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.1723910570144653, loss=4.972456455230713
I0131 14:29:25.346551 139774434195200 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0942604541778564, loss=5.055802345275879
I0131 14:30:11.585226 139774417409792 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8508058190345764, loss=5.590517044067383
I0131 14:30:57.470269 139774434195200 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7745614647865295, loss=6.098135948181152
I0131 14:31:44.535868 139774417409792 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.024786353111267, loss=4.923145294189453
I0131 14:32:30.564940 139774434195200 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7377654314041138, loss=5.979193687438965
I0131 14:33:16.607922 139774417409792 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8979904651641846, loss=5.179347038269043
I0131 14:34:02.914089 139774434195200 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1388535499572754, loss=4.933380603790283
I0131 14:34:16.526569 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:34:29.069819 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:34:55.729510 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:34:57.331787 139936116377408 submission_runner.py:408] Time since start: 4684.19s, 	Step: 9131, 	{'train/accuracy': 0.36238279938697815, 'train/loss': 3.1926872730255127, 'validation/accuracy': 0.3360999822616577, 'validation/loss': 3.323276996612549, 'validation/num_examples': 50000, 'test/accuracy': 0.2590000033378601, 'test/loss': 3.8314473628997803, 'test/num_examples': 10000, 'score': 4237.481600284576, 'total_duration': 4684.190475702286, 'accumulated_submission_time': 4237.481600284576, 'accumulated_eval_time': 445.9749083518982, 'accumulated_logging_time': 0.2604801654815674}
I0131 14:34:57.348424 139774417409792 logging_writer.py:48] [9131] accumulated_eval_time=445.974908, accumulated_logging_time=0.260480, accumulated_submission_time=4237.481600, global_step=9131, preemption_count=0, score=4237.481600, test/accuracy=0.259000, test/loss=3.831447, test/num_examples=10000, total_duration=4684.190476, train/accuracy=0.362383, train/loss=3.192687, validation/accuracy=0.336100, validation/loss=3.323277, validation/num_examples=50000
I0131 14:35:25.154926 139774434195200 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.0120062828063965, loss=4.854612827301025
I0131 14:36:10.930265 139774417409792 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8082650303840637, loss=5.804433822631836
I0131 14:36:56.987506 139774434195200 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.1012442111968994, loss=4.759660720825195
I0131 14:37:43.312572 139774417409792 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7283229231834412, loss=6.2626142501831055
I0131 14:38:29.343835 139774434195200 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.768724799156189, loss=6.028410911560059
I0131 14:39:15.228598 139774417409792 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6522203087806702, loss=6.105233192443848
I0131 14:40:01.464068 139774434195200 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9919432401657104, loss=4.735274314880371
I0131 14:40:47.446858 139774417409792 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7046741843223572, loss=6.188323974609375
I0131 14:41:33.869641 139774434195200 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.0114463567733765, loss=4.725116729736328
I0131 14:41:57.437243 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:42:10.390063 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:42:38.782050 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:42:40.386543 139936116377408 submission_runner.py:408] Time since start: 5147.25s, 	Step: 10053, 	{'train/accuracy': 0.394843727350235, 'train/loss': 3.041377305984497, 'validation/accuracy': 0.36329999566078186, 'validation/loss': 3.2022409439086914, 'validation/num_examples': 50000, 'test/accuracy': 0.28220000863075256, 'test/loss': 3.700833797454834, 'test/num_examples': 10000, 'score': 4657.512858867645, 'total_duration': 5147.245245218277, 'accumulated_submission_time': 4657.512858867645, 'accumulated_eval_time': 488.92419385910034, 'accumulated_logging_time': 0.28647589683532715}
I0131 14:42:40.404905 139774417409792 logging_writer.py:48] [10053] accumulated_eval_time=488.924194, accumulated_logging_time=0.286476, accumulated_submission_time=4657.512859, global_step=10053, preemption_count=0, score=4657.512859, test/accuracy=0.282200, test/loss=3.700834, test/num_examples=10000, total_duration=5147.245245, train/accuracy=0.394844, train/loss=3.041377, validation/accuracy=0.363300, validation/loss=3.202241, validation/num_examples=50000
I0131 14:42:59.491077 139774434195200 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7698189616203308, loss=6.172821044921875
I0131 14:43:43.497615 139774417409792 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7323428392410278, loss=5.677403450012207
I0131 14:44:29.743962 139774434195200 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7807722091674805, loss=5.570179462432861
I0131 14:45:16.038631 139774417409792 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.8864580392837524, loss=5.221174240112305
I0131 14:46:01.883697 139774434195200 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9479109644889832, loss=5.195395469665527
I0131 14:46:47.984791 139774417409792 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9804062843322754, loss=4.664375305175781
I0131 14:47:34.027981 139774434195200 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9505536556243896, loss=4.704425811767578
I0131 14:48:20.154990 139774417409792 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8673660159111023, loss=4.680617332458496
I0131 14:49:06.181071 139774434195200 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6998711824417114, loss=5.480825424194336
I0131 14:49:40.776668 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:49:53.442926 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:50:19.666290 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:50:21.272243 139936116377408 submission_runner.py:408] Time since start: 5608.13s, 	Step: 10977, 	{'train/accuracy': 0.4287695288658142, 'train/loss': 2.809704542160034, 'validation/accuracy': 0.3992599844932556, 'validation/loss': 2.950516700744629, 'validation/num_examples': 50000, 'test/accuracy': 0.30650001764297485, 'test/loss': 3.502833366394043, 'test/num_examples': 10000, 'score': 5077.823751926422, 'total_duration': 5608.130947113037, 'accumulated_submission_time': 5077.823751926422, 'accumulated_eval_time': 529.4197680950165, 'accumulated_logging_time': 0.31799912452697754}
I0131 14:50:21.288279 139774417409792 logging_writer.py:48] [10977] accumulated_eval_time=529.419768, accumulated_logging_time=0.317999, accumulated_submission_time=5077.823752, global_step=10977, preemption_count=0, score=5077.823752, test/accuracy=0.306500, test/loss=3.502833, test/num_examples=10000, total_duration=5608.130947, train/accuracy=0.428770, train/loss=2.809705, validation/accuracy=0.399260, validation/loss=2.950517, validation/num_examples=50000
I0131 14:50:30.828851 139774434195200 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9092086553573608, loss=4.556240558624268
I0131 14:51:13.446225 139774417409792 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.0056123733520508, loss=4.5911149978637695
I0131 14:51:59.773024 139774434195200 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9814570546150208, loss=4.712832450866699
I0131 14:52:46.202023 139774417409792 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9313666224479675, loss=4.630424976348877
I0131 14:53:32.439941 139774434195200 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7350903153419495, loss=6.047009468078613
I0131 14:54:18.526675 139774417409792 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7743591666221619, loss=5.094012260437012
I0131 14:55:04.515048 139774434195200 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9553468823432922, loss=4.587884902954102
I0131 14:55:50.486419 139774417409792 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.75386643409729, loss=5.547809600830078
I0131 14:56:36.688124 139774434195200 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9201107025146484, loss=4.510211944580078
I0131 14:57:21.326344 139936116377408 spec.py:321] Evaluating on the training split.
I0131 14:57:33.807576 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 14:58:00.147157 139936116377408 spec.py:349] Evaluating on the test split.
I0131 14:58:01.761595 139936116377408 submission_runner.py:408] Time since start: 6068.62s, 	Step: 11899, 	{'train/accuracy': 0.461249977350235, 'train/loss': 2.6188957691192627, 'validation/accuracy': 0.42489999532699585, 'validation/loss': 2.7885499000549316, 'validation/num_examples': 50000, 'test/accuracy': 0.3294000029563904, 'test/loss': 3.3375587463378906, 'test/num_examples': 10000, 'score': 5497.804342985153, 'total_duration': 6068.620263576508, 'accumulated_submission_time': 5497.804342985153, 'accumulated_eval_time': 569.8549757003784, 'accumulated_logging_time': 0.34359264373779297}
I0131 14:58:01.784742 139774417409792 logging_writer.py:48] [11899] accumulated_eval_time=569.854976, accumulated_logging_time=0.343593, accumulated_submission_time=5497.804343, global_step=11899, preemption_count=0, score=5497.804343, test/accuracy=0.329400, test/loss=3.337559, test/num_examples=10000, total_duration=6068.620264, train/accuracy=0.461250, train/loss=2.618896, validation/accuracy=0.424900, validation/loss=2.788550, validation/num_examples=50000
I0131 14:58:02.590683 139774434195200 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.7631525993347168, loss=6.021175384521484
I0131 14:58:43.721114 139774417409792 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0048965215682983, loss=4.398295879364014
I0131 14:59:29.655451 139774434195200 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.8474915623664856, loss=4.880704402923584
I0131 15:00:15.792912 139774417409792 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9443642497062683, loss=4.579526424407959
I0131 15:01:01.896740 139774434195200 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9539427757263184, loss=4.4433274269104
I0131 15:01:48.184735 139774417409792 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8693032264709473, loss=4.544550895690918
I0131 15:02:34.880597 139774434195200 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.072448492050171, loss=4.430009365081787
I0131 15:03:21.081413 139774417409792 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.8822381496429443, loss=4.878549098968506
I0131 15:04:07.378834 139774434195200 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.8647204041481018, loss=4.491790294647217
I0131 15:04:53.658971 139774417409792 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7752116918563843, loss=5.490868091583252
I0131 15:05:02.187349 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:05:15.035058 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:05:42.802827 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:05:44.410166 139936116377408 submission_runner.py:408] Time since start: 6531.27s, 	Step: 12820, 	{'train/accuracy': 0.4747070074081421, 'train/loss': 2.617384910583496, 'validation/accuracy': 0.4347599744796753, 'validation/loss': 2.79663348197937, 'validation/num_examples': 50000, 'test/accuracy': 0.33800002932548523, 'test/loss': 3.348545789718628, 'test/num_examples': 10000, 'score': 5918.146535158157, 'total_duration': 6531.268862962723, 'accumulated_submission_time': 5918.146535158157, 'accumulated_eval_time': 612.0777878761292, 'accumulated_logging_time': 0.3792257308959961}
I0131 15:05:44.427283 139774434195200 logging_writer.py:48] [12820] accumulated_eval_time=612.077788, accumulated_logging_time=0.379226, accumulated_submission_time=5918.146535, global_step=12820, preemption_count=0, score=5918.146535, test/accuracy=0.338000, test/loss=3.348546, test/num_examples=10000, total_duration=6531.268863, train/accuracy=0.474707, train/loss=2.617385, validation/accuracy=0.434760, validation/loss=2.796633, validation/num_examples=50000
I0131 15:06:16.997290 139774417409792 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.8703473210334778, loss=4.404757499694824
I0131 15:07:02.916846 139774434195200 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9674778580665588, loss=4.3928985595703125
I0131 15:07:49.123771 139774417409792 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9273521900177002, loss=4.458117961883545
I0131 15:08:35.387420 139774434195200 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9355624914169312, loss=4.363691329956055
I0131 15:09:21.319700 139774417409792 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.8947778940200806, loss=4.313896656036377
I0131 15:10:07.666362 139774434195200 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6196766495704651, loss=6.077939987182617
I0131 15:10:53.604177 139774417409792 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9112098813056946, loss=4.396241188049316
I0131 15:11:39.793472 139774434195200 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6757484674453735, loss=5.487392425537109
I0131 15:12:25.721455 139774417409792 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6837956309318542, loss=5.419509410858154
I0131 15:12:44.743597 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:12:57.744434 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:13:22.539635 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:13:24.139190 139936116377408 submission_runner.py:408] Time since start: 6991.00s, 	Step: 13743, 	{'train/accuracy': 0.4976562261581421, 'train/loss': 2.4742987155914307, 'validation/accuracy': 0.4613799750804901, 'validation/loss': 2.645811080932617, 'validation/num_examples': 50000, 'test/accuracy': 0.3629000186920166, 'test/loss': 3.206737995147705, 'test/num_examples': 10000, 'score': 6338.404702425003, 'total_duration': 6990.997879266739, 'accumulated_submission_time': 6338.404702425003, 'accumulated_eval_time': 651.4733724594116, 'accumulated_logging_time': 0.4065427780151367}
I0131 15:13:24.158750 139774434195200 logging_writer.py:48] [13743] accumulated_eval_time=651.473372, accumulated_logging_time=0.406543, accumulated_submission_time=6338.404702, global_step=13743, preemption_count=0, score=6338.404702, test/accuracy=0.362900, test/loss=3.206738, test/num_examples=10000, total_duration=6990.997879, train/accuracy=0.497656, train/loss=2.474299, validation/accuracy=0.461380, validation/loss=2.645811, validation/num_examples=50000
I0131 15:13:47.200540 139774417409792 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7895886301994324, loss=5.1223225593566895
I0131 15:14:31.977865 139774434195200 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6740861535072327, loss=5.812652587890625
I0131 15:15:18.264256 139774417409792 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8700207471847534, loss=4.314818382263184
I0131 15:16:04.731352 139774434195200 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0186694860458374, loss=4.333741188049316
I0131 15:16:50.472345 139774417409792 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9167863726615906, loss=4.3329033851623535
I0131 15:17:36.937885 139774434195200 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7696121335029602, loss=5.345949649810791
I0131 15:18:23.064381 139774417409792 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.9185178875923157, loss=4.3873291015625
I0131 15:19:09.019744 139774434195200 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8697278499603271, loss=4.223071098327637
I0131 15:19:55.004704 139774417409792 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8768453001976013, loss=4.297122478485107
I0131 15:20:24.157525 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:20:36.547774 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:21:03.794490 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:21:05.404422 139936116377408 submission_runner.py:408] Time since start: 7452.26s, 	Step: 14665, 	{'train/accuracy': 0.5162500143051147, 'train/loss': 2.3231639862060547, 'validation/accuracy': 0.4807399809360504, 'validation/loss': 2.4985995292663574, 'validation/num_examples': 50000, 'test/accuracy': 0.3760000169277191, 'test/loss': 3.0853447914123535, 'test/num_examples': 10000, 'score': 6758.347489833832, 'total_duration': 7452.263117074966, 'accumulated_submission_time': 6758.347489833832, 'accumulated_eval_time': 692.7202491760254, 'accumulated_logging_time': 0.43558502197265625}
I0131 15:21:05.422098 139774434195200 logging_writer.py:48] [14665] accumulated_eval_time=692.720249, accumulated_logging_time=0.435585, accumulated_submission_time=6758.347490, global_step=14665, preemption_count=0, score=6758.347490, test/accuracy=0.376000, test/loss=3.085345, test/num_examples=10000, total_duration=7452.263117, train/accuracy=0.516250, train/loss=2.323164, validation/accuracy=0.480740, validation/loss=2.498600, validation/num_examples=50000
I0131 15:21:19.727150 139774417409792 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9785051345825195, loss=4.291848182678223
I0131 15:22:03.396605 139774434195200 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9083926677703857, loss=4.327572345733643
I0131 15:22:49.383857 139774417409792 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.700530469417572, loss=6.084915637969971
I0131 15:23:36.390776 139774434195200 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8198172450065613, loss=4.931573867797852
I0131 15:24:22.658518 139774417409792 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9069217443466187, loss=4.256307125091553
I0131 15:25:09.062412 139774434195200 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8340262770652771, loss=4.651666164398193
I0131 15:25:55.124601 139774417409792 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.7907567620277405, loss=4.460000991821289
I0131 15:26:41.156768 139774434195200 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.8930747509002686, loss=4.2295002937316895
I0131 15:27:27.412877 139774417409792 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6551399827003479, loss=5.729743003845215
I0131 15:28:05.445099 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:28:17.871397 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:28:42.144684 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:28:43.749074 139936116377408 submission_runner.py:408] Time since start: 7910.61s, 	Step: 15584, 	{'train/accuracy': 0.5301562547683716, 'train/loss': 2.32466983795166, 'validation/accuracy': 0.48985999822616577, 'validation/loss': 2.5129311084747314, 'validation/num_examples': 50000, 'test/accuracy': 0.38200002908706665, 'test/loss': 3.095673084259033, 'test/num_examples': 10000, 'score': 7178.313052892685, 'total_duration': 7910.607768535614, 'accumulated_submission_time': 7178.313052892685, 'accumulated_eval_time': 731.0242302417755, 'accumulated_logging_time': 0.4632751941680908}
I0131 15:28:43.765932 139774434195200 logging_writer.py:48] [15584] accumulated_eval_time=731.024230, accumulated_logging_time=0.463275, accumulated_submission_time=7178.313053, global_step=15584, preemption_count=0, score=7178.313053, test/accuracy=0.382000, test/loss=3.095673, test/num_examples=10000, total_duration=7910.607769, train/accuracy=0.530156, train/loss=2.324670, validation/accuracy=0.489860, validation/loss=2.512931, validation/num_examples=50000
I0131 15:28:50.522681 139774417409792 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7713592052459717, loss=5.030923843383789
I0131 15:29:32.451634 139774434195200 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9066202044487, loss=4.190410137176514
I0131 15:30:18.600258 139774417409792 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6931507587432861, loss=5.92170524597168
I0131 15:31:04.765980 139774434195200 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8882670998573303, loss=4.5573015213012695
I0131 15:31:51.068073 139774417409792 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7688999772071838, loss=4.814806938171387
I0131 15:32:36.861888 139774434195200 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.7058724164962769, loss=5.903765678405762
I0131 15:33:23.077003 139774417409792 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.9137707352638245, loss=4.20353364944458
I0131 15:34:09.294788 139774434195200 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9099015593528748, loss=4.302450180053711
I0131 15:34:55.382649 139774417409792 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9387271404266357, loss=4.261014461517334
I0131 15:35:41.428015 139774434195200 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9436207413673401, loss=4.171931266784668
I0131 15:35:43.876161 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:35:56.489969 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:36:20.808831 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:36:22.419903 139936116377408 submission_runner.py:408] Time since start: 8369.28s, 	Step: 16507, 	{'train/accuracy': 0.564160168170929, 'train/loss': 2.104198932647705, 'validation/accuracy': 0.505620002746582, 'validation/loss': 2.3738718032836914, 'validation/num_examples': 50000, 'test/accuracy': 0.3920000195503235, 'test/loss': 2.980851411819458, 'test/num_examples': 10000, 'score': 7598.365840673447, 'total_duration': 8369.278602838516, 'accumulated_submission_time': 7598.365840673447, 'accumulated_eval_time': 769.5679693222046, 'accumulated_logging_time': 0.48979949951171875}
I0131 15:36:22.437357 139774417409792 logging_writer.py:48] [16507] accumulated_eval_time=769.567969, accumulated_logging_time=0.489799, accumulated_submission_time=7598.365841, global_step=16507, preemption_count=0, score=7598.365841, test/accuracy=0.392000, test/loss=2.980851, test/num_examples=10000, total_duration=8369.278603, train/accuracy=0.564160, train/loss=2.104199, validation/accuracy=0.505620, validation/loss=2.373872, validation/num_examples=50000
I0131 15:37:00.757412 139774434195200 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0046117305755615, loss=4.194180011749268
I0131 15:37:46.580622 139774417409792 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9178188443183899, loss=4.166984558105469
I0131 15:38:33.027458 139774434195200 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9296271800994873, loss=4.307293891906738
I0131 15:39:19.318804 139774417409792 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7795655131340027, loss=5.087184906005859
I0131 15:40:05.482795 139774434195200 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9195820093154907, loss=4.239758491516113
I0131 15:40:51.319215 139774417409792 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9444323182106018, loss=4.10576057434082
I0131 15:41:37.879946 139774434195200 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8839577436447144, loss=4.196115970611572
I0131 15:42:24.088964 139774417409792 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.9575526118278503, loss=4.054986476898193
I0131 15:43:10.265690 139774434195200 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7864679098129272, loss=4.75745153427124
I0131 15:43:22.453315 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:43:35.097957 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:43:58.883175 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:44:00.492883 139936116377408 submission_runner.py:408] Time since start: 8827.35s, 	Step: 17428, 	{'train/accuracy': 0.5541601181030273, 'train/loss': 2.172273874282837, 'validation/accuracy': 0.5168799757957458, 'validation/loss': 2.334871292114258, 'validation/num_examples': 50000, 'test/accuracy': 0.41030001640319824, 'test/loss': 2.9189651012420654, 'test/num_examples': 10000, 'score': 8018.321758508682, 'total_duration': 8827.351588010788, 'accumulated_submission_time': 8018.321758508682, 'accumulated_eval_time': 807.6075274944305, 'accumulated_logging_time': 0.5183203220367432}
I0131 15:44:00.509357 139774417409792 logging_writer.py:48] [17428] accumulated_eval_time=807.607527, accumulated_logging_time=0.518320, accumulated_submission_time=8018.321759, global_step=17428, preemption_count=0, score=8018.321759, test/accuracy=0.410300, test/loss=2.918965, test/num_examples=10000, total_duration=8827.351588, train/accuracy=0.554160, train/loss=2.172274, validation/accuracy=0.516880, validation/loss=2.334871, validation/num_examples=50000
I0131 15:44:29.476519 139774434195200 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8703885674476624, loss=4.173396110534668
I0131 15:45:15.351244 139774417409792 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8690997362136841, loss=4.162588596343994
I0131 15:46:01.315415 139774434195200 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9147273898124695, loss=4.108763694763184
I0131 15:46:47.592326 139774417409792 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.8761987686157227, loss=4.305214881896973
I0131 15:47:33.657978 139774434195200 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.038017749786377, loss=4.177583694458008
I0131 15:48:19.744069 139774417409792 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.9199588894844055, loss=4.195601463317871
I0131 15:49:05.850005 139774434195200 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7733716368675232, loss=5.853806018829346
I0131 15:49:51.755424 139774417409792 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9253056645393372, loss=4.070964336395264
I0131 15:50:37.935854 139774434195200 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9562177658081055, loss=4.202021598815918
I0131 15:51:00.504718 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:51:13.116393 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:51:41.687479 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:51:43.286404 139936116377408 submission_runner.py:408] Time since start: 9290.15s, 	Step: 18351, 	{'train/accuracy': 0.5694726705551147, 'train/loss': 2.0965421199798584, 'validation/accuracy': 0.5231199860572815, 'validation/loss': 2.2878384590148926, 'validation/num_examples': 50000, 'test/accuracy': 0.41290003061294556, 'test/loss': 2.89074444770813, 'test/num_examples': 10000, 'score': 8438.259444952011, 'total_duration': 9290.145105838776, 'accumulated_submission_time': 8438.259444952011, 'accumulated_eval_time': 850.389208316803, 'accumulated_logging_time': 0.544304609298706}
I0131 15:51:43.303490 139774417409792 logging_writer.py:48] [18351] accumulated_eval_time=850.389208, accumulated_logging_time=0.544305, accumulated_submission_time=8438.259445, global_step=18351, preemption_count=0, score=8438.259445, test/accuracy=0.412900, test/loss=2.890744, test/num_examples=10000, total_duration=9290.145106, train/accuracy=0.569473, train/loss=2.096542, validation/accuracy=0.523120, validation/loss=2.287838, validation/num_examples=50000
I0131 15:52:03.169857 139774434195200 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.5990388989448547, loss=5.788509368896484
I0131 15:52:46.957944 139774417409792 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8713198304176331, loss=4.113434791564941
I0131 15:53:33.102279 139774434195200 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9071817994117737, loss=4.072597503662109
I0131 15:54:22.074642 139774417409792 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.8972316980361938, loss=4.1290693283081055
I0131 15:55:09.062684 139774434195200 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.9709697365760803, loss=4.088804244995117
I0131 15:55:55.829997 139774417409792 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9886202812194824, loss=4.057255268096924
I0131 15:56:42.160816 139774434195200 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9407529234886169, loss=4.036454200744629
I0131 15:57:28.379266 139774417409792 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8050174713134766, loss=5.30961275100708
I0131 15:58:14.789835 139774434195200 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.897477924823761, loss=4.222749710083008
I0131 15:58:43.677154 139936116377408 spec.py:321] Evaluating on the training split.
I0131 15:58:56.333914 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 15:59:18.878304 139936116377408 spec.py:349] Evaluating on the test split.
I0131 15:59:20.489438 139936116377408 submission_runner.py:408] Time since start: 9747.35s, 	Step: 19264, 	{'train/accuracy': 0.5887304544448853, 'train/loss': 1.9993259906768799, 'validation/accuracy': 0.5343999862670898, 'validation/loss': 2.2401058673858643, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.851503372192383, 'test/num_examples': 10000, 'score': 8858.576433181763, 'total_duration': 9747.34814119339, 'accumulated_submission_time': 8858.576433181763, 'accumulated_eval_time': 887.2015011310577, 'accumulated_logging_time': 0.5706558227539062}
I0131 15:59:20.506815 139774417409792 logging_writer.py:48] [19264] accumulated_eval_time=887.201501, accumulated_logging_time=0.570656, accumulated_submission_time=8858.576433, global_step=19264, preemption_count=0, score=8858.576433, test/accuracy=0.423400, test/loss=2.851503, test/num_examples=10000, total_duration=9747.348141, train/accuracy=0.588730, train/loss=1.999326, validation/accuracy=0.534400, validation/loss=2.240106, validation/num_examples=50000
I0131 15:59:35.204710 139774434195200 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0723211765289307, loss=4.0898613929748535
I0131 16:00:18.588439 139774417409792 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9452800750732422, loss=4.384186744689941
I0131 16:01:04.983490 139774434195200 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.8992548584938049, loss=3.9964380264282227
I0131 16:01:51.320391 139774417409792 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.2390309572219849, loss=4.157234191894531
I0131 16:02:37.624655 139774434195200 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9885359406471252, loss=3.9663658142089844
I0131 16:03:23.954439 139774417409792 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8291298151016235, loss=4.246128559112549
I0131 16:04:10.339962 139774434195200 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.837514340877533, loss=4.208390712738037
I0131 16:04:56.413314 139774417409792 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9607645869255066, loss=4.0154709815979
I0131 16:05:43.098403 139774434195200 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.809042751789093, loss=4.811954021453857
I0131 16:06:20.745816 139936116377408 spec.py:321] Evaluating on the training split.
I0131 16:06:33.163876 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 16:07:00.896466 139936116377408 spec.py:349] Evaluating on the test split.
I0131 16:07:02.512299 139936116377408 submission_runner.py:408] Time since start: 10209.37s, 	Step: 20183, 	{'train/accuracy': 0.5824609398841858, 'train/loss': 2.048027515411377, 'validation/accuracy': 0.540619969367981, 'validation/loss': 2.2298035621643066, 'validation/num_examples': 50000, 'test/accuracy': 0.42600002884864807, 'test/loss': 2.823843002319336, 'test/num_examples': 10000, 'score': 9278.755592107773, 'total_duration': 10209.370984315872, 'accumulated_submission_time': 9278.755592107773, 'accumulated_eval_time': 928.9679560661316, 'accumulated_logging_time': 0.6004471778869629}
I0131 16:07:02.531445 139774417409792 logging_writer.py:48] [20183] accumulated_eval_time=928.967956, accumulated_logging_time=0.600447, accumulated_submission_time=9278.755592, global_step=20183, preemption_count=0, score=9278.755592, test/accuracy=0.426000, test/loss=2.823843, test/num_examples=10000, total_duration=10209.370984, train/accuracy=0.582461, train/loss=2.048028, validation/accuracy=0.540620, validation/loss=2.229804, validation/num_examples=50000
I0131 16:07:09.683327 139774434195200 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6722379922866821, loss=5.820193767547607
I0131 16:07:52.000789 139774417409792 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7989528775215149, loss=4.576518535614014
I0131 16:08:38.119498 139774434195200 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6921665072441101, loss=5.786092758178711
I0131 16:09:24.447700 139774417409792 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7213916182518005, loss=5.790789604187012
I0131 16:10:10.493265 139774434195200 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.9851683974266052, loss=4.129492282867432
I0131 16:10:56.561377 139774417409792 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.941356897354126, loss=4.032887935638428
I0131 16:11:42.959868 139774434195200 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8137385845184326, loss=5.358085632324219
I0131 16:12:28.991643 139774417409792 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8355115056037903, loss=4.658818244934082
I0131 16:13:15.139991 139774434195200 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7417718172073364, loss=5.289251327514648
I0131 16:14:01.297781 139774417409792 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.004020094871521, loss=4.028833866119385
I0131 16:14:02.964169 139936116377408 spec.py:321] Evaluating on the training split.
I0131 16:14:15.345162 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 16:14:45.640480 139936116377408 spec.py:349] Evaluating on the test split.
I0131 16:14:47.239966 139936116377408 submission_runner.py:408] Time since start: 10674.10s, 	Step: 21105, 	{'train/accuracy': 0.5959179401397705, 'train/loss': 1.9627938270568848, 'validation/accuracy': 0.5534799695014954, 'validation/loss': 2.157258987426758, 'validation/num_examples': 50000, 'test/accuracy': 0.4392000138759613, 'test/loss': 2.7591779232025146, 'test/num_examples': 10000, 'score': 9699.127198934555, 'total_duration': 10674.098667144775, 'accumulated_submission_time': 9699.127198934555, 'accumulated_eval_time': 973.2437303066254, 'accumulated_logging_time': 0.6334230899810791}
I0131 16:14:47.257722 139774434195200 logging_writer.py:48] [21105] accumulated_eval_time=973.243730, accumulated_logging_time=0.633423, accumulated_submission_time=9699.127199, global_step=21105, preemption_count=0, score=9699.127199, test/accuracy=0.439200, test/loss=2.759178, test/num_examples=10000, total_duration=10674.098667, train/accuracy=0.595918, train/loss=1.962794, validation/accuracy=0.553480, validation/loss=2.157259, validation/num_examples=50000
I0131 16:15:26.629978 139774417409792 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.9409608840942383, loss=4.149242877960205
I0131 16:16:12.956725 139774434195200 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6753056645393372, loss=5.789446830749512
I0131 16:16:59.342602 139774417409792 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8245095610618591, loss=4.555936813354492
I0131 16:17:45.777381 139774434195200 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7315099835395813, loss=5.700902462005615
I0131 16:18:31.827958 139774417409792 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6891664266586304, loss=5.646641731262207
I0131 16:19:17.810178 139774434195200 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7250267863273621, loss=5.504396915435791
I0131 16:20:04.016297 139774417409792 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.0437089204788208, loss=4.024001121520996
I0131 16:20:50.261026 139774434195200 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9627466797828674, loss=4.017785549163818
I0131 16:21:36.202548 139774417409792 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9235334396362305, loss=3.9924092292785645
I0131 16:21:47.343161 139936116377408 spec.py:321] Evaluating on the training split.
I0131 16:21:59.707352 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 16:22:24.790702 139936116377408 spec.py:349] Evaluating on the test split.
I0131 16:22:26.398624 139936116377408 submission_runner.py:408] Time since start: 11133.26s, 	Step: 22026, 	{'train/accuracy': 0.6109570264816284, 'train/loss': 1.8311398029327393, 'validation/accuracy': 0.5616799592971802, 'validation/loss': 2.0698909759521484, 'validation/num_examples': 50000, 'test/accuracy': 0.43870002031326294, 'test/loss': 2.6716549396514893, 'test/num_examples': 10000, 'score': 10119.155968666077, 'total_duration': 11133.25730252266, 'accumulated_submission_time': 10119.155968666077, 'accumulated_eval_time': 1012.2991769313812, 'accumulated_logging_time': 0.6604936122894287}
I0131 16:22:26.418747 139774434195200 logging_writer.py:48] [22026] accumulated_eval_time=1012.299177, accumulated_logging_time=0.660494, accumulated_submission_time=10119.155969, global_step=22026, preemption_count=0, score=10119.155969, test/accuracy=0.438700, test/loss=2.671655, test/num_examples=10000, total_duration=11133.257303, train/accuracy=0.610957, train/loss=1.831140, validation/accuracy=0.561680, validation/loss=2.069891, validation/num_examples=50000
I0131 16:22:56.314251 139774417409792 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9530864357948303, loss=4.110621452331543
I0131 16:23:42.085140 139774434195200 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.8420954346656799, loss=4.72522497177124
I0131 16:24:28.287410 139774417409792 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9259938597679138, loss=3.9409804344177246
I0131 16:25:14.314419 139774434195200 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6938538551330566, loss=5.380690097808838
I0131 16:26:00.224020 139774417409792 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8148387670516968, loss=4.493647575378418
I0131 16:26:46.670161 139774434195200 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.841102123260498, loss=4.319414138793945
I0131 16:27:32.592554 139774417409792 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.0727752447128296, loss=3.981187343597412
I0131 16:28:18.920545 139774434195200 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.9039880633354187, loss=4.325350284576416
I0131 16:29:05.114310 139774417409792 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9860826730728149, loss=4.130753517150879
I0131 16:29:26.788473 139936116377408 spec.py:321] Evaluating on the training split.
I0131 16:29:38.845606 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 16:30:03.322449 139936116377408 spec.py:349] Evaluating on the test split.
I0131 16:30:04.924380 139936116377408 submission_runner.py:408] Time since start: 11591.78s, 	Step: 22949, 	{'train/accuracy': 0.6087695360183716, 'train/loss': 1.8961181640625, 'validation/accuracy': 0.5664600133895874, 'validation/loss': 2.0866317749023438, 'validation/num_examples': 50000, 'test/accuracy': 0.450300008058548, 'test/loss': 2.7123382091522217, 'test/num_examples': 10000, 'score': 10539.468144655228, 'total_duration': 11591.783080339432, 'accumulated_submission_time': 10539.468144655228, 'accumulated_eval_time': 1050.4350891113281, 'accumulated_logging_time': 0.6903214454650879}
I0131 16:30:04.942260 139774434195200 logging_writer.py:48] [22949] accumulated_eval_time=1050.435089, accumulated_logging_time=0.690321, accumulated_submission_time=10539.468145, global_step=22949, preemption_count=0, score=10539.468145, test/accuracy=0.450300, test/loss=2.712338, test/num_examples=10000, total_duration=11591.783080, train/accuracy=0.608770, train/loss=1.896118, validation/accuracy=0.566460, validation/loss=2.086632, validation/num_examples=50000
I0131 16:30:25.615590 139774417409792 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9830577969551086, loss=4.000164031982422
I0131 16:31:09.759048 139774434195200 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.9616251587867737, loss=3.9216933250427246
I0131 16:31:56.037250 139774417409792 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.0856319665908813, loss=3.9343862533569336
I0131 16:32:42.616003 139774434195200 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.7563485503196716, loss=4.725648403167725
I0131 16:33:28.558729 139774417409792 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.9723055362701416, loss=3.922895908355713
I0131 16:34:14.792073 139774434195200 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7121980786323547, loss=5.5389933586120605
I0131 16:35:00.886974 139774417409792 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.861926794052124, loss=4.397794723510742
I0131 16:35:47.229468 139774434195200 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9055153131484985, loss=3.8189761638641357
I0131 16:36:33.727505 139774417409792 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8419929146766663, loss=4.207278251647949
I0131 16:37:05.033966 139936116377408 spec.py:321] Evaluating on the training split.
I0131 16:37:17.153534 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 16:37:45.738238 139936116377408 spec.py:349] Evaluating on the test split.
I0131 16:37:47.346865 139936116377408 submission_runner.py:408] Time since start: 12054.21s, 	Step: 23869, 	{'train/accuracy': 0.6216992139816284, 'train/loss': 1.8485908508300781, 'validation/accuracy': 0.5771799683570862, 'validation/loss': 2.052497148513794, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.6749086380004883, 'test/num_examples': 10000, 'score': 10959.501294612885, 'total_duration': 12054.205533742905, 'accumulated_submission_time': 10959.501294612885, 'accumulated_eval_time': 1092.7479412555695, 'accumulated_logging_time': 0.7183361053466797}
I0131 16:37:47.365526 139774434195200 logging_writer.py:48] [23869] accumulated_eval_time=1092.747941, accumulated_logging_time=0.718336, accumulated_submission_time=10959.501295, global_step=23869, preemption_count=0, score=10959.501295, test/accuracy=0.459300, test/loss=2.674909, test/num_examples=10000, total_duration=12054.205534, train/accuracy=0.621699, train/loss=1.848591, validation/accuracy=0.577180, validation/loss=2.052497, validation/num_examples=50000
I0131 16:38:00.079051 139774417409792 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9273565411567688, loss=3.981867790222168
I0131 16:38:43.264047 139774434195200 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7675966620445251, loss=4.782650947570801
I0131 16:39:29.517044 139774417409792 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9277710914611816, loss=4.111341953277588
I0131 16:40:16.099253 139774434195200 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9396013617515564, loss=4.554866790771484
I0131 16:41:02.243572 139774417409792 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9657129645347595, loss=3.92684006690979
I0131 16:41:48.571790 139774434195200 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8480957746505737, loss=5.15838623046875
I0131 16:42:34.725137 139774417409792 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.9053514003753662, loss=3.9509310722351074
I0131 16:43:20.783328 139774434195200 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.9588452577590942, loss=3.8965651988983154
I0131 16:44:07.050683 139774417409792 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.8120841383934021, loss=4.8440117835998535
I0131 16:44:47.613759 139936116377408 spec.py:321] Evaluating on the training split.
I0131 16:44:59.495604 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 16:45:25.546109 139936116377408 spec.py:349] Evaluating on the test split.
I0131 16:45:27.164032 139936116377408 submission_runner.py:408] Time since start: 12514.02s, 	Step: 24790, 	{'train/accuracy': 0.6266406178474426, 'train/loss': 1.8320873975753784, 'validation/accuracy': 0.5758000016212463, 'validation/loss': 2.063634157180786, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.6689186096191406, 'test/num_examples': 10000, 'score': 11379.689157009125, 'total_duration': 12514.022708654404, 'accumulated_submission_time': 11379.689157009125, 'accumulated_eval_time': 1132.2981944084167, 'accumulated_logging_time': 0.7464418411254883}
I0131 16:45:27.186953 139774434195200 logging_writer.py:48] [24790] accumulated_eval_time=1132.298194, accumulated_logging_time=0.746442, accumulated_submission_time=11379.689157, global_step=24790, preemption_count=0, score=11379.689157, test/accuracy=0.460600, test/loss=2.668919, test/num_examples=10000, total_duration=12514.022709, train/accuracy=0.626641, train/loss=1.832087, validation/accuracy=0.575800, validation/loss=2.063634, validation/num_examples=50000
I0131 16:45:31.567847 139774417409792 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7677363753318787, loss=5.640769958496094
I0131 16:46:13.416021 139774434195200 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8051870465278625, loss=5.427873611450195
I0131 16:46:59.478443 139774417409792 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7293714284896851, loss=5.730830192565918
I0131 16:47:46.054181 139774434195200 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9628300666809082, loss=4.323662281036377
I0131 16:48:32.374265 139774417409792 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.7665032148361206, loss=4.813014984130859
I0131 16:49:18.726976 139774434195200 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7968632578849792, loss=5.453378200531006
I0131 16:50:05.063148 139774417409792 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.9432991147041321, loss=3.849142551422119
I0131 16:50:51.282602 139774434195200 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9804067611694336, loss=3.9208297729492188
I0131 16:51:37.507415 139774417409792 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8002246618270874, loss=4.938338756561279
I0131 16:52:24.036044 139774434195200 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8550119996070862, loss=4.063824653625488
I0131 16:52:27.308286 139936116377408 spec.py:321] Evaluating on the training split.
I0131 16:52:39.407791 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 16:53:06.147952 139936116377408 spec.py:349] Evaluating on the test split.
I0131 16:53:07.758492 139936116377408 submission_runner.py:408] Time since start: 12974.62s, 	Step: 25709, 	{'train/accuracy': 0.6539843678474426, 'train/loss': 1.6860125064849854, 'validation/accuracy': 0.5881400108337402, 'validation/loss': 1.9701340198516846, 'validation/num_examples': 50000, 'test/accuracy': 0.4701000154018402, 'test/loss': 2.585104465484619, 'test/num_examples': 10000, 'score': 11799.745354413986, 'total_duration': 12974.617196798325, 'accumulated_submission_time': 11799.745354413986, 'accumulated_eval_time': 1172.748398065567, 'accumulated_logging_time': 0.7861979007720947}
I0131 16:53:07.777230 139774417409792 logging_writer.py:48] [25709] accumulated_eval_time=1172.748398, accumulated_logging_time=0.786198, accumulated_submission_time=11799.745354, global_step=25709, preemption_count=0, score=11799.745354, test/accuracy=0.470100, test/loss=2.585104, test/num_examples=10000, total_duration=12974.617197, train/accuracy=0.653984, train/loss=1.686013, validation/accuracy=0.588140, validation/loss=1.970134, validation/num_examples=50000
I0131 16:53:45.310596 139774434195200 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9729065895080566, loss=3.919414520263672
I0131 16:54:31.277025 139774417409792 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.9184777140617371, loss=4.040516376495361
I0131 16:55:17.430482 139774434195200 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.9205554127693176, loss=3.787418842315674
I0131 16:56:03.566489 139774417409792 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.7905278205871582, loss=5.657718658447266
I0131 16:56:49.612670 139774434195200 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9011030197143555, loss=4.1055474281311035
I0131 16:57:36.321306 139774417409792 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.051355242729187, loss=3.8337090015411377
I0131 16:58:22.596952 139774434195200 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.842197060585022, loss=4.487332344055176
I0131 16:59:08.835663 139774417409792 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.0307923555374146, loss=3.834282636642456
I0131 16:59:54.756358 139774434195200 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0402626991271973, loss=3.8663477897644043
I0131 17:00:07.821029 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:00:19.761620 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:00:47.546879 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:00:49.156262 139936116377408 submission_runner.py:408] Time since start: 13436.01s, 	Step: 26630, 	{'train/accuracy': 0.6346874833106995, 'train/loss': 1.760985255241394, 'validation/accuracy': 0.5895199775695801, 'validation/loss': 1.9683057069778442, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.573791742324829, 'test/num_examples': 10000, 'score': 12219.731608867645, 'total_duration': 13436.014949560165, 'accumulated_submission_time': 12219.731608867645, 'accumulated_eval_time': 1214.0835967063904, 'accumulated_logging_time': 0.8152399063110352}
I0131 17:00:49.174605 139774417409792 logging_writer.py:48] [26630] accumulated_eval_time=1214.083597, accumulated_logging_time=0.815240, accumulated_submission_time=12219.731609, global_step=26630, preemption_count=0, score=12219.731609, test/accuracy=0.478000, test/loss=2.573792, test/num_examples=10000, total_duration=13436.014950, train/accuracy=0.634687, train/loss=1.760985, validation/accuracy=0.589520, validation/loss=1.968306, validation/num_examples=50000
I0131 17:01:17.390988 139774434195200 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9533084630966187, loss=3.8797202110290527
I0131 17:02:03.159690 139774417409792 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.7985022068023682, loss=5.453566551208496
I0131 17:02:49.175962 139774434195200 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7876157164573669, loss=4.906672954559326
I0131 17:03:35.606414 139774417409792 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.9246450662612915, loss=3.7540335655212402
I0131 17:04:21.633270 139774434195200 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8965729475021362, loss=4.073002815246582
I0131 17:05:07.668869 139774417409792 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.974240243434906, loss=3.9535422325134277
I0131 17:05:53.397913 139774434195200 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9681357145309448, loss=3.7260406017303467
I0131 17:06:39.671633 139774417409792 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.840173065662384, loss=4.977292060852051
I0131 17:07:25.806812 139774434195200 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8739326596260071, loss=3.9826550483703613
I0131 17:07:49.499989 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:08:01.343817 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:08:30.590431 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:08:32.193790 139936116377408 submission_runner.py:408] Time since start: 13899.05s, 	Step: 27553, 	{'train/accuracy': 0.6429687142372131, 'train/loss': 1.7258527278900146, 'validation/accuracy': 0.5952999591827393, 'validation/loss': 1.9520374536514282, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.5679101943969727, 'test/num_examples': 10000, 'score': 12639.99943780899, 'total_duration': 13899.052494764328, 'accumulated_submission_time': 12639.99943780899, 'accumulated_eval_time': 1256.7773969173431, 'accumulated_logging_time': 0.8430163860321045}
I0131 17:08:32.215935 139774417409792 logging_writer.py:48] [27553] accumulated_eval_time=1256.777397, accumulated_logging_time=0.843016, accumulated_submission_time=12639.999438, global_step=27553, preemption_count=0, score=12639.999438, test/accuracy=0.476500, test/loss=2.567910, test/num_examples=10000, total_duration=13899.052495, train/accuracy=0.642969, train/loss=1.725853, validation/accuracy=0.595300, validation/loss=1.952037, validation/num_examples=50000
I0131 17:08:51.272695 139774434195200 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8356999158859253, loss=5.53529167175293
I0131 17:09:35.529299 139774417409792 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8228169083595276, loss=4.847344398498535
I0131 17:10:21.666464 139774434195200 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.8239697217941284, loss=5.65382719039917
I0131 17:11:07.889535 139774417409792 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.9808251857757568, loss=3.8664917945861816
I0131 17:11:53.854525 139774434195200 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.9981429576873779, loss=3.7821145057678223
I0131 17:12:40.062283 139774417409792 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.0068374872207642, loss=3.9308669567108154
I0131 17:13:26.040563 139774434195200 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.8660899996757507, loss=4.459550857543945
I0131 17:14:12.073274 139774417409792 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.9232442378997803, loss=3.8440608978271484
I0131 17:14:58.103994 139774434195200 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.95600426197052, loss=3.7864770889282227
I0131 17:15:32.399979 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:15:44.476306 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:16:10.047021 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:16:11.654754 139936116377408 submission_runner.py:408] Time since start: 14358.51s, 	Step: 28476, 	{'train/accuracy': 0.6584765315055847, 'train/loss': 1.6636217832565308, 'validation/accuracy': 0.595579981803894, 'validation/loss': 1.949023962020874, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.56640625, 'test/num_examples': 10000, 'score': 13060.122702360153, 'total_duration': 14358.513455867767, 'accumulated_submission_time': 13060.122702360153, 'accumulated_eval_time': 1296.0321819782257, 'accumulated_logging_time': 0.8778200149536133}
I0131 17:16:11.678533 139774417409792 logging_writer.py:48] [28476] accumulated_eval_time=1296.032182, accumulated_logging_time=0.877820, accumulated_submission_time=13060.122702, global_step=28476, preemption_count=0, score=13060.122702, test/accuracy=0.481200, test/loss=2.566406, test/num_examples=10000, total_duration=14358.513456, train/accuracy=0.658477, train/loss=1.663622, validation/accuracy=0.595580, validation/loss=1.949024, validation/num_examples=50000
I0131 17:16:21.618119 139774434195200 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.9839465618133545, loss=4.178224563598633
I0131 17:17:04.423206 139774417409792 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.8900920748710632, loss=3.934626579284668
I0131 17:17:50.500283 139774434195200 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.8923419117927551, loss=4.867161273956299
I0131 17:18:37.276588 139774417409792 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.8507348895072937, loss=4.995428562164307
I0131 17:19:23.387107 139774434195200 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.8545746207237244, loss=4.21633243560791
I0131 17:20:09.616301 139774417409792 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9790782332420349, loss=3.8763427734375
I0131 17:20:55.715875 139774434195200 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.808834969997406, loss=4.851569175720215
I0131 17:21:41.905279 139774417409792 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.8844494819641113, loss=4.184219837188721
I0131 17:22:28.218233 139774434195200 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.9008413553237915, loss=3.9991989135742188
I0131 17:23:11.660046 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:23:23.551944 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:23:51.421224 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:23:53.023259 139936116377408 submission_runner.py:408] Time since start: 14819.88s, 	Step: 29396, 	{'train/accuracy': 0.6520312428474426, 'train/loss': 1.674699306488037, 'validation/accuracy': 0.6082800030708313, 'validation/loss': 1.8775641918182373, 'validation/num_examples': 50000, 'test/accuracy': 0.48920002579689026, 'test/loss': 2.5049943923950195, 'test/num_examples': 10000, 'score': 13480.046933412552, 'total_duration': 14819.881961107254, 'accumulated_submission_time': 13480.046933412552, 'accumulated_eval_time': 1337.395380973816, 'accumulated_logging_time': 0.9113750457763672}
I0131 17:23:53.044943 139774417409792 logging_writer.py:48] [29396] accumulated_eval_time=1337.395381, accumulated_logging_time=0.911375, accumulated_submission_time=13480.046933, global_step=29396, preemption_count=0, score=13480.046933, test/accuracy=0.489200, test/loss=2.504994, test/num_examples=10000, total_duration=14819.881961, train/accuracy=0.652031, train/loss=1.674699, validation/accuracy=0.608280, validation/loss=1.877564, validation/num_examples=50000
I0131 17:23:55.035948 139774434195200 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.944918155670166, loss=3.9039547443389893
I0131 17:24:36.544498 139774417409792 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9353567361831665, loss=3.9482314586639404
I0131 17:25:22.529536 139774434195200 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.9592613577842712, loss=3.8013408184051514
I0131 17:26:08.914109 139774417409792 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.9963040947914124, loss=3.7125144004821777
I0131 17:26:55.169591 139774434195200 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.973656177520752, loss=3.795403003692627
I0131 17:27:41.483926 139774417409792 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.0394777059555054, loss=3.799683094024658
I0131 17:28:27.889904 139774434195200 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.0560741424560547, loss=3.7613604068756104
I0131 17:29:14.157816 139774417409792 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.9573419094085693, loss=4.156108379364014
I0131 17:29:59.852834 139774434195200 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.8442497253417969, loss=4.462985992431641
I0131 17:30:46.079437 139774417409792 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.083579659461975, loss=3.8799209594726562
I0131 17:30:53.273929 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:31:05.381681 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:31:37.384955 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:31:38.994054 139936116377408 submission_runner.py:408] Time since start: 15285.85s, 	Step: 30317, 	{'train/accuracy': 0.6537694931030273, 'train/loss': 1.6482510566711426, 'validation/accuracy': 0.6024599671363831, 'validation/loss': 1.8747574090957642, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.4801666736602783, 'test/num_examples': 10000, 'score': 13900.2150182724, 'total_duration': 15285.852749586105, 'accumulated_submission_time': 13900.2150182724, 'accumulated_eval_time': 1383.1154959201813, 'accumulated_logging_time': 0.945807695388794}
I0131 17:31:39.014873 139774434195200 logging_writer.py:48] [30317] accumulated_eval_time=1383.115496, accumulated_logging_time=0.945808, accumulated_submission_time=13900.215018, global_step=30317, preemption_count=0, score=13900.215018, test/accuracy=0.489300, test/loss=2.480167, test/num_examples=10000, total_duration=15285.852750, train/accuracy=0.653769, train/loss=1.648251, validation/accuracy=0.602460, validation/loss=1.874757, validation/num_examples=50000
I0131 17:32:13.101393 139774417409792 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.8662786483764648, loss=5.331523418426514
I0131 17:32:58.829926 139774434195200 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.9124972224235535, loss=4.077394962310791
I0131 17:33:45.032574 139774417409792 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.9709639549255371, loss=3.794123888015747
I0131 17:34:31.263874 139774434195200 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8711931109428406, loss=4.272919178009033
I0131 17:35:17.229285 139774417409792 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.8413527607917786, loss=5.08189058303833
I0131 17:36:03.401115 139774434195200 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.8911688327789307, loss=4.502044677734375
I0131 17:36:49.215399 139774417409792 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.0775724649429321, loss=3.682985782623291
I0131 17:37:35.256546 139774434195200 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.0197088718414307, loss=3.793351650238037
I0131 17:38:21.339032 139774417409792 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.8937559723854065, loss=4.884884357452393
I0131 17:38:39.342711 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:38:51.650135 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:39:17.340529 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:39:18.954344 139936116377408 submission_runner.py:408] Time since start: 15745.81s, 	Step: 31241, 	{'train/accuracy': 0.6695898175239563, 'train/loss': 1.5669158697128296, 'validation/accuracy': 0.6098799705505371, 'validation/loss': 1.8242343664169312, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.441969156265259, 'test/num_examples': 10000, 'score': 14320.48095369339, 'total_duration': 15745.813037872314, 'accumulated_submission_time': 14320.48095369339, 'accumulated_eval_time': 1422.7271156311035, 'accumulated_logging_time': 0.9807932376861572}
I0131 17:39:18.973423 139774434195200 logging_writer.py:48] [31241] accumulated_eval_time=1422.727116, accumulated_logging_time=0.980793, accumulated_submission_time=14320.480954, global_step=31241, preemption_count=0, score=14320.480954, test/accuracy=0.494800, test/loss=2.441969, test/num_examples=10000, total_duration=15745.813038, train/accuracy=0.669590, train/loss=1.566916, validation/accuracy=0.609880, validation/loss=1.824234, validation/num_examples=50000
I0131 17:39:42.808179 139774417409792 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7933698892593384, loss=4.703773498535156
I0131 17:40:27.874556 139774434195200 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.9409879446029663, loss=3.738816261291504
I0131 17:41:14.039497 139774417409792 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.9239334464073181, loss=4.150905132293701
I0131 17:42:00.453498 139774434195200 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0364656448364258, loss=3.745242118835449
I0131 17:42:46.412633 139774417409792 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9157881140708923, loss=3.8743484020233154
I0131 17:43:32.961722 139774434195200 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.0423094034194946, loss=3.887840986251831
I0131 17:44:19.164900 139774417409792 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.0143520832061768, loss=3.7431700229644775
I0131 17:45:05.147077 139774434195200 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.9818797707557678, loss=3.8165383338928223
I0131 17:45:51.249668 139774417409792 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8745821118354797, loss=5.572905540466309
I0131 17:46:19.251340 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:46:31.024963 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:47:01.098769 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:47:02.718743 139936116377408 submission_runner.py:408] Time since start: 16209.58s, 	Step: 32162, 	{'train/accuracy': 0.6646093726158142, 'train/loss': 1.623708724975586, 'validation/accuracy': 0.616159975528717, 'validation/loss': 1.8413524627685547, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.439016103744507, 'test/num_examples': 10000, 'score': 14740.702543497086, 'total_duration': 16209.5774371624, 'accumulated_submission_time': 14740.702543497086, 'accumulated_eval_time': 1466.1945168972015, 'accumulated_logging_time': 1.0090515613555908}
I0131 17:47:02.737931 139774434195200 logging_writer.py:48] [32162] accumulated_eval_time=1466.194517, accumulated_logging_time=1.009052, accumulated_submission_time=14740.702543, global_step=32162, preemption_count=0, score=14740.702543, test/accuracy=0.501000, test/loss=2.439016, test/num_examples=10000, total_duration=16209.577437, train/accuracy=0.664609, train/loss=1.623709, validation/accuracy=0.616160, validation/loss=1.841352, validation/num_examples=50000
I0131 17:47:18.222882 139774417409792 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.9090337753295898, loss=5.609408378601074
I0131 17:48:02.202038 139774434195200 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.9127123951911926, loss=3.9215989112854004
I0131 17:48:47.972224 139774417409792 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.9205160140991211, loss=3.7816996574401855
I0131 17:49:34.281106 139774434195200 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.801315188407898, loss=5.472117900848389
I0131 17:50:20.766092 139774417409792 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0690553188323975, loss=3.7227821350097656
I0131 17:51:06.584397 139774434195200 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9008675813674927, loss=3.722351551055908
I0131 17:51:52.853053 139774417409792 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0474547147750854, loss=3.657621383666992
I0131 17:52:38.778771 139774434195200 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.0309181213378906, loss=3.7643957138061523
I0131 17:53:25.151906 139774417409792 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.8224676847457886, loss=4.665990829467773
I0131 17:54:02.971934 139936116377408 spec.py:321] Evaluating on the training split.
I0131 17:54:14.892163 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 17:54:39.568228 139936116377408 spec.py:349] Evaluating on the test split.
I0131 17:54:41.180517 139936116377408 submission_runner.py:408] Time since start: 16668.04s, 	Step: 33084, 	{'train/accuracy': 0.6647070050239563, 'train/loss': 1.6350743770599365, 'validation/accuracy': 0.6173399686813354, 'validation/loss': 1.8481569290161133, 'validation/num_examples': 50000, 'test/accuracy': 0.49470001459121704, 'test/loss': 2.4732367992401123, 'test/num_examples': 10000, 'score': 15160.87868309021, 'total_duration': 16668.039219379425, 'accumulated_submission_time': 15160.87868309021, 'accumulated_eval_time': 1504.4031112194061, 'accumulated_logging_time': 1.0377919673919678}
I0131 17:54:41.203705 139774434195200 logging_writer.py:48] [33084] accumulated_eval_time=1504.403111, accumulated_logging_time=1.037792, accumulated_submission_time=15160.878683, global_step=33084, preemption_count=0, score=15160.878683, test/accuracy=0.494700, test/loss=2.473237, test/num_examples=10000, total_duration=16668.039219, train/accuracy=0.664707, train/loss=1.635074, validation/accuracy=0.617340, validation/loss=1.848157, validation/num_examples=50000
I0131 17:54:47.966941 139774417409792 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.0293534994125366, loss=3.6504364013671875
I0131 17:55:30.327486 139774434195200 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.028214931488037, loss=3.802457809448242
I0131 17:56:16.120785 139774417409792 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.9977869987487793, loss=3.7318313121795654
I0131 17:57:02.539613 139774434195200 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8348852396011353, loss=4.795144081115723
I0131 17:57:48.638918 139774417409792 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.8030500411987305, loss=4.915846347808838
I0131 17:58:34.970936 139774434195200 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.9678751230239868, loss=3.7268896102905273
I0131 17:59:21.150882 139774417409792 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.9746062159538269, loss=3.750577449798584
I0131 18:00:07.707430 139774434195200 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.9573619365692139, loss=3.801805019378662
I0131 18:00:53.750857 139774417409792 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.0386404991149902, loss=3.764021873474121
I0131 18:01:40.211245 139774434195200 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0887724161148071, loss=3.7957215309143066
I0131 18:01:41.509223 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:01:53.529246 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:02:19.106044 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:02:20.710165 139936116377408 submission_runner.py:408] Time since start: 17127.57s, 	Step: 34004, 	{'train/accuracy': 0.6825194954872131, 'train/loss': 1.5208829641342163, 'validation/accuracy': 0.6213200092315674, 'validation/loss': 1.7882128953933716, 'validation/num_examples': 50000, 'test/accuracy': 0.506600022315979, 'test/loss': 2.3888542652130127, 'test/num_examples': 10000, 'score': 15581.12509894371, 'total_duration': 17127.568870782852, 'accumulated_submission_time': 15581.12509894371, 'accumulated_eval_time': 1543.6040349006653, 'accumulated_logging_time': 1.0717051029205322}
I0131 18:02:20.733269 139774417409792 logging_writer.py:48] [34004] accumulated_eval_time=1543.604035, accumulated_logging_time=1.071705, accumulated_submission_time=15581.125099, global_step=34004, preemption_count=0, score=15581.125099, test/accuracy=0.506600, test/loss=2.388854, test/num_examples=10000, total_duration=17127.568871, train/accuracy=0.682519, train/loss=1.520883, validation/accuracy=0.621320, validation/loss=1.788213, validation/num_examples=50000
I0131 18:03:00.561439 139774434195200 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.9843440055847168, loss=3.756193161010742
I0131 18:03:46.737939 139774417409792 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.954031229019165, loss=3.851619005203247
I0131 18:04:33.182645 139774434195200 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.8184053301811218, loss=5.39105749130249
I0131 18:05:19.673280 139774417409792 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.9825364351272583, loss=3.7735040187835693
I0131 18:06:05.776391 139774434195200 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.047402024269104, loss=3.7479782104492188
I0131 18:06:52.027809 139774417409792 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.0434350967407227, loss=3.5834312438964844
I0131 18:07:38.122099 139774434195200 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.942147433757782, loss=3.625488758087158
I0131 18:08:24.328271 139774417409792 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8400001525878906, loss=4.656250953674316
I0131 18:09:10.640795 139774434195200 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8591827154159546, loss=4.6561598777771
I0131 18:09:20.860363 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:09:32.884718 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:09:59.469071 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:10:01.080914 139936116377408 submission_runner.py:408] Time since start: 17587.94s, 	Step: 34924, 	{'train/accuracy': 0.6859960556030273, 'train/loss': 1.5173721313476562, 'validation/accuracy': 0.6280999779701233, 'validation/loss': 1.7702747583389282, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.4008095264434814, 'test/num_examples': 10000, 'score': 16001.194901704788, 'total_duration': 17587.93961954117, 'accumulated_submission_time': 16001.194901704788, 'accumulated_eval_time': 1583.8245911598206, 'accumulated_logging_time': 1.1039619445800781}
I0131 18:10:01.109206 139774417409792 logging_writer.py:48] [34924] accumulated_eval_time=1583.824591, accumulated_logging_time=1.103962, accumulated_submission_time=16001.194902, global_step=34924, preemption_count=0, score=16001.194902, test/accuracy=0.503700, test/loss=2.400810, test/num_examples=10000, total_duration=17587.939620, train/accuracy=0.685996, train/loss=1.517372, validation/accuracy=0.628100, validation/loss=1.770275, validation/num_examples=50000
I0131 18:10:31.751684 139774434195200 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7855829000473022, loss=5.417829513549805
I0131 18:11:17.732016 139774417409792 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.9213271737098694, loss=5.149238586425781
I0131 18:12:04.040627 139774434195200 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.9620646238327026, loss=3.684032917022705
I0131 18:12:50.343275 139774417409792 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.9820330142974854, loss=4.0129170417785645
I0131 18:13:36.322541 139774434195200 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.9453423023223877, loss=3.6316018104553223
I0131 18:14:22.529084 139774417409792 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.1398826837539673, loss=3.672018051147461
I0131 18:15:08.544040 139774434195200 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.8896003365516663, loss=5.067693710327148
I0131 18:15:54.316399 139774417409792 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.928520917892456, loss=3.648500442504883
I0131 18:16:40.355967 139774434195200 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.8695232272148132, loss=5.487759113311768
I0131 18:17:01.520021 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:17:14.596539 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:17:43.143493 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:17:44.745175 139936116377408 submission_runner.py:408] Time since start: 18051.60s, 	Step: 35848, 	{'train/accuracy': 0.6784374713897705, 'train/loss': 1.537048101425171, 'validation/accuracy': 0.6295599937438965, 'validation/loss': 1.763940691947937, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.3965654373168945, 'test/num_examples': 10000, 'score': 16421.54888010025, 'total_duration': 18051.60387778282, 'accumulated_submission_time': 16421.54888010025, 'accumulated_eval_time': 1627.0497500896454, 'accumulated_logging_time': 1.141261339187622}
I0131 18:17:44.765154 139774417409792 logging_writer.py:48] [35848] accumulated_eval_time=1627.049750, accumulated_logging_time=1.141261, accumulated_submission_time=16421.548880, global_step=35848, preemption_count=0, score=16421.548880, test/accuracy=0.503700, test/loss=2.396565, test/num_examples=10000, total_duration=18051.603878, train/accuracy=0.678437, train/loss=1.537048, validation/accuracy=0.629560, validation/loss=1.763941, validation/num_examples=50000
I0131 18:18:05.826917 139774434195200 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.0080291032791138, loss=3.7487423419952393
I0131 18:18:50.188508 139774417409792 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.975825846195221, loss=3.573267936706543
I0131 18:19:36.655556 139774434195200 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.9887616634368896, loss=3.6262688636779785
I0131 18:20:22.868726 139774417409792 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.9859857559204102, loss=3.6436264514923096
I0131 18:21:09.072815 139774434195200 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.9230713248252869, loss=3.667052745819092
I0131 18:21:55.291072 139774417409792 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.0268511772155762, loss=3.9013538360595703
I0131 18:22:41.383607 139774434195200 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8378793597221375, loss=4.427459716796875
I0131 18:23:27.338687 139774417409792 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0173213481903076, loss=3.630364418029785
I0131 18:24:13.516219 139774434195200 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0073682069778442, loss=3.6528778076171875
I0131 18:24:44.878660 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:24:56.910476 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:25:22.878650 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:25:24.489891 139936116377408 submission_runner.py:408] Time since start: 18511.35s, 	Step: 36770, 	{'train/accuracy': 0.6861132383346558, 'train/loss': 1.495723843574524, 'validation/accuracy': 0.6327999830245972, 'validation/loss': 1.7372503280639648, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.3624677658081055, 'test/num_examples': 10000, 'score': 16841.60574913025, 'total_duration': 18511.348565340042, 'accumulated_submission_time': 16841.60574913025, 'accumulated_eval_time': 1666.6609530448914, 'accumulated_logging_time': 1.1702823638916016}
I0131 18:25:24.514683 139774417409792 logging_writer.py:48] [36770] accumulated_eval_time=1666.660953, accumulated_logging_time=1.170282, accumulated_submission_time=16841.605749, global_step=36770, preemption_count=0, score=16841.605749, test/accuracy=0.509800, test/loss=2.362468, test/num_examples=10000, total_duration=18511.348565, train/accuracy=0.686113, train/loss=1.495724, validation/accuracy=0.632800, validation/loss=1.737250, validation/num_examples=50000
I0131 18:25:36.824800 139774434195200 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0459403991699219, loss=3.7289350032806396
I0131 18:26:19.907856 139774417409792 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8366940021514893, loss=4.971101760864258
I0131 18:27:06.129314 139774434195200 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.9616814851760864, loss=3.637058734893799
I0131 18:27:52.348977 139774417409792 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.9558206796646118, loss=3.720679759979248
I0131 18:28:38.553880 139774434195200 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.9478350877761841, loss=3.9469356536865234
I0131 18:29:25.046273 139774417409792 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.9627845883369446, loss=3.5309512615203857
I0131 18:30:11.169486 139774434195200 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.9465594291687012, loss=3.5279738903045654
I0131 18:30:57.154636 139774417409792 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.0576115846633911, loss=3.7629928588867188
I0131 18:31:43.933623 139774434195200 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1030936241149902, loss=3.690115451812744
I0131 18:32:24.729087 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:32:36.621893 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:33:07.016573 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:33:08.618899 139936116377408 submission_runner.py:408] Time since start: 18975.48s, 	Step: 37690, 	{'train/accuracy': 0.7042187452316284, 'train/loss': 1.429381012916565, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.742835283279419, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.347130537033081, 'test/num_examples': 10000, 'score': 17261.76012301445, 'total_duration': 18975.477598905563, 'accumulated_submission_time': 17261.76012301445, 'accumulated_eval_time': 1710.5507550239563, 'accumulated_logging_time': 1.207282543182373}
I0131 18:33:08.642795 139774417409792 logging_writer.py:48] [37690] accumulated_eval_time=1710.550755, accumulated_logging_time=1.207283, accumulated_submission_time=17261.760123, global_step=37690, preemption_count=0, score=17261.760123, test/accuracy=0.508800, test/loss=2.347131, test/num_examples=10000, total_duration=18975.477599, train/accuracy=0.704219, train/loss=1.429381, validation/accuracy=0.632680, validation/loss=1.742835, validation/num_examples=50000
I0131 18:33:13.010555 139774434195200 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.8982694745063782, loss=3.8566737174987793
I0131 18:33:54.995760 139774417409792 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.877515971660614, loss=4.587008476257324
I0131 18:34:41.056164 139774434195200 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8939030170440674, loss=4.718984127044678
I0131 18:35:27.470341 139774417409792 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0666159391403198, loss=3.601848602294922
I0131 18:36:13.614581 139774434195200 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.8948651552200317, loss=5.493624687194824
I0131 18:36:59.483704 139774417409792 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.9019417762756348, loss=4.138845443725586
I0131 18:37:45.976036 139774434195200 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.9727784395217896, loss=3.682035446166992
I0131 18:38:32.517310 139774417409792 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.8852367401123047, loss=4.372968673706055
I0131 18:39:18.398815 139774434195200 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.1285481452941895, loss=3.6193220615386963
I0131 18:40:04.648371 139774417409792 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.0965149402618408, loss=3.700064182281494
I0131 18:40:08.803671 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:40:20.752337 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:40:44.304630 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:40:45.905780 139936116377408 submission_runner.py:408] Time since start: 19432.76s, 	Step: 38611, 	{'train/accuracy': 0.6838085651397705, 'train/loss': 1.5368809700012207, 'validation/accuracy': 0.6318199634552002, 'validation/loss': 1.7639278173446655, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.3935165405273438, 'test/num_examples': 10000, 'score': 17681.86350107193, 'total_duration': 19432.764481782913, 'accumulated_submission_time': 17681.86350107193, 'accumulated_eval_time': 1747.6528568267822, 'accumulated_logging_time': 1.2410552501678467}
I0131 18:40:45.930216 139774434195200 logging_writer.py:48] [38611] accumulated_eval_time=1747.652857, accumulated_logging_time=1.241055, accumulated_submission_time=17681.863501, global_step=38611, preemption_count=0, score=17681.863501, test/accuracy=0.508200, test/loss=2.393517, test/num_examples=10000, total_duration=19432.764482, train/accuracy=0.683809, train/loss=1.536881, validation/accuracy=0.631820, validation/loss=1.763928, validation/num_examples=50000
I0131 18:41:22.280640 139774417409792 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.8981772065162659, loss=4.500362396240234
I0131 18:42:08.873263 139774434195200 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.9860352277755737, loss=3.652564525604248
I0131 18:42:55.008956 139774417409792 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.9657306671142578, loss=3.5409817695617676
I0131 18:43:41.268029 139774434195200 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.8866817951202393, loss=3.9564404487609863
I0131 18:44:27.430407 139774417409792 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1029362678527832, loss=3.7659239768981934
I0131 18:45:13.929640 139774434195200 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.915174663066864, loss=3.996312141418457
I0131 18:46:00.143173 139774417409792 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.9049058556556702, loss=4.321753978729248
I0131 18:46:46.399338 139774434195200 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.8493780493736267, loss=4.341050148010254
I0131 18:47:32.890172 139774417409792 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.0098503828048706, loss=3.6850130558013916
I0131 18:47:46.054400 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:47:57.932289 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:48:25.662591 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:48:27.264569 139936116377408 submission_runner.py:408] Time since start: 19894.12s, 	Step: 39530, 	{'train/accuracy': 0.6908202767372131, 'train/loss': 1.493054986000061, 'validation/accuracy': 0.6378999948501587, 'validation/loss': 1.7385770082473755, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.360651969909668, 'test/num_examples': 10000, 'score': 18101.93009710312, 'total_duration': 19894.123265028, 'accumulated_submission_time': 18101.93009710312, 'accumulated_eval_time': 1788.8630316257477, 'accumulated_logging_time': 1.2756617069244385}
I0131 18:48:27.286206 139774434195200 logging_writer.py:48] [39530] accumulated_eval_time=1788.863032, accumulated_logging_time=1.275662, accumulated_submission_time=18101.930097, global_step=39530, preemption_count=0, score=18101.930097, test/accuracy=0.513400, test/loss=2.360652, test/num_examples=10000, total_duration=19894.123265, train/accuracy=0.690820, train/loss=1.493055, validation/accuracy=0.637900, validation/loss=1.738577, validation/num_examples=50000
I0131 18:48:55.479600 139774417409792 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.2232146263122559, loss=3.6260807514190674
I0131 18:49:41.261359 139774434195200 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.017958402633667, loss=3.744096517562866
I0131 18:50:27.600374 139774417409792 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.0224149227142334, loss=3.547865390777588
I0131 18:51:14.394346 139774434195200 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.9669201970100403, loss=3.6759707927703857
I0131 18:52:00.337605 139774417409792 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9114566445350647, loss=5.022468566894531
I0131 18:52:47.397932 139774434195200 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0687932968139648, loss=3.6659512519836426
I0131 18:53:34.172732 139774417409792 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.0186372995376587, loss=3.6123321056365967
I0131 18:54:20.323676 139774434195200 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.0593301057815552, loss=3.607727289199829
I0131 18:55:06.893311 139774417409792 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.9377931356430054, loss=3.614483594894409
I0131 18:55:27.410393 139936116377408 spec.py:321] Evaluating on the training split.
I0131 18:55:39.317475 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 18:56:04.608719 139936116377408 spec.py:349] Evaluating on the test split.
I0131 18:56:06.213446 139936116377408 submission_runner.py:408] Time since start: 20353.07s, 	Step: 40446, 	{'train/accuracy': 0.7030664086341858, 'train/loss': 1.4640662670135498, 'validation/accuracy': 0.6423999667167664, 'validation/loss': 1.7443610429763794, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.3539485931396484, 'test/num_examples': 10000, 'score': 18521.99777531624, 'total_duration': 20353.072145223618, 'accumulated_submission_time': 18521.99777531624, 'accumulated_eval_time': 1827.666071653366, 'accumulated_logging_time': 1.3061726093292236}
I0131 18:56:06.235092 139774434195200 logging_writer.py:48] [40446] accumulated_eval_time=1827.666072, accumulated_logging_time=1.306173, accumulated_submission_time=18521.997775, global_step=40446, preemption_count=0, score=18521.997775, test/accuracy=0.515900, test/loss=2.353949, test/num_examples=10000, total_duration=20353.072145, train/accuracy=0.703066, train/loss=1.464066, validation/accuracy=0.642400, validation/loss=1.744361, validation/num_examples=50000
I0131 18:56:28.089904 139774417409792 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0080419778823853, loss=3.578399896621704
I0131 18:57:12.724152 139774434195200 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.9402633309364319, loss=4.210668087005615
I0131 18:57:58.610804 139774417409792 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0795739889144897, loss=3.695831298828125
I0131 18:58:45.010082 139774434195200 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.9867804646492004, loss=3.7827091217041016
I0131 18:59:31.097353 139774417409792 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9239102005958557, loss=5.385242938995361
I0131 19:00:17.355253 139774434195200 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0146993398666382, loss=4.022221088409424
I0131 19:01:03.451097 139774417409792 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1011425256729126, loss=3.7289559841156006
I0131 19:01:49.396529 139774434195200 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.9797781705856323, loss=3.622701406478882
I0131 19:02:35.985527 139774417409792 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.9769993424415588, loss=3.8815484046936035
I0131 19:03:06.624073 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:03:18.526363 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:03:47.133160 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:03:48.733657 139936116377408 submission_runner.py:408] Time since start: 20815.59s, 	Step: 41368, 	{'train/accuracy': 0.6928125023841858, 'train/loss': 1.5066217184066772, 'validation/accuracy': 0.6416199803352356, 'validation/loss': 1.7189924716949463, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.3392860889434814, 'test/num_examples': 10000, 'score': 18942.32962703705, 'total_duration': 20815.592352867126, 'accumulated_submission_time': 18942.32962703705, 'accumulated_eval_time': 1869.7756474018097, 'accumulated_logging_time': 1.337547779083252}
I0131 19:03:48.757632 139774434195200 logging_writer.py:48] [41368] accumulated_eval_time=1869.775647, accumulated_logging_time=1.337548, accumulated_submission_time=18942.329627, global_step=41368, preemption_count=0, score=18942.329627, test/accuracy=0.521300, test/loss=2.339286, test/num_examples=10000, total_duration=20815.592353, train/accuracy=0.692813, train/loss=1.506622, validation/accuracy=0.641620, validation/loss=1.718992, validation/num_examples=50000
I0131 19:04:01.868165 139774417409792 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.045152187347412, loss=3.5423269271850586
I0131 19:04:45.079151 139774434195200 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8647712469100952, loss=4.852221488952637
I0131 19:05:31.017267 139774417409792 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.911575198173523, loss=4.947275161743164
I0131 19:06:17.315060 139774434195200 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9872665405273438, loss=5.414027214050293
I0131 19:07:03.352232 139774417409792 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.1387420892715454, loss=3.577005386352539
I0131 19:07:49.293503 139774434195200 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.8759198188781738, loss=5.025327205657959
I0131 19:08:35.567629 139774417409792 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.044732689857483, loss=3.6775014400482178
I0131 19:09:21.666864 139774434195200 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.9281943440437317, loss=4.715487480163574
I0131 19:10:07.877940 139774417409792 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.0817762613296509, loss=3.678109645843506
I0131 19:10:48.775198 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:11:00.689236 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:11:32.791354 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:11:34.406570 139936116377408 submission_runner.py:408] Time since start: 21281.27s, 	Step: 42290, 	{'train/accuracy': 0.6962890625, 'train/loss': 1.4691346883773804, 'validation/accuracy': 0.6421599984169006, 'validation/loss': 1.710872769355774, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.330148935317993, 'test/num_examples': 10000, 'score': 19362.287507772446, 'total_duration': 21281.26524925232, 'accumulated_submission_time': 19362.287507772446, 'accumulated_eval_time': 1915.4070200920105, 'accumulated_logging_time': 1.3723368644714355}
I0131 19:11:34.432634 139774434195200 logging_writer.py:48] [42290] accumulated_eval_time=1915.407020, accumulated_logging_time=1.372337, accumulated_submission_time=19362.287508, global_step=42290, preemption_count=0, score=19362.287508, test/accuracy=0.521500, test/loss=2.330149, test/num_examples=10000, total_duration=21281.265249, train/accuracy=0.696289, train/loss=1.469135, validation/accuracy=0.642160, validation/loss=1.710873, validation/num_examples=50000
I0131 19:11:38.799226 139774417409792 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.0252569913864136, loss=3.581382989883423
I0131 19:12:20.869369 139774434195200 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.9168508052825928, loss=4.038185119628906
I0131 19:13:06.717042 139774417409792 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9466594457626343, loss=3.6632626056671143
I0131 19:13:53.035935 139774434195200 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.9389474987983704, loss=4.450874328613281
I0131 19:14:39.099001 139774417409792 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.08320951461792, loss=3.578763008117676
I0131 19:15:25.271037 139774434195200 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0475250482559204, loss=3.5894927978515625
I0131 19:16:11.463384 139774417409792 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.025745153427124, loss=3.617361068725586
I0131 19:16:57.387777 139774434195200 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0293552875518799, loss=3.713953971862793
I0131 19:17:43.291459 139774417409792 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.0314524173736572, loss=3.6532351970672607
I0131 19:18:29.269237 139774434195200 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.0512231588363647, loss=3.6048197746276855
I0131 19:18:34.519283 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:18:46.488122 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:19:12.536305 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:19:14.146043 139936116377408 submission_runner.py:408] Time since start: 21741.00s, 	Step: 43213, 	{'train/accuracy': 0.707324206829071, 'train/loss': 1.4024572372436523, 'validation/accuracy': 0.647159993648529, 'validation/loss': 1.6708543300628662, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.284856081008911, 'test/num_examples': 10000, 'score': 19782.306488990784, 'total_duration': 21741.00474333763, 'accumulated_submission_time': 19782.306488990784, 'accumulated_eval_time': 1955.0337710380554, 'accumulated_logging_time': 1.4100327491760254}
I0131 19:19:14.169384 139774417409792 logging_writer.py:48] [43213] accumulated_eval_time=1955.033771, accumulated_logging_time=1.410033, accumulated_submission_time=19782.306489, global_step=43213, preemption_count=0, score=19782.306489, test/accuracy=0.527300, test/loss=2.284856, test/num_examples=10000, total_duration=21741.004743, train/accuracy=0.707324, train/loss=1.402457, validation/accuracy=0.647160, validation/loss=1.670854, validation/num_examples=50000
I0131 19:19:49.762993 139774434195200 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.021241545677185, loss=3.7638638019561768
I0131 19:20:35.671341 139774417409792 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.0113190412521362, loss=3.6144986152648926
I0131 19:21:22.095116 139774434195200 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9381486177444458, loss=4.0423054695129395
I0131 19:22:08.380313 139774417409792 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.0226199626922607, loss=3.687635660171509
I0131 19:22:54.398509 139774434195200 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.9235426187515259, loss=4.0858941078186035
I0131 19:23:40.820620 139774417409792 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.8541011810302734, loss=4.617455005645752
I0131 19:24:26.979980 139774434195200 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.939994752407074, loss=4.935654163360596
I0131 19:25:13.091872 139774417409792 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0961871147155762, loss=3.557025194168091
I0131 19:25:59.009526 139774434195200 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.044681191444397, loss=3.5550670623779297
I0131 19:26:14.553643 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:26:26.484156 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:26:55.333774 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:26:56.936807 139936116377408 submission_runner.py:408] Time since start: 22203.80s, 	Step: 44135, 	{'train/accuracy': 0.6955273151397705, 'train/loss': 1.4521766901016235, 'validation/accuracy': 0.6490199565887451, 'validation/loss': 1.6639647483825684, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.3044726848602295, 'test/num_examples': 10000, 'score': 20202.631172180176, 'total_duration': 22203.795504808426, 'accumulated_submission_time': 20202.631172180176, 'accumulated_eval_time': 1997.4169154167175, 'accumulated_logging_time': 1.4440712928771973}
I0131 19:26:56.962616 139774417409792 logging_writer.py:48] [44135] accumulated_eval_time=1997.416915, accumulated_logging_time=1.444071, accumulated_submission_time=20202.631172, global_step=44135, preemption_count=0, score=20202.631172, test/accuracy=0.521900, test/loss=2.304473, test/num_examples=10000, total_duration=22203.795505, train/accuracy=0.695527, train/loss=1.452177, validation/accuracy=0.649020, validation/loss=1.663965, validation/num_examples=50000
I0131 19:27:23.189717 139774434195200 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.0098525285720825, loss=3.8103504180908203
I0131 19:28:08.643094 139774417409792 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.1419845819473267, loss=3.6243622303009033
I0131 19:28:54.877187 139774434195200 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1316334009170532, loss=3.650503158569336
I0131 19:29:40.976631 139774417409792 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.0036582946777344, loss=3.599796772003174
I0131 19:30:26.993072 139774434195200 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.9696585536003113, loss=3.785892963409424
I0131 19:31:13.625705 139774417409792 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.0381983518600464, loss=3.5534262657165527
I0131 19:31:59.823058 139774434195200 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.0046806335449219, loss=3.9419116973876953
I0131 19:32:46.021535 139774417409792 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.9905725121498108, loss=3.7104949951171875
I0131 19:33:32.278925 139774434195200 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.0366003513336182, loss=3.888484001159668
I0131 19:33:57.160774 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:34:09.327951 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:34:36.903413 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:34:38.510249 139936116377408 submission_runner.py:408] Time since start: 22665.37s, 	Step: 45055, 	{'train/accuracy': 0.698925793170929, 'train/loss': 1.434710144996643, 'validation/accuracy': 0.647379994392395, 'validation/loss': 1.6674917936325073, 'validation/num_examples': 50000, 'test/accuracy': 0.5235000252723694, 'test/loss': 2.2860796451568604, 'test/num_examples': 10000, 'score': 20622.772392749786, 'total_duration': 22665.36895251274, 'accumulated_submission_time': 20622.772392749786, 'accumulated_eval_time': 2038.766408443451, 'accumulated_logging_time': 1.4792227745056152}
I0131 19:34:38.531710 139774417409792 logging_writer.py:48] [45055] accumulated_eval_time=2038.766408, accumulated_logging_time=1.479223, accumulated_submission_time=20622.772393, global_step=45055, preemption_count=0, score=20622.772393, test/accuracy=0.523500, test/loss=2.286080, test/num_examples=10000, total_duration=22665.368953, train/accuracy=0.698926, train/loss=1.434710, validation/accuracy=0.647380, validation/loss=1.667492, validation/num_examples=50000
I0131 19:34:56.802714 139774434195200 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.0222419500350952, loss=3.5316250324249268
I0131 19:35:41.232784 139774417409792 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.0215407609939575, loss=5.24760103225708
I0131 19:36:27.616828 139774434195200 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.0519263744354248, loss=3.6176183223724365
I0131 19:37:14.134020 139774417409792 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.0736403465270996, loss=3.550706624984741
I0131 19:38:00.312795 139774434195200 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.014818549156189, loss=3.5667519569396973
I0131 19:38:46.614778 139774417409792 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9337020516395569, loss=4.131648063659668
I0131 19:39:32.774004 139774434195200 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0854443311691284, loss=3.641434669494629
I0131 19:40:18.843765 139774417409792 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.0862447023391724, loss=3.66170072555542
I0131 19:41:05.318904 139774434195200 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.9754440784454346, loss=4.074968338012695
I0131 19:41:38.922782 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:41:50.837324 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:42:20.542787 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:42:22.147641 139936116377408 submission_runner.py:408] Time since start: 23129.01s, 	Step: 45974, 	{'train/accuracy': 0.7076953053474426, 'train/loss': 1.4150549173355103, 'validation/accuracy': 0.6478599905967712, 'validation/loss': 1.678601622581482, 'validation/num_examples': 50000, 'test/accuracy': 0.5283000469207764, 'test/loss': 2.2812087535858154, 'test/num_examples': 10000, 'score': 21043.10510325432, 'total_duration': 23129.006335258484, 'accumulated_submission_time': 21043.10510325432, 'accumulated_eval_time': 2081.991260766983, 'accumulated_logging_time': 1.511063575744629}
I0131 19:42:22.170828 139774417409792 logging_writer.py:48] [45974] accumulated_eval_time=2081.991261, accumulated_logging_time=1.511064, accumulated_submission_time=21043.105103, global_step=45974, preemption_count=0, score=21043.105103, test/accuracy=0.528300, test/loss=2.281209, test/num_examples=10000, total_duration=23129.006335, train/accuracy=0.707695, train/loss=1.415055, validation/accuracy=0.647860, validation/loss=1.678602, validation/num_examples=50000
I0131 19:42:32.886451 139774434195200 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9384883046150208, loss=3.9114904403686523
I0131 19:43:15.864294 139774417409792 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.0196205377578735, loss=4.036065578460693
I0131 19:44:01.803807 139774434195200 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.909542441368103, loss=4.4919257164001465
I0131 19:44:48.468855 139774417409792 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.1338893175125122, loss=3.5891592502593994
I0131 19:45:34.629429 139774434195200 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.9192065596580505, loss=4.445289134979248
I0131 19:46:21.143921 139774417409792 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.9667171835899353, loss=3.9486892223358154
I0131 19:47:07.245629 139774434195200 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.0388295650482178, loss=3.5753207206726074
I0131 19:47:53.304682 139774417409792 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.0538173913955688, loss=3.5536208152770996
I0131 19:48:39.316134 139774434195200 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.0814231634140015, loss=3.5837035179138184
I0131 19:49:22.306015 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:49:34.864688 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:50:02.410900 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:50:04.025735 139936116377408 submission_runner.py:408] Time since start: 23590.88s, 	Step: 46895, 	{'train/accuracy': 0.7375390529632568, 'train/loss': 1.2977427244186401, 'validation/accuracy': 0.6542400121688843, 'validation/loss': 1.6448653936386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.2646377086639404, 'test/num_examples': 10000, 'score': 21463.18249320984, 'total_duration': 23590.884415626526, 'accumulated_submission_time': 21463.18249320984, 'accumulated_eval_time': 2123.7109668254852, 'accumulated_logging_time': 1.544229507446289}
I0131 19:50:04.051957 139774417409792 logging_writer.py:48] [46895] accumulated_eval_time=2123.710967, accumulated_logging_time=1.544230, accumulated_submission_time=21463.182493, global_step=46895, preemption_count=0, score=21463.182493, test/accuracy=0.533100, test/loss=2.264638, test/num_examples=10000, total_duration=23590.884416, train/accuracy=0.737539, train/loss=1.297743, validation/accuracy=0.654240, validation/loss=1.644865, validation/num_examples=50000
I0131 19:50:06.438298 139774434195200 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.9412564039230347, loss=5.0601043701171875
I0131 19:50:47.682687 139774417409792 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.0739458799362183, loss=3.5723421573638916
I0131 19:51:33.694470 139774434195200 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.9942694902420044, loss=3.50567364692688
I0131 19:52:19.975376 139774417409792 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.9843930006027222, loss=5.042634010314941
I0131 19:53:06.235238 139774434195200 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.071189045906067, loss=3.6526198387145996
I0131 19:53:52.184809 139774417409792 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.0785096883773804, loss=4.101350784301758
I0131 19:54:38.503983 139774434195200 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0594439506530762, loss=3.572938919067383
I0131 19:55:24.747854 139774417409792 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.0260217189788818, loss=3.5170071125030518
I0131 19:56:10.772120 139774434195200 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.0965543985366821, loss=3.4641764163970947
I0131 19:56:56.880529 139774417409792 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.9543468356132507, loss=4.772355556488037
I0131 19:57:04.460710 139936116377408 spec.py:321] Evaluating on the training split.
I0131 19:57:16.567470 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 19:57:45.371268 139936116377408 spec.py:349] Evaluating on the test split.
I0131 19:57:46.989577 139936116377408 submission_runner.py:408] Time since start: 24053.85s, 	Step: 47818, 	{'train/accuracy': 0.7093945145606995, 'train/loss': 1.4072017669677734, 'validation/accuracy': 0.6576399803161621, 'validation/loss': 1.6411067247390747, 'validation/num_examples': 50000, 'test/accuracy': 0.5327000021934509, 'test/loss': 2.245063304901123, 'test/num_examples': 10000, 'score': 21883.532969236374, 'total_duration': 24053.848252534866, 'accumulated_submission_time': 21883.532969236374, 'accumulated_eval_time': 2166.239804506302, 'accumulated_logging_time': 1.5809102058410645}
I0131 19:57:47.014188 139774434195200 logging_writer.py:48] [47818] accumulated_eval_time=2166.239805, accumulated_logging_time=1.580910, accumulated_submission_time=21883.532969, global_step=47818, preemption_count=0, score=21883.532969, test/accuracy=0.532700, test/loss=2.245063, test/num_examples=10000, total_duration=24053.848253, train/accuracy=0.709395, train/loss=1.407202, validation/accuracy=0.657640, validation/loss=1.641107, validation/num_examples=50000
I0131 19:58:20.546431 139774417409792 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.035917043685913, loss=3.5607380867004395
I0131 19:59:06.339312 139774434195200 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9328243136405945, loss=4.351415634155273
I0131 19:59:52.264257 139774417409792 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.9517442584037781, loss=4.731112480163574
I0131 20:00:38.604265 139774434195200 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.0907691717147827, loss=3.6015918254852295
I0131 20:01:24.412554 139774417409792 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.1495747566223145, loss=3.6761727333068848
I0131 20:02:10.908993 139774434195200 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.0269216299057007, loss=3.570600986480713
I0131 20:02:56.879688 139774417409792 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.1267378330230713, loss=3.6130495071411133
I0131 20:03:42.916883 139774434195200 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.1158396005630493, loss=3.4268815517425537
I0131 20:04:29.234216 139774417409792 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.0785470008850098, loss=3.520991802215576
I0131 20:04:47.137867 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:04:59.070257 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:05:27.941012 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:05:29.549108 139936116377408 submission_runner.py:408] Time since start: 24516.41s, 	Step: 48741, 	{'train/accuracy': 0.7172656059265137, 'train/loss': 1.3772408962249756, 'validation/accuracy': 0.6559399962425232, 'validation/loss': 1.6439871788024902, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.262031078338623, 'test/num_examples': 10000, 'score': 22303.59793663025, 'total_duration': 24516.407782793045, 'accumulated_submission_time': 22303.59793663025, 'accumulated_eval_time': 2208.6510157585144, 'accumulated_logging_time': 1.6165921688079834}
I0131 20:05:29.578705 139774434195200 logging_writer.py:48] [48741] accumulated_eval_time=2208.651016, accumulated_logging_time=1.616592, accumulated_submission_time=22303.597937, global_step=48741, preemption_count=0, score=22303.597937, test/accuracy=0.531800, test/loss=2.262031, test/num_examples=10000, total_duration=24516.407783, train/accuracy=0.717266, train/loss=1.377241, validation/accuracy=0.655940, validation/loss=1.643987, validation/num_examples=50000
I0131 20:05:53.410048 139774417409792 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.047624945640564, loss=3.577056884765625
I0131 20:06:38.377217 139774434195200 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0256468057632446, loss=3.441063642501831
I0131 20:07:24.432723 139774417409792 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9204450249671936, loss=4.972330570220947
I0131 20:08:10.836577 139774434195200 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.9837170243263245, loss=5.396901607513428
I0131 20:08:56.690045 139774417409792 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.0614956617355347, loss=3.5099334716796875
I0131 20:09:42.828898 139774434195200 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.0126867294311523, loss=3.878425121307373
I0131 20:10:29.132003 139774417409792 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.0821329355239868, loss=3.585783004760742
I0131 20:11:14.981840 139774434195200 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.9931806325912476, loss=3.8757858276367188
I0131 20:12:01.121582 139774417409792 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.03617525100708, loss=3.5113987922668457
I0131 20:12:29.840168 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:12:41.779783 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:13:12.307900 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:13:13.913322 139936116377408 submission_runner.py:408] Time since start: 24980.77s, 	Step: 49664, 	{'train/accuracy': 0.71875, 'train/loss': 1.4258836507797241, 'validation/accuracy': 0.6511600017547607, 'validation/loss': 1.7147786617279053, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.343284845352173, 'test/num_examples': 10000, 'score': 22723.800693511963, 'total_duration': 24980.77202296257, 'accumulated_submission_time': 22723.800693511963, 'accumulated_eval_time': 2252.7241654396057, 'accumulated_logging_time': 1.656675100326538}
I0131 20:13:13.938659 139774434195200 logging_writer.py:48] [49664] accumulated_eval_time=2252.724165, accumulated_logging_time=1.656675, accumulated_submission_time=22723.800694, global_step=49664, preemption_count=0, score=22723.800694, test/accuracy=0.528100, test/loss=2.343285, test/num_examples=10000, total_duration=24980.772023, train/accuracy=0.718750, train/loss=1.425884, validation/accuracy=0.651160, validation/loss=1.714779, validation/num_examples=50000
I0131 20:13:28.638657 139774417409792 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0398439168930054, loss=3.500458240509033
I0131 20:14:11.921107 139774434195200 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.9986797571182251, loss=5.3014044761657715
I0131 20:14:58.135420 139774417409792 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.0415244102478027, loss=4.816353797912598
I0131 20:15:44.464773 139774434195200 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1291431188583374, loss=3.5054690837860107
I0131 20:16:30.756537 139774417409792 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9590956568717957, loss=4.512614727020264
I0131 20:17:16.799519 139774434195200 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0400359630584717, loss=3.7068583965301514
I0131 20:18:02.907204 139774417409792 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.00071382522583, loss=4.5465826988220215
I0131 20:18:48.799860 139774434195200 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.0274097919464111, loss=4.030836582183838
I0131 20:19:35.046513 139774417409792 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0376288890838623, loss=3.541330099105835
I0131 20:20:14.146078 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:20:26.081110 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:20:55.523636 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:20:57.130476 139936116377408 submission_runner.py:408] Time since start: 25443.99s, 	Step: 50586, 	{'train/accuracy': 0.7098437547683716, 'train/loss': 1.4361015558242798, 'validation/accuracy': 0.6548399925231934, 'validation/loss': 1.6722317934036255, 'validation/num_examples': 50000, 'test/accuracy': 0.5351999998092651, 'test/loss': 2.278514862060547, 'test/num_examples': 10000, 'score': 23143.94828104973, 'total_duration': 25443.989156007767, 'accumulated_submission_time': 23143.94828104973, 'accumulated_eval_time': 2295.7085387706757, 'accumulated_logging_time': 1.69374680519104}
I0131 20:20:57.159482 139774434195200 logging_writer.py:48] [50586] accumulated_eval_time=2295.708539, accumulated_logging_time=1.693747, accumulated_submission_time=23143.948281, global_step=50586, preemption_count=0, score=23143.948281, test/accuracy=0.535200, test/loss=2.278515, test/num_examples=10000, total_duration=25443.989156, train/accuracy=0.709844, train/loss=1.436102, validation/accuracy=0.654840, validation/loss=1.672232, validation/num_examples=50000
I0131 20:21:03.116325 139774417409792 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.0426498651504517, loss=5.3006510734558105
I0131 20:21:45.222424 139774434195200 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.9845496416091919, loss=4.035548686981201
I0131 20:22:31.324726 139774417409792 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.0562492609024048, loss=3.5158534049987793
I0131 20:23:17.665282 139774434195200 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.072468876838684, loss=3.5410120487213135
I0131 20:24:03.782099 139774417409792 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0437623262405396, loss=3.965454578399658
I0131 20:24:49.795029 139774434195200 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.079793930053711, loss=3.551589250564575
I0131 20:25:36.055383 139774417409792 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.080630898475647, loss=3.5083258152008057
I0131 20:26:22.610280 139774434195200 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.0369309186935425, loss=3.539015293121338
I0131 20:27:08.748434 139774417409792 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.0224549770355225, loss=3.8089709281921387
I0131 20:27:54.686739 139774434195200 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.0230793952941895, loss=3.650047779083252
I0131 20:27:57.557106 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:28:09.781938 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:28:37.418296 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:28:39.024837 139936116377408 submission_runner.py:408] Time since start: 25905.88s, 	Step: 51508, 	{'train/accuracy': 0.71498042345047, 'train/loss': 1.3889296054840088, 'validation/accuracy': 0.6620399951934814, 'validation/loss': 1.6310160160064697, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.247091293334961, 'test/num_examples': 10000, 'score': 23564.28750729561, 'total_duration': 25905.883530139923, 'accumulated_submission_time': 23564.28750729561, 'accumulated_eval_time': 2337.176256418228, 'accumulated_logging_time': 1.7330005168914795}
I0131 20:28:39.050888 139774417409792 logging_writer.py:48] [51508] accumulated_eval_time=2337.176256, accumulated_logging_time=1.733001, accumulated_submission_time=23564.287507, global_step=51508, preemption_count=0, score=23564.287507, test/accuracy=0.543500, test/loss=2.247091, test/num_examples=10000, total_duration=25905.883530, train/accuracy=0.714980, train/loss=1.388930, validation/accuracy=0.662040, validation/loss=1.631016, validation/num_examples=50000
I0131 20:29:16.905761 139774434195200 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.0011571645736694, loss=4.764309883117676
I0131 20:30:02.659197 139774417409792 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.1200709342956543, loss=3.5502543449401855
I0131 20:30:48.931302 139774434195200 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.9386879801750183, loss=4.227795600891113
I0131 20:31:35.166352 139774417409792 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9942483901977539, loss=4.061897277832031
I0131 20:32:21.313041 139774434195200 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0784318447113037, loss=3.843289852142334
I0131 20:33:07.309445 139774417409792 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.0556615591049194, loss=4.360478401184082
I0131 20:33:53.136945 139774434195200 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.1305742263793945, loss=3.642411708831787
I0131 20:34:39.035248 139774417409792 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9990394115447998, loss=3.7885217666625977
I0131 20:35:25.226381 139774434195200 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.9718195796012878, loss=3.628462553024292
I0131 20:35:39.195512 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:35:51.088915 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:36:18.105252 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:36:19.706484 139936116377408 submission_runner.py:408] Time since start: 26366.57s, 	Step: 52432, 	{'train/accuracy': 0.7337304353713989, 'train/loss': 1.2980376482009888, 'validation/accuracy': 0.6647999882698059, 'validation/loss': 1.5864545106887817, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.2272799015045166, 'test/num_examples': 10000, 'score': 23984.374716758728, 'total_duration': 26366.565184354782, 'accumulated_submission_time': 23984.374716758728, 'accumulated_eval_time': 2377.6872231960297, 'accumulated_logging_time': 1.7685375213623047}
I0131 20:36:19.735331 139774417409792 logging_writer.py:48] [52432] accumulated_eval_time=2377.687223, accumulated_logging_time=1.768538, accumulated_submission_time=23984.374717, global_step=52432, preemption_count=0, score=23984.374717, test/accuracy=0.535400, test/loss=2.227280, test/num_examples=10000, total_duration=26366.565184, train/accuracy=0.733730, train/loss=1.298038, validation/accuracy=0.664800, validation/loss=1.586455, validation/num_examples=50000
I0131 20:36:47.173659 139774434195200 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0604714155197144, loss=3.5167033672332764
I0131 20:37:32.928551 139774417409792 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.95359206199646, loss=3.939265727996826
I0131 20:38:18.786856 139774434195200 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.0095294713974, loss=3.5248403549194336
I0131 20:39:05.398780 139774417409792 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.0805851221084595, loss=3.5829954147338867
I0131 20:39:51.270172 139774434195200 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.9699868559837341, loss=4.465234756469727
I0131 20:40:37.175583 139774417409792 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9471438527107239, loss=5.056753635406494
I0131 20:41:23.449573 139774434195200 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2140332460403442, loss=3.491560697555542
I0131 20:42:09.426853 139774417409792 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.0666941404342651, loss=3.5888750553131104
I0131 20:42:55.559621 139774434195200 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.174983024597168, loss=5.206906795501709
I0131 20:43:19.783334 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:43:31.580484 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:43:58.353406 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:43:59.956956 139936116377408 submission_runner.py:408] Time since start: 26826.82s, 	Step: 53354, 	{'train/accuracy': 0.7135351300239563, 'train/loss': 1.3821560144424438, 'validation/accuracy': 0.6652799844741821, 'validation/loss': 1.5981616973876953, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.2386462688446045, 'test/num_examples': 10000, 'score': 24404.365221500397, 'total_duration': 26826.81566309929, 'accumulated_submission_time': 24404.365221500397, 'accumulated_eval_time': 2417.860870361328, 'accumulated_logging_time': 1.8072423934936523}
I0131 20:43:59.984534 139774417409792 logging_writer.py:48] [53354] accumulated_eval_time=2417.860870, accumulated_logging_time=1.807242, accumulated_submission_time=24404.365222, global_step=53354, preemption_count=0, score=24404.365222, test/accuracy=0.532200, test/loss=2.238646, test/num_examples=10000, total_duration=26826.815663, train/accuracy=0.713535, train/loss=1.382156, validation/accuracy=0.665280, validation/loss=1.598162, validation/num_examples=50000
I0131 20:44:18.656678 139774434195200 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.051186203956604, loss=3.6557095050811768
I0131 20:45:02.706545 139774417409792 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.991008460521698, loss=3.745008945465088
I0131 20:45:48.918190 139774434195200 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.116733431816101, loss=3.5174448490142822
I0131 20:46:35.426023 139774417409792 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.9680104851722717, loss=3.9884092807769775
I0131 20:47:21.566518 139774434195200 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9686324596405029, loss=4.026438236236572
I0131 20:48:07.830526 139774417409792 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.9647688865661621, loss=4.6692094802856445
I0131 20:48:53.877357 139774434195200 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0410809516906738, loss=4.626522064208984
I0131 20:49:40.015531 139774417409792 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9734575748443604, loss=4.051560878753662
I0131 20:50:26.268599 139774434195200 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.120597243309021, loss=3.491680860519409
I0131 20:50:59.986976 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:51:12.138318 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:51:41.605063 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:51:43.207564 139936116377408 submission_runner.py:408] Time since start: 27290.07s, 	Step: 54275, 	{'train/accuracy': 0.7189062237739563, 'train/loss': 1.361011028289795, 'validation/accuracy': 0.6606799960136414, 'validation/loss': 1.6143720149993896, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.222517967224121, 'test/num_examples': 10000, 'score': 24824.307297229767, 'total_duration': 27290.06626176834, 'accumulated_submission_time': 24824.307297229767, 'accumulated_eval_time': 2461.081460237503, 'accumulated_logging_time': 1.847649097442627}
I0131 20:51:43.233778 139774417409792 logging_writer.py:48] [54275] accumulated_eval_time=2461.081460, accumulated_logging_time=1.847649, accumulated_submission_time=24824.307297, global_step=54275, preemption_count=0, score=24824.307297, test/accuracy=0.545200, test/loss=2.222518, test/num_examples=10000, total_duration=27290.066262, train/accuracy=0.718906, train/loss=1.361011, validation/accuracy=0.660680, validation/loss=1.614372, validation/num_examples=50000
I0131 20:51:53.566283 139774434195200 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9862840175628662, loss=4.304457664489746
I0131 20:52:36.294315 139774417409792 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.146738052368164, loss=3.4986822605133057
I0131 20:53:22.205450 139774434195200 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.0899014472961426, loss=3.508345603942871
I0131 20:54:08.415304 139774417409792 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.9487064480781555, loss=3.936194658279419
I0131 20:54:54.516638 139774434195200 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.1097426414489746, loss=3.5898594856262207
I0131 20:55:40.980258 139774417409792 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.9599817991256714, loss=4.617040634155273
I0131 20:56:27.325524 139774434195200 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.301499366760254, loss=3.4864978790283203
I0131 20:57:13.520972 139774417409792 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2022697925567627, loss=3.5464696884155273
I0131 20:58:00.049205 139774434195200 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0481208562850952, loss=3.4197583198547363
I0131 20:58:43.536727 139936116377408 spec.py:321] Evaluating on the training split.
I0131 20:58:56.120716 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 20:59:21.196899 139936116377408 spec.py:349] Evaluating on the test split.
I0131 20:59:22.810068 139936116377408 submission_runner.py:408] Time since start: 27749.67s, 	Step: 55196, 	{'train/accuracy': 0.7296484112739563, 'train/loss': 1.3426628112792969, 'validation/accuracy': 0.664139986038208, 'validation/loss': 1.6135691404342651, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.220703601837158, 'test/num_examples': 10000, 'score': 25244.552402496338, 'total_duration': 27749.66876244545, 'accumulated_submission_time': 25244.552402496338, 'accumulated_eval_time': 2500.354782104492, 'accumulated_logging_time': 1.8835597038269043}
I0131 20:59:22.833820 139774417409792 logging_writer.py:48] [55196] accumulated_eval_time=2500.354782, accumulated_logging_time=1.883560, accumulated_submission_time=25244.552402, global_step=55196, preemption_count=0, score=25244.552402, test/accuracy=0.543500, test/loss=2.220704, test/num_examples=10000, total_duration=27749.668762, train/accuracy=0.729648, train/loss=1.342663, validation/accuracy=0.664140, validation/loss=1.613569, validation/num_examples=50000
I0131 20:59:24.826061 139774434195200 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.0288830995559692, loss=3.5936529636383057
I0131 21:00:06.458286 139774417409792 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1752623319625854, loss=3.5195155143737793
I0131 21:00:52.152830 139774434195200 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.0673894882202148, loss=3.4390554428100586
I0131 21:01:38.541734 139774417409792 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9885754585266113, loss=3.4819161891937256
I0131 21:02:24.705152 139774434195200 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.074769139289856, loss=3.576908826828003
I0131 21:03:10.769318 139774417409792 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.0777652263641357, loss=3.53680682182312
I0131 21:03:56.694473 139774434195200 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0997689962387085, loss=3.4825778007507324
I0131 21:04:42.472957 139774417409792 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.0775437355041504, loss=3.4946610927581787
I0131 21:05:28.625555 139774434195200 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.026694655418396, loss=3.6585822105407715
I0131 21:06:14.747106 139774417409792 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.1257468461990356, loss=3.8628056049346924
I0131 21:06:23.087464 139936116377408 spec.py:321] Evaluating on the training split.
I0131 21:06:35.005484 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 21:07:07.384465 139936116377408 spec.py:349] Evaluating on the test split.
I0131 21:07:08.985688 139936116377408 submission_runner.py:408] Time since start: 28215.84s, 	Step: 56120, 	{'train/accuracy': 0.7380273342132568, 'train/loss': 1.2842124700546265, 'validation/accuracy': 0.6672599911689758, 'validation/loss': 1.5820810794830322, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.1891727447509766, 'test/num_examples': 10000, 'score': 25664.747946739197, 'total_duration': 28215.844392061234, 'accumulated_submission_time': 25664.747946739197, 'accumulated_eval_time': 2546.253002643585, 'accumulated_logging_time': 1.9170572757720947}
I0131 21:07:09.009295 139774434195200 logging_writer.py:48] [56120] accumulated_eval_time=2546.253003, accumulated_logging_time=1.917057, accumulated_submission_time=25664.747947, global_step=56120, preemption_count=0, score=25664.747947, test/accuracy=0.550200, test/loss=2.189173, test/num_examples=10000, total_duration=28215.844392, train/accuracy=0.738027, train/loss=1.284212, validation/accuracy=0.667260, validation/loss=1.582081, validation/num_examples=50000
I0131 21:07:41.565458 139774417409792 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0037617683410645, loss=3.4461216926574707
I0131 21:08:27.895994 139774434195200 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.0864574909210205, loss=3.5058960914611816
I0131 21:09:14.121198 139774417409792 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2144831418991089, loss=3.559648036956787
I0131 21:10:00.345036 139774434195200 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.0801702737808228, loss=3.582014799118042
I0131 21:10:46.527477 139774417409792 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.077194333076477, loss=5.2843546867370605
I0131 21:11:32.529260 139774434195200 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2095736265182495, loss=3.4536428451538086
I0131 21:12:18.755545 139774417409792 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.0226383209228516, loss=4.81120491027832
I0131 21:13:04.768023 139774434195200 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.953194797039032, loss=4.864230632781982
I0131 21:13:50.720132 139774417409792 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.0165890455245972, loss=4.832343101501465
I0131 21:14:09.324860 139936116377408 spec.py:321] Evaluating on the training split.
I0131 21:14:21.145265 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 21:14:51.212664 139936116377408 spec.py:349] Evaluating on the test split.
I0131 21:14:52.814527 139936116377408 submission_runner.py:408] Time since start: 28679.67s, 	Step: 57042, 	{'train/accuracy': 0.7276562452316284, 'train/loss': 1.3468148708343506, 'validation/accuracy': 0.6674000024795532, 'validation/loss': 1.603977918624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5464000105857849, 'test/loss': 2.211744546890259, 'test/num_examples': 10000, 'score': 26085.003512620926, 'total_duration': 28679.673223495483, 'accumulated_submission_time': 26085.003512620926, 'accumulated_eval_time': 2589.74267745018, 'accumulated_logging_time': 1.9526276588439941}
I0131 21:14:52.840450 139774434195200 logging_writer.py:48] [57042] accumulated_eval_time=2589.742677, accumulated_logging_time=1.952628, accumulated_submission_time=26085.003513, global_step=57042, preemption_count=0, score=26085.003513, test/accuracy=0.546400, test/loss=2.211745, test/num_examples=10000, total_duration=28679.673223, train/accuracy=0.727656, train/loss=1.346815, validation/accuracy=0.667400, validation/loss=1.603978, validation/num_examples=50000
I0131 21:15:16.285979 139774417409792 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0300995111465454, loss=4.883240222930908
I0131 21:16:01.285860 139774434195200 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9310336709022522, loss=4.739978790283203
I0131 21:16:47.514597 139774417409792 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.071885585784912, loss=3.9834580421447754
I0131 21:17:33.933821 139774434195200 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.200743556022644, loss=3.445863723754883
I0131 21:18:20.032760 139774417409792 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.1328282356262207, loss=3.64268159866333
I0131 21:19:06.537888 139774434195200 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9882591366767883, loss=4.223711013793945
I0131 21:19:52.416825 139774417409792 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.0717453956604004, loss=3.5166144371032715
I0131 21:20:38.462610 139774434195200 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1419190168380737, loss=3.9292032718658447
I0131 21:21:24.588506 139774417409792 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.1224255561828613, loss=3.576307773590088
I0131 21:21:52.986369 139936116377408 spec.py:321] Evaluating on the training split.
I0131 21:22:04.937595 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 21:22:32.925444 139936116377408 spec.py:349] Evaluating on the test split.
I0131 21:22:34.533076 139936116377408 submission_runner.py:408] Time since start: 29141.39s, 	Step: 57963, 	{'train/accuracy': 0.7269140481948853, 'train/loss': 1.328726887702942, 'validation/accuracy': 0.667419970035553, 'validation/loss': 1.5865769386291504, 'validation/num_examples': 50000, 'test/accuracy': 0.5430999994277954, 'test/loss': 2.196040153503418, 'test/num_examples': 10000, 'score': 26505.091208696365, 'total_duration': 29141.391778469086, 'accumulated_submission_time': 26505.091208696365, 'accumulated_eval_time': 2631.2893874645233, 'accumulated_logging_time': 1.9895341396331787}
I0131 21:22:34.559357 139774434195200 logging_writer.py:48] [57963] accumulated_eval_time=2631.289387, accumulated_logging_time=1.989534, accumulated_submission_time=26505.091209, global_step=57963, preemption_count=0, score=26505.091209, test/accuracy=0.543100, test/loss=2.196040, test/num_examples=10000, total_duration=29141.391778, train/accuracy=0.726914, train/loss=1.328727, validation/accuracy=0.667420, validation/loss=1.586577, validation/num_examples=50000
I0131 21:22:49.655312 139774417409792 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1176749467849731, loss=3.481571674346924
I0131 21:23:33.011062 139774434195200 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.0538830757141113, loss=4.511286735534668
I0131 21:24:19.138063 139774417409792 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.0523468255996704, loss=4.665025234222412
I0131 21:25:05.570830 139774434195200 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0775425434112549, loss=3.4687423706054688
I0131 21:25:51.592940 139774417409792 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.1542856693267822, loss=3.4582924842834473
I0131 21:26:37.645665 139774434195200 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.9410460591316223, loss=4.2724151611328125
I0131 21:27:24.208340 139774417409792 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.0764782428741455, loss=3.7045254707336426
I0131 21:28:10.315250 139774434195200 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.1605054140090942, loss=3.4702179431915283
I0131 21:28:56.709137 139774417409792 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.0911489725112915, loss=3.4924075603485107
I0131 21:29:34.747055 139936116377408 spec.py:321] Evaluating on the training split.
I0131 21:29:47.714709 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 21:30:16.976583 139936116377408 spec.py:349] Evaluating on the test split.
I0131 21:30:18.585480 139936116377408 submission_runner.py:408] Time since start: 29605.44s, 	Step: 58883, 	{'train/accuracy': 0.7373241782188416, 'train/loss': 1.2906686067581177, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.6132616996765137, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.2018980979919434, 'test/num_examples': 10000, 'score': 26925.22168302536, 'total_duration': 29605.44416475296, 'accumulated_submission_time': 26925.22168302536, 'accumulated_eval_time': 2675.127780675888, 'accumulated_logging_time': 2.025451183319092}
I0131 21:30:18.611201 139774434195200 logging_writer.py:48] [58883] accumulated_eval_time=2675.127781, accumulated_logging_time=2.025451, accumulated_submission_time=26925.221683, global_step=58883, preemption_count=0, score=26925.221683, test/accuracy=0.545700, test/loss=2.201898, test/num_examples=10000, total_duration=29605.444165, train/accuracy=0.737324, train/loss=1.290669, validation/accuracy=0.666960, validation/loss=1.613262, validation/num_examples=50000
I0131 21:30:25.769194 139774417409792 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.0436375141143799, loss=4.465311527252197
I0131 21:31:08.051911 139774434195200 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.087351679801941, loss=3.4505696296691895
I0131 21:31:54.203327 139774417409792 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0742024183273315, loss=3.9404852390289307
I0131 21:32:40.721868 139774434195200 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.2392128705978394, loss=3.4701409339904785
I0131 21:33:26.890740 139774417409792 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.0420867204666138, loss=5.180542469024658
I0131 21:34:13.167272 139774434195200 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.0041855573654175, loss=5.323398113250732
I0131 21:34:59.327979 139774417409792 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.019588589668274, loss=5.262835502624512
I0131 21:35:45.592800 139774434195200 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.1426880359649658, loss=3.3708014488220215
I0131 21:36:31.937598 139774417409792 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.0080620050430298, loss=5.024986743927002
I0131 21:37:18.339465 139774434195200 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.0432052612304688, loss=3.710160255432129
I0131 21:37:18.894773 139936116377408 spec.py:321] Evaluating on the training split.
I0131 21:37:30.878743 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 21:38:01.086348 139936116377408 spec.py:349] Evaluating on the test split.
I0131 21:38:02.711132 139936116377408 submission_runner.py:408] Time since start: 30069.57s, 	Step: 59803, 	{'train/accuracy': 0.7282031178474426, 'train/loss': 1.2871874570846558, 'validation/accuracy': 0.6700199842453003, 'validation/loss': 1.5382176637649536, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.153144598007202, 'test/num_examples': 10000, 'score': 27345.44739818573, 'total_duration': 30069.56980085373, 'accumulated_submission_time': 27345.44739818573, 'accumulated_eval_time': 2718.944101333618, 'accumulated_logging_time': 2.061138868331909}
I0131 21:38:02.738155 139774417409792 logging_writer.py:48] [59803] accumulated_eval_time=2718.944101, accumulated_logging_time=2.061139, accumulated_submission_time=27345.447398, global_step=59803, preemption_count=0, score=27345.447398, test/accuracy=0.547800, test/loss=2.153145, test/num_examples=10000, total_duration=30069.569801, train/accuracy=0.728203, train/loss=1.287187, validation/accuracy=0.670020, validation/loss=1.538218, validation/num_examples=50000
I0131 21:38:42.978133 139774434195200 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.0376516580581665, loss=3.4340739250183105
I0131 21:39:28.813986 139774417409792 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0680431127548218, loss=4.134340763092041
I0131 21:40:15.471459 139774434195200 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.1484830379486084, loss=3.512564182281494
I0131 21:41:01.559060 139774417409792 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.1231015920639038, loss=3.49373722076416
I0131 21:41:47.917359 139774434195200 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0819989442825317, loss=4.449764251708984
I0131 21:42:34.335644 139774417409792 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.123613715171814, loss=3.384491443634033
I0131 21:43:20.570686 139774434195200 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.1412341594696045, loss=3.523386001586914
I0131 21:44:06.940955 139774417409792 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1407835483551025, loss=3.7449722290039062
I0131 21:44:52.818118 139774434195200 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.08056640625, loss=3.530543327331543
I0131 21:45:03.145433 139936116377408 spec.py:321] Evaluating on the training split.
I0131 21:45:14.855629 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 21:45:45.045301 139936116377408 spec.py:349] Evaluating on the test split.
I0131 21:45:46.650064 139936116377408 submission_runner.py:408] Time since start: 30533.51s, 	Step: 60724, 	{'train/accuracy': 0.733593761920929, 'train/loss': 1.3361026048660278, 'validation/accuracy': 0.6747199892997742, 'validation/loss': 1.5849941968917847, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.2170250415802, 'test/num_examples': 10000, 'score': 27765.797261953354, 'total_duration': 30533.50876736641, 'accumulated_submission_time': 27765.797261953354, 'accumulated_eval_time': 2762.44873046875, 'accumulated_logging_time': 2.0978400707244873}
I0131 21:45:46.675262 139774417409792 logging_writer.py:48] [60724] accumulated_eval_time=2762.448730, accumulated_logging_time=2.097840, accumulated_submission_time=27765.797262, global_step=60724, preemption_count=0, score=27765.797262, test/accuracy=0.545400, test/loss=2.217025, test/num_examples=10000, total_duration=30533.508767, train/accuracy=0.733594, train/loss=1.336103, validation/accuracy=0.674720, validation/loss=1.584994, validation/num_examples=50000
I0131 21:46:17.264214 139774434195200 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.1019866466522217, loss=3.5307579040527344
I0131 21:47:02.655793 139774417409792 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.035118818283081, loss=4.180088043212891
I0131 21:47:48.858512 139774434195200 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2524092197418213, loss=4.722123146057129
I0131 21:48:35.651098 139774417409792 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0975992679595947, loss=3.583277940750122
I0131 21:49:21.669072 139774434195200 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.0567615032196045, loss=4.478240489959717
I0131 21:50:08.204634 139774417409792 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.0677354335784912, loss=3.494215488433838
I0131 21:50:54.263446 139774434195200 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.231904149055481, loss=3.47257924079895
I0131 21:51:40.724183 139774417409792 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.1830761432647705, loss=3.4187893867492676
I0131 21:52:26.996279 139774434195200 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1092482805252075, loss=3.659538507461548
I0131 21:52:46.924257 139936116377408 spec.py:321] Evaluating on the training split.
I0131 21:52:58.778120 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 21:53:28.351428 139936116377408 spec.py:349] Evaluating on the test split.
I0131 21:53:29.960290 139936116377408 submission_runner.py:408] Time since start: 30996.82s, 	Step: 61645, 	{'train/accuracy': 0.7450000047683716, 'train/loss': 1.2544151544570923, 'validation/accuracy': 0.6762799620628357, 'validation/loss': 1.5565282106399536, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.175717830657959, 'test/num_examples': 10000, 'score': 28185.988286972046, 'total_duration': 30996.818990707397, 'accumulated_submission_time': 28185.988286972046, 'accumulated_eval_time': 2805.484763622284, 'accumulated_logging_time': 2.13291072845459}
I0131 21:53:29.984672 139774417409792 logging_writer.py:48] [61645] accumulated_eval_time=2805.484764, accumulated_logging_time=2.132911, accumulated_submission_time=28185.988287, global_step=61645, preemption_count=0, score=28185.988287, test/accuracy=0.550800, test/loss=2.175718, test/num_examples=10000, total_duration=30996.818991, train/accuracy=0.745000, train/loss=1.254415, validation/accuracy=0.676280, validation/loss=1.556528, validation/num_examples=50000
I0131 21:53:52.218541 139774434195200 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.0669010877609253, loss=4.454754829406738
I0131 21:54:36.796936 139774417409792 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.0962467193603516, loss=3.484990119934082
I0131 21:55:24.401602 139774434195200 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.1900147199630737, loss=3.4103612899780273
I0131 21:56:11.055398 139774417409792 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.114988923072815, loss=3.4258627891540527
I0131 21:56:57.074970 139774434195200 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.071419596672058, loss=3.6761112213134766
I0131 21:57:43.372757 139774417409792 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.1690130233764648, loss=3.4738543033599854
I0131 21:58:29.467949 139774434195200 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0762534141540527, loss=5.1893696784973145
I0131 21:59:15.593656 139774417409792 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.0553867816925049, loss=3.5542943477630615
I0131 22:00:01.723633 139774434195200 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.0760103464126587, loss=3.912463903427124
I0131 22:00:30.343680 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:00:42.385346 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:01:13.161536 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:01:14.760105 139936116377408 submission_runner.py:408] Time since start: 31461.62s, 	Step: 62563, 	{'train/accuracy': 0.7283593416213989, 'train/loss': 1.3276382684707642, 'validation/accuracy': 0.6751199960708618, 'validation/loss': 1.5634821653366089, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.1780121326446533, 'test/num_examples': 10000, 'score': 28606.290912389755, 'total_duration': 31461.618797063828, 'accumulated_submission_time': 28606.290912389755, 'accumulated_eval_time': 2849.9011821746826, 'accumulated_logging_time': 2.1665310859680176}
I0131 22:01:14.788872 139774417409792 logging_writer.py:48] [62563] accumulated_eval_time=2849.901182, accumulated_logging_time=2.166531, accumulated_submission_time=28606.290912, global_step=62563, preemption_count=0, score=28606.290912, test/accuracy=0.552100, test/loss=2.178012, test/num_examples=10000, total_duration=31461.618797, train/accuracy=0.728359, train/loss=1.327638, validation/accuracy=0.675120, validation/loss=1.563482, validation/num_examples=50000
I0131 22:01:29.868581 139774434195200 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0363224744796753, loss=3.5302894115448
I0131 22:02:13.190279 139774417409792 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.007265329360962, loss=4.493085861206055
I0131 22:02:59.362562 139774434195200 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.083914041519165, loss=4.974817276000977
I0131 22:03:45.713186 139774417409792 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1794297695159912, loss=3.4464635848999023
I0131 22:04:31.696876 139774434195200 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.2193619012832642, loss=3.4334540367126465
I0131 22:05:18.104277 139774417409792 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1572794914245605, loss=3.4665422439575195
I0131 22:06:04.557723 139774434195200 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0102812051773071, loss=4.689684867858887
I0131 22:06:50.751062 139774417409792 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.9929695129394531, loss=3.7622439861297607
I0131 22:07:37.115307 139774434195200 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0347671508789062, loss=5.149374008178711
I0131 22:08:14.864399 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:08:26.807874 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:08:58.237642 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:08:59.851393 139936116377408 submission_runner.py:408] Time since start: 31926.71s, 	Step: 63483, 	{'train/accuracy': 0.7360742092132568, 'train/loss': 1.2766711711883545, 'validation/accuracy': 0.6779199838638306, 'validation/loss': 1.5261484384536743, 'validation/num_examples': 50000, 'test/accuracy': 0.557200014591217, 'test/loss': 2.149172306060791, 'test/num_examples': 10000, 'score': 29026.310174703598, 'total_duration': 31926.710065841675, 'accumulated_submission_time': 29026.310174703598, 'accumulated_eval_time': 2894.8881330490112, 'accumulated_logging_time': 2.20430326461792}
I0131 22:08:59.886001 139774417409792 logging_writer.py:48] [63483] accumulated_eval_time=2894.888133, accumulated_logging_time=2.204303, accumulated_submission_time=29026.310175, global_step=63483, preemption_count=0, score=29026.310175, test/accuracy=0.557200, test/loss=2.149172, test/num_examples=10000, total_duration=31926.710066, train/accuracy=0.736074, train/loss=1.276671, validation/accuracy=0.677920, validation/loss=1.526148, validation/num_examples=50000
I0131 22:09:07.036791 139774434195200 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.0489838123321533, loss=4.132007598876953
I0131 22:09:49.158313 139774417409792 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.9907780885696411, loss=4.738025188446045
I0131 22:10:35.274503 139774434195200 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1991727352142334, loss=3.4761104583740234
I0131 22:11:21.775973 139774417409792 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.006438970565796, loss=4.710279941558838
I0131 22:12:08.143855 139774434195200 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.1473720073699951, loss=4.805136680603027
I0131 22:12:54.236166 139774417409792 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1538722515106201, loss=3.4260525703430176
I0131 22:13:40.304844 139774434195200 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.1644214391708374, loss=3.6552629470825195
I0131 22:14:26.589540 139774417409792 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.9959657788276672, loss=5.131350517272949
I0131 22:15:12.744447 139774434195200 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1519088745117188, loss=3.382805109024048
I0131 22:15:58.877274 139774417409792 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9906224608421326, loss=5.0268683433532715
I0131 22:15:59.886522 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:16:12.062191 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:16:41.300656 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:16:42.918985 139936116377408 submission_runner.py:408] Time since start: 32389.78s, 	Step: 64404, 	{'train/accuracy': 0.7432031035423279, 'train/loss': 1.2548317909240723, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.5307791233062744, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.1536831855773926, 'test/num_examples': 10000, 'score': 29446.250809907913, 'total_duration': 32389.777686357498, 'accumulated_submission_time': 29446.250809907913, 'accumulated_eval_time': 2937.9205870628357, 'accumulated_logging_time': 2.25028133392334}
I0131 22:16:42.943444 139774434195200 logging_writer.py:48] [64404] accumulated_eval_time=2937.920587, accumulated_logging_time=2.250281, accumulated_submission_time=29446.250810, global_step=64404, preemption_count=0, score=29446.250810, test/accuracy=0.548100, test/loss=2.153683, test/num_examples=10000, total_duration=32389.777686, train/accuracy=0.743203, train/loss=1.254832, validation/accuracy=0.679880, validation/loss=1.530779, validation/num_examples=50000
I0131 22:17:22.575700 139774417409792 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1589195728302002, loss=3.4562039375305176
I0131 22:18:08.526620 139774434195200 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0801236629486084, loss=3.7603824138641357
I0131 22:18:54.755827 139774417409792 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2068185806274414, loss=3.4453930854797363
I0131 22:19:41.202742 139774434195200 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.011296033859253, loss=3.722952365875244
I0131 22:20:27.560155 139774417409792 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.0556530952453613, loss=4.104745388031006
I0131 22:21:14.040182 139774434195200 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.0784738063812256, loss=4.89079475402832
I0131 22:22:00.506889 139774417409792 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.9381484389305115, loss=4.1154093742370605
I0131 22:22:46.506115 139774434195200 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.168841004371643, loss=3.520907402038574
I0131 22:23:32.837494 139774417409792 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1371456384658813, loss=5.174233436584473
I0131 22:23:43.131062 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:23:54.980591 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:24:19.856651 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:24:21.482763 139936116377408 submission_runner.py:408] Time since start: 32848.34s, 	Step: 65324, 	{'train/accuracy': 0.7388671636581421, 'train/loss': 1.2673910856246948, 'validation/accuracy': 0.6782000064849854, 'validation/loss': 1.518659234046936, 'validation/num_examples': 50000, 'test/accuracy': 0.5658000111579895, 'test/loss': 2.10740065574646, 'test/num_examples': 10000, 'score': 29866.379588842392, 'total_duration': 32848.34146499634, 'accumulated_submission_time': 29866.379588842392, 'accumulated_eval_time': 2976.272282600403, 'accumulated_logging_time': 2.2853519916534424}
I0131 22:24:21.510869 139774434195200 logging_writer.py:48] [65324] accumulated_eval_time=2976.272283, accumulated_logging_time=2.285352, accumulated_submission_time=29866.379589, global_step=65324, preemption_count=0, score=29866.379589, test/accuracy=0.565800, test/loss=2.107401, test/num_examples=10000, total_duration=32848.341465, train/accuracy=0.738867, train/loss=1.267391, validation/accuracy=0.678200, validation/loss=1.518659, validation/num_examples=50000
I0131 22:24:52.084881 139774417409792 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.0552419424057007, loss=3.4606423377990723
I0131 22:25:37.828297 139774434195200 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0991367101669312, loss=3.5500879287719727
I0131 22:26:24.283703 139774417409792 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.1185420751571655, loss=3.627577543258667
I0131 22:27:10.874333 139774434195200 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2762371301651, loss=3.427342414855957
I0131 22:27:57.031463 139774417409792 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.1611300706863403, loss=3.3988845348358154
I0131 22:28:43.174512 139774434195200 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.1046911478042603, loss=3.862644910812378
I0131 22:29:29.502616 139774417409792 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.1460862159729004, loss=3.367546558380127
I0131 22:30:15.871508 139774434195200 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.9897313714027405, loss=4.662353992462158
I0131 22:31:02.089448 139774417409792 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0474976301193237, loss=3.6323115825653076
I0131 22:31:21.643994 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:31:33.661696 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:32:04.019278 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:32:05.624070 139936116377408 submission_runner.py:408] Time since start: 33312.48s, 	Step: 66244, 	{'train/accuracy': 0.7400780916213989, 'train/loss': 1.2585868835449219, 'validation/accuracy': 0.6813799738883972, 'validation/loss': 1.5165966749191284, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.1452724933624268, 'test/num_examples': 10000, 'score': 30286.45598578453, 'total_duration': 33312.4827747345, 'accumulated_submission_time': 30286.45598578453, 'accumulated_eval_time': 3020.252357006073, 'accumulated_logging_time': 2.3225948810577393}
I0131 22:32:05.652754 139774434195200 logging_writer.py:48] [66244] accumulated_eval_time=3020.252357, accumulated_logging_time=2.322595, accumulated_submission_time=30286.455986, global_step=66244, preemption_count=0, score=30286.455986, test/accuracy=0.554700, test/loss=2.145272, test/num_examples=10000, total_duration=33312.482775, train/accuracy=0.740078, train/loss=1.258587, validation/accuracy=0.681380, validation/loss=1.516597, validation/num_examples=50000
I0131 22:32:28.290888 139774417409792 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.1308692693710327, loss=3.4423012733459473
I0131 22:33:12.641758 139774434195200 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0471874475479126, loss=3.688566207885742
I0131 22:33:58.903185 139774417409792 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1388976573944092, loss=3.340284824371338
I0131 22:34:45.599318 139774434195200 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1620697975158691, loss=3.493866443634033
I0131 22:35:31.713634 139774417409792 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.1026592254638672, loss=5.120944023132324
I0131 22:36:18.116695 139774434195200 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.0797396898269653, loss=3.817368984222412
I0131 22:37:04.361277 139774417409792 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.194445013999939, loss=4.507521629333496
I0131 22:37:50.331345 139774434195200 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.0549592971801758, loss=4.997645378112793
I0131 22:38:36.462521 139774417409792 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.170997142791748, loss=3.4605355262756348
I0131 22:39:05.880166 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:39:17.901852 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:39:45.238431 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:39:46.840541 139936116377408 submission_runner.py:408] Time since start: 33773.70s, 	Step: 67165, 	{'train/accuracy': 0.7415820360183716, 'train/loss': 1.2981585264205933, 'validation/accuracy': 0.6811999678611755, 'validation/loss': 1.5677835941314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.169353723526001, 'test/num_examples': 10000, 'score': 30706.626230478287, 'total_duration': 33773.69924163818, 'accumulated_submission_time': 30706.626230478287, 'accumulated_eval_time': 3061.212740421295, 'accumulated_logging_time': 2.3604607582092285}
I0131 22:39:46.865279 139774434195200 logging_writer.py:48] [67165] accumulated_eval_time=3061.212740, accumulated_logging_time=2.360461, accumulated_submission_time=30706.626230, global_step=67165, preemption_count=0, score=30706.626230, test/accuracy=0.556100, test/loss=2.169354, test/num_examples=10000, total_duration=33773.699242, train/accuracy=0.741582, train/loss=1.298159, validation/accuracy=0.681200, validation/loss=1.567784, validation/num_examples=50000
I0131 22:40:01.175157 139774417409792 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.0751286745071411, loss=4.4183173179626465
I0131 22:40:44.318500 139774434195200 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.015051007270813, loss=4.442846298217773
I0131 22:41:30.595055 139774417409792 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3169304132461548, loss=3.7204511165618896
I0131 22:42:17.224323 139774434195200 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1346949338912964, loss=3.4820098876953125
I0131 22:43:03.740080 139774417409792 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.1089849472045898, loss=3.5333995819091797
I0131 22:43:49.868766 139774434195200 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1365504264831543, loss=3.420199155807495
I0131 22:44:35.946849 139774417409792 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.2434790134429932, loss=3.4350709915161133
I0131 22:45:21.996628 139774434195200 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2030283212661743, loss=5.195702075958252
I0131 22:46:08.357963 139774417409792 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.0560928583145142, loss=3.923166513442993
I0131 22:46:47.130900 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:46:59.026106 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:47:27.121127 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:47:28.730287 139936116377408 submission_runner.py:408] Time since start: 34235.59s, 	Step: 68086, 	{'train/accuracy': 0.7633007764816284, 'train/loss': 1.1590244770050049, 'validation/accuracy': 0.6800999641418457, 'validation/loss': 1.5066139698028564, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.1319308280944824, 'test/num_examples': 10000, 'score': 31126.832008838654, 'total_duration': 34235.588967084885, 'accumulated_submission_time': 31126.832008838654, 'accumulated_eval_time': 3102.8121032714844, 'accumulated_logging_time': 2.3978052139282227}
I0131 22:47:28.758895 139774434195200 logging_writer.py:48] [68086] accumulated_eval_time=3102.812103, accumulated_logging_time=2.397805, accumulated_submission_time=31126.832009, global_step=68086, preemption_count=0, score=31126.832009, test/accuracy=0.557600, test/loss=2.131931, test/num_examples=10000, total_duration=34235.588967, train/accuracy=0.763301, train/loss=1.159024, validation/accuracy=0.680100, validation/loss=1.506614, validation/num_examples=50000
I0131 22:47:34.723019 139774417409792 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.087636947631836, loss=3.8701980113983154
I0131 22:48:16.499315 139774434195200 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1134140491485596, loss=3.7716314792633057
I0131 22:49:02.794460 139774417409792 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.1679984331130981, loss=3.6186652183532715
I0131 22:49:49.292446 139774434195200 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0921645164489746, loss=4.160263538360596
I0131 22:50:35.784961 139774417409792 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.2350096702575684, loss=3.419381856918335
I0131 22:51:21.867400 139774434195200 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.2080068588256836, loss=3.4359707832336426
I0131 22:52:08.411865 139774417409792 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1224182844161987, loss=3.4026472568511963
I0131 22:52:54.774030 139774434195200 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.107684850692749, loss=5.086182117462158
I0131 22:53:41.308055 139774417409792 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.3951168060302734, loss=3.544806957244873
I0131 22:54:27.647024 139774434195200 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1441340446472168, loss=3.3792426586151123
I0131 22:54:28.791278 139936116377408 spec.py:321] Evaluating on the training split.
I0131 22:54:40.629398 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 22:55:10.512137 139936116377408 spec.py:349] Evaluating on the test split.
I0131 22:55:12.127646 139936116377408 submission_runner.py:408] Time since start: 34698.99s, 	Step: 69004, 	{'train/accuracy': 0.7406054735183716, 'train/loss': 1.2410019636154175, 'validation/accuracy': 0.684719979763031, 'validation/loss': 1.4892326593399048, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.1219170093536377, 'test/num_examples': 10000, 'score': 31546.80517745018, 'total_duration': 34698.98634314537, 'accumulated_submission_time': 31546.80517745018, 'accumulated_eval_time': 3146.148453235626, 'accumulated_logging_time': 2.4373722076416016}
I0131 22:55:12.153416 139774417409792 logging_writer.py:48] [69004] accumulated_eval_time=3146.148453, accumulated_logging_time=2.437372, accumulated_submission_time=31546.805177, global_step=69004, preemption_count=0, score=31546.805177, test/accuracy=0.556600, test/loss=2.121917, test/num_examples=10000, total_duration=34698.986343, train/accuracy=0.740605, train/loss=1.241002, validation/accuracy=0.684720, validation/loss=1.489233, validation/num_examples=50000
I0131 22:55:51.550784 139774434195200 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0830148458480835, loss=3.375459671020508
I0131 22:56:37.545280 139774417409792 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.1502290964126587, loss=3.3500654697418213
I0131 22:57:23.834037 139774434195200 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2157206535339355, loss=3.4067275524139404
I0131 22:58:10.174176 139774417409792 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.0724483728408813, loss=3.3833038806915283
I0131 22:58:56.262863 139774434195200 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1939347982406616, loss=3.5060553550720215
I0131 22:59:42.607760 139774417409792 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.2231682538986206, loss=3.3764615058898926
I0131 23:00:28.813226 139774434195200 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.195875883102417, loss=3.4658522605895996
I0131 23:01:14.896335 139774417409792 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1704806089401245, loss=4.201598167419434
I0131 23:02:01.313694 139774434195200 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.1944595575332642, loss=3.4616293907165527
I0131 23:02:12.196305 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:02:24.112958 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:02:54.318939 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:02:55.921435 139936116377408 submission_runner.py:408] Time since start: 35162.78s, 	Step: 69925, 	{'train/accuracy': 0.7484570145606995, 'train/loss': 1.2337993383407593, 'validation/accuracy': 0.6854599714279175, 'validation/loss': 1.5051804780960083, 'validation/num_examples': 50000, 'test/accuracy': 0.5660000443458557, 'test/loss': 2.104238510131836, 'test/num_examples': 10000, 'score': 31966.791477441788, 'total_duration': 35162.78013944626, 'accumulated_submission_time': 31966.791477441788, 'accumulated_eval_time': 3189.873576402664, 'accumulated_logging_time': 2.472073554992676}
I0131 23:02:55.946466 139774417409792 logging_writer.py:48] [69925] accumulated_eval_time=3189.873576, accumulated_logging_time=2.472074, accumulated_submission_time=31966.791477, global_step=69925, preemption_count=0, score=31966.791477, test/accuracy=0.566000, test/loss=2.104239, test/num_examples=10000, total_duration=35162.780139, train/accuracy=0.748457, train/loss=1.233799, validation/accuracy=0.685460, validation/loss=1.505180, validation/num_examples=50000
I0131 23:03:26.118826 139774434195200 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1827889680862427, loss=3.3719542026519775
I0131 23:04:12.310340 139774417409792 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.13249671459198, loss=3.4692602157592773
I0131 23:04:58.103613 139774434195200 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.1685521602630615, loss=3.7175538539886475
I0131 23:05:44.190273 139774417409792 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.089785099029541, loss=4.221289157867432
I0131 23:06:30.390696 139774434195200 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1395738124847412, loss=3.5144684314727783
I0131 23:07:16.623762 139774417409792 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.2437819242477417, loss=3.412029504776001
I0131 23:08:02.746851 139774434195200 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.2309261560440063, loss=3.4590442180633545
I0131 23:08:48.863260 139774417409792 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1588491201400757, loss=3.4416003227233887
I0131 23:09:35.048284 139774434195200 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.1967076063156128, loss=3.3822503089904785
I0131 23:09:55.932681 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:10:07.983854 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:10:40.628291 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:10:42.234509 139936116377408 submission_runner.py:408] Time since start: 35629.09s, 	Step: 70847, 	{'train/accuracy': 0.7518359422683716, 'train/loss': 1.2066234350204468, 'validation/accuracy': 0.6805199980735779, 'validation/loss': 1.5176000595092773, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 2.1310625076293945, 'test/num_examples': 10000, 'score': 32386.718663215637, 'total_duration': 35629.093203783035, 'accumulated_submission_time': 32386.718663215637, 'accumulated_eval_time': 3236.175390481949, 'accumulated_logging_time': 2.508272171020508}
I0131 23:10:42.262697 139774417409792 logging_writer.py:48] [70847] accumulated_eval_time=3236.175390, accumulated_logging_time=2.508272, accumulated_submission_time=32386.718663, global_step=70847, preemption_count=0, score=32386.718663, test/accuracy=0.555300, test/loss=2.131063, test/num_examples=10000, total_duration=35629.093204, train/accuracy=0.751836, train/loss=1.206623, validation/accuracy=0.680520, validation/loss=1.517600, validation/num_examples=50000
I0131 23:11:03.692433 139774434195200 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.0559927225112915, loss=4.388434410095215
I0131 23:11:48.087171 139774417409792 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.2318670749664307, loss=5.158499240875244
I0131 23:12:34.286950 139774434195200 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2095764875411987, loss=3.4342033863067627
I0131 23:13:20.668670 139774417409792 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.0840691328048706, loss=3.519127368927002
I0131 23:14:06.907238 139774434195200 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.1068265438079834, loss=3.541134834289551
I0131 23:14:52.966761 139774417409792 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.1314674615859985, loss=4.181169509887695
I0131 23:15:39.179896 139774434195200 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1292331218719482, loss=3.5886430740356445
I0131 23:16:25.168570 139774417409792 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.0615861415863037, loss=4.203247547149658
I0131 23:17:11.196673 139774434195200 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1837091445922852, loss=3.316343069076538
I0131 23:17:42.671312 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:17:54.398622 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:18:19.216928 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:18:20.849598 139936116377408 submission_runner.py:408] Time since start: 36087.71s, 	Step: 71770, 	{'train/accuracy': 0.747363269329071, 'train/loss': 1.2343018054962158, 'validation/accuracy': 0.6899799704551697, 'validation/loss': 1.4829862117767334, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.08892560005188, 'test/num_examples': 10000, 'score': 32807.06958389282, 'total_duration': 36087.70830178261, 'accumulated_submission_time': 32807.06958389282, 'accumulated_eval_time': 3274.3536903858185, 'accumulated_logging_time': 2.5455551147460938}
I0131 23:18:20.878894 139774417409792 logging_writer.py:48] [71770] accumulated_eval_time=3274.353690, accumulated_logging_time=2.545555, accumulated_submission_time=32807.069584, global_step=71770, preemption_count=0, score=32807.069584, test/accuracy=0.564800, test/loss=2.088926, test/num_examples=10000, total_duration=36087.708302, train/accuracy=0.747363, train/loss=1.234302, validation/accuracy=0.689980, validation/loss=1.482986, validation/num_examples=50000
I0131 23:18:33.200370 139774434195200 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.025697112083435, loss=4.6063642501831055
I0131 23:19:16.183857 139774417409792 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1894104480743408, loss=3.390798807144165
I0131 23:20:02.498347 139774434195200 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.1718146800994873, loss=3.69598650932312
I0131 23:20:49.081277 139774417409792 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.1473711729049683, loss=4.992060661315918
I0131 23:21:35.575767 139774434195200 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.1922622919082642, loss=3.458329200744629
I0131 23:22:22.198642 139774417409792 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.139025330543518, loss=3.898679733276367
I0131 23:23:08.676210 139774434195200 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.005019187927246, loss=4.563969612121582
I0131 23:23:54.839725 139774417409792 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.185462236404419, loss=3.416492223739624
I0131 23:24:41.451980 139774434195200 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.0717734098434448, loss=4.046699047088623
I0131 23:25:21.086495 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:25:32.860829 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:26:04.574517 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:26:06.184031 139936116377408 submission_runner.py:408] Time since start: 36553.04s, 	Step: 72687, 	{'train/accuracy': 0.7515820264816284, 'train/loss': 1.2129734754562378, 'validation/accuracy': 0.6888999938964844, 'validation/loss': 1.4854317903518677, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0902061462402344, 'test/num_examples': 10000, 'score': 33227.219742536545, 'total_duration': 36553.0427236557, 'accumulated_submission_time': 33227.219742536545, 'accumulated_eval_time': 3319.4512207508087, 'accumulated_logging_time': 2.5845770835876465}
I0131 23:26:06.210625 139774417409792 logging_writer.py:48] [72687] accumulated_eval_time=3319.451221, accumulated_logging_time=2.584577, accumulated_submission_time=33227.219743, global_step=72687, preemption_count=0, score=33227.219743, test/accuracy=0.564000, test/loss=2.090206, test/num_examples=10000, total_duration=36553.042724, train/accuracy=0.751582, train/loss=1.212973, validation/accuracy=0.688900, validation/loss=1.485432, validation/num_examples=50000
I0131 23:26:11.770755 139774434195200 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.1248950958251953, loss=3.5319769382476807
I0131 23:26:53.514793 139774417409792 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.059151291847229, loss=4.78831672668457
I0131 23:27:39.586981 139774434195200 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.2192271947860718, loss=5.114199161529541
I0131 23:28:25.890294 139774417409792 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.2705579996109009, loss=4.377371311187744
I0131 23:29:12.097154 139774434195200 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.1494474411010742, loss=3.3744285106658936
I0131 23:29:58.033629 139774417409792 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.1236966848373413, loss=5.120617866516113
I0131 23:30:44.328218 139774434195200 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.2551214694976807, loss=3.536001443862915
I0131 23:31:30.466068 139774417409792 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.0881603956222534, loss=3.545714855194092
I0131 23:32:16.732041 139774434195200 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2824394702911377, loss=3.523122549057007
I0131 23:33:02.880445 139774417409792 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.1551002264022827, loss=3.428597927093506
I0131 23:33:06.207457 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:33:18.109198 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:33:47.272102 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:33:48.880785 139936116377408 submission_runner.py:408] Time since start: 37015.74s, 	Step: 73609, 	{'train/accuracy': 0.7533984184265137, 'train/loss': 1.191983938217163, 'validation/accuracy': 0.6872400045394897, 'validation/loss': 1.485839605331421, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 2.1044228076934814, 'test/num_examples': 10000, 'score': 33647.15845131874, 'total_duration': 37015.739466905594, 'accumulated_submission_time': 33647.15845131874, 'accumulated_eval_time': 3362.12451672554, 'accumulated_logging_time': 2.621159076690674}
I0131 23:33:48.910770 139774434195200 logging_writer.py:48] [73609] accumulated_eval_time=3362.124517, accumulated_logging_time=2.621159, accumulated_submission_time=33647.158451, global_step=73609, preemption_count=0, score=33647.158451, test/accuracy=0.567000, test/loss=2.104423, test/num_examples=10000, total_duration=37015.739467, train/accuracy=0.753398, train/loss=1.191984, validation/accuracy=0.687240, validation/loss=1.485840, validation/num_examples=50000
I0131 23:34:26.093411 139774417409792 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1890926361083984, loss=3.596442699432373
I0131 23:35:12.155993 139774434195200 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.2213072776794434, loss=3.4200897216796875
I0131 23:35:58.382260 139774417409792 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.100420594215393, loss=4.996434211730957
I0131 23:36:44.982191 139774434195200 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.1772451400756836, loss=3.3565969467163086
I0131 23:37:31.337529 139774417409792 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.2353624105453491, loss=5.146737098693848
I0131 23:38:17.651417 139774434195200 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.1664618253707886, loss=3.450249195098877
I0131 23:39:04.036245 139774417409792 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.188827633857727, loss=3.3972725868225098
I0131 23:39:50.165695 139774434195200 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.1252105236053467, loss=4.357357978820801
I0131 23:40:36.237454 139774417409792 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2339251041412354, loss=3.4077396392822266
I0131 23:40:49.254986 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:41:01.128418 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:41:29.836120 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:41:31.450809 139936116377408 submission_runner.py:408] Time since start: 37478.31s, 	Step: 74530, 	{'train/accuracy': 0.7507421970367432, 'train/loss': 1.195424199104309, 'validation/accuracy': 0.6930800080299377, 'validation/loss': 1.4571106433868408, 'validation/num_examples': 50000, 'test/accuracy': 0.5658000111579895, 'test/loss': 2.0698626041412354, 'test/num_examples': 10000, 'score': 34067.44424152374, 'total_duration': 37478.309512376785, 'accumulated_submission_time': 34067.44424152374, 'accumulated_eval_time': 3404.320331096649, 'accumulated_logging_time': 2.6617767810821533}
I0131 23:41:31.480301 139774434195200 logging_writer.py:48] [74530] accumulated_eval_time=3404.320331, accumulated_logging_time=2.661777, accumulated_submission_time=34067.444242, global_step=74530, preemption_count=0, score=34067.444242, test/accuracy=0.565800, test/loss=2.069863, test/num_examples=10000, total_duration=37478.309512, train/accuracy=0.750742, train/loss=1.195424, validation/accuracy=0.693080, validation/loss=1.457111, validation/num_examples=50000
I0131 23:41:59.708480 139774417409792 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.0642313957214355, loss=3.7575883865356445
I0131 23:42:44.508453 139774434195200 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.2767530679702759, loss=5.083766937255859
I0131 23:43:30.920120 139774417409792 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2093125581741333, loss=3.3638224601745605
I0131 23:44:17.415294 139774434195200 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1498101949691772, loss=4.506539344787598
I0131 23:45:03.234469 139774417409792 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.1480779647827148, loss=4.9323201179504395
I0131 23:45:49.710625 139774434195200 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.27125883102417, loss=3.466917037963867
I0131 23:46:35.762365 139774417409792 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1390430927276611, loss=3.3239643573760986
I0131 23:47:21.793807 139774434195200 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.267606496810913, loss=3.3901469707489014
I0131 23:48:08.115271 139774417409792 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.3776723146438599, loss=3.4366328716278076
I0131 23:48:31.775466 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:48:43.667169 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:49:12.090829 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:49:13.707065 139936116377408 submission_runner.py:408] Time since start: 37940.57s, 	Step: 75453, 	{'train/accuracy': 0.7525194883346558, 'train/loss': 1.1894110441207886, 'validation/accuracy': 0.6906599998474121, 'validation/loss': 1.454953908920288, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.06187105178833, 'test/num_examples': 10000, 'score': 34487.681963682175, 'total_duration': 37940.56576251984, 'accumulated_submission_time': 34487.681963682175, 'accumulated_eval_time': 3446.2519228458405, 'accumulated_logging_time': 2.700667142868042}
I0131 23:49:13.733240 139774434195200 logging_writer.py:48] [75453] accumulated_eval_time=3446.251923, accumulated_logging_time=2.700667, accumulated_submission_time=34487.681964, global_step=75453, preemption_count=0, score=34487.681964, test/accuracy=0.570600, test/loss=2.061871, test/num_examples=10000, total_duration=37940.565763, train/accuracy=0.752519, train/loss=1.189411, validation/accuracy=0.690660, validation/loss=1.454954, validation/num_examples=50000
I0131 23:49:32.788761 139774417409792 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.2071877717971802, loss=3.495652675628662
I0131 23:50:16.763079 139774434195200 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.1656649112701416, loss=3.8422694206237793
I0131 23:51:02.864823 139774417409792 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.0427566766738892, loss=4.24971342086792
I0131 23:51:49.249136 139774434195200 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.0737488269805908, loss=3.5678789615631104
I0131 23:52:35.361127 139774417409792 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1945487260818481, loss=3.334906816482544
I0131 23:53:21.801336 139774434195200 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.1102943420410156, loss=3.5586304664611816
I0131 23:54:08.115193 139774417409792 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1225824356079102, loss=3.267911672592163
I0131 23:54:54.022081 139774434195200 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.175472617149353, loss=3.397951602935791
I0131 23:55:40.308798 139774417409792 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.184476613998413, loss=3.4632561206817627
I0131 23:56:14.124321 139936116377408 spec.py:321] Evaluating on the training split.
I0131 23:56:25.984248 139936116377408 spec.py:333] Evaluating on the validation split.
I0131 23:56:53.344104 139936116377408 spec.py:349] Evaluating on the test split.
I0131 23:56:54.950873 139936116377408 submission_runner.py:408] Time since start: 38401.81s, 	Step: 76374, 	{'train/accuracy': 0.7610741853713989, 'train/loss': 1.1607592105865479, 'validation/accuracy': 0.6957799792289734, 'validation/loss': 1.4432342052459717, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 2.0531435012817383, 'test/num_examples': 10000, 'score': 34908.01672363281, 'total_duration': 38401.809537410736, 'accumulated_submission_time': 34908.01672363281, 'accumulated_eval_time': 3487.0784368515015, 'accumulated_logging_time': 2.736494779586792}
I0131 23:56:54.985220 139774434195200 logging_writer.py:48] [76374] accumulated_eval_time=3487.078437, accumulated_logging_time=2.736495, accumulated_submission_time=34908.016724, global_step=76374, preemption_count=0, score=34908.016724, test/accuracy=0.568800, test/loss=2.053144, test/num_examples=10000, total_duration=38401.809537, train/accuracy=0.761074, train/loss=1.160759, validation/accuracy=0.695780, validation/loss=1.443234, validation/num_examples=50000
I0131 23:57:05.711341 139774417409792 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.1301445960998535, loss=4.818319797515869
I0131 23:57:47.820301 139774434195200 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.057151436805725, loss=4.41793155670166
I0131 23:58:33.906758 139774417409792 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.3487948179244995, loss=5.093630790710449
I0131 23:59:20.467848 139774434195200 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.1427721977233887, loss=3.4472789764404297
I0201 00:00:06.456053 139774417409792 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.081308126449585, loss=4.356671333312988
I0201 00:00:52.268784 139774434195200 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1186565160751343, loss=4.8241095542907715
I0201 00:01:38.882238 139774417409792 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.2316861152648926, loss=3.3553009033203125
I0201 00:02:24.959446 139774434195200 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.2078876495361328, loss=3.322206974029541
I0201 00:03:10.973203 139774417409792 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.1188688278198242, loss=3.3597166538238525
I0201 00:03:54.984143 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:04:07.159875 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:04:35.433581 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:04:37.034569 139936116377408 submission_runner.py:408] Time since start: 38863.89s, 	Step: 77298, 	{'train/accuracy': 0.7732812166213989, 'train/loss': 1.1148550510406494, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.447901725769043, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 2.070373773574829, 'test/num_examples': 10000, 'score': 35327.95745301247, 'total_duration': 38863.893261671066, 'accumulated_submission_time': 35327.95745301247, 'accumulated_eval_time': 3529.1288471221924, 'accumulated_logging_time': 2.7815897464752197}
I0201 00:04:37.073105 139774434195200 logging_writer.py:48] [77298] accumulated_eval_time=3529.128847, accumulated_logging_time=2.781590, accumulated_submission_time=35327.957453, global_step=77298, preemption_count=0, score=35327.957453, test/accuracy=0.568600, test/loss=2.070374, test/num_examples=10000, total_duration=38863.893262, train/accuracy=0.773281, train/loss=1.114855, validation/accuracy=0.693720, validation/loss=1.447902, validation/num_examples=50000
I0201 00:04:38.270017 139774417409792 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.285260558128357, loss=3.3498902320861816
I0201 00:05:19.380779 139774434195200 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.1422933340072632, loss=3.5231685638427734
I0201 00:06:05.306487 139774417409792 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.244011402130127, loss=3.3231656551361084
I0201 00:06:51.779664 139774434195200 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.1600401401519775, loss=3.6276798248291016
I0201 00:07:38.221669 139774417409792 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.2131671905517578, loss=3.4271607398986816
I0201 00:08:24.345011 139774434195200 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.1430087089538574, loss=5.025144577026367
I0201 00:09:10.525824 139774417409792 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.1114753484725952, loss=3.6658554077148438
I0201 00:09:56.388725 139774434195200 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.2905277013778687, loss=3.344752788543701
I0201 00:10:42.645145 139774417409792 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.1947884559631348, loss=3.3682990074157715
I0201 00:11:28.765092 139774434195200 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.2215605974197388, loss=3.4051053524017334
I0201 00:11:37.300648 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:11:49.178348 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:12:16.934024 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:12:18.543220 139936116377408 submission_runner.py:408] Time since start: 39325.40s, 	Step: 78220, 	{'train/accuracy': 0.7600781321525574, 'train/loss': 1.1587432622909546, 'validation/accuracy': 0.6990599632263184, 'validation/loss': 1.4184653759002686, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 2.019524097442627, 'test/num_examples': 10000, 'score': 35748.12657356262, 'total_duration': 39325.401894807816, 'accumulated_submission_time': 35748.12657356262, 'accumulated_eval_time': 3570.3713760375977, 'accumulated_logging_time': 2.8311731815338135}
I0201 00:12:18.573354 139774417409792 logging_writer.py:48] [78220] accumulated_eval_time=3570.371376, accumulated_logging_time=2.831173, accumulated_submission_time=35748.126574, global_step=78220, preemption_count=0, score=35748.126574, test/accuracy=0.575800, test/loss=2.019524, test/num_examples=10000, total_duration=39325.401895, train/accuracy=0.760078, train/loss=1.158743, validation/accuracy=0.699060, validation/loss=1.418465, validation/num_examples=50000
I0201 00:12:50.782694 139774434195200 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1636292934417725, loss=4.444098472595215
I0201 00:13:36.674800 139774417409792 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.2712641954421997, loss=3.35127592086792
I0201 00:14:22.910436 139774434195200 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.2388441562652588, loss=3.4256129264831543
I0201 00:15:09.451186 139774417409792 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.1305880546569824, loss=3.7911620140075684
I0201 00:15:55.606303 139774434195200 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.2853515148162842, loss=3.3718760013580322
I0201 00:16:41.748837 139774417409792 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.2550283670425415, loss=4.895965576171875
I0201 00:17:28.042404 139774434195200 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.2690435647964478, loss=5.094207763671875
I0201 00:18:14.157789 139774417409792 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.2050927877426147, loss=4.324915409088135
I0201 00:19:00.232921 139774434195200 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.254172682762146, loss=3.379599094390869
I0201 00:19:18.989794 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:19:30.816352 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:19:59.319801 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:20:00.928153 139936116377408 submission_runner.py:408] Time since start: 39787.79s, 	Step: 79142, 	{'train/accuracy': 0.7625195384025574, 'train/loss': 1.148511528968811, 'validation/accuracy': 0.6945199966430664, 'validation/loss': 1.4347800016403198, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 2.055783987045288, 'test/num_examples': 10000, 'score': 36168.48544359207, 'total_duration': 39787.786851882935, 'accumulated_submission_time': 36168.48544359207, 'accumulated_eval_time': 3612.3097426891327, 'accumulated_logging_time': 2.8709871768951416}
I0201 00:20:00.956243 139774417409792 logging_writer.py:48] [79142] accumulated_eval_time=3612.309743, accumulated_logging_time=2.870987, accumulated_submission_time=36168.485444, global_step=79142, preemption_count=0, score=36168.485444, test/accuracy=0.573000, test/loss=2.055784, test/num_examples=10000, total_duration=39787.786852, train/accuracy=0.762520, train/loss=1.148512, validation/accuracy=0.694520, validation/loss=1.434780, validation/num_examples=50000
I0201 00:20:24.435340 139774434195200 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.1647233963012695, loss=5.02356481552124
I0201 00:21:08.621874 139774417409792 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.2575608491897583, loss=3.4111924171447754
I0201 00:21:54.794646 139774434195200 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.2163938283920288, loss=3.316068172454834
I0201 00:22:41.224968 139774417409792 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.1849164962768555, loss=3.31276798248291
I0201 00:23:27.485514 139774434195200 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.1594696044921875, loss=4.54813289642334
I0201 00:24:13.526087 139774417409792 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.3242995738983154, loss=4.999246120452881
I0201 00:24:59.549418 139774434195200 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.2439581155776978, loss=3.4339842796325684
I0201 00:25:45.736038 139774417409792 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.218435287475586, loss=3.360689640045166
I0201 00:26:31.898462 139774434195200 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.1590343713760376, loss=3.255629777908325
I0201 00:27:01.307465 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:27:13.382095 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:27:42.866449 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:27:44.474220 139936116377408 submission_runner.py:408] Time since start: 40251.33s, 	Step: 80065, 	{'train/accuracy': 0.7742968797683716, 'train/loss': 1.1092896461486816, 'validation/accuracy': 0.697380006313324, 'validation/loss': 1.4453190565109253, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 2.067552328109741, 'test/num_examples': 10000, 'score': 36588.779423713684, 'total_duration': 40251.33292555809, 'accumulated_submission_time': 36588.779423713684, 'accumulated_eval_time': 3655.476496696472, 'accumulated_logging_time': 2.9082815647125244}
I0201 00:27:44.505566 139774417409792 logging_writer.py:48] [80065] accumulated_eval_time=3655.476497, accumulated_logging_time=2.908282, accumulated_submission_time=36588.779424, global_step=80065, preemption_count=0, score=36588.779424, test/accuracy=0.570800, test/loss=2.067552, test/num_examples=10000, total_duration=40251.332926, train/accuracy=0.774297, train/loss=1.109290, validation/accuracy=0.697380, validation/loss=1.445319, validation/num_examples=50000
I0201 00:27:58.797178 139774434195200 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.279542088508606, loss=3.496957302093506
I0201 00:28:41.971085 139774417409792 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.1339589357376099, loss=3.9577689170837402
I0201 00:29:28.019260 139774434195200 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.3590402603149414, loss=3.42006516456604
I0201 00:30:14.303054 139774417409792 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.1296954154968262, loss=4.4105353355407715
I0201 00:31:00.350757 139774434195200 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.3792321681976318, loss=4.8792724609375
I0201 00:31:46.670297 139774417409792 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.3123902082443237, loss=3.424039840698242
I0201 00:32:32.751004 139774434195200 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.2834726572036743, loss=3.4260005950927734
I0201 00:33:18.888892 139774417409792 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.2401574850082397, loss=4.261188983917236
I0201 00:34:05.105076 139774434195200 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.1728943586349487, loss=3.303471803665161
I0201 00:34:44.781907 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:34:56.823356 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:35:24.759796 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:35:26.366764 139936116377408 submission_runner.py:408] Time since start: 40713.23s, 	Step: 80988, 	{'train/accuracy': 0.7635741829872131, 'train/loss': 1.1445281505584717, 'validation/accuracy': 0.7017799615859985, 'validation/loss': 1.4090139865875244, 'validation/num_examples': 50000, 'test/accuracy': 0.5790000557899475, 'test/loss': 2.017435073852539, 'test/num_examples': 10000, 'score': 37008.99789881706, 'total_duration': 40713.225462675095, 'accumulated_submission_time': 37008.99789881706, 'accumulated_eval_time': 3697.0613508224487, 'accumulated_logging_time': 2.9498679637908936}
I0201 00:35:26.399851 139774417409792 logging_writer.py:48] [80988] accumulated_eval_time=3697.061351, accumulated_logging_time=2.949868, accumulated_submission_time=37008.997899, global_step=80988, preemption_count=0, score=37008.997899, test/accuracy=0.579000, test/loss=2.017435, test/num_examples=10000, total_duration=40713.225463, train/accuracy=0.763574, train/loss=1.144528, validation/accuracy=0.701780, validation/loss=1.409014, validation/num_examples=50000
I0201 00:35:31.570457 139774434195200 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.1706538200378418, loss=3.63368558883667
I0201 00:36:13.006381 139774417409792 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.2499436140060425, loss=3.2182371616363525
I0201 00:36:59.005425 139774434195200 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.260956048965454, loss=4.886223793029785
I0201 00:37:45.786792 139774417409792 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.2994742393493652, loss=3.437094211578369
I0201 00:38:32.445677 139774434195200 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.27676522731781, loss=3.338989019393921
I0201 00:39:18.320283 139774417409792 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.2071170806884766, loss=3.5096089839935303
I0201 00:40:04.719255 139774434195200 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.2225167751312256, loss=4.682051658630371
I0201 00:40:50.802402 139774417409792 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.3076503276824951, loss=5.038405895233154
I0201 00:41:36.980555 139774434195200 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.225475549697876, loss=5.004817962646484
I0201 00:42:23.206496 139774417409792 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.2076523303985596, loss=3.3764097690582275
I0201 00:42:26.628851 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:42:39.340482 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:43:08.805636 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:43:10.409958 139936116377408 submission_runner.py:408] Time since start: 41177.27s, 	Step: 81909, 	{'train/accuracy': 0.7632030844688416, 'train/loss': 1.1502982378005981, 'validation/accuracy': 0.6981399655342102, 'validation/loss': 1.4378381967544556, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 2.034175157546997, 'test/num_examples': 10000, 'score': 37429.169956445694, 'total_duration': 41177.268661022186, 'accumulated_submission_time': 37429.169956445694, 'accumulated_eval_time': 3740.842449903488, 'accumulated_logging_time': 2.9926469326019287}
I0201 00:43:10.436835 139774434195200 logging_writer.py:48] [81909] accumulated_eval_time=3740.842450, accumulated_logging_time=2.992647, accumulated_submission_time=37429.169956, global_step=81909, preemption_count=0, score=37429.169956, test/accuracy=0.577100, test/loss=2.034175, test/num_examples=10000, total_duration=41177.268661, train/accuracy=0.763203, train/loss=1.150298, validation/accuracy=0.698140, validation/loss=1.437838, validation/num_examples=50000
I0201 00:43:47.532203 139774417409792 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.211294174194336, loss=4.637289047241211
I0201 00:44:33.500156 139774434195200 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.2173963785171509, loss=3.391824722290039
I0201 00:45:19.862307 139774417409792 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.3358694314956665, loss=5.02873420715332
I0201 00:46:06.279189 139774434195200 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.1563518047332764, loss=3.4017269611358643
I0201 00:46:52.125395 139774417409792 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.1980363130569458, loss=3.2955517768859863
I0201 00:47:38.515382 139774434195200 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.1592539548873901, loss=3.979334831237793
I0201 00:48:24.998497 139774417409792 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.2113847732543945, loss=4.74293327331543
I0201 00:49:11.025475 139774434195200 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.2610598802566528, loss=3.339632272720337
I0201 00:49:57.007967 139774417409792 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.3239840269088745, loss=3.408858299255371
I0201 00:50:10.719205 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:50:22.738824 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:50:55.347581 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:50:56.957058 139936116377408 submission_runner.py:408] Time since start: 41643.82s, 	Step: 82831, 	{'train/accuracy': 0.7751367092132568, 'train/loss': 1.1169837713241577, 'validation/accuracy': 0.7000600099563599, 'validation/loss': 1.4290475845336914, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 2.0344953536987305, 'test/num_examples': 10000, 'score': 37849.39163827896, 'total_duration': 41643.815761089325, 'accumulated_submission_time': 37849.39163827896, 'accumulated_eval_time': 3787.0803208351135, 'accumulated_logging_time': 3.0316107273101807}
I0201 00:50:56.984687 139774434195200 logging_writer.py:48] [82831] accumulated_eval_time=3787.080321, accumulated_logging_time=3.031611, accumulated_submission_time=37849.391638, global_step=82831, preemption_count=0, score=37849.391638, test/accuracy=0.574600, test/loss=2.034495, test/num_examples=10000, total_duration=41643.815761, train/accuracy=0.775137, train/loss=1.116984, validation/accuracy=0.700060, validation/loss=1.429048, validation/num_examples=50000
I0201 00:51:24.793457 139774417409792 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2330446243286133, loss=3.317610025405884
I0201 00:52:10.601327 139774434195200 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.1465437412261963, loss=4.187417984008789
I0201 00:52:56.447944 139774417409792 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.2176705598831177, loss=3.310680389404297
I0201 00:53:42.758667 139774434195200 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.3407745361328125, loss=3.4729342460632324
I0201 00:54:28.847771 139774417409792 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.252898097038269, loss=4.821027755737305
I0201 00:55:14.978627 139774434195200 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.3381609916687012, loss=5.0041608810424805
I0201 00:56:00.865756 139774417409792 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2149426937103271, loss=3.3081090450286865
I0201 00:56:47.005449 139774434195200 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.1704334020614624, loss=3.943091869354248
I0201 00:57:33.190467 139774417409792 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.2389984130859375, loss=4.648699760437012
I0201 00:57:57.086249 139936116377408 spec.py:321] Evaluating on the training split.
I0201 00:58:09.176898 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 00:58:40.125822 139936116377408 spec.py:349] Evaluating on the test split.
I0201 00:58:41.728238 139936116377408 submission_runner.py:408] Time since start: 42108.59s, 	Step: 83754, 	{'train/accuracy': 0.7632226347923279, 'train/loss': 1.1648662090301514, 'validation/accuracy': 0.6999799609184265, 'validation/loss': 1.4325613975524902, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 2.039259433746338, 'test/num_examples': 10000, 'score': 38269.435145139694, 'total_duration': 42108.586940288544, 'accumulated_submission_time': 38269.435145139694, 'accumulated_eval_time': 3831.722299337387, 'accumulated_logging_time': 3.0690486431121826}
I0201 00:58:41.755224 139774434195200 logging_writer.py:48] [83754] accumulated_eval_time=3831.722299, accumulated_logging_time=3.069049, accumulated_submission_time=38269.435145, global_step=83754, preemption_count=0, score=38269.435145, test/accuracy=0.576900, test/loss=2.039259, test/num_examples=10000, total_duration=42108.586940, train/accuracy=0.763223, train/loss=1.164866, validation/accuracy=0.699980, validation/loss=1.432561, validation/num_examples=50000
I0201 00:59:00.410758 139774417409792 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.1510509252548218, loss=3.5227408409118652
I0201 00:59:44.221856 139774434195200 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.375441074371338, loss=3.353579044342041
I0201 01:00:30.174003 139774417409792 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.1490682363510132, loss=3.584380626678467
I0201 01:01:16.767501 139774434195200 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.310897946357727, loss=3.3254780769348145
I0201 01:02:03.308962 139774417409792 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.2466927766799927, loss=3.373340606689453
I0201 01:02:49.423694 139774434195200 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.2210674285888672, loss=4.17518424987793
I0201 01:03:35.588102 139774417409792 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.372092366218567, loss=3.3258860111236572
I0201 01:04:21.741281 139774434195200 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.2615938186645508, loss=3.4427788257598877
I0201 01:05:07.771719 139774417409792 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.1970608234405518, loss=3.5437846183776855
I0201 01:05:41.774727 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:05:53.681197 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 01:06:19.185956 139936116377408 spec.py:349] Evaluating on the test split.
I0201 01:06:20.797845 139936116377408 submission_runner.py:408] Time since start: 42567.66s, 	Step: 84676, 	{'train/accuracy': 0.7675585746765137, 'train/loss': 1.1792206764221191, 'validation/accuracy': 0.7015199661254883, 'validation/loss': 1.4529019594192505, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 2.06295108795166, 'test/num_examples': 10000, 'score': 38689.39722657204, 'total_duration': 42567.656548023224, 'accumulated_submission_time': 38689.39722657204, 'accumulated_eval_time': 3870.7454376220703, 'accumulated_logging_time': 3.1060070991516113}
I0201 01:06:20.827717 139774434195200 logging_writer.py:48] [84676] accumulated_eval_time=3870.745438, accumulated_logging_time=3.106007, accumulated_submission_time=38689.397227, global_step=84676, preemption_count=0, score=38689.397227, test/accuracy=0.578800, test/loss=2.062951, test/num_examples=10000, total_duration=42567.656548, train/accuracy=0.767559, train/loss=1.179221, validation/accuracy=0.701520, validation/loss=1.452902, validation/num_examples=50000
I0201 01:06:30.763630 139774417409792 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.2928979396820068, loss=3.340867519378662
I0201 01:07:13.054584 139774434195200 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.2335504293441772, loss=3.313292980194092
I0201 01:07:58.864504 139774417409792 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.3987528085708618, loss=5.005512237548828
I0201 01:08:45.139735 139774434195200 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.2270537614822388, loss=3.609179735183716
I0201 01:09:31.631759 139774417409792 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.272360920906067, loss=3.376126766204834
I0201 01:10:17.791931 139774434195200 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.293128490447998, loss=3.305539846420288
I0201 01:11:04.114243 139774417409792 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.4305922985076904, loss=4.991829872131348
I0201 01:11:50.333859 139774434195200 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.2313812971115112, loss=3.3033478260040283
I0201 01:12:36.502166 139774417409792 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.2812179327011108, loss=3.4521985054016113
I0201 01:13:21.129868 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:13:33.051661 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 01:14:04.065618 139936116377408 spec.py:349] Evaluating on the test split.
I0201 01:14:05.670219 139936116377408 submission_runner.py:408] Time since start: 43032.53s, 	Step: 85598, 	{'train/accuracy': 0.77259761095047, 'train/loss': 1.1098675727844238, 'validation/accuracy': 0.7027599811553955, 'validation/loss': 1.4117202758789062, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 2.0271904468536377, 'test/num_examples': 10000, 'score': 39109.64110136032, 'total_duration': 43032.52892065048, 'accumulated_submission_time': 39109.64110136032, 'accumulated_eval_time': 3915.285789489746, 'accumulated_logging_time': 3.146677255630493}
I0201 01:14:05.702046 139774434195200 logging_writer.py:48] [85598] accumulated_eval_time=3915.285789, accumulated_logging_time=3.146677, accumulated_submission_time=39109.641101, global_step=85598, preemption_count=0, score=39109.641101, test/accuracy=0.581100, test/loss=2.027190, test/num_examples=10000, total_duration=43032.528921, train/accuracy=0.772598, train/loss=1.109868, validation/accuracy=0.702760, validation/loss=1.411720, validation/num_examples=50000
I0201 01:14:06.901204 139774417409792 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.3111486434936523, loss=3.3875298500061035
I0201 01:14:48.011448 139774434195200 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.3249608278274536, loss=3.35768723487854
I0201 01:15:33.923934 139774417409792 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.1128891706466675, loss=3.5570716857910156
I0201 01:16:20.170802 139774434195200 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.2293263673782349, loss=3.3947739601135254
I0201 01:17:06.286960 139774417409792 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.361260175704956, loss=3.4034676551818848
I0201 01:17:52.317106 139774434195200 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.0984525680541992, loss=3.5343737602233887
I0201 01:18:38.514156 139774417409792 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.326291799545288, loss=4.928138732910156
I0201 01:19:24.751518 139774434195200 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.3051203489303589, loss=4.641877174377441
I0201 01:20:11.630715 139774417409792 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.5705872774124146, loss=4.889952659606934
I0201 01:20:57.783435 139774434195200 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.236899733543396, loss=3.2090094089508057
I0201 01:21:05.961066 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:21:17.794158 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 01:21:51.186533 139936116377408 spec.py:349] Evaluating on the test split.
I0201 01:21:52.793307 139936116377408 submission_runner.py:408] Time since start: 43499.65s, 	Step: 86519, 	{'train/accuracy': 0.7715820074081421, 'train/loss': 1.1164155006408691, 'validation/accuracy': 0.7065399885177612, 'validation/loss': 1.3979099988937378, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 2.0163145065307617, 'test/num_examples': 10000, 'score': 39529.84063959122, 'total_duration': 43499.65197634697, 'accumulated_submission_time': 39529.84063959122, 'accumulated_eval_time': 3962.1179814338684, 'accumulated_logging_time': 3.1898202896118164}
I0201 01:21:52.824336 139774417409792 logging_writer.py:48] [86519] accumulated_eval_time=3962.117981, accumulated_logging_time=3.189820, accumulated_submission_time=39529.840640, global_step=86519, preemption_count=0, score=39529.840640, test/accuracy=0.583600, test/loss=2.016315, test/num_examples=10000, total_duration=43499.651976, train/accuracy=0.771582, train/loss=1.116416, validation/accuracy=0.706540, validation/loss=1.397910, validation/num_examples=50000
I0201 01:22:25.508903 139774434195200 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.2504507303237915, loss=4.081596374511719
I0201 01:23:11.560959 139774417409792 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.2277748584747314, loss=3.318621873855591
I0201 01:23:57.768521 139774434195200 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.2189949750900269, loss=3.710888624191284
I0201 01:24:44.247444 139774417409792 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.1606557369232178, loss=3.940988302230835
I0201 01:25:30.277345 139774434195200 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.1555875539779663, loss=3.4401116371154785
I0201 01:26:16.429321 139774417409792 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.2294909954071045, loss=3.322603702545166
I0201 01:27:02.435307 139774434195200 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.2819136381149292, loss=3.3162331581115723
I0201 01:27:48.682144 139774417409792 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.1721422672271729, loss=3.4942121505737305
I0201 01:28:34.873177 139774434195200 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3284653425216675, loss=3.3113250732421875
I0201 01:28:52.956587 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:29:05.128282 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 01:29:35.567896 139936116377408 spec.py:349] Evaluating on the test split.
I0201 01:29:37.169218 139936116377408 submission_runner.py:408] Time since start: 43964.03s, 	Step: 87441, 	{'train/accuracy': 0.769726574420929, 'train/loss': 1.133750319480896, 'validation/accuracy': 0.7074599862098694, 'validation/loss': 1.397431492805481, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 2.0062777996063232, 'test/num_examples': 10000, 'score': 39949.914578437805, 'total_duration': 43964.02792263031, 'accumulated_submission_time': 39949.914578437805, 'accumulated_eval_time': 4006.3306062221527, 'accumulated_logging_time': 3.2322468757629395}
I0201 01:29:37.199658 139774417409792 logging_writer.py:48] [87441] accumulated_eval_time=4006.330606, accumulated_logging_time=3.232247, accumulated_submission_time=39949.914578, global_step=87441, preemption_count=0, score=39949.914578, test/accuracy=0.582100, test/loss=2.006278, test/num_examples=10000, total_duration=43964.027923, train/accuracy=0.769727, train/loss=1.133750, validation/accuracy=0.707460, validation/loss=1.397431, validation/num_examples=50000
I0201 01:30:01.024332 139774434195200 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.327546238899231, loss=3.3424232006073
I0201 01:30:45.853465 139774417409792 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.301240086555481, loss=4.229333400726318
I0201 01:31:32.150819 139774434195200 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.2308307886123657, loss=4.213422775268555
I0201 01:32:18.707838 139774417409792 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.146466851234436, loss=4.414241313934326
I0201 01:33:04.569069 139774434195200 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.3882311582565308, loss=3.324589252471924
I0201 01:33:50.663217 139774417409792 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.2074593305587769, loss=4.4108500480651855
I0201 01:34:36.699195 139774434195200 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.3053100109100342, loss=3.306694984436035
I0201 01:35:22.869046 139774417409792 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.2531027793884277, loss=4.744572639465332
I0201 01:36:09.018073 139774434195200 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.2796188592910767, loss=3.24959659576416
I0201 01:36:37.194761 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:36:49.149217 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 01:37:16.038392 139936116377408 spec.py:349] Evaluating on the test split.
I0201 01:37:17.644215 139936116377408 submission_runner.py:408] Time since start: 44424.50s, 	Step: 88363, 	{'train/accuracy': 0.7766796946525574, 'train/loss': 1.0799094438552856, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.3791130781173706, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.9934600591659546, 'test/num_examples': 10000, 'score': 40369.85162806511, 'total_duration': 44424.50291514397, 'accumulated_submission_time': 40369.85162806511, 'accumulated_eval_time': 4046.7800641059875, 'accumulated_logging_time': 3.2729523181915283}
I0201 01:37:17.672519 139774417409792 logging_writer.py:48] [88363] accumulated_eval_time=4046.780064, accumulated_logging_time=3.272952, accumulated_submission_time=40369.851628, global_step=88363, preemption_count=0, score=40369.851628, test/accuracy=0.584500, test/loss=1.993460, test/num_examples=10000, total_duration=44424.502915, train/accuracy=0.776680, train/loss=1.079909, validation/accuracy=0.706420, validation/loss=1.379113, validation/num_examples=50000
I0201 01:37:32.768652 139774434195200 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.403959035873413, loss=4.8447771072387695
I0201 01:38:15.850168 139774417409792 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.1831833124160767, loss=4.086704254150391
I0201 01:39:02.416674 139774434195200 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.2921103239059448, loss=3.3084797859191895
I0201 01:39:49.004031 139774417409792 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.2785580158233643, loss=3.3413426876068115
I0201 01:40:35.611353 139774434195200 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.3508665561676025, loss=3.4464972019195557
I0201 01:41:22.415856 139774417409792 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.2170581817626953, loss=3.5822486877441406
I0201 01:42:09.221455 139774434195200 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.2283868789672852, loss=3.259397268295288
I0201 01:42:55.531834 139774417409792 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.3364553451538086, loss=3.3757669925689697
I0201 01:43:41.912304 139774434195200 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.2009872198104858, loss=3.5308971405029297
I0201 01:44:18.015381 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:44:30.112861 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 01:45:00.370886 139936116377408 spec.py:349] Evaluating on the test split.
I0201 01:45:01.981438 139936116377408 submission_runner.py:408] Time since start: 44888.84s, 	Step: 89279, 	{'train/accuracy': 0.7904296517372131, 'train/loss': 1.0638716220855713, 'validation/accuracy': 0.7069599628448486, 'validation/loss': 1.4218652248382568, 'validation/num_examples': 50000, 'test/accuracy': 0.5841000080108643, 'test/loss': 2.0293197631835938, 'test/num_examples': 10000, 'score': 40790.13759255409, 'total_duration': 44888.84012579918, 'accumulated_submission_time': 40790.13759255409, 'accumulated_eval_time': 4090.746128797531, 'accumulated_logging_time': 3.310702085494995}
I0201 01:45:02.022511 139774417409792 logging_writer.py:48] [89279] accumulated_eval_time=4090.746129, accumulated_logging_time=3.310702, accumulated_submission_time=40790.137593, global_step=89279, preemption_count=0, score=40790.137593, test/accuracy=0.584100, test/loss=2.029320, test/num_examples=10000, total_duration=44888.840126, train/accuracy=0.790430, train/loss=1.063872, validation/accuracy=0.706960, validation/loss=1.421865, validation/num_examples=50000
I0201 01:45:10.765230 139774434195200 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.2056612968444824, loss=3.5870015621185303
I0201 01:45:52.868355 139774417409792 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.410847544670105, loss=4.603187084197998
I0201 01:46:38.868341 139774434195200 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.152721643447876, loss=4.157957553863525
I0201 01:47:25.504221 139774417409792 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.2877715826034546, loss=3.357337474822998
I0201 01:48:11.618713 139774434195200 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.3413177728652954, loss=3.5933313369750977
I0201 01:48:57.794872 139774417409792 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.4135104417800903, loss=4.860230922698975
I0201 01:49:44.366847 139774434195200 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.2567211389541626, loss=3.2608907222747803
I0201 01:50:30.487428 139774417409792 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.3298747539520264, loss=3.2934961318969727
I0201 01:51:17.036093 139774434195200 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.4290364980697632, loss=4.961712837219238
I0201 01:52:02.432658 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:52:14.472707 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 01:52:42.432440 139936116377408 spec.py:349] Evaluating on the test split.
I0201 01:52:44.042383 139936116377408 submission_runner.py:408] Time since start: 45350.90s, 	Step: 90199, 	{'train/accuracy': 0.7699609398841858, 'train/loss': 1.117898941040039, 'validation/accuracy': 0.7085599899291992, 'validation/loss': 1.3838273286819458, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.990799069404602, 'test/num_examples': 10000, 'score': 41210.48593831062, 'total_duration': 45350.90107703209, 'accumulated_submission_time': 41210.48593831062, 'accumulated_eval_time': 4132.355852603912, 'accumulated_logging_time': 3.3663439750671387}
I0201 01:52:44.075630 139774417409792 logging_writer.py:48] [90199] accumulated_eval_time=4132.355853, accumulated_logging_time=3.366344, accumulated_submission_time=41210.485938, global_step=90199, preemption_count=0, score=41210.485938, test/accuracy=0.583700, test/loss=1.990799, test/num_examples=10000, total_duration=45350.901077, train/accuracy=0.769961, train/loss=1.117899, validation/accuracy=0.708560, validation/loss=1.383827, validation/num_examples=50000
I0201 01:52:44.875356 139774434195200 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2580739259719849, loss=3.8776605129241943
I0201 01:53:25.952943 139774417409792 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.326464295387268, loss=3.793610095977783
I0201 01:54:12.336238 139774434195200 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.2577592134475708, loss=3.2977256774902344
I0201 01:54:58.275905 139774417409792 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.2664841413497925, loss=3.274405002593994
I0201 01:55:44.597483 139774434195200 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.1951998472213745, loss=4.290074348449707
I0201 01:56:30.691745 139774417409792 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.316157579421997, loss=3.2502357959747314
I0201 01:57:17.201576 139774434195200 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.3359708786010742, loss=3.6760494709014893
I0201 01:58:03.252902 139774417409792 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.2344796657562256, loss=3.4477899074554443
I0201 01:58:49.304130 139774434195200 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.3043550252914429, loss=3.9153733253479004
I0201 01:59:35.743707 139774417409792 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.256920576095581, loss=3.3071489334106445
I0201 01:59:44.187962 139936116377408 spec.py:321] Evaluating on the training split.
I0201 01:59:56.223806 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:00:19.871206 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:00:21.481374 139936116377408 submission_runner.py:408] Time since start: 45808.34s, 	Step: 91120, 	{'train/accuracy': 0.7765820026397705, 'train/loss': 1.0714341402053833, 'validation/accuracy': 0.7109000086784363, 'validation/loss': 1.358513593673706, 'validation/num_examples': 50000, 'test/accuracy': 0.5908000469207764, 'test/loss': 1.9569255113601685, 'test/num_examples': 10000, 'score': 41630.54138350487, 'total_duration': 45808.34008026123, 'accumulated_submission_time': 41630.54138350487, 'accumulated_eval_time': 4169.649255514145, 'accumulated_logging_time': 3.409395456314087}
I0201 02:00:21.511780 139774434195200 logging_writer.py:48] [91120] accumulated_eval_time=4169.649256, accumulated_logging_time=3.409395, accumulated_submission_time=41630.541384, global_step=91120, preemption_count=0, score=41630.541384, test/accuracy=0.590800, test/loss=1.956926, test/num_examples=10000, total_duration=45808.340080, train/accuracy=0.776582, train/loss=1.071434, validation/accuracy=0.710900, validation/loss=1.358514, validation/num_examples=50000
I0201 02:00:53.872144 139774417409792 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.3325117826461792, loss=3.216521739959717
I0201 02:01:39.615195 139774434195200 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.2410659790039062, loss=3.589427947998047
I0201 02:02:26.276908 139774417409792 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.3318052291870117, loss=3.2407619953155518
I0201 02:03:12.836156 139774434195200 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.282881736755371, loss=4.808169364929199
I0201 02:03:58.859142 139774417409792 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.3275405168533325, loss=3.2951762676239014
I0201 02:04:45.122656 139774434195200 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.2926380634307861, loss=3.266371965408325
I0201 02:05:31.339967 139774417409792 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.260414719581604, loss=4.707118988037109
I0201 02:06:17.504037 139774434195200 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.1648516654968262, loss=3.5024237632751465
I0201 02:07:03.397485 139774417409792 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.3322982788085938, loss=3.3821682929992676
I0201 02:07:21.487604 139936116377408 spec.py:321] Evaluating on the training split.
I0201 02:07:33.412869 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:08:04.362967 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:08:05.969615 139936116377408 submission_runner.py:408] Time since start: 46272.83s, 	Step: 92041, 	{'train/accuracy': 0.7859570384025574, 'train/loss': 1.049735188484192, 'validation/accuracy': 0.7099999785423279, 'validation/loss': 1.3754807710647583, 'validation/num_examples': 50000, 'test/accuracy': 0.5905000567436218, 'test/loss': 1.9786850214004517, 'test/num_examples': 10000, 'score': 42050.45993804932, 'total_duration': 46272.828310251236, 'accumulated_submission_time': 42050.45993804932, 'accumulated_eval_time': 4214.131242513657, 'accumulated_logging_time': 3.4492197036743164}
I0201 02:08:05.999593 139774434195200 logging_writer.py:48] [92041] accumulated_eval_time=4214.131243, accumulated_logging_time=3.449220, accumulated_submission_time=42050.459938, global_step=92041, preemption_count=0, score=42050.459938, test/accuracy=0.590500, test/loss=1.978685, test/num_examples=10000, total_duration=46272.828310, train/accuracy=0.785957, train/loss=1.049735, validation/accuracy=0.710000, validation/loss=1.375481, validation/num_examples=50000
I0201 02:08:29.821172 139774417409792 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.3626288175582886, loss=4.818142890930176
I0201 02:09:14.254750 139774434195200 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.2514039278030396, loss=3.308104991912842
I0201 02:10:00.243326 139774417409792 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.370059847831726, loss=4.8749308586120605
I0201 02:10:46.691342 139774434195200 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.2934682369232178, loss=4.443996906280518
I0201 02:11:32.792745 139774417409792 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.2268909215927124, loss=4.205285549163818
I0201 02:12:19.631214 139774434195200 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.3335076570510864, loss=3.285447597503662
I0201 02:13:05.898904 139774417409792 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.3186581134796143, loss=4.6625447273254395
I0201 02:13:52.078668 139774434195200 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.3214597702026367, loss=3.3204355239868164
I0201 02:14:38.453427 139774417409792 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.2344841957092285, loss=4.095030784606934
I0201 02:15:06.403822 139936116377408 spec.py:321] Evaluating on the training split.
I0201 02:15:18.308462 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:15:48.179816 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:15:49.785599 139936116377408 submission_runner.py:408] Time since start: 46736.64s, 	Step: 92962, 	{'train/accuracy': 0.7772070169448853, 'train/loss': 1.0873217582702637, 'validation/accuracy': 0.7098199725151062, 'validation/loss': 1.380346417427063, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 1.974666714668274, 'test/num_examples': 10000, 'score': 42470.80727481842, 'total_duration': 46736.644303798676, 'accumulated_submission_time': 42470.80727481842, 'accumulated_eval_time': 4257.513018131256, 'accumulated_logging_time': 3.4884932041168213}
I0201 02:15:49.817781 139774434195200 logging_writer.py:48] [92962] accumulated_eval_time=4257.513018, accumulated_logging_time=3.488493, accumulated_submission_time=42470.807275, global_step=92962, preemption_count=0, score=42470.807275, test/accuracy=0.590100, test/loss=1.974667, test/num_examples=10000, total_duration=46736.644304, train/accuracy=0.777207, train/loss=1.087322, validation/accuracy=0.709820, validation/loss=1.380346, validation/num_examples=50000
I0201 02:16:05.302775 139774417409792 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.244478464126587, loss=3.894681930541992
I0201 02:16:48.418263 139774434195200 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.4302647113800049, loss=3.279571056365967
I0201 02:17:34.584500 139774417409792 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.2853972911834717, loss=3.8743786811828613
I0201 02:18:20.989301 139774434195200 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.2466334104537964, loss=3.309208631515503
I0201 02:19:07.105049 139774417409792 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.3352285623550415, loss=3.800912380218506
I0201 02:19:53.049494 139774434195200 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.2629083395004272, loss=4.035665512084961
I0201 02:20:39.644590 139774417409792 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.2326719760894775, loss=3.4788284301757812
I0201 02:21:25.812456 139774434195200 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.191190242767334, loss=3.376896858215332
I0201 02:22:12.245324 139774417409792 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.2103841304779053, loss=3.294121026992798
I0201 02:22:49.955501 139936116377408 spec.py:321] Evaluating on the training split.
I0201 02:23:01.893280 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:23:31.613668 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:23:33.222429 139936116377408 submission_runner.py:408] Time since start: 47200.08s, 	Step: 93883, 	{'train/accuracy': 0.781445324420929, 'train/loss': 1.0885802507400513, 'validation/accuracy': 0.7143999934196472, 'validation/loss': 1.3779401779174805, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 1.9860336780548096, 'test/num_examples': 10000, 'score': 42890.88827753067, 'total_duration': 47200.08113312721, 'accumulated_submission_time': 42890.88827753067, 'accumulated_eval_time': 4300.7799434661865, 'accumulated_logging_time': 3.5301735401153564}
I0201 02:23:33.254144 139774434195200 logging_writer.py:48] [93883] accumulated_eval_time=4300.779943, accumulated_logging_time=3.530174, accumulated_submission_time=42890.888278, global_step=93883, preemption_count=0, score=42890.888278, test/accuracy=0.590100, test/loss=1.986034, test/num_examples=10000, total_duration=47200.081133, train/accuracy=0.781445, train/loss=1.088580, validation/accuracy=0.714400, validation/loss=1.377940, validation/num_examples=50000
I0201 02:23:40.400603 139774417409792 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.2661019563674927, loss=4.351166248321533
I0201 02:24:22.483521 139774434195200 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.2568795680999756, loss=3.2332844734191895
I0201 02:25:08.450033 139774417409792 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.555391788482666, loss=4.852560043334961
I0201 02:25:54.810104 139774434195200 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.3145610094070435, loss=4.708642959594727
I0201 02:26:41.089040 139774417409792 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.3481812477111816, loss=4.730819225311279
I0201 02:27:27.015799 139774434195200 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.4029474258422852, loss=3.421955108642578
I0201 02:28:13.403950 139774417409792 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.2693439722061157, loss=3.6041483879089355
I0201 02:28:59.307711 139774434195200 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.425400972366333, loss=3.294002056121826
I0201 02:29:45.418520 139774417409792 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.3765010833740234, loss=3.5264406204223633
I0201 02:30:31.554140 139774434195200 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.28580904006958, loss=3.4197440147399902
I0201 02:30:33.515211 139936116377408 spec.py:321] Evaluating on the training split.
I0201 02:30:45.337656 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:31:15.001474 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:31:16.620417 139936116377408 submission_runner.py:408] Time since start: 47663.48s, 	Step: 94806, 	{'train/accuracy': 0.7893944978713989, 'train/loss': 1.0502266883850098, 'validation/accuracy': 0.7142399549484253, 'validation/loss': 1.3713054656982422, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.966513991355896, 'test/num_examples': 10000, 'score': 43311.09178900719, 'total_duration': 47663.47908735275, 'accumulated_submission_time': 43311.09178900719, 'accumulated_eval_time': 4343.885124206543, 'accumulated_logging_time': 3.5719377994537354}
I0201 02:31:16.657575 139774417409792 logging_writer.py:48] [94806] accumulated_eval_time=4343.885124, accumulated_logging_time=3.571938, accumulated_submission_time=43311.091789, global_step=94806, preemption_count=0, score=43311.091789, test/accuracy=0.589300, test/loss=1.966514, test/num_examples=10000, total_duration=47663.479087, train/accuracy=0.789394, train/loss=1.050227, validation/accuracy=0.714240, validation/loss=1.371305, validation/num_examples=50000
I0201 02:31:55.527916 139774434195200 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.3682961463928223, loss=4.900445938110352
I0201 02:32:41.551476 139774417409792 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.4193145036697388, loss=4.327763557434082
I0201 02:33:28.484421 139774434195200 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.3411149978637695, loss=3.249474048614502
I0201 02:34:15.422313 139774417409792 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.389930248260498, loss=3.3089494705200195
I0201 02:35:01.447416 139774434195200 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.3566569089889526, loss=3.2739901542663574
I0201 02:35:47.726206 139774417409792 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.4417848587036133, loss=4.648370265960693
I0201 02:36:34.064268 139774434195200 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.4361298084259033, loss=4.87891149520874
I0201 02:37:20.245904 139774417409792 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.3174197673797607, loss=3.4279513359069824
I0201 02:38:06.514102 139774434195200 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.1974914073944092, loss=3.856106758117676
I0201 02:38:16.900430 139936116377408 spec.py:321] Evaluating on the training split.
I0201 02:38:28.797595 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:39:01.471338 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:39:03.094371 139936116377408 submission_runner.py:408] Time since start: 48129.95s, 	Step: 95724, 	{'train/accuracy': 0.7759374976158142, 'train/loss': 1.094131588935852, 'validation/accuracy': 0.7149199843406677, 'validation/loss': 1.3668842315673828, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.9620274305343628, 'test/num_examples': 10000, 'score': 43731.27753829956, 'total_duration': 48129.953058719635, 'accumulated_submission_time': 43731.27753829956, 'accumulated_eval_time': 4390.079038143158, 'accumulated_logging_time': 3.618546485900879}
I0201 02:39:03.123618 139774417409792 logging_writer.py:48] [95724] accumulated_eval_time=4390.079038, accumulated_logging_time=3.618546, accumulated_submission_time=43731.277538, global_step=95724, preemption_count=0, score=43731.277538, test/accuracy=0.592300, test/loss=1.962027, test/num_examples=10000, total_duration=48129.953059, train/accuracy=0.775937, train/loss=1.094132, validation/accuracy=0.714920, validation/loss=1.366884, validation/num_examples=50000
I0201 02:39:33.700760 139774434195200 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.3940831422805786, loss=4.749370574951172
I0201 02:40:19.157182 139774417409792 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.2756142616271973, loss=3.433492660522461
I0201 02:41:05.526527 139774434195200 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.422250747680664, loss=3.289673089981079
I0201 02:41:51.768324 139774417409792 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.4347747564315796, loss=4.950845718383789
I0201 02:42:37.705788 139774434195200 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.2991832494735718, loss=3.5506646633148193
I0201 02:43:24.066143 139774417409792 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.3396660089492798, loss=3.219182252883911
I0201 02:44:10.401343 139774434195200 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.4075244665145874, loss=4.927517890930176
I0201 02:44:56.281141 139774417409792 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.3094110488891602, loss=3.2613062858581543
I0201 02:45:42.819683 139774434195200 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.3401563167572021, loss=3.2680420875549316
I0201 02:46:03.306215 139936116377408 spec.py:321] Evaluating on the training split.
I0201 02:46:15.099659 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:46:45.929600 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:46:47.532014 139936116377408 submission_runner.py:408] Time since start: 48594.39s, 	Step: 96646, 	{'train/accuracy': 0.7854687571525574, 'train/loss': 1.0612901449203491, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.3597755432128906, 'validation/num_examples': 50000, 'test/accuracy': 0.5936000347137451, 'test/loss': 1.9557676315307617, 'test/num_examples': 10000, 'score': 44151.39954543114, 'total_duration': 48594.39071774483, 'accumulated_submission_time': 44151.39954543114, 'accumulated_eval_time': 4434.304823875427, 'accumulated_logging_time': 3.6608870029449463}
I0201 02:46:47.570567 139774417409792 logging_writer.py:48] [96646] accumulated_eval_time=4434.304824, accumulated_logging_time=3.660887, accumulated_submission_time=44151.399545, global_step=96646, preemption_count=0, score=44151.399545, test/accuracy=0.593600, test/loss=1.955768, test/num_examples=10000, total_duration=48594.390718, train/accuracy=0.785469, train/loss=1.061290, validation/accuracy=0.714340, validation/loss=1.359776, validation/num_examples=50000
I0201 02:47:09.397381 139774434195200 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.4054598808288574, loss=3.169264078140259
I0201 02:47:53.403158 139774417409792 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.4195562601089478, loss=3.251993417739868
I0201 02:48:39.684204 139774434195200 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.4223055839538574, loss=3.303751230239868
I0201 02:49:25.906389 139774417409792 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.3451497554779053, loss=3.4716787338256836
I0201 02:50:11.945585 139774434195200 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.439896821975708, loss=4.846283912658691
I0201 02:50:58.063311 139774417409792 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.2242109775543213, loss=3.8077938556671143
I0201 02:51:44.700049 139774434195200 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.2665857076644897, loss=4.561594009399414
I0201 02:52:30.961208 139774417409792 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.3716144561767578, loss=4.862457275390625
I0201 02:53:17.271619 139774434195200 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.2714780569076538, loss=3.5166425704956055
I0201 02:53:47.802532 139936116377408 spec.py:321] Evaluating on the training split.
I0201 02:53:59.518383 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 02:54:28.941510 139936116377408 spec.py:349] Evaluating on the test split.
I0201 02:54:30.538831 139936116377408 submission_runner.py:408] Time since start: 49057.40s, 	Step: 97568, 	{'train/accuracy': 0.7908984422683716, 'train/loss': 1.0401599407196045, 'validation/accuracy': 0.7178399562835693, 'validation/loss': 1.3506696224212646, 'validation/num_examples': 50000, 'test/accuracy': 0.594700038433075, 'test/loss': 1.9495747089385986, 'test/num_examples': 10000, 'score': 44571.573185920715, 'total_duration': 49057.397535562515, 'accumulated_submission_time': 44571.573185920715, 'accumulated_eval_time': 4477.041138410568, 'accumulated_logging_time': 3.7091317176818848}
I0201 02:54:30.568513 139774417409792 logging_writer.py:48] [97568] accumulated_eval_time=4477.041138, accumulated_logging_time=3.709132, accumulated_submission_time=44571.573186, global_step=97568, preemption_count=0, score=44571.573186, test/accuracy=0.594700, test/loss=1.949575, test/num_examples=10000, total_duration=49057.397536, train/accuracy=0.790898, train/loss=1.040160, validation/accuracy=0.717840, validation/loss=1.350670, validation/num_examples=50000
I0201 02:54:43.685335 139774434195200 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.2385586500167847, loss=3.403430700302124
I0201 02:55:26.638910 139774417409792 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.400246024131775, loss=3.564730644226074
I0201 02:56:12.998784 139774434195200 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.3962458372116089, loss=4.230702877044678
I0201 02:56:59.789241 139774417409792 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.5644532442092896, loss=3.2254180908203125
I0201 02:57:46.188075 139774434195200 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.3402462005615234, loss=3.2354726791381836
I0201 02:58:32.465746 139774417409792 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.293054223060608, loss=3.3458502292633057
I0201 02:59:18.663826 139774434195200 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.3988063335418701, loss=3.2475006580352783
I0201 03:00:04.701710 139774417409792 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.4183127880096436, loss=3.3431005477905273
I0201 03:00:50.621535 139774434195200 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.5189058780670166, loss=4.816232204437256
I0201 03:01:30.613278 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:01:42.790230 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:02:13.874685 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:02:15.481748 139936116377408 submission_runner.py:408] Time since start: 49522.34s, 	Step: 98488, 	{'train/accuracy': 0.8031054735183716, 'train/loss': 0.9849762320518494, 'validation/accuracy': 0.7144799828529358, 'validation/loss': 1.3556915521621704, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.9732284545898438, 'test/num_examples': 10000, 'score': 44991.559143066406, 'total_duration': 49522.340443611145, 'accumulated_submission_time': 44991.559143066406, 'accumulated_eval_time': 4521.90961265564, 'accumulated_logging_time': 3.7491395473480225}
I0201 03:02:15.511636 139774417409792 logging_writer.py:48] [98488] accumulated_eval_time=4521.909613, accumulated_logging_time=3.749140, accumulated_submission_time=44991.559143, global_step=98488, preemption_count=0, score=44991.559143, test/accuracy=0.590200, test/loss=1.973228, test/num_examples=10000, total_duration=49522.340444, train/accuracy=0.803105, train/loss=0.984976, validation/accuracy=0.714480, validation/loss=1.355692, validation/num_examples=50000
I0201 03:02:20.677721 139774434195200 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.5240811109542847, loss=4.9450364112854
I0201 03:03:02.472943 139774417409792 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.420743465423584, loss=4.85801887512207
I0201 03:03:48.330115 139774434195200 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.4101788997650146, loss=3.2046642303466797
I0201 03:04:34.746164 139774417409792 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.3450446128845215, loss=3.1712794303894043
I0201 03:05:20.886106 139774434195200 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.5209267139434814, loss=3.3333547115325928
I0201 03:06:07.108987 139774417409792 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.40108060836792, loss=4.851995944976807
I0201 03:06:53.246429 139774434195200 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.381971001625061, loss=3.2727017402648926
I0201 03:07:39.478443 139774417409792 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.4594429731369019, loss=4.790359020233154
I0201 03:08:25.417772 139774434195200 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.52156662940979, loss=3.3039512634277344
I0201 03:09:11.607671 139774417409792 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.3916531801223755, loss=3.1761474609375
I0201 03:09:15.896118 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:09:27.930186 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:09:57.753118 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:09:59.351491 139936116377408 submission_runner.py:408] Time since start: 49986.21s, 	Step: 99411, 	{'train/accuracy': 0.7883203029632568, 'train/loss': 1.0426161289215088, 'validation/accuracy': 0.7203399538993835, 'validation/loss': 1.3304067850112915, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.932340145111084, 'test/num_examples': 10000, 'score': 45411.88533329964, 'total_duration': 49986.21019554138, 'accumulated_submission_time': 45411.88533329964, 'accumulated_eval_time': 4565.364971160889, 'accumulated_logging_time': 3.7899091243743896}
I0201 03:09:59.381534 139774434195200 logging_writer.py:48] [99411] accumulated_eval_time=4565.364971, accumulated_logging_time=3.789909, accumulated_submission_time=45411.885333, global_step=99411, preemption_count=0, score=45411.885333, test/accuracy=0.599900, test/loss=1.932340, test/num_examples=10000, total_duration=49986.210196, train/accuracy=0.788320, train/loss=1.042616, validation/accuracy=0.720340, validation/loss=1.330407, validation/num_examples=50000
I0201 03:10:35.910035 139774417409792 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.4687185287475586, loss=4.7315192222595215
I0201 03:11:21.858222 139774434195200 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.3977621793746948, loss=3.575550079345703
I0201 03:12:08.297057 139774417409792 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.2723114490509033, loss=4.438477993011475
I0201 03:12:54.502215 139774434195200 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.2701694965362549, loss=3.957603931427002
I0201 03:13:40.661046 139774417409792 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.3239706754684448, loss=4.500734806060791
I0201 03:14:26.984597 139774434195200 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.2744431495666504, loss=3.1738765239715576
I0201 03:15:13.117044 139774417409792 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.4485383033752441, loss=4.840566635131836
I0201 03:15:58.953609 139774434195200 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.384400486946106, loss=4.664087295532227
I0201 03:16:45.278012 139774417409792 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.346863031387329, loss=4.492906093597412
I0201 03:16:59.381826 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:17:11.518840 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:17:39.000210 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:17:40.606503 139936116377408 submission_runner.py:408] Time since start: 50447.47s, 	Step: 100332, 	{'train/accuracy': 0.7882617115974426, 'train/loss': 1.0813069343566895, 'validation/accuracy': 0.7168599963188171, 'validation/loss': 1.389996886253357, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.9910321235656738, 'test/num_examples': 10000, 'score': 45831.827260017395, 'total_duration': 50447.46520733833, 'accumulated_submission_time': 45831.827260017395, 'accumulated_eval_time': 4606.589636087418, 'accumulated_logging_time': 3.8308794498443604}
I0201 03:17:40.640024 139774434195200 logging_writer.py:48] [100332] accumulated_eval_time=4606.589636, accumulated_logging_time=3.830879, accumulated_submission_time=45831.827260, global_step=100332, preemption_count=0, score=45831.827260, test/accuracy=0.591300, test/loss=1.991032, test/num_examples=10000, total_duration=50447.465207, train/accuracy=0.788262, train/loss=1.081307, validation/accuracy=0.716860, validation/loss=1.389997, validation/num_examples=50000
I0201 03:18:08.061882 139774417409792 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.3336246013641357, loss=3.3055546283721924
I0201 03:18:53.023302 139774434195200 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.594852089881897, loss=3.220370292663574
I0201 03:19:39.069582 139774417409792 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.303138017654419, loss=3.996816396713257
I0201 03:20:25.444679 139774434195200 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.4276244640350342, loss=3.178190231323242
I0201 03:21:11.406945 139774417409792 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.4361319541931152, loss=3.2214059829711914
I0201 03:21:57.564837 139774434195200 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.3907558917999268, loss=4.448119640350342
I0201 03:22:43.911119 139774417409792 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.5151829719543457, loss=3.123887777328491
I0201 03:23:30.014486 139774434195200 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.2683101892471313, loss=3.7346878051757812
I0201 03:24:16.313371 139774417409792 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.3492757081985474, loss=3.4402332305908203
I0201 03:24:40.769695 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:24:52.742457 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:25:18.953153 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:25:20.560111 139936116377408 submission_runner.py:408] Time since start: 50907.42s, 	Step: 101255, 	{'train/accuracy': 0.8056640625, 'train/loss': 1.0057238340377808, 'validation/accuracy': 0.7194799780845642, 'validation/loss': 1.3632384538650513, 'validation/num_examples': 50000, 'test/accuracy': 0.5976000428199768, 'test/loss': 1.9517590999603271, 'test/num_examples': 10000, 'score': 46251.89824414253, 'total_duration': 50907.4188015461, 'accumulated_submission_time': 46251.89824414253, 'accumulated_eval_time': 4646.38002371788, 'accumulated_logging_time': 3.875702142715454}
I0201 03:25:20.593377 139774434195200 logging_writer.py:48] [101255] accumulated_eval_time=4646.380024, accumulated_logging_time=3.875702, accumulated_submission_time=46251.898244, global_step=101255, preemption_count=0, score=46251.898244, test/accuracy=0.597600, test/loss=1.951759, test/num_examples=10000, total_duration=50907.418802, train/accuracy=0.805664, train/loss=1.005724, validation/accuracy=0.719480, validation/loss=1.363238, validation/num_examples=50000
I0201 03:25:38.849668 139774417409792 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.4706814289093018, loss=3.169614791870117
I0201 03:26:22.683819 139774434195200 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.5025856494903564, loss=4.884853363037109
I0201 03:27:08.705601 139774417409792 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.420365571975708, loss=3.220690965652466
I0201 03:27:55.032991 139774434195200 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.3321670293807983, loss=4.208974838256836
I0201 03:28:41.218102 139774417409792 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.3321017026901245, loss=3.35469651222229
I0201 03:29:27.279756 139774434195200 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.5181459188461304, loss=4.4434733390808105
I0201 03:30:13.556137 139774417409792 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.3382503986358643, loss=3.355734348297119
I0201 03:30:59.418574 139774434195200 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.3904470205307007, loss=3.2608845233917236
I0201 03:31:45.609937 139774417409792 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.2733765840530396, loss=4.007678985595703
I0201 03:32:20.969696 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:32:32.859874 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:32:58.446517 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:33:00.060991 139936116377408 submission_runner.py:408] Time since start: 51366.92s, 	Step: 102178, 	{'train/accuracy': 0.785839855670929, 'train/loss': 1.0791743993759155, 'validation/accuracy': 0.7178199887275696, 'validation/loss': 1.3750110864639282, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.9908602237701416, 'test/num_examples': 10000, 'score': 46672.21717500687, 'total_duration': 51366.919667720795, 'accumulated_submission_time': 46672.21717500687, 'accumulated_eval_time': 4685.471276283264, 'accumulated_logging_time': 3.9191551208496094}
I0201 03:33:00.096910 139774434195200 logging_writer.py:48] [102178] accumulated_eval_time=4685.471276, accumulated_logging_time=3.919155, accumulated_submission_time=46672.217175, global_step=102178, preemption_count=0, score=46672.217175, test/accuracy=0.595500, test/loss=1.990860, test/num_examples=10000, total_duration=51366.919668, train/accuracy=0.785840, train/loss=1.079174, validation/accuracy=0.717820, validation/loss=1.375011, validation/num_examples=50000
I0201 03:33:09.234620 139774417409792 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.2735908031463623, loss=3.858278751373291
I0201 03:33:51.231418 139774434195200 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.411635398864746, loss=3.7390661239624023
I0201 03:34:37.252705 139774417409792 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.3687528371810913, loss=3.7550649642944336
I0201 03:35:23.923527 139774434195200 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.4091665744781494, loss=3.192528009414673
I0201 03:36:10.547070 139774417409792 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.494923710823059, loss=3.1836328506469727
I0201 03:36:56.602124 139774434195200 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.3319940567016602, loss=3.397045373916626
I0201 03:37:43.022631 139774417409792 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.5179686546325684, loss=4.883755207061768
I0201 03:38:29.254060 139774434195200 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.4843173027038574, loss=4.778261661529541
I0201 03:39:15.733379 139774417409792 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.4852654933929443, loss=4.699939727783203
I0201 03:40:00.453640 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:40:12.542505 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:40:43.108330 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:40:44.708178 139936116377408 submission_runner.py:408] Time since start: 51831.57s, 	Step: 103099, 	{'train/accuracy': 0.8001952767372131, 'train/loss': 1.0049448013305664, 'validation/accuracy': 0.7236599922180176, 'validation/loss': 1.3200238943099976, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.9122182130813599, 'test/num_examples': 10000, 'score': 47092.515449762344, 'total_duration': 51831.56687259674, 'accumulated_submission_time': 47092.515449762344, 'accumulated_eval_time': 4729.725798368454, 'accumulated_logging_time': 3.966155767440796}
I0201 03:40:44.740849 139774434195200 logging_writer.py:48] [103099] accumulated_eval_time=4729.725798, accumulated_logging_time=3.966156, accumulated_submission_time=47092.515450, global_step=103099, preemption_count=0, score=47092.515450, test/accuracy=0.601400, test/loss=1.912218, test/num_examples=10000, total_duration=51831.566873, train/accuracy=0.800195, train/loss=1.004945, validation/accuracy=0.723660, validation/loss=1.320024, validation/num_examples=50000
I0201 03:40:45.541869 139774417409792 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.4139350652694702, loss=3.1687161922454834
I0201 03:41:26.535180 139774434195200 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.4117320775985718, loss=4.171801567077637
I0201 03:42:12.538277 139774417409792 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.416540503501892, loss=3.1291027069091797
I0201 03:42:58.779378 139774434195200 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.3535016775131226, loss=3.2838966846466064
I0201 03:43:45.135401 139774417409792 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.3413288593292236, loss=3.850404739379883
I0201 03:44:31.221901 139774434195200 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.5258047580718994, loss=3.1841039657592773
I0201 03:45:17.378059 139774417409792 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.6419179439544678, loss=3.163522243499756
I0201 03:46:03.555518 139774434195200 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.4914590120315552, loss=4.695703506469727
I0201 03:46:49.959124 139774417409792 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.304921269416809, loss=4.331924915313721
I0201 03:47:35.987916 139774434195200 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.412916898727417, loss=3.145185947418213
I0201 03:47:44.978087 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:47:57.028128 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:48:23.075310 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:48:24.680269 139936116377408 submission_runner.py:408] Time since start: 52291.54s, 	Step: 104021, 	{'train/accuracy': 0.802050769329071, 'train/loss': 1.0305854082107544, 'validation/accuracy': 0.723580002784729, 'validation/loss': 1.3682783842086792, 'validation/num_examples': 50000, 'test/accuracy': 0.6020000576972961, 'test/loss': 1.9694104194641113, 'test/num_examples': 10000, 'score': 47512.693658828735, 'total_duration': 52291.53897356987, 'accumulated_submission_time': 47512.693658828735, 'accumulated_eval_time': 4769.427984714508, 'accumulated_logging_time': 4.009620189666748}
I0201 03:48:24.710749 139774417409792 logging_writer.py:48] [104021] accumulated_eval_time=4769.427985, accumulated_logging_time=4.009620, accumulated_submission_time=47512.693659, global_step=104021, preemption_count=0, score=47512.693659, test/accuracy=0.602000, test/loss=1.969410, test/num_examples=10000, total_duration=52291.538974, train/accuracy=0.802051, train/loss=1.030585, validation/accuracy=0.723580, validation/loss=1.368278, validation/num_examples=50000
I0201 03:48:56.496207 139774434195200 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.384282112121582, loss=3.4032115936279297
I0201 03:49:42.191977 139774417409792 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.4415903091430664, loss=4.619897842407227
I0201 03:50:28.489489 139774434195200 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.558779239654541, loss=3.1592607498168945
I0201 03:51:15.002175 139774417409792 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.430572748184204, loss=3.0668013095855713
I0201 03:52:00.833547 139774434195200 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.379294514656067, loss=3.1095762252807617
I0201 03:52:47.078785 139774417409792 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.3030898571014404, loss=3.8462915420532227
I0201 03:53:33.200583 139774434195200 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.5056354999542236, loss=3.183316230773926
I0201 03:54:19.863592 139774417409792 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.4667683839797974, loss=3.869802474975586
I0201 03:55:06.218752 139774434195200 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.5109139680862427, loss=3.216458320617676
I0201 03:55:24.915574 139936116377408 spec.py:321] Evaluating on the training split.
I0201 03:55:37.742288 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 03:56:09.384519 139936116377408 spec.py:349] Evaluating on the test split.
I0201 03:56:10.982764 139936116377408 submission_runner.py:408] Time since start: 52757.84s, 	Step: 104942, 	{'train/accuracy': 0.7951952815055847, 'train/loss': 1.0146361589431763, 'validation/accuracy': 0.7224000096321106, 'validation/loss': 1.322086215019226, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.9171737432479858, 'test/num_examples': 10000, 'score': 47932.840673685074, 'total_duration': 52757.84145927429, 'accumulated_submission_time': 47932.840673685074, 'accumulated_eval_time': 4815.495166063309, 'accumulated_logging_time': 4.050985097885132}
I0201 03:56:11.013299 139774417409792 logging_writer.py:48] [104942] accumulated_eval_time=4815.495166, accumulated_logging_time=4.050985, accumulated_submission_time=47932.840674, global_step=104942, preemption_count=0, score=47932.840674, test/accuracy=0.602500, test/loss=1.917174, test/num_examples=10000, total_duration=52757.841459, train/accuracy=0.795195, train/loss=1.014636, validation/accuracy=0.722400, validation/loss=1.322086, validation/num_examples=50000
I0201 03:56:34.826654 139774434195200 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.4518193006515503, loss=3.1763501167297363
I0201 03:57:19.693068 139774417409792 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.4957457780838013, loss=3.1545698642730713
I0201 03:58:05.977508 139774434195200 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.4729822874069214, loss=4.462810039520264
I0201 03:58:52.726650 139774417409792 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.471806526184082, loss=3.1507136821746826
I0201 03:59:38.817486 139774434195200 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.2596300840377808, loss=3.783587694168091
I0201 04:00:25.319658 139774417409792 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.4410781860351562, loss=3.210170030593872
I0201 04:01:11.552952 139774434195200 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.4384076595306396, loss=4.402470588684082
I0201 04:01:57.863441 139774417409792 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.392685055732727, loss=3.363861322402954
I0201 04:02:44.011034 139774434195200 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.41472589969635, loss=3.206296443939209
I0201 04:03:11.095562 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:03:23.202653 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:03:49.448886 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:03:51.057154 139936116377408 submission_runner.py:408] Time since start: 53217.92s, 	Step: 105860, 	{'train/accuracy': 0.79212886095047, 'train/loss': 1.0310273170471191, 'validation/accuracy': 0.7222200036048889, 'validation/loss': 1.32786226272583, 'validation/num_examples': 50000, 'test/accuracy': 0.6021000146865845, 'test/loss': 1.924709677696228, 'test/num_examples': 10000, 'score': 48352.493470191956, 'total_duration': 53217.91585254669, 'accumulated_submission_time': 48352.493470191956, 'accumulated_eval_time': 4855.456739187241, 'accumulated_logging_time': 4.464348077774048}
I0201 04:03:51.092310 139774417409792 logging_writer.py:48] [105860] accumulated_eval_time=4855.456739, accumulated_logging_time=4.464348, accumulated_submission_time=48352.493470, global_step=105860, preemption_count=0, score=48352.493470, test/accuracy=0.602100, test/loss=1.924710, test/num_examples=10000, total_duration=53217.915853, train/accuracy=0.792129, train/loss=1.031027, validation/accuracy=0.722220, validation/loss=1.327862, validation/num_examples=50000
I0201 04:04:07.373364 139774434195200 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.359772801399231, loss=4.159671783447266
I0201 04:04:50.643857 139774417409792 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.3152294158935547, loss=4.09783935546875
I0201 04:05:36.779356 139774434195200 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.44707190990448, loss=3.455545425415039
I0201 04:06:23.404835 139774417409792 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.3935586214065552, loss=3.1473798751831055
I0201 04:07:09.578886 139774434195200 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.4782449007034302, loss=3.3934545516967773
I0201 04:07:55.980611 139774417409792 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.6958072185516357, loss=4.81865119934082
I0201 04:08:42.495520 139774434195200 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.471320390701294, loss=3.205820322036743
I0201 04:09:28.825047 139774417409792 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.4147976636886597, loss=3.142792224884033
I0201 04:10:15.405373 139774434195200 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.439128041267395, loss=3.1605236530303955
I0201 04:10:51.517440 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:11:03.608905 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:11:32.852229 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:11:34.459771 139936116377408 submission_runner.py:408] Time since start: 53681.32s, 	Step: 106780, 	{'train/accuracy': 0.8042968511581421, 'train/loss': 0.9922462105751038, 'validation/accuracy': 0.7252999544143677, 'validation/loss': 1.32797110080719, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.9118989706039429, 'test/num_examples': 10000, 'score': 48772.86058592796, 'total_duration': 53681.318464279175, 'accumulated_submission_time': 48772.86058592796, 'accumulated_eval_time': 4898.399055242538, 'accumulated_logging_time': 4.509274482727051}
I0201 04:11:34.494823 139774417409792 logging_writer.py:48] [106780] accumulated_eval_time=4898.399055, accumulated_logging_time=4.509274, accumulated_submission_time=48772.860586, global_step=106780, preemption_count=0, score=48772.860586, test/accuracy=0.609900, test/loss=1.911899, test/num_examples=10000, total_duration=53681.318464, train/accuracy=0.804297, train/loss=0.992246, validation/accuracy=0.725300, validation/loss=1.327971, validation/num_examples=50000
I0201 04:11:42.837994 139774434195200 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.3874974250793457, loss=3.6345131397247314
I0201 04:12:25.232391 139774417409792 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.5124098062515259, loss=4.716892242431641
I0201 04:13:11.358626 139774434195200 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.6319427490234375, loss=3.2975106239318848
I0201 04:13:57.931090 139774417409792 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.4010316133499146, loss=4.126106262207031
I0201 04:14:44.351534 139774434195200 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.4613035917282104, loss=3.9731361865997314
I0201 04:15:30.474056 139774417409792 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.3902746438980103, loss=4.545564651489258
I0201 04:16:16.609253 139774434195200 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.3669991493225098, loss=3.3683197498321533
I0201 04:17:02.898234 139774417409792 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.4763526916503906, loss=3.144597291946411
I0201 04:17:49.343690 139774434195200 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.393204689025879, loss=3.2127277851104736
I0201 04:18:34.619334 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:18:46.465383 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:19:17.349490 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:19:18.959569 139936116377408 submission_runner.py:408] Time since start: 54145.82s, 	Step: 107699, 	{'train/accuracy': 0.802734375, 'train/loss': 0.9875910878181458, 'validation/accuracy': 0.7257199883460999, 'validation/loss': 1.3120582103729248, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.9079450368881226, 'test/num_examples': 10000, 'score': 49192.9281001091, 'total_duration': 54145.81826877594, 'accumulated_submission_time': 49192.9281001091, 'accumulated_eval_time': 4942.739294528961, 'accumulated_logging_time': 4.553928375244141}
I0201 04:19:18.991540 139774417409792 logging_writer.py:48] [107699] accumulated_eval_time=4942.739295, accumulated_logging_time=4.553928, accumulated_submission_time=49192.928100, global_step=107699, preemption_count=0, score=49192.928100, test/accuracy=0.602700, test/loss=1.907945, test/num_examples=10000, total_duration=54145.818269, train/accuracy=0.802734, train/loss=0.987591, validation/accuracy=0.725720, validation/loss=1.312058, validation/num_examples=50000
I0201 04:19:19.789670 139774434195200 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.321121096611023, loss=3.594188690185547
I0201 04:20:00.731951 139774417409792 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.6005699634552002, loss=4.934920787811279
I0201 04:20:46.782484 139774434195200 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.6014117002487183, loss=4.585443019866943
I0201 04:21:32.933022 139774417409792 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.3189562559127808, loss=3.1124515533447266
I0201 04:22:19.574277 139774434195200 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.3431555032730103, loss=3.716735363006592
I0201 04:23:05.901555 139774417409792 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.474247694015503, loss=3.2207295894622803
I0201 04:23:52.346113 139774434195200 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.4471931457519531, loss=3.1166248321533203
I0201 04:24:38.866317 139774417409792 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.3657523393630981, loss=3.822530746459961
I0201 04:25:25.009261 139774434195200 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.4758124351501465, loss=3.3958053588867188
I0201 04:26:11.494359 139774417409792 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.4676238298416138, loss=3.17183256149292
I0201 04:26:19.374635 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:26:31.296610 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:26:59.579406 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:27:01.178950 139936116377408 submission_runner.py:408] Time since start: 54608.04s, 	Step: 108619, 	{'train/accuracy': 0.8020898103713989, 'train/loss': 0.9975753426551819, 'validation/accuracy': 0.7286199927330017, 'validation/loss': 1.3085391521453857, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.904710292816162, 'test/num_examples': 10000, 'score': 49613.25364589691, 'total_duration': 54608.0376534462, 'accumulated_submission_time': 49613.25364589691, 'accumulated_eval_time': 4984.543601036072, 'accumulated_logging_time': 4.5956196784973145}
I0201 04:27:01.213118 139774434195200 logging_writer.py:48] [108619] accumulated_eval_time=4984.543601, accumulated_logging_time=4.595620, accumulated_submission_time=49613.253646, global_step=108619, preemption_count=0, score=49613.253646, test/accuracy=0.605100, test/loss=1.904710, test/num_examples=10000, total_duration=54608.037653, train/accuracy=0.802090, train/loss=0.997575, validation/accuracy=0.728620, validation/loss=1.308539, validation/num_examples=50000
I0201 04:27:33.852049 139774417409792 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.5393627882003784, loss=3.1667041778564453
I0201 04:28:19.893609 139774434195200 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.5494606494903564, loss=3.1440954208374023
I0201 04:29:06.425039 139774417409792 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.4032838344573975, loss=3.3467562198638916
I0201 04:29:52.769796 139774434195200 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.3193635940551758, loss=3.3934221267700195
I0201 04:30:38.811626 139774417409792 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.4895708560943604, loss=3.1715409755706787
I0201 04:31:25.143033 139774434195200 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.5237324237823486, loss=3.076667308807373
I0201 04:32:11.484562 139774417409792 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.5412472486495972, loss=4.476180076599121
I0201 04:32:57.514194 139774434195200 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.5625826120376587, loss=3.2191147804260254
I0201 04:33:43.958074 139774417409792 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.5134927034378052, loss=3.1488184928894043
I0201 04:34:01.422013 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:34:13.474194 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:34:40.728581 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:34:42.336129 139936116377408 submission_runner.py:408] Time since start: 55069.19s, 	Step: 109540, 	{'train/accuracy': 0.8037304282188416, 'train/loss': 0.9704073071479797, 'validation/accuracy': 0.7299799919128418, 'validation/loss': 1.2849246263504028, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.8863749504089355, 'test/num_examples': 10000, 'score': 50033.40543913841, 'total_duration': 55069.19483280182, 'accumulated_submission_time': 50033.40543913841, 'accumulated_eval_time': 5025.457714557648, 'accumulated_logging_time': 4.6387939453125}
I0201 04:34:42.367711 139774434195200 logging_writer.py:48] [109540] accumulated_eval_time=5025.457715, accumulated_logging_time=4.638794, accumulated_submission_time=50033.405439, global_step=109540, preemption_count=0, score=50033.405439, test/accuracy=0.607400, test/loss=1.886375, test/num_examples=10000, total_duration=55069.194833, train/accuracy=0.803730, train/loss=0.970407, validation/accuracy=0.729980, validation/loss=1.284925, validation/num_examples=50000
I0201 04:35:06.600757 139774417409792 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.8011029958724976, loss=4.743662357330322
I0201 04:35:51.129350 139774434195200 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.4859400987625122, loss=3.8557934761047363
I0201 04:36:37.094006 139774417409792 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.4919180870056152, loss=3.155827760696411
I0201 04:37:23.602949 139774434195200 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.4171993732452393, loss=3.467381715774536
I0201 04:38:09.725158 139774417409792 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.5183789730072021, loss=3.5303266048431396
I0201 04:38:56.320856 139774434195200 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.427812933921814, loss=3.5965185165405273
I0201 04:39:42.641914 139774417409792 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.5228749513626099, loss=3.375565528869629
I0201 04:40:28.763835 139774434195200 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.419316291809082, loss=3.133376121520996
I0201 04:41:15.087038 139774417409792 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.5281890630722046, loss=3.145108222961426
I0201 04:41:42.347824 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:41:54.233936 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:42:26.364400 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:42:27.990621 139936116377408 submission_runner.py:408] Time since start: 55534.85s, 	Step: 110460, 	{'train/accuracy': 0.8193163871765137, 'train/loss': 0.9186491370201111, 'validation/accuracy': 0.7305399775505066, 'validation/loss': 1.2858415842056274, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.892633080482483, 'test/num_examples': 10000, 'score': 50453.32793235779, 'total_duration': 55534.849299669266, 'accumulated_submission_time': 50453.32793235779, 'accumulated_eval_time': 5071.100483894348, 'accumulated_logging_time': 4.6809704303741455}
I0201 04:42:28.030584 139774434195200 logging_writer.py:48] [110460] accumulated_eval_time=5071.100484, accumulated_logging_time=4.680970, accumulated_submission_time=50453.327932, global_step=110460, preemption_count=0, score=50453.327932, test/accuracy=0.607700, test/loss=1.892633, test/num_examples=10000, total_duration=55534.849300, train/accuracy=0.819316, train/loss=0.918649, validation/accuracy=0.730540, validation/loss=1.285842, validation/num_examples=50000
I0201 04:42:44.323392 139774417409792 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.4790773391723633, loss=4.516515254974365
I0201 04:43:27.833302 139774434195200 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.5389758348464966, loss=3.117509603500366
I0201 04:44:13.595576 139774417409792 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.5546271800994873, loss=3.196293354034424
I0201 04:44:59.743236 139774434195200 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.6313681602478027, loss=3.3005545139312744
I0201 04:45:45.900024 139774417409792 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.7505877017974854, loss=4.745858669281006
I0201 04:46:32.086669 139774434195200 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.3466681241989136, loss=3.4057345390319824
I0201 04:47:18.258345 139774417409792 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.3860468864440918, loss=3.957620859146118
I0201 04:48:04.408179 139774434195200 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.5154361724853516, loss=4.138012886047363
I0201 04:48:50.459312 139774417409792 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.5155609846115112, loss=3.093770980834961
I0201 04:49:28.254152 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:49:40.199916 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:50:11.889444 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:50:13.503293 139936116377408 submission_runner.py:408] Time since start: 56000.36s, 	Step: 111382, 	{'train/accuracy': 0.8045117259025574, 'train/loss': 0.9946995973587036, 'validation/accuracy': 0.7307999730110168, 'validation/loss': 1.30134117603302, 'validation/num_examples': 50000, 'test/accuracy': 0.6093000173568726, 'test/loss': 1.9110552072525024, 'test/num_examples': 10000, 'score': 50873.48181271553, 'total_duration': 56000.36197376251, 'accumulated_submission_time': 50873.48181271553, 'accumulated_eval_time': 5116.349600315094, 'accumulated_logging_time': 4.732409715652466}
I0201 04:50:13.540559 139774434195200 logging_writer.py:48] [111382] accumulated_eval_time=5116.349600, accumulated_logging_time=4.732410, accumulated_submission_time=50873.481813, global_step=111382, preemption_count=0, score=50873.481813, test/accuracy=0.609300, test/loss=1.911055, test/num_examples=10000, total_duration=56000.361974, train/accuracy=0.804512, train/loss=0.994700, validation/accuracy=0.730800, validation/loss=1.301341, validation/num_examples=50000
I0201 04:50:21.090452 139774417409792 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.6456788778305054, loss=4.3815836906433105
I0201 04:51:03.312793 139774434195200 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.4408106803894043, loss=4.326388359069824
I0201 04:51:49.527028 139774417409792 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.5681177377700806, loss=3.239154815673828
I0201 04:52:35.671405 139774434195200 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.4548871517181396, loss=3.065221071243286
I0201 04:53:21.814826 139774417409792 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.4834158420562744, loss=3.2881250381469727
I0201 04:54:07.875991 139774434195200 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.5076045989990234, loss=3.532464027404785
I0201 04:54:53.880128 139774417409792 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.5234874486923218, loss=4.515700340270996
I0201 04:55:40.177964 139774434195200 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.5486814975738525, loss=4.2985992431640625
I0201 04:56:26.168964 139774417409792 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.5685653686523438, loss=3.095618724822998
I0201 04:57:12.200985 139774434195200 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.4498043060302734, loss=3.478148937225342
I0201 04:57:13.930033 139936116377408 spec.py:321] Evaluating on the training split.
I0201 04:57:25.885276 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 04:57:57.136005 139936116377408 spec.py:349] Evaluating on the test split.
I0201 04:57:58.737690 139936116377408 submission_runner.py:408] Time since start: 56465.60s, 	Step: 112305, 	{'train/accuracy': 0.8092968463897705, 'train/loss': 0.9513814449310303, 'validation/accuracy': 0.7310999631881714, 'validation/loss': 1.288879632949829, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.8916889429092407, 'test/num_examples': 10000, 'score': 51293.81174302101, 'total_duration': 56465.59639286995, 'accumulated_submission_time': 51293.81174302101, 'accumulated_eval_time': 5161.157242059708, 'accumulated_logging_time': 4.781898736953735}
I0201 04:57:58.769460 139774417409792 logging_writer.py:48] [112305] accumulated_eval_time=5161.157242, accumulated_logging_time=4.781899, accumulated_submission_time=51293.811743, global_step=112305, preemption_count=0, score=51293.811743, test/accuracy=0.609000, test/loss=1.891689, test/num_examples=10000, total_duration=56465.596393, train/accuracy=0.809297, train/loss=0.951381, validation/accuracy=0.731100, validation/loss=1.288880, validation/num_examples=50000
I0201 04:58:37.717784 139774434195200 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.6411464214324951, loss=3.1986207962036133
I0201 04:59:23.683480 139774417409792 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.6972763538360596, loss=3.1992387771606445
I0201 05:00:10.149360 139774434195200 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.3741497993469238, loss=3.689870834350586
I0201 05:00:56.327934 139774417409792 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.4272483587265015, loss=3.403299331665039
I0201 05:01:42.662204 139774434195200 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.4985058307647705, loss=3.179117441177368
I0201 05:02:28.791171 139774417409792 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.4909571409225464, loss=3.21950364112854
I0201 05:03:14.876087 139774434195200 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.4205327033996582, loss=3.118712902069092
I0201 05:04:00.922849 139774417409792 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.5637160539627075, loss=3.1482763290405273
I0201 05:04:47.223921 139774434195200 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.4910143613815308, loss=3.0743513107299805
I0201 05:04:58.847345 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:05:11.115996 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:05:41.064287 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:05:42.674968 139936116377408 submission_runner.py:408] Time since start: 56929.53s, 	Step: 113227, 	{'train/accuracy': 0.81849604845047, 'train/loss': 0.898280680179596, 'validation/accuracy': 0.7363399863243103, 'validation/loss': 1.2454630136489868, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.8420090675354004, 'test/num_examples': 10000, 'score': 51713.83294534683, 'total_duration': 56929.53366589546, 'accumulated_submission_time': 51713.83294534683, 'accumulated_eval_time': 5204.9848573207855, 'accumulated_logging_time': 4.822944164276123}
I0201 05:05:42.708685 139774417409792 logging_writer.py:48] [113227] accumulated_eval_time=5204.984857, accumulated_logging_time=4.822944, accumulated_submission_time=51713.832945, global_step=113227, preemption_count=0, score=51713.832945, test/accuracy=0.617700, test/loss=1.842009, test/num_examples=10000, total_duration=56929.533666, train/accuracy=0.818496, train/loss=0.898281, validation/accuracy=0.736340, validation/loss=1.245463, validation/num_examples=50000
I0201 05:06:12.086331 139774434195200 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.52633535861969, loss=3.1510276794433594
I0201 05:06:57.279786 139774417409792 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.657137155532837, loss=4.468080520629883
I0201 05:07:43.612686 139774434195200 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.4549585580825806, loss=3.532142400741577
I0201 05:08:30.283961 139774417409792 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.4687215089797974, loss=3.20849347114563
I0201 05:09:16.345616 139774434195200 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.4945127964019775, loss=3.146289825439453
I0201 05:10:02.565654 139774417409792 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.4791873693466187, loss=3.115218162536621
I0201 05:10:48.981981 139774434195200 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.6028307676315308, loss=3.2722392082214355
I0201 05:11:35.091816 139774417409792 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.4111768007278442, loss=3.8106331825256348
I0201 05:12:21.432595 139774434195200 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.5510233640670776, loss=3.0690529346466064
I0201 05:12:42.879287 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:12:54.897882 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:13:20.823665 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:13:22.442930 139936116377408 submission_runner.py:408] Time since start: 57389.30s, 	Step: 114148, 	{'train/accuracy': 0.8068945407867432, 'train/loss': 0.9638850688934326, 'validation/accuracy': 0.7357199788093567, 'validation/loss': 1.2680931091308594, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.8680979013442993, 'test/num_examples': 10000, 'score': 52133.946326971054, 'total_duration': 57389.301633358, 'accumulated_submission_time': 52133.946326971054, 'accumulated_eval_time': 5244.5484964847565, 'accumulated_logging_time': 4.866366386413574}
I0201 05:13:22.474861 139774417409792 logging_writer.py:48] [114148] accumulated_eval_time=5244.548496, accumulated_logging_time=4.866366, accumulated_submission_time=52133.946327, global_step=114148, preemption_count=0, score=52133.946327, test/accuracy=0.614500, test/loss=1.868098, test/num_examples=10000, total_duration=57389.301633, train/accuracy=0.806895, train/loss=0.963885, validation/accuracy=0.735720, validation/loss=1.268093, validation/num_examples=50000
I0201 05:13:43.502475 139774434195200 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.5017569065093994, loss=3.0705020427703857
I0201 05:14:27.615339 139774417409792 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.3898773193359375, loss=3.0327117443084717
I0201 05:15:13.806314 139774434195200 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.461370587348938, loss=3.595423460006714
I0201 05:16:00.229407 139774417409792 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.6384247541427612, loss=3.1408274173736572
I0201 05:16:46.273955 139774434195200 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.4473620653152466, loss=3.301516056060791
I0201 05:17:32.191649 139774417409792 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.4755183458328247, loss=3.180488348007202
I0201 05:18:18.381361 139774434195200 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.7993543148040771, loss=4.808818817138672
I0201 05:19:04.608263 139774417409792 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.4878771305084229, loss=3.3819258213043213
I0201 05:19:50.824308 139774434195200 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.7277073860168457, loss=3.57525897026062
I0201 05:20:22.822599 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:20:34.800503 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:21:02.623664 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:21:04.237129 139936116377408 submission_runner.py:408] Time since start: 57851.10s, 	Step: 115071, 	{'train/accuracy': 0.81201171875, 'train/loss': 0.9675384163856506, 'validation/accuracy': 0.7343999743461609, 'validation/loss': 1.2889907360076904, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.8860503435134888, 'test/num_examples': 10000, 'score': 52554.237533807755, 'total_duration': 57851.095802783966, 'accumulated_submission_time': 52554.237533807755, 'accumulated_eval_time': 5285.962988376617, 'accumulated_logging_time': 4.9072349071502686}
I0201 05:21:04.276148 139774417409792 logging_writer.py:48] [115071] accumulated_eval_time=5285.962988, accumulated_logging_time=4.907235, accumulated_submission_time=52554.237534, global_step=115071, preemption_count=0, score=52554.237534, test/accuracy=0.614300, test/loss=1.886050, test/num_examples=10000, total_duration=57851.095803, train/accuracy=0.812012, train/loss=0.967538, validation/accuracy=0.734400, validation/loss=1.288991, validation/num_examples=50000
I0201 05:21:16.199324 139774434195200 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.4987046718597412, loss=3.1022253036499023
I0201 05:21:59.009140 139774417409792 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.3733456134796143, loss=3.8480513095855713
I0201 05:22:45.319254 139774434195200 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.564578652381897, loss=3.133171558380127
I0201 05:23:31.601758 139774417409792 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.4789929389953613, loss=3.0647330284118652
I0201 05:24:17.815884 139774434195200 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.628706693649292, loss=3.181624412536621
I0201 05:25:04.102673 139774417409792 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.6318628787994385, loss=3.1948087215423584
I0201 05:25:50.530970 139774434195200 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.5619736909866333, loss=3.8307790756225586
I0201 05:26:36.990990 139774417409792 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.5680460929870605, loss=3.107416868209839
I0201 05:27:23.463087 139774434195200 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.5895717144012451, loss=4.687318801879883
I0201 05:28:04.631830 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:28:16.535485 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:28:46.962812 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:28:48.567437 139936116377408 submission_runner.py:408] Time since start: 58315.43s, 	Step: 115991, 	{'train/accuracy': 0.8161523342132568, 'train/loss': 0.93476402759552, 'validation/accuracy': 0.7350599765777588, 'validation/loss': 1.2736767530441284, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.8698878288269043, 'test/num_examples': 10000, 'score': 52974.534670591354, 'total_duration': 58315.426132917404, 'accumulated_submission_time': 52974.534670591354, 'accumulated_eval_time': 5329.898587703705, 'accumulated_logging_time': 4.956961631774902}
I0201 05:28:48.603048 139774417409792 logging_writer.py:48] [115991] accumulated_eval_time=5329.898588, accumulated_logging_time=4.956962, accumulated_submission_time=52974.534671, global_step=115991, preemption_count=0, score=52974.534671, test/accuracy=0.617200, test/loss=1.869888, test/num_examples=10000, total_duration=58315.426133, train/accuracy=0.816152, train/loss=0.934764, validation/accuracy=0.735060, validation/loss=1.273677, validation/num_examples=50000
I0201 05:28:52.575408 139774434195200 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.5664044618606567, loss=3.1350715160369873
I0201 05:29:34.233208 139774417409792 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.5731977224349976, loss=4.169106483459473
I0201 05:30:20.485388 139774434195200 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.6551258563995361, loss=3.1504390239715576
I0201 05:31:07.192675 139774417409792 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.5694921016693115, loss=3.1878702640533447
I0201 05:31:54.115347 139774434195200 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.5453630685806274, loss=3.252725124359131
I0201 05:32:40.264856 139774417409792 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.486523985862732, loss=3.8489365577697754
I0201 05:33:26.812793 139774434195200 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.680195689201355, loss=3.126091241836548
I0201 05:34:12.839655 139774417409792 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.683887243270874, loss=3.0981295108795166
I0201 05:34:59.018744 139774434195200 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.565536379814148, loss=3.113037586212158
I0201 05:35:45.525238 139774417409792 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.7240650653839111, loss=3.1498005390167236
I0201 05:35:48.909913 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:36:00.832965 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:36:28.873326 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:36:30.481942 139936116377408 submission_runner.py:408] Time since start: 58777.34s, 	Step: 116909, 	{'train/accuracy': 0.8122069835662842, 'train/loss': 0.9754149317741394, 'validation/accuracy': 0.7371199727058411, 'validation/loss': 1.2887877225875854, 'validation/num_examples': 50000, 'test/accuracy': 0.6185000538825989, 'test/loss': 1.8774621486663818, 'test/num_examples': 10000, 'score': 53394.782676935196, 'total_duration': 58777.340618133545, 'accumulated_submission_time': 53394.782676935196, 'accumulated_eval_time': 5371.47057056427, 'accumulated_logging_time': 5.00386118888855}
I0201 05:36:30.522767 139774434195200 logging_writer.py:48] [116909] accumulated_eval_time=5371.470571, accumulated_logging_time=5.003861, accumulated_submission_time=53394.782677, global_step=116909, preemption_count=0, score=53394.782677, test/accuracy=0.618500, test/loss=1.877462, test/num_examples=10000, total_duration=58777.340618, train/accuracy=0.812207, train/loss=0.975415, validation/accuracy=0.737120, validation/loss=1.288788, validation/num_examples=50000
I0201 05:37:07.840532 139774417409792 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.6362378597259521, loss=3.2027556896209717
I0201 05:37:53.581152 139774434195200 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.6927348375320435, loss=3.192157506942749
I0201 05:38:39.755571 139774417409792 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.5274059772491455, loss=3.79837965965271
I0201 05:39:26.062597 139774434195200 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.6777647733688354, loss=3.1216022968292236
I0201 05:40:12.100197 139774417409792 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.8721554279327393, loss=4.75444221496582
I0201 05:40:58.117963 139774434195200 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.6887505054473877, loss=4.503115653991699
I0201 05:41:44.833691 139774417409792 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.651869297027588, loss=4.520142555236816
I0201 05:42:31.044995 139774434195200 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.6369547843933105, loss=3.0296578407287598
I0201 05:43:17.720595 139774417409792 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.608837604522705, loss=3.18331241607666
I0201 05:43:30.605925 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:43:42.589109 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:44:14.130651 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:44:15.741555 139936116377408 submission_runner.py:408] Time since start: 59242.60s, 	Step: 117830, 	{'train/accuracy': 0.81898432970047, 'train/loss': 0.918323814868927, 'validation/accuracy': 0.7381199598312378, 'validation/loss': 1.2621748447418213, 'validation/num_examples': 50000, 'test/accuracy': 0.617900013923645, 'test/loss': 1.8480632305145264, 'test/num_examples': 10000, 'score': 53814.80868077278, 'total_duration': 59242.60025715828, 'accumulated_submission_time': 53814.80868077278, 'accumulated_eval_time': 5416.606198310852, 'accumulated_logging_time': 5.0547590255737305}
I0201 05:44:15.773854 139774434195200 logging_writer.py:48] [117830] accumulated_eval_time=5416.606198, accumulated_logging_time=5.054759, accumulated_submission_time=53814.808681, global_step=117830, preemption_count=0, score=53814.808681, test/accuracy=0.617900, test/loss=1.848063, test/num_examples=10000, total_duration=59242.600257, train/accuracy=0.818984, train/loss=0.918324, validation/accuracy=0.738120, validation/loss=1.262175, validation/num_examples=50000
I0201 05:44:43.979730 139774417409792 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.7466257810592651, loss=4.455150127410889
I0201 05:45:29.161572 139774434195200 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.8054784536361694, loss=4.753068923950195
I0201 05:46:15.283276 139774417409792 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.5816644430160522, loss=4.462762355804443
I0201 05:47:01.489920 139774434195200 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.4255702495574951, loss=3.82226824760437
I0201 05:47:47.544687 139774417409792 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.5805743932724, loss=3.71351957321167
I0201 05:48:33.740080 139774434195200 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.7907536029815674, loss=3.1226346492767334
I0201 05:49:20.128480 139774417409792 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.535624623298645, loss=4.013790607452393
I0201 05:50:06.292704 139774434195200 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.4850870370864868, loss=3.370122194290161
I0201 05:50:52.342092 139774417409792 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.5891411304473877, loss=3.1239423751831055
I0201 05:51:16.048671 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:51:28.027423 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:51:59.446899 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:52:01.053696 139936116377408 submission_runner.py:408] Time since start: 59707.91s, 	Step: 118753, 	{'train/accuracy': 0.8180468678474426, 'train/loss': 0.9298332929611206, 'validation/accuracy': 0.7390599846839905, 'validation/loss': 1.263730764389038, 'validation/num_examples': 50000, 'test/accuracy': 0.6194000244140625, 'test/loss': 1.8396508693695068, 'test/num_examples': 10000, 'score': 54235.02416753769, 'total_duration': 59707.91237425804, 'accumulated_submission_time': 54235.02416753769, 'accumulated_eval_time': 5461.611189365387, 'accumulated_logging_time': 5.096725225448608}
I0201 05:52:01.093269 139774434195200 logging_writer.py:48] [118753] accumulated_eval_time=5461.611189, accumulated_logging_time=5.096725, accumulated_submission_time=54235.024168, global_step=118753, preemption_count=0, score=54235.024168, test/accuracy=0.619400, test/loss=1.839651, test/num_examples=10000, total_duration=59707.912374, train/accuracy=0.818047, train/loss=0.929833, validation/accuracy=0.739060, validation/loss=1.263731, validation/num_examples=50000
I0201 05:52:20.164780 139774417409792 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.5539026260375977, loss=3.341033935546875
I0201 05:53:04.491058 139774434195200 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.468979835510254, loss=2.9987564086914062
I0201 05:53:50.728350 139774417409792 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.6334081888198853, loss=3.2386868000030518
I0201 05:54:37.451739 139774434195200 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.6216799020767212, loss=3.638417959213257
I0201 05:55:23.498177 139774417409792 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.59571373462677, loss=4.155032157897949
I0201 05:56:09.510293 139774434195200 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.5979282855987549, loss=3.0861306190490723
I0201 05:56:55.559455 139774417409792 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.5642982721328735, loss=3.944941997528076
I0201 05:57:41.687177 139774434195200 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.6096080541610718, loss=3.1850333213806152
I0201 05:58:28.192285 139774417409792 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.7290544509887695, loss=3.9162373542785645
I0201 05:59:01.531859 139936116377408 spec.py:321] Evaluating on the training split.
I0201 05:59:13.545755 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 05:59:41.280385 139936116377408 spec.py:349] Evaluating on the test split.
I0201 05:59:42.893386 139936116377408 submission_runner.py:408] Time since start: 60169.75s, 	Step: 119674, 	{'train/accuracy': 0.8340820074081421, 'train/loss': 0.8498026728630066, 'validation/accuracy': 0.7407199740409851, 'validation/loss': 1.2424492835998535, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.8454985618591309, 'test/num_examples': 10000, 'score': 54655.402338027954, 'total_duration': 60169.75206565857, 'accumulated_submission_time': 54655.402338027954, 'accumulated_eval_time': 5502.972692966461, 'accumulated_logging_time': 5.1483683586120605}
I0201 05:59:42.930296 139774434195200 logging_writer.py:48] [119674] accumulated_eval_time=5502.972693, accumulated_logging_time=5.148368, accumulated_submission_time=54655.402338, global_step=119674, preemption_count=0, score=54655.402338, test/accuracy=0.614000, test/loss=1.845499, test/num_examples=10000, total_duration=60169.752066, train/accuracy=0.834082, train/loss=0.849803, validation/accuracy=0.740720, validation/loss=1.242449, validation/num_examples=50000
I0201 05:59:53.662650 139774417409792 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.7768466472625732, loss=4.44253396987915
I0201 06:00:36.128211 139774434195200 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.5466147661209106, loss=3.606400966644287
I0201 06:01:22.380189 139774417409792 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.5945489406585693, loss=3.0908126831054688
I0201 06:02:09.000639 139774434195200 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.6031161546707153, loss=3.2642898559570312
I0201 06:02:55.414636 139774417409792 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.5754915475845337, loss=3.118190050125122
I0201 06:03:41.849630 139774434195200 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.6071463823318481, loss=3.3164761066436768
I0201 06:04:28.229279 139774417409792 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.7230463027954102, loss=3.5384860038757324
I0201 06:05:14.678508 139774434195200 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.6595191955566406, loss=3.1111083030700684
I0201 06:06:00.840679 139774417409792 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.6459509134292603, loss=3.1925880908966064
I0201 06:06:43.153251 139936116377408 spec.py:321] Evaluating on the training split.
I0201 06:06:55.119878 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 06:07:20.582594 139936116377408 spec.py:349] Evaluating on the test split.
I0201 06:07:22.189242 139936116377408 submission_runner.py:408] Time since start: 60629.05s, 	Step: 120593, 	{'train/accuracy': 0.8211132884025574, 'train/loss': 0.9188494682312012, 'validation/accuracy': 0.7406799793243408, 'validation/loss': 1.249154806137085, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.8333444595336914, 'test/num_examples': 10000, 'score': 55075.563134908676, 'total_duration': 60629.047945261, 'accumulated_submission_time': 55075.563134908676, 'accumulated_eval_time': 5542.008673429489, 'accumulated_logging_time': 5.199536323547363}
I0201 06:07:22.222018 139774434195200 logging_writer.py:48] [120593] accumulated_eval_time=5542.008673, accumulated_logging_time=5.199536, accumulated_submission_time=55075.563135, global_step=120593, preemption_count=0, score=55075.563135, test/accuracy=0.623900, test/loss=1.833344, test/num_examples=10000, total_duration=60629.047945, train/accuracy=0.821113, train/loss=0.918849, validation/accuracy=0.740680, validation/loss=1.249155, validation/num_examples=50000
I0201 06:07:25.407914 139774417409792 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.685290813446045, loss=3.142271041870117
I0201 06:08:06.917489 139774434195200 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.7905441522598267, loss=3.0972743034362793
I0201 06:08:52.671664 139774417409792 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.5980463027954102, loss=3.8461267948150635
I0201 06:09:39.070042 139774434195200 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.5852223634719849, loss=3.2601993083953857
I0201 06:10:25.313978 139774417409792 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.6731796264648438, loss=4.304415702819824
I0201 06:11:11.542360 139774434195200 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.7688788175582886, loss=3.0999724864959717
I0201 06:11:57.974482 139774417409792 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.615425944328308, loss=4.24366569519043
I0201 06:12:43.927762 139774434195200 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.565636157989502, loss=3.443011522293091
I0201 06:13:30.476625 139774417409792 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.776902437210083, loss=4.249865531921387
I0201 06:14:16.707701 139774434195200 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.6309400796890259, loss=3.03395676612854
I0201 06:14:22.391784 139936116377408 spec.py:321] Evaluating on the training split.
I0201 06:14:34.331823 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 06:15:02.779281 139936116377408 spec.py:349] Evaluating on the test split.
I0201 06:15:04.390745 139936116377408 submission_runner.py:408] Time since start: 61091.25s, 	Step: 121514, 	{'train/accuracy': 0.8241796493530273, 'train/loss': 0.8857353925704956, 'validation/accuracy': 0.7432799935340881, 'validation/loss': 1.2298856973648071, 'validation/num_examples': 50000, 'test/accuracy': 0.6218000054359436, 'test/loss': 1.8167213201522827, 'test/num_examples': 10000, 'score': 55495.675506830215, 'total_duration': 61091.24942660332, 'accumulated_submission_time': 55495.675506830215, 'accumulated_eval_time': 5584.007612705231, 'accumulated_logging_time': 5.241910457611084}
I0201 06:15:04.425162 139774417409792 logging_writer.py:48] [121514] accumulated_eval_time=5584.007613, accumulated_logging_time=5.241910, accumulated_submission_time=55495.675507, global_step=121514, preemption_count=0, score=55495.675507, test/accuracy=0.621800, test/loss=1.816721, test/num_examples=10000, total_duration=61091.249427, train/accuracy=0.824180, train/loss=0.885735, validation/accuracy=0.743280, validation/loss=1.229886, validation/num_examples=50000
I0201 06:15:39.389033 139774434195200 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.5476118326187134, loss=3.0916740894317627
I0201 06:16:25.094088 139774417409792 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.6512782573699951, loss=3.089449167251587
I0201 06:17:11.457388 139774434195200 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.5513718128204346, loss=3.355712652206421
I0201 06:17:57.809923 139774417409792 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.68881094455719, loss=3.0815067291259766
I0201 06:18:43.917707 139774434195200 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.732904314994812, loss=3.1381866931915283
I0201 06:19:30.294173 139774417409792 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.6707583665847778, loss=3.0703651905059814
I0201 06:20:16.695019 139774434195200 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.5943989753723145, loss=3.353364944458008
I0201 06:21:03.028183 139774417409792 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.6036577224731445, loss=3.392256259918213
I0201 06:21:49.324696 139774434195200 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.7288566827774048, loss=4.328364372253418
I0201 06:22:04.784621 139936116377408 spec.py:321] Evaluating on the training split.
I0201 06:22:16.805612 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 06:22:48.041322 139936116377408 spec.py:349] Evaluating on the test split.
I0201 06:22:49.642126 139936116377408 submission_runner.py:408] Time since start: 61556.50s, 	Step: 122435, 	{'train/accuracy': 0.8314648270606995, 'train/loss': 0.890949010848999, 'validation/accuracy': 0.7434200048446655, 'validation/loss': 1.2616345882415771, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.8468738794326782, 'test/num_examples': 10000, 'score': 55915.97837305069, 'total_duration': 61556.50082373619, 'accumulated_submission_time': 55915.97837305069, 'accumulated_eval_time': 5628.865107297897, 'accumulated_logging_time': 5.285839796066284}
I0201 06:22:49.678426 139774417409792 logging_writer.py:48] [122435] accumulated_eval_time=5628.865107, accumulated_logging_time=5.285840, accumulated_submission_time=55915.978373, global_step=122435, preemption_count=0, score=55915.978373, test/accuracy=0.623000, test/loss=1.846874, test/num_examples=10000, total_duration=61556.500824, train/accuracy=0.831465, train/loss=0.890949, validation/accuracy=0.743420, validation/loss=1.261635, validation/num_examples=50000
I0201 06:23:16.038760 139774434195200 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.569106936454773, loss=3.0956411361694336
I0201 06:24:01.047962 139774417409792 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.7038688659667969, loss=3.0525224208831787
I0201 06:24:47.177693 139774434195200 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.6454397439956665, loss=3.072925090789795
I0201 06:25:33.416926 139774417409792 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.8013280630111694, loss=3.107039213180542
I0201 06:26:19.454335 139774434195200 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.620806097984314, loss=3.2752151489257812
I0201 06:27:05.661929 139774417409792 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.5431153774261475, loss=3.5201470851898193
I0201 06:27:51.778008 139774434195200 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.5786725282669067, loss=3.828547954559326
I0201 06:28:38.032349 139774417409792 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.7634270191192627, loss=3.1415586471557617
I0201 06:29:24.272688 139774434195200 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.719728708267212, loss=3.2292981147766113
I0201 06:29:49.931835 139936116377408 spec.py:321] Evaluating on the training split.
I0201 06:30:01.893114 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 06:30:31.457457 139936116377408 spec.py:349] Evaluating on the test split.
I0201 06:30:33.053334 139936116377408 submission_runner.py:408] Time since start: 62019.91s, 	Step: 123357, 	{'train/accuracy': 0.8240624666213989, 'train/loss': 0.9000077843666077, 'validation/accuracy': 0.7458999752998352, 'validation/loss': 1.2291886806488037, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8252770900726318, 'test/num_examples': 10000, 'score': 56336.17573904991, 'total_duration': 62019.912034511566, 'accumulated_submission_time': 56336.17573904991, 'accumulated_eval_time': 5671.9865918159485, 'accumulated_logging_time': 5.331271648406982}
I0201 06:30:33.089847 139774417409792 logging_writer.py:48] [123357] accumulated_eval_time=5671.986592, accumulated_logging_time=5.331272, accumulated_submission_time=56336.175739, global_step=123357, preemption_count=0, score=56336.175739, test/accuracy=0.627100, test/loss=1.825277, test/num_examples=10000, total_duration=62019.912035, train/accuracy=0.824062, train/loss=0.900008, validation/accuracy=0.745900, validation/loss=1.229189, validation/num_examples=50000
I0201 06:30:50.555898 139774434195200 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.588387131690979, loss=3.082422971725464
I0201 06:31:34.006209 139774417409792 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.7714331150054932, loss=3.1764564514160156
I0201 06:32:20.515362 139774434195200 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.8106049299240112, loss=4.443206310272217
I0201 06:33:07.296344 139774417409792 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.6369943618774414, loss=3.1069133281707764
I0201 06:33:53.163831 139774434195200 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.7637393474578857, loss=4.2599196434021
I0201 06:34:39.638496 139774417409792 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.6630877256393433, loss=4.08777379989624
I0201 06:35:26.144145 139774434195200 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.7754164934158325, loss=3.068221092224121
I0201 06:36:12.295725 139774417409792 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.7150797843933105, loss=3.2572555541992188
I0201 06:36:58.409891 139774434195200 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.8702905178070068, loss=3.0860509872436523
I0201 06:37:33.208772 139936116377408 spec.py:321] Evaluating on the training split.
I0201 06:37:45.083845 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 06:38:12.062818 139936116377408 spec.py:349] Evaluating on the test split.
I0201 06:38:13.674024 139936116377408 submission_runner.py:408] Time since start: 62480.53s, 	Step: 124277, 	{'train/accuracy': 0.8270507454872131, 'train/loss': 0.9012371897697449, 'validation/accuracy': 0.7419599890708923, 'validation/loss': 1.2539701461791992, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.840736985206604, 'test/num_examples': 10000, 'score': 56756.237129449844, 'total_duration': 62480.532720565796, 'accumulated_submission_time': 56756.237129449844, 'accumulated_eval_time': 5712.451839923859, 'accumulated_logging_time': 5.377405405044556}
I0201 06:38:13.710572 139774417409792 logging_writer.py:48] [124277] accumulated_eval_time=5712.451840, accumulated_logging_time=5.377405, accumulated_submission_time=56756.237129, global_step=124277, preemption_count=0, score=56756.237129, test/accuracy=0.622300, test/loss=1.840737, test/num_examples=10000, total_duration=62480.532721, train/accuracy=0.827051, train/loss=0.901237, validation/accuracy=0.741960, validation/loss=1.253970, validation/num_examples=50000
I0201 06:38:23.245822 139774434195200 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.598671555519104, loss=3.0957863330841064
I0201 06:39:05.620158 139774417409792 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.6124504804611206, loss=3.7578012943267822
I0201 06:39:51.368109 139774434195200 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.6765873432159424, loss=3.832730531692505
I0201 06:40:37.665643 139774417409792 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.705684781074524, loss=3.5986223220825195
I0201 06:41:23.720896 139774434195200 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.6511361598968506, loss=3.8949155807495117
I0201 06:42:10.297391 139774417409792 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.7864934206008911, loss=3.0306055545806885
I0201 06:42:56.392710 139774434195200 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.6050591468811035, loss=3.0329575538635254
I0201 06:43:42.377426 139774417409792 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.7838068008422852, loss=3.16396427154541
I0201 06:44:29.125366 139774434195200 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.8488271236419678, loss=3.0104150772094727
I0201 06:45:14.050497 139936116377408 spec.py:321] Evaluating on the training split.
I0201 06:45:26.151911 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 06:45:55.827373 139936116377408 spec.py:349] Evaluating on the test split.
I0201 06:45:57.436397 139936116377408 submission_runner.py:408] Time since start: 62944.30s, 	Step: 125199, 	{'train/accuracy': 0.8361327648162842, 'train/loss': 0.8307301998138428, 'validation/accuracy': 0.746999979019165, 'validation/loss': 1.1995173692703247, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.7952983379364014, 'test/num_examples': 10000, 'score': 57176.52000498772, 'total_duration': 62944.29509925842, 'accumulated_submission_time': 57176.52000498772, 'accumulated_eval_time': 5755.837740182877, 'accumulated_logging_time': 5.423417568206787}
I0201 06:45:57.470165 139774417409792 logging_writer.py:48] [125199] accumulated_eval_time=5755.837740, accumulated_logging_time=5.423418, accumulated_submission_time=57176.520005, global_step=125199, preemption_count=0, score=57176.520005, test/accuracy=0.629300, test/loss=1.795298, test/num_examples=10000, total_duration=62944.295099, train/accuracy=0.836133, train/loss=0.830730, validation/accuracy=0.747000, validation/loss=1.199517, validation/num_examples=50000
I0201 06:45:58.272024 139774434195200 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.7481436729431152, loss=4.170062065124512
I0201 06:46:39.297773 139774417409792 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.5901442766189575, loss=3.195512533187866
I0201 06:47:25.427840 139774434195200 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.7356783151626587, loss=2.993800163269043
I0201 06:48:11.857613 139774417409792 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.6897610425949097, loss=2.9891669750213623
I0201 06:48:58.095849 139774434195200 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.821556568145752, loss=4.347936630249023
I0201 06:49:44.315911 139774417409792 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.6146931648254395, loss=3.8586249351501465
I0201 06:50:30.769757 139774434195200 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.656138300895691, loss=3.2629330158233643
I0201 06:51:17.079922 139774417409792 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.9771876335144043, loss=4.605088710784912
I0201 06:52:03.481284 139774434195200 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.781821370124817, loss=3.010493516921997
I0201 06:52:49.718441 139774417409792 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.588148832321167, loss=3.315807342529297
I0201 06:52:57.778738 139936116377408 spec.py:321] Evaluating on the training split.
I0201 06:53:09.793287 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 06:53:40.991360 139936116377408 spec.py:349] Evaluating on the test split.
I0201 06:53:42.610878 139936116377408 submission_runner.py:408] Time since start: 63409.47s, 	Step: 126119, 	{'train/accuracy': 0.8282421827316284, 'train/loss': 0.8835758566856384, 'validation/accuracy': 0.7492600083351135, 'validation/loss': 1.218082070350647, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8145524263381958, 'test/num_examples': 10000, 'score': 57596.76955342293, 'total_duration': 63409.46954703331, 'accumulated_submission_time': 57596.76955342293, 'accumulated_eval_time': 5800.6698315143585, 'accumulated_logging_time': 5.46803092956543}
I0201 06:53:42.655330 139774434195200 logging_writer.py:48] [126119] accumulated_eval_time=5800.669832, accumulated_logging_time=5.468031, accumulated_submission_time=57596.769553, global_step=126119, preemption_count=0, score=57596.769553, test/accuracy=0.627400, test/loss=1.814552, test/num_examples=10000, total_duration=63409.469547, train/accuracy=0.828242, train/loss=0.883576, validation/accuracy=0.749260, validation/loss=1.218082, validation/num_examples=50000
I0201 06:54:15.504616 139774417409792 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.7573846578598022, loss=3.398463249206543
I0201 06:55:01.140351 139774434195200 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.0363285541534424, loss=4.629043102264404
I0201 06:55:47.558693 139774417409792 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.9195449352264404, loss=4.5109405517578125
I0201 06:56:34.090295 139774434195200 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.7692784070968628, loss=2.9838147163391113
I0201 06:57:20.538211 139774417409792 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.7886840105056763, loss=4.361734390258789
I0201 06:58:06.690442 139774434195200 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.6698284149169922, loss=3.0965394973754883
I0201 06:58:52.569186 139774417409792 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.5692377090454102, loss=3.0926899909973145
I0201 06:59:39.227479 139774434195200 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.56862473487854, loss=3.8444619178771973
I0201 07:00:25.508770 139774417409792 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.7830522060394287, loss=3.1396377086639404
I0201 07:00:42.614654 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:00:54.431564 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:01:19.694647 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:01:21.297114 139936116377408 submission_runner.py:408] Time since start: 63868.16s, 	Step: 127039, 	{'train/accuracy': 0.8307812213897705, 'train/loss': 0.8714902400970459, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.2164604663848877, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.8085092306137085, 'test/num_examples': 10000, 'score': 58016.67089056969, 'total_duration': 63868.15580749512, 'accumulated_submission_time': 58016.67089056969, 'accumulated_eval_time': 5839.35227394104, 'accumulated_logging_time': 5.522529602050781}
I0201 07:01:21.333869 139774434195200 logging_writer.py:48] [127039] accumulated_eval_time=5839.352274, accumulated_logging_time=5.522530, accumulated_submission_time=58016.670891, global_step=127039, preemption_count=0, score=58016.670891, test/accuracy=0.625500, test/loss=1.808509, test/num_examples=10000, total_duration=63868.155807, train/accuracy=0.830781, train/loss=0.871490, validation/accuracy=0.747060, validation/loss=1.216460, validation/num_examples=50000
I0201 07:01:45.956656 139774417409792 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.6472729444503784, loss=3.003100872039795
I0201 07:02:30.427008 139774434195200 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.6141223907470703, loss=3.262922525405884
I0201 07:03:16.798677 139774417409792 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.7081645727157593, loss=3.1301217079162598
I0201 07:04:03.434764 139774434195200 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.682651162147522, loss=2.992859363555908
I0201 07:04:49.276229 139774417409792 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.690092921257019, loss=3.031116485595703
I0201 07:05:35.909905 139774434195200 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.0201098918914795, loss=3.09037184715271
I0201 07:06:22.042399 139774417409792 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.6755821704864502, loss=3.1999571323394775
I0201 07:07:08.282885 139774434195200 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.5929791927337646, loss=3.062467336654663
I0201 07:07:54.554228 139774417409792 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.7903975248336792, loss=4.1327996253967285
I0201 07:08:21.326825 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:08:33.983775 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:09:03.481652 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:09:05.085075 139936116377408 submission_runner.py:408] Time since start: 64331.94s, 	Step: 127959, 	{'train/accuracy': 0.8356054425239563, 'train/loss': 0.8296651244163513, 'validation/accuracy': 0.7488399744033813, 'validation/loss': 1.2022349834442139, 'validation/num_examples': 50000, 'test/accuracy': 0.6254000067710876, 'test/loss': 1.7854843139648438, 'test/num_examples': 10000, 'score': 58436.60658454895, 'total_duration': 64331.94377684593, 'accumulated_submission_time': 58436.60658454895, 'accumulated_eval_time': 5883.1105625629425, 'accumulated_logging_time': 5.568439722061157}
I0201 07:09:05.119450 139774434195200 logging_writer.py:48] [127959] accumulated_eval_time=5883.110563, accumulated_logging_time=5.568440, accumulated_submission_time=58436.606585, global_step=127959, preemption_count=0, score=58436.606585, test/accuracy=0.625400, test/loss=1.785484, test/num_examples=10000, total_duration=64331.943777, train/accuracy=0.835605, train/loss=0.829665, validation/accuracy=0.748840, validation/loss=1.202235, validation/num_examples=50000
I0201 07:09:21.793482 139774417409792 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.7575668096542358, loss=2.9949097633361816
I0201 07:10:05.288369 139774434195200 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.7612677812576294, loss=3.4210731983184814
I0201 07:10:51.074996 139774417409792 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.815877079963684, loss=3.1046600341796875
I0201 07:11:37.508326 139774434195200 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.7860037088394165, loss=3.0895581245422363
I0201 07:12:23.692205 139774417409792 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.8971410989761353, loss=4.538162708282471
I0201 07:13:09.951214 139774434195200 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.8066017627716064, loss=3.962709426879883
I0201 07:13:56.038392 139774417409792 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.6055278778076172, loss=3.6375157833099365
I0201 07:14:42.184931 139774434195200 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.6865755319595337, loss=3.855686664581299
I0201 07:15:28.477357 139774417409792 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.8339884281158447, loss=2.9977991580963135
I0201 07:16:05.454858 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:16:17.385494 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:16:44.892747 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:16:46.501283 139936116377408 submission_runner.py:408] Time since start: 64793.36s, 	Step: 128881, 	{'train/accuracy': 0.8422460556030273, 'train/loss': 0.8313546776771545, 'validation/accuracy': 0.752020001411438, 'validation/loss': 1.212985873222351, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.7977409362792969, 'test/num_examples': 10000, 'score': 58856.884100198746, 'total_duration': 64793.35998272896, 'accumulated_submission_time': 58856.884100198746, 'accumulated_eval_time': 5924.156987428665, 'accumulated_logging_time': 5.612093925476074}
I0201 07:16:46.536773 139774434195200 logging_writer.py:48] [128881] accumulated_eval_time=5924.156987, accumulated_logging_time=5.612094, accumulated_submission_time=58856.884100, global_step=128881, preemption_count=0, score=58856.884100, test/accuracy=0.629000, test/loss=1.797741, test/num_examples=10000, total_duration=64793.359983, train/accuracy=0.842246, train/loss=0.831355, validation/accuracy=0.752020, validation/loss=1.212986, validation/num_examples=50000
I0201 07:16:54.479268 139774417409792 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.7431243658065796, loss=3.237929582595825
I0201 07:17:36.515514 139774434195200 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.6394946575164795, loss=3.8294365406036377
I0201 07:18:22.383752 139774417409792 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.8770722150802612, loss=3.720306873321533
I0201 07:19:08.725306 139774434195200 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.7659542560577393, loss=3.5200369358062744
I0201 07:19:54.590470 139774417409792 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.8875246047973633, loss=3.0591037273406982
I0201 07:20:40.880954 139774434195200 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.8218826055526733, loss=2.9939613342285156
I0201 07:21:27.278979 139774417409792 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.693269968032837, loss=3.4635679721832275
I0201 07:22:13.388658 139774434195200 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.6518954038619995, loss=3.1190245151519775
I0201 07:22:59.361496 139774417409792 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.9177953004837036, loss=4.583393573760986
I0201 07:23:45.697130 139774434195200 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.7140096426010132, loss=2.991969347000122
I0201 07:23:46.644327 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:23:58.598779 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:24:29.387903 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:24:30.997658 139936116377408 submission_runner.py:408] Time since start: 65257.86s, 	Step: 129804, 	{'train/accuracy': 0.8338280916213989, 'train/loss': 0.8379896283149719, 'validation/accuracy': 0.7499600052833557, 'validation/loss': 1.1926852464675903, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.786934494972229, 'test/num_examples': 10000, 'score': 59276.93350100517, 'total_duration': 65257.85636162758, 'accumulated_submission_time': 59276.93350100517, 'accumulated_eval_time': 5968.510313987732, 'accumulated_logging_time': 5.657716512680054}
I0201 07:24:31.034101 139774417409792 logging_writer.py:48] [129804] accumulated_eval_time=5968.510314, accumulated_logging_time=5.657717, accumulated_submission_time=59276.933501, global_step=129804, preemption_count=0, score=59276.933501, test/accuracy=0.629800, test/loss=1.786934, test/num_examples=10000, total_duration=65257.856362, train/accuracy=0.833828, train/loss=0.837990, validation/accuracy=0.749960, validation/loss=1.192685, validation/num_examples=50000
I0201 07:25:10.678813 139774434195200 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.0764260292053223, loss=4.5663251876831055
I0201 07:25:56.722777 139774417409792 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.7115724086761475, loss=3.357715129852295
I0201 07:26:43.023848 139774434195200 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.751571536064148, loss=3.771533727645874
I0201 07:27:29.545859 139774417409792 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.8559024333953857, loss=4.087070465087891
I0201 07:28:15.707637 139774434195200 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.6930515766143799, loss=3.004112958908081
I0201 07:29:01.778227 139774417409792 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.7864872217178345, loss=4.183053493499756
I0201 07:29:48.095298 139774434195200 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.7745168209075928, loss=2.9794065952301025
I0201 07:30:34.508608 139774417409792 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.9180545806884766, loss=4.178173065185547
I0201 07:31:20.400789 139774434195200 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.8110771179199219, loss=3.0088675022125244
I0201 07:31:31.045923 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:31:43.151191 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:32:11.925160 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:32:13.529523 139936116377408 submission_runner.py:408] Time since start: 65720.39s, 	Step: 130725, 	{'train/accuracy': 0.8382812142372131, 'train/loss': 0.8320725560188293, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.1947848796844482, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.7743438482284546, 'test/num_examples': 10000, 'score': 59696.888902425766, 'total_duration': 65720.388225317, 'accumulated_submission_time': 59696.888902425766, 'accumulated_eval_time': 6010.993898153305, 'accumulated_logging_time': 5.702925443649292}
I0201 07:32:13.566593 139774417409792 logging_writer.py:48] [130725] accumulated_eval_time=6010.993898, accumulated_logging_time=5.702925, accumulated_submission_time=59696.888902, global_step=130725, preemption_count=0, score=59696.888902, test/accuracy=0.631200, test/loss=1.774344, test/num_examples=10000, total_duration=65720.388225, train/accuracy=0.838281, train/loss=0.832073, validation/accuracy=0.750320, validation/loss=1.194785, validation/num_examples=50000
I0201 07:32:43.756675 139774434195200 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.7613238096237183, loss=3.759598731994629
I0201 07:33:29.173178 139774417409792 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.8930615186691284, loss=4.238524436950684
I0201 07:34:15.463359 139774434195200 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.687275767326355, loss=2.947251319885254
I0201 07:35:01.770370 139774417409792 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.849313735961914, loss=3.9351511001586914
I0201 07:35:47.827962 139774434195200 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.7717183828353882, loss=3.090397596359253
I0201 07:36:34.088980 139774417409792 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.9066368341445923, loss=4.345759868621826
I0201 07:37:20.565476 139774434195200 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.872559666633606, loss=3.969670057296753
I0201 07:38:06.697879 139774417409792 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.2326924800872803, loss=4.526541709899902
I0201 07:38:52.589327 139774434195200 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.734918236732483, loss=3.21421480178833
I0201 07:39:13.735991 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:39:25.666962 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:39:57.221098 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:39:58.830372 139936116377408 submission_runner.py:408] Time since start: 66185.69s, 	Step: 131647, 	{'train/accuracy': 0.8476171493530273, 'train/loss': 0.8037877678871155, 'validation/accuracy': 0.7534599900245667, 'validation/loss': 1.1936339139938354, 'validation/num_examples': 50000, 'test/accuracy': 0.6288000345230103, 'test/loss': 1.7920557260513306, 'test/num_examples': 10000, 'score': 60117.00132703781, 'total_duration': 66185.6890695095, 'accumulated_submission_time': 60117.00132703781, 'accumulated_eval_time': 6056.08829331398, 'accumulated_logging_time': 5.74944281578064}
I0201 07:39:58.869808 139774417409792 logging_writer.py:48] [131647] accumulated_eval_time=6056.088293, accumulated_logging_time=5.749443, accumulated_submission_time=60117.001327, global_step=131647, preemption_count=0, score=60117.001327, test/accuracy=0.628800, test/loss=1.792056, test/num_examples=10000, total_duration=66185.689070, train/accuracy=0.847617, train/loss=0.803788, validation/accuracy=0.753460, validation/loss=1.193634, validation/num_examples=50000
I0201 07:40:20.313020 139774434195200 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.778292179107666, loss=2.983553647994995
I0201 07:41:04.467513 139774417409792 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.8691543340682983, loss=3.1138365268707275
I0201 07:41:50.989308 139774434195200 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.743306040763855, loss=3.0083212852478027
I0201 07:42:37.520105 139774417409792 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.7860828638076782, loss=3.1095759868621826
I0201 07:43:23.774705 139774434195200 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.8215285539627075, loss=3.0694377422332764
I0201 07:44:09.798940 139774417409792 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.928036093711853, loss=4.3694539070129395
I0201 07:44:55.905973 139774434195200 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.8110644817352295, loss=3.0255889892578125
I0201 07:45:42.262184 139774417409792 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.856105923652649, loss=3.1152596473693848
I0201 07:46:28.751457 139774434195200 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.9177196025848389, loss=3.1829581260681152
I0201 07:46:59.189676 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:47:11.321411 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:47:41.491682 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:47:43.095905 139936116377408 submission_runner.py:408] Time since start: 66649.95s, 	Step: 132568, 	{'train/accuracy': 0.8365820050239563, 'train/loss': 0.8467892408370972, 'validation/accuracy': 0.7517600059509277, 'validation/loss': 1.205349326133728, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.788927674293518, 'test/num_examples': 10000, 'score': 60537.263917684555, 'total_duration': 66649.95460033417, 'accumulated_submission_time': 60537.263917684555, 'accumulated_eval_time': 6099.994526147842, 'accumulated_logging_time': 5.798482418060303}
I0201 07:47:43.133287 139774417409792 logging_writer.py:48] [132568] accumulated_eval_time=6099.994526, accumulated_logging_time=5.798482, accumulated_submission_time=60537.263918, global_step=132568, preemption_count=0, score=60537.263918, test/accuracy=0.635500, test/loss=1.788928, test/num_examples=10000, total_duration=66649.954600, train/accuracy=0.836582, train/loss=0.846789, validation/accuracy=0.751760, validation/loss=1.205349, validation/num_examples=50000
I0201 07:47:56.252362 139774434195200 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.7108558416366577, loss=3.026306390762329
I0201 07:48:39.148244 139774417409792 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.9750492572784424, loss=2.9644696712493896
I0201 07:49:25.200054 139774434195200 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.821298360824585, loss=3.826482057571411
I0201 07:50:11.824622 139774417409792 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.9925843477249146, loss=3.080669641494751
I0201 07:50:57.927014 139774434195200 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.7418417930603027, loss=3.2437329292297363
I0201 07:51:44.252071 139774417409792 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.858099102973938, loss=2.9796056747436523
I0201 07:52:30.586030 139774434195200 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.8328642845153809, loss=2.963883638381958
I0201 07:53:16.548269 139774417409792 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.864438772201538, loss=3.7894463539123535
I0201 07:54:02.620910 139774434195200 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.8049914836883545, loss=3.075502634048462
I0201 07:54:43.417922 139936116377408 spec.py:321] Evaluating on the training split.
I0201 07:54:55.376498 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 07:55:20.800710 139936116377408 spec.py:349] Evaluating on the test split.
I0201 07:55:22.401201 139936116377408 submission_runner.py:408] Time since start: 67109.26s, 	Step: 133490, 	{'train/accuracy': 0.8384960889816284, 'train/loss': 0.835568904876709, 'validation/accuracy': 0.7537399530410767, 'validation/loss': 1.1961098909378052, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.7881088256835938, 'test/num_examples': 10000, 'score': 60957.490884542465, 'total_duration': 67109.25989794731, 'accumulated_submission_time': 60957.490884542465, 'accumulated_eval_time': 6138.977798938751, 'accumulated_logging_time': 5.845485210418701}
I0201 07:55:22.438617 139774417409792 logging_writer.py:48] [133490] accumulated_eval_time=6138.977799, accumulated_logging_time=5.845485, accumulated_submission_time=60957.490885, global_step=133490, preemption_count=0, score=60957.490885, test/accuracy=0.633600, test/loss=1.788109, test/num_examples=10000, total_duration=67109.259898, train/accuracy=0.838496, train/loss=0.835569, validation/accuracy=0.753740, validation/loss=1.196110, validation/num_examples=50000
I0201 07:55:26.808922 139774434195200 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.96601402759552, loss=3.084461212158203
I0201 07:56:08.368460 139774417409792 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.102573871612549, loss=4.414411544799805
I0201 07:56:54.305157 139774434195200 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.812244176864624, loss=4.061293125152588
I0201 07:57:40.665323 139774417409792 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.86489999294281, loss=3.032589912414551
I0201 07:58:27.220848 139774434195200 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.8314433097839355, loss=3.1033637523651123
I0201 07:59:13.519044 139774417409792 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.6902662515640259, loss=3.2825253009796143
I0201 07:59:59.967612 139774434195200 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.7113198041915894, loss=3.383934497833252
I0201 08:00:45.774432 139774417409792 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.9323914051055908, loss=3.617457151412964
I0201 08:01:31.933966 139774434195200 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.1615219116210938, loss=4.4677581787109375
I0201 08:02:18.429358 139774417409792 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.6386467218399048, loss=3.257506847381592
I0201 08:02:22.695964 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:02:34.663330 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:03:07.447801 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:03:09.062179 139936116377408 submission_runner.py:408] Time since start: 67575.92s, 	Step: 134411, 	{'train/accuracy': 0.8448827862739563, 'train/loss': 0.7935135364532471, 'validation/accuracy': 0.7524799704551697, 'validation/loss': 1.1886016130447388, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.7869489192962646, 'test/num_examples': 10000, 'score': 61377.691182136536, 'total_duration': 67575.92088413239, 'accumulated_submission_time': 61377.691182136536, 'accumulated_eval_time': 6185.34400844574, 'accumulated_logging_time': 5.892377853393555}
I0201 08:03:09.097128 139774434195200 logging_writer.py:48] [134411] accumulated_eval_time=6185.344008, accumulated_logging_time=5.892378, accumulated_submission_time=61377.691182, global_step=134411, preemption_count=0, score=61377.691182, test/accuracy=0.633600, test/loss=1.786949, test/num_examples=10000, total_duration=67575.920884, train/accuracy=0.844883, train/loss=0.793514, validation/accuracy=0.752480, validation/loss=1.188602, validation/num_examples=50000
I0201 08:03:45.652154 139774417409792 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.699978232383728, loss=3.1475701332092285
I0201 08:04:31.351831 139774434195200 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.734912633895874, loss=2.9554200172424316
I0201 08:05:18.003701 139774417409792 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.7573866844177246, loss=3.4938323497772217
I0201 08:06:04.615741 139774434195200 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.9028092622756958, loss=3.657813549041748
I0201 08:06:50.725756 139774417409792 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.7965089082717896, loss=3.1073408126831055
I0201 08:07:37.265260 139774434195200 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.6504969596862793, loss=3.5312936305999756
I0201 08:08:23.506261 139774417409792 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.8964003324508667, loss=2.9872686862945557
I0201 08:09:10.244065 139774434195200 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.7666881084442139, loss=2.9220402240753174
I0201 08:09:56.485665 139774417409792 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.7697820663452148, loss=3.06794810295105
I0201 08:10:09.329802 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:10:21.197530 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:10:50.532438 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:10:52.130545 139936116377408 submission_runner.py:408] Time since start: 68038.99s, 	Step: 135329, 	{'train/accuracy': 0.8392773270606995, 'train/loss': 0.8438341617584229, 'validation/accuracy': 0.7569599747657776, 'validation/loss': 1.196811318397522, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.7893301248550415, 'test/num_examples': 10000, 'score': 61797.867267131805, 'total_duration': 68038.98924589157, 'accumulated_submission_time': 61797.867267131805, 'accumulated_eval_time': 6228.144732713699, 'accumulated_logging_time': 5.9372031688690186}
I0201 08:10:52.165823 139774434195200 logging_writer.py:48] [135329] accumulated_eval_time=6228.144733, accumulated_logging_time=5.937203, accumulated_submission_time=61797.867267, global_step=135329, preemption_count=0, score=61797.867267, test/accuracy=0.637300, test/loss=1.789330, test/num_examples=10000, total_duration=68038.989246, train/accuracy=0.839277, train/loss=0.843834, validation/accuracy=0.756960, validation/loss=1.196811, validation/num_examples=50000
I0201 08:11:20.787189 139774417409792 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.8046011924743652, loss=2.9776999950408936
I0201 08:12:06.091628 139774434195200 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.8659791946411133, loss=3.0101914405822754
I0201 08:12:52.219856 139774417409792 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.7760154008865356, loss=3.0451831817626953
I0201 08:13:38.599061 139774434195200 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.8639813661575317, loss=3.2390711307525635
I0201 08:14:24.569827 139774417409792 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.718717336654663, loss=3.100006341934204
I0201 08:15:10.491197 139774434195200 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.8166489601135254, loss=3.1896493434906006
I0201 08:15:56.496931 139774417409792 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.8195985555648804, loss=3.412827491760254
I0201 08:16:42.584578 139774434195200 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.655007004737854, loss=3.348137378692627
I0201 08:17:29.051639 139774417409792 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.7844901084899902, loss=3.0199804306030273
I0201 08:17:52.369722 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:18:04.794002 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:18:33.107088 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:18:34.725975 139936116377408 submission_runner.py:408] Time since start: 68501.58s, 	Step: 136252, 	{'train/accuracy': 0.84193354845047, 'train/loss': 0.8302668333053589, 'validation/accuracy': 0.758080005645752, 'validation/loss': 1.1913862228393555, 'validation/num_examples': 50000, 'test/accuracy': 0.6366000175476074, 'test/loss': 1.7862942218780518, 'test/num_examples': 10000, 'score': 62218.01303982735, 'total_duration': 68501.58464884758, 'accumulated_submission_time': 62218.01303982735, 'accumulated_eval_time': 6270.500958204269, 'accumulated_logging_time': 5.982234716415405}
I0201 08:18:34.770848 139774434195200 logging_writer.py:48] [136252] accumulated_eval_time=6270.500958, accumulated_logging_time=5.982235, accumulated_submission_time=62218.013040, global_step=136252, preemption_count=0, score=62218.013040, test/accuracy=0.636600, test/loss=1.786294, test/num_examples=10000, total_duration=68501.584649, train/accuracy=0.841934, train/loss=0.830267, validation/accuracy=0.758080, validation/loss=1.191386, validation/num_examples=50000
I0201 08:18:54.221638 139774417409792 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.814106822013855, loss=3.751332998275757
I0201 08:19:38.284122 139774434195200 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.8450250625610352, loss=2.9839532375335693
I0201 08:20:24.544597 139774417409792 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.974914789199829, loss=3.0602307319641113
I0201 08:21:11.120215 139774434195200 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.8163174390792847, loss=2.976958751678467
I0201 08:21:57.422000 139774417409792 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.101813316345215, loss=2.9544167518615723
I0201 08:22:43.921274 139774434195200 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.8975478410720825, loss=3.053849458694458
I0201 08:23:30.373139 139774417409792 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.9730710983276367, loss=2.9970908164978027
I0201 08:24:16.694822 139774434195200 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.8545951843261719, loss=2.9086339473724365
I0201 08:25:03.028380 139774417409792 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.8091235160827637, loss=4.086055755615234
I0201 08:25:34.804673 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:25:46.707010 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:26:17.014623 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:26:18.624343 139936116377408 submission_runner.py:408] Time since start: 68965.48s, 	Step: 137171, 	{'train/accuracy': 0.8501171469688416, 'train/loss': 0.7863731980323792, 'validation/accuracy': 0.7578799724578857, 'validation/loss': 1.1653896570205688, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.7534481287002563, 'test/num_examples': 10000, 'score': 62637.987129449844, 'total_duration': 68965.4830365181, 'accumulated_submission_time': 62637.987129449844, 'accumulated_eval_time': 6314.320621013641, 'accumulated_logging_time': 6.038216590881348}
I0201 08:26:18.660705 139774434195200 logging_writer.py:48] [137171] accumulated_eval_time=6314.320621, accumulated_logging_time=6.038217, accumulated_submission_time=62637.987129, global_step=137171, preemption_count=0, score=62637.987129, test/accuracy=0.638100, test/loss=1.753448, test/num_examples=10000, total_duration=68965.483037, train/accuracy=0.850117, train/loss=0.786373, validation/accuracy=0.757880, validation/loss=1.165390, validation/num_examples=50000
I0201 08:26:30.582608 139774417409792 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.8066818714141846, loss=3.050126075744629
I0201 08:27:13.322100 139774434195200 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.8305590152740479, loss=2.9727487564086914
I0201 08:27:59.300406 139774417409792 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.9204020500183105, loss=3.0162830352783203
I0201 08:28:45.870204 139774434195200 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.017954111099243, loss=4.313198089599609
I0201 08:29:32.019350 139774417409792 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.916678786277771, loss=3.0436198711395264
I0201 08:30:18.492146 139774434195200 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.775914192199707, loss=2.8932456970214844
I0201 08:31:04.479644 139774417409792 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.888716220855713, loss=2.9458961486816406
I0201 08:31:50.704839 139774434195200 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.878161072731018, loss=2.9506492614746094
I0201 08:32:37.105325 139774417409792 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.9564870595932007, loss=2.9591903686523438
I0201 08:33:19.001089 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:33:30.985224 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:34:00.227411 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:34:01.833258 139936116377408 submission_runner.py:408] Time since start: 69428.69s, 	Step: 138092, 	{'train/accuracy': 0.8501366972923279, 'train/loss': 0.8256229758262634, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.1994614601135254, 'validation/num_examples': 50000, 'test/accuracy': 0.6454000473022461, 'test/loss': 1.7836123704910278, 'test/num_examples': 10000, 'score': 63058.27062392235, 'total_duration': 69428.69195985794, 'accumulated_submission_time': 63058.27062392235, 'accumulated_eval_time': 6357.152781248093, 'accumulated_logging_time': 6.084154844284058}
I0201 08:34:01.870680 139774434195200 logging_writer.py:48] [138092] accumulated_eval_time=6357.152781, accumulated_logging_time=6.084155, accumulated_submission_time=63058.270624, global_step=138092, preemption_count=0, score=63058.270624, test/accuracy=0.645400, test/loss=1.783612, test/num_examples=10000, total_duration=69428.691960, train/accuracy=0.850137, train/loss=0.825623, validation/accuracy=0.759880, validation/loss=1.199461, validation/num_examples=50000
I0201 08:34:05.446109 139774417409792 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.8359487056732178, loss=3.7368972301483154
I0201 08:34:46.853056 139774434195200 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.9348522424697876, loss=2.983790874481201
I0201 08:35:32.802207 139774417409792 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.9274650812149048, loss=2.9368443489074707
I0201 08:36:19.823599 139774434195200 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.2149131298065186, loss=4.2994890213012695
I0201 08:37:05.950606 139774417409792 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.899669885635376, loss=2.99362850189209
I0201 08:37:51.906254 139774434195200 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.0345098972320557, loss=2.995750665664673
I0201 08:38:38.380990 139774417409792 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.8793641328811646, loss=2.9093093872070312
I0201 08:39:24.756778 139774434195200 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.8008958101272583, loss=2.9399821758270264
I0201 08:40:11.140166 139774417409792 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.974774718284607, loss=2.997800588607788
I0201 08:40:57.499448 139774434195200 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.1623146533966064, loss=3.929669141769409
I0201 08:41:01.941419 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:41:13.881660 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:41:46.501561 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:41:48.111359 139936116377408 submission_runner.py:408] Time since start: 69894.97s, 	Step: 139011, 	{'train/accuracy': 0.8479101657867432, 'train/loss': 0.7939903736114502, 'validation/accuracy': 0.7591599822044373, 'validation/loss': 1.1697139739990234, 'validation/num_examples': 50000, 'test/accuracy': 0.6362000107765198, 'test/loss': 1.7635866403579712, 'test/num_examples': 10000, 'score': 63478.28254342079, 'total_duration': 69894.97005820274, 'accumulated_submission_time': 63478.28254342079, 'accumulated_eval_time': 6403.32272028923, 'accumulated_logging_time': 6.133307695388794}
I0201 08:41:48.150619 139774417409792 logging_writer.py:48] [139011] accumulated_eval_time=6403.322720, accumulated_logging_time=6.133308, accumulated_submission_time=63478.282543, global_step=139011, preemption_count=0, score=63478.282543, test/accuracy=0.636200, test/loss=1.763587, test/num_examples=10000, total_duration=69894.970058, train/accuracy=0.847910, train/loss=0.793990, validation/accuracy=0.759160, validation/loss=1.169714, validation/num_examples=50000
I0201 08:42:24.592016 139774434195200 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.8429465293884277, loss=2.974730968475342
I0201 08:43:10.587431 139774417409792 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.0015721321105957, loss=3.010093927383423
I0201 08:43:56.706491 139774434195200 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.833942174911499, loss=3.1564419269561768
I0201 08:44:43.454706 139774417409792 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.9711990356445312, loss=4.02333927154541
I0201 08:45:29.500861 139774434195200 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.083256244659424, loss=4.306247234344482
I0201 08:46:15.753692 139774417409792 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.0286309719085693, loss=2.9707045555114746
I0201 08:47:02.050049 139774434195200 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.8975058794021606, loss=2.901395082473755
I0201 08:47:48.251764 139774417409792 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.944733738899231, loss=3.8801958560943604
I0201 08:48:34.665288 139774434195200 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.9962613582611084, loss=3.2706263065338135
I0201 08:48:48.244881 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:49:00.197479 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:49:31.398301 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:49:33.010736 139936116377408 submission_runner.py:408] Time since start: 70359.87s, 	Step: 139931, 	{'train/accuracy': 0.8531054258346558, 'train/loss': 0.7984217405319214, 'validation/accuracy': 0.7590999603271484, 'validation/loss': 1.1816964149475098, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.7529109716415405, 'test/num_examples': 10000, 'score': 63898.31989693642, 'total_duration': 70359.86943912506, 'accumulated_submission_time': 63898.31989693642, 'accumulated_eval_time': 6448.088568687439, 'accumulated_logging_time': 6.181832790374756}
I0201 08:49:33.045924 139774417409792 logging_writer.py:48] [139931] accumulated_eval_time=6448.088569, accumulated_logging_time=6.181833, accumulated_submission_time=63898.319897, global_step=139931, preemption_count=0, score=63898.319897, test/accuracy=0.640300, test/loss=1.752911, test/num_examples=10000, total_duration=70359.869439, train/accuracy=0.853105, train/loss=0.798422, validation/accuracy=0.759100, validation/loss=1.181696, validation/num_examples=50000
I0201 08:50:00.860631 139774434195200 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.8952821493148804, loss=3.7374844551086426
I0201 08:50:46.233577 139774417409792 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.8968018293380737, loss=2.796530246734619
I0201 08:51:32.908816 139774434195200 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.781826376914978, loss=2.9479494094848633
I0201 08:52:19.381935 139774417409792 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.7407190799713135, loss=3.006197690963745
I0201 08:53:05.475166 139774434195200 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.0844876766204834, loss=4.134415626525879
I0201 08:53:51.690539 139774417409792 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.7911661863327026, loss=3.560368061065674
I0201 08:54:38.028069 139774434195200 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.0046327114105225, loss=3.027528762817383
I0201 08:55:24.363336 139774417409792 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.962295413017273, loss=3.011831760406494
I0201 08:56:10.459678 139774434195200 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.8751919269561768, loss=2.9935784339904785
I0201 08:56:33.088541 139936116377408 spec.py:321] Evaluating on the training split.
I0201 08:56:44.959928 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 08:57:17.599952 139936116377408 spec.py:349] Evaluating on the test split.
I0201 08:57:19.210058 139936116377408 submission_runner.py:408] Time since start: 70826.07s, 	Step: 140851, 	{'train/accuracy': 0.8634960651397705, 'train/loss': 0.751483142375946, 'validation/accuracy': 0.7616999745368958, 'validation/loss': 1.166806936264038, 'validation/num_examples': 50000, 'test/accuracy': 0.643500030040741, 'test/loss': 1.751853346824646, 'test/num_examples': 10000, 'score': 64318.303194999695, 'total_duration': 70826.06875395775, 'accumulated_submission_time': 64318.303194999695, 'accumulated_eval_time': 6494.210072994232, 'accumulated_logging_time': 6.228420257568359}
I0201 08:57:19.253282 139774417409792 logging_writer.py:48] [140851] accumulated_eval_time=6494.210073, accumulated_logging_time=6.228420, accumulated_submission_time=64318.303195, global_step=140851, preemption_count=0, score=64318.303195, test/accuracy=0.643500, test/loss=1.751853, test/num_examples=10000, total_duration=70826.068754, train/accuracy=0.863496, train/loss=0.751483, validation/accuracy=0.761700, validation/loss=1.166807, validation/num_examples=50000
I0201 08:57:39.093912 139774434195200 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.08756685256958, loss=2.9683589935302734
I0201 08:58:22.915122 139774417409792 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.139410972595215, loss=4.282968521118164
I0201 08:59:09.250010 139774434195200 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.9737417697906494, loss=3.2704665660858154
I0201 08:59:55.649383 139774417409792 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.0779378414154053, loss=2.9816884994506836
I0201 09:00:41.761933 139774434195200 logging_writer.py:48] [141300] global_step=141300, grad_norm=1.9116188287734985, loss=2.904648780822754
I0201 09:01:28.331527 139774417409792 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.8175593614578247, loss=2.9939472675323486
I0201 09:02:15.010407 139774434195200 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.1441233158111572, loss=4.190441608428955
I0201 09:03:01.010353 139774417409792 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.250170946121216, loss=2.924410104751587
I0201 09:03:47.518894 139774434195200 logging_writer.py:48] [141700] global_step=141700, grad_norm=1.9978141784667969, loss=3.0003490447998047
I0201 09:04:19.211734 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:04:31.174504 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:04:59.845524 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:05:01.447952 139936116377408 submission_runner.py:408] Time since start: 71288.31s, 	Step: 141770, 	{'train/accuracy': 0.849609375, 'train/loss': 0.7915339469909668, 'validation/accuracy': 0.7606199979782104, 'validation/loss': 1.1642590761184692, 'validation/num_examples': 50000, 'test/accuracy': 0.6408000588417053, 'test/loss': 1.7490770816802979, 'test/num_examples': 10000, 'score': 64738.19510626793, 'total_duration': 71288.30664849281, 'accumulated_submission_time': 64738.19510626793, 'accumulated_eval_time': 6536.446292638779, 'accumulated_logging_time': 6.2816667556762695}
I0201 09:05:01.487460 139774417409792 logging_writer.py:48] [141770] accumulated_eval_time=6536.446293, accumulated_logging_time=6.281667, accumulated_submission_time=64738.195106, global_step=141770, preemption_count=0, score=64738.195106, test/accuracy=0.640800, test/loss=1.749077, test/num_examples=10000, total_duration=71288.306648, train/accuracy=0.849609, train/loss=0.791534, validation/accuracy=0.760620, validation/loss=1.164259, validation/num_examples=50000
I0201 09:05:13.813739 139774434195200 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.9571224451065063, loss=3.5429582595825195
I0201 09:05:56.617364 139774417409792 logging_writer.py:48] [141900] global_step=141900, grad_norm=1.9310904741287231, loss=2.957589626312256
I0201 09:06:42.836781 139774434195200 logging_writer.py:48] [142000] global_step=142000, grad_norm=1.9499627351760864, loss=3.97891902923584
I0201 09:07:29.738985 139774417409792 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.9699971675872803, loss=3.9389798641204834
I0201 09:08:16.105708 139774434195200 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.1176083087921143, loss=2.9701626300811768
I0201 09:09:02.211289 139774417409792 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.04313325881958, loss=2.9746594429016113
I0201 09:09:48.135738 139774434195200 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.2063660621643066, loss=4.299685478210449
I0201 09:10:34.585750 139774417409792 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.0544168949127197, loss=3.8131117820739746
I0201 09:11:20.749216 139774434195200 logging_writer.py:48] [142600] global_step=142600, grad_norm=1.9493104219436646, loss=2.9412455558776855
I0201 09:12:01.469424 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:12:13.730554 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:12:47.458334 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:12:49.065569 139936116377408 submission_runner.py:408] Time since start: 71755.92s, 	Step: 142689, 	{'train/accuracy': 0.8543359041213989, 'train/loss': 0.7828488945960999, 'validation/accuracy': 0.7647799849510193, 'validation/loss': 1.1637057065963745, 'validation/num_examples': 50000, 'test/accuracy': 0.6442000269889832, 'test/loss': 1.7491698265075684, 'test/num_examples': 10000, 'score': 65158.11840724945, 'total_duration': 71755.92424988747, 'accumulated_submission_time': 65158.11840724945, 'accumulated_eval_time': 6584.042410612106, 'accumulated_logging_time': 6.332995891571045}
I0201 09:12:49.108006 139774417409792 logging_writer.py:48] [142689] accumulated_eval_time=6584.042411, accumulated_logging_time=6.332996, accumulated_submission_time=65158.118407, global_step=142689, preemption_count=0, score=65158.118407, test/accuracy=0.644200, test/loss=1.749170, test/num_examples=10000, total_duration=71755.924250, train/accuracy=0.854336, train/loss=0.782849, validation/accuracy=0.764780, validation/loss=1.163706, validation/num_examples=50000
I0201 09:12:53.876492 139774434195200 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.0424745082855225, loss=2.918088674545288
I0201 09:13:35.681042 139774417409792 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.279825210571289, loss=3.9181931018829346
I0201 09:14:21.676120 139774434195200 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.1177051067352295, loss=2.981719493865967
I0201 09:15:08.042397 139774417409792 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.2978663444519043, loss=4.194565773010254
I0201 09:15:54.318801 139774434195200 logging_writer.py:48] [143100] global_step=143100, grad_norm=1.9749317169189453, loss=3.1245365142822266
I0201 09:16:40.464312 139774417409792 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.0390565395355225, loss=4.081882476806641
I0201 09:17:26.744159 139774434195200 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.9121054410934448, loss=3.132211446762085
I0201 09:18:12.874238 139774417409792 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.9656188488006592, loss=2.9598021507263184
I0201 09:18:59.192842 139774434195200 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.046633005142212, loss=2.9261293411254883
I0201 09:19:45.289691 139774417409792 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.867514967918396, loss=3.421069860458374
I0201 09:19:49.509124 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:20:01.380657 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:20:33.345819 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:20:34.951290 139936116377408 submission_runner.py:408] Time since start: 72221.81s, 	Step: 143611, 	{'train/accuracy': 0.86376953125, 'train/loss': 0.744042694568634, 'validation/accuracy': 0.7639600038528442, 'validation/loss': 1.1600041389465332, 'validation/num_examples': 50000, 'test/accuracy': 0.643500030040741, 'test/loss': 1.7458293437957764, 'test/num_examples': 10000, 'score': 65578.4606962204, 'total_duration': 72221.8099834919, 'accumulated_submission_time': 65578.4606962204, 'accumulated_eval_time': 6629.484586715698, 'accumulated_logging_time': 6.386404514312744}
I0201 09:20:34.989768 139774434195200 logging_writer.py:48] [143611] accumulated_eval_time=6629.484587, accumulated_logging_time=6.386405, accumulated_submission_time=65578.460696, global_step=143611, preemption_count=0, score=65578.460696, test/accuracy=0.643500, test/loss=1.745829, test/num_examples=10000, total_duration=72221.809983, train/accuracy=0.863770, train/loss=0.744043, validation/accuracy=0.763960, validation/loss=1.160004, validation/num_examples=50000
I0201 09:21:11.688045 139774417409792 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.0280849933624268, loss=3.130462408065796
I0201 09:21:57.608298 139774434195200 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.118039608001709, loss=3.0643248558044434
I0201 09:22:44.047816 139774417409792 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.9773972034454346, loss=2.902998924255371
I0201 09:23:30.595762 139774434195200 logging_writer.py:48] [144000] global_step=144000, grad_norm=1.953199863433838, loss=2.974940299987793
I0201 09:24:16.490012 139774417409792 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.1964666843414307, loss=3.9958250522613525
I0201 09:25:02.927156 139774434195200 logging_writer.py:48] [144200] global_step=144200, grad_norm=1.9940210580825806, loss=3.3499577045440674
I0201 09:25:48.995166 139774417409792 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.049597978591919, loss=3.210012674331665
I0201 09:26:34.958466 139774434195200 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.045518398284912, loss=2.8713490962982178
I0201 09:27:21.145997 139774417409792 logging_writer.py:48] [144500] global_step=144500, grad_norm=1.961485505104065, loss=2.984156847000122
I0201 09:27:35.204475 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:27:47.108181 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:28:18.896142 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:28:20.517517 139936116377408 submission_runner.py:408] Time since start: 72687.38s, 	Step: 144532, 	{'train/accuracy': 0.8549999594688416, 'train/loss': 0.7654373645782471, 'validation/accuracy': 0.7643399834632874, 'validation/loss': 1.1485599279403687, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.74350905418396, 'test/num_examples': 10000, 'score': 65998.61735081673, 'total_duration': 72687.37620162964, 'accumulated_submission_time': 65998.61735081673, 'accumulated_eval_time': 6674.7976150512695, 'accumulated_logging_time': 6.435186386108398}
I0201 09:28:20.559995 139774434195200 logging_writer.py:48] [144532] accumulated_eval_time=6674.797615, accumulated_logging_time=6.435186, accumulated_submission_time=65998.617351, global_step=144532, preemption_count=0, score=65998.617351, test/accuracy=0.642900, test/loss=1.743509, test/num_examples=10000, total_duration=72687.376202, train/accuracy=0.855000, train/loss=0.765437, validation/accuracy=0.764340, validation/loss=1.148560, validation/num_examples=50000
I0201 09:28:48.028999 139774417409792 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.159910202026367, loss=3.7855706214904785
I0201 09:29:33.111930 139774434195200 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.0035393238067627, loss=3.3809654712677
I0201 09:30:19.354013 139774417409792 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.0697619915008545, loss=3.2016541957855225
I0201 09:31:06.004998 139774434195200 logging_writer.py:48] [144900] global_step=144900, grad_norm=1.980379343032837, loss=3.5990331172943115
I0201 09:31:51.954626 139774417409792 logging_writer.py:48] [145000] global_step=145000, grad_norm=1.8623179197311401, loss=3.2639193534851074
I0201 09:32:38.281940 139774434195200 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.9438986778259277, loss=3.1765995025634766
I0201 09:33:24.790739 139774417409792 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.0377161502838135, loss=3.0173749923706055
I0201 09:34:10.881750 139774434195200 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.164034843444824, loss=3.146202802658081
I0201 09:34:57.206542 139774417409792 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.1604080200195312, loss=4.077530860900879
I0201 09:35:20.837359 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:35:32.830619 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:36:03.571660 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:36:05.178731 139936116377408 submission_runner.py:408] Time since start: 73152.04s, 	Step: 145453, 	{'train/accuracy': 0.8571484088897705, 'train/loss': 0.7539299130439758, 'validation/accuracy': 0.7656399607658386, 'validation/loss': 1.1399657726287842, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7225067615509033, 'test/num_examples': 10000, 'score': 66418.83790254593, 'total_duration': 73152.03741383553, 'accumulated_submission_time': 66418.83790254593, 'accumulated_eval_time': 6719.138984918594, 'accumulated_logging_time': 6.487768888473511}
I0201 09:36:05.221186 139774434195200 logging_writer.py:48] [145453] accumulated_eval_time=6719.138985, accumulated_logging_time=6.487769, accumulated_submission_time=66418.837903, global_step=145453, preemption_count=0, score=66418.837903, test/accuracy=0.645200, test/loss=1.722507, test/num_examples=10000, total_duration=73152.037414, train/accuracy=0.857148, train/loss=0.753930, validation/accuracy=0.765640, validation/loss=1.139966, validation/num_examples=50000
I0201 09:36:24.276152 139774417409792 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.9799799919128418, loss=3.218355417251587
I0201 09:37:08.394555 139774434195200 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.7017877101898193, loss=4.462850570678711
I0201 09:37:54.279072 139774417409792 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.083306074142456, loss=2.961153507232666
I0201 09:38:40.475595 139774434195200 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.005429267883301, loss=2.8799362182617188
I0201 09:39:26.696112 139774417409792 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.004086971282959, loss=2.9480209350585938
I0201 09:40:12.792697 139774434195200 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.1399271488189697, loss=2.9278624057769775
I0201 09:40:58.931378 139774417409792 logging_writer.py:48] [146100] global_step=146100, grad_norm=1.9869768619537354, loss=3.694253444671631
I0201 09:41:45.425386 139774434195200 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.2777066230773926, loss=4.355591773986816
I0201 09:42:31.636106 139774417409792 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.9725261926651, loss=2.826667308807373
I0201 09:43:05.254067 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:43:17.348979 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:43:49.413109 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:43:51.015514 139936116377408 submission_runner.py:408] Time since start: 73617.87s, 	Step: 146374, 	{'train/accuracy': 0.8620703220367432, 'train/loss': 0.7350453734397888, 'validation/accuracy': 0.7666599750518799, 'validation/loss': 1.1349040269851685, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.715340256690979, 'test/num_examples': 10000, 'score': 66838.8134508133, 'total_duration': 73617.8742146492, 'accumulated_submission_time': 66838.8134508133, 'accumulated_eval_time': 6764.900428771973, 'accumulated_logging_time': 6.540493726730347}
I0201 09:43:51.054275 139774434195200 logging_writer.py:48] [146374] accumulated_eval_time=6764.900429, accumulated_logging_time=6.540494, accumulated_submission_time=66838.813451, global_step=146374, preemption_count=0, score=66838.813451, test/accuracy=0.645700, test/loss=1.715340, test/num_examples=10000, total_duration=73617.874215, train/accuracy=0.862070, train/loss=0.735045, validation/accuracy=0.766660, validation/loss=1.134904, validation/num_examples=50000
I0201 09:44:01.785317 139774417409792 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.446321725845337, loss=4.402144432067871
I0201 09:44:44.410301 139774434195200 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.2207019329071045, loss=2.912640333175659
I0201 09:45:30.484340 139774417409792 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.3180949687957764, loss=4.255653381347656
I0201 09:46:16.660698 139774434195200 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.07167649269104, loss=2.935143232345581
I0201 09:47:02.753583 139774417409792 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.0457050800323486, loss=2.893371105194092
I0201 09:47:48.546449 139774434195200 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.187286853790283, loss=2.894578456878662
I0201 09:48:34.845261 139774417409792 logging_writer.py:48] [147000] global_step=147000, grad_norm=1.9363421201705933, loss=3.068256378173828
I0201 09:49:21.092993 139774434195200 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.1663737297058105, loss=2.91219162940979
I0201 09:50:07.257694 139774417409792 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.195369005203247, loss=3.3858349323272705
I0201 09:50:51.213375 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:51:03.377830 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:51:34.741555 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:51:36.358155 139936116377408 submission_runner.py:408] Time since start: 74083.22s, 	Step: 147297, 	{'train/accuracy': 0.8604687452316284, 'train/loss': 0.7560346722602844, 'validation/accuracy': 0.766759991645813, 'validation/loss': 1.1506201028823853, 'validation/num_examples': 50000, 'test/accuracy': 0.6461000442504883, 'test/loss': 1.7426823377609253, 'test/num_examples': 10000, 'score': 67258.91440343857, 'total_duration': 74083.21685099602, 'accumulated_submission_time': 67258.91440343857, 'accumulated_eval_time': 6810.045199871063, 'accumulated_logging_time': 6.589900016784668}
I0201 09:51:36.394468 139774434195200 logging_writer.py:48] [147297] accumulated_eval_time=6810.045200, accumulated_logging_time=6.589900, accumulated_submission_time=67258.914403, global_step=147297, preemption_count=0, score=67258.914403, test/accuracy=0.646100, test/loss=1.742682, test/num_examples=10000, total_duration=74083.216851, train/accuracy=0.860469, train/loss=0.756035, validation/accuracy=0.766760, validation/loss=1.150620, validation/num_examples=50000
I0201 09:51:37.983903 139774417409792 logging_writer.py:48] [147300] global_step=147300, grad_norm=1.9672402143478394, loss=2.861823081970215
I0201 09:52:19.394550 139774434195200 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.032370090484619, loss=3.567589282989502
I0201 09:53:05.062523 139774417409792 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.401912212371826, loss=4.0884246826171875
I0201 09:53:51.442494 139774434195200 logging_writer.py:48] [147600] global_step=147600, grad_norm=1.9499257802963257, loss=3.496694564819336
I0201 09:54:38.139450 139774417409792 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.23195219039917, loss=3.2467238903045654
I0201 09:55:24.315227 139774434195200 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.118828296661377, loss=2.9364051818847656
I0201 09:56:10.439912 139774417409792 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.1259353160858154, loss=2.8799524307250977
I0201 09:56:56.511845 139774434195200 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.1104955673217773, loss=3.155142307281494
I0201 09:57:42.751949 139774417409792 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.359862804412842, loss=4.054304122924805
I0201 09:58:29.050921 139774434195200 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.1317310333251953, loss=2.9258875846862793
I0201 09:58:36.554042 139936116377408 spec.py:321] Evaluating on the training split.
I0201 09:58:48.462375 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 09:59:18.038953 139936116377408 spec.py:349] Evaluating on the test split.
I0201 09:59:19.648285 139936116377408 submission_runner.py:408] Time since start: 74546.51s, 	Step: 148218, 	{'train/accuracy': 0.8648632764816284, 'train/loss': 0.7450129389762878, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1421996355056763, 'validation/num_examples': 50000, 'test/accuracy': 0.6454000473022461, 'test/loss': 1.7250229120254517, 'test/num_examples': 10000, 'score': 67679.01679587364, 'total_duration': 74546.50698709488, 'accumulated_submission_time': 67679.01679587364, 'accumulated_eval_time': 6853.139424085617, 'accumulated_logging_time': 6.635733366012573}
I0201 09:59:19.689139 139774417409792 logging_writer.py:48] [148218] accumulated_eval_time=6853.139424, accumulated_logging_time=6.635733, accumulated_submission_time=67679.016796, global_step=148218, preemption_count=0, score=67679.016796, test/accuracy=0.645400, test/loss=1.725023, test/num_examples=10000, total_duration=74546.506987, train/accuracy=0.864863, train/loss=0.745013, validation/accuracy=0.766980, validation/loss=1.142200, validation/num_examples=50000
I0201 09:59:53.239558 139774434195200 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.105492115020752, loss=2.9143335819244385
I0201 10:00:39.251066 139774417409792 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.1639862060546875, loss=2.938732862472534
I0201 10:01:25.355601 139774434195200 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.227344512939453, loss=4.26500129699707
I0201 10:02:12.280334 139774417409792 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.114490509033203, loss=2.9768731594085693
I0201 10:02:58.794609 139774434195200 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.0332400798797607, loss=3.181364059448242
I0201 10:03:44.388039 139774417409792 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.0874619483947754, loss=2.934966564178467
I0201 10:04:30.821836 139774434195200 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.133970260620117, loss=3.2336530685424805
I0201 10:05:17.097272 139774417409792 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.579271078109741, loss=4.287930965423584
I0201 10:06:03.242961 139774434195200 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.096737861633301, loss=2.9384851455688477
I0201 10:06:19.791107 139936116377408 spec.py:321] Evaluating on the training split.
I0201 10:06:31.886744 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 10:07:03.396131 139936116377408 spec.py:349] Evaluating on the test split.
I0201 10:07:05.008264 139936116377408 submission_runner.py:408] Time since start: 75011.87s, 	Step: 149138, 	{'train/accuracy': 0.8698437213897705, 'train/loss': 0.7154492139816284, 'validation/accuracy': 0.768839955329895, 'validation/loss': 1.1309633255004883, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.7095143795013428, 'test/num_examples': 10000, 'score': 68098.65920686722, 'total_duration': 75011.86696529388, 'accumulated_submission_time': 68098.65920686722, 'accumulated_eval_time': 6898.35661482811, 'accumulated_logging_time': 7.087894439697266}
I0201 10:07:05.048851 139774417409792 logging_writer.py:48] [149138] accumulated_eval_time=6898.356615, accumulated_logging_time=7.087894, accumulated_submission_time=68098.659207, global_step=149138, preemption_count=0, score=68098.659207, test/accuracy=0.649000, test/loss=1.709514, test/num_examples=10000, total_duration=75011.866965, train/accuracy=0.869844, train/loss=0.715449, validation/accuracy=0.768840, validation/loss=1.130963, validation/num_examples=50000
I0201 10:07:30.078354 139774434195200 logging_writer.py:48] [149200] global_step=149200, grad_norm=1.9589629173278809, loss=3.0811855792999268
I0201 10:08:14.704450 139774417409792 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.087348222732544, loss=3.540773868560791
I0201 10:09:00.906233 139774434195200 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.0323500633239746, loss=3.298128604888916
I0201 10:09:47.469423 139774417409792 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.4228904247283936, loss=4.130300998687744
I0201 10:10:33.764842 139774434195200 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.2233195304870605, loss=3.8499951362609863
I0201 10:11:20.140303 139774417409792 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.155500650405884, loss=2.9745309352874756
I0201 10:12:06.664440 139774434195200 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.120650291442871, loss=2.8437442779541016
I0201 10:12:52.882036 139774417409792 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.12666916847229, loss=2.9536595344543457
I0201 10:13:39.268435 139774434195200 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.2196123600006104, loss=2.9554238319396973
I0201 10:14:05.291705 139936116377408 spec.py:321] Evaluating on the training split.
I0201 10:14:17.356227 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 10:14:49.289772 139936116377408 spec.py:349] Evaluating on the test split.
I0201 10:14:50.902191 139936116377408 submission_runner.py:408] Time since start: 75477.76s, 	Step: 150058, 	{'train/accuracy': 0.8729296922683716, 'train/loss': 0.7006752490997314, 'validation/accuracy': 0.7681399583816528, 'validation/loss': 1.1254583597183228, 'validation/num_examples': 50000, 'test/accuracy': 0.650700032711029, 'test/loss': 1.709939956665039, 'test/num_examples': 10000, 'score': 68518.84233808517, 'total_duration': 75477.76089262962, 'accumulated_submission_time': 68518.84233808517, 'accumulated_eval_time': 6943.96710062027, 'accumulated_logging_time': 7.140713453292847}
I0201 10:14:50.939769 139774417409792 logging_writer.py:48] [150058] accumulated_eval_time=6943.967101, accumulated_logging_time=7.140713, accumulated_submission_time=68518.842338, global_step=150058, preemption_count=0, score=68518.842338, test/accuracy=0.650700, test/loss=1.709940, test/num_examples=10000, total_duration=75477.760893, train/accuracy=0.872930, train/loss=0.700675, validation/accuracy=0.768140, validation/loss=1.125458, validation/num_examples=50000
I0201 10:15:08.002220 139774434195200 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.1872992515563965, loss=2.9193496704101562
I0201 10:15:51.881340 139774417409792 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.5516700744628906, loss=4.085385322570801
I0201 10:16:38.137344 139774434195200 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.6652450561523438, loss=4.32444429397583
I0201 10:17:24.360604 139774417409792 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.127089262008667, loss=2.9957218170166016
I0201 10:18:10.742285 139774434195200 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.0585601329803467, loss=2.9168715476989746
I0201 10:18:56.894967 139774417409792 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.3070805072784424, loss=2.996108055114746
I0201 10:19:43.206694 139774434195200 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.5224921703338623, loss=4.076841354370117
I0201 10:20:29.609395 139774417409792 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.120166063308716, loss=3.1940202713012695
I0201 10:21:15.883353 139774434195200 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.4506945610046387, loss=4.041836738586426
I0201 10:21:51.445058 139936116377408 spec.py:321] Evaluating on the training split.
I0201 10:22:04.451069 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 10:22:32.621885 139936116377408 spec.py:349] Evaluating on the test split.
I0201 10:22:34.219219 139936116377408 submission_runner.py:408] Time since start: 75941.08s, 	Step: 150978, 	{'train/accuracy': 0.8682031035423279, 'train/loss': 0.7284000515937805, 'validation/accuracy': 0.770039975643158, 'validation/loss': 1.1370346546173096, 'validation/num_examples': 50000, 'test/accuracy': 0.6529000401496887, 'test/loss': 1.707403540611267, 'test/num_examples': 10000, 'score': 68939.2911374569, 'total_duration': 75941.07792234421, 'accumulated_submission_time': 68939.2911374569, 'accumulated_eval_time': 6986.741266012192, 'accumulated_logging_time': 7.1879754066467285}
I0201 10:22:34.257405 139774417409792 logging_writer.py:48] [150978] accumulated_eval_time=6986.741266, accumulated_logging_time=7.187975, accumulated_submission_time=68939.291137, global_step=150978, preemption_count=0, score=68939.291137, test/accuracy=0.652900, test/loss=1.707404, test/num_examples=10000, total_duration=75941.077922, train/accuracy=0.868203, train/loss=0.728400, validation/accuracy=0.770040, validation/loss=1.137035, validation/num_examples=50000
I0201 10:22:43.394100 139774434195200 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.102015495300293, loss=2.905121326446533
I0201 10:23:25.573427 139774417409792 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.298394203186035, loss=2.844963312149048
I0201 10:24:11.759803 139774434195200 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.156140089035034, loss=2.9573752880096436
I0201 10:24:57.974236 139774417409792 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.0894529819488525, loss=3.0455434322357178
I0201 10:25:44.165384 139774434195200 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.173529624938965, loss=2.81119441986084
I0201 10:26:30.306196 139774417409792 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.145129919052124, loss=3.16268253326416
I0201 10:27:16.827934 139774434195200 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.278865098953247, loss=3.3187286853790283
I0201 10:28:02.907944 139774417409792 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.1190381050109863, loss=3.268463134765625
I0201 10:28:48.972505 139774434195200 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.3026978969573975, loss=3.021186351776123
I0201 10:29:34.937800 139774417409792 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.3847529888153076, loss=3.9126791954040527
I0201 10:29:34.954829 139936116377408 spec.py:321] Evaluating on the training split.
I0201 10:29:46.879633 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 10:30:15.116119 139936116377408 spec.py:349] Evaluating on the test split.
I0201 10:30:16.729122 139936116377408 submission_runner.py:408] Time since start: 76403.59s, 	Step: 151901, 	{'train/accuracy': 0.8696093559265137, 'train/loss': 0.7336189150810242, 'validation/accuracy': 0.7696399688720703, 'validation/loss': 1.1401625871658325, 'validation/num_examples': 50000, 'test/accuracy': 0.6514000296592712, 'test/loss': 1.7238764762878418, 'test/num_examples': 10000, 'score': 69359.93056178093, 'total_duration': 76403.58780241013, 'accumulated_submission_time': 69359.93056178093, 'accumulated_eval_time': 7028.515541791916, 'accumulated_logging_time': 7.23703145980835}
I0201 10:30:16.775704 139774434195200 logging_writer.py:48] [151901] accumulated_eval_time=7028.515542, accumulated_logging_time=7.237031, accumulated_submission_time=69359.930562, global_step=151901, preemption_count=0, score=69359.930562, test/accuracy=0.651400, test/loss=1.723876, test/num_examples=10000, total_duration=76403.587802, train/accuracy=0.869609, train/loss=0.733619, validation/accuracy=0.769640, validation/loss=1.140163, validation/num_examples=50000
I0201 10:30:57.675687 139774417409792 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.1304688453674316, loss=2.895833730697632
I0201 10:31:43.808534 139774434195200 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.0343284606933594, loss=3.495466709136963
I0201 10:32:30.164531 139774417409792 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.6745755672454834, loss=4.020569801330566
I0201 10:33:16.440717 139774434195200 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.044689655303955, loss=2.9198827743530273
I0201 10:34:02.703685 139774417409792 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.192028284072876, loss=2.8303067684173584
I0201 10:34:49.098167 139774434195200 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.2557051181793213, loss=2.853586435317993
I0201 10:35:35.315919 139774417409792 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.3830008506774902, loss=2.874506711959839
I0201 10:36:21.631434 139774434195200 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.622267723083496, loss=4.170237064361572
I0201 10:37:08.057733 139774417409792 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.01287579536438, loss=3.345957040786743
I0201 10:37:16.895730 139936116377408 spec.py:321] Evaluating on the training split.
I0201 10:37:28.977360 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 10:37:57.600647 139936116377408 spec.py:349] Evaluating on the test split.
I0201 10:37:59.202219 139936116377408 submission_runner.py:408] Time since start: 76866.06s, 	Step: 152821, 	{'train/accuracy': 0.8742968440055847, 'train/loss': 0.6973341703414917, 'validation/accuracy': 0.7716000080108643, 'validation/loss': 1.1223145723342896, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.7144511938095093, 'test/num_examples': 10000, 'score': 69779.99265003204, 'total_duration': 76866.06092453003, 'accumulated_submission_time': 69779.99265003204, 'accumulated_eval_time': 7070.822064638138, 'accumulated_logging_time': 7.29412579536438}
I0201 10:37:59.243345 139774434195200 logging_writer.py:48] [152821] accumulated_eval_time=7070.822065, accumulated_logging_time=7.294126, accumulated_submission_time=69779.992650, global_step=152821, preemption_count=0, score=69779.992650, test/accuracy=0.649200, test/loss=1.714451, test/num_examples=10000, total_duration=76866.060925, train/accuracy=0.874297, train/loss=0.697334, validation/accuracy=0.771600, validation/loss=1.122315, validation/num_examples=50000
I0201 10:38:31.082070 139774417409792 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.0908496379852295, loss=2.8968939781188965
I0201 10:39:17.344023 139774434195200 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.111372709274292, loss=3.310584783554077
I0201 10:40:03.675279 139774417409792 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.2345166206359863, loss=2.8842992782592773
I0201 10:40:50.079929 139774434195200 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.190897226333618, loss=2.90136456489563
I0201 10:41:36.313252 139774417409792 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.1476974487304688, loss=2.9752886295318604
I0201 10:42:22.863941 139774434195200 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.4277865886688232, loss=2.9490761756896973
I0201 10:43:09.157650 139774417409792 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.2883243560791016, loss=2.940213680267334
I0201 10:43:55.117244 139774434195200 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.2456817626953125, loss=2.817856788635254
I0201 10:44:41.634771 139774417409792 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.2369301319122314, loss=4.2154998779296875
I0201 10:44:59.334799 139936116377408 spec.py:321] Evaluating on the training split.
I0201 10:45:11.347462 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 10:45:39.550863 139936116377408 spec.py:349] Evaluating on the test split.
I0201 10:45:41.151539 139936116377408 submission_runner.py:408] Time since start: 77328.01s, 	Step: 153740, 	{'train/accuracy': 0.8701757788658142, 'train/loss': 0.7270835041999817, 'validation/accuracy': 0.7702800035476685, 'validation/loss': 1.1377615928649902, 'validation/num_examples': 50000, 'test/accuracy': 0.6533000469207764, 'test/loss': 1.7147181034088135, 'test/num_examples': 10000, 'score': 70200.02810454369, 'total_duration': 77328.01024198532, 'accumulated_submission_time': 70200.02810454369, 'accumulated_eval_time': 7112.63879776001, 'accumulated_logging_time': 7.344912052154541}
I0201 10:45:41.193563 139774434195200 logging_writer.py:48] [153740] accumulated_eval_time=7112.638798, accumulated_logging_time=7.344912, accumulated_submission_time=70200.028105, global_step=153740, preemption_count=0, score=70200.028105, test/accuracy=0.653300, test/loss=1.714718, test/num_examples=10000, total_duration=77328.010242, train/accuracy=0.870176, train/loss=0.727084, validation/accuracy=0.770280, validation/loss=1.137762, validation/num_examples=50000
I0201 10:46:05.428995 139774417409792 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.5732455253601074, loss=4.03361701965332
I0201 10:46:50.149781 139774434195200 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.2462854385375977, loss=3.1182501316070557
I0201 10:47:36.624399 139774417409792 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.3039464950561523, loss=2.890268325805664
I0201 10:48:23.290692 139774434195200 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.369002103805542, loss=2.8647587299346924
I0201 10:49:09.470000 139774417409792 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.0183324813842773, loss=2.9508798122406006
I0201 10:49:55.529318 139774434195200 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.2050745487213135, loss=2.897653579711914
I0201 10:50:41.579340 139774417409792 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.34367036819458, loss=3.4074018001556396
I0201 10:51:27.644708 139774434195200 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.488121509552002, loss=3.6888465881347656
I0201 10:52:13.930815 139774417409792 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.2178666591644287, loss=3.8368172645568848
I0201 10:52:41.260869 139936116377408 spec.py:321] Evaluating on the training split.
I0201 10:52:53.240000 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 10:53:21.721435 139936116377408 spec.py:349] Evaluating on the test split.
I0201 10:53:23.329647 139936116377408 submission_runner.py:408] Time since start: 77790.19s, 	Step: 154661, 	{'train/accuracy': 0.873828113079071, 'train/loss': 0.7049767971038818, 'validation/accuracy': 0.7745800018310547, 'validation/loss': 1.1225615739822388, 'validation/num_examples': 50000, 'test/accuracy': 0.659000039100647, 'test/loss': 1.699967861175537, 'test/num_examples': 10000, 'score': 70620.03921198845, 'total_duration': 77790.1883494854, 'accumulated_submission_time': 70620.03921198845, 'accumulated_eval_time': 7154.707575559616, 'accumulated_logging_time': 7.395971059799194}
I0201 10:53:23.371298 139774434195200 logging_writer.py:48] [154661] accumulated_eval_time=7154.707576, accumulated_logging_time=7.395971, accumulated_submission_time=70620.039212, global_step=154661, preemption_count=0, score=70620.039212, test/accuracy=0.659000, test/loss=1.699968, test/num_examples=10000, total_duration=77790.188349, train/accuracy=0.873828, train/loss=0.704977, validation/accuracy=0.774580, validation/loss=1.122562, validation/num_examples=50000
I0201 10:53:39.255482 139774417409792 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.1529061794281006, loss=2.802483081817627
I0201 10:54:22.799312 139774434195200 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.2890143394470215, loss=3.1259820461273193
I0201 10:55:09.100980 139774417409792 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.2915515899658203, loss=2.9402976036071777
I0201 10:55:55.519690 139774434195200 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.3228375911712646, loss=2.8556101322174072
I0201 10:56:41.701237 139774417409792 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.2469329833984375, loss=2.873555898666382
I0201 10:57:29.081279 139774434195200 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.1451334953308105, loss=3.002589225769043
I0201 10:58:16.066840 139774417409792 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.595538377761841, loss=4.316329479217529
I0201 10:59:02.465663 139774434195200 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.224503517150879, loss=2.836888313293457
I0201 10:59:48.581844 139774417409792 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.131798505783081, loss=3.2464306354522705
I0201 11:00:23.446196 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:00:35.535552 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:01:08.580991 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:01:10.186767 139936116377408 submission_runner.py:408] Time since start: 78257.05s, 	Step: 155577, 	{'train/accuracy': 0.8754296898841858, 'train/loss': 0.6761949062347412, 'validation/accuracy': 0.7723399996757507, 'validation/loss': 1.1067827939987183, 'validation/num_examples': 50000, 'test/accuracy': 0.6546000242233276, 'test/loss': 1.6750155687332153, 'test/num_examples': 10000, 'score': 71040.05707144737, 'total_duration': 78257.04546570778, 'accumulated_submission_time': 71040.05707144737, 'accumulated_eval_time': 7201.4481337070465, 'accumulated_logging_time': 7.4474194049835205}
I0201 11:01:10.229706 139774434195200 logging_writer.py:48] [155577] accumulated_eval_time=7201.448134, accumulated_logging_time=7.447419, accumulated_submission_time=71040.057071, global_step=155577, preemption_count=0, score=71040.057071, test/accuracy=0.654600, test/loss=1.675016, test/num_examples=10000, total_duration=78257.045466, train/accuracy=0.875430, train/loss=0.676195, validation/accuracy=0.772340, validation/loss=1.106783, validation/num_examples=50000
I0201 11:01:19.759370 139774417409792 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.7830286026000977, loss=4.367579936981201
I0201 11:02:02.355324 139774434195200 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.2030529975891113, loss=3.5010459423065186
I0201 11:02:48.391994 139774417409792 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.437774896621704, loss=4.020181655883789
I0201 11:03:34.960326 139774434195200 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.3262758255004883, loss=3.235091209411621
I0201 11:04:21.262191 139774417409792 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.2942700386047363, loss=3.107956647872925
I0201 11:05:07.322931 139774434195200 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.1586790084838867, loss=2.8647472858428955
I0201 11:05:53.726143 139774417409792 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.1454861164093018, loss=2.7633283138275146
I0201 11:06:39.972393 139774434195200 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.17704439163208, loss=2.8168487548828125
I0201 11:07:26.517102 139774417409792 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.3619484901428223, loss=2.904155731201172
I0201 11:08:10.563049 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:08:22.443872 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:08:56.859796 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:08:58.461857 139936116377408 submission_runner.py:408] Time since start: 78725.32s, 	Step: 156497, 	{'train/accuracy': 0.8728905916213989, 'train/loss': 0.7006733417510986, 'validation/accuracy': 0.775439977645874, 'validation/loss': 1.110991358757019, 'validation/num_examples': 50000, 'test/accuracy': 0.6582000255584717, 'test/loss': 1.6929787397384644, 'test/num_examples': 10000, 'score': 71460.33196163177, 'total_duration': 78725.32055997849, 'accumulated_submission_time': 71460.33196163177, 'accumulated_eval_time': 7249.346954345703, 'accumulated_logging_time': 7.500328063964844}
I0201 11:08:58.501036 139774434195200 logging_writer.py:48] [156497] accumulated_eval_time=7249.346954, accumulated_logging_time=7.500328, accumulated_submission_time=71460.331962, global_step=156497, preemption_count=0, score=71460.331962, test/accuracy=0.658200, test/loss=1.692979, test/num_examples=10000, total_duration=78725.320560, train/accuracy=0.872891, train/loss=0.700673, validation/accuracy=0.775440, validation/loss=1.110991, validation/num_examples=50000
I0201 11:09:00.090200 139774417409792 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.1067934036254883, loss=3.6665470600128174
I0201 11:09:41.259267 139774434195200 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.3662335872650146, loss=3.2057576179504395
I0201 11:10:27.388375 139774417409792 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.213399887084961, loss=3.0658297538757324
I0201 11:11:13.483976 139774434195200 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.811548948287964, loss=4.261534690856934
I0201 11:11:59.812054 139774417409792 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.2399723529815674, loss=2.8123316764831543
I0201 11:12:46.020792 139774434195200 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.2906947135925293, loss=3.1638882160186768
I0201 11:13:32.314572 139774417409792 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.2697064876556396, loss=2.9190759658813477
I0201 11:14:18.587521 139774434195200 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.13482928276062, loss=3.3651723861694336
I0201 11:15:04.603204 139774417409792 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.304316520690918, loss=2.8833155632019043
I0201 11:15:50.630914 139774434195200 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.2702903747558594, loss=3.4312996864318848
I0201 11:15:58.762596 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:16:10.819984 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:16:39.454910 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:16:41.066459 139936116377408 submission_runner.py:408] Time since start: 79187.93s, 	Step: 157419, 	{'train/accuracy': 0.8755663633346558, 'train/loss': 0.6870554089546204, 'validation/accuracy': 0.7741000056266785, 'validation/loss': 1.113128900527954, 'validation/num_examples': 50000, 'test/accuracy': 0.65420001745224, 'test/loss': 1.6954491138458252, 'test/num_examples': 10000, 'score': 71880.53656959534, 'total_duration': 79187.9251627922, 'accumulated_submission_time': 71880.53656959534, 'accumulated_eval_time': 7291.650812864304, 'accumulated_logging_time': 7.549129009246826}
I0201 11:16:41.108970 139774417409792 logging_writer.py:48] [157419] accumulated_eval_time=7291.650813, accumulated_logging_time=7.549129, accumulated_submission_time=71880.536570, global_step=157419, preemption_count=0, score=71880.536570, test/accuracy=0.654200, test/loss=1.695449, test/num_examples=10000, total_duration=79187.925163, train/accuracy=0.875566, train/loss=0.687055, validation/accuracy=0.774100, validation/loss=1.113129, validation/num_examples=50000
I0201 11:17:13.702764 139774434195200 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.311061143875122, loss=2.863433837890625
I0201 11:17:59.444105 139774417409792 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.5940985679626465, loss=3.9825825691223145
I0201 11:18:46.147423 139774434195200 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.2588651180267334, loss=2.8582727909088135
I0201 11:19:32.710281 139774417409792 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.352968692779541, loss=2.8635330200195312
I0201 11:20:18.817863 139774434195200 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.218015670776367, loss=2.7711658477783203
I0201 11:21:04.982061 139774417409792 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.171170711517334, loss=2.7815651893615723
I0201 11:21:51.204830 139774434195200 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.287233829498291, loss=3.6881155967712402
I0201 11:22:37.305852 139774417409792 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.2318737506866455, loss=2.963230609893799
I0201 11:23:23.371991 139774434195200 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.220654249191284, loss=2.8221518993377686
I0201 11:23:41.372947 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:23:53.123065 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:24:21.161590 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:24:22.771728 139936116377408 submission_runner.py:408] Time since start: 79649.63s, 	Step: 158341, 	{'train/accuracy': 0.8795117139816284, 'train/loss': 0.6774889230728149, 'validation/accuracy': 0.7754999995231628, 'validation/loss': 1.105984091758728, 'validation/num_examples': 50000, 'test/accuracy': 0.6583000421524048, 'test/loss': 1.6818146705627441, 'test/num_examples': 10000, 'score': 72300.74264979362, 'total_duration': 79649.6304256916, 'accumulated_submission_time': 72300.74264979362, 'accumulated_eval_time': 7333.049576044083, 'accumulated_logging_time': 7.60271143913269}
I0201 11:24:22.811250 139774417409792 logging_writer.py:48] [158341] accumulated_eval_time=7333.049576, accumulated_logging_time=7.602711, accumulated_submission_time=72300.742650, global_step=158341, preemption_count=0, score=72300.742650, test/accuracy=0.658300, test/loss=1.681815, test/num_examples=10000, total_duration=79649.630426, train/accuracy=0.879512, train/loss=0.677489, validation/accuracy=0.775500, validation/loss=1.105984, validation/num_examples=50000
I0201 11:24:46.646625 139774434195200 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.4902915954589844, loss=2.814621925354004
I0201 11:25:31.258294 139774417409792 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.781121253967285, loss=4.204460620880127
I0201 11:26:17.873524 139774434195200 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.273662567138672, loss=3.0112051963806152
I0201 11:27:04.364223 139774417409792 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.7375385761260986, loss=4.059292316436768
I0201 11:27:50.517263 139774434195200 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.1572818756103516, loss=3.2327070236206055
I0201 11:28:37.075942 139774417409792 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.2784273624420166, loss=2.782285690307617
I0201 11:29:23.763322 139774434195200 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.3132317066192627, loss=3.6145424842834473
I0201 11:30:10.116801 139774417409792 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.385634660720825, loss=3.847733974456787
I0201 11:30:56.073434 139774434195200 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.176671028137207, loss=2.9143354892730713
I0201 11:31:23.206866 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:31:35.365451 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:32:07.437349 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:32:09.044368 139936116377408 submission_runner.py:408] Time since start: 80115.90s, 	Step: 159260, 	{'train/accuracy': 0.8759960532188416, 'train/loss': 0.6908382773399353, 'validation/accuracy': 0.775879979133606, 'validation/loss': 1.1087701320648193, 'validation/num_examples': 50000, 'test/accuracy': 0.6544000506401062, 'test/loss': 1.6908684968948364, 'test/num_examples': 10000, 'score': 72721.07917380333, 'total_duration': 80115.90304541588, 'accumulated_submission_time': 72721.07917380333, 'accumulated_eval_time': 7378.887080192566, 'accumulated_logging_time': 7.6535325050354}
I0201 11:32:09.094822 139774417409792 logging_writer.py:48] [159260] accumulated_eval_time=7378.887080, accumulated_logging_time=7.653533, accumulated_submission_time=72721.079174, global_step=159260, preemption_count=0, score=72721.079174, test/accuracy=0.654400, test/loss=1.690868, test/num_examples=10000, total_duration=80115.903045, train/accuracy=0.875996, train/loss=0.690838, validation/accuracy=0.775880, validation/loss=1.108770, validation/num_examples=50000
I0201 11:32:25.683654 139774434195200 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.4268410205841064, loss=2.9177627563476562
I0201 11:33:09.033645 139774417409792 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.265164613723755, loss=3.0904977321624756
I0201 11:33:55.086889 139774434195200 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.3747856616973877, loss=2.787414789199829
I0201 11:34:41.536510 139774417409792 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.2618210315704346, loss=2.8033604621887207
I0201 11:35:27.640117 139774434195200 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.557316541671753, loss=2.8153533935546875
I0201 11:36:13.844314 139774417409792 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.482642889022827, loss=2.898663282394409
I0201 11:37:00.308760 139774434195200 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.3562724590301514, loss=3.735302448272705
I0201 11:37:46.610420 139774417409792 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.3820993900299072, loss=3.417375087738037
I0201 11:38:33.032154 139774434195200 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.5152721405029297, loss=2.8215386867523193
I0201 11:39:09.185191 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:39:21.205080 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:39:55.180061 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:39:56.780161 139936116377408 submission_runner.py:408] Time since start: 80583.64s, 	Step: 160179, 	{'train/accuracy': 0.8782421946525574, 'train/loss': 0.6889281868934631, 'validation/accuracy': 0.7750399708747864, 'validation/loss': 1.1090731620788574, 'validation/num_examples': 50000, 'test/accuracy': 0.6558000445365906, 'test/loss': 1.691611409187317, 'test/num_examples': 10000, 'score': 73141.11105513573, 'total_duration': 80583.63886260986, 'accumulated_submission_time': 73141.11105513573, 'accumulated_eval_time': 7426.482050657272, 'accumulated_logging_time': 7.7148168087005615}
I0201 11:39:56.819533 139774417409792 logging_writer.py:48] [160179] accumulated_eval_time=7426.482051, accumulated_logging_time=7.714817, accumulated_submission_time=73141.111055, global_step=160179, preemption_count=0, score=73141.111055, test/accuracy=0.655800, test/loss=1.691611, test/num_examples=10000, total_duration=80583.638863, train/accuracy=0.878242, train/loss=0.688928, validation/accuracy=0.775040, validation/loss=1.109073, validation/num_examples=50000
I0201 11:40:05.549195 139774434195200 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.3955957889556885, loss=3.155709743499756
I0201 11:40:47.990901 139774417409792 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.47017502784729, loss=3.8526575565338135
I0201 11:41:33.896563 139774434195200 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.3122048377990723, loss=2.7902348041534424
I0201 11:42:20.158705 139774417409792 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.864253044128418, loss=3.3001317977905273
I0201 11:43:06.431794 139774434195200 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.3374454975128174, loss=3.4211349487304688
I0201 11:43:52.245728 139774417409792 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.5221567153930664, loss=3.7188286781311035
I0201 11:44:38.462144 139774434195200 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.728376865386963, loss=3.981733798980713
I0201 11:45:24.604460 139774417409792 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.3218581676483154, loss=2.787123680114746
I0201 11:46:10.762730 139774434195200 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.295877695083618, loss=3.461268901824951
I0201 11:46:56.987517 139774417409792 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.448082685470581, loss=2.9088730812072754
I0201 11:46:57.001423 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:47:09.255023 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:47:41.389733 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:47:42.994816 139936116377408 submission_runner.py:408] Time since start: 81049.85s, 	Step: 161101, 	{'train/accuracy': 0.8794531226158142, 'train/loss': 0.6894750595092773, 'validation/accuracy': 0.7768999934196472, 'validation/loss': 1.1162922382354736, 'validation/num_examples': 50000, 'test/accuracy': 0.6583000421524048, 'test/loss': 1.6969735622406006, 'test/num_examples': 10000, 'score': 73561.23503017426, 'total_duration': 81049.8535144329, 'accumulated_submission_time': 73561.23503017426, 'accumulated_eval_time': 7472.475435972214, 'accumulated_logging_time': 7.764083385467529}
I0201 11:47:43.038736 139774434195200 logging_writer.py:48] [161101] accumulated_eval_time=7472.475436, accumulated_logging_time=7.764083, accumulated_submission_time=73561.235030, global_step=161101, preemption_count=0, score=73561.235030, test/accuracy=0.658300, test/loss=1.696974, test/num_examples=10000, total_duration=81049.853514, train/accuracy=0.879453, train/loss=0.689475, validation/accuracy=0.776900, validation/loss=1.116292, validation/num_examples=50000
I0201 11:48:23.857956 139774417409792 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.2896370887756348, loss=2.778305768966675
I0201 11:49:09.891127 139774434195200 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.374332904815674, loss=2.8518893718719482
I0201 11:49:56.404927 139774417409792 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.835977554321289, loss=3.8767473697662354
I0201 11:50:42.730524 139774434195200 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.249532461166382, loss=2.785555601119995
I0201 11:51:28.794250 139774417409792 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.3665740489959717, loss=2.8682799339294434
I0201 11:52:15.565649 139774434195200 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.4171969890594482, loss=3.0993640422821045
I0201 11:53:01.801421 139774417409792 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.6097514629364014, loss=3.9111411571502686
I0201 11:53:48.039730 139774434195200 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.158745527267456, loss=2.81848406791687
I0201 11:54:34.368750 139774417409792 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.5647592544555664, loss=3.4301846027374268
I0201 11:54:43.207944 139936116377408 spec.py:321] Evaluating on the training split.
I0201 11:54:55.089410 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 11:55:20.919536 139936116377408 spec.py:349] Evaluating on the test split.
I0201 11:55:22.526100 139936116377408 submission_runner.py:408] Time since start: 81509.38s, 	Step: 162021, 	{'train/accuracy': 0.8860155940055847, 'train/loss': 0.6461138129234314, 'validation/accuracy': 0.7784199714660645, 'validation/loss': 1.0914040803909302, 'validation/num_examples': 50000, 'test/accuracy': 0.6588000059127808, 'test/loss': 1.671212911605835, 'test/num_examples': 10000, 'score': 73981.34681868553, 'total_duration': 81509.38480234146, 'accumulated_submission_time': 73981.34681868553, 'accumulated_eval_time': 7511.793575048447, 'accumulated_logging_time': 7.818175554275513}
I0201 11:55:22.571073 139774434195200 logging_writer.py:48] [162021] accumulated_eval_time=7511.793575, accumulated_logging_time=7.818176, accumulated_submission_time=73981.346819, global_step=162021, preemption_count=0, score=73981.346819, test/accuracy=0.658800, test/loss=1.671213, test/num_examples=10000, total_duration=81509.384802, train/accuracy=0.886016, train/loss=0.646114, validation/accuracy=0.778420, validation/loss=1.091404, validation/num_examples=50000
I0201 11:55:54.356333 139774417409792 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.4301183223724365, loss=2.798692464828491
I0201 11:56:40.072305 139774434195200 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.36501407623291, loss=2.81143856048584
I0201 11:57:26.397706 139774417409792 logging_writer.py:48] [162300] global_step=162300, grad_norm=3.06929349899292, loss=4.196280479431152
I0201 11:58:12.894437 139774434195200 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.490384817123413, loss=2.833941698074341
I0201 11:58:58.826429 139774417409792 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.396970272064209, loss=2.7401862144470215
I0201 11:59:45.117578 139774434195200 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.559425115585327, loss=3.020866870880127
I0201 12:00:31.399841 139774417409792 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.3345935344696045, loss=3.171550989151001
I0201 12:01:17.130693 139774434195200 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.9846620559692383, loss=4.340959548950195
I0201 12:02:03.610061 139774417409792 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.4894967079162598, loss=2.8062775135040283
I0201 12:02:22.679864 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:02:34.803338 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:03:06.118785 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:03:07.726608 139936116377408 submission_runner.py:408] Time since start: 81974.59s, 	Step: 162943, 	{'train/accuracy': 0.8807421922683716, 'train/loss': 0.6805300712585449, 'validation/accuracy': 0.7788800001144409, 'validation/loss': 1.1066820621490479, 'validation/num_examples': 50000, 'test/accuracy': 0.6619000434875488, 'test/loss': 1.6778147220611572, 'test/num_examples': 10000, 'score': 74401.3993074894, 'total_duration': 81974.58531212807, 'accumulated_submission_time': 74401.3993074894, 'accumulated_eval_time': 7556.840314865112, 'accumulated_logging_time': 7.872089624404907}
I0201 12:03:07.773452 139774434195200 logging_writer.py:48] [162943] accumulated_eval_time=7556.840315, accumulated_logging_time=7.872090, accumulated_submission_time=74401.399307, global_step=162943, preemption_count=0, score=74401.399307, test/accuracy=0.661900, test/loss=1.677815, test/num_examples=10000, total_duration=81974.585312, train/accuracy=0.880742, train/loss=0.680530, validation/accuracy=0.778880, validation/loss=1.106682, validation/num_examples=50000
I0201 12:03:30.796727 139774417409792 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.632936954498291, loss=2.86580228805542
I0201 12:04:15.188776 139774434195200 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.3157153129577637, loss=2.7780752182006836
I0201 12:05:01.047651 139774417409792 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.455507755279541, loss=3.0351476669311523
I0201 12:05:47.240058 139774434195200 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.3213632106781006, loss=3.4904897212982178
I0201 12:06:33.279888 139774417409792 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.23984956741333, loss=3.058635950088501
I0201 12:07:19.211020 139774434195200 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.338210105895996, loss=2.7685225009918213
I0201 12:08:05.542123 139774417409792 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.893083095550537, loss=4.19407844543457
I0201 12:08:51.474162 139774434195200 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.3786098957061768, loss=2.823840379714966
I0201 12:09:37.727716 139774417409792 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.4725239276885986, loss=2.8053221702575684
I0201 12:10:08.010904 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:10:19.815703 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:10:53.589138 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:10:55.189032 139936116377408 submission_runner.py:408] Time since start: 82442.05s, 	Step: 163867, 	{'train/accuracy': 0.8832812309265137, 'train/loss': 0.6668259501457214, 'validation/accuracy': 0.7802599668502808, 'validation/loss': 1.0908979177474976, 'validation/num_examples': 50000, 'test/accuracy': 0.6641000509262085, 'test/loss': 1.6616779565811157, 'test/num_examples': 10000, 'score': 74821.57655787468, 'total_duration': 82442.0477347374, 'accumulated_submission_time': 74821.57655787468, 'accumulated_eval_time': 7604.018439769745, 'accumulated_logging_time': 7.928055763244629}
I0201 12:10:55.231089 139774434195200 logging_writer.py:48] [163867] accumulated_eval_time=7604.018440, accumulated_logging_time=7.928056, accumulated_submission_time=74821.576558, global_step=163867, preemption_count=0, score=74821.576558, test/accuracy=0.664100, test/loss=1.661678, test/num_examples=10000, total_duration=82442.047735, train/accuracy=0.883281, train/loss=0.666826, validation/accuracy=0.780260, validation/loss=1.090898, validation/num_examples=50000
I0201 12:11:08.721860 139774417409792 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.236088752746582, loss=2.811563730239868
I0201 12:11:51.945186 139774434195200 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.535917282104492, loss=2.820039987564087
I0201 12:12:37.787997 139774417409792 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.6283836364746094, loss=3.619603395462036
I0201 12:13:24.231182 139774434195200 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.5378174781799316, loss=2.8966188430786133
I0201 12:14:10.351188 139774417409792 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.5607612133026123, loss=2.8208117485046387
I0201 12:14:56.436866 139774434195200 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.713085174560547, loss=3.7081050872802734
I0201 12:15:42.725193 139774417409792 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.543473958969116, loss=2.7875664234161377
I0201 12:16:28.939055 139774434195200 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.3848516941070557, loss=2.816093921661377
I0201 12:17:15.043941 139774417409792 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.490591526031494, loss=2.795314311981201
I0201 12:17:55.196123 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:18:07.282511 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:18:39.546028 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:18:41.149163 139936116377408 submission_runner.py:408] Time since start: 82908.01s, 	Step: 164789, 	{'train/accuracy': 0.887988269329071, 'train/loss': 0.6550906896591187, 'validation/accuracy': 0.7813000082969666, 'validation/loss': 1.0903257131576538, 'validation/num_examples': 50000, 'test/accuracy': 0.6593000292778015, 'test/loss': 1.6780301332473755, 'test/num_examples': 10000, 'score': 75241.48506331444, 'total_duration': 82908.00784730911, 'accumulated_submission_time': 75241.48506331444, 'accumulated_eval_time': 7649.971467733383, 'accumulated_logging_time': 7.979364395141602}
I0201 12:18:41.193388 139774434195200 logging_writer.py:48] [164789] accumulated_eval_time=7649.971468, accumulated_logging_time=7.979364, accumulated_submission_time=75241.485063, global_step=164789, preemption_count=0, score=75241.485063, test/accuracy=0.659300, test/loss=1.678030, test/num_examples=10000, total_duration=82908.007847, train/accuracy=0.887988, train/loss=0.655091, validation/accuracy=0.781300, validation/loss=1.090326, validation/num_examples=50000
I0201 12:18:45.949307 139774417409792 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.368795871734619, loss=2.7736434936523438
I0201 12:19:27.447744 139774434195200 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.2420029640197754, loss=2.9255330562591553
I0201 12:20:13.161136 139774417409792 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.3536481857299805, loss=2.8394854068756104
I0201 12:20:59.297747 139774434195200 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.7037765979766846, loss=2.8696465492248535
I0201 12:21:45.915164 139774417409792 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.395247459411621, loss=2.8411104679107666
I0201 12:22:31.988057 139774434195200 logging_writer.py:48] [165300] global_step=165300, grad_norm=3.268711805343628, loss=4.229325294494629
I0201 12:23:17.922752 139774417409792 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.6371729373931885, loss=2.806894302368164
I0201 12:24:03.767336 139774434195200 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.30635929107666, loss=3.1201839447021484
I0201 12:24:49.614864 139774417409792 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.5142295360565186, loss=2.894453763961792
I0201 12:25:35.700267 139774434195200 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.0451769828796387, loss=2.8057401180267334
I0201 12:25:41.380721 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:25:53.262445 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:26:21.168627 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:26:22.779232 139936116377408 submission_runner.py:408] Time since start: 83369.64s, 	Step: 165714, 	{'train/accuracy': 0.8838671445846558, 'train/loss': 0.6711214780807495, 'validation/accuracy': 0.7797999978065491, 'validation/loss': 1.0984795093536377, 'validation/num_examples': 50000, 'test/accuracy': 0.663100004196167, 'test/loss': 1.6789699792861938, 'test/num_examples': 10000, 'score': 75661.61553740501, 'total_duration': 83369.6378827095, 'accumulated_submission_time': 75661.61553740501, 'accumulated_eval_time': 7691.369910478592, 'accumulated_logging_time': 8.033177137374878}
I0201 12:26:22.825620 139774417409792 logging_writer.py:48] [165714] accumulated_eval_time=7691.369910, accumulated_logging_time=8.033177, accumulated_submission_time=75661.615537, global_step=165714, preemption_count=0, score=75661.615537, test/accuracy=0.663100, test/loss=1.678970, test/num_examples=10000, total_duration=83369.637883, train/accuracy=0.883867, train/loss=0.671121, validation/accuracy=0.779800, validation/loss=1.098480, validation/num_examples=50000
I0201 12:26:57.486184 139774434195200 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.846118450164795, loss=4.083584308624268
I0201 12:27:43.417448 139774417409792 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.2182257175445557, loss=4.326594352722168
I0201 12:28:29.727533 139774434195200 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.4574546813964844, loss=2.787667751312256
I0201 12:29:16.262953 139774417409792 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.514409065246582, loss=2.8901853561401367
I0201 12:30:02.179598 139774434195200 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.4615230560302734, loss=2.812077760696411
I0201 12:30:48.394449 139774417409792 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.3756749629974365, loss=2.8433947563171387
I0201 12:31:34.935407 139774434195200 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.905729293823242, loss=4.070734024047852
I0201 12:32:21.150892 139774417409792 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.7007405757904053, loss=2.8601465225219727
I0201 12:33:07.354298 139774434195200 logging_writer.py:48] [166600] global_step=166600, grad_norm=3.5091183185577393, loss=4.28330659866333
I0201 12:33:23.110213 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:33:35.238696 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:34:06.709139 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:34:08.321314 139936116377408 submission_runner.py:408] Time since start: 83835.18s, 	Step: 166636, 	{'train/accuracy': 0.8857812285423279, 'train/loss': 0.6534203290939331, 'validation/accuracy': 0.7819399833679199, 'validation/loss': 1.0902855396270752, 'validation/num_examples': 50000, 'test/accuracy': 0.6624000072479248, 'test/loss': 1.6653751134872437, 'test/num_examples': 10000, 'score': 76081.84141349792, 'total_duration': 83835.18001461029, 'accumulated_submission_time': 76081.84141349792, 'accumulated_eval_time': 7736.580994844437, 'accumulated_logging_time': 8.090880393981934}
I0201 12:34:08.364787 139774417409792 logging_writer.py:48] [166636] accumulated_eval_time=7736.580995, accumulated_logging_time=8.090880, accumulated_submission_time=76081.841413, global_step=166636, preemption_count=0, score=76081.841413, test/accuracy=0.662400, test/loss=1.665375, test/num_examples=10000, total_duration=83835.180015, train/accuracy=0.885781, train/loss=0.653420, validation/accuracy=0.781940, validation/loss=1.090286, validation/num_examples=50000
I0201 12:34:34.173151 139774434195200 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.564011573791504, loss=2.717731475830078
I0201 12:35:19.052698 139774417409792 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.8919906616210938, loss=3.27327823638916
I0201 12:36:05.545680 139774434195200 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.361982583999634, loss=3.2339565753936768
I0201 12:36:51.865002 139774417409792 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.4737370014190674, loss=2.7674145698547363
I0201 12:37:37.955940 139774434195200 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.842168092727661, loss=3.097827911376953
I0201 12:38:24.472379 139774417409792 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.539121150970459, loss=2.8213768005371094
I0201 12:39:10.868297 139774434195200 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.3647544384002686, loss=3.254025936126709
I0201 12:39:56.977161 139774417409792 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.421854019165039, loss=3.4289326667785645
I0201 12:40:43.370252 139774434195200 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.4890284538269043, loss=2.7337353229522705
I0201 12:41:08.398122 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:41:20.337200 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:41:51.146852 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:41:52.749477 139936116377408 submission_runner.py:408] Time since start: 84299.61s, 	Step: 167556, 	{'train/accuracy': 0.8882226347923279, 'train/loss': 0.6422659754753113, 'validation/accuracy': 0.7817999720573425, 'validation/loss': 1.0872855186462402, 'validation/num_examples': 50000, 'test/accuracy': 0.6653000116348267, 'test/loss': 1.656856894493103, 'test/num_examples': 10000, 'score': 76501.81643271446, 'total_duration': 84299.6081571579, 'accumulated_submission_time': 76501.81643271446, 'accumulated_eval_time': 7780.9323217868805, 'accumulated_logging_time': 8.14522910118103}
I0201 12:41:52.799595 139774417409792 logging_writer.py:48] [167556] accumulated_eval_time=7780.932322, accumulated_logging_time=8.145229, accumulated_submission_time=76501.816433, global_step=167556, preemption_count=0, score=76501.816433, test/accuracy=0.665300, test/loss=1.656857, test/num_examples=10000, total_duration=84299.608157, train/accuracy=0.888223, train/loss=0.642266, validation/accuracy=0.781800, validation/loss=1.087286, validation/num_examples=50000
I0201 12:42:10.669948 139774434195200 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.5661239624023438, loss=2.8865981101989746
I0201 12:42:54.608051 139774417409792 logging_writer.py:48] [167700] global_step=167700, grad_norm=3.337641477584839, loss=4.271750450134277
I0201 12:43:40.803471 139774434195200 logging_writer.py:48] [167800] global_step=167800, grad_norm=2.437670946121216, loss=2.785259246826172
I0201 12:44:27.128838 139774417409792 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.433058977127075, loss=2.8827950954437256
I0201 12:45:13.169842 139774434195200 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.4430623054504395, loss=2.727057695388794
I0201 12:45:59.507325 139774417409792 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.6300740242004395, loss=3.0547571182250977
I0201 12:46:45.986274 139774434195200 logging_writer.py:48] [168200] global_step=168200, grad_norm=2.6230788230895996, loss=2.8446948528289795
I0201 12:47:31.982953 139774417409792 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.679527521133423, loss=2.7737340927124023
I0201 12:48:18.358259 139774434195200 logging_writer.py:48] [168400] global_step=168400, grad_norm=3.09861159324646, loss=4.1270599365234375
I0201 12:48:52.974436 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:49:05.009592 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:49:36.917163 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:49:38.516624 139936116377408 submission_runner.py:408] Time since start: 84765.38s, 	Step: 168477, 	{'train/accuracy': 0.8875390291213989, 'train/loss': 0.6451851725578308, 'validation/accuracy': 0.7829200029373169, 'validation/loss': 1.0794780254364014, 'validation/num_examples': 50000, 'test/accuracy': 0.6647000312805176, 'test/loss': 1.650307297706604, 'test/num_examples': 10000, 'score': 76921.93040585518, 'total_duration': 84765.37529754639, 'accumulated_submission_time': 76921.93040585518, 'accumulated_eval_time': 7826.474480390549, 'accumulated_logging_time': 8.208962202072144}
I0201 12:49:38.561310 139774417409792 logging_writer.py:48] [168477] accumulated_eval_time=7826.474480, accumulated_logging_time=8.208962, accumulated_submission_time=76921.930406, global_step=168477, preemption_count=0, score=76921.930406, test/accuracy=0.664700, test/loss=1.650307, test/num_examples=10000, total_duration=84765.375298, train/accuracy=0.887539, train/loss=0.645185, validation/accuracy=0.782920, validation/loss=1.079478, validation/num_examples=50000
I0201 12:49:48.079259 139774434195200 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.528167963027954, loss=2.888667106628418
I0201 12:50:30.344064 139774417409792 logging_writer.py:48] [168600] global_step=168600, grad_norm=2.4804065227508545, loss=2.917386770248413
I0201 12:51:16.565712 139774434195200 logging_writer.py:48] [168700] global_step=168700, grad_norm=2.8260369300842285, loss=3.838623285293579
I0201 12:52:03.157198 139774417409792 logging_writer.py:48] [168800] global_step=168800, grad_norm=3.250540018081665, loss=4.238940238952637
I0201 12:52:49.700860 139774434195200 logging_writer.py:48] [168900] global_step=168900, grad_norm=2.54052472114563, loss=2.7453746795654297
I0201 12:53:35.866483 139774417409792 logging_writer.py:48] [169000] global_step=169000, grad_norm=3.1227221488952637, loss=4.118156909942627
I0201 12:54:22.228947 139774434195200 logging_writer.py:48] [169100] global_step=169100, grad_norm=3.328993320465088, loss=4.204841136932373
I0201 12:55:08.276345 139774417409792 logging_writer.py:48] [169200] global_step=169200, grad_norm=2.524411201477051, loss=2.8043155670166016
I0201 12:55:54.127851 139774434195200 logging_writer.py:48] [169300] global_step=169300, grad_norm=2.4857122898101807, loss=2.8706741333007812
I0201 12:56:38.696134 139936116377408 spec.py:321] Evaluating on the training split.
I0201 12:56:50.525022 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 12:57:22.312562 139936116377408 spec.py:349] Evaluating on the test split.
I0201 12:57:23.915944 139936116377408 submission_runner.py:408] Time since start: 85230.77s, 	Step: 169398, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.6406536102294922, 'validation/accuracy': 0.7831999659538269, 'validation/loss': 1.078544020652771, 'validation/num_examples': 50000, 'test/accuracy': 0.6653000116348267, 'test/loss': 1.6556425094604492, 'test/num_examples': 10000, 'score': 77342.00812506676, 'total_duration': 85230.77464270592, 'accumulated_submission_time': 77342.00812506676, 'accumulated_eval_time': 7871.694277763367, 'accumulated_logging_time': 8.263505935668945}
I0201 12:57:23.957419 139774417409792 logging_writer.py:48] [169398] accumulated_eval_time=7871.694278, accumulated_logging_time=8.263506, accumulated_submission_time=77342.008125, global_step=169398, preemption_count=0, score=77342.008125, test/accuracy=0.665300, test/loss=1.655643, test/num_examples=10000, total_duration=85230.774643, train/accuracy=0.887422, train/loss=0.640654, validation/accuracy=0.783200, validation/loss=1.078544, validation/num_examples=50000
I0201 12:57:25.148854 139774434195200 logging_writer.py:48] [169400] global_step=169400, grad_norm=2.6170005798339844, loss=2.775111675262451
I0201 12:58:06.551227 139774417409792 logging_writer.py:48] [169500] global_step=169500, grad_norm=2.742725133895874, loss=3.614424467086792
I0201 12:58:52.274343 139774434195200 logging_writer.py:48] [169600] global_step=169600, grad_norm=2.398383855819702, loss=2.7268941402435303
I0201 12:59:38.581585 139774417409792 logging_writer.py:48] [169700] global_step=169700, grad_norm=2.393298864364624, loss=2.819258689880371
I0201 13:00:22.046684 139774434195200 logging_writer.py:48] [169795] global_step=169795, preemption_count=0, score=77520.012076
I0201 13:00:22.703096 139936116377408 checkpoints.py:490] Saving checkpoint at step: 169795
I0201 13:00:23.982692 139936116377408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_2/checkpoint_169795
I0201 13:00:24.000887 139936116377408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_2/checkpoint_169795.
I0201 13:00:24.694340 139936116377408 submission_runner.py:583] Tuning trial 2/5
I0201 13:00:24.694555 139936116377408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0201 13:00:24.702094 139936116377408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.84359121322632, 'total_duration': 62.57128691673279, 'accumulated_submission_time': 35.84359121322632, 'accumulated_eval_time': 26.727606058120728, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (844, {'train/accuracy': 0.0159765612334013, 'train/loss': 6.382034778594971, 'validation/accuracy': 0.01599999889731407, 'validation/loss': 6.395730972290039, 'validation/num_examples': 50000, 'test/accuracy': 0.01140000019222498, 'test/loss': 6.438459873199463, 'test/num_examples': 10000, 'score': 455.78622579574585, 'total_duration': 526.145667552948, 'accumulated_submission_time': 455.78622579574585, 'accumulated_eval_time': 70.29547381401062, 'accumulated_logging_time': 0.020416259765625, 'global_step': 844, 'preemption_count': 0}), (1764, {'train/accuracy': 0.04486327990889549, 'train/loss': 5.828395366668701, 'validation/accuracy': 0.04315999895334244, 'validation/loss': 5.85699987411499, 'validation/num_examples': 50000, 'test/accuracy': 0.03350000083446503, 'test/loss': 5.981106758117676, 'test/num_examples': 10000, 'score': 876.1266677379608, 'total_duration': 986.7028484344482, 'accumulated_submission_time': 876.1266677379608, 'accumulated_eval_time': 110.43268370628357, 'accumulated_logging_time': 0.0518798828125, 'global_step': 1764, 'preemption_count': 0}), (2684, {'train/accuracy': 0.07359375059604645, 'train/loss': 5.448266983032227, 'validation/accuracy': 0.06747999787330627, 'validation/loss': 5.489231586456299, 'validation/num_examples': 50000, 'test/accuracy': 0.05020000413060188, 'test/loss': 5.668014049530029, 'test/num_examples': 10000, 'score': 1296.2641365528107, 'total_duration': 1450.8591334819794, 'accumulated_submission_time': 1296.2641365528107, 'accumulated_eval_time': 154.37527751922607, 'accumulated_logging_time': 0.07917928695678711, 'global_step': 2684, 'preemption_count': 0}), (3604, {'train/accuracy': 0.11046874523162842, 'train/loss': 5.068248271942139, 'validation/accuracy': 0.10171999782323837, 'validation/loss': 5.126224994659424, 'validation/num_examples': 50000, 'test/accuracy': 0.07670000195503235, 'test/loss': 5.356302261352539, 'test/num_examples': 10000, 'score': 1716.3032870292664, 'total_duration': 1915.3701028823853, 'accumulated_submission_time': 1716.3032870292664, 'accumulated_eval_time': 198.7751681804657, 'accumulated_logging_time': 0.10387563705444336, 'global_step': 3604, 'preemption_count': 0}), (4522, {'train/accuracy': 0.1553320288658142, 'train/loss': 4.668953895568848, 'validation/accuracy': 0.13806000351905823, 'validation/loss': 4.754825592041016, 'validation/num_examples': 50000, 'test/accuracy': 0.09940000623464584, 'test/loss': 5.049302577972412, 'test/num_examples': 10000, 'score': 2136.6385333538055, 'total_duration': 2379.705213546753, 'accumulated_submission_time': 2136.6385333538055, 'accumulated_eval_time': 242.7034797668457, 'accumulated_logging_time': 0.12747573852539062, 'global_step': 4522, 'preemption_count': 0}), (5441, {'train/accuracy': 0.19896483421325684, 'train/loss': 4.309050559997559, 'validation/accuracy': 0.1846799999475479, 'validation/loss': 4.396907806396484, 'validation/num_examples': 50000, 'test/accuracy': 0.13460001349449158, 'test/loss': 4.739532470703125, 'test/num_examples': 10000, 'score': 2556.660418987274, 'total_duration': 2840.099986076355, 'accumulated_submission_time': 2556.660418987274, 'accumulated_eval_time': 283.0039734840393, 'accumulated_logging_time': 0.15241026878356934, 'global_step': 5441, 'preemption_count': 0}), (6362, {'train/accuracy': 0.24464842677116394, 'train/loss': 3.974132537841797, 'validation/accuracy': 0.2258399873971939, 'validation/loss': 4.073367118835449, 'validation/num_examples': 50000, 'test/accuracy': 0.1720000058412552, 'test/loss': 4.464690208435059, 'test/num_examples': 10000, 'score': 2976.674590110779, 'total_duration': 3302.356697320938, 'accumulated_submission_time': 2976.674590110779, 'accumulated_eval_time': 325.17077374458313, 'accumulated_logging_time': 0.18027997016906738, 'global_step': 6362, 'preemption_count': 0}), (7286, {'train/accuracy': 0.29001951217651367, 'train/loss': 3.6638667583465576, 'validation/accuracy': 0.2622399926185608, 'validation/loss': 3.812857151031494, 'validation/num_examples': 50000, 'test/accuracy': 0.1998000144958496, 'test/loss': 4.2360687255859375, 'test/num_examples': 10000, 'score': 3396.9373681545258, 'total_duration': 3763.5866770744324, 'accumulated_submission_time': 3396.9373681545258, 'accumulated_eval_time': 366.0614733695984, 'accumulated_logging_time': 0.20946073532104492, 'global_step': 7286, 'preemption_count': 0}), (8209, {'train/accuracy': 0.3228124976158142, 'train/loss': 3.440293550491333, 'validation/accuracy': 0.30069997906684875, 'validation/loss': 3.549184560775757, 'validation/num_examples': 50000, 'test/accuracy': 0.233800008893013, 'test/loss': 4.013123989105225, 'test/num_examples': 10000, 'score': 3817.3410184383392, 'total_duration': 4223.172071218491, 'accumulated_submission_time': 3817.3410184383392, 'accumulated_eval_time': 405.1697099208832, 'accumulated_logging_time': 0.23502397537231445, 'global_step': 8209, 'preemption_count': 0}), (9131, {'train/accuracy': 0.36238279938697815, 'train/loss': 3.1926872730255127, 'validation/accuracy': 0.3360999822616577, 'validation/loss': 3.323276996612549, 'validation/num_examples': 50000, 'test/accuracy': 0.2590000033378601, 'test/loss': 3.8314473628997803, 'test/num_examples': 10000, 'score': 4237.481600284576, 'total_duration': 4684.190475702286, 'accumulated_submission_time': 4237.481600284576, 'accumulated_eval_time': 445.9749083518982, 'accumulated_logging_time': 0.2604801654815674, 'global_step': 9131, 'preemption_count': 0}), (10053, {'train/accuracy': 0.394843727350235, 'train/loss': 3.041377305984497, 'validation/accuracy': 0.36329999566078186, 'validation/loss': 3.2022409439086914, 'validation/num_examples': 50000, 'test/accuracy': 0.28220000863075256, 'test/loss': 3.700833797454834, 'test/num_examples': 10000, 'score': 4657.512858867645, 'total_duration': 5147.245245218277, 'accumulated_submission_time': 4657.512858867645, 'accumulated_eval_time': 488.92419385910034, 'accumulated_logging_time': 0.28647589683532715, 'global_step': 10053, 'preemption_count': 0}), (10977, {'train/accuracy': 0.4287695288658142, 'train/loss': 2.809704542160034, 'validation/accuracy': 0.3992599844932556, 'validation/loss': 2.950516700744629, 'validation/num_examples': 50000, 'test/accuracy': 0.30650001764297485, 'test/loss': 3.502833366394043, 'test/num_examples': 10000, 'score': 5077.823751926422, 'total_duration': 5608.130947113037, 'accumulated_submission_time': 5077.823751926422, 'accumulated_eval_time': 529.4197680950165, 'accumulated_logging_time': 0.31799912452697754, 'global_step': 10977, 'preemption_count': 0}), (11899, {'train/accuracy': 0.461249977350235, 'train/loss': 2.6188957691192627, 'validation/accuracy': 0.42489999532699585, 'validation/loss': 2.7885499000549316, 'validation/num_examples': 50000, 'test/accuracy': 0.3294000029563904, 'test/loss': 3.3375587463378906, 'test/num_examples': 10000, 'score': 5497.804342985153, 'total_duration': 6068.620263576508, 'accumulated_submission_time': 5497.804342985153, 'accumulated_eval_time': 569.8549757003784, 'accumulated_logging_time': 0.34359264373779297, 'global_step': 11899, 'preemption_count': 0}), (12820, {'train/accuracy': 0.4747070074081421, 'train/loss': 2.617384910583496, 'validation/accuracy': 0.4347599744796753, 'validation/loss': 2.79663348197937, 'validation/num_examples': 50000, 'test/accuracy': 0.33800002932548523, 'test/loss': 3.348545789718628, 'test/num_examples': 10000, 'score': 5918.146535158157, 'total_duration': 6531.268862962723, 'accumulated_submission_time': 5918.146535158157, 'accumulated_eval_time': 612.0777878761292, 'accumulated_logging_time': 0.3792257308959961, 'global_step': 12820, 'preemption_count': 0}), (13743, {'train/accuracy': 0.4976562261581421, 'train/loss': 2.4742987155914307, 'validation/accuracy': 0.4613799750804901, 'validation/loss': 2.645811080932617, 'validation/num_examples': 50000, 'test/accuracy': 0.3629000186920166, 'test/loss': 3.206737995147705, 'test/num_examples': 10000, 'score': 6338.404702425003, 'total_duration': 6990.997879266739, 'accumulated_submission_time': 6338.404702425003, 'accumulated_eval_time': 651.4733724594116, 'accumulated_logging_time': 0.4065427780151367, 'global_step': 13743, 'preemption_count': 0}), (14665, {'train/accuracy': 0.5162500143051147, 'train/loss': 2.3231639862060547, 'validation/accuracy': 0.4807399809360504, 'validation/loss': 2.4985995292663574, 'validation/num_examples': 50000, 'test/accuracy': 0.3760000169277191, 'test/loss': 3.0853447914123535, 'test/num_examples': 10000, 'score': 6758.347489833832, 'total_duration': 7452.263117074966, 'accumulated_submission_time': 6758.347489833832, 'accumulated_eval_time': 692.7202491760254, 'accumulated_logging_time': 0.43558502197265625, 'global_step': 14665, 'preemption_count': 0}), (15584, {'train/accuracy': 0.5301562547683716, 'train/loss': 2.32466983795166, 'validation/accuracy': 0.48985999822616577, 'validation/loss': 2.5129311084747314, 'validation/num_examples': 50000, 'test/accuracy': 0.38200002908706665, 'test/loss': 3.095673084259033, 'test/num_examples': 10000, 'score': 7178.313052892685, 'total_duration': 7910.607768535614, 'accumulated_submission_time': 7178.313052892685, 'accumulated_eval_time': 731.0242302417755, 'accumulated_logging_time': 0.4632751941680908, 'global_step': 15584, 'preemption_count': 0}), (16507, {'train/accuracy': 0.564160168170929, 'train/loss': 2.104198932647705, 'validation/accuracy': 0.505620002746582, 'validation/loss': 2.3738718032836914, 'validation/num_examples': 50000, 'test/accuracy': 0.3920000195503235, 'test/loss': 2.980851411819458, 'test/num_examples': 10000, 'score': 7598.365840673447, 'total_duration': 8369.278602838516, 'accumulated_submission_time': 7598.365840673447, 'accumulated_eval_time': 769.5679693222046, 'accumulated_logging_time': 0.48979949951171875, 'global_step': 16507, 'preemption_count': 0}), (17428, {'train/accuracy': 0.5541601181030273, 'train/loss': 2.172273874282837, 'validation/accuracy': 0.5168799757957458, 'validation/loss': 2.334871292114258, 'validation/num_examples': 50000, 'test/accuracy': 0.41030001640319824, 'test/loss': 2.9189651012420654, 'test/num_examples': 10000, 'score': 8018.321758508682, 'total_duration': 8827.351588010788, 'accumulated_submission_time': 8018.321758508682, 'accumulated_eval_time': 807.6075274944305, 'accumulated_logging_time': 0.5183203220367432, 'global_step': 17428, 'preemption_count': 0}), (18351, {'train/accuracy': 0.5694726705551147, 'train/loss': 2.0965421199798584, 'validation/accuracy': 0.5231199860572815, 'validation/loss': 2.2878384590148926, 'validation/num_examples': 50000, 'test/accuracy': 0.41290003061294556, 'test/loss': 2.89074444770813, 'test/num_examples': 10000, 'score': 8438.259444952011, 'total_duration': 9290.145105838776, 'accumulated_submission_time': 8438.259444952011, 'accumulated_eval_time': 850.389208316803, 'accumulated_logging_time': 0.544304609298706, 'global_step': 18351, 'preemption_count': 0}), (19264, {'train/accuracy': 0.5887304544448853, 'train/loss': 1.9993259906768799, 'validation/accuracy': 0.5343999862670898, 'validation/loss': 2.2401058673858643, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.851503372192383, 'test/num_examples': 10000, 'score': 8858.576433181763, 'total_duration': 9747.34814119339, 'accumulated_submission_time': 8858.576433181763, 'accumulated_eval_time': 887.2015011310577, 'accumulated_logging_time': 0.5706558227539062, 'global_step': 19264, 'preemption_count': 0}), (20183, {'train/accuracy': 0.5824609398841858, 'train/loss': 2.048027515411377, 'validation/accuracy': 0.540619969367981, 'validation/loss': 2.2298035621643066, 'validation/num_examples': 50000, 'test/accuracy': 0.42600002884864807, 'test/loss': 2.823843002319336, 'test/num_examples': 10000, 'score': 9278.755592107773, 'total_duration': 10209.370984315872, 'accumulated_submission_time': 9278.755592107773, 'accumulated_eval_time': 928.9679560661316, 'accumulated_logging_time': 0.6004471778869629, 'global_step': 20183, 'preemption_count': 0}), (21105, {'train/accuracy': 0.5959179401397705, 'train/loss': 1.9627938270568848, 'validation/accuracy': 0.5534799695014954, 'validation/loss': 2.157258987426758, 'validation/num_examples': 50000, 'test/accuracy': 0.4392000138759613, 'test/loss': 2.7591779232025146, 'test/num_examples': 10000, 'score': 9699.127198934555, 'total_duration': 10674.098667144775, 'accumulated_submission_time': 9699.127198934555, 'accumulated_eval_time': 973.2437303066254, 'accumulated_logging_time': 0.6334230899810791, 'global_step': 21105, 'preemption_count': 0}), (22026, {'train/accuracy': 0.6109570264816284, 'train/loss': 1.8311398029327393, 'validation/accuracy': 0.5616799592971802, 'validation/loss': 2.0698909759521484, 'validation/num_examples': 50000, 'test/accuracy': 0.43870002031326294, 'test/loss': 2.6716549396514893, 'test/num_examples': 10000, 'score': 10119.155968666077, 'total_duration': 11133.25730252266, 'accumulated_submission_time': 10119.155968666077, 'accumulated_eval_time': 1012.2991769313812, 'accumulated_logging_time': 0.6604936122894287, 'global_step': 22026, 'preemption_count': 0}), (22949, {'train/accuracy': 0.6087695360183716, 'train/loss': 1.8961181640625, 'validation/accuracy': 0.5664600133895874, 'validation/loss': 2.0866317749023438, 'validation/num_examples': 50000, 'test/accuracy': 0.450300008058548, 'test/loss': 2.7123382091522217, 'test/num_examples': 10000, 'score': 10539.468144655228, 'total_duration': 11591.783080339432, 'accumulated_submission_time': 10539.468144655228, 'accumulated_eval_time': 1050.4350891113281, 'accumulated_logging_time': 0.6903214454650879, 'global_step': 22949, 'preemption_count': 0}), (23869, {'train/accuracy': 0.6216992139816284, 'train/loss': 1.8485908508300781, 'validation/accuracy': 0.5771799683570862, 'validation/loss': 2.052497148513794, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.6749086380004883, 'test/num_examples': 10000, 'score': 10959.501294612885, 'total_duration': 12054.205533742905, 'accumulated_submission_time': 10959.501294612885, 'accumulated_eval_time': 1092.7479412555695, 'accumulated_logging_time': 0.7183361053466797, 'global_step': 23869, 'preemption_count': 0}), (24790, {'train/accuracy': 0.6266406178474426, 'train/loss': 1.8320873975753784, 'validation/accuracy': 0.5758000016212463, 'validation/loss': 2.063634157180786, 'validation/num_examples': 50000, 'test/accuracy': 0.46060001850128174, 'test/loss': 2.6689186096191406, 'test/num_examples': 10000, 'score': 11379.689157009125, 'total_duration': 12514.022708654404, 'accumulated_submission_time': 11379.689157009125, 'accumulated_eval_time': 1132.2981944084167, 'accumulated_logging_time': 0.7464418411254883, 'global_step': 24790, 'preemption_count': 0}), (25709, {'train/accuracy': 0.6539843678474426, 'train/loss': 1.6860125064849854, 'validation/accuracy': 0.5881400108337402, 'validation/loss': 1.9701340198516846, 'validation/num_examples': 50000, 'test/accuracy': 0.4701000154018402, 'test/loss': 2.585104465484619, 'test/num_examples': 10000, 'score': 11799.745354413986, 'total_duration': 12974.617196798325, 'accumulated_submission_time': 11799.745354413986, 'accumulated_eval_time': 1172.748398065567, 'accumulated_logging_time': 0.7861979007720947, 'global_step': 25709, 'preemption_count': 0}), (26630, {'train/accuracy': 0.6346874833106995, 'train/loss': 1.760985255241394, 'validation/accuracy': 0.5895199775695801, 'validation/loss': 1.9683057069778442, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.573791742324829, 'test/num_examples': 10000, 'score': 12219.731608867645, 'total_duration': 13436.014949560165, 'accumulated_submission_time': 12219.731608867645, 'accumulated_eval_time': 1214.0835967063904, 'accumulated_logging_time': 0.8152399063110352, 'global_step': 26630, 'preemption_count': 0}), (27553, {'train/accuracy': 0.6429687142372131, 'train/loss': 1.7258527278900146, 'validation/accuracy': 0.5952999591827393, 'validation/loss': 1.9520374536514282, 'validation/num_examples': 50000, 'test/accuracy': 0.4765000343322754, 'test/loss': 2.5679101943969727, 'test/num_examples': 10000, 'score': 12639.99943780899, 'total_duration': 13899.052494764328, 'accumulated_submission_time': 12639.99943780899, 'accumulated_eval_time': 1256.7773969173431, 'accumulated_logging_time': 0.8430163860321045, 'global_step': 27553, 'preemption_count': 0}), (28476, {'train/accuracy': 0.6584765315055847, 'train/loss': 1.6636217832565308, 'validation/accuracy': 0.595579981803894, 'validation/loss': 1.949023962020874, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.56640625, 'test/num_examples': 10000, 'score': 13060.122702360153, 'total_duration': 14358.513455867767, 'accumulated_submission_time': 13060.122702360153, 'accumulated_eval_time': 1296.0321819782257, 'accumulated_logging_time': 0.8778200149536133, 'global_step': 28476, 'preemption_count': 0}), (29396, {'train/accuracy': 0.6520312428474426, 'train/loss': 1.674699306488037, 'validation/accuracy': 0.6082800030708313, 'validation/loss': 1.8775641918182373, 'validation/num_examples': 50000, 'test/accuracy': 0.48920002579689026, 'test/loss': 2.5049943923950195, 'test/num_examples': 10000, 'score': 13480.046933412552, 'total_duration': 14819.881961107254, 'accumulated_submission_time': 13480.046933412552, 'accumulated_eval_time': 1337.395380973816, 'accumulated_logging_time': 0.9113750457763672, 'global_step': 29396, 'preemption_count': 0}), (30317, {'train/accuracy': 0.6537694931030273, 'train/loss': 1.6482510566711426, 'validation/accuracy': 0.6024599671363831, 'validation/loss': 1.8747574090957642, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.4801666736602783, 'test/num_examples': 10000, 'score': 13900.2150182724, 'total_duration': 15285.852749586105, 'accumulated_submission_time': 13900.2150182724, 'accumulated_eval_time': 1383.1154959201813, 'accumulated_logging_time': 0.945807695388794, 'global_step': 30317, 'preemption_count': 0}), (31241, {'train/accuracy': 0.6695898175239563, 'train/loss': 1.5669158697128296, 'validation/accuracy': 0.6098799705505371, 'validation/loss': 1.8242343664169312, 'validation/num_examples': 50000, 'test/accuracy': 0.49480003118515015, 'test/loss': 2.441969156265259, 'test/num_examples': 10000, 'score': 14320.48095369339, 'total_duration': 15745.813037872314, 'accumulated_submission_time': 14320.48095369339, 'accumulated_eval_time': 1422.7271156311035, 'accumulated_logging_time': 0.9807932376861572, 'global_step': 31241, 'preemption_count': 0}), (32162, {'train/accuracy': 0.6646093726158142, 'train/loss': 1.623708724975586, 'validation/accuracy': 0.616159975528717, 'validation/loss': 1.8413524627685547, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.439016103744507, 'test/num_examples': 10000, 'score': 14740.702543497086, 'total_duration': 16209.5774371624, 'accumulated_submission_time': 14740.702543497086, 'accumulated_eval_time': 1466.1945168972015, 'accumulated_logging_time': 1.0090515613555908, 'global_step': 32162, 'preemption_count': 0}), (33084, {'train/accuracy': 0.6647070050239563, 'train/loss': 1.6350743770599365, 'validation/accuracy': 0.6173399686813354, 'validation/loss': 1.8481569290161133, 'validation/num_examples': 50000, 'test/accuracy': 0.49470001459121704, 'test/loss': 2.4732367992401123, 'test/num_examples': 10000, 'score': 15160.87868309021, 'total_duration': 16668.039219379425, 'accumulated_submission_time': 15160.87868309021, 'accumulated_eval_time': 1504.4031112194061, 'accumulated_logging_time': 1.0377919673919678, 'global_step': 33084, 'preemption_count': 0}), (34004, {'train/accuracy': 0.6825194954872131, 'train/loss': 1.5208829641342163, 'validation/accuracy': 0.6213200092315674, 'validation/loss': 1.7882128953933716, 'validation/num_examples': 50000, 'test/accuracy': 0.506600022315979, 'test/loss': 2.3888542652130127, 'test/num_examples': 10000, 'score': 15581.12509894371, 'total_duration': 17127.568870782852, 'accumulated_submission_time': 15581.12509894371, 'accumulated_eval_time': 1543.6040349006653, 'accumulated_logging_time': 1.0717051029205322, 'global_step': 34004, 'preemption_count': 0}), (34924, {'train/accuracy': 0.6859960556030273, 'train/loss': 1.5173721313476562, 'validation/accuracy': 0.6280999779701233, 'validation/loss': 1.7702747583389282, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.4008095264434814, 'test/num_examples': 10000, 'score': 16001.194901704788, 'total_duration': 17587.93961954117, 'accumulated_submission_time': 16001.194901704788, 'accumulated_eval_time': 1583.8245911598206, 'accumulated_logging_time': 1.1039619445800781, 'global_step': 34924, 'preemption_count': 0}), (35848, {'train/accuracy': 0.6784374713897705, 'train/loss': 1.537048101425171, 'validation/accuracy': 0.6295599937438965, 'validation/loss': 1.763940691947937, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.3965654373168945, 'test/num_examples': 10000, 'score': 16421.54888010025, 'total_duration': 18051.60387778282, 'accumulated_submission_time': 16421.54888010025, 'accumulated_eval_time': 1627.0497500896454, 'accumulated_logging_time': 1.141261339187622, 'global_step': 35848, 'preemption_count': 0}), (36770, {'train/accuracy': 0.6861132383346558, 'train/loss': 1.495723843574524, 'validation/accuracy': 0.6327999830245972, 'validation/loss': 1.7372503280639648, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.3624677658081055, 'test/num_examples': 10000, 'score': 16841.60574913025, 'total_duration': 18511.348565340042, 'accumulated_submission_time': 16841.60574913025, 'accumulated_eval_time': 1666.6609530448914, 'accumulated_logging_time': 1.1702823638916016, 'global_step': 36770, 'preemption_count': 0}), (37690, {'train/accuracy': 0.7042187452316284, 'train/loss': 1.429381012916565, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.742835283279419, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.347130537033081, 'test/num_examples': 10000, 'score': 17261.76012301445, 'total_duration': 18975.477598905563, 'accumulated_submission_time': 17261.76012301445, 'accumulated_eval_time': 1710.5507550239563, 'accumulated_logging_time': 1.207282543182373, 'global_step': 37690, 'preemption_count': 0}), (38611, {'train/accuracy': 0.6838085651397705, 'train/loss': 1.5368809700012207, 'validation/accuracy': 0.6318199634552002, 'validation/loss': 1.7639278173446655, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.3935165405273438, 'test/num_examples': 10000, 'score': 17681.86350107193, 'total_duration': 19432.764481782913, 'accumulated_submission_time': 17681.86350107193, 'accumulated_eval_time': 1747.6528568267822, 'accumulated_logging_time': 1.2410552501678467, 'global_step': 38611, 'preemption_count': 0}), (39530, {'train/accuracy': 0.6908202767372131, 'train/loss': 1.493054986000061, 'validation/accuracy': 0.6378999948501587, 'validation/loss': 1.7385770082473755, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.360651969909668, 'test/num_examples': 10000, 'score': 18101.93009710312, 'total_duration': 19894.123265028, 'accumulated_submission_time': 18101.93009710312, 'accumulated_eval_time': 1788.8630316257477, 'accumulated_logging_time': 1.2756617069244385, 'global_step': 39530, 'preemption_count': 0}), (40446, {'train/accuracy': 0.7030664086341858, 'train/loss': 1.4640662670135498, 'validation/accuracy': 0.6423999667167664, 'validation/loss': 1.7443610429763794, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.3539485931396484, 'test/num_examples': 10000, 'score': 18521.99777531624, 'total_duration': 20353.072145223618, 'accumulated_submission_time': 18521.99777531624, 'accumulated_eval_time': 1827.666071653366, 'accumulated_logging_time': 1.3061726093292236, 'global_step': 40446, 'preemption_count': 0}), (41368, {'train/accuracy': 0.6928125023841858, 'train/loss': 1.5066217184066772, 'validation/accuracy': 0.6416199803352356, 'validation/loss': 1.7189924716949463, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.3392860889434814, 'test/num_examples': 10000, 'score': 18942.32962703705, 'total_duration': 20815.592352867126, 'accumulated_submission_time': 18942.32962703705, 'accumulated_eval_time': 1869.7756474018097, 'accumulated_logging_time': 1.337547779083252, 'global_step': 41368, 'preemption_count': 0}), (42290, {'train/accuracy': 0.6962890625, 'train/loss': 1.4691346883773804, 'validation/accuracy': 0.6421599984169006, 'validation/loss': 1.710872769355774, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.330148935317993, 'test/num_examples': 10000, 'score': 19362.287507772446, 'total_duration': 21281.26524925232, 'accumulated_submission_time': 19362.287507772446, 'accumulated_eval_time': 1915.4070200920105, 'accumulated_logging_time': 1.3723368644714355, 'global_step': 42290, 'preemption_count': 0}), (43213, {'train/accuracy': 0.707324206829071, 'train/loss': 1.4024572372436523, 'validation/accuracy': 0.647159993648529, 'validation/loss': 1.6708543300628662, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.284856081008911, 'test/num_examples': 10000, 'score': 19782.306488990784, 'total_duration': 21741.00474333763, 'accumulated_submission_time': 19782.306488990784, 'accumulated_eval_time': 1955.0337710380554, 'accumulated_logging_time': 1.4100327491760254, 'global_step': 43213, 'preemption_count': 0}), (44135, {'train/accuracy': 0.6955273151397705, 'train/loss': 1.4521766901016235, 'validation/accuracy': 0.6490199565887451, 'validation/loss': 1.6639647483825684, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.3044726848602295, 'test/num_examples': 10000, 'score': 20202.631172180176, 'total_duration': 22203.795504808426, 'accumulated_submission_time': 20202.631172180176, 'accumulated_eval_time': 1997.4169154167175, 'accumulated_logging_time': 1.4440712928771973, 'global_step': 44135, 'preemption_count': 0}), (45055, {'train/accuracy': 0.698925793170929, 'train/loss': 1.434710144996643, 'validation/accuracy': 0.647379994392395, 'validation/loss': 1.6674917936325073, 'validation/num_examples': 50000, 'test/accuracy': 0.5235000252723694, 'test/loss': 2.2860796451568604, 'test/num_examples': 10000, 'score': 20622.772392749786, 'total_duration': 22665.36895251274, 'accumulated_submission_time': 20622.772392749786, 'accumulated_eval_time': 2038.766408443451, 'accumulated_logging_time': 1.4792227745056152, 'global_step': 45055, 'preemption_count': 0}), (45974, {'train/accuracy': 0.7076953053474426, 'train/loss': 1.4150549173355103, 'validation/accuracy': 0.6478599905967712, 'validation/loss': 1.678601622581482, 'validation/num_examples': 50000, 'test/accuracy': 0.5283000469207764, 'test/loss': 2.2812087535858154, 'test/num_examples': 10000, 'score': 21043.10510325432, 'total_duration': 23129.006335258484, 'accumulated_submission_time': 21043.10510325432, 'accumulated_eval_time': 2081.991260766983, 'accumulated_logging_time': 1.511063575744629, 'global_step': 45974, 'preemption_count': 0}), (46895, {'train/accuracy': 0.7375390529632568, 'train/loss': 1.2977427244186401, 'validation/accuracy': 0.6542400121688843, 'validation/loss': 1.6448653936386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.2646377086639404, 'test/num_examples': 10000, 'score': 21463.18249320984, 'total_duration': 23590.884415626526, 'accumulated_submission_time': 21463.18249320984, 'accumulated_eval_time': 2123.7109668254852, 'accumulated_logging_time': 1.544229507446289, 'global_step': 46895, 'preemption_count': 0}), (47818, {'train/accuracy': 0.7093945145606995, 'train/loss': 1.4072017669677734, 'validation/accuracy': 0.6576399803161621, 'validation/loss': 1.6411067247390747, 'validation/num_examples': 50000, 'test/accuracy': 0.5327000021934509, 'test/loss': 2.245063304901123, 'test/num_examples': 10000, 'score': 21883.532969236374, 'total_duration': 24053.848252534866, 'accumulated_submission_time': 21883.532969236374, 'accumulated_eval_time': 2166.239804506302, 'accumulated_logging_time': 1.5809102058410645, 'global_step': 47818, 'preemption_count': 0}), (48741, {'train/accuracy': 0.7172656059265137, 'train/loss': 1.3772408962249756, 'validation/accuracy': 0.6559399962425232, 'validation/loss': 1.6439871788024902, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.262031078338623, 'test/num_examples': 10000, 'score': 22303.59793663025, 'total_duration': 24516.407782793045, 'accumulated_submission_time': 22303.59793663025, 'accumulated_eval_time': 2208.6510157585144, 'accumulated_logging_time': 1.6165921688079834, 'global_step': 48741, 'preemption_count': 0}), (49664, {'train/accuracy': 0.71875, 'train/loss': 1.4258836507797241, 'validation/accuracy': 0.6511600017547607, 'validation/loss': 1.7147786617279053, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.343284845352173, 'test/num_examples': 10000, 'score': 22723.800693511963, 'total_duration': 24980.77202296257, 'accumulated_submission_time': 22723.800693511963, 'accumulated_eval_time': 2252.7241654396057, 'accumulated_logging_time': 1.656675100326538, 'global_step': 49664, 'preemption_count': 0}), (50586, {'train/accuracy': 0.7098437547683716, 'train/loss': 1.4361015558242798, 'validation/accuracy': 0.6548399925231934, 'validation/loss': 1.6722317934036255, 'validation/num_examples': 50000, 'test/accuracy': 0.5351999998092651, 'test/loss': 2.278514862060547, 'test/num_examples': 10000, 'score': 23143.94828104973, 'total_duration': 25443.989156007767, 'accumulated_submission_time': 23143.94828104973, 'accumulated_eval_time': 2295.7085387706757, 'accumulated_logging_time': 1.69374680519104, 'global_step': 50586, 'preemption_count': 0}), (51508, {'train/accuracy': 0.71498042345047, 'train/loss': 1.3889296054840088, 'validation/accuracy': 0.6620399951934814, 'validation/loss': 1.6310160160064697, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.247091293334961, 'test/num_examples': 10000, 'score': 23564.28750729561, 'total_duration': 25905.883530139923, 'accumulated_submission_time': 23564.28750729561, 'accumulated_eval_time': 2337.176256418228, 'accumulated_logging_time': 1.7330005168914795, 'global_step': 51508, 'preemption_count': 0}), (52432, {'train/accuracy': 0.7337304353713989, 'train/loss': 1.2980376482009888, 'validation/accuracy': 0.6647999882698059, 'validation/loss': 1.5864545106887817, 'validation/num_examples': 50000, 'test/accuracy': 0.5354000329971313, 'test/loss': 2.2272799015045166, 'test/num_examples': 10000, 'score': 23984.374716758728, 'total_duration': 26366.565184354782, 'accumulated_submission_time': 23984.374716758728, 'accumulated_eval_time': 2377.6872231960297, 'accumulated_logging_time': 1.7685375213623047, 'global_step': 52432, 'preemption_count': 0}), (53354, {'train/accuracy': 0.7135351300239563, 'train/loss': 1.3821560144424438, 'validation/accuracy': 0.6652799844741821, 'validation/loss': 1.5981616973876953, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.2386462688446045, 'test/num_examples': 10000, 'score': 24404.365221500397, 'total_duration': 26826.81566309929, 'accumulated_submission_time': 24404.365221500397, 'accumulated_eval_time': 2417.860870361328, 'accumulated_logging_time': 1.8072423934936523, 'global_step': 53354, 'preemption_count': 0}), (54275, {'train/accuracy': 0.7189062237739563, 'train/loss': 1.361011028289795, 'validation/accuracy': 0.6606799960136414, 'validation/loss': 1.6143720149993896, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.222517967224121, 'test/num_examples': 10000, 'score': 24824.307297229767, 'total_duration': 27290.06626176834, 'accumulated_submission_time': 24824.307297229767, 'accumulated_eval_time': 2461.081460237503, 'accumulated_logging_time': 1.847649097442627, 'global_step': 54275, 'preemption_count': 0}), (55196, {'train/accuracy': 0.7296484112739563, 'train/loss': 1.3426628112792969, 'validation/accuracy': 0.664139986038208, 'validation/loss': 1.6135691404342651, 'validation/num_examples': 50000, 'test/accuracy': 0.5435000061988831, 'test/loss': 2.220703601837158, 'test/num_examples': 10000, 'score': 25244.552402496338, 'total_duration': 27749.66876244545, 'accumulated_submission_time': 25244.552402496338, 'accumulated_eval_time': 2500.354782104492, 'accumulated_logging_time': 1.8835597038269043, 'global_step': 55196, 'preemption_count': 0}), (56120, {'train/accuracy': 0.7380273342132568, 'train/loss': 1.2842124700546265, 'validation/accuracy': 0.6672599911689758, 'validation/loss': 1.5820810794830322, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.1891727447509766, 'test/num_examples': 10000, 'score': 25664.747946739197, 'total_duration': 28215.844392061234, 'accumulated_submission_time': 25664.747946739197, 'accumulated_eval_time': 2546.253002643585, 'accumulated_logging_time': 1.9170572757720947, 'global_step': 56120, 'preemption_count': 0}), (57042, {'train/accuracy': 0.7276562452316284, 'train/loss': 1.3468148708343506, 'validation/accuracy': 0.6674000024795532, 'validation/loss': 1.603977918624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5464000105857849, 'test/loss': 2.211744546890259, 'test/num_examples': 10000, 'score': 26085.003512620926, 'total_duration': 28679.673223495483, 'accumulated_submission_time': 26085.003512620926, 'accumulated_eval_time': 2589.74267745018, 'accumulated_logging_time': 1.9526276588439941, 'global_step': 57042, 'preemption_count': 0}), (57963, {'train/accuracy': 0.7269140481948853, 'train/loss': 1.328726887702942, 'validation/accuracy': 0.667419970035553, 'validation/loss': 1.5865769386291504, 'validation/num_examples': 50000, 'test/accuracy': 0.5430999994277954, 'test/loss': 2.196040153503418, 'test/num_examples': 10000, 'score': 26505.091208696365, 'total_duration': 29141.391778469086, 'accumulated_submission_time': 26505.091208696365, 'accumulated_eval_time': 2631.2893874645233, 'accumulated_logging_time': 1.9895341396331787, 'global_step': 57963, 'preemption_count': 0}), (58883, {'train/accuracy': 0.7373241782188416, 'train/loss': 1.2906686067581177, 'validation/accuracy': 0.6669600009918213, 'validation/loss': 1.6132616996765137, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.2018980979919434, 'test/num_examples': 10000, 'score': 26925.22168302536, 'total_duration': 29605.44416475296, 'accumulated_submission_time': 26925.22168302536, 'accumulated_eval_time': 2675.127780675888, 'accumulated_logging_time': 2.025451183319092, 'global_step': 58883, 'preemption_count': 0}), (59803, {'train/accuracy': 0.7282031178474426, 'train/loss': 1.2871874570846558, 'validation/accuracy': 0.6700199842453003, 'validation/loss': 1.5382176637649536, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.153144598007202, 'test/num_examples': 10000, 'score': 27345.44739818573, 'total_duration': 30069.56980085373, 'accumulated_submission_time': 27345.44739818573, 'accumulated_eval_time': 2718.944101333618, 'accumulated_logging_time': 2.061138868331909, 'global_step': 59803, 'preemption_count': 0}), (60724, {'train/accuracy': 0.733593761920929, 'train/loss': 1.3361026048660278, 'validation/accuracy': 0.6747199892997742, 'validation/loss': 1.5849941968917847, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.2170250415802, 'test/num_examples': 10000, 'score': 27765.797261953354, 'total_duration': 30533.50876736641, 'accumulated_submission_time': 27765.797261953354, 'accumulated_eval_time': 2762.44873046875, 'accumulated_logging_time': 2.0978400707244873, 'global_step': 60724, 'preemption_count': 0}), (61645, {'train/accuracy': 0.7450000047683716, 'train/loss': 1.2544151544570923, 'validation/accuracy': 0.6762799620628357, 'validation/loss': 1.5565282106399536, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.175717830657959, 'test/num_examples': 10000, 'score': 28185.988286972046, 'total_duration': 30996.818990707397, 'accumulated_submission_time': 28185.988286972046, 'accumulated_eval_time': 2805.484763622284, 'accumulated_logging_time': 2.13291072845459, 'global_step': 61645, 'preemption_count': 0}), (62563, {'train/accuracy': 0.7283593416213989, 'train/loss': 1.3276382684707642, 'validation/accuracy': 0.6751199960708618, 'validation/loss': 1.5634821653366089, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.1780121326446533, 'test/num_examples': 10000, 'score': 28606.290912389755, 'total_duration': 31461.618797063828, 'accumulated_submission_time': 28606.290912389755, 'accumulated_eval_time': 2849.9011821746826, 'accumulated_logging_time': 2.1665310859680176, 'global_step': 62563, 'preemption_count': 0}), (63483, {'train/accuracy': 0.7360742092132568, 'train/loss': 1.2766711711883545, 'validation/accuracy': 0.6779199838638306, 'validation/loss': 1.5261484384536743, 'validation/num_examples': 50000, 'test/accuracy': 0.557200014591217, 'test/loss': 2.149172306060791, 'test/num_examples': 10000, 'score': 29026.310174703598, 'total_duration': 31926.710065841675, 'accumulated_submission_time': 29026.310174703598, 'accumulated_eval_time': 2894.8881330490112, 'accumulated_logging_time': 2.20430326461792, 'global_step': 63483, 'preemption_count': 0}), (64404, {'train/accuracy': 0.7432031035423279, 'train/loss': 1.2548317909240723, 'validation/accuracy': 0.6798799633979797, 'validation/loss': 1.5307791233062744, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.1536831855773926, 'test/num_examples': 10000, 'score': 29446.250809907913, 'total_duration': 32389.777686357498, 'accumulated_submission_time': 29446.250809907913, 'accumulated_eval_time': 2937.9205870628357, 'accumulated_logging_time': 2.25028133392334, 'global_step': 64404, 'preemption_count': 0}), (65324, {'train/accuracy': 0.7388671636581421, 'train/loss': 1.2673910856246948, 'validation/accuracy': 0.6782000064849854, 'validation/loss': 1.518659234046936, 'validation/num_examples': 50000, 'test/accuracy': 0.5658000111579895, 'test/loss': 2.10740065574646, 'test/num_examples': 10000, 'score': 29866.379588842392, 'total_duration': 32848.34146499634, 'accumulated_submission_time': 29866.379588842392, 'accumulated_eval_time': 2976.272282600403, 'accumulated_logging_time': 2.2853519916534424, 'global_step': 65324, 'preemption_count': 0}), (66244, {'train/accuracy': 0.7400780916213989, 'train/loss': 1.2585868835449219, 'validation/accuracy': 0.6813799738883972, 'validation/loss': 1.5165966749191284, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.1452724933624268, 'test/num_examples': 10000, 'score': 30286.45598578453, 'total_duration': 33312.4827747345, 'accumulated_submission_time': 30286.45598578453, 'accumulated_eval_time': 3020.252357006073, 'accumulated_logging_time': 2.3225948810577393, 'global_step': 66244, 'preemption_count': 0}), (67165, {'train/accuracy': 0.7415820360183716, 'train/loss': 1.2981585264205933, 'validation/accuracy': 0.6811999678611755, 'validation/loss': 1.5677835941314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.169353723526001, 'test/num_examples': 10000, 'score': 30706.626230478287, 'total_duration': 33773.69924163818, 'accumulated_submission_time': 30706.626230478287, 'accumulated_eval_time': 3061.212740421295, 'accumulated_logging_time': 2.3604607582092285, 'global_step': 67165, 'preemption_count': 0}), (68086, {'train/accuracy': 0.7633007764816284, 'train/loss': 1.1590244770050049, 'validation/accuracy': 0.6800999641418457, 'validation/loss': 1.5066139698028564, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.1319308280944824, 'test/num_examples': 10000, 'score': 31126.832008838654, 'total_duration': 34235.588967084885, 'accumulated_submission_time': 31126.832008838654, 'accumulated_eval_time': 3102.8121032714844, 'accumulated_logging_time': 2.3978052139282227, 'global_step': 68086, 'preemption_count': 0}), (69004, {'train/accuracy': 0.7406054735183716, 'train/loss': 1.2410019636154175, 'validation/accuracy': 0.684719979763031, 'validation/loss': 1.4892326593399048, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.1219170093536377, 'test/num_examples': 10000, 'score': 31546.80517745018, 'total_duration': 34698.98634314537, 'accumulated_submission_time': 31546.80517745018, 'accumulated_eval_time': 3146.148453235626, 'accumulated_logging_time': 2.4373722076416016, 'global_step': 69004, 'preemption_count': 0}), (69925, {'train/accuracy': 0.7484570145606995, 'train/loss': 1.2337993383407593, 'validation/accuracy': 0.6854599714279175, 'validation/loss': 1.5051804780960083, 'validation/num_examples': 50000, 'test/accuracy': 0.5660000443458557, 'test/loss': 2.104238510131836, 'test/num_examples': 10000, 'score': 31966.791477441788, 'total_duration': 35162.78013944626, 'accumulated_submission_time': 31966.791477441788, 'accumulated_eval_time': 3189.873576402664, 'accumulated_logging_time': 2.472073554992676, 'global_step': 69925, 'preemption_count': 0}), (70847, {'train/accuracy': 0.7518359422683716, 'train/loss': 1.2066234350204468, 'validation/accuracy': 0.6805199980735779, 'validation/loss': 1.5176000595092773, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 2.1310625076293945, 'test/num_examples': 10000, 'score': 32386.718663215637, 'total_duration': 35629.093203783035, 'accumulated_submission_time': 32386.718663215637, 'accumulated_eval_time': 3236.175390481949, 'accumulated_logging_time': 2.508272171020508, 'global_step': 70847, 'preemption_count': 0}), (71770, {'train/accuracy': 0.747363269329071, 'train/loss': 1.2343018054962158, 'validation/accuracy': 0.6899799704551697, 'validation/loss': 1.4829862117767334, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.08892560005188, 'test/num_examples': 10000, 'score': 32807.06958389282, 'total_duration': 36087.70830178261, 'accumulated_submission_time': 32807.06958389282, 'accumulated_eval_time': 3274.3536903858185, 'accumulated_logging_time': 2.5455551147460938, 'global_step': 71770, 'preemption_count': 0}), (72687, {'train/accuracy': 0.7515820264816284, 'train/loss': 1.2129734754562378, 'validation/accuracy': 0.6888999938964844, 'validation/loss': 1.4854317903518677, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0902061462402344, 'test/num_examples': 10000, 'score': 33227.219742536545, 'total_duration': 36553.0427236557, 'accumulated_submission_time': 33227.219742536545, 'accumulated_eval_time': 3319.4512207508087, 'accumulated_logging_time': 2.5845770835876465, 'global_step': 72687, 'preemption_count': 0}), (73609, {'train/accuracy': 0.7533984184265137, 'train/loss': 1.191983938217163, 'validation/accuracy': 0.6872400045394897, 'validation/loss': 1.485839605331421, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 2.1044228076934814, 'test/num_examples': 10000, 'score': 33647.15845131874, 'total_duration': 37015.739466905594, 'accumulated_submission_time': 33647.15845131874, 'accumulated_eval_time': 3362.12451672554, 'accumulated_logging_time': 2.621159076690674, 'global_step': 73609, 'preemption_count': 0}), (74530, {'train/accuracy': 0.7507421970367432, 'train/loss': 1.195424199104309, 'validation/accuracy': 0.6930800080299377, 'validation/loss': 1.4571106433868408, 'validation/num_examples': 50000, 'test/accuracy': 0.5658000111579895, 'test/loss': 2.0698626041412354, 'test/num_examples': 10000, 'score': 34067.44424152374, 'total_duration': 37478.309512376785, 'accumulated_submission_time': 34067.44424152374, 'accumulated_eval_time': 3404.320331096649, 'accumulated_logging_time': 2.6617767810821533, 'global_step': 74530, 'preemption_count': 0}), (75453, {'train/accuracy': 0.7525194883346558, 'train/loss': 1.1894110441207886, 'validation/accuracy': 0.6906599998474121, 'validation/loss': 1.454953908920288, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.06187105178833, 'test/num_examples': 10000, 'score': 34487.681963682175, 'total_duration': 37940.56576251984, 'accumulated_submission_time': 34487.681963682175, 'accumulated_eval_time': 3446.2519228458405, 'accumulated_logging_time': 2.700667142868042, 'global_step': 75453, 'preemption_count': 0}), (76374, {'train/accuracy': 0.7610741853713989, 'train/loss': 1.1607592105865479, 'validation/accuracy': 0.6957799792289734, 'validation/loss': 1.4432342052459717, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 2.0531435012817383, 'test/num_examples': 10000, 'score': 34908.01672363281, 'total_duration': 38401.809537410736, 'accumulated_submission_time': 34908.01672363281, 'accumulated_eval_time': 3487.0784368515015, 'accumulated_logging_time': 2.736494779586792, 'global_step': 76374, 'preemption_count': 0}), (77298, {'train/accuracy': 0.7732812166213989, 'train/loss': 1.1148550510406494, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.447901725769043, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 2.070373773574829, 'test/num_examples': 10000, 'score': 35327.95745301247, 'total_duration': 38863.893261671066, 'accumulated_submission_time': 35327.95745301247, 'accumulated_eval_time': 3529.1288471221924, 'accumulated_logging_time': 2.7815897464752197, 'global_step': 77298, 'preemption_count': 0}), (78220, {'train/accuracy': 0.7600781321525574, 'train/loss': 1.1587432622909546, 'validation/accuracy': 0.6990599632263184, 'validation/loss': 1.4184653759002686, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 2.019524097442627, 'test/num_examples': 10000, 'score': 35748.12657356262, 'total_duration': 39325.401894807816, 'accumulated_submission_time': 35748.12657356262, 'accumulated_eval_time': 3570.3713760375977, 'accumulated_logging_time': 2.8311731815338135, 'global_step': 78220, 'preemption_count': 0}), (79142, {'train/accuracy': 0.7625195384025574, 'train/loss': 1.148511528968811, 'validation/accuracy': 0.6945199966430664, 'validation/loss': 1.4347800016403198, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 2.055783987045288, 'test/num_examples': 10000, 'score': 36168.48544359207, 'total_duration': 39787.786851882935, 'accumulated_submission_time': 36168.48544359207, 'accumulated_eval_time': 3612.3097426891327, 'accumulated_logging_time': 2.8709871768951416, 'global_step': 79142, 'preemption_count': 0}), (80065, {'train/accuracy': 0.7742968797683716, 'train/loss': 1.1092896461486816, 'validation/accuracy': 0.697380006313324, 'validation/loss': 1.4453190565109253, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 2.067552328109741, 'test/num_examples': 10000, 'score': 36588.779423713684, 'total_duration': 40251.33292555809, 'accumulated_submission_time': 36588.779423713684, 'accumulated_eval_time': 3655.476496696472, 'accumulated_logging_time': 2.9082815647125244, 'global_step': 80065, 'preemption_count': 0}), (80988, {'train/accuracy': 0.7635741829872131, 'train/loss': 1.1445281505584717, 'validation/accuracy': 0.7017799615859985, 'validation/loss': 1.4090139865875244, 'validation/num_examples': 50000, 'test/accuracy': 0.5790000557899475, 'test/loss': 2.017435073852539, 'test/num_examples': 10000, 'score': 37008.99789881706, 'total_duration': 40713.225462675095, 'accumulated_submission_time': 37008.99789881706, 'accumulated_eval_time': 3697.0613508224487, 'accumulated_logging_time': 2.9498679637908936, 'global_step': 80988, 'preemption_count': 0}), (81909, {'train/accuracy': 0.7632030844688416, 'train/loss': 1.1502982378005981, 'validation/accuracy': 0.6981399655342102, 'validation/loss': 1.4378381967544556, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 2.034175157546997, 'test/num_examples': 10000, 'score': 37429.169956445694, 'total_duration': 41177.268661022186, 'accumulated_submission_time': 37429.169956445694, 'accumulated_eval_time': 3740.842449903488, 'accumulated_logging_time': 2.9926469326019287, 'global_step': 81909, 'preemption_count': 0}), (82831, {'train/accuracy': 0.7751367092132568, 'train/loss': 1.1169837713241577, 'validation/accuracy': 0.7000600099563599, 'validation/loss': 1.4290475845336914, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 2.0344953536987305, 'test/num_examples': 10000, 'score': 37849.39163827896, 'total_duration': 41643.815761089325, 'accumulated_submission_time': 37849.39163827896, 'accumulated_eval_time': 3787.0803208351135, 'accumulated_logging_time': 3.0316107273101807, 'global_step': 82831, 'preemption_count': 0}), (83754, {'train/accuracy': 0.7632226347923279, 'train/loss': 1.1648662090301514, 'validation/accuracy': 0.6999799609184265, 'validation/loss': 1.4325613975524902, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 2.039259433746338, 'test/num_examples': 10000, 'score': 38269.435145139694, 'total_duration': 42108.586940288544, 'accumulated_submission_time': 38269.435145139694, 'accumulated_eval_time': 3831.722299337387, 'accumulated_logging_time': 3.0690486431121826, 'global_step': 83754, 'preemption_count': 0}), (84676, {'train/accuracy': 0.7675585746765137, 'train/loss': 1.1792206764221191, 'validation/accuracy': 0.7015199661254883, 'validation/loss': 1.4529019594192505, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 2.06295108795166, 'test/num_examples': 10000, 'score': 38689.39722657204, 'total_duration': 42567.656548023224, 'accumulated_submission_time': 38689.39722657204, 'accumulated_eval_time': 3870.7454376220703, 'accumulated_logging_time': 3.1060070991516113, 'global_step': 84676, 'preemption_count': 0}), (85598, {'train/accuracy': 0.77259761095047, 'train/loss': 1.1098675727844238, 'validation/accuracy': 0.7027599811553955, 'validation/loss': 1.4117202758789062, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 2.0271904468536377, 'test/num_examples': 10000, 'score': 39109.64110136032, 'total_duration': 43032.52892065048, 'accumulated_submission_time': 39109.64110136032, 'accumulated_eval_time': 3915.285789489746, 'accumulated_logging_time': 3.146677255630493, 'global_step': 85598, 'preemption_count': 0}), (86519, {'train/accuracy': 0.7715820074081421, 'train/loss': 1.1164155006408691, 'validation/accuracy': 0.7065399885177612, 'validation/loss': 1.3979099988937378, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 2.0163145065307617, 'test/num_examples': 10000, 'score': 39529.84063959122, 'total_duration': 43499.65197634697, 'accumulated_submission_time': 39529.84063959122, 'accumulated_eval_time': 3962.1179814338684, 'accumulated_logging_time': 3.1898202896118164, 'global_step': 86519, 'preemption_count': 0}), (87441, {'train/accuracy': 0.769726574420929, 'train/loss': 1.133750319480896, 'validation/accuracy': 0.7074599862098694, 'validation/loss': 1.397431492805481, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 2.0062777996063232, 'test/num_examples': 10000, 'score': 39949.914578437805, 'total_duration': 43964.02792263031, 'accumulated_submission_time': 39949.914578437805, 'accumulated_eval_time': 4006.3306062221527, 'accumulated_logging_time': 3.2322468757629395, 'global_step': 87441, 'preemption_count': 0}), (88363, {'train/accuracy': 0.7766796946525574, 'train/loss': 1.0799094438552856, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.3791130781173706, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.9934600591659546, 'test/num_examples': 10000, 'score': 40369.85162806511, 'total_duration': 44424.50291514397, 'accumulated_submission_time': 40369.85162806511, 'accumulated_eval_time': 4046.7800641059875, 'accumulated_logging_time': 3.2729523181915283, 'global_step': 88363, 'preemption_count': 0}), (89279, {'train/accuracy': 0.7904296517372131, 'train/loss': 1.0638716220855713, 'validation/accuracy': 0.7069599628448486, 'validation/loss': 1.4218652248382568, 'validation/num_examples': 50000, 'test/accuracy': 0.5841000080108643, 'test/loss': 2.0293197631835938, 'test/num_examples': 10000, 'score': 40790.13759255409, 'total_duration': 44888.84012579918, 'accumulated_submission_time': 40790.13759255409, 'accumulated_eval_time': 4090.746128797531, 'accumulated_logging_time': 3.310702085494995, 'global_step': 89279, 'preemption_count': 0}), (90199, {'train/accuracy': 0.7699609398841858, 'train/loss': 1.117898941040039, 'validation/accuracy': 0.7085599899291992, 'validation/loss': 1.3838273286819458, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.990799069404602, 'test/num_examples': 10000, 'score': 41210.48593831062, 'total_duration': 45350.90107703209, 'accumulated_submission_time': 41210.48593831062, 'accumulated_eval_time': 4132.355852603912, 'accumulated_logging_time': 3.3663439750671387, 'global_step': 90199, 'preemption_count': 0}), (91120, {'train/accuracy': 0.7765820026397705, 'train/loss': 1.0714341402053833, 'validation/accuracy': 0.7109000086784363, 'validation/loss': 1.358513593673706, 'validation/num_examples': 50000, 'test/accuracy': 0.5908000469207764, 'test/loss': 1.9569255113601685, 'test/num_examples': 10000, 'score': 41630.54138350487, 'total_duration': 45808.34008026123, 'accumulated_submission_time': 41630.54138350487, 'accumulated_eval_time': 4169.649255514145, 'accumulated_logging_time': 3.409395456314087, 'global_step': 91120, 'preemption_count': 0}), (92041, {'train/accuracy': 0.7859570384025574, 'train/loss': 1.049735188484192, 'validation/accuracy': 0.7099999785423279, 'validation/loss': 1.3754807710647583, 'validation/num_examples': 50000, 'test/accuracy': 0.5905000567436218, 'test/loss': 1.9786850214004517, 'test/num_examples': 10000, 'score': 42050.45993804932, 'total_duration': 46272.828310251236, 'accumulated_submission_time': 42050.45993804932, 'accumulated_eval_time': 4214.131242513657, 'accumulated_logging_time': 3.4492197036743164, 'global_step': 92041, 'preemption_count': 0}), (92962, {'train/accuracy': 0.7772070169448853, 'train/loss': 1.0873217582702637, 'validation/accuracy': 0.7098199725151062, 'validation/loss': 1.380346417427063, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 1.974666714668274, 'test/num_examples': 10000, 'score': 42470.80727481842, 'total_duration': 46736.644303798676, 'accumulated_submission_time': 42470.80727481842, 'accumulated_eval_time': 4257.513018131256, 'accumulated_logging_time': 3.4884932041168213, 'global_step': 92962, 'preemption_count': 0}), (93883, {'train/accuracy': 0.781445324420929, 'train/loss': 1.0885802507400513, 'validation/accuracy': 0.7143999934196472, 'validation/loss': 1.3779401779174805, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 1.9860336780548096, 'test/num_examples': 10000, 'score': 42890.88827753067, 'total_duration': 47200.08113312721, 'accumulated_submission_time': 42890.88827753067, 'accumulated_eval_time': 4300.7799434661865, 'accumulated_logging_time': 3.5301735401153564, 'global_step': 93883, 'preemption_count': 0}), (94806, {'train/accuracy': 0.7893944978713989, 'train/loss': 1.0502266883850098, 'validation/accuracy': 0.7142399549484253, 'validation/loss': 1.3713054656982422, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.966513991355896, 'test/num_examples': 10000, 'score': 43311.09178900719, 'total_duration': 47663.47908735275, 'accumulated_submission_time': 43311.09178900719, 'accumulated_eval_time': 4343.885124206543, 'accumulated_logging_time': 3.5719377994537354, 'global_step': 94806, 'preemption_count': 0}), (95724, {'train/accuracy': 0.7759374976158142, 'train/loss': 1.094131588935852, 'validation/accuracy': 0.7149199843406677, 'validation/loss': 1.3668842315673828, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.9620274305343628, 'test/num_examples': 10000, 'score': 43731.27753829956, 'total_duration': 48129.953058719635, 'accumulated_submission_time': 43731.27753829956, 'accumulated_eval_time': 4390.079038143158, 'accumulated_logging_time': 3.618546485900879, 'global_step': 95724, 'preemption_count': 0}), (96646, {'train/accuracy': 0.7854687571525574, 'train/loss': 1.0612901449203491, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.3597755432128906, 'validation/num_examples': 50000, 'test/accuracy': 0.5936000347137451, 'test/loss': 1.9557676315307617, 'test/num_examples': 10000, 'score': 44151.39954543114, 'total_duration': 48594.39071774483, 'accumulated_submission_time': 44151.39954543114, 'accumulated_eval_time': 4434.304823875427, 'accumulated_logging_time': 3.6608870029449463, 'global_step': 96646, 'preemption_count': 0}), (97568, {'train/accuracy': 0.7908984422683716, 'train/loss': 1.0401599407196045, 'validation/accuracy': 0.7178399562835693, 'validation/loss': 1.3506696224212646, 'validation/num_examples': 50000, 'test/accuracy': 0.594700038433075, 'test/loss': 1.9495747089385986, 'test/num_examples': 10000, 'score': 44571.573185920715, 'total_duration': 49057.397535562515, 'accumulated_submission_time': 44571.573185920715, 'accumulated_eval_time': 4477.041138410568, 'accumulated_logging_time': 3.7091317176818848, 'global_step': 97568, 'preemption_count': 0}), (98488, {'train/accuracy': 0.8031054735183716, 'train/loss': 0.9849762320518494, 'validation/accuracy': 0.7144799828529358, 'validation/loss': 1.3556915521621704, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.9732284545898438, 'test/num_examples': 10000, 'score': 44991.559143066406, 'total_duration': 49522.340443611145, 'accumulated_submission_time': 44991.559143066406, 'accumulated_eval_time': 4521.90961265564, 'accumulated_logging_time': 3.7491395473480225, 'global_step': 98488, 'preemption_count': 0}), (99411, {'train/accuracy': 0.7883203029632568, 'train/loss': 1.0426161289215088, 'validation/accuracy': 0.7203399538993835, 'validation/loss': 1.3304067850112915, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.932340145111084, 'test/num_examples': 10000, 'score': 45411.88533329964, 'total_duration': 49986.21019554138, 'accumulated_submission_time': 45411.88533329964, 'accumulated_eval_time': 4565.364971160889, 'accumulated_logging_time': 3.7899091243743896, 'global_step': 99411, 'preemption_count': 0}), (100332, {'train/accuracy': 0.7882617115974426, 'train/loss': 1.0813069343566895, 'validation/accuracy': 0.7168599963188171, 'validation/loss': 1.389996886253357, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.9910321235656738, 'test/num_examples': 10000, 'score': 45831.827260017395, 'total_duration': 50447.46520733833, 'accumulated_submission_time': 45831.827260017395, 'accumulated_eval_time': 4606.589636087418, 'accumulated_logging_time': 3.8308794498443604, 'global_step': 100332, 'preemption_count': 0}), (101255, {'train/accuracy': 0.8056640625, 'train/loss': 1.0057238340377808, 'validation/accuracy': 0.7194799780845642, 'validation/loss': 1.3632384538650513, 'validation/num_examples': 50000, 'test/accuracy': 0.5976000428199768, 'test/loss': 1.9517590999603271, 'test/num_examples': 10000, 'score': 46251.89824414253, 'total_duration': 50907.4188015461, 'accumulated_submission_time': 46251.89824414253, 'accumulated_eval_time': 4646.38002371788, 'accumulated_logging_time': 3.875702142715454, 'global_step': 101255, 'preemption_count': 0}), (102178, {'train/accuracy': 0.785839855670929, 'train/loss': 1.0791743993759155, 'validation/accuracy': 0.7178199887275696, 'validation/loss': 1.3750110864639282, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.9908602237701416, 'test/num_examples': 10000, 'score': 46672.21717500687, 'total_duration': 51366.919667720795, 'accumulated_submission_time': 46672.21717500687, 'accumulated_eval_time': 4685.471276283264, 'accumulated_logging_time': 3.9191551208496094, 'global_step': 102178, 'preemption_count': 0}), (103099, {'train/accuracy': 0.8001952767372131, 'train/loss': 1.0049448013305664, 'validation/accuracy': 0.7236599922180176, 'validation/loss': 1.3200238943099976, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.9122182130813599, 'test/num_examples': 10000, 'score': 47092.515449762344, 'total_duration': 51831.56687259674, 'accumulated_submission_time': 47092.515449762344, 'accumulated_eval_time': 4729.725798368454, 'accumulated_logging_time': 3.966155767440796, 'global_step': 103099, 'preemption_count': 0}), (104021, {'train/accuracy': 0.802050769329071, 'train/loss': 1.0305854082107544, 'validation/accuracy': 0.723580002784729, 'validation/loss': 1.3682783842086792, 'validation/num_examples': 50000, 'test/accuracy': 0.6020000576972961, 'test/loss': 1.9694104194641113, 'test/num_examples': 10000, 'score': 47512.693658828735, 'total_duration': 52291.53897356987, 'accumulated_submission_time': 47512.693658828735, 'accumulated_eval_time': 4769.427984714508, 'accumulated_logging_time': 4.009620189666748, 'global_step': 104021, 'preemption_count': 0}), (104942, {'train/accuracy': 0.7951952815055847, 'train/loss': 1.0146361589431763, 'validation/accuracy': 0.7224000096321106, 'validation/loss': 1.322086215019226, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.9171737432479858, 'test/num_examples': 10000, 'score': 47932.840673685074, 'total_duration': 52757.84145927429, 'accumulated_submission_time': 47932.840673685074, 'accumulated_eval_time': 4815.495166063309, 'accumulated_logging_time': 4.050985097885132, 'global_step': 104942, 'preemption_count': 0}), (105860, {'train/accuracy': 0.79212886095047, 'train/loss': 1.0310273170471191, 'validation/accuracy': 0.7222200036048889, 'validation/loss': 1.32786226272583, 'validation/num_examples': 50000, 'test/accuracy': 0.6021000146865845, 'test/loss': 1.924709677696228, 'test/num_examples': 10000, 'score': 48352.493470191956, 'total_duration': 53217.91585254669, 'accumulated_submission_time': 48352.493470191956, 'accumulated_eval_time': 4855.456739187241, 'accumulated_logging_time': 4.464348077774048, 'global_step': 105860, 'preemption_count': 0}), (106780, {'train/accuracy': 0.8042968511581421, 'train/loss': 0.9922462105751038, 'validation/accuracy': 0.7252999544143677, 'validation/loss': 1.32797110080719, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.9118989706039429, 'test/num_examples': 10000, 'score': 48772.86058592796, 'total_duration': 53681.318464279175, 'accumulated_submission_time': 48772.86058592796, 'accumulated_eval_time': 4898.399055242538, 'accumulated_logging_time': 4.509274482727051, 'global_step': 106780, 'preemption_count': 0}), (107699, {'train/accuracy': 0.802734375, 'train/loss': 0.9875910878181458, 'validation/accuracy': 0.7257199883460999, 'validation/loss': 1.3120582103729248, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.9079450368881226, 'test/num_examples': 10000, 'score': 49192.9281001091, 'total_duration': 54145.81826877594, 'accumulated_submission_time': 49192.9281001091, 'accumulated_eval_time': 4942.739294528961, 'accumulated_logging_time': 4.553928375244141, 'global_step': 107699, 'preemption_count': 0}), (108619, {'train/accuracy': 0.8020898103713989, 'train/loss': 0.9975753426551819, 'validation/accuracy': 0.7286199927330017, 'validation/loss': 1.3085391521453857, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.904710292816162, 'test/num_examples': 10000, 'score': 49613.25364589691, 'total_duration': 54608.0376534462, 'accumulated_submission_time': 49613.25364589691, 'accumulated_eval_time': 4984.543601036072, 'accumulated_logging_time': 4.5956196784973145, 'global_step': 108619, 'preemption_count': 0}), (109540, {'train/accuracy': 0.8037304282188416, 'train/loss': 0.9704073071479797, 'validation/accuracy': 0.7299799919128418, 'validation/loss': 1.2849246263504028, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.8863749504089355, 'test/num_examples': 10000, 'score': 50033.40543913841, 'total_duration': 55069.19483280182, 'accumulated_submission_time': 50033.40543913841, 'accumulated_eval_time': 5025.457714557648, 'accumulated_logging_time': 4.6387939453125, 'global_step': 109540, 'preemption_count': 0}), (110460, {'train/accuracy': 0.8193163871765137, 'train/loss': 0.9186491370201111, 'validation/accuracy': 0.7305399775505066, 'validation/loss': 1.2858415842056274, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.892633080482483, 'test/num_examples': 10000, 'score': 50453.32793235779, 'total_duration': 55534.849299669266, 'accumulated_submission_time': 50453.32793235779, 'accumulated_eval_time': 5071.100483894348, 'accumulated_logging_time': 4.6809704303741455, 'global_step': 110460, 'preemption_count': 0}), (111382, {'train/accuracy': 0.8045117259025574, 'train/loss': 0.9946995973587036, 'validation/accuracy': 0.7307999730110168, 'validation/loss': 1.30134117603302, 'validation/num_examples': 50000, 'test/accuracy': 0.6093000173568726, 'test/loss': 1.9110552072525024, 'test/num_examples': 10000, 'score': 50873.48181271553, 'total_duration': 56000.36197376251, 'accumulated_submission_time': 50873.48181271553, 'accumulated_eval_time': 5116.349600315094, 'accumulated_logging_time': 4.732409715652466, 'global_step': 111382, 'preemption_count': 0}), (112305, {'train/accuracy': 0.8092968463897705, 'train/loss': 0.9513814449310303, 'validation/accuracy': 0.7310999631881714, 'validation/loss': 1.288879632949829, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.8916889429092407, 'test/num_examples': 10000, 'score': 51293.81174302101, 'total_duration': 56465.59639286995, 'accumulated_submission_time': 51293.81174302101, 'accumulated_eval_time': 5161.157242059708, 'accumulated_logging_time': 4.781898736953735, 'global_step': 112305, 'preemption_count': 0}), (113227, {'train/accuracy': 0.81849604845047, 'train/loss': 0.898280680179596, 'validation/accuracy': 0.7363399863243103, 'validation/loss': 1.2454630136489868, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.8420090675354004, 'test/num_examples': 10000, 'score': 51713.83294534683, 'total_duration': 56929.53366589546, 'accumulated_submission_time': 51713.83294534683, 'accumulated_eval_time': 5204.9848573207855, 'accumulated_logging_time': 4.822944164276123, 'global_step': 113227, 'preemption_count': 0}), (114148, {'train/accuracy': 0.8068945407867432, 'train/loss': 0.9638850688934326, 'validation/accuracy': 0.7357199788093567, 'validation/loss': 1.2680931091308594, 'validation/num_examples': 50000, 'test/accuracy': 0.6145000457763672, 'test/loss': 1.8680979013442993, 'test/num_examples': 10000, 'score': 52133.946326971054, 'total_duration': 57389.301633358, 'accumulated_submission_time': 52133.946326971054, 'accumulated_eval_time': 5244.5484964847565, 'accumulated_logging_time': 4.866366386413574, 'global_step': 114148, 'preemption_count': 0}), (115071, {'train/accuracy': 0.81201171875, 'train/loss': 0.9675384163856506, 'validation/accuracy': 0.7343999743461609, 'validation/loss': 1.2889907360076904, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.8860503435134888, 'test/num_examples': 10000, 'score': 52554.237533807755, 'total_duration': 57851.095802783966, 'accumulated_submission_time': 52554.237533807755, 'accumulated_eval_time': 5285.962988376617, 'accumulated_logging_time': 4.9072349071502686, 'global_step': 115071, 'preemption_count': 0}), (115991, {'train/accuracy': 0.8161523342132568, 'train/loss': 0.93476402759552, 'validation/accuracy': 0.7350599765777588, 'validation/loss': 1.2736767530441284, 'validation/num_examples': 50000, 'test/accuracy': 0.6172000169754028, 'test/loss': 1.8698878288269043, 'test/num_examples': 10000, 'score': 52974.534670591354, 'total_duration': 58315.426132917404, 'accumulated_submission_time': 52974.534670591354, 'accumulated_eval_time': 5329.898587703705, 'accumulated_logging_time': 4.956961631774902, 'global_step': 115991, 'preemption_count': 0}), (116909, {'train/accuracy': 0.8122069835662842, 'train/loss': 0.9754149317741394, 'validation/accuracy': 0.7371199727058411, 'validation/loss': 1.2887877225875854, 'validation/num_examples': 50000, 'test/accuracy': 0.6185000538825989, 'test/loss': 1.8774621486663818, 'test/num_examples': 10000, 'score': 53394.782676935196, 'total_duration': 58777.340618133545, 'accumulated_submission_time': 53394.782676935196, 'accumulated_eval_time': 5371.47057056427, 'accumulated_logging_time': 5.00386118888855, 'global_step': 116909, 'preemption_count': 0}), (117830, {'train/accuracy': 0.81898432970047, 'train/loss': 0.918323814868927, 'validation/accuracy': 0.7381199598312378, 'validation/loss': 1.2621748447418213, 'validation/num_examples': 50000, 'test/accuracy': 0.617900013923645, 'test/loss': 1.8480632305145264, 'test/num_examples': 10000, 'score': 53814.80868077278, 'total_duration': 59242.60025715828, 'accumulated_submission_time': 53814.80868077278, 'accumulated_eval_time': 5416.606198310852, 'accumulated_logging_time': 5.0547590255737305, 'global_step': 117830, 'preemption_count': 0}), (118753, {'train/accuracy': 0.8180468678474426, 'train/loss': 0.9298332929611206, 'validation/accuracy': 0.7390599846839905, 'validation/loss': 1.263730764389038, 'validation/num_examples': 50000, 'test/accuracy': 0.6194000244140625, 'test/loss': 1.8396508693695068, 'test/num_examples': 10000, 'score': 54235.02416753769, 'total_duration': 59707.91237425804, 'accumulated_submission_time': 54235.02416753769, 'accumulated_eval_time': 5461.611189365387, 'accumulated_logging_time': 5.096725225448608, 'global_step': 118753, 'preemption_count': 0}), (119674, {'train/accuracy': 0.8340820074081421, 'train/loss': 0.8498026728630066, 'validation/accuracy': 0.7407199740409851, 'validation/loss': 1.2424492835998535, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.8454985618591309, 'test/num_examples': 10000, 'score': 54655.402338027954, 'total_duration': 60169.75206565857, 'accumulated_submission_time': 54655.402338027954, 'accumulated_eval_time': 5502.972692966461, 'accumulated_logging_time': 5.1483683586120605, 'global_step': 119674, 'preemption_count': 0}), (120593, {'train/accuracy': 0.8211132884025574, 'train/loss': 0.9188494682312012, 'validation/accuracy': 0.7406799793243408, 'validation/loss': 1.249154806137085, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.8333444595336914, 'test/num_examples': 10000, 'score': 55075.563134908676, 'total_duration': 60629.047945261, 'accumulated_submission_time': 55075.563134908676, 'accumulated_eval_time': 5542.008673429489, 'accumulated_logging_time': 5.199536323547363, 'global_step': 120593, 'preemption_count': 0}), (121514, {'train/accuracy': 0.8241796493530273, 'train/loss': 0.8857353925704956, 'validation/accuracy': 0.7432799935340881, 'validation/loss': 1.2298856973648071, 'validation/num_examples': 50000, 'test/accuracy': 0.6218000054359436, 'test/loss': 1.8167213201522827, 'test/num_examples': 10000, 'score': 55495.675506830215, 'total_duration': 61091.24942660332, 'accumulated_submission_time': 55495.675506830215, 'accumulated_eval_time': 5584.007612705231, 'accumulated_logging_time': 5.241910457611084, 'global_step': 121514, 'preemption_count': 0}), (122435, {'train/accuracy': 0.8314648270606995, 'train/loss': 0.890949010848999, 'validation/accuracy': 0.7434200048446655, 'validation/loss': 1.2616345882415771, 'validation/num_examples': 50000, 'test/accuracy': 0.6230000257492065, 'test/loss': 1.8468738794326782, 'test/num_examples': 10000, 'score': 55915.97837305069, 'total_duration': 61556.50082373619, 'accumulated_submission_time': 55915.97837305069, 'accumulated_eval_time': 5628.865107297897, 'accumulated_logging_time': 5.285839796066284, 'global_step': 122435, 'preemption_count': 0}), (123357, {'train/accuracy': 0.8240624666213989, 'train/loss': 0.9000077843666077, 'validation/accuracy': 0.7458999752998352, 'validation/loss': 1.2291886806488037, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.8252770900726318, 'test/num_examples': 10000, 'score': 56336.17573904991, 'total_duration': 62019.912034511566, 'accumulated_submission_time': 56336.17573904991, 'accumulated_eval_time': 5671.9865918159485, 'accumulated_logging_time': 5.331271648406982, 'global_step': 123357, 'preemption_count': 0}), (124277, {'train/accuracy': 0.8270507454872131, 'train/loss': 0.9012371897697449, 'validation/accuracy': 0.7419599890708923, 'validation/loss': 1.2539701461791992, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.840736985206604, 'test/num_examples': 10000, 'score': 56756.237129449844, 'total_duration': 62480.532720565796, 'accumulated_submission_time': 56756.237129449844, 'accumulated_eval_time': 5712.451839923859, 'accumulated_logging_time': 5.377405405044556, 'global_step': 124277, 'preemption_count': 0}), (125199, {'train/accuracy': 0.8361327648162842, 'train/loss': 0.8307301998138428, 'validation/accuracy': 0.746999979019165, 'validation/loss': 1.1995173692703247, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.7952983379364014, 'test/num_examples': 10000, 'score': 57176.52000498772, 'total_duration': 62944.29509925842, 'accumulated_submission_time': 57176.52000498772, 'accumulated_eval_time': 5755.837740182877, 'accumulated_logging_time': 5.423417568206787, 'global_step': 125199, 'preemption_count': 0}), (126119, {'train/accuracy': 0.8282421827316284, 'train/loss': 0.8835758566856384, 'validation/accuracy': 0.7492600083351135, 'validation/loss': 1.218082070350647, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8145524263381958, 'test/num_examples': 10000, 'score': 57596.76955342293, 'total_duration': 63409.46954703331, 'accumulated_submission_time': 57596.76955342293, 'accumulated_eval_time': 5800.6698315143585, 'accumulated_logging_time': 5.46803092956543, 'global_step': 126119, 'preemption_count': 0}), (127039, {'train/accuracy': 0.8307812213897705, 'train/loss': 0.8714902400970459, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.2164604663848877, 'validation/num_examples': 50000, 'test/accuracy': 0.6255000233650208, 'test/loss': 1.8085092306137085, 'test/num_examples': 10000, 'score': 58016.67089056969, 'total_duration': 63868.15580749512, 'accumulated_submission_time': 58016.67089056969, 'accumulated_eval_time': 5839.35227394104, 'accumulated_logging_time': 5.522529602050781, 'global_step': 127039, 'preemption_count': 0}), (127959, {'train/accuracy': 0.8356054425239563, 'train/loss': 0.8296651244163513, 'validation/accuracy': 0.7488399744033813, 'validation/loss': 1.2022349834442139, 'validation/num_examples': 50000, 'test/accuracy': 0.6254000067710876, 'test/loss': 1.7854843139648438, 'test/num_examples': 10000, 'score': 58436.60658454895, 'total_duration': 64331.94377684593, 'accumulated_submission_time': 58436.60658454895, 'accumulated_eval_time': 5883.1105625629425, 'accumulated_logging_time': 5.568439722061157, 'global_step': 127959, 'preemption_count': 0}), (128881, {'train/accuracy': 0.8422460556030273, 'train/loss': 0.8313546776771545, 'validation/accuracy': 0.752020001411438, 'validation/loss': 1.212985873222351, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.7977409362792969, 'test/num_examples': 10000, 'score': 58856.884100198746, 'total_duration': 64793.35998272896, 'accumulated_submission_time': 58856.884100198746, 'accumulated_eval_time': 5924.156987428665, 'accumulated_logging_time': 5.612093925476074, 'global_step': 128881, 'preemption_count': 0}), (129804, {'train/accuracy': 0.8338280916213989, 'train/loss': 0.8379896283149719, 'validation/accuracy': 0.7499600052833557, 'validation/loss': 1.1926852464675903, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.786934494972229, 'test/num_examples': 10000, 'score': 59276.93350100517, 'total_duration': 65257.85636162758, 'accumulated_submission_time': 59276.93350100517, 'accumulated_eval_time': 5968.510313987732, 'accumulated_logging_time': 5.657716512680054, 'global_step': 129804, 'preemption_count': 0}), (130725, {'train/accuracy': 0.8382812142372131, 'train/loss': 0.8320725560188293, 'validation/accuracy': 0.7503199577331543, 'validation/loss': 1.1947848796844482, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.7743438482284546, 'test/num_examples': 10000, 'score': 59696.888902425766, 'total_duration': 65720.388225317, 'accumulated_submission_time': 59696.888902425766, 'accumulated_eval_time': 6010.993898153305, 'accumulated_logging_time': 5.702925443649292, 'global_step': 130725, 'preemption_count': 0}), (131647, {'train/accuracy': 0.8476171493530273, 'train/loss': 0.8037877678871155, 'validation/accuracy': 0.7534599900245667, 'validation/loss': 1.1936339139938354, 'validation/num_examples': 50000, 'test/accuracy': 0.6288000345230103, 'test/loss': 1.7920557260513306, 'test/num_examples': 10000, 'score': 60117.00132703781, 'total_duration': 66185.6890695095, 'accumulated_submission_time': 60117.00132703781, 'accumulated_eval_time': 6056.08829331398, 'accumulated_logging_time': 5.74944281578064, 'global_step': 131647, 'preemption_count': 0}), (132568, {'train/accuracy': 0.8365820050239563, 'train/loss': 0.8467892408370972, 'validation/accuracy': 0.7517600059509277, 'validation/loss': 1.205349326133728, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.788927674293518, 'test/num_examples': 10000, 'score': 60537.263917684555, 'total_duration': 66649.95460033417, 'accumulated_submission_time': 60537.263917684555, 'accumulated_eval_time': 6099.994526147842, 'accumulated_logging_time': 5.798482418060303, 'global_step': 132568, 'preemption_count': 0}), (133490, {'train/accuracy': 0.8384960889816284, 'train/loss': 0.835568904876709, 'validation/accuracy': 0.7537399530410767, 'validation/loss': 1.1961098909378052, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.7881088256835938, 'test/num_examples': 10000, 'score': 60957.490884542465, 'total_duration': 67109.25989794731, 'accumulated_submission_time': 60957.490884542465, 'accumulated_eval_time': 6138.977798938751, 'accumulated_logging_time': 5.845485210418701, 'global_step': 133490, 'preemption_count': 0}), (134411, {'train/accuracy': 0.8448827862739563, 'train/loss': 0.7935135364532471, 'validation/accuracy': 0.7524799704551697, 'validation/loss': 1.1886016130447388, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.7869489192962646, 'test/num_examples': 10000, 'score': 61377.691182136536, 'total_duration': 67575.92088413239, 'accumulated_submission_time': 61377.691182136536, 'accumulated_eval_time': 6185.34400844574, 'accumulated_logging_time': 5.892377853393555, 'global_step': 134411, 'preemption_count': 0}), (135329, {'train/accuracy': 0.8392773270606995, 'train/loss': 0.8438341617584229, 'validation/accuracy': 0.7569599747657776, 'validation/loss': 1.196811318397522, 'validation/num_examples': 50000, 'test/accuracy': 0.6373000144958496, 'test/loss': 1.7893301248550415, 'test/num_examples': 10000, 'score': 61797.867267131805, 'total_duration': 68038.98924589157, 'accumulated_submission_time': 61797.867267131805, 'accumulated_eval_time': 6228.144732713699, 'accumulated_logging_time': 5.9372031688690186, 'global_step': 135329, 'preemption_count': 0}), (136252, {'train/accuracy': 0.84193354845047, 'train/loss': 0.8302668333053589, 'validation/accuracy': 0.758080005645752, 'validation/loss': 1.1913862228393555, 'validation/num_examples': 50000, 'test/accuracy': 0.6366000175476074, 'test/loss': 1.7862942218780518, 'test/num_examples': 10000, 'score': 62218.01303982735, 'total_duration': 68501.58464884758, 'accumulated_submission_time': 62218.01303982735, 'accumulated_eval_time': 6270.500958204269, 'accumulated_logging_time': 5.982234716415405, 'global_step': 136252, 'preemption_count': 0}), (137171, {'train/accuracy': 0.8501171469688416, 'train/loss': 0.7863731980323792, 'validation/accuracy': 0.7578799724578857, 'validation/loss': 1.1653896570205688, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.7534481287002563, 'test/num_examples': 10000, 'score': 62637.987129449844, 'total_duration': 68965.4830365181, 'accumulated_submission_time': 62637.987129449844, 'accumulated_eval_time': 6314.320621013641, 'accumulated_logging_time': 6.038216590881348, 'global_step': 137171, 'preemption_count': 0}), (138092, {'train/accuracy': 0.8501366972923279, 'train/loss': 0.8256229758262634, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.1994614601135254, 'validation/num_examples': 50000, 'test/accuracy': 0.6454000473022461, 'test/loss': 1.7836123704910278, 'test/num_examples': 10000, 'score': 63058.27062392235, 'total_duration': 69428.69195985794, 'accumulated_submission_time': 63058.27062392235, 'accumulated_eval_time': 6357.152781248093, 'accumulated_logging_time': 6.084154844284058, 'global_step': 138092, 'preemption_count': 0}), (139011, {'train/accuracy': 0.8479101657867432, 'train/loss': 0.7939903736114502, 'validation/accuracy': 0.7591599822044373, 'validation/loss': 1.1697139739990234, 'validation/num_examples': 50000, 'test/accuracy': 0.6362000107765198, 'test/loss': 1.7635866403579712, 'test/num_examples': 10000, 'score': 63478.28254342079, 'total_duration': 69894.97005820274, 'accumulated_submission_time': 63478.28254342079, 'accumulated_eval_time': 6403.32272028923, 'accumulated_logging_time': 6.133307695388794, 'global_step': 139011, 'preemption_count': 0}), (139931, {'train/accuracy': 0.8531054258346558, 'train/loss': 0.7984217405319214, 'validation/accuracy': 0.7590999603271484, 'validation/loss': 1.1816964149475098, 'validation/num_examples': 50000, 'test/accuracy': 0.6403000354766846, 'test/loss': 1.7529109716415405, 'test/num_examples': 10000, 'score': 63898.31989693642, 'total_duration': 70359.86943912506, 'accumulated_submission_time': 63898.31989693642, 'accumulated_eval_time': 6448.088568687439, 'accumulated_logging_time': 6.181832790374756, 'global_step': 139931, 'preemption_count': 0}), (140851, {'train/accuracy': 0.8634960651397705, 'train/loss': 0.751483142375946, 'validation/accuracy': 0.7616999745368958, 'validation/loss': 1.166806936264038, 'validation/num_examples': 50000, 'test/accuracy': 0.643500030040741, 'test/loss': 1.751853346824646, 'test/num_examples': 10000, 'score': 64318.303194999695, 'total_duration': 70826.06875395775, 'accumulated_submission_time': 64318.303194999695, 'accumulated_eval_time': 6494.210072994232, 'accumulated_logging_time': 6.228420257568359, 'global_step': 140851, 'preemption_count': 0}), (141770, {'train/accuracy': 0.849609375, 'train/loss': 0.7915339469909668, 'validation/accuracy': 0.7606199979782104, 'validation/loss': 1.1642590761184692, 'validation/num_examples': 50000, 'test/accuracy': 0.6408000588417053, 'test/loss': 1.7490770816802979, 'test/num_examples': 10000, 'score': 64738.19510626793, 'total_duration': 71288.30664849281, 'accumulated_submission_time': 64738.19510626793, 'accumulated_eval_time': 6536.446292638779, 'accumulated_logging_time': 6.2816667556762695, 'global_step': 141770, 'preemption_count': 0}), (142689, {'train/accuracy': 0.8543359041213989, 'train/loss': 0.7828488945960999, 'validation/accuracy': 0.7647799849510193, 'validation/loss': 1.1637057065963745, 'validation/num_examples': 50000, 'test/accuracy': 0.6442000269889832, 'test/loss': 1.7491698265075684, 'test/num_examples': 10000, 'score': 65158.11840724945, 'total_duration': 71755.92424988747, 'accumulated_submission_time': 65158.11840724945, 'accumulated_eval_time': 6584.042410612106, 'accumulated_logging_time': 6.332995891571045, 'global_step': 142689, 'preemption_count': 0}), (143611, {'train/accuracy': 0.86376953125, 'train/loss': 0.744042694568634, 'validation/accuracy': 0.7639600038528442, 'validation/loss': 1.1600041389465332, 'validation/num_examples': 50000, 'test/accuracy': 0.643500030040741, 'test/loss': 1.7458293437957764, 'test/num_examples': 10000, 'score': 65578.4606962204, 'total_duration': 72221.8099834919, 'accumulated_submission_time': 65578.4606962204, 'accumulated_eval_time': 6629.484586715698, 'accumulated_logging_time': 6.386404514312744, 'global_step': 143611, 'preemption_count': 0}), (144532, {'train/accuracy': 0.8549999594688416, 'train/loss': 0.7654373645782471, 'validation/accuracy': 0.7643399834632874, 'validation/loss': 1.1485599279403687, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.74350905418396, 'test/num_examples': 10000, 'score': 65998.61735081673, 'total_duration': 72687.37620162964, 'accumulated_submission_time': 65998.61735081673, 'accumulated_eval_time': 6674.7976150512695, 'accumulated_logging_time': 6.435186386108398, 'global_step': 144532, 'preemption_count': 0}), (145453, {'train/accuracy': 0.8571484088897705, 'train/loss': 0.7539299130439758, 'validation/accuracy': 0.7656399607658386, 'validation/loss': 1.1399657726287842, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7225067615509033, 'test/num_examples': 10000, 'score': 66418.83790254593, 'total_duration': 73152.03741383553, 'accumulated_submission_time': 66418.83790254593, 'accumulated_eval_time': 6719.138984918594, 'accumulated_logging_time': 6.487768888473511, 'global_step': 145453, 'preemption_count': 0}), (146374, {'train/accuracy': 0.8620703220367432, 'train/loss': 0.7350453734397888, 'validation/accuracy': 0.7666599750518799, 'validation/loss': 1.1349040269851685, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.715340256690979, 'test/num_examples': 10000, 'score': 66838.8134508133, 'total_duration': 73617.8742146492, 'accumulated_submission_time': 66838.8134508133, 'accumulated_eval_time': 6764.900428771973, 'accumulated_logging_time': 6.540493726730347, 'global_step': 146374, 'preemption_count': 0}), (147297, {'train/accuracy': 0.8604687452316284, 'train/loss': 0.7560346722602844, 'validation/accuracy': 0.766759991645813, 'validation/loss': 1.1506201028823853, 'validation/num_examples': 50000, 'test/accuracy': 0.6461000442504883, 'test/loss': 1.7426823377609253, 'test/num_examples': 10000, 'score': 67258.91440343857, 'total_duration': 74083.21685099602, 'accumulated_submission_time': 67258.91440343857, 'accumulated_eval_time': 6810.045199871063, 'accumulated_logging_time': 6.589900016784668, 'global_step': 147297, 'preemption_count': 0}), (148218, {'train/accuracy': 0.8648632764816284, 'train/loss': 0.7450129389762878, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1421996355056763, 'validation/num_examples': 50000, 'test/accuracy': 0.6454000473022461, 'test/loss': 1.7250229120254517, 'test/num_examples': 10000, 'score': 67679.01679587364, 'total_duration': 74546.50698709488, 'accumulated_submission_time': 67679.01679587364, 'accumulated_eval_time': 6853.139424085617, 'accumulated_logging_time': 6.635733366012573, 'global_step': 148218, 'preemption_count': 0}), (149138, {'train/accuracy': 0.8698437213897705, 'train/loss': 0.7154492139816284, 'validation/accuracy': 0.768839955329895, 'validation/loss': 1.1309633255004883, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.7095143795013428, 'test/num_examples': 10000, 'score': 68098.65920686722, 'total_duration': 75011.86696529388, 'accumulated_submission_time': 68098.65920686722, 'accumulated_eval_time': 6898.35661482811, 'accumulated_logging_time': 7.087894439697266, 'global_step': 149138, 'preemption_count': 0}), (150058, {'train/accuracy': 0.8729296922683716, 'train/loss': 0.7006752490997314, 'validation/accuracy': 0.7681399583816528, 'validation/loss': 1.1254583597183228, 'validation/num_examples': 50000, 'test/accuracy': 0.650700032711029, 'test/loss': 1.709939956665039, 'test/num_examples': 10000, 'score': 68518.84233808517, 'total_duration': 75477.76089262962, 'accumulated_submission_time': 68518.84233808517, 'accumulated_eval_time': 6943.96710062027, 'accumulated_logging_time': 7.140713453292847, 'global_step': 150058, 'preemption_count': 0}), (150978, {'train/accuracy': 0.8682031035423279, 'train/loss': 0.7284000515937805, 'validation/accuracy': 0.770039975643158, 'validation/loss': 1.1370346546173096, 'validation/num_examples': 50000, 'test/accuracy': 0.6529000401496887, 'test/loss': 1.707403540611267, 'test/num_examples': 10000, 'score': 68939.2911374569, 'total_duration': 75941.07792234421, 'accumulated_submission_time': 68939.2911374569, 'accumulated_eval_time': 6986.741266012192, 'accumulated_logging_time': 7.1879754066467285, 'global_step': 150978, 'preemption_count': 0}), (151901, {'train/accuracy': 0.8696093559265137, 'train/loss': 0.7336189150810242, 'validation/accuracy': 0.7696399688720703, 'validation/loss': 1.1401625871658325, 'validation/num_examples': 50000, 'test/accuracy': 0.6514000296592712, 'test/loss': 1.7238764762878418, 'test/num_examples': 10000, 'score': 69359.93056178093, 'total_duration': 76403.58780241013, 'accumulated_submission_time': 69359.93056178093, 'accumulated_eval_time': 7028.515541791916, 'accumulated_logging_time': 7.23703145980835, 'global_step': 151901, 'preemption_count': 0}), (152821, {'train/accuracy': 0.8742968440055847, 'train/loss': 0.6973341703414917, 'validation/accuracy': 0.7716000080108643, 'validation/loss': 1.1223145723342896, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.7144511938095093, 'test/num_examples': 10000, 'score': 69779.99265003204, 'total_duration': 76866.06092453003, 'accumulated_submission_time': 69779.99265003204, 'accumulated_eval_time': 7070.822064638138, 'accumulated_logging_time': 7.29412579536438, 'global_step': 152821, 'preemption_count': 0}), (153740, {'train/accuracy': 0.8701757788658142, 'train/loss': 0.7270835041999817, 'validation/accuracy': 0.7702800035476685, 'validation/loss': 1.1377615928649902, 'validation/num_examples': 50000, 'test/accuracy': 0.6533000469207764, 'test/loss': 1.7147181034088135, 'test/num_examples': 10000, 'score': 70200.02810454369, 'total_duration': 77328.01024198532, 'accumulated_submission_time': 70200.02810454369, 'accumulated_eval_time': 7112.63879776001, 'accumulated_logging_time': 7.344912052154541, 'global_step': 153740, 'preemption_count': 0}), (154661, {'train/accuracy': 0.873828113079071, 'train/loss': 0.7049767971038818, 'validation/accuracy': 0.7745800018310547, 'validation/loss': 1.1225615739822388, 'validation/num_examples': 50000, 'test/accuracy': 0.659000039100647, 'test/loss': 1.699967861175537, 'test/num_examples': 10000, 'score': 70620.03921198845, 'total_duration': 77790.1883494854, 'accumulated_submission_time': 70620.03921198845, 'accumulated_eval_time': 7154.707575559616, 'accumulated_logging_time': 7.395971059799194, 'global_step': 154661, 'preemption_count': 0}), (155577, {'train/accuracy': 0.8754296898841858, 'train/loss': 0.6761949062347412, 'validation/accuracy': 0.7723399996757507, 'validation/loss': 1.1067827939987183, 'validation/num_examples': 50000, 'test/accuracy': 0.6546000242233276, 'test/loss': 1.6750155687332153, 'test/num_examples': 10000, 'score': 71040.05707144737, 'total_duration': 78257.04546570778, 'accumulated_submission_time': 71040.05707144737, 'accumulated_eval_time': 7201.4481337070465, 'accumulated_logging_time': 7.4474194049835205, 'global_step': 155577, 'preemption_count': 0}), (156497, {'train/accuracy': 0.8728905916213989, 'train/loss': 0.7006733417510986, 'validation/accuracy': 0.775439977645874, 'validation/loss': 1.110991358757019, 'validation/num_examples': 50000, 'test/accuracy': 0.6582000255584717, 'test/loss': 1.6929787397384644, 'test/num_examples': 10000, 'score': 71460.33196163177, 'total_duration': 78725.32055997849, 'accumulated_submission_time': 71460.33196163177, 'accumulated_eval_time': 7249.346954345703, 'accumulated_logging_time': 7.500328063964844, 'global_step': 156497, 'preemption_count': 0}), (157419, {'train/accuracy': 0.8755663633346558, 'train/loss': 0.6870554089546204, 'validation/accuracy': 0.7741000056266785, 'validation/loss': 1.113128900527954, 'validation/num_examples': 50000, 'test/accuracy': 0.65420001745224, 'test/loss': 1.6954491138458252, 'test/num_examples': 10000, 'score': 71880.53656959534, 'total_duration': 79187.9251627922, 'accumulated_submission_time': 71880.53656959534, 'accumulated_eval_time': 7291.650812864304, 'accumulated_logging_time': 7.549129009246826, 'global_step': 157419, 'preemption_count': 0}), (158341, {'train/accuracy': 0.8795117139816284, 'train/loss': 0.6774889230728149, 'validation/accuracy': 0.7754999995231628, 'validation/loss': 1.105984091758728, 'validation/num_examples': 50000, 'test/accuracy': 0.6583000421524048, 'test/loss': 1.6818146705627441, 'test/num_examples': 10000, 'score': 72300.74264979362, 'total_duration': 79649.6304256916, 'accumulated_submission_time': 72300.74264979362, 'accumulated_eval_time': 7333.049576044083, 'accumulated_logging_time': 7.60271143913269, 'global_step': 158341, 'preemption_count': 0}), (159260, {'train/accuracy': 0.8759960532188416, 'train/loss': 0.6908382773399353, 'validation/accuracy': 0.775879979133606, 'validation/loss': 1.1087701320648193, 'validation/num_examples': 50000, 'test/accuracy': 0.6544000506401062, 'test/loss': 1.6908684968948364, 'test/num_examples': 10000, 'score': 72721.07917380333, 'total_duration': 80115.90304541588, 'accumulated_submission_time': 72721.07917380333, 'accumulated_eval_time': 7378.887080192566, 'accumulated_logging_time': 7.6535325050354, 'global_step': 159260, 'preemption_count': 0}), (160179, {'train/accuracy': 0.8782421946525574, 'train/loss': 0.6889281868934631, 'validation/accuracy': 0.7750399708747864, 'validation/loss': 1.1090731620788574, 'validation/num_examples': 50000, 'test/accuracy': 0.6558000445365906, 'test/loss': 1.691611409187317, 'test/num_examples': 10000, 'score': 73141.11105513573, 'total_duration': 80583.63886260986, 'accumulated_submission_time': 73141.11105513573, 'accumulated_eval_time': 7426.482050657272, 'accumulated_logging_time': 7.7148168087005615, 'global_step': 160179, 'preemption_count': 0}), (161101, {'train/accuracy': 0.8794531226158142, 'train/loss': 0.6894750595092773, 'validation/accuracy': 0.7768999934196472, 'validation/loss': 1.1162922382354736, 'validation/num_examples': 50000, 'test/accuracy': 0.6583000421524048, 'test/loss': 1.6969735622406006, 'test/num_examples': 10000, 'score': 73561.23503017426, 'total_duration': 81049.8535144329, 'accumulated_submission_time': 73561.23503017426, 'accumulated_eval_time': 7472.475435972214, 'accumulated_logging_time': 7.764083385467529, 'global_step': 161101, 'preemption_count': 0}), (162021, {'train/accuracy': 0.8860155940055847, 'train/loss': 0.6461138129234314, 'validation/accuracy': 0.7784199714660645, 'validation/loss': 1.0914040803909302, 'validation/num_examples': 50000, 'test/accuracy': 0.6588000059127808, 'test/loss': 1.671212911605835, 'test/num_examples': 10000, 'score': 73981.34681868553, 'total_duration': 81509.38480234146, 'accumulated_submission_time': 73981.34681868553, 'accumulated_eval_time': 7511.793575048447, 'accumulated_logging_time': 7.818175554275513, 'global_step': 162021, 'preemption_count': 0}), (162943, {'train/accuracy': 0.8807421922683716, 'train/loss': 0.6805300712585449, 'validation/accuracy': 0.7788800001144409, 'validation/loss': 1.1066820621490479, 'validation/num_examples': 50000, 'test/accuracy': 0.6619000434875488, 'test/loss': 1.6778147220611572, 'test/num_examples': 10000, 'score': 74401.3993074894, 'total_duration': 81974.58531212807, 'accumulated_submission_time': 74401.3993074894, 'accumulated_eval_time': 7556.840314865112, 'accumulated_logging_time': 7.872089624404907, 'global_step': 162943, 'preemption_count': 0}), (163867, {'train/accuracy': 0.8832812309265137, 'train/loss': 0.6668259501457214, 'validation/accuracy': 0.7802599668502808, 'validation/loss': 1.0908979177474976, 'validation/num_examples': 50000, 'test/accuracy': 0.6641000509262085, 'test/loss': 1.6616779565811157, 'test/num_examples': 10000, 'score': 74821.57655787468, 'total_duration': 82442.0477347374, 'accumulated_submission_time': 74821.57655787468, 'accumulated_eval_time': 7604.018439769745, 'accumulated_logging_time': 7.928055763244629, 'global_step': 163867, 'preemption_count': 0}), (164789, {'train/accuracy': 0.887988269329071, 'train/loss': 0.6550906896591187, 'validation/accuracy': 0.7813000082969666, 'validation/loss': 1.0903257131576538, 'validation/num_examples': 50000, 'test/accuracy': 0.6593000292778015, 'test/loss': 1.6780301332473755, 'test/num_examples': 10000, 'score': 75241.48506331444, 'total_duration': 82908.00784730911, 'accumulated_submission_time': 75241.48506331444, 'accumulated_eval_time': 7649.971467733383, 'accumulated_logging_time': 7.979364395141602, 'global_step': 164789, 'preemption_count': 0}), (165714, {'train/accuracy': 0.8838671445846558, 'train/loss': 0.6711214780807495, 'validation/accuracy': 0.7797999978065491, 'validation/loss': 1.0984795093536377, 'validation/num_examples': 50000, 'test/accuracy': 0.663100004196167, 'test/loss': 1.6789699792861938, 'test/num_examples': 10000, 'score': 75661.61553740501, 'total_duration': 83369.6378827095, 'accumulated_submission_time': 75661.61553740501, 'accumulated_eval_time': 7691.369910478592, 'accumulated_logging_time': 8.033177137374878, 'global_step': 165714, 'preemption_count': 0}), (166636, {'train/accuracy': 0.8857812285423279, 'train/loss': 0.6534203290939331, 'validation/accuracy': 0.7819399833679199, 'validation/loss': 1.0902855396270752, 'validation/num_examples': 50000, 'test/accuracy': 0.6624000072479248, 'test/loss': 1.6653751134872437, 'test/num_examples': 10000, 'score': 76081.84141349792, 'total_duration': 83835.18001461029, 'accumulated_submission_time': 76081.84141349792, 'accumulated_eval_time': 7736.580994844437, 'accumulated_logging_time': 8.090880393981934, 'global_step': 166636, 'preemption_count': 0}), (167556, {'train/accuracy': 0.8882226347923279, 'train/loss': 0.6422659754753113, 'validation/accuracy': 0.7817999720573425, 'validation/loss': 1.0872855186462402, 'validation/num_examples': 50000, 'test/accuracy': 0.6653000116348267, 'test/loss': 1.656856894493103, 'test/num_examples': 10000, 'score': 76501.81643271446, 'total_duration': 84299.6081571579, 'accumulated_submission_time': 76501.81643271446, 'accumulated_eval_time': 7780.9323217868805, 'accumulated_logging_time': 8.14522910118103, 'global_step': 167556, 'preemption_count': 0}), (168477, {'train/accuracy': 0.8875390291213989, 'train/loss': 0.6451851725578308, 'validation/accuracy': 0.7829200029373169, 'validation/loss': 1.0794780254364014, 'validation/num_examples': 50000, 'test/accuracy': 0.6647000312805176, 'test/loss': 1.650307297706604, 'test/num_examples': 10000, 'score': 76921.93040585518, 'total_duration': 84765.37529754639, 'accumulated_submission_time': 76921.93040585518, 'accumulated_eval_time': 7826.474480390549, 'accumulated_logging_time': 8.208962202072144, 'global_step': 168477, 'preemption_count': 0}), (169398, {'train/accuracy': 0.8874218463897705, 'train/loss': 0.6406536102294922, 'validation/accuracy': 0.7831999659538269, 'validation/loss': 1.078544020652771, 'validation/num_examples': 50000, 'test/accuracy': 0.6653000116348267, 'test/loss': 1.6556425094604492, 'test/num_examples': 10000, 'score': 77342.00812506676, 'total_duration': 85230.77464270592, 'accumulated_submission_time': 77342.00812506676, 'accumulated_eval_time': 7871.694277763367, 'accumulated_logging_time': 8.263505935668945, 'global_step': 169398, 'preemption_count': 0})], 'global_step': 169795}
I0201 13:00:24.703183 139936116377408 submission_runner.py:586] Timing: 77520.01207590103
I0201 13:00:24.703279 139936116377408 submission_runner.py:588] Total number of evals: 185
I0201 13:00:24.703330 139936116377408 submission_runner.py:589] ====================
I0201 13:00:24.703383 139936116377408 submission_runner.py:542] Using RNG seed 2064292405
I0201 13:00:24.704918 139936116377408 submission_runner.py:551] --- Tuning run 3/5 ---
I0201 13:00:24.705027 139936116377408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_3.
I0201 13:00:24.708598 139936116377408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_3/hparams.json.
I0201 13:00:24.709495 139936116377408 submission_runner.py:206] Initializing dataset.
I0201 13:00:24.719171 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0201 13:00:24.729001 139936116377408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0201 13:00:24.930510 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0201 13:00:29.183545 139936116377408 submission_runner.py:213] Initializing model.
I0201 13:00:35.509262 139936116377408 submission_runner.py:255] Initializing optimizer.
I0201 13:00:35.963726 139936116377408 submission_runner.py:262] Initializing metrics bundle.
I0201 13:00:35.963912 139936116377408 submission_runner.py:280] Initializing checkpoint and logger.
I0201 13:00:35.979521 139936116377408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_3 with prefix checkpoint_
I0201 13:00:35.979636 139936116377408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0201 13:00:52.201803 139936116377408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0201 13:01:08.141461 139936116377408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_3/flags_0.json.
I0201 13:01:08.146389 139936116377408 submission_runner.py:314] Starting training loop.
I0201 13:01:45.165661 139774392231680 logging_writer.py:48] [0] global_step=0, grad_norm=0.365526020526886, loss=6.9077558517456055
I0201 13:01:45.179293 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:01:53.417280 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:02:12.639654 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:02:14.240103 139936116377408 submission_runner.py:408] Time since start: 66.09s, 	Step: 1, 	{'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 37.03280329704285, 'total_duration': 66.09364891052246, 'accumulated_submission_time': 37.03280329704285, 'accumulated_eval_time': 29.060741901397705, 'accumulated_logging_time': 0}
I0201 13:02:14.250139 139774400624384 logging_writer.py:48] [1] accumulated_eval_time=29.060742, accumulated_logging_time=0, accumulated_submission_time=37.032803, global_step=1, preemption_count=0, score=37.032803, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=66.093649, train/accuracy=0.000977, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0201 13:03:20.576345 139774434195200 logging_writer.py:48] [100] global_step=100, grad_norm=0.3900865316390991, loss=6.9059882164001465
I0201 13:04:06.644090 139774417409792 logging_writer.py:48] [200] global_step=200, grad_norm=0.47955623269081116, loss=6.892996311187744
I0201 13:04:52.882147 139774434195200 logging_writer.py:48] [300] global_step=300, grad_norm=0.6525566577911377, loss=6.850131034851074
I0201 13:05:39.535926 139774417409792 logging_writer.py:48] [400] global_step=400, grad_norm=0.7248287796974182, loss=6.80684232711792
I0201 13:06:26.292608 139774434195200 logging_writer.py:48] [500] global_step=500, grad_norm=0.9784239530563354, loss=6.820516586303711
I0201 13:07:13.447815 139774417409792 logging_writer.py:48] [600] global_step=600, grad_norm=1.15289306640625, loss=6.723775863647461
I0201 13:07:59.835539 139774434195200 logging_writer.py:48] [700] global_step=700, grad_norm=1.9653042554855347, loss=6.629521369934082
I0201 13:08:47.034163 139774417409792 logging_writer.py:48] [800] global_step=800, grad_norm=1.0488638877868652, loss=6.7048821449279785
I0201 13:09:14.492712 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:09:27.783594 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:09:56.584114 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:09:58.191743 139936116377408 submission_runner.py:408] Time since start: 530.05s, 	Step: 860, 	{'train/accuracy': 0.012519530951976776, 'train/loss': 6.41873836517334, 'validation/accuracy': 0.012719999998807907, 'validation/loss': 6.431779384613037, 'validation/num_examples': 50000, 'test/accuracy': 0.01080000028014183, 'test/loss': 6.47079610824585, 'test/num_examples': 10000, 'score': 457.21945309638977, 'total_duration': 530.0452859401703, 'accumulated_submission_time': 457.21945309638977, 'accumulated_eval_time': 72.75973510742188, 'accumulated_logging_time': 0.01935410499572754}
I0201 13:09:58.210279 139774434195200 logging_writer.py:48] [860] accumulated_eval_time=72.759735, accumulated_logging_time=0.019354, accumulated_submission_time=457.219453, global_step=860, preemption_count=0, score=457.219453, test/accuracy=0.010800, test/loss=6.470796, test/num_examples=10000, total_duration=530.045286, train/accuracy=0.012520, train/loss=6.418738, validation/accuracy=0.012720, validation/loss=6.431779, validation/num_examples=50000
I0201 13:10:14.470324 139774417409792 logging_writer.py:48] [900] global_step=900, grad_norm=1.7167344093322754, loss=6.659823417663574
I0201 13:10:57.835467 139774434195200 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9167710542678833, loss=6.463069915771484
I0201 13:11:44.866789 139774417409792 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.232525110244751, loss=6.73720121383667
I0201 13:12:31.865950 139774434195200 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.7787821292877197, loss=6.3848114013671875
I0201 13:13:18.429579 139774417409792 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8803110122680664, loss=6.704368591308594
I0201 13:14:05.434326 139774434195200 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.9123189449310303, loss=6.3954362869262695
I0201 13:14:52.016470 139774417409792 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.8849802017211914, loss=6.421629905700684
I0201 13:15:38.631870 139774434195200 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.8870600461959839, loss=6.155239582061768
I0201 13:16:25.301531 139774417409792 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.473210573196411, loss=6.104147911071777
I0201 13:16:58.478857 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:17:11.777979 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:17:47.040742 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:17:48.648395 139936116377408 submission_runner.py:408] Time since start: 1000.50s, 	Step: 1773, 	{'train/accuracy': 0.03847656026482582, 'train/loss': 5.859482288360596, 'validation/accuracy': 0.03543999791145325, 'validation/loss': 5.89152193069458, 'validation/num_examples': 50000, 'test/accuracy': 0.030300000682473183, 'test/loss': 6.007748603820801, 'test/num_examples': 10000, 'score': 877.4289219379425, 'total_duration': 1000.501962184906, 'accumulated_submission_time': 877.4289219379425, 'accumulated_eval_time': 122.92927050590515, 'accumulated_logging_time': 0.04998922348022461}
I0201 13:17:48.666072 139774434195200 logging_writer.py:48] [1773] accumulated_eval_time=122.929271, accumulated_logging_time=0.049989, accumulated_submission_time=877.428922, global_step=1773, preemption_count=0, score=877.428922, test/accuracy=0.030300, test/loss=6.007749, test/num_examples=10000, total_duration=1000.501962, train/accuracy=0.038477, train/loss=5.859482, validation/accuracy=0.035440, validation/loss=5.891522, validation/num_examples=50000
I0201 13:17:59.771174 139774417409792 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.056711196899414, loss=6.110658645629883
I0201 13:18:42.717696 139774434195200 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.1731674671173096, loss=6.463726043701172
I0201 13:19:29.118893 139774417409792 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.282196521759033, loss=6.603789806365967
I0201 13:20:15.768779 139774434195200 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.168393135070801, loss=5.994540214538574
I0201 13:21:01.835055 139774417409792 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.9409583806991577, loss=6.293904781341553
I0201 13:21:48.182995 139774434195200 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.250455617904663, loss=5.979686260223389
I0201 13:22:34.491120 139774417409792 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.0990066528320312, loss=5.862176895141602
I0201 13:23:20.948761 139774434195200 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.750533103942871, loss=6.493756294250488
I0201 13:24:07.166951 139774417409792 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.1047887802124023, loss=6.238729476928711
I0201 13:24:48.901763 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:25:01.623075 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:25:34.068237 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:25:35.668348 139936116377408 submission_runner.py:408] Time since start: 1467.52s, 	Step: 2692, 	{'train/accuracy': 0.06236327812075615, 'train/loss': 5.426368713378906, 'validation/accuracy': 0.06021999940276146, 'validation/loss': 5.460067272186279, 'validation/num_examples': 50000, 'test/accuracy': 0.044600002467632294, 'test/loss': 5.647171974182129, 'test/num_examples': 10000, 'score': 1297.60599899292, 'total_duration': 1467.5219156742096, 'accumulated_submission_time': 1297.60599899292, 'accumulated_eval_time': 169.69586300849915, 'accumulated_logging_time': 0.07787775993347168}
I0201 13:25:35.683209 139774434195200 logging_writer.py:48] [2692] accumulated_eval_time=169.695863, accumulated_logging_time=0.077878, accumulated_submission_time=1297.605999, global_step=2692, preemption_count=0, score=1297.605999, test/accuracy=0.044600, test/loss=5.647172, test/num_examples=10000, total_duration=1467.521916, train/accuracy=0.062363, train/loss=5.426369, validation/accuracy=0.060220, validation/loss=5.460067, validation/num_examples=50000
I0201 13:25:39.255055 139774417409792 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.6361024379730225, loss=6.044617176055908
I0201 13:26:21.350136 139774434195200 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.0956661701202393, loss=5.840678691864014
I0201 13:27:07.493893 139774417409792 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.1842756271362305, loss=5.820326328277588
I0201 13:27:53.870408 139774434195200 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.059281349182129, loss=5.7206621170043945
I0201 13:28:39.931254 139774417409792 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.8947067260742188, loss=5.655803680419922
I0201 13:29:26.118547 139774434195200 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.6823627948760986, loss=6.578264236450195
I0201 13:30:12.197526 139774417409792 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.7787065505981445, loss=5.693294048309326
I0201 13:30:58.238916 139774434195200 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.6713101863861084, loss=5.992044448852539
I0201 13:31:44.806364 139774417409792 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.107060194015503, loss=5.637170791625977
I0201 13:32:31.162508 139774434195200 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.7556126117706299, loss=5.67620325088501
I0201 13:32:35.857270 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:32:49.528044 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:33:08.785567 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:33:10.394066 139936116377408 submission_runner.py:408] Time since start: 1922.25s, 	Step: 3612, 	{'train/accuracy': 0.09099609404802322, 'train/loss': 5.120298385620117, 'validation/accuracy': 0.08423999696969986, 'validation/loss': 5.156952857971191, 'validation/num_examples': 50000, 'test/accuracy': 0.0625, 'test/loss': 5.394311428070068, 'test/num_examples': 10000, 'score': 1717.723210811615, 'total_duration': 1922.2475941181183, 'accumulated_submission_time': 1717.723210811615, 'accumulated_eval_time': 204.23260712623596, 'accumulated_logging_time': 0.10209107398986816}
I0201 13:33:10.410858 139774417409792 logging_writer.py:48] [3612] accumulated_eval_time=204.232607, accumulated_logging_time=0.102091, accumulated_submission_time=1717.723211, global_step=3612, preemption_count=0, score=1717.723211, test/accuracy=0.062500, test/loss=5.394311, test/num_examples=10000, total_duration=1922.247594, train/accuracy=0.090996, train/loss=5.120298, validation/accuracy=0.084240, validation/loss=5.156953, validation/num_examples=50000
I0201 13:33:48.622397 139774434195200 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.3192267417907715, loss=5.5615644454956055
I0201 13:34:35.769602 139774417409792 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.8830137252807617, loss=5.556939601898193
I0201 13:35:22.876250 139774434195200 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.344832420349121, loss=5.4912309646606445
I0201 13:36:10.028142 139774417409792 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.587607979774475, loss=6.462715148925781
I0201 13:36:57.182787 139774434195200 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.5054696798324585, loss=6.44287109375
I0201 13:37:44.484680 139774417409792 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.5710757970809937, loss=5.60418176651001
I0201 13:38:31.810228 139774434195200 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.5132299661636353, loss=6.4113078117370605
I0201 13:39:19.350677 139774417409792 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.857077956199646, loss=5.391970157623291
I0201 13:40:06.432877 139774434195200 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.7055796384811401, loss=6.177700042724609
I0201 13:40:10.793126 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:40:24.501298 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:40:54.665166 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:40:56.270316 139936116377408 submission_runner.py:408] Time since start: 2388.12s, 	Step: 4511, 	{'train/accuracy': 0.1286914050579071, 'train/loss': 4.747590065002441, 'validation/accuracy': 0.11927999556064606, 'validation/loss': 4.806865692138672, 'validation/num_examples': 50000, 'test/accuracy': 0.08970000594854355, 'test/loss': 5.0951080322265625, 'test/num_examples': 10000, 'score': 2138.045210123062, 'total_duration': 2388.123862028122, 'accumulated_submission_time': 2138.045210123062, 'accumulated_eval_time': 249.70977568626404, 'accumulated_logging_time': 0.13258743286132812}
I0201 13:40:56.287277 139774417409792 logging_writer.py:48] [4511] accumulated_eval_time=249.709776, accumulated_logging_time=0.132587, accumulated_submission_time=2138.045210, global_step=4511, preemption_count=0, score=2138.045210, test/accuracy=0.089700, test/loss=5.095108, test/num_examples=10000, total_duration=2388.123862, train/accuracy=0.128691, train/loss=4.747590, validation/accuracy=0.119280, validation/loss=4.806866, validation/num_examples=50000
I0201 13:41:32.972036 139774434195200 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.9388326406478882, loss=5.2876458168029785
I0201 13:42:19.129106 139774417409792 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.811171293258667, loss=5.389535427093506
I0201 13:43:05.545629 139774434195200 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.001354217529297, loss=5.189614295959473
I0201 13:43:51.833343 139774417409792 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.7650731801986694, loss=5.903433322906494
I0201 13:44:38.202364 139774434195200 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.7082664966583252, loss=5.758045673370361
I0201 13:45:24.444112 139774417409792 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.9031977653503418, loss=5.327182769775391
I0201 13:46:10.745977 139774434195200 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.114166021347046, loss=5.196053981781006
I0201 13:46:56.905788 139774417409792 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.2553229331970215, loss=5.106658935546875
I0201 13:47:43.116209 139774434195200 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.9696153402328491, loss=5.047003746032715
I0201 13:47:56.579206 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:48:09.304328 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:48:41.757111 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:48:43.366761 139936116377408 submission_runner.py:408] Time since start: 2855.22s, 	Step: 5431, 	{'train/accuracy': 0.16908203065395355, 'train/loss': 4.369624137878418, 'validation/accuracy': 0.15493999421596527, 'validation/loss': 4.4540791511535645, 'validation/num_examples': 50000, 'test/accuracy': 0.11290000379085541, 'test/loss': 4.7985992431640625, 'test/num_examples': 10000, 'score': 2558.279573202133, 'total_duration': 2855.2203080654144, 'accumulated_submission_time': 2558.279573202133, 'accumulated_eval_time': 296.49737071990967, 'accumulated_logging_time': 0.1598045825958252}
I0201 13:48:43.384298 139774417409792 logging_writer.py:48] [5431] accumulated_eval_time=296.497371, accumulated_logging_time=0.159805, accumulated_submission_time=2558.279573, global_step=5431, preemption_count=0, score=2558.279573, test/accuracy=0.112900, test/loss=4.798599, test/num_examples=10000, total_duration=2855.220308, train/accuracy=0.169082, train/loss=4.369624, validation/accuracy=0.154940, validation/loss=4.454079, validation/num_examples=50000
I0201 13:49:11.172674 139774434195200 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.794502854347229, loss=5.040096759796143
I0201 13:49:56.336743 139774417409792 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.492020845413208, loss=6.301397800445557
I0201 13:50:42.718751 139774434195200 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.2457729578018188, loss=6.290464401245117
I0201 13:51:28.947968 139774417409792 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.9661821126937866, loss=5.119581699371338
I0201 13:52:15.280645 139774434195200 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.004505157470703, loss=4.846854209899902
I0201 13:53:01.469592 139774417409792 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.7821272611618042, loss=4.915102481842041
I0201 13:53:47.646573 139774434195200 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.0059077739715576, loss=4.871615409851074
I0201 13:54:33.745509 139774417409792 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.8330775499343872, loss=5.253162384033203
I0201 13:55:20.319890 139774434195200 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.4815374612808228, loss=6.259864330291748
I0201 13:55:43.859832 139936116377408 spec.py:321] Evaluating on the training split.
I0201 13:55:56.426663 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 13:56:21.180605 139936116377408 spec.py:349] Evaluating on the test split.
I0201 13:56:22.815070 139936116377408 submission_runner.py:408] Time since start: 3314.67s, 	Step: 6353, 	{'train/accuracy': 0.20998045802116394, 'train/loss': 4.027159690856934, 'validation/accuracy': 0.19377999007701874, 'validation/loss': 4.129310607910156, 'validation/num_examples': 50000, 'test/accuracy': 0.14410001039505005, 'test/loss': 4.515932083129883, 'test/num_examples': 10000, 'score': 2978.6933076381683, 'total_duration': 3314.668624162674, 'accumulated_submission_time': 2978.6933076381683, 'accumulated_eval_time': 335.4525935649872, 'accumulated_logging_time': 0.19125723838806152}
I0201 13:56:22.831506 139774417409792 logging_writer.py:48] [6353] accumulated_eval_time=335.452594, accumulated_logging_time=0.191257, accumulated_submission_time=2978.693308, global_step=6353, preemption_count=0, score=2978.693308, test/accuracy=0.144100, test/loss=4.515932, test/num_examples=10000, total_duration=3314.668624, train/accuracy=0.209980, train/loss=4.027160, validation/accuracy=0.193780, validation/loss=4.129311, validation/num_examples=50000
I0201 13:56:41.891669 139774434195200 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.465606451034546, loss=5.694707870483398
I0201 13:57:26.273532 139774417409792 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.245168685913086, loss=4.939999580383301
I0201 13:58:12.740438 139774434195200 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.9618340730667114, loss=4.820116996765137
I0201 13:58:59.250433 139774417409792 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.8100519180297852, loss=5.101065635681152
I0201 13:59:45.448586 139774434195200 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.5920476913452148, loss=4.856794357299805
I0201 14:00:31.573553 139774417409792 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.1513757705688477, loss=4.622011661529541
I0201 14:01:17.587543 139774434195200 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.8509057760238647, loss=4.667993545532227
I0201 14:02:03.844841 139774417409792 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.004734516143799, loss=4.51904296875
I0201 14:02:49.850548 139774434195200 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.835192322731018, loss=4.580561637878418
I0201 14:03:23.184309 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:03:35.763628 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:04:07.063605 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:04:08.665591 139936116377408 submission_runner.py:408] Time since start: 3780.52s, 	Step: 7274, 	{'train/accuracy': 0.24982421100139618, 'train/loss': 3.7537617683410645, 'validation/accuracy': 0.23155999183654785, 'validation/loss': 3.8504300117492676, 'validation/num_examples': 50000, 'test/accuracy': 0.1762000024318695, 'test/loss': 4.282338619232178, 'test/num_examples': 10000, 'score': 3398.987615585327, 'total_duration': 3780.519155740738, 'accumulated_submission_time': 3398.987615585327, 'accumulated_eval_time': 380.9338798522949, 'accumulated_logging_time': 0.21874260902404785}
I0201 14:04:08.682550 139774417409792 logging_writer.py:48] [7274] accumulated_eval_time=380.933880, accumulated_logging_time=0.218743, accumulated_submission_time=3398.987616, global_step=7274, preemption_count=0, score=3398.987616, test/accuracy=0.176200, test/loss=4.282339, test/num_examples=10000, total_duration=3780.519156, train/accuracy=0.249824, train/loss=3.753762, validation/accuracy=0.231560, validation/loss=3.850430, validation/num_examples=50000
I0201 14:04:19.396265 139774434195200 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.602460503578186, loss=6.227555274963379
I0201 14:05:02.277695 139774417409792 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.3231680393218994, loss=4.551799774169922
I0201 14:05:48.355155 139774434195200 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.924382209777832, loss=4.40955924987793
I0201 14:06:34.604362 139774417409792 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.8360812664031982, loss=4.751377582550049
I0201 14:07:20.614632 139774434195200 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.9881336688995361, loss=4.383936405181885
I0201 14:08:06.572656 139774417409792 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.6307766437530518, loss=4.534711837768555
I0201 14:08:52.755843 139774434195200 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.231539487838745, loss=4.854646682739258
I0201 14:09:38.964219 139774417409792 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.559483289718628, loss=6.043805122375488
I0201 14:10:25.306889 139774434195200 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.9945523738861084, loss=4.472112655639648
I0201 14:11:08.762107 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:11:21.232967 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:11:53.242887 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:11:54.848956 139936116377408 submission_runner.py:408] Time since start: 4246.70s, 	Step: 8196, 	{'train/accuracy': 0.2862304747104645, 'train/loss': 3.5035860538482666, 'validation/accuracy': 0.2628999948501587, 'validation/loss': 3.6230437755584717, 'validation/num_examples': 50000, 'test/accuracy': 0.20350000262260437, 'test/loss': 4.090365886688232, 'test/num_examples': 10000, 'score': 3819.009551525116, 'total_duration': 4246.702522277832, 'accumulated_submission_time': 3819.009551525116, 'accumulated_eval_time': 427.0207488536835, 'accumulated_logging_time': 0.24491047859191895}
I0201 14:11:54.867389 139774417409792 logging_writer.py:48] [8196] accumulated_eval_time=427.020749, accumulated_logging_time=0.244910, accumulated_submission_time=3819.009552, global_step=8196, preemption_count=0, score=3819.009552, test/accuracy=0.203500, test/loss=4.090366, test/num_examples=10000, total_duration=4246.702522, train/accuracy=0.286230, train/loss=3.503586, validation/accuracy=0.262900, validation/loss=3.623044, validation/num_examples=50000
I0201 14:11:56.858931 139774434195200 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.4758291244506836, loss=5.296319007873535
I0201 14:12:38.380004 139774417409792 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8336091041564941, loss=4.474757194519043
I0201 14:13:24.313281 139774434195200 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.817821979522705, loss=4.226605415344238
I0201 14:14:10.849188 139774417409792 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.9962760210037231, loss=4.309154033660889
I0201 14:14:56.997129 139774434195200 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.5278294086456299, loss=5.148021697998047
I0201 14:15:43.317821 139774417409792 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.149865746498108, loss=5.836866855621338
I0201 14:16:30.028949 139774434195200 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.017711877822876, loss=4.157614231109619
I0201 14:17:16.142077 139774417409792 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.579098105430603, loss=5.744415283203125
I0201 14:18:02.627030 139774434195200 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.9514119625091553, loss=4.604743003845215
I0201 14:18:49.222352 139774417409792 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.067746162414551, loss=4.1485466957092285
I0201 14:18:54.972454 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:19:07.766855 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:19:39.122655 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:19:40.729662 139936116377408 submission_runner.py:408] Time since start: 4712.58s, 	Step: 9114, 	{'train/accuracy': 0.33785155415534973, 'train/loss': 3.160243511199951, 'validation/accuracy': 0.30473998188972473, 'validation/loss': 3.333974599838257, 'validation/num_examples': 50000, 'test/accuracy': 0.2339000105857849, 'test/loss': 3.833681344985962, 'test/num_examples': 10000, 'score': 4239.054158687592, 'total_duration': 4712.58321595192, 'accumulated_submission_time': 4239.054158687592, 'accumulated_eval_time': 472.77794551849365, 'accumulated_logging_time': 0.27527666091918945}
I0201 14:19:40.745476 139774434195200 logging_writer.py:48] [9114] accumulated_eval_time=472.777946, accumulated_logging_time=0.275277, accumulated_submission_time=4239.054159, global_step=9114, preemption_count=0, score=4239.054159, test/accuracy=0.233900, test/loss=3.833681, test/num_examples=10000, total_duration=4712.583216, train/accuracy=0.337852, train/loss=3.160244, validation/accuracy=0.304740, validation/loss=3.333975, validation/num_examples=50000
I0201 14:20:16.238823 139774417409792 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.200786828994751, loss=4.061712265014648
I0201 14:21:02.066302 139774434195200 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.6489081382751465, loss=5.469296932220459
I0201 14:21:48.198154 139774417409792 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.8656775951385498, loss=3.910430908203125
I0201 14:22:34.400599 139774434195200 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.676790714263916, loss=6.089743614196777
I0201 14:23:20.279528 139774417409792 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.3723355531692505, loss=5.774538993835449
I0201 14:24:06.362913 139774434195200 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.3701199293136597, loss=5.890382766723633
I0201 14:24:52.191144 139774417409792 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.2022063732147217, loss=3.9788928031921387
I0201 14:25:38.386026 139774434195200 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.4567922353744507, loss=5.93769645690918
I0201 14:26:24.656397 139774417409792 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.8343496322631836, loss=3.861510753631592
I0201 14:26:40.763040 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:26:53.385576 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:27:25.693564 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:27:27.302369 139936116377408 submission_runner.py:408] Time since start: 5179.16s, 	Step: 10037, 	{'train/accuracy': 0.35658201575279236, 'train/loss': 3.0313377380371094, 'validation/accuracy': 0.3318600058555603, 'validation/loss': 3.1532087326049805, 'validation/num_examples': 50000, 'test/accuracy': 0.2568000257015228, 'test/loss': 3.6938228607177734, 'test/num_examples': 10000, 'score': 4659.012718915939, 'total_duration': 5179.155911445618, 'accumulated_submission_time': 4659.012718915939, 'accumulated_eval_time': 519.3172528743744, 'accumulated_logging_time': 0.30214810371398926}
I0201 14:27:27.320574 139774434195200 logging_writer.py:48] [10037] accumulated_eval_time=519.317253, accumulated_logging_time=0.302148, accumulated_submission_time=4659.012719, global_step=10037, preemption_count=0, score=4659.012719, test/accuracy=0.256800, test/loss=3.693823, test/num_examples=10000, total_duration=5179.155911, train/accuracy=0.356582, train/loss=3.031338, validation/accuracy=0.331860, validation/loss=3.153209, validation/num_examples=50000
I0201 14:27:52.737317 139774417409792 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.4691332578659058, loss=5.9953694343566895
I0201 14:28:37.753415 139774434195200 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.4805492162704468, loss=5.300229549407959
I0201 14:29:24.042733 139774417409792 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.5690748691558838, loss=5.193521499633789
I0201 14:30:10.377712 139774434195200 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.542100429534912, loss=4.69625997543335
I0201 14:30:56.183168 139774417409792 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.7227493524551392, loss=4.675318241119385
I0201 14:31:42.382335 139774434195200 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.9983863830566406, loss=3.882436513900757
I0201 14:32:28.508300 139774417409792 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.9526333808898926, loss=3.871739149093628
I0201 14:33:14.616630 139774434195200 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.5507961511611938, loss=3.83414363861084
I0201 14:34:00.722777 139774417409792 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.3425605297088623, loss=5.053888320922852
I0201 14:34:27.568899 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:34:40.176001 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:35:11.661160 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:35:13.261833 139936116377408 submission_runner.py:408] Time since start: 5645.12s, 	Step: 10960, 	{'train/accuracy': 0.3778125047683716, 'train/loss': 2.9234459400177, 'validation/accuracy': 0.34775999188423157, 'validation/loss': 3.085547685623169, 'validation/num_examples': 50000, 'test/accuracy': 0.2639000117778778, 'test/loss': 3.631510019302368, 'test/num_examples': 10000, 'score': 5079.202866315842, 'total_duration': 5645.115402698517, 'accumulated_submission_time': 5079.202866315842, 'accumulated_eval_time': 565.0101907253265, 'accumulated_logging_time': 0.33082032203674316}
I0201 14:35:13.277744 139774434195200 logging_writer.py:48] [10960] accumulated_eval_time=565.010191, accumulated_logging_time=0.330820, accumulated_submission_time=5079.202866, global_step=10960, preemption_count=0, score=5079.202866, test/accuracy=0.263900, test/loss=3.631510, test/num_examples=10000, total_duration=5645.115403, train/accuracy=0.377813, train/loss=2.923446, validation/accuracy=0.347760, validation/loss=3.085548, validation/num_examples=50000
I0201 14:35:29.550548 139774417409792 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.9622468948364258, loss=3.785567045211792
I0201 14:36:13.432429 139774434195200 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.829077124595642, loss=3.7895288467407227
I0201 14:36:59.504826 139774417409792 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.9562143087387085, loss=3.9750473499298096
I0201 14:37:46.013705 139774434195200 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.6656752824783325, loss=3.8363566398620605
I0201 14:38:31.854498 139774417409792 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.4100717306137085, loss=5.776902198791504
I0201 14:39:18.068762 139774434195200 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.3461405038833618, loss=4.55648946762085
I0201 14:40:04.168234 139774417409792 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.994840145111084, loss=3.752859354019165
I0201 14:40:50.161708 139774434195200 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.4315015077590942, loss=5.155243396759033
I0201 14:41:36.778673 139774417409792 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.8958392143249512, loss=3.6813509464263916
I0201 14:42:13.407226 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:42:25.955363 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:42:53.743289 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:42:55.350528 139936116377408 submission_runner.py:408] Time since start: 6107.20s, 	Step: 11881, 	{'train/accuracy': 0.41001951694488525, 'train/loss': 2.7307326793670654, 'validation/accuracy': 0.37143999338150024, 'validation/loss': 2.922337532043457, 'validation/num_examples': 50000, 'test/accuracy': 0.2891000211238861, 'test/loss': 3.4765114784240723, 'test/num_examples': 10000, 'score': 5499.275423049927, 'total_duration': 6107.204093456268, 'accumulated_submission_time': 5499.275423049927, 'accumulated_eval_time': 606.9534866809845, 'accumulated_logging_time': 0.3557870388031006}
I0201 14:42:55.367246 139774434195200 logging_writer.py:48] [11881] accumulated_eval_time=606.953487, accumulated_logging_time=0.355787, accumulated_submission_time=5499.275423, global_step=11881, preemption_count=0, score=5499.275423, test/accuracy=0.289100, test/loss=3.476511, test/num_examples=10000, total_duration=6107.204093, train/accuracy=0.410020, train/loss=2.730733, validation/accuracy=0.371440, validation/loss=2.922338, validation/num_examples=50000
I0201 14:43:03.308021 139774417409792 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.281475305557251, loss=5.78522253036499
I0201 14:43:45.794098 139774434195200 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.326288938522339, loss=3.494887351989746
I0201 14:44:32.245630 139774417409792 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.461046814918518, loss=4.23546028137207
I0201 14:45:18.707082 139774434195200 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.5129292011260986, loss=3.8746211528778076
I0201 14:46:04.802519 139774417409792 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.8864331245422363, loss=3.628190517425537
I0201 14:46:50.908850 139774434195200 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.5158989429473877, loss=3.7467331886291504
I0201 14:47:37.458393 139774417409792 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.045346260070801, loss=3.5901386737823486
I0201 14:48:23.529422 139774434195200 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.875794768333435, loss=4.283825874328613
I0201 14:49:09.746061 139774417409792 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.8318159580230713, loss=3.6591193675994873
I0201 14:49:55.458451 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:50:08.068595 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:50:40.128991 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:50:41.731902 139936116377408 submission_runner.py:408] Time since start: 6573.59s, 	Step: 12800, 	{'train/accuracy': 0.42429685592651367, 'train/loss': 2.650355815887451, 'validation/accuracy': 0.3928999900817871, 'validation/loss': 2.803166151046753, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.3898091316223145, 'test/num_examples': 10000, 'score': 5919.309090852737, 'total_duration': 6573.585469007492, 'accumulated_submission_time': 5919.309090852737, 'accumulated_eval_time': 653.2269532680511, 'accumulated_logging_time': 0.38291049003601074}
I0201 14:50:41.749444 139774434195200 logging_writer.py:48] [12800] accumulated_eval_time=653.226953, accumulated_logging_time=0.382910, accumulated_submission_time=5919.309091, global_step=12800, preemption_count=0, score=5919.309091, test/accuracy=0.299200, test/loss=3.389809, test/num_examples=10000, total_duration=6573.585469, train/accuracy=0.424297, train/loss=2.650356, validation/accuracy=0.392900, validation/loss=2.803166, validation/num_examples=50000
I0201 14:50:42.633712 139774417409792 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.475555419921875, loss=5.0981526374816895
I0201 14:51:24.039497 139774434195200 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6644996404647827, loss=3.537203073501587
I0201 14:52:09.992699 139774417409792 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.7837775945663452, loss=3.6083743572235107
I0201 14:52:56.378159 139774434195200 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.5982773303985596, loss=3.7343103885650635
I0201 14:53:42.661984 139774417409792 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.9458287954330444, loss=3.556140661239624
I0201 14:54:28.900137 139774434195200 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.582970142364502, loss=3.483619213104248
I0201 14:55:14.872320 139774417409792 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.063147783279419, loss=5.868905067443848
I0201 14:56:00.945790 139774434195200 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.81258225440979, loss=3.536257028579712
I0201 14:56:47.276252 139774417409792 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.3747491836547852, loss=5.183558940887451
I0201 14:57:33.396418 139774434195200 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.2230415344238281, loss=5.102428436279297
I0201 14:57:41.877867 139936116377408 spec.py:321] Evaluating on the training split.
I0201 14:57:54.469391 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 14:58:24.169502 139936116377408 spec.py:349] Evaluating on the test split.
I0201 14:58:25.769515 139936116377408 submission_runner.py:408] Time since start: 7037.62s, 	Step: 13720, 	{'train/accuracy': 0.4307031035423279, 'train/loss': 2.6292409896850586, 'validation/accuracy': 0.3979800045490265, 'validation/loss': 2.789198398590088, 'validation/num_examples': 50000, 'test/accuracy': 0.3100000023841858, 'test/loss': 3.3641951084136963, 'test/num_examples': 10000, 'score': 6338.901314973831, 'total_duration': 7037.623072147369, 'accumulated_submission_time': 6338.901314973831, 'accumulated_eval_time': 697.1185910701752, 'accumulated_logging_time': 0.8888986110687256}
I0201 14:58:25.785674 139774417409792 logging_writer.py:48] [13720] accumulated_eval_time=697.118591, accumulated_logging_time=0.888899, accumulated_submission_time=6338.901315, global_step=13720, preemption_count=0, score=6338.901315, test/accuracy=0.310000, test/loss=3.364195, test/num_examples=10000, total_duration=7037.623072, train/accuracy=0.430703, train/loss=2.629241, validation/accuracy=0.397980, validation/loss=2.789198, validation/num_examples=50000
I0201 14:58:58.539492 139774434195200 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.3717750310897827, loss=4.649185657501221
I0201 14:59:44.355398 139774417409792 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.1616871356964111, loss=5.557545185089111
I0201 15:00:30.537161 139774434195200 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.7681846618652344, loss=3.493396520614624
I0201 15:01:16.848879 139774417409792 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.7354320287704468, loss=3.4871857166290283
I0201 15:02:03.320013 139774434195200 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.628228783607483, loss=3.523224353790283
I0201 15:02:49.281075 139774417409792 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.2978018522262573, loss=4.916030406951904
I0201 15:03:35.493290 139774434195200 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.6247645616531372, loss=3.609816312789917
I0201 15:04:21.766432 139774417409792 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.679320216178894, loss=3.4189841747283936
I0201 15:05:07.903428 139774434195200 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.790755033493042, loss=3.4702367782592773
I0201 15:05:26.076857 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:05:38.680552 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 15:06:00.808564 139936116377408 spec.py:349] Evaluating on the test split.
I0201 15:06:02.436631 139936116377408 submission_runner.py:408] Time since start: 7494.29s, 	Step: 14641, 	{'train/accuracy': 0.44679686427116394, 'train/loss': 2.5055689811706543, 'validation/accuracy': 0.4116399884223938, 'validation/loss': 2.6887874603271484, 'validation/num_examples': 50000, 'test/accuracy': 0.31620001792907715, 'test/loss': 3.266141653060913, 'test/num_examples': 10000, 'score': 6759.135899543762, 'total_duration': 7494.290160655975, 'accumulated_submission_time': 6759.135899543762, 'accumulated_eval_time': 733.4783155918121, 'accumulated_logging_time': 0.914440393447876}
I0201 15:06:02.460068 139774417409792 logging_writer.py:48] [14641] accumulated_eval_time=733.478316, accumulated_logging_time=0.914440, accumulated_submission_time=6759.135900, global_step=14641, preemption_count=0, score=6759.135900, test/accuracy=0.316200, test/loss=3.266142, test/num_examples=10000, total_duration=7494.290161, train/accuracy=0.446797, train/loss=2.505569, validation/accuracy=0.411640, validation/loss=2.688787, validation/num_examples=50000
I0201 15:06:26.476042 139774434195200 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.7177815437316895, loss=3.375361680984497
I0201 15:07:13.198803 139774417409792 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.5560503005981445, loss=3.4478302001953125
I0201 15:07:59.159003 139774434195200 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.108367681503296, loss=5.896974563598633
I0201 15:08:46.204022 139774417409792 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.6101410388946533, loss=4.397274494171143
I0201 15:09:32.400248 139774434195200 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.6582250595092773, loss=3.350338935852051
I0201 15:10:18.728157 139774417409792 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.3863041400909424, loss=4.066707134246826
I0201 15:11:04.856388 139774434195200 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.5756726264953613, loss=3.8321759700775146
I0201 15:11:51.255082 139774417409792 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.6891857385635376, loss=3.402017116546631
I0201 15:12:37.559627 139774434195200 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.115752100944519, loss=5.465580463409424
I0201 15:13:02.681133 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:13:15.230094 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 15:13:37.790570 139936116377408 spec.py:349] Evaluating on the test split.
I0201 15:13:39.401283 139936116377408 submission_runner.py:408] Time since start: 7951.25s, 	Step: 15556, 	{'train/accuracy': 0.4633203148841858, 'train/loss': 2.4123597145080566, 'validation/accuracy': 0.43077999353408813, 'validation/loss': 2.577693462371826, 'validation/num_examples': 50000, 'test/accuracy': 0.3327000141143799, 'test/loss': 3.1715259552001953, 'test/num_examples': 10000, 'score': 7179.2973692417145, 'total_duration': 7951.254838228226, 'accumulated_submission_time': 7179.2973692417145, 'accumulated_eval_time': 770.1984448432922, 'accumulated_logging_time': 0.9485998153686523}
I0201 15:13:39.421329 139774417409792 logging_writer.py:48] [15556] accumulated_eval_time=770.198445, accumulated_logging_time=0.948600, accumulated_submission_time=7179.297369, global_step=15556, preemption_count=0, score=7179.297369, test/accuracy=0.332700, test/loss=3.171526, test/num_examples=10000, total_duration=7951.254838, train/accuracy=0.463320, train/loss=2.412360, validation/accuracy=0.430780, validation/loss=2.577693, validation/num_examples=50000
I0201 15:13:57.299027 139774434195200 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.256117582321167, loss=4.587532043457031
I0201 15:14:42.324879 139774417409792 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.6435269117355347, loss=3.409191846847534
I0201 15:15:28.521579 139774434195200 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9922013282775879, loss=5.688113212585449
I0201 15:16:14.986803 139774417409792 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5045857429504395, loss=3.8222153186798096
I0201 15:17:01.127972 139774434195200 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.2918412685394287, loss=4.290595531463623
I0201 15:17:47.374932 139774417409792 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.1415894031524658, loss=5.669033050537109
I0201 15:18:33.540428 139774434195200 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.726929783821106, loss=3.4051451683044434
I0201 15:19:20.173899 139774417409792 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.7105481624603271, loss=3.5650157928466797
I0201 15:20:06.430687 139774434195200 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.476353645324707, loss=3.402026891708374
I0201 15:20:39.835845 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:20:52.208378 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 15:21:27.378157 139936116377408 spec.py:349] Evaluating on the test split.
I0201 15:21:28.991070 139936116377408 submission_runner.py:408] Time since start: 8420.84s, 	Step: 16474, 	{'train/accuracy': 0.4699999988079071, 'train/loss': 2.36291766166687, 'validation/accuracy': 0.43789997696876526, 'validation/loss': 2.5307412147521973, 'validation/num_examples': 50000, 'test/accuracy': 0.3393000066280365, 'test/loss': 3.1288845539093018, 'test/num_examples': 10000, 'score': 7599.65398979187, 'total_duration': 8420.844632863998, 'accumulated_submission_time': 7599.65398979187, 'accumulated_eval_time': 819.353661775589, 'accumulated_logging_time': 0.9791200160980225}
I0201 15:21:29.008535 139774417409792 logging_writer.py:48] [16474] accumulated_eval_time=819.353662, accumulated_logging_time=0.979120, accumulated_submission_time=7599.653990, global_step=16474, preemption_count=0, score=7599.653990, test/accuracy=0.339300, test/loss=3.128885, test/num_examples=10000, total_duration=8420.844633, train/accuracy=0.470000, train/loss=2.362918, validation/accuracy=0.437900, validation/loss=2.530741, validation/num_examples=50000
I0201 15:21:39.724133 139774434195200 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.5601791143417358, loss=3.308084726333618
I0201 15:22:21.902787 139774417409792 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.8097001314163208, loss=3.4545717239379883
I0201 15:23:08.208372 139774434195200 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.534138560295105, loss=3.2893054485321045
I0201 15:23:54.611023 139774417409792 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.6873551607131958, loss=3.5208029747009277
I0201 15:24:40.555123 139774434195200 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.3426649570465088, loss=4.773001194000244
I0201 15:25:26.595035 139774417409792 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.5228105783462524, loss=3.4488134384155273
I0201 15:26:12.532774 139774434195200 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.5710597038269043, loss=3.27455997467041
I0201 15:26:58.687403 139774417409792 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.6070057153701782, loss=3.433056116104126
I0201 15:27:45.037471 139774434195200 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.6999711990356445, loss=3.2092785835266113
I0201 15:28:29.105642 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:28:41.291849 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 15:29:09.445191 139936116377408 spec.py:349] Evaluating on the test split.
I0201 15:29:11.116367 139936116377408 submission_runner.py:408] Time since start: 8882.97s, 	Step: 17397, 	{'train/accuracy': 0.47802734375, 'train/loss': 2.3279898166656494, 'validation/accuracy': 0.4379799962043762, 'validation/loss': 2.5215203762054443, 'validation/num_examples': 50000, 'test/accuracy': 0.3379000127315521, 'test/loss': 3.118520498275757, 'test/num_examples': 10000, 'score': 8019.693618297577, 'total_duration': 8882.96993303299, 'accumulated_submission_time': 8019.693618297577, 'accumulated_eval_time': 861.3643939495087, 'accumulated_logging_time': 1.0059688091278076}
I0201 15:29:11.136320 139774417409792 logging_writer.py:48] [17397] accumulated_eval_time=861.364394, accumulated_logging_time=1.005969, accumulated_submission_time=8019.693618, global_step=17397, preemption_count=0, score=8019.693618, test/accuracy=0.337900, test/loss=3.118520, test/num_examples=10000, total_duration=8882.969933, train/accuracy=0.478027, train/loss=2.327990, validation/accuracy=0.437980, validation/loss=2.521520, validation/num_examples=50000
I0201 15:29:12.721773 139774434195200 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.3717732429504395, loss=4.241036891937256
I0201 15:29:53.928899 139774417409792 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.5934323072433472, loss=3.391319751739502
I0201 15:30:40.140814 139774434195200 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.9397799968719482, loss=3.3966331481933594
I0201 15:31:26.529747 139774417409792 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.5145537853240967, loss=3.390451431274414
I0201 15:32:13.116928 139774434195200 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.487251877784729, loss=3.573759078979492
I0201 15:32:59.194380 139774417409792 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.780017614364624, loss=3.368194341659546
I0201 15:33:45.312412 139774434195200 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.895231008529663, loss=3.342390537261963
I0201 15:34:31.716314 139774417409792 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0818390846252441, loss=5.663876533508301
I0201 15:35:18.016166 139774434195200 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.7920001745224, loss=3.235724925994873
I0201 15:36:04.155066 139774417409792 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.6185898780822754, loss=3.482361078262329
I0201 15:36:11.254445 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:36:23.819025 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 15:36:53.489406 139936116377408 spec.py:349] Evaluating on the test split.
I0201 15:36:55.091080 139936116377408 submission_runner.py:408] Time since start: 9346.94s, 	Step: 18317, 	{'train/accuracy': 0.4999414086341858, 'train/loss': 2.242021322250366, 'validation/accuracy': 0.44091999530792236, 'validation/loss': 2.529330015182495, 'validation/num_examples': 50000, 'test/accuracy': 0.33970001339912415, 'test/loss': 3.14664888381958, 'test/num_examples': 10000, 'score': 8439.754869222641, 'total_duration': 9346.944638490677, 'accumulated_submission_time': 8439.754869222641, 'accumulated_eval_time': 905.2010207176208, 'accumulated_logging_time': 1.0355210304260254}
I0201 15:36:55.108915 139774434195200 logging_writer.py:48] [18317] accumulated_eval_time=905.201021, accumulated_logging_time=1.035521, accumulated_submission_time=8439.754869, global_step=18317, preemption_count=0, score=8439.754869, test/accuracy=0.339700, test/loss=3.146649, test/num_examples=10000, total_duration=9346.944638, train/accuracy=0.499941, train/loss=2.242021, validation/accuracy=0.440920, validation/loss=2.529330, validation/num_examples=50000
I0201 15:37:29.260642 139774417409792 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.9984644651412964, loss=5.658810615539551
I0201 15:38:15.214506 139774434195200 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.5228428840637207, loss=3.336954116821289
I0201 15:39:01.495520 139774417409792 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.4368821382522583, loss=3.2915985584259033
I0201 15:39:47.525153 139774434195200 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.8263589143753052, loss=3.3593268394470215
I0201 15:40:34.231402 139774417409792 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.7503023147583008, loss=3.2755439281463623
I0201 15:41:20.709375 139774434195200 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.637292742729187, loss=3.2311017513275146
I0201 15:42:07.241709 139774417409792 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.5522921085357666, loss=3.2429795265197754
I0201 15:42:53.392697 139774434195200 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.3911656141281128, loss=5.033899307250977
I0201 15:43:39.560681 139774417409792 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.6115498542785645, loss=3.594512462615967
I0201 15:43:55.508468 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:44:08.148739 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 15:44:39.389356 139936116377408 spec.py:349] Evaluating on the test split.
I0201 15:44:41.002087 139936116377408 submission_runner.py:408] Time since start: 9812.86s, 	Step: 19236, 	{'train/accuracy': 0.48960936069488525, 'train/loss': 2.2664780616760254, 'validation/accuracy': 0.4534199833869934, 'validation/loss': 2.4464144706726074, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.072669744491577, 'test/num_examples': 10000, 'score': 8860.098046064377, 'total_duration': 9812.855647802353, 'accumulated_submission_time': 8860.098046064377, 'accumulated_eval_time': 950.6946420669556, 'accumulated_logging_time': 1.0625567436218262}
I0201 15:44:41.019070 139774434195200 logging_writer.py:48] [19236] accumulated_eval_time=950.694642, accumulated_logging_time=1.062557, accumulated_submission_time=8860.098046, global_step=19236, preemption_count=0, score=8860.098046, test/accuracy=0.347300, test/loss=3.072670, test/num_examples=10000, total_duration=9812.855648, train/accuracy=0.489609, train/loss=2.266478, validation/accuracy=0.453420, validation/loss=2.446414, validation/num_examples=50000
I0201 15:45:06.873326 139774417409792 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.4888062477111816, loss=3.3057823181152344
I0201 15:45:51.993055 139774434195200 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.3569397926330566, loss=3.7027335166931152
I0201 15:46:38.258735 139774417409792 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.6661145687103271, loss=3.238532543182373
I0201 15:47:24.555288 139774434195200 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.701104760169983, loss=3.3963403701782227
I0201 15:48:10.489609 139774417409792 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.5910558700561523, loss=3.2417092323303223
I0201 15:48:56.523866 139774434195200 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.4711194038391113, loss=3.6058168411254883
I0201 15:49:42.664697 139774417409792 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.4568811655044556, loss=3.5480151176452637
I0201 15:50:28.587581 139774434195200 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.5855867862701416, loss=3.215005397796631
I0201 15:51:15.096822 139774417409792 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.3486063480377197, loss=4.333436965942383
I0201 15:51:41.455196 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:51:54.060693 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 15:52:21.276105 139936116377408 spec.py:349] Evaluating on the test split.
I0201 15:52:22.879538 139936116377408 submission_runner.py:408] Time since start: 10274.73s, 	Step: 20159, 	{'train/accuracy': 0.4975976347923279, 'train/loss': 2.2221951484680176, 'validation/accuracy': 0.461139976978302, 'validation/loss': 2.4042558670043945, 'validation/num_examples': 50000, 'test/accuracy': 0.35190001130104065, 'test/loss': 3.0374844074249268, 'test/num_examples': 10000, 'score': 9280.475875854492, 'total_duration': 10274.733105897903, 'accumulated_submission_time': 9280.475875854492, 'accumulated_eval_time': 992.1189947128296, 'accumulated_logging_time': 1.0902252197265625}
I0201 15:52:22.898411 139774434195200 logging_writer.py:48] [20159] accumulated_eval_time=992.118995, accumulated_logging_time=1.090225, accumulated_submission_time=9280.475876, global_step=20159, preemption_count=0, score=9280.475876, test/accuracy=0.351900, test/loss=3.037484, test/num_examples=10000, total_duration=10274.733106, train/accuracy=0.497598, train/loss=2.222195, validation/accuracy=0.461140, validation/loss=2.404256, validation/num_examples=50000
I0201 15:52:39.606858 139774417409792 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1715682744979858, loss=5.696170806884766
I0201 15:53:23.434496 139774434195200 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.2631531953811646, loss=4.038713455200195
I0201 15:54:09.690609 139774417409792 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0945014953613281, loss=5.621057987213135
I0201 15:54:55.974435 139774434195200 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.01161527633667, loss=5.669754981994629
I0201 15:55:42.062811 139774417409792 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.5626561641693115, loss=3.3469574451446533
I0201 15:56:28.354766 139774434195200 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.6174132823944092, loss=3.1373438835144043
I0201 15:57:14.655551 139774417409792 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.3163596391677856, loss=5.100175857543945
I0201 15:58:00.723295 139774434195200 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.3229494094848633, loss=4.081718444824219
I0201 15:58:47.076814 139774417409792 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9760472178459167, loss=5.038168430328369
I0201 15:59:23.231089 139936116377408 spec.py:321] Evaluating on the training split.
I0201 15:59:35.778465 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:00:07.824782 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:00:09.427160 139936116377408 submission_runner.py:408] Time since start: 10741.28s, 	Step: 21080, 	{'train/accuracy': 0.5208203196525574, 'train/loss': 2.0787808895111084, 'validation/accuracy': 0.4711199998855591, 'validation/loss': 2.326721429824829, 'validation/num_examples': 50000, 'test/accuracy': 0.36650002002716064, 'test/loss': 2.948438882827759, 'test/num_examples': 10000, 'score': 9700.750081539154, 'total_duration': 10741.280704021454, 'accumulated_submission_time': 9700.750081539154, 'accumulated_eval_time': 1038.3150515556335, 'accumulated_logging_time': 1.1201841831207275}
I0201 16:00:09.448948 139774434195200 logging_writer.py:48] [21080] accumulated_eval_time=1038.315052, accumulated_logging_time=1.120184, accumulated_submission_time=9700.750082, global_step=21080, preemption_count=0, score=9700.750082, test/accuracy=0.366500, test/loss=2.948439, test/num_examples=10000, total_duration=10741.280704, train/accuracy=0.520820, train/loss=2.078781, validation/accuracy=0.471120, validation/loss=2.326721, validation/num_examples=50000
I0201 16:00:17.816989 139774417409792 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.4581493139266968, loss=3.275386095046997
I0201 16:01:00.076535 139774434195200 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.6943707466125488, loss=3.3907570838928223
I0201 16:01:46.719633 139774417409792 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0190563201904297, loss=5.651691913604736
I0201 16:02:32.897152 139774434195200 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.357109785079956, loss=3.967297077178955
I0201 16:03:19.098968 139774417409792 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0005403757095337, loss=5.498342514038086
I0201 16:04:05.430320 139774434195200 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.0311516523361206, loss=5.493676662445068
I0201 16:04:51.435469 139774417409792 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0209282636642456, loss=5.361196517944336
I0201 16:05:37.329231 139774434195200 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.3955810070037842, loss=3.1843438148498535
I0201 16:06:23.585668 139774417409792 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.576957106590271, loss=3.282473564147949
I0201 16:07:09.938468 139774434195200 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5545045137405396, loss=3.167247772216797
I0201 16:07:09.954184 139936116377408 spec.py:321] Evaluating on the training split.
I0201 16:07:22.439849 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:07:54.742309 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:07:56.346701 139936116377408 submission_runner.py:408] Time since start: 11208.20s, 	Step: 22001, 	{'train/accuracy': 0.5149999856948853, 'train/loss': 2.116328239440918, 'validation/accuracy': 0.47957998514175415, 'validation/loss': 2.289320707321167, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.8965604305267334, 'test/num_examples': 10000, 'score': 10121.196420192719, 'total_duration': 11208.20026922226, 'accumulated_submission_time': 10121.196420192719, 'accumulated_eval_time': 1084.7075653076172, 'accumulated_logging_time': 1.152724266052246}
I0201 16:07:56.364977 139774417409792 logging_writer.py:48] [22001] accumulated_eval_time=1084.707565, accumulated_logging_time=1.152724, accumulated_submission_time=10121.196420, global_step=22001, preemption_count=0, score=10121.196420, test/accuracy=0.377500, test/loss=2.896560, test/num_examples=10000, total_duration=11208.200269, train/accuracy=0.515000, train/loss=2.116328, validation/accuracy=0.479580, validation/loss=2.289321, validation/num_examples=50000
I0201 16:08:37.521977 139774434195200 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.6268682479858398, loss=3.357672929763794
I0201 16:09:23.605552 139774417409792 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.320053219795227, loss=4.250637054443359
I0201 16:10:09.927709 139774434195200 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.5768195390701294, loss=3.0963733196258545
I0201 16:10:56.033486 139774417409792 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.1831752061843872, loss=5.164778709411621
I0201 16:11:42.522148 139774434195200 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.2841787338256836, loss=3.9499869346618652
I0201 16:12:28.709724 139774417409792 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.4018605947494507, loss=3.652617931365967
I0201 16:13:14.784679 139774434195200 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.740355372428894, loss=3.1590590476989746
I0201 16:14:00.855686 139774417409792 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.2919949293136597, loss=3.7279484272003174
I0201 16:14:47.027869 139774434195200 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.5550158023834229, loss=3.356062173843384
I0201 16:14:56.601149 139936116377408 spec.py:321] Evaluating on the training split.
I0201 16:15:09.464894 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:15:39.861874 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:15:41.463309 139936116377408 submission_runner.py:408] Time since start: 11673.32s, 	Step: 22922, 	{'train/accuracy': 0.521191418170929, 'train/loss': 2.095719814300537, 'validation/accuracy': 0.48517999053001404, 'validation/loss': 2.2728850841522217, 'validation/num_examples': 50000, 'test/accuracy': 0.37610000371932983, 'test/loss': 2.9281606674194336, 'test/num_examples': 10000, 'score': 10541.374686002731, 'total_duration': 11673.316876888275, 'accumulated_submission_time': 10541.374686002731, 'accumulated_eval_time': 1129.569720506668, 'accumulated_logging_time': 1.1803789138793945}
I0201 16:15:41.483258 139774417409792 logging_writer.py:48] [22922] accumulated_eval_time=1129.569721, accumulated_logging_time=1.180379, accumulated_submission_time=10541.374686, global_step=22922, preemption_count=0, score=10541.374686, test/accuracy=0.376100, test/loss=2.928161, test/num_examples=10000, total_duration=11673.316877, train/accuracy=0.521191, train/loss=2.095720, validation/accuracy=0.485180, validation/loss=2.272885, validation/num_examples=50000
I0201 16:16:13.442894 139774434195200 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.5790269374847412, loss=3.1463074684143066
I0201 16:16:59.356210 139774417409792 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.67478609085083, loss=3.0826940536499023
I0201 16:17:45.431868 139774434195200 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.6861807107925415, loss=3.0918936729431152
I0201 16:18:31.649341 139774417409792 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.1694389581680298, loss=4.318110466003418
I0201 16:19:17.896558 139774434195200 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.6681398153305054, loss=3.1399073600769043
I0201 16:20:04.286005 139774417409792 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.1809322834014893, loss=5.3909687995910645
I0201 16:20:50.299221 139774434195200 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.3584343194961548, loss=3.8157200813293457
I0201 16:21:36.489684 139774417409792 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.624712347984314, loss=3.0063562393188477
I0201 16:22:23.314264 139774434195200 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.6333980560302734, loss=3.4320363998413086
I0201 16:22:41.573193 139936116377408 spec.py:321] Evaluating on the training split.
I0201 16:22:53.495253 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:23:25.287525 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:23:26.896872 139936116377408 submission_runner.py:408] Time since start: 12138.75s, 	Step: 23841, 	{'train/accuracy': 0.5325781106948853, 'train/loss': 2.0169644355773926, 'validation/accuracy': 0.4902399778366089, 'validation/loss': 2.243086338043213, 'validation/num_examples': 50000, 'test/accuracy': 0.38450002670288086, 'test/loss': 2.868086576461792, 'test/num_examples': 10000, 'score': 10961.407228469849, 'total_duration': 12138.750408172607, 'accumulated_submission_time': 10961.407228469849, 'accumulated_eval_time': 1174.8933689594269, 'accumulated_logging_time': 1.210268497467041}
I0201 16:23:26.921501 139774417409792 logging_writer.py:48] [23841] accumulated_eval_time=1174.893369, accumulated_logging_time=1.210268, accumulated_submission_time=10961.407228, global_step=23841, preemption_count=0, score=10961.407228, test/accuracy=0.384500, test/loss=2.868087, test/num_examples=10000, total_duration=12138.750408, train/accuracy=0.532578, train/loss=2.016964, validation/accuracy=0.490240, validation/loss=2.243086, validation/num_examples=50000
I0201 16:23:50.745659 139774434195200 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.4822065830230713, loss=3.096980333328247
I0201 16:24:35.749817 139774417409792 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.3822126388549805, loss=4.370084285736084
I0201 16:25:21.987779 139774434195200 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.7226756811141968, loss=3.3802595138549805
I0201 16:26:08.419634 139774417409792 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.468972086906433, loss=4.055129051208496
I0201 16:26:54.415832 139774434195200 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.850222110748291, loss=3.0905468463897705
I0201 16:27:40.583739 139774417409792 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.428633689880371, loss=4.886262893676758
I0201 16:28:26.537513 139774434195200 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.4984620809555054, loss=3.1161792278289795
I0201 16:29:12.913109 139774417409792 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.6227542161941528, loss=3.1090314388275146
I0201 16:29:59.124523 139774434195200 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.421325445175171, loss=4.457708358764648
I0201 16:30:27.081251 139936116377408 spec.py:321] Evaluating on the training split.
I0201 16:30:38.914939 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:31:12.709532 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:31:14.320896 139936116377408 submission_runner.py:408] Time since start: 12606.17s, 	Step: 24762, 	{'train/accuracy': 0.5335351228713989, 'train/loss': 2.0236427783966064, 'validation/accuracy': 0.4941200017929077, 'validation/loss': 2.203388214111328, 'validation/num_examples': 50000, 'test/accuracy': 0.3880000114440918, 'test/loss': 2.834979295730591, 'test/num_examples': 10000, 'score': 11381.509302854538, 'total_duration': 12606.17445731163, 'accumulated_submission_time': 11381.509302854538, 'accumulated_eval_time': 1222.1330163478851, 'accumulated_logging_time': 1.244988203048706}
I0201 16:31:14.339297 139774417409792 logging_writer.py:48] [24762] accumulated_eval_time=1222.133016, accumulated_logging_time=1.244988, accumulated_submission_time=11381.509303, global_step=24762, preemption_count=0, score=11381.509303, test/accuracy=0.388000, test/loss=2.834979, test/num_examples=10000, total_duration=12606.174457, train/accuracy=0.533535, train/loss=2.023643, validation/accuracy=0.494120, validation/loss=2.203388, validation/num_examples=50000
I0201 16:31:29.819267 139774434195200 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.1553794145584106, loss=5.472201347351074
I0201 16:32:13.454792 139774417409792 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.2701388597488403, loss=5.204160213470459
I0201 16:32:59.444857 139774434195200 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.2761781215667725, loss=5.670541763305664
I0201 16:33:46.069570 139774417409792 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.4374712705612183, loss=3.6292285919189453
I0201 16:34:32.025497 139774434195200 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2214455604553223, loss=4.435802936553955
I0201 16:35:18.200716 139774417409792 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.146754264831543, loss=5.3037519454956055
I0201 16:36:04.233795 139774434195200 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.5337374210357666, loss=3.0083484649658203
I0201 16:36:50.105125 139774417409792 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.5134762525558472, loss=3.043940305709839
I0201 16:37:35.949966 139774434195200 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0878033638000488, loss=4.610767364501953
I0201 16:38:14.426273 139936116377408 spec.py:321] Evaluating on the training split.
I0201 16:38:26.495746 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:38:55.242413 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:38:56.836826 139936116377408 submission_runner.py:408] Time since start: 13068.69s, 	Step: 25685, 	{'train/accuracy': 0.5403710603713989, 'train/loss': 2.0134925842285156, 'validation/accuracy': 0.5018999576568604, 'validation/loss': 2.1954407691955566, 'validation/num_examples': 50000, 'test/accuracy': 0.3906000256538391, 'test/loss': 2.8306357860565186, 'test/num_examples': 10000, 'score': 11801.537393569946, 'total_duration': 13068.69039440155, 'accumulated_submission_time': 11801.537393569946, 'accumulated_eval_time': 1264.5435791015625, 'accumulated_logging_time': 1.27274751663208}
I0201 16:38:56.855797 139774417409792 logging_writer.py:48] [25685] accumulated_eval_time=1264.543579, accumulated_logging_time=1.272748, accumulated_submission_time=11801.537394, global_step=25685, preemption_count=0, score=11801.537394, test/accuracy=0.390600, test/loss=2.830636, test/num_examples=10000, total_duration=13068.690394, train/accuracy=0.540371, train/loss=2.013493, validation/accuracy=0.501900, validation/loss=2.195441, validation/num_examples=50000
I0201 16:39:03.206421 139774434195200 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.3929100036621094, loss=3.29671049118042
I0201 16:39:45.301325 139774417409792 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.472322940826416, loss=3.047311544418335
I0201 16:40:31.493025 139774434195200 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.5851303339004517, loss=3.3038644790649414
I0201 16:41:17.837118 139774417409792 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.5728676319122314, loss=2.895052909851074
I0201 16:42:04.130836 139774434195200 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.2938132286071777, loss=5.558987617492676
I0201 16:42:50.217345 139774417409792 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.497719168663025, loss=3.3884928226470947
I0201 16:43:36.679566 139774434195200 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.7052429914474487, loss=2.938300132751465
I0201 16:44:23.330439 139774417409792 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.473467230796814, loss=3.937577247619629
I0201 16:45:10.104810 139774434195200 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.6459943056106567, loss=2.9127566814422607
I0201 16:45:56.201772 139774417409792 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6673450469970703, loss=2.9956235885620117
I0201 16:45:57.258831 139936116377408 spec.py:321] Evaluating on the training split.
I0201 16:46:10.108297 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:46:40.816328 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:46:42.423203 139936116377408 submission_runner.py:408] Time since start: 13534.28s, 	Step: 26604, 	{'train/accuracy': 0.5479491949081421, 'train/loss': 1.9419466257095337, 'validation/accuracy': 0.5062199831008911, 'validation/loss': 2.1494569778442383, 'validation/num_examples': 50000, 'test/accuracy': 0.3939000070095062, 'test/loss': 2.7954912185668945, 'test/num_examples': 10000, 'score': 12221.882104635239, 'total_duration': 13534.276743412018, 'accumulated_submission_time': 12221.882104635239, 'accumulated_eval_time': 1309.7079060077667, 'accumulated_logging_time': 1.3029565811157227}
I0201 16:46:42.442291 139774434195200 logging_writer.py:48] [26604] accumulated_eval_time=1309.707906, accumulated_logging_time=1.302957, accumulated_submission_time=12221.882105, global_step=26604, preemption_count=0, score=12221.882105, test/accuracy=0.393900, test/loss=2.795491, test/num_examples=10000, total_duration=13534.276743, train/accuracy=0.547949, train/loss=1.941947, validation/accuracy=0.506220, validation/loss=2.149457, validation/num_examples=50000
I0201 16:47:22.423968 139774417409792 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.5918519496917725, loss=3.1107141971588135
I0201 16:48:08.427805 139774434195200 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.0904484987258911, loss=5.232888221740723
I0201 16:48:54.434977 139774417409792 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1993821859359741, loss=4.533041477203369
I0201 16:49:40.623554 139774434195200 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.584924340248108, loss=2.8367276191711426
I0201 16:50:26.623055 139774417409792 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.4968798160552979, loss=3.3122429847717285
I0201 16:51:12.811267 139774434195200 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.750046730041504, loss=3.1360623836517334
I0201 16:51:58.967609 139774417409792 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6726441383361816, loss=2.8095927238464355
I0201 16:52:44.907365 139774434195200 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.202307105064392, loss=4.622897148132324
I0201 16:53:31.200862 139774417409792 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.4046534299850464, loss=3.1860103607177734
I0201 16:53:42.830917 139936116377408 spec.py:321] Evaluating on the training split.
I0201 16:53:54.853055 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 16:54:21.390094 139936116377408 spec.py:349] Evaluating on the test split.
I0201 16:54:22.996961 139936116377408 submission_runner.py:408] Time since start: 13994.85s, 	Step: 27527, 	{'train/accuracy': 0.5578905940055847, 'train/loss': 1.9007580280303955, 'validation/accuracy': 0.51419997215271, 'validation/loss': 2.116635322570801, 'validation/num_examples': 50000, 'test/accuracy': 0.40630000829696655, 'test/loss': 2.749185562133789, 'test/num_examples': 10000, 'score': 12642.213397979736, 'total_duration': 13994.85052037239, 'accumulated_submission_time': 12642.213397979736, 'accumulated_eval_time': 1349.8739371299744, 'accumulated_logging_time': 1.3320088386535645}
I0201 16:54:23.018823 139774434195200 logging_writer.py:48] [27527] accumulated_eval_time=1349.873937, accumulated_logging_time=1.332009, accumulated_submission_time=12642.213398, global_step=27527, preemption_count=0, score=12642.213398, test/accuracy=0.406300, test/loss=2.749186, test/num_examples=10000, total_duration=13994.850520, train/accuracy=0.557891, train/loss=1.900758, validation/accuracy=0.514200, validation/loss=2.116635, validation/num_examples=50000
I0201 16:54:52.416713 139774417409792 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.2997313737869263, loss=5.452591419219971
I0201 16:55:38.287114 139774434195200 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.070949912071228, loss=4.518220901489258
I0201 16:56:24.760027 139774417409792 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.1048810482025146, loss=5.529444217681885
I0201 16:57:10.875510 139774434195200 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.4187792539596558, loss=3.072809934616089
I0201 16:57:56.731365 139774417409792 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.6190587282180786, loss=2.895728588104248
I0201 16:58:42.948369 139774434195200 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.61073899269104, loss=3.0778186321258545
I0201 16:59:29.215828 139774417409792 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.4800899028778076, loss=3.9392693042755127
I0201 17:00:15.354490 139774434195200 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.6051934957504272, loss=3.0212743282318115
I0201 17:01:01.700627 139774417409792 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.5160599946975708, loss=2.9050629138946533
I0201 17:01:23.120975 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:01:35.229956 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:02:07.065052 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:02:08.661397 139936116377408 submission_runner.py:408] Time since start: 14460.51s, 	Step: 28448, 	{'train/accuracy': 0.5541796684265137, 'train/loss': 1.9155114889144897, 'validation/accuracy': 0.5175999999046326, 'validation/loss': 2.0890777111053467, 'validation/num_examples': 50000, 'test/accuracy': 0.4115000069141388, 'test/loss': 2.720576763153076, 'test/num_examples': 10000, 'score': 13062.25780081749, 'total_duration': 14460.51496386528, 'accumulated_submission_time': 13062.25780081749, 'accumulated_eval_time': 1395.4143552780151, 'accumulated_logging_time': 1.3635451793670654}
I0201 17:02:08.679919 139774434195200 logging_writer.py:48] [28448] accumulated_eval_time=1395.414355, accumulated_logging_time=1.363545, accumulated_submission_time=13062.257801, global_step=28448, preemption_count=0, score=13062.257801, test/accuracy=0.411500, test/loss=2.720577, test/num_examples=10000, total_duration=14460.514964, train/accuracy=0.554180, train/loss=1.915511, validation/accuracy=0.517600, validation/loss=2.089078, validation/num_examples=50000
I0201 17:02:29.724138 139774417409792 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.4889953136444092, loss=3.4752273559570312
I0201 17:03:14.461698 139774434195200 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.304276466369629, loss=3.1596288681030273
I0201 17:04:00.794641 139774417409792 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.3854011297225952, loss=4.436727523803711
I0201 17:04:47.352957 139774434195200 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.261364459991455, loss=4.721925735473633
I0201 17:05:33.416944 139774417409792 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.4402869939804077, loss=3.5242953300476074
I0201 17:06:19.461661 139774434195200 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6386054754257202, loss=2.9986493587493896
I0201 17:07:05.738450 139774417409792 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.3205680847167969, loss=4.459418296813965
I0201 17:07:51.904257 139774434195200 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.4099879264831543, loss=3.4783055782318115
I0201 17:08:37.847086 139774417409792 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.3585911989212036, loss=3.2691943645477295
I0201 17:09:08.909498 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:09:20.864013 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:09:52.343569 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:09:53.953620 139936116377408 submission_runner.py:408] Time since start: 14925.81s, 	Step: 29369, 	{'train/accuracy': 0.5678515434265137, 'train/loss': 1.865001916885376, 'validation/accuracy': 0.5196200013160706, 'validation/loss': 2.0869064331054688, 'validation/num_examples': 50000, 'test/accuracy': 0.4068000316619873, 'test/loss': 2.7158117294311523, 'test/num_examples': 10000, 'score': 13482.429998636246, 'total_duration': 14925.807185173035, 'accumulated_submission_time': 13482.429998636246, 'accumulated_eval_time': 1440.4584770202637, 'accumulated_logging_time': 1.391657829284668}
I0201 17:09:53.976395 139774434195200 logging_writer.py:48] [29369] accumulated_eval_time=1440.458477, accumulated_logging_time=1.391658, accumulated_submission_time=13482.429999, global_step=29369, preemption_count=0, score=13482.429999, test/accuracy=0.406800, test/loss=2.715812, test/num_examples=10000, total_duration=14925.807185, train/accuracy=0.567852, train/loss=1.865002, validation/accuracy=0.519620, validation/loss=2.086906, validation/num_examples=50000
I0201 17:10:06.674199 139774417409792 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.6083077192306519, loss=3.09338641166687
I0201 17:10:49.759989 139774434195200 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.5520046949386597, loss=3.099641799926758
I0201 17:11:35.823358 139774417409792 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.5873905420303345, loss=2.888986110687256
I0201 17:12:22.220008 139774434195200 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.6678886413574219, loss=2.7858457565307617
I0201 17:13:08.419702 139774417409792 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.520599126815796, loss=2.9228062629699707
I0201 17:13:54.523327 139774434195200 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.7119455337524414, loss=2.9510912895202637
I0201 17:14:40.874732 139774417409792 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.5000617504119873, loss=2.8687100410461426
I0201 17:15:27.257994 139774434195200 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.3760437965393066, loss=3.467982053756714
I0201 17:16:13.237778 139774417409792 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.3382630348205566, loss=3.9678454399108887
I0201 17:16:53.978449 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:17:06.320080 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:17:38.890646 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:17:40.496029 139936116377408 submission_runner.py:408] Time since start: 15392.35s, 	Step: 30290, 	{'train/accuracy': 0.5793359279632568, 'train/loss': 1.8263520002365112, 'validation/accuracy': 0.5206999778747559, 'validation/loss': 2.103342294692993, 'validation/num_examples': 50000, 'test/accuracy': 0.41290003061294556, 'test/loss': 2.727597951889038, 'test/num_examples': 10000, 'score': 13902.37486410141, 'total_duration': 15392.34959602356, 'accumulated_submission_time': 13902.37486410141, 'accumulated_eval_time': 1486.9760718345642, 'accumulated_logging_time': 1.4241070747375488}
I0201 17:17:40.518311 139774434195200 logging_writer.py:48] [30290] accumulated_eval_time=1486.976072, accumulated_logging_time=1.424107, accumulated_submission_time=13902.374864, global_step=30290, preemption_count=0, score=13902.374864, test/accuracy=0.412900, test/loss=2.727598, test/num_examples=10000, total_duration=15392.349596, train/accuracy=0.579336, train/loss=1.826352, validation/accuracy=0.520700, validation/loss=2.103342, validation/num_examples=50000
I0201 17:17:44.886180 139774417409792 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.6556891202926636, loss=2.9448776245117188
I0201 17:18:27.048098 139774434195200 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.2627012729644775, loss=5.15561056137085
I0201 17:19:13.089123 139774417409792 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.5366811752319336, loss=3.3681137561798096
I0201 17:19:59.041779 139774434195200 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.4719550609588623, loss=2.9339585304260254
I0201 17:20:45.330684 139774417409792 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.4315612316131592, loss=3.654613494873047
I0201 17:21:31.337020 139774434195200 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.237101674079895, loss=4.872555255889893
I0201 17:22:17.458816 139774417409792 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.5220891237258911, loss=3.890378952026367
I0201 17:23:03.556134 139774434195200 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.6881943941116333, loss=2.7721877098083496
I0201 17:23:49.540272 139774417409792 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.6661409139633179, loss=2.9076719284057617
I0201 17:24:35.665096 139774434195200 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.3059923648834229, loss=4.5671892166137695
I0201 17:24:40.554970 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:24:52.459344 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:25:22.252198 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:25:23.857896 139936116377408 submission_runner.py:408] Time since start: 15855.71s, 	Step: 31212, 	{'train/accuracy': 0.564160168170929, 'train/loss': 1.8538535833358765, 'validation/accuracy': 0.527899980545044, 'validation/loss': 2.041722297668457, 'validation/num_examples': 50000, 'test/accuracy': 0.4142000079154968, 'test/loss': 2.687079906463623, 'test/num_examples': 10000, 'score': 14322.354754447937, 'total_duration': 15855.711465358734, 'accumulated_submission_time': 14322.354754447937, 'accumulated_eval_time': 1530.2789916992188, 'accumulated_logging_time': 1.4560437202453613}
I0201 17:25:23.876991 139774417409792 logging_writer.py:48] [31212] accumulated_eval_time=1530.278992, accumulated_logging_time=1.456044, accumulated_submission_time=14322.354754, global_step=31212, preemption_count=0, score=14322.354754, test/accuracy=0.414200, test/loss=2.687080, test/num_examples=10000, total_duration=15855.711465, train/accuracy=0.564160, train/loss=1.853854, validation/accuracy=0.527900, validation/loss=2.041722, validation/num_examples=50000
I0201 17:26:00.566844 139774434195200 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.3749157190322876, loss=4.347099781036377
I0201 17:26:46.445291 139774417409792 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.6980655193328857, loss=2.874148368835449
I0201 17:27:32.310358 139774434195200 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.382138967514038, loss=3.5400092601776123
I0201 17:28:18.367690 139774417409792 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.631094217300415, loss=2.806011199951172
I0201 17:29:04.381901 139774434195200 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.6473790407180786, loss=3.0229196548461914
I0201 17:29:50.405757 139774417409792 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.573435664176941, loss=3.0413811206817627
I0201 17:30:36.419986 139774434195200 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.6066957712173462, loss=2.881185531616211
I0201 17:31:22.656332 139774417409792 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.5858789682388306, loss=2.9451591968536377
I0201 17:32:09.095304 139774434195200 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.3721108436584473, loss=5.480081081390381
I0201 17:32:24.062739 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:32:36.092597 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:33:08.101519 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:33:09.710425 139936116377408 submission_runner.py:408] Time since start: 16321.56s, 	Step: 32134, 	{'train/accuracy': 0.5700390338897705, 'train/loss': 1.8478858470916748, 'validation/accuracy': 0.531279981136322, 'validation/loss': 2.034501314163208, 'validation/num_examples': 50000, 'test/accuracy': 0.41930001974105835, 'test/loss': 2.6696832180023193, 'test/num_examples': 10000, 'score': 14742.48341703415, 'total_duration': 16321.563993692398, 'accumulated_submission_time': 14742.48341703415, 'accumulated_eval_time': 1575.9266781806946, 'accumulated_logging_time': 1.4843404293060303}
I0201 17:33:09.730386 139774417409792 logging_writer.py:48] [32134] accumulated_eval_time=1575.926678, accumulated_logging_time=1.484340, accumulated_submission_time=14742.483417, global_step=32134, preemption_count=0, score=14742.483417, test/accuracy=0.419300, test/loss=2.669683, test/num_examples=10000, total_duration=16321.563994, train/accuracy=0.570039, train/loss=1.847886, validation/accuracy=0.531280, validation/loss=2.034501, validation/num_examples=50000
I0201 17:33:36.322956 139774434195200 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.281019687652588, loss=5.5040059089660645
I0201 17:34:22.265053 139774417409792 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.3350739479064941, loss=3.1323494911193848
I0201 17:35:08.447386 139774434195200 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.4624264240264893, loss=2.9605088233947754
I0201 17:35:54.945442 139774417409792 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1574110984802246, loss=5.373452663421631
I0201 17:36:41.442476 139774434195200 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.444502353668213, loss=2.842721700668335
I0201 17:37:27.590416 139774417409792 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.8505973815917969, loss=2.8249671459198
I0201 17:38:13.965564 139774434195200 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.6555134057998657, loss=2.7420144081115723
I0201 17:39:00.219465 139774417409792 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.727562665939331, loss=2.877788543701172
I0201 17:39:46.624810 139774434195200 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.3050057888031006, loss=4.304014682769775
I0201 17:40:10.081186 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:40:21.942195 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:40:55.118188 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:40:56.722679 139936116377408 submission_runner.py:408] Time since start: 16788.58s, 	Step: 33052, 	{'train/accuracy': 0.5787890553474426, 'train/loss': 1.8214852809906006, 'validation/accuracy': 0.5297799706459045, 'validation/loss': 2.06502366065979, 'validation/num_examples': 50000, 'test/accuracy': 0.4118000268936157, 'test/loss': 2.7029848098754883, 'test/num_examples': 10000, 'score': 15162.777776002884, 'total_duration': 16788.576245307922, 'accumulated_submission_time': 15162.777776002884, 'accumulated_eval_time': 1622.5681648254395, 'accumulated_logging_time': 1.513521432876587}
I0201 17:40:56.745289 139774417409792 logging_writer.py:48] [33052] accumulated_eval_time=1622.568165, accumulated_logging_time=1.513521, accumulated_submission_time=15162.777776, global_step=33052, preemption_count=0, score=15162.777776, test/accuracy=0.411800, test/loss=2.702985, test/num_examples=10000, total_duration=16788.576245, train/accuracy=0.578789, train/loss=1.821485, validation/accuracy=0.529780, validation/loss=2.065024, validation/num_examples=50000
I0201 17:41:16.209842 139774434195200 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.666988730430603, loss=2.7959299087524414
I0201 17:42:00.571784 139774417409792 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.5760657787322998, loss=2.9686944484710693
I0201 17:42:46.841676 139774434195200 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8343747854232788, loss=2.8661866188049316
I0201 17:43:33.090044 139774417409792 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.2258774042129517, loss=4.443403244018555
I0201 17:44:19.206721 139774434195200 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.2218430042266846, loss=4.69025182723999
I0201 17:45:05.382480 139774417409792 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.7087793350219727, loss=2.880948305130005
I0201 17:45:51.336764 139774434195200 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.5884894132614136, loss=2.969074249267578
I0201 17:46:38.301134 139774417409792 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.5154842138290405, loss=2.9422309398651123
I0201 17:47:24.112527 139774434195200 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.6874724626541138, loss=2.845527172088623
I0201 17:47:56.735999 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:48:08.845205 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:48:40.930906 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:48:42.550304 139936116377408 submission_runner.py:408] Time since start: 17254.40s, 	Step: 33973, 	{'train/accuracy': 0.5707616806030273, 'train/loss': 1.8383631706237793, 'validation/accuracy': 0.5380600094795227, 'validation/loss': 2.0063157081604004, 'validation/num_examples': 50000, 'test/accuracy': 0.4244000315666199, 'test/loss': 2.666775941848755, 'test/num_examples': 10000, 'score': 15582.711037397385, 'total_duration': 17254.40386199951, 'accumulated_submission_time': 15582.711037397385, 'accumulated_eval_time': 1668.3824818134308, 'accumulated_logging_time': 1.5456140041351318}
I0201 17:48:42.570201 139774417409792 logging_writer.py:48] [33973] accumulated_eval_time=1668.382482, accumulated_logging_time=1.545614, accumulated_submission_time=15582.711037, global_step=33973, preemption_count=0, score=15582.711037, test/accuracy=0.424400, test/loss=2.666776, test/num_examples=10000, total_duration=17254.403862, train/accuracy=0.570762, train/loss=1.838363, validation/accuracy=0.538060, validation/loss=2.006316, validation/num_examples=50000
I0201 17:48:53.691654 139774434195200 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7024132013320923, loss=2.9217262268066406
I0201 17:49:36.720509 139774417409792 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.6708570718765259, loss=2.8722591400146484
I0201 17:50:22.884598 139774434195200 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.5609973669052124, loss=3.0654029846191406
I0201 17:51:09.322056 139774417409792 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.3486888408660889, loss=5.311223983764648
I0201 17:51:55.625419 139774434195200 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.5661044120788574, loss=2.9238059520721436
I0201 17:52:41.879467 139774417409792 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.662723183631897, loss=2.9270803928375244
I0201 17:53:28.103956 139774434195200 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.5674567222595215, loss=2.6689867973327637
I0201 17:54:14.264480 139774417409792 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.6704151630401611, loss=2.72935152053833
I0201 17:55:00.381244 139774434195200 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.1978195905685425, loss=4.209547519683838
I0201 17:55:42.984891 139936116377408 spec.py:321] Evaluating on the training split.
I0201 17:55:55.055992 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 17:56:27.502825 139936116377408 spec.py:349] Evaluating on the test split.
I0201 17:56:29.112605 139936116377408 submission_runner.py:408] Time since start: 17720.97s, 	Step: 34894, 	{'train/accuracy': 0.5763476490974426, 'train/loss': 1.8116711378097534, 'validation/accuracy': 0.5345199704170227, 'validation/loss': 2.015291452407837, 'validation/num_examples': 50000, 'test/accuracy': 0.4220000207424164, 'test/loss': 2.628847360610962, 'test/num_examples': 10000, 'score': 16003.069641828537, 'total_duration': 17720.966168165207, 'accumulated_submission_time': 16003.069641828537, 'accumulated_eval_time': 1714.5101835727692, 'accumulated_logging_time': 1.5746049880981445}
I0201 17:56:29.136343 139774417409792 logging_writer.py:48] [34894] accumulated_eval_time=1714.510184, accumulated_logging_time=1.574605, accumulated_submission_time=16003.069642, global_step=34894, preemption_count=0, score=16003.069642, test/accuracy=0.422000, test/loss=2.628847, test/num_examples=10000, total_duration=17720.966168, train/accuracy=0.576348, train/loss=1.811671, validation/accuracy=0.534520, validation/loss=2.015291, validation/num_examples=50000
I0201 17:56:31.913631 139774434195200 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.4593663215637207, loss=4.239516258239746
I0201 17:57:13.925337 139774417409792 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.1376575231552124, loss=5.307648181915283
I0201 17:58:00.234697 139774434195200 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.2633755207061768, loss=4.954354763031006
I0201 17:58:46.357342 139774417409792 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.5203206539154053, loss=2.8468735218048096
I0201 17:59:32.641249 139774434195200 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.5895588397979736, loss=3.3452560901641846
I0201 18:00:18.781383 139774417409792 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.6141021251678467, loss=2.712332248687744
I0201 18:01:05.091221 139774434195200 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.7183640003204346, loss=2.8431692123413086
I0201 18:01:51.354874 139774417409792 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.2038346529006958, loss=4.847416877746582
I0201 18:02:37.631511 139774434195200 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.7588410377502441, loss=2.9097249507904053
I0201 18:03:23.685657 139774417409792 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.3653244972229004, loss=5.41001033782959
I0201 18:03:29.294360 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:03:41.199835 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:04:15.688709 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:04:17.294122 139936116377408 submission_runner.py:408] Time since start: 18189.15s, 	Step: 35814, 	{'train/accuracy': 0.581250011920929, 'train/loss': 1.7759336233139038, 'validation/accuracy': 0.5402399897575378, 'validation/loss': 1.991693377494812, 'validation/num_examples': 50000, 'test/accuracy': 0.42170003056526184, 'test/loss': 2.6568825244903564, 'test/num_examples': 10000, 'score': 16423.170471429825, 'total_duration': 18189.147683382034, 'accumulated_submission_time': 16423.170471429825, 'accumulated_eval_time': 1762.5099685192108, 'accumulated_logging_time': 1.608259916305542}
I0201 18:04:17.314203 139774434195200 logging_writer.py:48] [35814] accumulated_eval_time=1762.509969, accumulated_logging_time=1.608260, accumulated_submission_time=16423.170471, global_step=35814, preemption_count=0, score=16423.170471, test/accuracy=0.421700, test/loss=2.656883, test/num_examples=10000, total_duration=18189.147683, train/accuracy=0.581250, train/loss=1.775934, validation/accuracy=0.540240, validation/loss=1.991693, validation/num_examples=50000
I0201 18:04:52.557563 139774417409792 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.6633797883987427, loss=2.90539288520813
I0201 18:05:38.245831 139774434195200 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6675426959991455, loss=2.740065336227417
I0201 18:06:24.828448 139774417409792 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7672094106674194, loss=2.744772434234619
I0201 18:07:10.837283 139774434195200 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.5520179271697998, loss=2.7793140411376953
I0201 18:07:57.038990 139774417409792 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.5892695188522339, loss=2.850188970565796
I0201 18:08:43.476633 139774434195200 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.6531702280044556, loss=3.18644642829895
I0201 18:09:29.475658 139774417409792 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.440704345703125, loss=3.98512601852417
I0201 18:10:15.426815 139774434195200 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.7515363693237305, loss=2.764594316482544
I0201 18:11:01.452093 139774417409792 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.520405888557434, loss=2.772160530090332
I0201 18:11:17.723848 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:11:29.774412 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:12:00.375605 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:12:01.986269 139936116377408 submission_runner.py:408] Time since start: 18653.84s, 	Step: 36737, 	{'train/accuracy': 0.5838086009025574, 'train/loss': 1.7920989990234375, 'validation/accuracy': 0.5418800115585327, 'validation/loss': 1.9881590604782104, 'validation/num_examples': 50000, 'test/accuracy': 0.4239000082015991, 'test/loss': 2.632972478866577, 'test/num_examples': 10000, 'score': 16843.52310347557, 'total_duration': 18653.83980345726, 'accumulated_submission_time': 16843.52310347557, 'accumulated_eval_time': 1806.7723808288574, 'accumulated_logging_time': 1.6376621723175049}
I0201 18:12:02.017952 139774434195200 logging_writer.py:48] [36737] accumulated_eval_time=1806.772381, accumulated_logging_time=1.637662, accumulated_submission_time=16843.523103, global_step=36737, preemption_count=0, score=16843.523103, test/accuracy=0.423900, test/loss=2.632972, test/num_examples=10000, total_duration=18653.839803, train/accuracy=0.583809, train/loss=1.792099, validation/accuracy=0.541880, validation/loss=1.988159, validation/num_examples=50000
I0201 18:12:27.439980 139774417409792 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.529177188873291, loss=2.8037095069885254
I0201 18:13:12.725676 139774434195200 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.2391749620437622, loss=4.780972003936768
I0201 18:13:59.026112 139774417409792 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.6568927764892578, loss=2.893630027770996
I0201 18:14:45.303716 139774434195200 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.6711182594299316, loss=2.839265823364258
I0201 18:15:31.405500 139774417409792 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.6694977283477783, loss=3.2689642906188965
I0201 18:16:17.778164 139774434195200 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.6179752349853516, loss=2.6158347129821777
I0201 18:17:04.059608 139774417409792 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.5250096321105957, loss=2.635838508605957
I0201 18:17:50.089819 139774434195200 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.6404320001602173, loss=2.929849863052368
I0201 18:18:36.714459 139774417409792 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.8178316354751587, loss=2.713277578353882
I0201 18:19:02.345663 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:19:14.413601 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:19:47.350631 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:19:48.949192 139936116377408 submission_runner.py:408] Time since start: 19120.80s, 	Step: 37657, 	{'train/accuracy': 0.5879687070846558, 'train/loss': 1.7495322227478027, 'validation/accuracy': 0.5480200052261353, 'validation/loss': 1.946392297744751, 'validation/num_examples': 50000, 'test/accuracy': 0.4320000112056732, 'test/loss': 2.6023566722869873, 'test/num_examples': 10000, 'score': 17263.79153227806, 'total_duration': 19120.8027510643, 'accumulated_submission_time': 17263.79153227806, 'accumulated_eval_time': 1853.3759117126465, 'accumulated_logging_time': 1.6815898418426514}
I0201 18:19:48.973104 139774434195200 logging_writer.py:48] [37657] accumulated_eval_time=1853.375912, accumulated_logging_time=1.681590, accumulated_submission_time=17263.791532, global_step=37657, preemption_count=0, score=17263.791532, test/accuracy=0.432000, test/loss=2.602357, test/num_examples=10000, total_duration=19120.802751, train/accuracy=0.587969, train/loss=1.749532, validation/accuracy=0.548020, validation/loss=1.946392, validation/num_examples=50000
I0201 18:20:06.452433 139774417409792 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.557927131652832, loss=3.126336097717285
I0201 18:20:50.429391 139774434195200 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.318331241607666, loss=4.207860469818115
I0201 18:21:36.819453 139774417409792 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.2723485231399536, loss=4.352370262145996
I0201 18:22:23.082161 139774434195200 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.592633605003357, loss=2.6997759342193604
I0201 18:23:09.257625 139774417409792 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1905986070632935, loss=5.396302223205566
I0201 18:23:54.965183 139774434195200 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.4765113592147827, loss=3.5182881355285645
I0201 18:24:41.007565 139774417409792 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.8381456136703491, loss=2.8584396839141846
I0201 18:25:27.129901 139774434195200 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.4066617488861084, loss=3.9041404724121094
I0201 18:26:13.286598 139774417409792 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.6513069868087769, loss=2.7168450355529785
I0201 18:26:49.365875 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:27:01.214243 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:27:32.293477 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:27:33.900797 139936116377408 submission_runner.py:408] Time since start: 19585.75s, 	Step: 38580, 	{'train/accuracy': 0.5942773222923279, 'train/loss': 1.714832067489624, 'validation/accuracy': 0.5480599999427795, 'validation/loss': 1.926216721534729, 'validation/num_examples': 50000, 'test/accuracy': 0.4335000216960907, 'test/loss': 2.5650787353515625, 'test/num_examples': 10000, 'score': 17684.12568449974, 'total_duration': 19585.754362106323, 'accumulated_submission_time': 17684.12568449974, 'accumulated_eval_time': 1897.9108562469482, 'accumulated_logging_time': 1.71612548828125}
I0201 18:27:33.929971 139774434195200 logging_writer.py:48] [38580] accumulated_eval_time=1897.910856, accumulated_logging_time=1.716125, accumulated_submission_time=17684.125684, global_step=38580, preemption_count=0, score=17684.125684, test/accuracy=0.433500, test/loss=2.565079, test/num_examples=10000, total_duration=19585.754362, train/accuracy=0.594277, train/loss=1.714832, validation/accuracy=0.548060, validation/loss=1.926217, validation/num_examples=50000
I0201 18:27:42.275007 139774417409792 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.8298741579055786, loss=2.8783528804779053
I0201 18:28:24.961617 139774434195200 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.2162779569625854, loss=4.081985950469971
I0201 18:29:11.390093 139774417409792 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.66987943649292, loss=2.7722811698913574
I0201 18:29:57.433769 139774434195200 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.6877927780151367, loss=2.737999200820923
I0201 18:30:43.356464 139774417409792 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.464991807937622, loss=3.289513111114502
I0201 18:31:29.479715 139774434195200 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.7149319648742676, loss=2.9347023963928223
I0201 18:32:15.802821 139774417409792 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.5980093479156494, loss=3.2566001415252686
I0201 18:33:01.904520 139774434195200 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.5788447856903076, loss=3.9140784740448
I0201 18:33:47.980289 139774417409792 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.4565528631210327, loss=3.8287060260772705
I0201 18:34:34.193894 139774434195200 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.9396158456802368, loss=2.8472516536712646
I0201 18:34:34.207517 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:34:46.158765 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:35:18.955019 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:35:20.560049 139936116377408 submission_runner.py:408] Time since start: 20052.41s, 	Step: 39501, 	{'train/accuracy': 0.6201366782188416, 'train/loss': 1.614801287651062, 'validation/accuracy': 0.5507599711418152, 'validation/loss': 1.94295072555542, 'validation/num_examples': 50000, 'test/accuracy': 0.43650001287460327, 'test/loss': 2.5802359580993652, 'test/num_examples': 10000, 'score': 18104.34602546692, 'total_duration': 20052.413615226746, 'accumulated_submission_time': 18104.34602546692, 'accumulated_eval_time': 1944.2633888721466, 'accumulated_logging_time': 1.7545132637023926}
I0201 18:35:20.586178 139774417409792 logging_writer.py:48] [39501] accumulated_eval_time=1944.263389, accumulated_logging_time=1.754513, accumulated_submission_time=18104.346025, global_step=39501, preemption_count=0, score=18104.346025, test/accuracy=0.436500, test/loss=2.580236, test/num_examples=10000, total_duration=20052.413615, train/accuracy=0.620137, train/loss=1.614801, validation/accuracy=0.550760, validation/loss=1.942951, validation/num_examples=50000
I0201 18:36:01.850675 139774434195200 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7647103071212769, loss=2.8671343326568604
I0201 18:36:47.988677 139774417409792 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.6397852897644043, loss=2.898965358734131
I0201 18:37:34.287280 139774434195200 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.6767244338989258, loss=2.678269624710083
I0201 18:38:20.462167 139774417409792 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.4493268728256226, loss=2.738863706588745
I0201 18:39:06.500175 139774434195200 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.219314694404602, loss=4.815079689025879
I0201 18:39:52.945582 139774417409792 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.7123490571975708, loss=2.7996902465820312
I0201 18:40:39.050584 139774434195200 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.6846709251403809, loss=2.633288860321045
I0201 18:41:25.354148 139774417409792 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.7455902099609375, loss=2.703641176223755
I0201 18:42:11.642154 139774434195200 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.8244452476501465, loss=2.7392642498016357
I0201 18:42:21.024069 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:42:33.037378 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:43:08.012749 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:43:09.619401 139936116377408 submission_runner.py:408] Time since start: 20521.47s, 	Step: 40422, 	{'train/accuracy': 0.5883983969688416, 'train/loss': 1.758009433746338, 'validation/accuracy': 0.5488199591636658, 'validation/loss': 1.9514132738113403, 'validation/num_examples': 50000, 'test/accuracy': 0.430400013923645, 'test/loss': 2.604454517364502, 'test/num_examples': 10000, 'score': 18524.727516174316, 'total_duration': 20521.472969293594, 'accumulated_submission_time': 18524.727516174316, 'accumulated_eval_time': 1992.8587412834167, 'accumulated_logging_time': 1.7895410060882568}
I0201 18:43:09.644285 139774417409792 logging_writer.py:48] [40422] accumulated_eval_time=1992.858741, accumulated_logging_time=1.789541, accumulated_submission_time=18524.727516, global_step=40422, preemption_count=0, score=18524.727516, test/accuracy=0.430400, test/loss=2.604455, test/num_examples=10000, total_duration=20521.472969, train/accuracy=0.588398, train/loss=1.758009, validation/accuracy=0.548820, validation/loss=1.951413, validation/num_examples=50000
I0201 18:43:41.350527 139774434195200 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.6359570026397705, loss=2.750290870666504
I0201 18:44:27.132353 139774417409792 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.3940613269805908, loss=3.671292781829834
I0201 18:45:13.389748 139774434195200 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.5453369617462158, loss=2.81740140914917
I0201 18:45:59.314477 139774417409792 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6046751737594604, loss=3.004351854324341
I0201 18:46:45.284121 139774434195200 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.2157429456710815, loss=5.2527265548706055
I0201 18:47:31.269162 139774417409792 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.7600212097167969, loss=3.3336915969848633
I0201 18:48:17.333624 139774434195200 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7381364107131958, loss=2.84244704246521
I0201 18:49:03.523203 139774417409792 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.5372626781463623, loss=2.736684560775757
I0201 18:49:49.792229 139774434195200 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.6508457660675049, loss=3.171809434890747
I0201 18:50:09.682431 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:50:21.588603 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:50:52.130484 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:50:53.751588 139936116377408 submission_runner.py:408] Time since start: 20985.61s, 	Step: 41344, 	{'train/accuracy': 0.6026757955551147, 'train/loss': 1.6814193725585938, 'validation/accuracy': 0.5562199950218201, 'validation/loss': 1.8999054431915283, 'validation/num_examples': 50000, 'test/accuracy': 0.4415000081062317, 'test/loss': 2.5421695709228516, 'test/num_examples': 10000, 'score': 18944.705970048904, 'total_duration': 20985.605131864548, 'accumulated_submission_time': 18944.705970048904, 'accumulated_eval_time': 2036.9278779029846, 'accumulated_logging_time': 1.8252899646759033}
I0201 18:50:53.779090 139774417409792 logging_writer.py:48] [41344] accumulated_eval_time=2036.927878, accumulated_logging_time=1.825290, accumulated_submission_time=18944.705970, global_step=41344, preemption_count=0, score=18944.705970, test/accuracy=0.441500, test/loss=2.542170, test/num_examples=10000, total_duration=20985.605132, train/accuracy=0.602676, train/loss=1.681419, validation/accuracy=0.556220, validation/loss=1.899905, validation/num_examples=50000
I0201 18:51:16.391077 139774434195200 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.6089508533477783, loss=2.6343231201171875
I0201 18:52:01.232230 139774417409792 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.2166918516159058, loss=4.558399200439453
I0201 18:52:47.322268 139774434195200 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.4351660013198853, loss=4.748703479766846
I0201 18:53:33.558238 139774417409792 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.3172160387039185, loss=5.338975429534912
I0201 18:54:19.663529 139774434195200 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.941286325454712, loss=2.6919047832489014
I0201 18:55:05.737725 139774417409792 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1936323642730713, loss=4.8626933097839355
I0201 18:55:51.855945 139774434195200 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.6502585411071777, loss=2.7640135288238525
I0201 18:56:37.872512 139774417409792 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.4061719179153442, loss=4.369030475616455
I0201 18:57:24.006877 139774434195200 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.6505863666534424, loss=2.746407985687256
I0201 18:57:53.976615 139936116377408 spec.py:321] Evaluating on the training split.
I0201 18:58:06.166400 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 18:58:41.026261 139936116377408 spec.py:349] Evaluating on the test split.
I0201 18:58:42.633880 139936116377408 submission_runner.py:408] Time since start: 21454.49s, 	Step: 42267, 	{'train/accuracy': 0.6179882884025574, 'train/loss': 1.5960801839828491, 'validation/accuracy': 0.560479998588562, 'validation/loss': 1.867920994758606, 'validation/num_examples': 50000, 'test/accuracy': 0.442300021648407, 'test/loss': 2.526688814163208, 'test/num_examples': 10000, 'score': 19364.844741344452, 'total_duration': 21454.487444639206, 'accumulated_submission_time': 19364.844741344452, 'accumulated_eval_time': 2085.58514547348, 'accumulated_logging_time': 1.8638558387756348}
I0201 18:58:42.655516 139774417409792 logging_writer.py:48] [42267] accumulated_eval_time=2085.585145, accumulated_logging_time=1.863856, accumulated_submission_time=19364.844741, global_step=42267, preemption_count=0, score=19364.844741, test/accuracy=0.442300, test/loss=2.526689, test/num_examples=10000, total_duration=21454.487445, train/accuracy=0.617988, train/loss=1.596080, validation/accuracy=0.560480, validation/loss=1.867921, validation/num_examples=50000
I0201 18:58:56.143670 139774434195200 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6045246124267578, loss=2.737644910812378
I0201 18:59:39.862219 139774417409792 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.5101048946380615, loss=3.375119209289551
I0201 19:00:25.995925 139774434195200 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.70548677444458, loss=2.9084296226501465
I0201 19:01:12.654466 139774417409792 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.2535486221313477, loss=3.9749646186828613
I0201 19:01:58.672817 139774434195200 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.9620660543441772, loss=2.695639133453369
I0201 19:02:45.013094 139774417409792 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.5808700323104858, loss=2.6739258766174316
I0201 19:03:31.102214 139774434195200 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.6651448011398315, loss=2.7050788402557373
I0201 19:04:17.456043 139774417409792 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.6332985162734985, loss=2.8296806812286377
I0201 19:05:03.422583 139774434195200 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.8219367265701294, loss=2.765803575515747
I0201 19:05:42.961696 139936116377408 spec.py:321] Evaluating on the training split.
I0201 19:05:55.012055 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 19:06:26.651664 139936116377408 spec.py:349] Evaluating on the test split.
I0201 19:06:28.266039 139936116377408 submission_runner.py:408] Time since start: 21920.12s, 	Step: 43187, 	{'train/accuracy': 0.5931640267372131, 'train/loss': 1.7232056856155396, 'validation/accuracy': 0.5553199648857117, 'validation/loss': 1.9169193506240845, 'validation/num_examples': 50000, 'test/accuracy': 0.44120001792907715, 'test/loss': 2.5503687858581543, 'test/num_examples': 10000, 'score': 19785.094200372696, 'total_duration': 21920.119605779648, 'accumulated_submission_time': 19785.094200372696, 'accumulated_eval_time': 2130.889495611191, 'accumulated_logging_time': 1.8945605754852295}
I0201 19:06:28.288544 139774417409792 logging_writer.py:48] [43187] accumulated_eval_time=2130.889496, accumulated_logging_time=1.894561, accumulated_submission_time=19785.094200, global_step=43187, preemption_count=0, score=19785.094200, test/accuracy=0.441200, test/loss=2.550369, test/num_examples=10000, total_duration=21920.119606, train/accuracy=0.593164, train/loss=1.723206, validation/accuracy=0.555320, validation/loss=1.916919, validation/num_examples=50000
I0201 19:06:33.841151 139774434195200 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.761001467704773, loss=2.780154228210449
I0201 19:07:16.083097 139774417409792 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.6914260387420654, loss=2.997032642364502
I0201 19:08:01.953504 139774434195200 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.575028896331787, loss=2.72829008102417
I0201 19:08:48.284473 139774417409792 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.4552252292633057, loss=3.4474027156829834
I0201 19:09:34.285234 139774434195200 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.8421963453292847, loss=2.876114845275879
I0201 19:10:20.425468 139774417409792 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.5097835063934326, loss=3.4620227813720703
I0201 19:11:06.956899 139774434195200 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.287827968597412, loss=4.273528099060059
I0201 19:11:53.137160 139774417409792 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.5550849437713623, loss=4.769705772399902
I0201 19:12:39.669943 139774434195200 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.6709887981414795, loss=2.690670967102051
I0201 19:13:25.733351 139774417409792 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6200387477874756, loss=2.6834630966186523
I0201 19:13:28.561591 139936116377408 spec.py:321] Evaluating on the training split.
I0201 19:13:40.627245 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 19:14:15.084498 139936116377408 spec.py:349] Evaluating on the test split.
I0201 19:14:16.683635 139936116377408 submission_runner.py:408] Time since start: 22388.54s, 	Step: 44108, 	{'train/accuracy': 0.6003515720367432, 'train/loss': 1.6775927543640137, 'validation/accuracy': 0.5586400032043457, 'validation/loss': 1.8800852298736572, 'validation/num_examples': 50000, 'test/accuracy': 0.4472000300884247, 'test/loss': 2.5132040977478027, 'test/num_examples': 10000, 'score': 20205.310017347336, 'total_duration': 22388.537200927734, 'accumulated_submission_time': 20205.310017347336, 'accumulated_eval_time': 2179.0115325450897, 'accumulated_logging_time': 1.9270873069763184}
I0201 19:14:16.707553 139774434195200 logging_writer.py:48] [44108] accumulated_eval_time=2179.011533, accumulated_logging_time=1.927087, accumulated_submission_time=20205.310017, global_step=44108, preemption_count=0, score=20205.310017, test/accuracy=0.447200, test/loss=2.513204, test/num_examples=10000, total_duration=22388.537201, train/accuracy=0.600352, train/loss=1.677593, validation/accuracy=0.558640, validation/loss=1.880085, validation/num_examples=50000
I0201 19:14:54.920424 139774417409792 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.558547019958496, loss=3.0469017028808594
I0201 19:15:40.731194 139774434195200 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.8775370121002197, loss=2.764672040939331
I0201 19:16:27.104916 139774417409792 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.9028130769729614, loss=2.7836217880249023
I0201 19:17:13.095749 139774434195200 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.8470489978790283, loss=2.8028905391693115
I0201 19:17:58.993543 139774417409792 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.624283790588379, loss=3.0288326740264893
I0201 19:18:45.214207 139774434195200 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.6179778575897217, loss=2.720611810684204
I0201 19:19:31.392984 139774417409792 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.6147725582122803, loss=3.2223024368286133
I0201 19:20:17.545433 139774434195200 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7022600173950195, loss=2.907954692840576
I0201 19:21:03.523572 139774417409792 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.52280592918396, loss=3.1199655532836914
I0201 19:21:16.863623 139936116377408 spec.py:321] Evaluating on the training split.
I0201 19:21:29.119359 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 19:22:01.540713 139936116377408 spec.py:349] Evaluating on the test split.
I0201 19:22:03.157055 139936116377408 submission_runner.py:408] Time since start: 22855.01s, 	Step: 45030, 	{'train/accuracy': 0.6131640672683716, 'train/loss': 1.6342071294784546, 'validation/accuracy': 0.5626999735832214, 'validation/loss': 1.8697541952133179, 'validation/num_examples': 50000, 'test/accuracy': 0.44200003147125244, 'test/loss': 2.5277490615844727, 'test/num_examples': 10000, 'score': 20625.409114599228, 'total_duration': 22855.010596990585, 'accumulated_submission_time': 20625.409114599228, 'accumulated_eval_time': 2225.3049223423004, 'accumulated_logging_time': 1.9600763320922852}
I0201 19:22:03.180965 139774434195200 logging_writer.py:48] [45030] accumulated_eval_time=2225.304922, accumulated_logging_time=1.960076, accumulated_submission_time=20625.409115, global_step=45030, preemption_count=0, score=20625.409115, test/accuracy=0.442000, test/loss=2.527749, test/num_examples=10000, total_duration=22855.010597, train/accuracy=0.613164, train/loss=1.634207, validation/accuracy=0.562700, validation/loss=1.869754, validation/num_examples=50000
I0201 19:22:31.371914 139774417409792 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.7876662015914917, loss=2.6989920139312744
I0201 19:23:16.996123 139774434195200 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.3668770790100098, loss=5.139743328094482
I0201 19:24:03.406455 139774417409792 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.9472895860671997, loss=2.78460955619812
I0201 19:24:49.718631 139774434195200 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.7695317268371582, loss=2.7158849239349365
I0201 19:25:35.863335 139774417409792 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.8109220266342163, loss=2.6479687690734863
I0201 19:26:21.965001 139774434195200 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.5995855331420898, loss=3.4617059230804443
I0201 19:27:08.058404 139774417409792 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.7266271114349365, loss=2.825005054473877
I0201 19:27:54.177965 139774434195200 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.8260835409164429, loss=2.7307281494140625
I0201 19:28:40.309106 139774417409792 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.5754424333572388, loss=3.434781312942505
I0201 19:29:03.542384 139936116377408 spec.py:321] Evaluating on the training split.
I0201 19:29:15.650918 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 19:29:48.556006 139936116377408 spec.py:349] Evaluating on the test split.
I0201 19:29:50.161977 139936116377408 submission_runner.py:408] Time since start: 23322.02s, 	Step: 45952, 	{'train/accuracy': 0.6003320217132568, 'train/loss': 1.708828091621399, 'validation/accuracy': 0.5588200092315674, 'validation/loss': 1.8941869735717773, 'validation/num_examples': 50000, 'test/accuracy': 0.43790000677108765, 'test/loss': 2.5504677295684814, 'test/num_examples': 10000, 'score': 21045.712456464767, 'total_duration': 23322.01553273201, 'accumulated_submission_time': 21045.712456464767, 'accumulated_eval_time': 2271.9245131015778, 'accumulated_logging_time': 1.9946684837341309}
I0201 19:29:50.183535 139774434195200 logging_writer.py:48] [45952] accumulated_eval_time=2271.924513, accumulated_logging_time=1.994668, accumulated_submission_time=21045.712456, global_step=45952, preemption_count=0, score=21045.712456, test/accuracy=0.437900, test/loss=2.550468, test/num_examples=10000, total_duration=23322.015533, train/accuracy=0.600332, train/loss=1.708828, validation/accuracy=0.558820, validation/loss=1.894187, validation/num_examples=50000
I0201 19:30:09.623229 139774417409792 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.5653274059295654, loss=3.182103157043457
I0201 19:30:53.970288 139774434195200 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.566230297088623, loss=3.3684043884277344
I0201 19:31:40.018663 139774417409792 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.2885957956314087, loss=4.06346321105957
I0201 19:32:26.664909 139774434195200 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.7713866233825684, loss=2.690082311630249
I0201 19:33:12.590098 139774417409792 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.3200682401657104, loss=4.0153045654296875
I0201 19:33:58.694685 139774434195200 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.5884476900100708, loss=3.3136749267578125
I0201 19:34:44.708207 139774417409792 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.2853894233703613, loss=2.725160598754883
I0201 19:35:31.085085 139774434195200 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.6522780656814575, loss=2.636345148086548
I0201 19:36:17.361278 139774417409792 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.7861227989196777, loss=2.809685230255127
I0201 19:36:50.461556 139936116377408 spec.py:321] Evaluating on the training split.
I0201 19:37:02.746050 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 19:37:37.123488 139936116377408 spec.py:349] Evaluating on the test split.
I0201 19:37:38.738485 139936116377408 submission_runner.py:408] Time since start: 23790.59s, 	Step: 46874, 	{'train/accuracy': 0.6097265481948853, 'train/loss': 1.6467933654785156, 'validation/accuracy': 0.5633599758148193, 'validation/loss': 1.8516621589660645, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.517657518386841, 'test/num_examples': 10000, 'score': 21465.933866262436, 'total_duration': 23790.59200644493, 'accumulated_submission_time': 21465.933866262436, 'accumulated_eval_time': 2320.2013940811157, 'accumulated_logging_time': 2.025162935256958}
I0201 19:37:38.763791 139774434195200 logging_writer.py:48] [46874] accumulated_eval_time=2320.201394, accumulated_logging_time=2.025163, accumulated_submission_time=21465.933866, global_step=46874, preemption_count=0, score=21465.933866, test/accuracy=0.442200, test/loss=2.517658, test/num_examples=10000, total_duration=23790.592006, train/accuracy=0.609727, train/loss=1.646793, validation/accuracy=0.563360, validation/loss=1.851662, validation/num_examples=50000
I0201 19:37:49.479040 139774417409792 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.450846552848816, loss=4.957242965698242
I0201 19:38:32.439588 139774434195200 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.531620979309082, loss=2.6022696495056152
I0201 19:39:18.285421 139774417409792 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7609649896621704, loss=2.630147695541382
I0201 19:40:04.572482 139774434195200 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.3595287799835205, loss=4.924768447875977
I0201 19:40:50.496455 139774417409792 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.7376444339752197, loss=2.78129243850708
I0201 19:41:36.715369 139774434195200 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.5900933742523193, loss=3.5190062522888184
I0201 19:42:22.861438 139774417409792 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.6065077781677246, loss=2.6960620880126953
I0201 19:43:09.871804 139774434195200 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.648276448249817, loss=2.640233278274536
I0201 19:43:56.409486 139774417409792 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7135716676712036, loss=2.661902666091919
I0201 19:44:39.082301 139936116377408 spec.py:321] Evaluating on the training split.
I0201 19:44:51.182146 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 19:45:21.523539 139936116377408 spec.py:349] Evaluating on the test split.
I0201 19:45:23.130629 139936116377408 submission_runner.py:408] Time since start: 24254.98s, 	Step: 47793, 	{'train/accuracy': 0.6098241806030273, 'train/loss': 1.677674412727356, 'validation/accuracy': 0.5647000074386597, 'validation/loss': 1.8973495960235596, 'validation/num_examples': 50000, 'test/accuracy': 0.44510000944137573, 'test/loss': 2.5339388847351074, 'test/num_examples': 10000, 'score': 21886.19406223297, 'total_duration': 24254.984174966812, 'accumulated_submission_time': 21886.19406223297, 'accumulated_eval_time': 2364.2496979236603, 'accumulated_logging_time': 2.061084032058716}
I0201 19:45:23.156646 139774434195200 logging_writer.py:48] [47793] accumulated_eval_time=2364.249698, accumulated_logging_time=2.061084, accumulated_submission_time=21886.194062, global_step=47793, preemption_count=0, score=21886.194062, test/accuracy=0.445100, test/loss=2.533939, test/num_examples=10000, total_duration=24254.984175, train/accuracy=0.609824, train/loss=1.677674, validation/accuracy=0.564700, validation/loss=1.897350, validation/num_examples=50000
I0201 19:45:26.339244 139774417409792 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.4990639686584473, loss=4.491783618927002
I0201 19:46:08.273715 139774434195200 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7922321557998657, loss=2.6556339263916016
I0201 19:46:54.253918 139774417409792 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.3881573677062988, loss=3.8716068267822266
I0201 19:47:41.062916 139774434195200 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.417236566543579, loss=4.493483066558838
I0201 19:48:27.547950 139774417409792 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.9459842443466187, loss=2.7498836517333984
I0201 19:49:13.715516 139774434195200 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.521093726158142, loss=2.8131511211395264
I0201 19:49:59.905920 139774417409792 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.8132362365722656, loss=2.6946358680725098
I0201 19:50:46.145971 139774434195200 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.833816409111023, loss=2.803539752960205
I0201 19:51:32.393320 139774417409792 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.914040446281433, loss=2.524719476699829
I0201 19:52:19.033099 139774434195200 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.7285562753677368, loss=2.6440391540527344
I0201 19:52:23.365043 139936116377408 spec.py:321] Evaluating on the training split.
I0201 19:52:35.493414 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 19:53:09.687828 139936116377408 spec.py:349] Evaluating on the test split.
I0201 19:53:11.304076 139936116377408 submission_runner.py:408] Time since start: 24723.16s, 	Step: 48711, 	{'train/accuracy': 0.6181249618530273, 'train/loss': 1.6265902519226074, 'validation/accuracy': 0.5595600008964539, 'validation/loss': 1.886021375656128, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.5331673622131348, 'test/num_examples': 10000, 'score': 22306.34263277054, 'total_duration': 24723.15763783455, 'accumulated_submission_time': 22306.34263277054, 'accumulated_eval_time': 2412.188717842102, 'accumulated_logging_time': 2.0992519855499268}
I0201 19:53:11.329148 139774417409792 logging_writer.py:48] [48711] accumulated_eval_time=2412.188718, accumulated_logging_time=2.099252, accumulated_submission_time=22306.342633, global_step=48711, preemption_count=0, score=22306.342633, test/accuracy=0.443200, test/loss=2.533167, test/num_examples=10000, total_duration=24723.157638, train/accuracy=0.618125, train/loss=1.626590, validation/accuracy=0.559560, validation/loss=1.886021, validation/num_examples=50000
I0201 19:53:48.238513 139774434195200 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.7374298572540283, loss=2.6082205772399902
I0201 19:54:34.126449 139774417409792 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7154054641723633, loss=2.540682792663574
I0201 19:55:20.345650 139774434195200 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.5480566024780273, loss=4.908635139465332
I0201 19:56:06.735275 139774417409792 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.4405523538589478, loss=5.364020824432373
I0201 19:56:52.977210 139774434195200 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.8799394369125366, loss=2.5654566287994385
I0201 19:57:39.256101 139774417409792 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.7405195236206055, loss=3.142467737197876
I0201 19:58:25.362723 139774434195200 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.7197332382202148, loss=2.6486189365386963
I0201 19:59:11.689151 139774417409792 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.5631792545318604, loss=3.114626884460449
I0201 19:59:57.934638 139774434195200 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.831823706626892, loss=2.6506645679473877
I0201 20:00:11.552393 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:00:24.267913 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:00:57.690457 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:00:59.300324 139936116377408 submission_runner.py:408] Time since start: 25191.15s, 	Step: 49631, 	{'train/accuracy': 0.6079882383346558, 'train/loss': 1.6691138744354248, 'validation/accuracy': 0.5669800043106079, 'validation/loss': 1.862606406211853, 'validation/num_examples': 50000, 'test/accuracy': 0.444100022315979, 'test/loss': 2.522622585296631, 'test/num_examples': 10000, 'score': 22726.509202957153, 'total_duration': 25191.15389060974, 'accumulated_submission_time': 22726.509202957153, 'accumulated_eval_time': 2459.936644077301, 'accumulated_logging_time': 2.1330649852752686}
I0201 20:00:59.322424 139774417409792 logging_writer.py:48] [49631] accumulated_eval_time=2459.936644, accumulated_logging_time=2.133065, accumulated_submission_time=22726.509203, global_step=49631, preemption_count=0, score=22726.509203, test/accuracy=0.444100, test/loss=2.522623, test/num_examples=10000, total_duration=25191.153891, train/accuracy=0.607988, train/loss=1.669114, validation/accuracy=0.566980, validation/loss=1.862606, validation/num_examples=50000
I0201 20:01:27.124473 139774434195200 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.8545809984207153, loss=2.5788285732269287
I0201 20:02:12.746985 139774417409792 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.35901939868927, loss=5.230925559997559
I0201 20:02:59.063304 139774434195200 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.2365517616271973, loss=4.535516738891602
I0201 20:03:45.550707 139774417409792 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.650579810142517, loss=2.577202796936035
I0201 20:04:31.753688 139774434195200 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.2304130792617798, loss=4.103356838226318
I0201 20:05:17.812003 139774417409792 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.611061692237854, loss=2.900557279586792
I0201 20:06:03.754978 139774434195200 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.3795108795166016, loss=4.191927909851074
I0201 20:06:49.573787 139774417409792 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.459498405456543, loss=3.3783178329467773
I0201 20:07:35.818523 139774434195200 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.6454622745513916, loss=2.6365935802459717
I0201 20:07:59.754585 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:08:11.936811 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:08:45.005895 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:08:46.620665 139936116377408 submission_runner.py:408] Time since start: 25658.47s, 	Step: 50554, 	{'train/accuracy': 0.6143358945846558, 'train/loss': 1.6266354322433472, 'validation/accuracy': 0.5703999996185303, 'validation/loss': 1.8336740732192993, 'validation/num_examples': 50000, 'test/accuracy': 0.4585000276565552, 'test/loss': 2.471269369125366, 'test/num_examples': 10000, 'score': 23146.882725715637, 'total_duration': 25658.474204063416, 'accumulated_submission_time': 23146.882725715637, 'accumulated_eval_time': 2506.8026852607727, 'accumulated_logging_time': 2.1660990715026855}
I0201 20:08:46.648642 139774417409792 logging_writer.py:48] [50554] accumulated_eval_time=2506.802685, accumulated_logging_time=2.166099, accumulated_submission_time=23146.882726, global_step=50554, preemption_count=0, score=23146.882726, test/accuracy=0.458500, test/loss=2.471269, test/num_examples=10000, total_duration=25658.474204, train/accuracy=0.614336, train/loss=1.626635, validation/accuracy=0.570400, validation/loss=1.833674, validation/num_examples=50000
I0201 20:09:05.318329 139774434195200 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.376804232597351, loss=5.2017998695373535
I0201 20:09:49.461174 139774417409792 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.426047444343567, loss=3.391568660736084
I0201 20:10:35.677935 139774434195200 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8370320796966553, loss=2.5478007793426514
I0201 20:11:22.279517 139774417409792 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.8124207258224487, loss=2.6548726558685303
I0201 20:12:08.828555 139774434195200 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.53624427318573, loss=3.2575936317443848
I0201 20:12:54.784164 139774417409792 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.8233520984649658, loss=2.699181079864502
I0201 20:13:40.930898 139774434195200 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7463223934173584, loss=2.590456008911133
I0201 20:14:27.326692 139774417409792 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.7495027780532837, loss=2.6458890438079834
I0201 20:15:13.323540 139774434195200 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.578598976135254, loss=3.1093578338623047
I0201 20:15:46.649139 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:15:58.781646 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:16:28.991758 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:16:30.599528 139936116377408 submission_runner.py:408] Time since start: 26122.45s, 	Step: 51474, 	{'train/accuracy': 0.6382616758346558, 'train/loss': 1.5179091691970825, 'validation/accuracy': 0.5744999647140503, 'validation/loss': 1.8161741495132446, 'validation/num_examples': 50000, 'test/accuracy': 0.4506000280380249, 'test/loss': 2.467916250228882, 'test/num_examples': 10000, 'score': 23566.82369351387, 'total_duration': 26122.45309472084, 'accumulated_submission_time': 23566.82369351387, 'accumulated_eval_time': 2550.7530856132507, 'accumulated_logging_time': 2.205763101577759}
I0201 20:16:30.625914 139774417409792 logging_writer.py:48] [51474] accumulated_eval_time=2550.753086, accumulated_logging_time=2.205763, accumulated_submission_time=23566.823694, global_step=51474, preemption_count=0, score=23566.823694, test/accuracy=0.450600, test/loss=2.467916, test/num_examples=10000, total_duration=26122.453095, train/accuracy=0.638262, train/loss=1.517909, validation/accuracy=0.574500, validation/loss=1.816174, validation/num_examples=50000
I0201 20:16:41.343742 139774434195200 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.6837587356567383, loss=2.778247833251953
I0201 20:17:24.512012 139774417409792 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.6123911142349243, loss=4.486903667449951
I0201 20:18:10.641998 139774434195200 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8046869039535522, loss=2.5673410892486572
I0201 20:18:56.988991 139774417409792 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.4877959489822388, loss=3.6680378913879395
I0201 20:19:43.323945 139774434195200 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.4350459575653076, loss=3.5039358139038086
I0201 20:20:29.798201 139774417409792 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.5001475811004639, loss=3.1080613136291504
I0201 20:21:15.937614 139774434195200 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.3489298820495605, loss=3.893173933029175
I0201 20:22:02.538795 139774417409792 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7348923683166504, loss=2.7399673461914062
I0201 20:22:48.624413 139774434195200 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.5934499502182007, loss=2.9942359924316406
I0201 20:23:30.907359 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:23:43.816579 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:24:15.628695 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:24:17.242484 139936116377408 submission_runner.py:408] Time since start: 26589.10s, 	Step: 52393, 	{'train/accuracy': 0.615527331829071, 'train/loss': 1.6107457876205444, 'validation/accuracy': 0.5764999985694885, 'validation/loss': 1.7930006980895996, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.4591097831726074, 'test/num_examples': 10000, 'score': 23987.047943353653, 'total_duration': 26589.09598493576, 'accumulated_submission_time': 23987.047943353653, 'accumulated_eval_time': 2597.088151693344, 'accumulated_logging_time': 2.242366075515747}
I0201 20:24:17.278308 139774417409792 logging_writer.py:48] [52393] accumulated_eval_time=2597.088152, accumulated_logging_time=2.242366, accumulated_submission_time=23987.047943, global_step=52393, preemption_count=0, score=23987.047943, test/accuracy=0.453100, test/loss=2.459110, test/num_examples=10000, total_duration=26589.095985, train/accuracy=0.615527, train/loss=1.610746, validation/accuracy=0.576500, validation/loss=1.793001, validation/num_examples=50000
I0201 20:24:20.459117 139774434195200 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.6267281770706177, loss=2.788806200027466
I0201 20:25:02.160820 139774417409792 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.7381097078323364, loss=2.5773541927337646
I0201 20:25:48.413929 139774434195200 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.6172459125518799, loss=3.2932443618774414
I0201 20:26:34.676827 139774417409792 logging_writer.py:48] [52700] global_step=52700, grad_norm=2.0711324214935303, loss=2.6428356170654297
I0201 20:27:20.944665 139774434195200 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8438050746917725, loss=2.6991524696350098
I0201 20:28:07.272938 139774417409792 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.497146725654602, loss=4.086400985717773
I0201 20:28:53.217651 139774434195200 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.529819369316101, loss=4.979808330535889
I0201 20:29:39.465735 139774417409792 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.8447705507278442, loss=2.6236162185668945
I0201 20:30:25.784011 139774434195200 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.6212941408157349, loss=2.691981315612793
I0201 20:31:11.892305 139774417409792 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.3973664045333862, loss=5.132387638092041
I0201 20:31:17.562124 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:31:29.471487 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:32:04.662661 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:32:06.262660 139936116377408 submission_runner.py:408] Time since start: 27058.12s, 	Step: 53314, 	{'train/accuracy': 0.6172460913658142, 'train/loss': 1.6188424825668335, 'validation/accuracy': 0.5728799700737, 'validation/loss': 1.8209742307662964, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.461902141571045, 'test/num_examples': 10000, 'score': 24407.272254943848, 'total_duration': 27058.116228818893, 'accumulated_submission_time': 24407.272254943848, 'accumulated_eval_time': 2645.788686990738, 'accumulated_logging_time': 2.2893104553222656}
I0201 20:32:06.285454 139774434195200 logging_writer.py:48] [53314] accumulated_eval_time=2645.788687, accumulated_logging_time=2.289310, accumulated_submission_time=24407.272255, global_step=53314, preemption_count=0, score=24407.272255, test/accuracy=0.462200, test/loss=2.461902, test/num_examples=10000, total_duration=27058.116229, train/accuracy=0.617246, train/loss=1.618842, validation/accuracy=0.572880, validation/loss=1.820974, validation/num_examples=50000
I0201 20:32:41.600765 139774417409792 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.7496438026428223, loss=2.908682346343994
I0201 20:33:27.565157 139774434195200 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.4831655025482178, loss=2.9438743591308594
I0201 20:34:13.631648 139774417409792 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.9042718410491943, loss=2.5497725009918213
I0201 20:34:59.712791 139774434195200 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.4780006408691406, loss=3.2894272804260254
I0201 20:35:46.314919 139774417409792 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.568008303642273, loss=3.475532054901123
I0201 20:36:32.532519 139774434195200 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.452147126197815, loss=4.391335487365723
I0201 20:37:18.820496 139774417409792 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.2606878280639648, loss=4.344010353088379
I0201 20:38:05.200782 139774434195200 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.5980665683746338, loss=3.531623363494873
I0201 20:38:51.440808 139774417409792 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.6515353918075562, loss=2.6564688682556152
I0201 20:39:06.414138 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:39:18.491821 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:39:53.004753 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:39:54.615946 139936116377408 submission_runner.py:408] Time since start: 27526.47s, 	Step: 54234, 	{'train/accuracy': 0.6295117139816284, 'train/loss': 1.57720947265625, 'validation/accuracy': 0.572219967842102, 'validation/loss': 1.8321927785873413, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.464775800704956, 'test/num_examples': 10000, 'score': 24827.344648122787, 'total_duration': 27526.469512939453, 'accumulated_submission_time': 24827.344648122787, 'accumulated_eval_time': 2693.9905047416687, 'accumulated_logging_time': 2.3210394382476807}
I0201 20:39:54.638498 139774434195200 logging_writer.py:48] [54234] accumulated_eval_time=2693.990505, accumulated_logging_time=2.321039, accumulated_submission_time=24827.344648, global_step=54234, preemption_count=0, score=24827.344648, test/accuracy=0.457900, test/loss=2.464776, test/num_examples=10000, total_duration=27526.469513, train/accuracy=0.629512, train/loss=1.577209, validation/accuracy=0.572220, validation/loss=1.832193, validation/num_examples=50000
I0201 20:40:21.240040 139774417409792 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.4088329076766968, loss=3.8260018825531006
I0201 20:41:06.744472 139774434195200 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8572015762329102, loss=2.6480329036712646
I0201 20:41:53.048549 139774417409792 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.7173818349838257, loss=2.5973691940307617
I0201 20:42:39.823810 139774434195200 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.5858490467071533, loss=3.2216594219207764
I0201 20:43:26.138103 139774417409792 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.8368607759475708, loss=2.657726287841797
I0201 20:44:12.657275 139774434195200 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.3632980585098267, loss=4.342124938964844
I0201 20:44:58.794834 139774417409792 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.7862446308135986, loss=2.5221264362335205
I0201 20:45:45.439640 139774434195200 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.1524410247802734, loss=2.7532410621643066
I0201 20:46:31.882192 139774417409792 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.7175382375717163, loss=2.531456232070923
I0201 20:46:55.066123 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:47:07.364966 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:47:41.534759 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:47:43.150938 139936116377408 submission_runner.py:408] Time since start: 27995.00s, 	Step: 55152, 	{'train/accuracy': 0.6147655844688416, 'train/loss': 1.6305612325668335, 'validation/accuracy': 0.5796599984169006, 'validation/loss': 1.788099765777588, 'validation/num_examples': 50000, 'test/accuracy': 0.46670001745224, 'test/loss': 2.4314701557159424, 'test/num_examples': 10000, 'score': 25247.712097883224, 'total_duration': 27995.004477739334, 'accumulated_submission_time': 25247.712097883224, 'accumulated_eval_time': 2742.075278520584, 'accumulated_logging_time': 2.35662841796875}
I0201 20:47:43.176630 139774434195200 logging_writer.py:48] [55152] accumulated_eval_time=2742.075279, accumulated_logging_time=2.356628, accumulated_submission_time=25247.712098, global_step=55152, preemption_count=0, score=25247.712098, test/accuracy=0.466700, test/loss=2.431470, test/num_examples=10000, total_duration=27995.004478, train/accuracy=0.614766, train/loss=1.630561, validation/accuracy=0.579660, validation/loss=1.788100, validation/num_examples=50000
I0201 20:48:02.616461 139774417409792 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.8308531045913696, loss=2.768218517303467
I0201 20:48:47.053079 139774434195200 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.7293047904968262, loss=2.65047025680542
I0201 20:49:33.284584 139774417409792 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8156949281692505, loss=2.498540163040161
I0201 20:50:19.718050 139774434195200 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.9173115491867065, loss=2.607764959335327
I0201 20:51:05.870796 139774417409792 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.7301698923110962, loss=2.6977577209472656
I0201 20:51:51.972096 139774434195200 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.6290558576583862, loss=2.616070508956909
I0201 20:52:38.250214 139774417409792 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.7380266189575195, loss=2.615312337875366
I0201 20:53:24.274219 139774434195200 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.7563374042510986, loss=2.6230859756469727
I0201 20:54:10.637805 139774417409792 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7599705457687378, loss=2.864844799041748
I0201 20:54:43.332877 139936116377408 spec.py:321] Evaluating on the training split.
I0201 20:54:55.410293 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 20:55:25.534223 139936116377408 spec.py:349] Evaluating on the test split.
I0201 20:55:27.155434 139936116377408 submission_runner.py:408] Time since start: 28459.01s, 	Step: 56070, 	{'train/accuracy': 0.6223242282867432, 'train/loss': 1.5823334455490112, 'validation/accuracy': 0.5771600008010864, 'validation/loss': 1.789430856704712, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.429868221282959, 'test/num_examples': 10000, 'score': 25667.8112885952, 'total_duration': 28459.00898051262, 'accumulated_submission_time': 25667.8112885952, 'accumulated_eval_time': 2785.8978073596954, 'accumulated_logging_time': 2.3918521404266357}
I0201 20:55:27.183760 139774434195200 logging_writer.py:48] [56070] accumulated_eval_time=2785.897807, accumulated_logging_time=2.391852, accumulated_submission_time=25667.811289, global_step=56070, preemption_count=0, score=25667.811289, test/accuracy=0.464500, test/loss=2.429868, test/num_examples=10000, total_duration=28459.008981, train/accuracy=0.622324, train/loss=1.582333, validation/accuracy=0.577160, validation/loss=1.789431, validation/num_examples=50000
I0201 20:55:39.490193 139774417409792 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.6725608110427856, loss=3.07122802734375
I0201 20:56:22.790687 139774434195200 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.9197217226028442, loss=2.534303665161133
I0201 20:57:09.441797 139774417409792 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.6625128984451294, loss=2.568690776824951
I0201 20:57:55.784674 139774434195200 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.6821891069412231, loss=2.7265920639038086
I0201 20:58:42.211839 139774417409792 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.775937795639038, loss=2.7382733821868896
I0201 20:59:28.481487 139774434195200 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.5907331705093384, loss=5.363356590270996
I0201 21:00:14.716477 139774417409792 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.756572961807251, loss=2.497445821762085
I0201 21:01:00.779472 139774434195200 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2902042865753174, loss=4.604611396789551
I0201 21:01:47.130686 139774417409792 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.4028973579406738, loss=4.611591815948486
I0201 21:02:27.497403 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:02:39.489897 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:03:14.281434 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:03:15.893737 139936116377408 submission_runner.py:408] Time since start: 28927.75s, 	Step: 56989, 	{'train/accuracy': 0.6273437142372131, 'train/loss': 1.5826400518417358, 'validation/accuracy': 0.5783799886703491, 'validation/loss': 1.807153582572937, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.4639346599578857, 'test/num_examples': 10000, 'score': 26088.068242549896, 'total_duration': 28927.74728178978, 'accumulated_submission_time': 26088.068242549896, 'accumulated_eval_time': 2834.294125318527, 'accumulated_logging_time': 2.4296727180480957}
I0201 21:03:15.927579 139774434195200 logging_writer.py:48] [56989] accumulated_eval_time=2834.294125, accumulated_logging_time=2.429673, accumulated_submission_time=26088.068243, global_step=56989, preemption_count=0, score=26088.068243, test/accuracy=0.457800, test/loss=2.463935, test/num_examples=10000, total_duration=28927.747282, train/accuracy=0.627344, train/loss=1.582640, validation/accuracy=0.578380, validation/loss=1.807154, validation/num_examples=50000
I0201 21:03:20.691248 139774417409792 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.439591884613037, loss=4.635547637939453
I0201 21:04:02.615339 139774434195200 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.2692776918411255, loss=4.606334209442139
I0201 21:04:48.845898 139774417409792 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.5127195119857788, loss=4.472543716430664
I0201 21:05:35.517675 139774434195200 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.7611416578292847, loss=3.454862594604492
I0201 21:06:21.777803 139774417409792 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.7901420593261719, loss=2.4660675525665283
I0201 21:07:07.824861 139774434195200 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.7999032735824585, loss=2.8362603187561035
I0201 21:07:54.200070 139774417409792 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.5836585760116577, loss=3.688465118408203
I0201 21:08:40.457337 139774434195200 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9762080907821655, loss=2.6158008575439453
I0201 21:09:26.593644 139774417409792 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.4571105241775513, loss=3.2743654251098633
I0201 21:10:12.896915 139774434195200 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.7022502422332764, loss=2.680271625518799
I0201 21:10:16.274233 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:10:28.396842 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:11:01.192033 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:11:02.806710 139936116377408 submission_runner.py:408] Time since start: 29394.66s, 	Step: 57909, 	{'train/accuracy': 0.6226171851158142, 'train/loss': 1.5738970041275024, 'validation/accuracy': 0.578719973564148, 'validation/loss': 1.7816462516784668, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.410048246383667, 'test/num_examples': 10000, 'score': 26508.355674743652, 'total_duration': 29394.660277605057, 'accumulated_submission_time': 26508.355674743652, 'accumulated_eval_time': 2880.826591491699, 'accumulated_logging_time': 2.475525379180908}
I0201 21:11:02.830456 139774417409792 logging_writer.py:48] [57909] accumulated_eval_time=2880.826591, accumulated_logging_time=2.475525, accumulated_submission_time=26508.355675, global_step=57909, preemption_count=0, score=26508.355675, test/accuracy=0.463900, test/loss=2.410048, test/num_examples=10000, total_duration=29394.660278, train/accuracy=0.622617, train/loss=1.573897, validation/accuracy=0.578720, validation/loss=1.781646, validation/num_examples=50000
I0201 21:11:40.544745 139774434195200 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.7900238037109375, loss=2.543161630630493
I0201 21:12:26.594683 139774417409792 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.6788549423217773, loss=4.1529130935668945
I0201 21:13:12.835197 139774434195200 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.5005497932434082, loss=4.326175689697266
I0201 21:13:59.164506 139774417409792 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.686418890953064, loss=2.5711135864257812
I0201 21:14:45.256295 139774434195200 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9214478731155396, loss=2.5265254974365234
I0201 21:15:31.597795 139774417409792 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.501613736152649, loss=3.820411443710327
I0201 21:16:18.223950 139774434195200 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.7695441246032715, loss=2.927222728729248
I0201 21:17:04.622327 139774417409792 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.6885055303573608, loss=2.653877019882202
I0201 21:17:51.055249 139774434195200 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.2116189002990723, loss=2.6315643787384033
I0201 21:18:02.815550 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:18:14.811623 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:18:47.889219 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:18:49.494904 139936116377408 submission_runner.py:408] Time since start: 29861.35s, 	Step: 58827, 	{'train/accuracy': 0.6302929520606995, 'train/loss': 1.5352495908737183, 'validation/accuracy': 0.5891000032424927, 'validation/loss': 1.726948857307434, 'validation/num_examples': 50000, 'test/accuracy': 0.4725000262260437, 'test/loss': 2.381237268447876, 'test/num_examples': 10000, 'score': 26928.282299280167, 'total_duration': 29861.34847187996, 'accumulated_submission_time': 26928.282299280167, 'accumulated_eval_time': 2927.505961894989, 'accumulated_logging_time': 2.509864330291748}
I0201 21:18:49.518819 139774417409792 logging_writer.py:48] [58827] accumulated_eval_time=2927.505962, accumulated_logging_time=2.509864, accumulated_submission_time=26928.282299, global_step=58827, preemption_count=0, score=26928.282299, test/accuracy=0.472500, test/loss=2.381237, test/num_examples=10000, total_duration=29861.348472, train/accuracy=0.630293, train/loss=1.535250, validation/accuracy=0.589100, validation/loss=1.726949, validation/num_examples=50000
I0201 21:19:18.979566 139774434195200 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.554016351699829, loss=4.12436056137085
I0201 21:20:04.821055 139774417409792 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.7634470462799072, loss=2.5506818294525146
I0201 21:20:50.860502 139774434195200 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.533033013343811, loss=3.3189921379089355
I0201 21:21:37.340862 139774417409792 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.8864794969558716, loss=2.472966432571411
I0201 21:22:23.627245 139774434195200 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.2852418422698975, loss=5.17264986038208
I0201 21:23:09.928557 139774417409792 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.438420057296753, loss=5.329681873321533
I0201 21:23:56.040760 139774434195200 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.5745776891708374, loss=5.21748161315918
I0201 21:24:42.363572 139774417409792 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.7654749155044556, loss=2.527661085128784
I0201 21:25:28.622796 139774434195200 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2856768369674683, loss=4.959181785583496
I0201 21:25:49.527714 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:26:01.575624 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:26:36.511419 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:26:38.120200 139936116377408 submission_runner.py:408] Time since start: 30329.97s, 	Step: 59747, 	{'train/accuracy': 0.6289257407188416, 'train/loss': 1.5401300191879272, 'validation/accuracy': 0.581559956073761, 'validation/loss': 1.7661179304122925, 'validation/num_examples': 50000, 'test/accuracy': 0.4588000178337097, 'test/loss': 2.400780200958252, 'test/num_examples': 10000, 'score': 27348.234403848648, 'total_duration': 30329.973765850067, 'accumulated_submission_time': 27348.234403848648, 'accumulated_eval_time': 2976.0984501838684, 'accumulated_logging_time': 2.5432207584381104}
I0201 21:26:38.146704 139774417409792 logging_writer.py:48] [59747] accumulated_eval_time=2976.098450, accumulated_logging_time=2.543221, accumulated_submission_time=27348.234404, global_step=59747, preemption_count=0, score=27348.234404, test/accuracy=0.458800, test/loss=2.400780, test/num_examples=10000, total_duration=30329.973766, train/accuracy=0.628926, train/loss=1.540130, validation/accuracy=0.581560, validation/loss=1.766118, validation/num_examples=50000
I0201 21:26:59.583457 139774434195200 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.555286169052124, loss=2.931206464767456
I0201 21:27:43.941522 139774417409792 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.755913496017456, loss=2.5250895023345947
I0201 21:28:30.227344 139774434195200 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.4937238693237305, loss=3.5555307865142822
I0201 21:29:16.880918 139774417409792 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.781082272529602, loss=2.5795931816101074
I0201 21:30:02.970166 139774434195200 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.8952651023864746, loss=2.539114475250244
I0201 21:30:49.044230 139774417409792 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.6770683526992798, loss=4.100224494934082
I0201 21:31:35.199466 139774434195200 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.071434497833252, loss=2.535684823989868
I0201 21:32:21.538912 139774417409792 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.0251612663269043, loss=2.6834046840667725
I0201 21:33:07.695982 139774434195200 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.7435814142227173, loss=3.0059590339660645
I0201 21:33:38.276153 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:33:50.359760 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:34:22.799059 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:34:24.400435 139936116377408 submission_runner.py:408] Time since start: 30796.25s, 	Step: 60667, 	{'train/accuracy': 0.6617382764816284, 'train/loss': 1.412854790687561, 'validation/accuracy': 0.5889999866485596, 'validation/loss': 1.731900691986084, 'validation/num_examples': 50000, 'test/accuracy': 0.4660000205039978, 'test/loss': 2.385641574859619, 'test/num_examples': 10000, 'score': 27768.30704021454, 'total_duration': 30796.253998041153, 'accumulated_submission_time': 27768.30704021454, 'accumulated_eval_time': 3022.2227504253387, 'accumulated_logging_time': 2.578721761703491}
I0201 21:34:24.428665 139774417409792 logging_writer.py:48] [60667] accumulated_eval_time=3022.222750, accumulated_logging_time=2.578722, accumulated_submission_time=27768.307040, global_step=60667, preemption_count=0, score=27768.307040, test/accuracy=0.466000, test/loss=2.385642, test/num_examples=10000, total_duration=30796.253998, train/accuracy=0.661738, train/loss=1.412855, validation/accuracy=0.589000, validation/loss=1.731901, validation/num_examples=50000
I0201 21:34:37.927556 139774434195200 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.7239915132522583, loss=2.6648240089416504
I0201 21:35:21.454118 139774417409792 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.9077394008636475, loss=2.6739425659179688
I0201 21:36:07.927800 139774434195200 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.61717689037323, loss=3.7032227516174316
I0201 21:36:54.507609 139774417409792 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.7242414951324463, loss=4.4849419593811035
I0201 21:37:40.697539 139774434195200 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.8789869546890259, loss=2.7084853649139404
I0201 21:38:27.156481 139774417409792 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.3778705596923828, loss=4.103104591369629
I0201 21:39:14.078868 139774434195200 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.757035255432129, loss=2.574401378631592
I0201 21:40:00.307181 139774417409792 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8665920495986938, loss=2.5783703327178955
I0201 21:40:46.561542 139774434195200 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.3746204376220703, loss=2.519454002380371
I0201 21:41:24.509437 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:41:36.804930 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:42:10.797711 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:42:12.405089 139936116377408 submission_runner.py:408] Time since start: 31264.26s, 	Step: 61583, 	{'train/accuracy': 0.6290038824081421, 'train/loss': 1.5284565687179565, 'validation/accuracy': 0.5866400003433228, 'validation/loss': 1.7359672784805298, 'validation/num_examples': 50000, 'test/accuracy': 0.47290003299713135, 'test/loss': 2.3630123138427734, 'test/num_examples': 10000, 'score': 28188.32996058464, 'total_duration': 31264.258637428284, 'accumulated_submission_time': 28188.32996058464, 'accumulated_eval_time': 3070.1183915138245, 'accumulated_logging_time': 2.6172053813934326}
I0201 21:42:12.437062 139774417409792 logging_writer.py:48] [61583] accumulated_eval_time=3070.118392, accumulated_logging_time=2.617205, accumulated_submission_time=28188.329961, global_step=61583, preemption_count=0, score=28188.329961, test/accuracy=0.472900, test/loss=2.363012, test/num_examples=10000, total_duration=31264.258637, train/accuracy=0.629004, train/loss=1.528457, validation/accuracy=0.586640, validation/loss=1.735967, validation/num_examples=50000
I0201 21:42:19.590126 139774434195200 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.8307029008865356, loss=2.890955924987793
I0201 21:43:02.039613 139774417409792 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.4551416635513306, loss=4.042980670928955
I0201 21:43:48.267361 139774434195200 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.7078622579574585, loss=2.6747045516967773
I0201 21:44:34.558101 139774417409792 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.8353164196014404, loss=2.5675032138824463
I0201 21:45:20.898610 139774434195200 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.7949132919311523, loss=2.5893869400024414
I0201 21:46:07.249321 139774417409792 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.8629412651062012, loss=2.849236249923706
I0201 21:46:53.648714 139774434195200 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.8262288570404053, loss=2.5234625339508057
I0201 21:47:40.088524 139774417409792 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.5331472158432007, loss=5.183670997619629
I0201 21:48:26.684526 139774434195200 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.6531165838241577, loss=2.608396530151367
I0201 21:49:13.083226 139774417409792 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9168137311935425, loss=3.299295663833618
I0201 21:49:13.098384 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:49:25.260962 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:49:58.279976 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:49:59.881068 139936116377408 submission_runner.py:408] Time since start: 31731.73s, 	Step: 62501, 	{'train/accuracy': 0.6339648365974426, 'train/loss': 1.5344921350479126, 'validation/accuracy': 0.5881800055503845, 'validation/loss': 1.743263602256775, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.3870527744293213, 'test/num_examples': 10000, 'score': 28608.93250131607, 'total_duration': 31731.734622240067, 'accumulated_submission_time': 28608.93250131607, 'accumulated_eval_time': 3116.9010808467865, 'accumulated_logging_time': 2.6603078842163086}
I0201 21:49:59.910395 139774434195200 logging_writer.py:48] [62501] accumulated_eval_time=3116.901081, accumulated_logging_time=2.660308, accumulated_submission_time=28608.932501, global_step=62501, preemption_count=0, score=28608.932501, test/accuracy=0.469000, test/loss=2.387053, test/num_examples=10000, total_duration=31731.734622, train/accuracy=0.633965, train/loss=1.534492, validation/accuracy=0.588180, validation/loss=1.743264, validation/num_examples=50000
I0201 21:50:41.386803 139774417409792 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.6944035291671753, loss=2.673675537109375
I0201 21:51:27.339217 139774434195200 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.5752989053726196, loss=4.170090198516846
I0201 21:52:13.902643 139774417409792 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.379307508468628, loss=4.8634562492370605
I0201 21:52:59.959259 139774434195200 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.8212509155273438, loss=2.493887424468994
I0201 21:53:46.117976 139774417409792 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.960373044013977, loss=2.5844335556030273
I0201 21:54:32.566342 139774434195200 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.7705262899398804, loss=2.4970481395721436
I0201 21:55:18.814781 139774417409792 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.52566397190094, loss=4.403702735900879
I0201 21:56:05.142895 139774434195200 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.6945955753326416, loss=3.003779888153076
I0201 21:56:51.225812 139774417409792 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.3674582242965698, loss=5.142800807952881
I0201 21:57:00.106347 139936116377408 spec.py:321] Evaluating on the training split.
I0201 21:57:12.359941 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 21:57:47.244201 139936116377408 spec.py:349] Evaluating on the test split.
I0201 21:57:48.845465 139936116377408 submission_runner.py:408] Time since start: 32200.70s, 	Step: 63421, 	{'train/accuracy': 0.6504101157188416, 'train/loss': 1.453066349029541, 'validation/accuracy': 0.5853399634361267, 'validation/loss': 1.746640682220459, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.4003686904907227, 'test/num_examples': 10000, 'score': 29029.071456193924, 'total_duration': 32200.69903111458, 'accumulated_submission_time': 29029.071456193924, 'accumulated_eval_time': 3165.640180826187, 'accumulated_logging_time': 2.6990957260131836}
I0201 21:57:48.869818 139774434195200 logging_writer.py:48] [63421] accumulated_eval_time=3165.640181, accumulated_logging_time=2.699096, accumulated_submission_time=29029.071456, global_step=63421, preemption_count=0, score=29029.071456, test/accuracy=0.468700, test/loss=2.400369, test/num_examples=10000, total_duration=32200.699031, train/accuracy=0.650410, train/loss=1.453066, validation/accuracy=0.585340, validation/loss=1.746641, validation/num_examples=50000
I0201 21:58:21.274754 139774417409792 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.6072697639465332, loss=3.5613598823547363
I0201 21:59:07.093516 139774434195200 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.55240797996521, loss=4.468803882598877
I0201 21:59:53.262195 139774417409792 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.7840029001235962, loss=2.551013469696045
I0201 22:00:40.043104 139774434195200 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.397519588470459, loss=4.435206890106201
I0201 22:01:25.935116 139774417409792 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.8614269495010376, loss=4.718837738037109
I0201 22:02:12.842347 139774434195200 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.7403509616851807, loss=2.54803729057312
I0201 22:02:58.513649 139774417409792 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.9191920757293701, loss=2.8478920459747314
I0201 22:03:44.672778 139774434195200 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.508217692375183, loss=5.093719482421875
I0201 22:04:30.949362 139774417409792 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.8603538274765015, loss=2.4473962783813477
I0201 22:04:49.057225 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:05:01.043928 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 22:05:34.419642 139936116377408 spec.py:349] Evaluating on the test split.
I0201 22:05:36.031068 139936116377408 submission_runner.py:408] Time since start: 32667.88s, 	Step: 64341, 	{'train/accuracy': 0.6320703029632568, 'train/loss': 1.5366462469100952, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.7273471355438232, 'validation/num_examples': 50000, 'test/accuracy': 0.4739000201225281, 'test/loss': 2.3589258193969727, 'test/num_examples': 10000, 'score': 29449.20278072357, 'total_duration': 32667.884612083435, 'accumulated_submission_time': 29449.20278072357, 'accumulated_eval_time': 3212.6139874458313, 'accumulated_logging_time': 2.7327611446380615}
I0201 22:05:36.059633 139774434195200 logging_writer.py:48] [64341] accumulated_eval_time=3212.613987, accumulated_logging_time=2.732761, accumulated_submission_time=29449.202781, global_step=64341, preemption_count=0, score=29449.202781, test/accuracy=0.473900, test/loss=2.358926, test/num_examples=10000, total_duration=32667.884612, train/accuracy=0.632070, train/loss=1.536646, validation/accuracy=0.588980, validation/loss=1.727347, validation/num_examples=50000
I0201 22:05:59.890447 139774417409792 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.4941133260726929, loss=4.90000581741333
I0201 22:06:44.749561 139774434195200 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9165794849395752, loss=2.553666114807129
I0201 22:07:31.070798 139774417409792 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8225418329238892, loss=2.9597744941711426
I0201 22:08:17.446809 139774434195200 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.8076783418655396, loss=2.5064492225646973
I0201 22:09:03.911745 139774417409792 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.5945907831192017, loss=2.946988344192505
I0201 22:09:50.421074 139774434195200 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.858277440071106, loss=3.522622585296631
I0201 22:10:36.620121 139774417409792 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.5149754285812378, loss=4.77196741104126
I0201 22:11:22.757949 139774434195200 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.4763154983520508, loss=3.620802164077759
I0201 22:12:09.121743 139774417409792 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.881914496421814, loss=2.608262777328491
I0201 22:12:36.282902 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:12:48.410536 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 22:13:20.629943 139936116377408 spec.py:349] Evaluating on the test split.
I0201 22:13:22.228316 139936116377408 submission_runner.py:408] Time since start: 33134.08s, 	Step: 65261, 	{'train/accuracy': 0.6381250023841858, 'train/loss': 1.5205624103546143, 'validation/accuracy': 0.5915200114250183, 'validation/loss': 1.7391788959503174, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.375885486602783, 'test/num_examples': 10000, 'score': 29869.367889881134, 'total_duration': 33134.08188343048, 'accumulated_submission_time': 29869.367889881134, 'accumulated_eval_time': 3258.5594029426575, 'accumulated_logging_time': 2.7715091705322266}
I0201 22:13:22.255151 139774434195200 logging_writer.py:48] [65261] accumulated_eval_time=3258.559403, accumulated_logging_time=2.771509, accumulated_submission_time=29869.367890, global_step=65261, preemption_count=0, score=29869.367890, test/accuracy=0.475400, test/loss=2.375885, test/num_examples=10000, total_duration=33134.081883, train/accuracy=0.638125, train/loss=1.520562, validation/accuracy=0.591520, validation/loss=1.739179, validation/num_examples=50000
I0201 22:13:38.148309 139774417409792 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.5482659339904785, loss=5.135567665100098
I0201 22:14:21.854115 139774434195200 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.7950221300125122, loss=2.552067756652832
I0201 22:15:07.854303 139774417409792 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.7760916948318481, loss=2.589566469192505
I0201 22:15:54.018984 139774434195200 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.7972389459609985, loss=2.868948459625244
I0201 22:16:40.115220 139774417409792 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.0204017162323, loss=2.5588855743408203
I0201 22:17:26.279692 139774434195200 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.7511374950408936, loss=2.517327308654785
I0201 22:18:12.032111 139774417409792 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.807867407798767, loss=3.1727805137634277
I0201 22:18:58.023856 139774434195200 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.934129238128662, loss=2.462780714035034
I0201 22:19:44.635408 139774417409792 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.4807547330856323, loss=4.5221405029296875
I0201 22:20:22.638413 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:20:34.746871 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 22:21:06.545550 139936116377408 spec.py:349] Evaluating on the test split.
I0201 22:21:08.158409 139936116377408 submission_runner.py:408] Time since start: 33600.01s, 	Step: 66184, 	{'train/accuracy': 0.6523046493530273, 'train/loss': 1.447373628616333, 'validation/accuracy': 0.595259964466095, 'validation/loss': 1.7073568105697632, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.3564023971557617, 'test/num_examples': 10000, 'score': 30289.691901922226, 'total_duration': 33600.01197576523, 'accumulated_submission_time': 30289.691901922226, 'accumulated_eval_time': 3304.079433441162, 'accumulated_logging_time': 2.809138774871826}
I0201 22:21:08.187581 139774434195200 logging_writer.py:48] [66184] accumulated_eval_time=3304.079433, accumulated_logging_time=2.809139, accumulated_submission_time=30289.691902, global_step=66184, preemption_count=0, score=30289.691902, test/accuracy=0.476300, test/loss=2.356402, test/num_examples=10000, total_duration=33600.011976, train/accuracy=0.652305, train/loss=1.447374, validation/accuracy=0.595260, validation/loss=1.707357, validation/num_examples=50000
I0201 22:21:14.934581 139774417409792 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.7286925315856934, loss=2.8819832801818848
I0201 22:21:57.620657 139774434195200 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.7997599840164185, loss=2.4424338340759277
I0201 22:22:43.907552 139774417409792 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.8095670938491821, loss=2.992469549179077
I0201 22:23:30.769734 139774434195200 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.0458643436431885, loss=2.4515156745910645
I0201 22:24:17.191838 139774417409792 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.8163747787475586, loss=2.5902271270751953
I0201 22:25:03.455639 139774434195200 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.6050890684127808, loss=5.089095115661621
I0201 22:25:49.726999 139774417409792 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.8582727909088135, loss=3.051778554916382
I0201 22:26:36.202820 139774434195200 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.562229037284851, loss=4.204943656921387
I0201 22:27:22.430607 139774417409792 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.5236117839813232, loss=4.889595985412598
I0201 22:28:08.677908 139774434195200 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.7840924263000488, loss=2.505934476852417
I0201 22:28:08.694518 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:28:20.783794 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 22:28:54.537435 139936116377408 spec.py:349] Evaluating on the test split.
I0201 22:28:56.139048 139936116377408 submission_runner.py:408] Time since start: 34067.99s, 	Step: 67101, 	{'train/accuracy': 0.6419531106948853, 'train/loss': 1.4803026914596558, 'validation/accuracy': 0.6005600094795227, 'validation/loss': 1.6780091524124146, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.3071646690368652, 'test/num_examples': 10000, 'score': 30710.141847133636, 'total_duration': 34067.99260735512, 'accumulated_submission_time': 30710.141847133636, 'accumulated_eval_time': 3351.5239536762238, 'accumulated_logging_time': 2.8487424850463867}
I0201 22:28:56.164155 139774417409792 logging_writer.py:48] [67101] accumulated_eval_time=3351.523954, accumulated_logging_time=2.848742, accumulated_submission_time=30710.141847, global_step=67101, preemption_count=0, score=30710.141847, test/accuracy=0.482100, test/loss=2.307165, test/num_examples=10000, total_duration=34067.992607, train/accuracy=0.641953, train/loss=1.480303, validation/accuracy=0.600560, validation/loss=1.678009, validation/num_examples=50000
I0201 22:29:37.521490 139774434195200 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.5126866102218628, loss=3.9872472286224365
I0201 22:30:23.408154 139774417409792 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.586957573890686, loss=4.075122356414795
I0201 22:31:10.028796 139774434195200 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.994743824005127, loss=2.9046988487243652
I0201 22:31:56.182286 139774417409792 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.191411256790161, loss=2.6236414909362793
I0201 22:32:42.812245 139774434195200 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.8524307012557983, loss=2.7090609073638916
I0201 22:33:29.077626 139774417409792 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.054487705230713, loss=2.5546178817749023
I0201 22:34:15.451812 139774434195200 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.9030605554580688, loss=2.5215930938720703
I0201 22:35:01.582738 139774417409792 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.8080527782440186, loss=5.221381187438965
I0201 22:35:47.872256 139774434195200 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.7200323343276978, loss=3.2104740142822266
I0201 22:35:56.470170 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:36:08.752761 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 22:36:40.791085 139936116377408 spec.py:349] Evaluating on the test split.
I0201 22:36:42.394282 139936116377408 submission_runner.py:408] Time since start: 34534.25s, 	Step: 68020, 	{'train/accuracy': 0.6422265768051147, 'train/loss': 1.4910420179367065, 'validation/accuracy': 0.5968599915504456, 'validation/loss': 1.6952730417251587, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.322556495666504, 'test/num_examples': 10000, 'score': 31130.390008687973, 'total_duration': 34534.2478351593, 'accumulated_submission_time': 31130.390008687973, 'accumulated_eval_time': 3397.4480471611023, 'accumulated_logging_time': 2.8839402198791504}
I0201 22:36:42.423007 139774417409792 logging_writer.py:48] [68020] accumulated_eval_time=3397.448047, accumulated_logging_time=2.883940, accumulated_submission_time=31130.390009, global_step=68020, preemption_count=0, score=31130.390009, test/accuracy=0.483600, test/loss=2.322556, test/num_examples=10000, total_duration=34534.247835, train/accuracy=0.642227, train/loss=1.491042, validation/accuracy=0.596860, validation/loss=1.695273, validation/num_examples=50000
I0201 22:37:14.991440 139774434195200 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.6709703207015991, loss=3.2026782035827637
I0201 22:38:00.681116 139774417409792 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.5778613090515137, loss=3.0014142990112305
I0201 22:38:47.004202 139774434195200 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.7089128494262695, loss=2.8581511974334717
I0201 22:39:33.321877 139774417409792 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.6099154949188232, loss=3.61946702003479
I0201 22:40:19.288334 139774434195200 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.064507007598877, loss=2.5596814155578613
I0201 22:41:05.690004 139774417409792 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.8316235542297363, loss=2.4875335693359375
I0201 22:41:51.966030 139774434195200 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.8397610187530518, loss=2.4006998538970947
I0201 22:42:38.322240 139774417409792 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.4987534284591675, loss=5.002588748931885
I0201 22:43:24.951099 139774434195200 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.927319049835205, loss=2.6706626415252686
I0201 22:43:42.620471 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:43:54.803334 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 22:44:26.026528 139936116377408 spec.py:349] Evaluating on the test split.
I0201 22:44:27.628607 139936116377408 submission_runner.py:408] Time since start: 34999.48s, 	Step: 68940, 	{'train/accuracy': 0.646289050579071, 'train/loss': 1.4966928958892822, 'validation/accuracy': 0.5961999893188477, 'validation/loss': 1.7258793115615845, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.361830711364746, 'test/num_examples': 10000, 'score': 31550.531436681747, 'total_duration': 34999.48217225075, 'accumulated_submission_time': 31550.531436681747, 'accumulated_eval_time': 3442.4561920166016, 'accumulated_logging_time': 2.9215943813323975}
I0201 22:44:27.655504 139774417409792 logging_writer.py:48] [68940] accumulated_eval_time=3442.456192, accumulated_logging_time=2.921594, accumulated_submission_time=31550.531437, global_step=68940, preemption_count=0, score=31550.531437, test/accuracy=0.481600, test/loss=2.361831, test/num_examples=10000, total_duration=34999.482172, train/accuracy=0.646289, train/loss=1.496693, validation/accuracy=0.596200, validation/loss=1.725879, validation/num_examples=50000
I0201 22:44:51.868287 139774434195200 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.0283968448638916, loss=2.440171241760254
I0201 22:45:36.983580 139774417409792 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.8435128927230835, loss=2.509763717651367
I0201 22:46:23.364559 139774434195200 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.9478864669799805, loss=2.4365079402923584
I0201 22:47:09.571966 139774417409792 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.832448959350586, loss=2.519535541534424
I0201 22:47:55.521278 139774434195200 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.01166033744812, loss=2.5170016288757324
I0201 22:48:41.634756 139774417409792 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.9050989151000977, loss=2.595247745513916
I0201 22:49:27.760392 139774434195200 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.8320013284683228, loss=2.5268962383270264
I0201 22:50:13.948868 139774417409792 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.7517560720443726, loss=2.541731834411621
I0201 22:51:00.093792 139774434195200 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.8531897068023682, loss=3.673463821411133
I0201 22:51:27.990803 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:51:40.365838 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 22:52:14.962922 139936116377408 spec.py:349] Evaluating on the test split.
I0201 22:52:16.562022 139936116377408 submission_runner.py:408] Time since start: 35468.42s, 	Step: 69862, 	{'train/accuracy': 0.6507031321525574, 'train/loss': 1.4504530429840088, 'validation/accuracy': 0.6015200018882751, 'validation/loss': 1.6813979148864746, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.3317134380340576, 'test/num_examples': 10000, 'score': 31970.809402942657, 'total_duration': 35468.415583610535, 'accumulated_submission_time': 31970.809402942657, 'accumulated_eval_time': 3491.027411222458, 'accumulated_logging_time': 2.9576942920684814}
I0201 22:52:16.589735 139774417409792 logging_writer.py:48] [69862] accumulated_eval_time=3491.027411, accumulated_logging_time=2.957694, accumulated_submission_time=31970.809403, global_step=69862, preemption_count=0, score=31970.809403, test/accuracy=0.483800, test/loss=2.331713, test/num_examples=10000, total_duration=35468.415584, train/accuracy=0.650703, train/loss=1.450453, validation/accuracy=0.601520, validation/loss=1.681398, validation/num_examples=50000
I0201 22:52:32.061314 139774434195200 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.8824739456176758, loss=2.5903546810150146
I0201 22:53:15.766634 139774417409792 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.880683183670044, loss=2.455695629119873
I0201 22:54:02.307928 139774434195200 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.7670432329177856, loss=2.5579733848571777
I0201 22:54:48.410682 139774417409792 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.874530553817749, loss=2.9019031524658203
I0201 22:55:34.445210 139774434195200 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.7563356161117554, loss=3.756946086883545
I0201 22:56:20.812257 139774417409792 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.634550929069519, loss=2.69993257522583
I0201 22:57:06.987477 139774434195200 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.8277037143707275, loss=2.557171583175659
I0201 22:57:52.999916 139774417409792 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.195411443710327, loss=2.5492782592773438
I0201 22:58:39.070394 139774434195200 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.9400688409805298, loss=2.5914013385772705
I0201 22:59:16.662042 139936116377408 spec.py:321] Evaluating on the training split.
I0201 22:59:28.770835 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:00:01.534174 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:00:03.164305 139936116377408 submission_runner.py:408] Time since start: 35935.02s, 	Step: 70783, 	{'train/accuracy': 0.6480273008346558, 'train/loss': 1.50771164894104, 'validation/accuracy': 0.600879967212677, 'validation/loss': 1.7230969667434692, 'validation/num_examples': 50000, 'test/accuracy': 0.48030000925064087, 'test/loss': 2.354217767715454, 'test/num_examples': 10000, 'score': 32390.825472831726, 'total_duration': 35935.01786708832, 'accumulated_submission_time': 32390.825472831726, 'accumulated_eval_time': 3537.5296635627747, 'accumulated_logging_time': 2.9942171573638916}
I0201 23:00:03.193577 139774417409792 logging_writer.py:48] [70783] accumulated_eval_time=3537.529664, accumulated_logging_time=2.994217, accumulated_submission_time=32390.825473, global_step=70783, preemption_count=0, score=32390.825473, test/accuracy=0.480300, test/loss=2.354218, test/num_examples=10000, total_duration=35935.017867, train/accuracy=0.648027, train/loss=1.507712, validation/accuracy=0.600880, validation/loss=1.723097, validation/num_examples=50000
I0201 23:00:10.343860 139774434195200 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.817915439605713, loss=2.494464159011841
I0201 23:00:52.541411 139774417409792 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.509083867073059, loss=3.996175765991211
I0201 23:01:38.647650 139774434195200 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.8205814361572266, loss=5.160286903381348
I0201 23:02:25.187095 139774417409792 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.856616497039795, loss=2.4901204109191895
I0201 23:03:11.468865 139774434195200 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.0418243408203125, loss=2.684828042984009
I0201 23:03:57.785109 139774417409792 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.0203099250793457, loss=2.740300178527832
I0201 23:04:43.909999 139774434195200 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.4825642108917236, loss=3.774522542953491
I0201 23:05:30.060741 139774417409792 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.8436042070388794, loss=2.8571603298187256
I0201 23:06:16.428298 139774434195200 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.7033673524856567, loss=3.794567584991455
I0201 23:07:02.590471 139774417409792 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.0082178115844727, loss=2.3989081382751465
I0201 23:07:03.220493 139936116377408 spec.py:321] Evaluating on the training split.
I0201 23:07:15.543789 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:07:49.407925 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:07:51.007658 139936116377408 submission_runner.py:408] Time since start: 36402.86s, 	Step: 71703, 	{'train/accuracy': 0.656054675579071, 'train/loss': 1.4267289638519287, 'validation/accuracy': 0.6021999716758728, 'validation/loss': 1.6627541780471802, 'validation/num_examples': 50000, 'test/accuracy': 0.4830000102519989, 'test/loss': 2.303077459335327, 'test/num_examples': 10000, 'score': 32810.79419326782, 'total_duration': 36402.86121177673, 'accumulated_submission_time': 32810.79419326782, 'accumulated_eval_time': 3585.3168003559113, 'accumulated_logging_time': 3.033379316329956}
I0201 23:07:51.038584 139774434195200 logging_writer.py:48] [71703] accumulated_eval_time=3585.316800, accumulated_logging_time=3.033379, accumulated_submission_time=32810.794193, global_step=71703, preemption_count=0, score=32810.794193, test/accuracy=0.483000, test/loss=2.303077, test/num_examples=10000, total_duration=36402.861212, train/accuracy=0.656055, train/loss=1.426729, validation/accuracy=0.602200, validation/loss=1.662754, validation/num_examples=50000
I0201 23:08:31.699698 139774417409792 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.4714213609695435, loss=4.371188163757324
I0201 23:09:17.885597 139774434195200 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.8787645101547241, loss=2.420069456100464
I0201 23:10:04.322432 139774417409792 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.8262161016464233, loss=2.904677152633667
I0201 23:10:50.827303 139774434195200 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.5766334533691406, loss=4.947967529296875
I0201 23:11:37.023163 139774417409792 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.768810749053955, loss=2.5848424434661865
I0201 23:12:23.571840 139774434195200 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.6289006471633911, loss=3.1918885707855225
I0201 23:13:10.072978 139774417409792 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.3283429145812988, loss=4.227413177490234
I0201 23:13:56.231023 139774434195200 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.8385170698165894, loss=2.5594940185546875
I0201 23:14:42.793506 139774417409792 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.6342687606811523, loss=3.4769935607910156
I0201 23:14:51.379606 139936116377408 spec.py:321] Evaluating on the training split.
I0201 23:15:04.344821 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:15:37.005818 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:15:38.616227 139936116377408 submission_runner.py:408] Time since start: 36870.47s, 	Step: 72620, 	{'train/accuracy': 0.6702929735183716, 'train/loss': 1.3864797353744507, 'validation/accuracy': 0.605679988861084, 'validation/loss': 1.6824146509170532, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.331911087036133, 'test/num_examples': 10000, 'score': 33231.07704329491, 'total_duration': 36870.469790935516, 'accumulated_submission_time': 33231.07704329491, 'accumulated_eval_time': 3632.5534229278564, 'accumulated_logging_time': 3.0752978324890137}
I0201 23:15:38.643996 139774434195200 logging_writer.py:48] [72620] accumulated_eval_time=3632.553423, accumulated_logging_time=3.075298, accumulated_submission_time=33231.077043, global_step=72620, preemption_count=0, score=33231.077043, test/accuracy=0.490600, test/loss=2.331911, test/num_examples=10000, total_duration=36870.469791, train/accuracy=0.670293, train/loss=1.386480, validation/accuracy=0.605680, validation/loss=1.682415, validation/num_examples=50000
I0201 23:16:11.179374 139774417409792 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.753380298614502, loss=2.704298496246338
I0201 23:16:56.923891 139774434195200 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.4249303340911865, loss=4.599485874176025
I0201 23:17:43.301584 139774417409792 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.5235180854797363, loss=5.0816874504089355
I0201 23:18:29.825371 139774434195200 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.7882518768310547, loss=4.026247024536133
I0201 23:19:15.987884 139774417409792 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.9268925189971924, loss=2.4537131786346436
I0201 23:20:02.420315 139774434195200 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.7421777248382568, loss=5.0957112312316895
I0201 23:20:48.630722 139774417409792 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.015068769454956, loss=2.7026479244232178
I0201 23:21:34.875916 139774434195200 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.8498518466949463, loss=2.6560983657836914
I0201 23:22:21.188680 139774417409792 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.017287015914917, loss=2.7125935554504395
I0201 23:22:38.890590 139936116377408 spec.py:321] Evaluating on the training split.
I0201 23:22:51.084762 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:23:24.208907 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:23:25.820071 139936116377408 submission_runner.py:408] Time since start: 37337.67s, 	Step: 73540, 	{'train/accuracy': 0.6487694978713989, 'train/loss': 1.4816597700119019, 'validation/accuracy': 0.6056999564170837, 'validation/loss': 1.6786365509033203, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.3274142742156982, 'test/num_examples': 10000, 'score': 33651.26502633095, 'total_duration': 37337.673639297485, 'accumulated_submission_time': 33651.26502633095, 'accumulated_eval_time': 3679.4828877449036, 'accumulated_logging_time': 3.113532781600952}
I0201 23:23:25.849688 139774434195200 logging_writer.py:48] [73540] accumulated_eval_time=3679.482888, accumulated_logging_time=3.113533, accumulated_submission_time=33651.265026, global_step=73540, preemption_count=0, score=33651.265026, test/accuracy=0.482600, test/loss=2.327414, test/num_examples=10000, total_duration=37337.673639, train/accuracy=0.648769, train/loss=1.481660, validation/accuracy=0.605700, validation/loss=1.678637, validation/num_examples=50000
I0201 23:23:50.069666 139774417409792 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.9730027914047241, loss=2.514688014984131
I0201 23:24:35.352424 139774434195200 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.854786992073059, loss=2.7507824897766113
I0201 23:25:21.688938 139774417409792 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.9956374168395996, loss=2.4157588481903076
I0201 23:26:08.153175 139774434195200 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.488194227218628, loss=4.982485771179199
I0201 23:26:53.930811 139774417409792 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.0094616413116455, loss=2.4730100631713867
I0201 23:27:40.421924 139774434195200 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.1490659713745117, loss=5.227218151092529
I0201 23:28:26.631296 139774417409792 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.07940673828125, loss=2.5923893451690674
I0201 23:29:12.788330 139774434195200 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.9953216314315796, loss=2.4196698665618896
I0201 23:29:59.149706 139774417409792 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.928468942642212, loss=4.008633136749268
I0201 23:30:25.919677 139936116377408 spec.py:321] Evaluating on the training split.
I0201 23:30:37.938161 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:31:14.107473 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:31:15.709848 139936116377408 submission_runner.py:408] Time since start: 37807.56s, 	Step: 74460, 	{'train/accuracy': 0.650585949420929, 'train/loss': 1.4430195093154907, 'validation/accuracy': 0.6067399978637695, 'validation/loss': 1.657494306564331, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.2823092937469482, 'test/num_examples': 10000, 'score': 34071.27830410004, 'total_duration': 37807.56340956688, 'accumulated_submission_time': 34071.27830410004, 'accumulated_eval_time': 3729.273057460785, 'accumulated_logging_time': 3.151975631713867}
I0201 23:31:15.735982 139774434195200 logging_writer.py:48] [74460] accumulated_eval_time=3729.273057, accumulated_logging_time=3.151976, accumulated_submission_time=34071.278304, global_step=74460, preemption_count=0, score=34071.278304, test/accuracy=0.485200, test/loss=2.282309, test/num_examples=10000, total_duration=37807.563410, train/accuracy=0.650586, train/loss=1.443020, validation/accuracy=0.606740, validation/loss=1.657494, validation/num_examples=50000
I0201 23:31:32.009172 139774417409792 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.8398126363754272, loss=2.451413631439209
I0201 23:32:15.917238 139774434195200 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.699209213256836, loss=3.080551862716675
I0201 23:33:01.952644 139774417409792 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.9466534852981567, loss=5.10211181640625
I0201 23:33:48.339742 139774434195200 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.8510339260101318, loss=2.4627904891967773
I0201 23:34:34.338180 139774417409792 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.6596125364303589, loss=4.183167457580566
I0201 23:35:20.453929 139774434195200 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.472620964050293, loss=4.751284599304199
I0201 23:36:06.957444 139774417409792 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.7973370552062988, loss=2.6183760166168213
I0201 23:36:53.025870 139774434195200 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.004706859588623, loss=2.4007375240325928
I0201 23:37:38.955799 139774417409792 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.8942718505859375, loss=2.457338571548462
I0201 23:38:16.119935 139936116377408 spec.py:321] Evaluating on the training split.
I0201 23:38:27.980347 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:39:02.239268 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:39:03.841410 139936116377408 submission_runner.py:408] Time since start: 38275.69s, 	Step: 75382, 	{'train/accuracy': 0.6629296541213989, 'train/loss': 1.3945977687835693, 'validation/accuracy': 0.6041199564933777, 'validation/loss': 1.6651949882507324, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3012421131134033, 'test/num_examples': 10000, 'score': 34491.6043074131, 'total_duration': 38275.6949763298, 'accumulated_submission_time': 34491.6043074131, 'accumulated_eval_time': 3776.9945294857025, 'accumulated_logging_time': 3.188735008239746}
I0201 23:39:03.873134 139774434195200 logging_writer.py:48] [75382] accumulated_eval_time=3776.994529, accumulated_logging_time=3.188735, accumulated_submission_time=34491.604307, global_step=75382, preemption_count=0, score=34491.604307, test/accuracy=0.486700, test/loss=2.301242, test/num_examples=10000, total_duration=38275.694976, train/accuracy=0.662930, train/loss=1.394598, validation/accuracy=0.604120, validation/loss=1.665195, validation/num_examples=50000
I0201 23:39:11.410500 139774417409792 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.053960084915161, loss=2.5203354358673096
I0201 23:39:53.991620 139774434195200 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.850228190422058, loss=2.615795135498047
I0201 23:40:40.184400 139774417409792 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.8226442337036133, loss=3.145232915878296
I0201 23:41:26.468297 139774434195200 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.6117901802062988, loss=3.780813694000244
I0201 23:42:12.849782 139774417409792 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.034317970275879, loss=2.7566287517547607
I0201 23:42:58.648794 139774434195200 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.9814107418060303, loss=2.448892593383789
I0201 23:43:44.921030 139774417409792 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.9270967245101929, loss=2.75589919090271
I0201 23:44:31.181968 139774434195200 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.9556550979614258, loss=2.3673086166381836
I0201 23:45:17.283369 139774417409792 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.9641731977462769, loss=2.4851086139678955
I0201 23:46:03.373809 139774434195200 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.1362643241882324, loss=2.5409047603607178
I0201 23:46:04.187788 139936116377408 spec.py:321] Evaluating on the training split.
I0201 23:46:16.112320 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:46:50.111946 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:46:51.714866 139936116377408 submission_runner.py:408] Time since start: 38743.57s, 	Step: 76303, 	{'train/accuracy': 0.6522851586341858, 'train/loss': 1.4518017768859863, 'validation/accuracy': 0.6080399751663208, 'validation/loss': 1.6486347913742065, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.312741279602051, 'test/num_examples': 10000, 'score': 34911.860114097595, 'total_duration': 38743.56843471527, 'accumulated_submission_time': 34911.860114097595, 'accumulated_eval_time': 3824.521594762802, 'accumulated_logging_time': 3.2313475608825684}
I0201 23:46:51.741475 139774417409792 logging_writer.py:48] [76303] accumulated_eval_time=3824.521595, accumulated_logging_time=3.231348, accumulated_submission_time=34911.860114, global_step=76303, preemption_count=0, score=34911.860114, test/accuracy=0.486100, test/loss=2.312741, test/num_examples=10000, total_duration=38743.568435, train/accuracy=0.652285, train/loss=1.451802, validation/accuracy=0.608040, validation/loss=1.648635, validation/num_examples=50000
I0201 23:47:32.261604 139774434195200 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.6257250308990479, loss=4.640030860900879
I0201 23:48:18.303659 139774417409792 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.7825380563735962, loss=4.033487319946289
I0201 23:49:04.775454 139774434195200 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.769452452659607, loss=4.970047473907471
I0201 23:49:50.930132 139774417409792 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.8724790811538696, loss=2.5991013050079346
I0201 23:50:37.248220 139774434195200 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.8597145080566406, loss=3.9854676723480225
I0201 23:51:23.510656 139774417409792 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.6232072114944458, loss=4.646214962005615
I0201 23:52:09.952654 139774434195200 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1170058250427246, loss=2.394127368927002
I0201 23:52:55.889760 139774417409792 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.1180691719055176, loss=2.4121804237365723
I0201 23:53:42.164667 139774434195200 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.962330937385559, loss=2.4482855796813965
I0201 23:53:51.897272 139936116377408 spec.py:321] Evaluating on the training split.
I0201 23:54:04.035991 139936116377408 spec.py:333] Evaluating on the validation split.
I0201 23:54:37.217674 139936116377408 spec.py:349] Evaluating on the test split.
I0201 23:54:38.816040 139936116377408 submission_runner.py:408] Time since start: 39210.67s, 	Step: 77223, 	{'train/accuracy': 0.6526171565055847, 'train/loss': 1.4243426322937012, 'validation/accuracy': 0.605459988117218, 'validation/loss': 1.6445677280426025, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.2839086055755615, 'test/num_examples': 10000, 'score': 35331.959324359894, 'total_duration': 39210.669605731964, 'accumulated_submission_time': 35331.959324359894, 'accumulated_eval_time': 3871.440359354019, 'accumulated_logging_time': 3.267023801803589}
I0201 23:54:38.845664 139774417409792 logging_writer.py:48] [77223] accumulated_eval_time=3871.440359, accumulated_logging_time=3.267024, accumulated_submission_time=35331.959324, global_step=77223, preemption_count=0, score=35331.959324, test/accuracy=0.490100, test/loss=2.283909, test/num_examples=10000, total_duration=39210.669606, train/accuracy=0.652617, train/loss=1.424343, validation/accuracy=0.605460, validation/loss=1.644568, validation/num_examples=50000
I0201 23:55:10.194703 139774434195200 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.9309728145599365, loss=2.5057759284973145
I0201 23:55:55.964672 139774417409792 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.8156299591064453, loss=2.6540608406066895
I0201 23:56:42.386662 139774434195200 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.9235576391220093, loss=2.4173662662506104
I0201 23:57:28.689401 139774417409792 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.8548669815063477, loss=2.891266107559204
I0201 23:58:15.022054 139774434195200 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.9835598468780518, loss=2.570195436477661
I0201 23:59:01.472666 139774417409792 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.600651502609253, loss=4.929035663604736
I0201 23:59:47.531082 139774434195200 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.727177381515503, loss=2.927105665206909
I0202 00:00:33.744821 139774417409792 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.9028804302215576, loss=2.3486337661743164
I0202 00:01:19.974081 139774434195200 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.057643175125122, loss=2.504028558731079
I0202 00:01:39.189922 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:01:51.272923 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:02:26.673903 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:02:28.293221 139936116377408 submission_runner.py:408] Time since start: 39680.15s, 	Step: 78143, 	{'train/accuracy': 0.6633203029632568, 'train/loss': 1.4052098989486694, 'validation/accuracy': 0.6102399826049805, 'validation/loss': 1.643385887145996, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.2711172103881836, 'test/num_examples': 10000, 'score': 35752.245290756226, 'total_duration': 39680.146780490875, 'accumulated_submission_time': 35752.245290756226, 'accumulated_eval_time': 3920.543641090393, 'accumulated_logging_time': 3.3071439266204834}
I0202 00:02:28.319562 139774417409792 logging_writer.py:48] [78143] accumulated_eval_time=3920.543641, accumulated_logging_time=3.307144, accumulated_submission_time=35752.245291, global_step=78143, preemption_count=0, score=35752.245291, test/accuracy=0.495700, test/loss=2.271117, test/num_examples=10000, total_duration=39680.146780, train/accuracy=0.663320, train/loss=1.405210, validation/accuracy=0.610240, validation/loss=1.643386, validation/num_examples=50000
I0202 00:02:51.323019 139774434195200 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.9903901815414429, loss=2.481625556945801
I0202 00:03:36.045580 139774417409792 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.6295244693756104, loss=4.07548713684082
I0202 00:04:22.342136 139774434195200 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.1651084423065186, loss=2.4273598194122314
I0202 00:05:08.633622 139774417409792 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.8979071378707886, loss=2.4650962352752686
I0202 00:05:54.740570 139774434195200 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.8284780979156494, loss=3.0857646465301514
I0202 00:06:40.936399 139774417409792 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.102525234222412, loss=2.430924892425537
I0202 00:07:27.012236 139774434195200 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.3926081657409668, loss=4.790650844573975
I0202 00:08:13.227319 139774417409792 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.774802327156067, loss=5.033973693847656
I0202 00:08:59.401139 139774434195200 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.8253263235092163, loss=3.9374613761901855
I0202 00:09:28.750139 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:09:40.787751 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:10:14.898101 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:10:16.496160 139936116377408 submission_runner.py:408] Time since start: 40148.35s, 	Step: 79065, 	{'train/accuracy': 0.6604882478713989, 'train/loss': 1.4064991474151611, 'validation/accuracy': 0.6160799860954285, 'validation/loss': 1.6271092891693115, 'validation/num_examples': 50000, 'test/accuracy': 0.4962000250816345, 'test/loss': 2.260464668273926, 'test/num_examples': 10000, 'score': 36172.618824243546, 'total_duration': 40148.349724531174, 'accumulated_submission_time': 36172.618824243546, 'accumulated_eval_time': 3968.2896530628204, 'accumulated_logging_time': 3.3433430194854736}
I0202 00:10:16.525057 139774417409792 logging_writer.py:48] [79065] accumulated_eval_time=3968.289653, accumulated_logging_time=3.343343, accumulated_submission_time=36172.618824, global_step=79065, preemption_count=0, score=36172.618824, test/accuracy=0.496200, test/loss=2.260465, test/num_examples=10000, total_duration=40148.349725, train/accuracy=0.660488, train/loss=1.406499, validation/accuracy=0.616080, validation/loss=1.627109, validation/num_examples=50000
I0202 00:10:30.802000 139774434195200 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.9651379585266113, loss=2.4188718795776367
I0202 00:11:14.464728 139774417409792 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.6139006614685059, loss=5.017481327056885
I0202 00:12:00.573690 139774434195200 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.9853276014328003, loss=2.5299482345581055
I0202 00:12:46.868158 139774417409792 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.8553773164749146, loss=2.392822504043579
I0202 00:13:32.848078 139774434195200 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.996949315071106, loss=2.400280714035034
I0202 00:14:19.386150 139774417409792 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.5706731081008911, loss=4.283210277557373
I0202 00:15:05.634099 139774434195200 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.6005202531814575, loss=4.9706130027771
I0202 00:15:51.619113 139774417409792 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.0084712505340576, loss=2.5483779907226562
I0202 00:16:38.028397 139774434195200 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.8922040462493896, loss=2.4359097480773926
I0202 00:17:16.654579 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:17:28.829228 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:18:04.448215 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:18:06.055983 139936116377408 submission_runner.py:408] Time since start: 40617.91s, 	Step: 79985, 	{'train/accuracy': 0.6540429592132568, 'train/loss': 1.444997787475586, 'validation/accuracy': 0.6112200021743774, 'validation/loss': 1.641564965248108, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.274275779724121, 'test/num_examples': 10000, 'score': 36592.68895483017, 'total_duration': 40617.90951514244, 'accumulated_submission_time': 36592.68895483017, 'accumulated_eval_time': 4017.691013813019, 'accumulated_logging_time': 3.384563684463501}
I0202 00:18:06.094261 139774417409792 logging_writer.py:48] [79985] accumulated_eval_time=4017.691014, accumulated_logging_time=3.384564, accumulated_submission_time=36592.688955, global_step=79985, preemption_count=0, score=36592.688955, test/accuracy=0.492300, test/loss=2.274276, test/num_examples=10000, total_duration=40617.909515, train/accuracy=0.654043, train/loss=1.444998, validation/accuracy=0.611220, validation/loss=1.641565, validation/num_examples=50000
I0202 00:18:12.454618 139774434195200 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.9894764423370361, loss=2.292036294937134
I0202 00:18:54.986623 139774417409792 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.856099247932434, loss=2.680084228515625
I0202 00:19:40.991682 139774434195200 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.7242605686187744, loss=3.3907580375671387
I0202 00:20:27.399282 139774417409792 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.8673796653747559, loss=2.437901020050049
I0202 00:21:13.894309 139774434195200 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.6749945878982544, loss=4.042154312133789
I0202 00:22:00.034119 139774417409792 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.7259198427200317, loss=4.803576946258545
I0202 00:22:46.219408 139774434195200 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.9426982402801514, loss=2.496284246444702
I0202 00:23:32.347510 139774417409792 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.0800890922546387, loss=2.5275816917419434
I0202 00:24:18.910597 139774434195200 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.9021660089492798, loss=3.793588161468506
I0202 00:25:04.826773 139774417409792 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.947754144668579, loss=2.391054153442383
I0202 00:25:06.401521 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:25:18.596054 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:25:53.184309 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:25:54.784689 139936116377408 submission_runner.py:408] Time since start: 41086.64s, 	Step: 80905, 	{'train/accuracy': 0.6605077981948853, 'train/loss': 1.3962000608444214, 'validation/accuracy': 0.6113799810409546, 'validation/loss': 1.6233501434326172, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.255664110183716, 'test/num_examples': 10000, 'score': 37012.93760061264, 'total_duration': 41086.63825082779, 'accumulated_submission_time': 37012.93760061264, 'accumulated_eval_time': 4066.0741584300995, 'accumulated_logging_time': 3.4339022636413574}
I0202 00:25:54.813174 139774434195200 logging_writer.py:48] [80905] accumulated_eval_time=4066.074158, accumulated_logging_time=3.433902, accumulated_submission_time=37012.937601, global_step=80905, preemption_count=0, score=37012.937601, test/accuracy=0.493700, test/loss=2.255664, test/num_examples=10000, total_duration=41086.638251, train/accuracy=0.660508, train/loss=1.396200, validation/accuracy=0.611380, validation/loss=1.623350, validation/num_examples=50000
I0202 00:26:34.434624 139774417409792 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.760256052017212, loss=2.8770556449890137
I0202 00:27:20.281918 139774434195200 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.030740261077881, loss=2.289799451828003
I0202 00:28:06.626427 139774417409792 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.590140461921692, loss=4.812927722930908
I0202 00:28:52.671280 139774434195200 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.130117416381836, loss=2.4486398696899414
I0202 00:29:39.061671 139774417409792 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.0291848182678223, loss=2.378519296646118
I0202 00:30:25.348315 139774434195200 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.076869010925293, loss=2.659972906112671
I0202 00:31:11.424351 139774417409792 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.8007036447525024, loss=4.387331962585449
I0202 00:31:57.684734 139774434195200 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.790838360786438, loss=4.981201648712158
I0202 00:32:43.860589 139774417409792 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.605064868927002, loss=4.949435710906982
I0202 00:32:55.010169 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:33:07.399106 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:33:41.843065 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:33:43.442992 139936116377408 submission_runner.py:408] Time since start: 41555.30s, 	Step: 81826, 	{'train/accuracy': 0.6890624761581421, 'train/loss': 1.2909141778945923, 'validation/accuracy': 0.6201599836349487, 'validation/loss': 1.59787118434906, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.2272136211395264, 'test/num_examples': 10000, 'score': 37433.077839136124, 'total_duration': 41555.29655838013, 'accumulated_submission_time': 37433.077839136124, 'accumulated_eval_time': 4114.506975889206, 'accumulated_logging_time': 3.4718174934387207}
I0202 00:33:43.472348 139774434195200 logging_writer.py:48] [81826] accumulated_eval_time=4114.506976, accumulated_logging_time=3.471817, accumulated_submission_time=37433.077839, global_step=81826, preemption_count=0, score=37433.077839, test/accuracy=0.503600, test/loss=2.227214, test/num_examples=10000, total_duration=41555.296558, train/accuracy=0.689062, train/loss=1.290914, validation/accuracy=0.620160, validation/loss=1.597871, validation/num_examples=50000
I0202 00:34:13.417293 139774417409792 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.1724853515625, loss=2.496074914932251
I0202 00:34:59.072974 139774434195200 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.9196501970291138, loss=4.394186973571777
I0202 00:35:45.181956 139774417409792 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.2145838737487793, loss=2.5132901668548584
I0202 00:36:31.407880 139774434195200 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.8231064081192017, loss=5.099064350128174
I0202 00:37:17.619153 139774417409792 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.8830509185791016, loss=2.532179594039917
I0202 00:38:03.999767 139774434195200 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.094750165939331, loss=2.3656857013702393
I0202 00:38:50.180940 139774417409792 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.6449096202850342, loss=3.3421905040740967
I0202 00:39:36.843019 139774434195200 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.6085861921310425, loss=4.610766410827637
I0202 00:40:23.379320 139774417409792 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.878588318824768, loss=2.311030149459839
I0202 00:40:43.854700 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:40:55.971837 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:41:24.960407 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:41:26.581999 139936116377408 submission_runner.py:408] Time since start: 42018.44s, 	Step: 82746, 	{'train/accuracy': 0.6591405868530273, 'train/loss': 1.4403164386749268, 'validation/accuracy': 0.6159799695014954, 'validation/loss': 1.641635537147522, 'validation/num_examples': 50000, 'test/accuracy': 0.4896000325679779, 'test/loss': 2.2967171669006348, 'test/num_examples': 10000, 'score': 37853.40079331398, 'total_duration': 42018.43554711342, 'accumulated_submission_time': 37853.40079331398, 'accumulated_eval_time': 4157.234240293503, 'accumulated_logging_time': 3.51223087310791}
I0202 00:41:26.618000 139774434195200 logging_writer.py:48] [82746] accumulated_eval_time=4157.234240, accumulated_logging_time=3.512231, accumulated_submission_time=37853.400793, global_step=82746, preemption_count=0, score=37853.400793, test/accuracy=0.489600, test/loss=2.296717, test/num_examples=10000, total_duration=42018.435547, train/accuracy=0.659141, train/loss=1.440316, validation/accuracy=0.615980, validation/loss=1.641636, validation/num_examples=50000
I0202 00:41:48.427711 139774417409792 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.203626871109009, loss=2.521921396255493
I0202 00:42:33.408309 139774434195200 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.8466987609863281, loss=2.3452208042144775
I0202 00:43:19.513220 139774417409792 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.7728173732757568, loss=3.706733465194702
I0202 00:44:06.036870 139774434195200 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1690685749053955, loss=2.334838390350342
I0202 00:44:52.150853 139774417409792 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.084855556488037, loss=2.6530990600585938
I0202 00:45:38.545502 139774434195200 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.741589903831482, loss=4.839778423309326
I0202 00:46:24.664198 139774417409792 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1090314388275146, loss=4.980968475341797
I0202 00:47:10.974799 139774434195200 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.031139850616455, loss=2.3242363929748535
I0202 00:47:57.019978 139774417409792 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.682046890258789, loss=3.3599536418914795
I0202 00:48:26.973902 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:48:39.054186 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:49:14.637360 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:49:16.243968 139936116377408 submission_runner.py:408] Time since start: 42488.10s, 	Step: 83666, 	{'train/accuracy': 0.663769543170929, 'train/loss': 1.3689274787902832, 'validation/accuracy': 0.616320013999939, 'validation/loss': 1.5968698263168335, 'validation/num_examples': 50000, 'test/accuracy': 0.49890002608299255, 'test/loss': 2.235353469848633, 'test/num_examples': 10000, 'score': 38273.69846391678, 'total_duration': 42488.097521305084, 'accumulated_submission_time': 38273.69846391678, 'accumulated_eval_time': 4206.504290103912, 'accumulated_logging_time': 3.5590643882751465}
I0202 00:49:16.274530 139774434195200 logging_writer.py:48] [83666] accumulated_eval_time=4206.504290, accumulated_logging_time=3.559064, accumulated_submission_time=38273.698464, global_step=83666, preemption_count=0, score=38273.698464, test/accuracy=0.498900, test/loss=2.235353, test/num_examples=10000, total_duration=42488.097521, train/accuracy=0.663770, train/loss=1.368927, validation/accuracy=0.616320, validation/loss=1.596870, validation/num_examples=50000
I0202 00:49:30.162084 139774417409792 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.5922393798828125, loss=4.430963516235352
I0202 00:50:13.845745 139774434195200 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.943347692489624, loss=2.7430479526519775
I0202 00:50:59.971866 139774417409792 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.2922093868255615, loss=2.41848087310791
I0202 00:51:46.402713 139774434195200 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.9102345705032349, loss=2.872466564178467
I0202 00:52:32.693724 139774417409792 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.0988383293151855, loss=2.410970449447632
I0202 00:53:18.850345 139774434195200 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.0483591556549072, loss=2.4060750007629395
I0202 00:54:04.969672 139774417409792 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.7611653804779053, loss=3.690326452255249
I0202 00:54:50.886215 139774434195200 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.2206385135650635, loss=2.4356844425201416
I0202 00:55:37.023215 139774417409792 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.9471102952957153, loss=2.6067042350769043
I0202 00:56:16.492192 139936116377408 spec.py:321] Evaluating on the training split.
I0202 00:56:28.413548 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 00:57:02.271966 139936116377408 spec.py:349] Evaluating on the test split.
I0202 00:57:03.876984 139936116377408 submission_runner.py:408] Time since start: 42955.73s, 	Step: 84587, 	{'train/accuracy': 0.6851366758346558, 'train/loss': 1.2849160432815552, 'validation/accuracy': 0.6199399828910828, 'validation/loss': 1.5707097053527832, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.218472480773926, 'test/num_examples': 10000, 'score': 38693.85767745972, 'total_duration': 42955.73055052757, 'accumulated_submission_time': 38693.85767745972, 'accumulated_eval_time': 4253.889084339142, 'accumulated_logging_time': 3.6006920337677}
I0202 00:57:03.908981 139774434195200 logging_writer.py:48] [84587] accumulated_eval_time=4253.889084, accumulated_logging_time=3.600692, accumulated_submission_time=38693.857677, global_step=84587, preemption_count=0, score=38693.857677, test/accuracy=0.497400, test/loss=2.218472, test/num_examples=10000, total_duration=42955.730551, train/accuracy=0.685137, train/loss=1.284916, validation/accuracy=0.619940, validation/loss=1.570710, validation/num_examples=50000
I0202 00:57:09.470200 139774417409792 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.827324628829956, loss=2.700732707977295
I0202 00:57:51.648687 139774434195200 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.225342035293579, loss=2.41593599319458
I0202 00:58:37.750270 139774417409792 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.010803699493408, loss=2.3940484523773193
I0202 00:59:24.082963 139774434195200 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.8387982845306396, loss=4.923286437988281
I0202 01:00:10.473103 139774417409792 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.115783452987671, loss=2.778843402862549
I0202 01:00:56.993469 139774434195200 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.100675106048584, loss=2.4471497535705566
I0202 01:01:43.562946 139774417409792 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.2848868370056152, loss=2.3676438331604004
I0202 01:02:29.588337 139774434195200 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.8574788570404053, loss=4.930570602416992
I0202 01:03:15.767858 139774417409792 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.012831687927246, loss=2.367748975753784
I0202 01:04:01.784824 139774434195200 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.954949975013733, loss=2.546003818511963
I0202 01:04:04.303709 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:04:16.440161 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:04:50.341788 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:04:51.943803 139936116377408 submission_runner.py:408] Time since start: 43423.80s, 	Step: 85507, 	{'train/accuracy': 0.6639453172683716, 'train/loss': 1.3985720872879028, 'validation/accuracy': 0.6188200116157532, 'validation/loss': 1.608765721321106, 'validation/num_examples': 50000, 'test/accuracy': 0.4985000193119049, 'test/loss': 2.248077869415283, 'test/num_examples': 10000, 'score': 39114.19425010681, 'total_duration': 43423.79737305641, 'accumulated_submission_time': 39114.19425010681, 'accumulated_eval_time': 4301.529177188873, 'accumulated_logging_time': 3.6426284313201904}
I0202 01:04:51.970847 139774417409792 logging_writer.py:48] [85507] accumulated_eval_time=4301.529177, accumulated_logging_time=3.642628, accumulated_submission_time=39114.194250, global_step=85507, preemption_count=0, score=39114.194250, test/accuracy=0.498500, test/loss=2.248078, test/num_examples=10000, total_duration=43423.797373, train/accuracy=0.663945, train/loss=1.398572, validation/accuracy=0.618820, validation/loss=1.608766, validation/num_examples=50000
I0202 01:05:30.327867 139774434195200 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.01782488822937, loss=2.450117349624634
I0202 01:06:16.274086 139774417409792 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.0126900672912598, loss=2.3756678104400635
I0202 01:07:02.799283 139774434195200 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.926095724105835, loss=2.7405431270599365
I0202 01:07:48.861700 139774417409792 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.9557894468307495, loss=2.4222757816314697
I0202 01:08:34.972182 139774434195200 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.9281251430511475, loss=2.474079132080078
I0202 01:09:21.326377 139774417409792 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.7603226900100708, loss=2.7046430110931396
I0202 01:10:07.556153 139774434195200 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.9492056369781494, loss=4.842456817626953
I0202 01:10:53.649199 139774417409792 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.2583606243133545, loss=4.469352722167969
I0202 01:11:40.528706 139774434195200 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.7729178667068481, loss=4.834999084472656
I0202 01:11:52.218940 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:12:04.726737 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:12:38.873315 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:12:40.480958 139936116377408 submission_runner.py:408] Time since start: 43892.33s, 	Step: 86427, 	{'train/accuracy': 0.673535168170929, 'train/loss': 1.353287935256958, 'validation/accuracy': 0.6249600052833557, 'validation/loss': 1.5730277299880981, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.218822717666626, 'test/num_examples': 10000, 'score': 39534.38470721245, 'total_duration': 43892.334481716156, 'accumulated_submission_time': 39534.38470721245, 'accumulated_eval_time': 4349.79114151001, 'accumulated_logging_time': 3.6794145107269287}
I0202 01:12:40.512317 139774417409792 logging_writer.py:48] [86427] accumulated_eval_time=4349.791142, accumulated_logging_time=3.679415, accumulated_submission_time=39534.384707, global_step=86427, preemption_count=0, score=39534.384707, test/accuracy=0.503400, test/loss=2.218823, test/num_examples=10000, total_duration=43892.334482, train/accuracy=0.673535, train/loss=1.353288, validation/accuracy=0.624960, validation/loss=1.573028, validation/num_examples=50000
I0202 01:13:10.078418 139774434195200 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.8737808465957642, loss=2.3054544925689697
I0202 01:13:55.926225 139774417409792 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.7528599500656128, loss=3.527758836746216
I0202 01:14:42.434497 139774434195200 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.9605872631072998, loss=2.3791940212249756
I0202 01:15:29.023024 139774417409792 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.7834746837615967, loss=2.9562578201293945
I0202 01:16:15.205245 139774434195200 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.8091979026794434, loss=3.3399758338928223
I0202 01:17:01.580717 139774417409792 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.0573673248291016, loss=2.575240135192871
I0202 01:17:47.821885 139774434195200 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.9351118803024292, loss=2.4175877571105957
I0202 01:18:34.178293 139774417409792 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.05299711227417, loss=2.3430070877075195
I0202 01:19:20.541791 139774434195200 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.241990089416504, loss=2.7712855339050293
I0202 01:19:40.501844 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:19:52.765666 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:20:28.033973 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:20:29.640220 139936116377408 submission_runner.py:408] Time since start: 44361.49s, 	Step: 87345, 	{'train/accuracy': 0.6821679472923279, 'train/loss': 1.2937649488449097, 'validation/accuracy': 0.6266599893569946, 'validation/loss': 1.55876624584198, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.2032463550567627, 'test/num_examples': 10000, 'score': 39954.312836647034, 'total_duration': 44361.49378180504, 'accumulated_submission_time': 39954.312836647034, 'accumulated_eval_time': 4398.92951130867, 'accumulated_logging_time': 3.7220330238342285}
I0202 01:20:29.668580 139774417409792 logging_writer.py:48] [87345] accumulated_eval_time=4398.929511, accumulated_logging_time=3.722033, accumulated_submission_time=39954.312837, global_step=87345, preemption_count=0, score=39954.312837, test/accuracy=0.506200, test/loss=2.203246, test/num_examples=10000, total_duration=44361.493782, train/accuracy=0.682168, train/loss=1.293765, validation/accuracy=0.626660, validation/loss=1.558766, validation/num_examples=50000
I0202 01:20:51.890880 139774434195200 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.0779993534088135, loss=2.3652777671813965
I0202 01:21:36.825403 139774417409792 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.2184407711029053, loss=2.433572769165039
I0202 01:22:23.834658 139774434195200 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.7453114986419678, loss=3.791472911834717
I0202 01:23:10.271952 139774417409792 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.7999669313430786, loss=3.7311813831329346
I0202 01:23:56.563272 139774434195200 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.6509047746658325, loss=4.086989402770996
I0202 01:24:42.807421 139774417409792 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.0503346920013428, loss=2.3833138942718506
I0202 01:25:29.124284 139774434195200 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.895786166191101, loss=4.044020652770996
I0202 01:26:15.500820 139774417409792 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.06616473197937, loss=2.4160382747650146
I0202 01:27:01.831397 139774434195200 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.730502963066101, loss=4.569305419921875
I0202 01:27:29.787334 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:27:41.873551 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:28:16.207006 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:28:17.815956 139936116377408 submission_runner.py:408] Time since start: 44829.67s, 	Step: 88262, 	{'train/accuracy': 0.6767578125, 'train/loss': 1.3330546617507935, 'validation/accuracy': 0.6308199763298035, 'validation/loss': 1.5420633554458618, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.181927442550659, 'test/num_examples': 10000, 'score': 40374.37486696243, 'total_duration': 44829.66952109337, 'accumulated_submission_time': 40374.37486696243, 'accumulated_eval_time': 4446.9581387043, 'accumulated_logging_time': 3.759573459625244}
I0202 01:28:17.847565 139774417409792 logging_writer.py:48] [88262] accumulated_eval_time=4446.958139, accumulated_logging_time=3.759573, accumulated_submission_time=40374.374867, global_step=88262, preemption_count=0, score=40374.374867, test/accuracy=0.510200, test/loss=2.181927, test/num_examples=10000, total_duration=44829.669521, train/accuracy=0.676758, train/loss=1.333055, validation/accuracy=0.630820, validation/loss=1.542063, validation/num_examples=50000
I0202 01:28:33.322532 139774434195200 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.5332586765289307, loss=2.3743913173675537
I0202 01:29:17.076644 139774417409792 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.988277792930603, loss=4.774301528930664
I0202 01:30:03.378993 139774434195200 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.7412768602371216, loss=3.529978036880493
I0202 01:30:50.058939 139774417409792 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.019740343093872, loss=2.38883900642395
I0202 01:31:36.251215 139774434195200 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.2647523880004883, loss=2.360283136367798
I0202 01:32:22.840888 139774417409792 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.9383599758148193, loss=2.56111741065979
I0202 01:33:09.473641 139774434195200 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.8487355709075928, loss=2.8271732330322266
I0202 01:33:55.417833 139774417409792 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.085646390914917, loss=2.2823286056518555
I0202 01:34:41.546425 139774434195200 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.1389760971069336, loss=2.512465476989746
I0202 01:35:18.228378 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:35:30.232358 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:36:05.012120 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:36:06.613069 139936116377408 submission_runner.py:408] Time since start: 45298.47s, 	Step: 89181, 	{'train/accuracy': 0.6698632836341858, 'train/loss': 1.3340215682983398, 'validation/accuracy': 0.6282399892807007, 'validation/loss': 1.5469225645065308, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.215367317199707, 'test/num_examples': 10000, 'score': 40794.69830536842, 'total_duration': 45298.46662902832, 'accumulated_submission_time': 40794.69830536842, 'accumulated_eval_time': 4495.342826843262, 'accumulated_logging_time': 3.800304889678955}
I0202 01:36:06.646180 139774417409792 logging_writer.py:48] [89181] accumulated_eval_time=4495.342827, accumulated_logging_time=3.800305, accumulated_submission_time=40794.698305, global_step=89181, preemption_count=0, score=40794.698305, test/accuracy=0.509600, test/loss=2.215367, test/num_examples=10000, total_duration=45298.466629, train/accuracy=0.669863, train/loss=1.334022, validation/accuracy=0.628240, validation/loss=1.546923, validation/num_examples=50000
I0202 01:36:14.572561 139774434195200 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.9073481559753418, loss=2.7056288719177246
I0202 01:36:57.180480 139774417409792 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.8168970346450806, loss=2.8438093662261963
I0202 01:37:43.230967 139774434195200 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.6435686349868774, loss=4.333923816680908
I0202 01:38:29.549319 139774417409792 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.706084132194519, loss=3.58394193649292
I0202 01:39:16.041279 139774434195200 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.8945424556732178, loss=2.4306159019470215
I0202 01:40:02.319703 139774417409792 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.0837883949279785, loss=2.801988124847412
I0202 01:40:48.584342 139774434195200 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.9336904287338257, loss=4.76290225982666
I0202 01:41:34.925046 139774417409792 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.117091178894043, loss=2.2420237064361572
I0202 01:42:21.427492 139774434195200 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.1573002338409424, loss=2.3139023780822754
I0202 01:43:06.666609 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:43:18.667273 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:43:50.329712 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:43:51.932656 139936116377408 submission_runner.py:408] Time since start: 45763.79s, 	Step: 90099, 	{'train/accuracy': 0.68359375, 'train/loss': 1.305981159210205, 'validation/accuracy': 0.6265999674797058, 'validation/loss': 1.5628786087036133, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2054131031036377, 'test/num_examples': 10000, 'score': 41214.659896850586, 'total_duration': 45763.786215782166, 'accumulated_submission_time': 41214.659896850586, 'accumulated_eval_time': 4540.608873128891, 'accumulated_logging_time': 3.844949960708618}
I0202 01:43:51.969374 139774417409792 logging_writer.py:48] [90099] accumulated_eval_time=4540.608873, accumulated_logging_time=3.844950, accumulated_submission_time=41214.659897, global_step=90099, preemption_count=0, score=41214.659897, test/accuracy=0.506300, test/loss=2.205413, test/num_examples=10000, total_duration=45763.786216, train/accuracy=0.683594, train/loss=1.305981, validation/accuracy=0.626600, validation/loss=1.562879, validation/num_examples=50000
I0202 01:43:52.765120 139774434195200 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6325645446777344, loss=4.924952507019043
I0202 01:44:34.125044 139774417409792 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.7517551183700562, loss=3.158681869506836
I0202 01:45:20.309362 139774434195200 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.8117610216140747, loss=3.0712931156158447
I0202 01:46:07.276320 139774417409792 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.3169949054718018, loss=2.282571792602539
I0202 01:46:53.484886 139774434195200 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.2934772968292236, loss=2.383455276489258
I0202 01:47:39.672720 139774417409792 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.7405369281768799, loss=3.8687024116516113
I0202 01:48:26.133458 139774434195200 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.0005040168762207, loss=2.2490053176879883
I0202 01:49:12.593079 139774417409792 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.809482216835022, loss=2.915501594543457
I0202 01:49:58.767215 139774434195200 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.086299419403076, loss=2.556591033935547
I0202 01:50:44.967715 139774417409792 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.0014443397521973, loss=3.3035080432891846
I0202 01:50:52.373084 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:51:04.802210 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:51:39.433593 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:51:41.044473 139936116377408 submission_runner.py:408] Time since start: 46232.90s, 	Step: 91018, 	{'train/accuracy': 0.6847070455551147, 'train/loss': 1.3044037818908691, 'validation/accuracy': 0.6290199756622314, 'validation/loss': 1.5476787090301514, 'validation/num_examples': 50000, 'test/accuracy': 0.5110000371932983, 'test/loss': 2.183170795440674, 'test/num_examples': 10000, 'score': 41635.006526470184, 'total_duration': 46232.89801955223, 'accumulated_submission_time': 41635.006526470184, 'accumulated_eval_time': 4589.280221223831, 'accumulated_logging_time': 3.8913192749023438}
I0202 01:51:41.077721 139774434195200 logging_writer.py:48] [91018] accumulated_eval_time=4589.280221, accumulated_logging_time=3.891319, accumulated_submission_time=41635.006526, global_step=91018, preemption_count=0, score=41635.006526, test/accuracy=0.511000, test/loss=2.183171, test/num_examples=10000, total_duration=46232.898020, train/accuracy=0.684707, train/loss=1.304404, validation/accuracy=0.629020, validation/loss=1.547679, validation/num_examples=50000
I0202 01:52:14.905370 139774417409792 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.4054665565490723, loss=2.414825439453125
I0202 01:53:00.710661 139774434195200 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.033501386642456, loss=2.2285728454589844
I0202 01:53:47.324352 139774417409792 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.177292823791504, loss=2.814821720123291
I0202 01:54:33.694260 139774434195200 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.049373149871826, loss=2.2813315391540527
I0202 01:55:20.064529 139774417409792 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.22529935836792, loss=4.712998867034912
I0202 01:56:06.706664 139774434195200 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.048114061355591, loss=2.320726156234741
I0202 01:56:52.828790 139774417409792 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.1856062412261963, loss=2.304753303527832
I0202 01:57:39.257472 139774434195200 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.5306565761566162, loss=4.6224164962768555
I0202 01:58:25.555643 139774417409792 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.9236654043197632, loss=2.6528806686401367
I0202 01:58:41.291672 139936116377408 spec.py:321] Evaluating on the training split.
I0202 01:58:53.222633 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 01:59:26.725431 139936116377408 spec.py:349] Evaluating on the test split.
I0202 01:59:28.335844 139936116377408 submission_runner.py:408] Time since start: 46700.19s, 	Step: 91936, 	{'train/accuracy': 0.6799609065055847, 'train/loss': 1.3173344135284424, 'validation/accuracy': 0.6291599869728088, 'validation/loss': 1.534510850906372, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.1739141941070557, 'test/num_examples': 10000, 'score': 42055.1633245945, 'total_duration': 46700.18941116333, 'accumulated_submission_time': 42055.1633245945, 'accumulated_eval_time': 4636.324400186539, 'accumulated_logging_time': 3.9344964027404785}
I0202 01:59:28.364436 139774434195200 logging_writer.py:48] [91936] accumulated_eval_time=4636.324400, accumulated_logging_time=3.934496, accumulated_submission_time=42055.163325, global_step=91936, preemption_count=0, score=42055.163325, test/accuracy=0.509300, test/loss=2.173914, test/num_examples=10000, total_duration=46700.189411, train/accuracy=0.679961, train/loss=1.317334, validation/accuracy=0.629160, validation/loss=1.534511, validation/num_examples=50000
I0202 01:59:54.152238 139774417409792 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.057192325592041, loss=2.476261615753174
I0202 02:00:39.275582 139774434195200 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.9480056762695312, loss=4.622311115264893
I0202 02:01:25.496192 139774417409792 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.217956781387329, loss=2.371058464050293
I0202 02:02:11.804483 139774434195200 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.8745099306106567, loss=4.8338398933410645
I0202 02:02:57.844727 139774417409792 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.7721879482269287, loss=4.171550273895264
I0202 02:03:43.932815 139774434195200 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.8195561170578003, loss=3.7528555393218994
I0202 02:04:30.485115 139774417409792 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.0808937549591064, loss=2.3635339736938477
I0202 02:05:16.430910 139774434195200 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.7668142318725586, loss=4.514172554016113
I0202 02:06:02.604053 139774417409792 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.3870599269866943, loss=2.4557642936706543
I0202 02:06:28.449680 139936116377408 spec.py:321] Evaluating on the training split.
I0202 02:06:40.387538 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 02:07:15.851326 139936116377408 spec.py:349] Evaluating on the test split.
I0202 02:07:17.465777 139936116377408 submission_runner.py:408] Time since start: 47169.32s, 	Step: 92858, 	{'train/accuracy': 0.6845703125, 'train/loss': 1.30815589427948, 'validation/accuracy': 0.6358000040054321, 'validation/loss': 1.5391371250152588, 'validation/num_examples': 50000, 'test/accuracy': 0.5106000304222107, 'test/loss': 2.1962108612060547, 'test/num_examples': 10000, 'score': 42475.191940784454, 'total_duration': 47169.319345235825, 'accumulated_submission_time': 42475.191940784454, 'accumulated_eval_time': 4685.3404994010925, 'accumulated_logging_time': 3.972641944885254}
I0202 02:07:17.494178 139774434195200 logging_writer.py:48] [92858] accumulated_eval_time=4685.340499, accumulated_logging_time=3.972642, accumulated_submission_time=42475.191941, global_step=92858, preemption_count=0, score=42475.191941, test/accuracy=0.510600, test/loss=2.196211, test/num_examples=10000, total_duration=47169.319345, train/accuracy=0.684570, train/loss=1.308156, validation/accuracy=0.635800, validation/loss=1.539137, validation/num_examples=50000
I0202 02:07:34.550255 139774417409792 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.9601051807403564, loss=3.6181695461273193
I0202 02:08:18.548946 139774434195200 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.8527240753173828, loss=3.244083881378174
I0202 02:09:04.566317 139774417409792 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.1960177421569824, loss=2.382192611694336
I0202 02:09:50.826728 139774434195200 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.060372829437256, loss=3.2733817100524902
I0202 02:10:37.090326 139774417409792 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.0998666286468506, loss=2.38228702545166
I0202 02:11:23.237670 139774434195200 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.778464913368225, loss=3.089533805847168
I0202 02:12:09.506226 139774417409792 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.095285177230835, loss=3.53974986076355
I0202 02:12:55.728432 139774434195200 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.9497294425964355, loss=2.650552749633789
I0202 02:13:41.961935 139774417409792 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.0691847801208496, loss=2.4336202144622803
I0202 02:14:17.932650 139936116377408 spec.py:321] Evaluating on the training split.
I0202 02:14:30.036268 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 02:15:02.796367 139936116377408 spec.py:349] Evaluating on the test split.
I0202 02:15:04.407496 139936116377408 submission_runner.py:408] Time since start: 47636.26s, 	Step: 93780, 	{'train/accuracy': 0.7090038657188416, 'train/loss': 1.1813215017318726, 'validation/accuracy': 0.6354999542236328, 'validation/loss': 1.5112653970718384, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.147897481918335, 'test/num_examples': 10000, 'score': 42895.57374858856, 'total_duration': 47636.26103925705, 'accumulated_submission_time': 42895.57374858856, 'accumulated_eval_time': 4731.815329551697, 'accumulated_logging_time': 4.010065317153931}
I0202 02:15:04.442360 139774434195200 logging_writer.py:48] [93780] accumulated_eval_time=4731.815330, accumulated_logging_time=4.010065, accumulated_submission_time=42895.573749, global_step=93780, preemption_count=0, score=42895.573749, test/accuracy=0.510300, test/loss=2.147897, test/num_examples=10000, total_duration=47636.261039, train/accuracy=0.709004, train/loss=1.181322, validation/accuracy=0.635500, validation/loss=1.511265, validation/num_examples=50000
I0202 02:15:12.780677 139774417409792 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.053135633468628, loss=2.3570590019226074
I0202 02:15:55.324444 139774434195200 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.7816942930221558, loss=3.936652183532715
I0202 02:16:41.303602 139774417409792 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.2905635833740234, loss=2.2420637607574463
I0202 02:17:27.827238 139774434195200 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.9459333419799805, loss=4.781908988952637
I0202 02:18:13.827926 139774417409792 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.798632264137268, loss=4.4355149269104
I0202 02:19:00.112874 139774434195200 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.7943345308303833, loss=4.615100860595703
I0202 02:19:46.433928 139774417409792 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.9661461114883423, loss=2.554089069366455
I0202 02:20:32.909691 139774434195200 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.1542372703552246, loss=2.8069915771484375
I0202 02:21:19.281768 139774417409792 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.331770181655884, loss=2.3687713146209717
I0202 02:22:04.555147 139936116377408 spec.py:321] Evaluating on the training split.
I0202 02:22:16.700017 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 02:22:51.770675 139936116377408 spec.py:349] Evaluating on the test split.
I0202 02:22:53.372927 139936116377408 submission_runner.py:408] Time since start: 48105.23s, 	Step: 94699, 	{'train/accuracy': 0.6853711009025574, 'train/loss': 1.2983242273330688, 'validation/accuracy': 0.634939968585968, 'validation/loss': 1.5220338106155396, 'validation/num_examples': 50000, 'test/accuracy': 0.516700029373169, 'test/loss': 2.158723831176758, 'test/num_examples': 10000, 'score': 43315.627083063126, 'total_duration': 48105.22648501396, 'accumulated_submission_time': 43315.627083063126, 'accumulated_eval_time': 4780.6331214904785, 'accumulated_logging_time': 4.056293964385986}
I0202 02:22:53.402981 139774434195200 logging_writer.py:48] [94699] accumulated_eval_time=4780.633121, accumulated_logging_time=4.056294, accumulated_submission_time=43315.627083, global_step=94699, preemption_count=0, score=43315.627083, test/accuracy=0.516700, test/loss=2.158724, test/num_examples=10000, total_duration=48105.226485, train/accuracy=0.685371, train/loss=1.298324, validation/accuracy=0.634940, validation/loss=1.522034, validation/num_examples=50000
I0202 02:22:54.200814 139774417409792 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.9244170188903809, loss=2.67549991607666
I0202 02:23:35.492504 139774434195200 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.04547381401062, loss=2.609596014022827
I0202 02:24:21.574397 139774417409792 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.0681469440460205, loss=4.848771095275879
I0202 02:25:08.101205 139774434195200 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.105114698410034, loss=3.952631711959839
I0202 02:25:54.345304 139774417409792 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.9568848609924316, loss=2.219249963760376
I0202 02:26:40.766600 139774434195200 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.310238838195801, loss=2.3306610584259033
I0202 02:27:27.029414 139774417409792 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.3518028259277344, loss=2.319596290588379
I0202 02:28:13.252378 139774434195200 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.3191871643066406, loss=4.508240222930908
I0202 02:28:59.322631 139774417409792 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.96614408493042, loss=4.839022636413574
I0202 02:29:45.541699 139774434195200 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.092728614807129, loss=2.452756643295288
I0202 02:29:53.436257 139936116377408 spec.py:321] Evaluating on the training split.
I0202 02:30:06.474901 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 02:30:42.201825 139936116377408 spec.py:349] Evaluating on the test split.
I0202 02:30:43.802097 139936116377408 submission_runner.py:408] Time since start: 48575.66s, 	Step: 95619, 	{'train/accuracy': 0.6918359398841858, 'train/loss': 1.3312885761260986, 'validation/accuracy': 0.6355999708175659, 'validation/loss': 1.5759185552597046, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.203728199005127, 'test/num_examples': 10000, 'score': 43735.60219955444, 'total_duration': 48575.65565729141, 'accumulated_submission_time': 43735.60219955444, 'accumulated_eval_time': 4830.998948812485, 'accumulated_logging_time': 4.0974836349487305}
I0202 02:30:43.831753 139774417409792 logging_writer.py:48] [95619] accumulated_eval_time=4830.998949, accumulated_logging_time=4.097484, accumulated_submission_time=43735.602200, global_step=95619, preemption_count=0, score=43735.602200, test/accuracy=0.518200, test/loss=2.203728, test/num_examples=10000, total_duration=48575.655657, train/accuracy=0.691836, train/loss=1.331289, validation/accuracy=0.635600, validation/loss=1.575919, validation/num_examples=50000
I0202 02:31:16.985779 139774434195200 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.8707311153411865, loss=3.2904443740844727
I0202 02:32:02.981659 139774417409792 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.860412359237671, loss=4.624578475952148
I0202 02:32:48.841140 139774434195200 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.0903422832489014, loss=2.6446990966796875
I0202 02:33:35.161844 139774417409792 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.1872973442077637, loss=2.2727179527282715
I0202 02:34:21.200284 139774434195200 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.905597448348999, loss=4.834676265716553
I0202 02:35:07.361611 139774417409792 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.9746099710464478, loss=2.7986154556274414
I0202 02:35:53.463627 139774434195200 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.0566070079803467, loss=2.268996477127075
I0202 02:36:39.854187 139774417409792 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.8381681442260742, loss=4.85543155670166
I0202 02:37:26.120336 139774434195200 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.2091970443725586, loss=2.261967658996582
I0202 02:37:44.220583 139936116377408 spec.py:321] Evaluating on the training split.
I0202 02:37:56.415051 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 02:38:28.028453 139936116377408 spec.py:349] Evaluating on the test split.
I0202 02:38:29.635539 139936116377408 submission_runner.py:408] Time since start: 49041.49s, 	Step: 96541, 	{'train/accuracy': 0.6978124976158142, 'train/loss': 1.2508113384246826, 'validation/accuracy': 0.6395999789237976, 'validation/loss': 1.5194545984268188, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.156029462814331, 'test/num_examples': 10000, 'score': 44155.92951416969, 'total_duration': 49041.48910880089, 'accumulated_submission_time': 44155.92951416969, 'accumulated_eval_time': 4876.413905143738, 'accumulated_logging_time': 4.138431787490845}
I0202 02:38:29.668898 139774417409792 logging_writer.py:48] [96541] accumulated_eval_time=4876.413905, accumulated_logging_time=4.138432, accumulated_submission_time=44155.929514, global_step=96541, preemption_count=0, score=44155.929514, test/accuracy=0.520500, test/loss=2.156029, test/num_examples=10000, total_duration=49041.489109, train/accuracy=0.697812, train/loss=1.250811, validation/accuracy=0.639600, validation/loss=1.519455, validation/num_examples=50000
I0202 02:38:53.487831 139774434195200 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.451807737350464, loss=2.2241225242614746
I0202 02:39:38.641090 139774417409792 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.243220090866089, loss=2.1864449977874756
I0202 02:40:25.072054 139774434195200 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.342097520828247, loss=2.326629877090454
I0202 02:41:11.525113 139774417409792 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.2221641540527344, loss=2.3967299461364746
I0202 02:41:57.531299 139774434195200 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.9259835481643677, loss=2.5907278060913086
I0202 02:42:43.744909 139774417409792 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.9768871068954468, loss=4.681796073913574
I0202 02:43:29.713468 139774434195200 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.8100639581680298, loss=3.1577630043029785
I0202 02:44:15.981166 139774417409792 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.8449550867080688, loss=4.353782653808594
I0202 02:45:02.343403 139774434195200 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.8198496103286743, loss=4.8221588134765625
I0202 02:45:29.728413 139936116377408 spec.py:321] Evaluating on the training split.
I0202 02:45:41.771958 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 02:46:15.554298 139936116377408 spec.py:349] Evaluating on the test split.
I0202 02:46:17.157697 139936116377408 submission_runner.py:408] Time since start: 49509.01s, 	Step: 97461, 	{'train/accuracy': 0.6885156035423279, 'train/loss': 1.2628841400146484, 'validation/accuracy': 0.642579972743988, 'validation/loss': 1.4694026708602905, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.106327772140503, 'test/num_examples': 10000, 'score': 44575.93083691597, 'total_duration': 49509.01126098633, 'accumulated_submission_time': 44575.93083691597, 'accumulated_eval_time': 4923.843191146851, 'accumulated_logging_time': 4.1819233894348145}
I0202 02:46:17.191410 139774417409792 logging_writer.py:48] [97461] accumulated_eval_time=4923.843191, accumulated_logging_time=4.181923, accumulated_submission_time=44575.930837, global_step=97461, preemption_count=0, score=44575.930837, test/accuracy=0.525500, test/loss=2.106328, test/num_examples=10000, total_duration=49509.011261, train/accuracy=0.688516, train/loss=1.262884, validation/accuracy=0.642580, validation/loss=1.469403, validation/num_examples=50000
I0202 02:46:33.059072 139774434195200 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.0571467876434326, loss=2.6378865242004395
I0202 02:47:17.186645 139774417409792 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.9583685398101807, loss=2.4810585975646973
I0202 02:48:03.611196 139774434195200 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.1493959426879883, loss=2.7161498069763184
I0202 02:48:49.841806 139774417409792 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.9072034358978271, loss=3.799499034881592
I0202 02:49:35.913160 139774434195200 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.6697497367858887, loss=2.253953218460083
I0202 02:50:22.400870 139774417409792 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.3261425495147705, loss=2.2598423957824707
I0202 02:51:08.586257 139774434195200 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.094538688659668, loss=2.4943535327911377
I0202 02:51:54.716844 139774417409792 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.4071667194366455, loss=2.254840850830078
I0202 02:52:40.706288 139774434195200 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.1776347160339355, loss=2.456395387649536
I0202 02:53:17.263684 139936116377408 spec.py:321] Evaluating on the training split.
I0202 02:53:29.345694 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 02:54:03.606947 139936116377408 spec.py:349] Evaluating on the test split.
I0202 02:54:05.209933 139936116377408 submission_runner.py:408] Time since start: 49977.06s, 	Step: 98381, 	{'train/accuracy': 0.6943945288658142, 'train/loss': 1.2435778379440308, 'validation/accuracy': 0.6418399810791016, 'validation/loss': 1.479137659072876, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.118631362915039, 'test/num_examples': 10000, 'score': 44995.94634652138, 'total_duration': 49977.063490867615, 'accumulated_submission_time': 44995.94634652138, 'accumulated_eval_time': 4971.78942322731, 'accumulated_logging_time': 4.224483251571655}
I0202 02:54:05.244190 139774417409792 logging_writer.py:48] [98381] accumulated_eval_time=4971.789423, accumulated_logging_time=4.224483, accumulated_submission_time=44995.946347, global_step=98381, preemption_count=0, score=44995.946347, test/accuracy=0.521900, test/loss=2.118631, test/num_examples=10000, total_duration=49977.063491, train/accuracy=0.694395, train/loss=1.243578, validation/accuracy=0.641840, validation/loss=1.479138, validation/num_examples=50000
I0202 02:54:13.165759 139774434195200 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.8037439584732056, loss=4.715510845184326
I0202 02:54:55.712640 139774417409792 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.371626377105713, loss=4.932089805603027
I0202 02:55:42.010750 139774434195200 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.9463008642196655, loss=4.787288665771484
I0202 02:56:28.644252 139774417409792 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.1120564937591553, loss=2.1220908164978027
I0202 02:57:15.011911 139774434195200 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.4008736610412598, loss=2.1943986415863037
I0202 02:58:01.461323 139774417409792 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.047422170639038, loss=2.3925726413726807
I0202 02:58:47.976296 139774434195200 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.189131736755371, loss=4.779690742492676
I0202 02:59:34.264182 139774417409792 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.1851611137390137, loss=2.2302112579345703
I0202 03:00:20.490845 139774434195200 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.9500505924224854, loss=4.6165289878845215
I0202 03:01:05.604059 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:01:17.656012 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:01:50.686563 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:01:52.289146 139936116377408 submission_runner.py:408] Time since start: 50444.14s, 	Step: 99296, 	{'train/accuracy': 0.6937890648841858, 'train/loss': 1.2605602741241455, 'validation/accuracy': 0.6408799886703491, 'validation/loss': 1.5083057880401611, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.170624017715454, 'test/num_examples': 10000, 'score': 45416.24973320961, 'total_duration': 50444.14269065857, 'accumulated_submission_time': 45416.24973320961, 'accumulated_eval_time': 5018.474480867386, 'accumulated_logging_time': 4.268015146255493}
I0202 03:01:52.332289 139774417409792 logging_writer.py:48] [99296] accumulated_eval_time=5018.474481, accumulated_logging_time=4.268015, accumulated_submission_time=45416.249733, global_step=99296, preemption_count=0, score=45416.249733, test/accuracy=0.513900, test/loss=2.170624, test/num_examples=10000, total_duration=50444.142691, train/accuracy=0.693789, train/loss=1.260560, validation/accuracy=0.640880, validation/loss=1.508306, validation/num_examples=50000
I0202 03:01:54.330188 139774434195200 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.2250499725341797, loss=2.3719351291656494
I0202 03:02:36.056463 139774417409792 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.2089903354644775, loss=2.1429202556610107
I0202 03:03:22.276869 139774434195200 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.9800364971160889, loss=4.717109680175781
I0202 03:04:08.716630 139774417409792 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.0751516819000244, loss=2.772773027420044
I0202 03:04:54.904596 139774434195200 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.7542386054992676, loss=4.080443859100342
I0202 03:05:41.458392 139774417409792 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.205841302871704, loss=3.425811767578125
I0202 03:06:27.815268 139774434195200 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.8014167547225952, loss=4.270205020904541
I0202 03:07:14.111988 139774417409792 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.2569525241851807, loss=2.0984599590301514
I0202 03:08:00.633498 139774434195200 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.809674859046936, loss=4.798670768737793
I0202 03:08:46.816663 139774417409792 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.0832977294921875, loss=4.540256500244141
I0202 03:08:52.501294 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:09:04.699784 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:09:39.459570 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:09:41.052591 139936116377408 submission_runner.py:408] Time since start: 50912.91s, 	Step: 100214, 	{'train/accuracy': 0.6874804496765137, 'train/loss': 1.2632285356521606, 'validation/accuracy': 0.6426999568939209, 'validation/loss': 1.4662885665893555, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.1122829914093018, 'test/num_examples': 10000, 'score': 45836.359743356705, 'total_duration': 50912.906153678894, 'accumulated_submission_time': 45836.359743356705, 'accumulated_eval_time': 5067.025764942169, 'accumulated_logging_time': 4.322429895401001}
I0202 03:09:41.083185 139774434195200 logging_writer.py:48] [100214] accumulated_eval_time=5067.025765, accumulated_logging_time=4.322430, accumulated_submission_time=45836.359743, global_step=100214, preemption_count=0, score=45836.359743, test/accuracy=0.517700, test/loss=2.112283, test/num_examples=10000, total_duration=50912.906154, train/accuracy=0.687480, train/loss=1.263229, validation/accuracy=0.642700, validation/loss=1.466289, validation/num_examples=50000
I0202 03:10:16.679079 139774417409792 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.0347940921783447, loss=4.235284805297852
I0202 03:11:02.569771 139774434195200 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.2910287380218506, loss=2.365335702896118
I0202 03:11:48.944705 139774417409792 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.466580867767334, loss=2.229588270187378
I0202 03:12:35.398325 139774434195200 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.9112643003463745, loss=3.4262614250183105
I0202 03:13:21.421091 139774417409792 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.352398157119751, loss=2.1714847087860107
I0202 03:14:07.757173 139774434195200 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.5008511543273926, loss=2.2688701152801514
I0202 03:14:53.757576 139774417409792 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.9218034744262695, loss=4.199357032775879
I0202 03:15:40.101560 139774434195200 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.2624943256378174, loss=2.064958333969116
I0202 03:16:26.326114 139774417409792 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.2286667823791504, loss=3.0547900199890137
I0202 03:16:41.173074 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:16:53.100702 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:17:24.276001 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:17:25.888110 139936116377408 submission_runner.py:408] Time since start: 51377.74s, 	Step: 101134, 	{'train/accuracy': 0.6952343583106995, 'train/loss': 1.2298552989959717, 'validation/accuracy': 0.646399974822998, 'validation/loss': 1.4554753303527832, 'validation/num_examples': 50000, 'test/accuracy': 0.5227000117301941, 'test/loss': 2.1116693019866943, 'test/num_examples': 10000, 'score': 46256.391678094864, 'total_duration': 51377.74165701866, 'accumulated_submission_time': 46256.391678094864, 'accumulated_eval_time': 5111.740765094757, 'accumulated_logging_time': 4.362881422042847}
I0202 03:17:25.926107 139774434195200 logging_writer.py:48] [101134] accumulated_eval_time=5111.740765, accumulated_logging_time=4.362881, accumulated_submission_time=46256.391678, global_step=101134, preemption_count=0, score=46256.391678, test/accuracy=0.522700, test/loss=2.111669, test/num_examples=10000, total_duration=51377.741657, train/accuracy=0.695234, train/loss=1.229855, validation/accuracy=0.646400, validation/loss=1.455475, validation/num_examples=50000
I0202 03:17:52.522616 139774417409792 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.3023974895477295, loss=2.6142303943634033
I0202 03:18:37.909556 139774434195200 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.2250256538391113, loss=2.104825258255005
I0202 03:19:24.265280 139774417409792 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.2242438793182373, loss=4.811871528625488
I0202 03:20:10.310883 139774434195200 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.3077080249786377, loss=2.1472792625427246
I0202 03:20:56.222173 139774417409792 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.7894402742385864, loss=3.7670111656188965
I0202 03:21:42.415532 139774434195200 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.093806266784668, loss=2.490309715270996
I0202 03:22:28.969617 139774417409792 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.8713420629501343, loss=4.134207725524902
I0202 03:23:14.975792 139774434195200 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.006601333618164, loss=2.387066602706909
I0202 03:24:01.059854 139774417409792 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.1381278038024902, loss=2.283608913421631
I0202 03:24:26.169645 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:24:38.371957 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:25:11.124632 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:25:12.728364 139936116377408 submission_runner.py:408] Time since start: 51844.58s, 	Step: 102056, 	{'train/accuracy': 0.6937499642372131, 'train/loss': 1.2900238037109375, 'validation/accuracy': 0.638759970664978, 'validation/loss': 1.5294249057769775, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.172455310821533, 'test/num_examples': 10000, 'score': 46676.57629132271, 'total_duration': 51844.58192944527, 'accumulated_submission_time': 46676.57629132271, 'accumulated_eval_time': 5158.299484729767, 'accumulated_logging_time': 4.4107465744018555}
I0202 03:25:12.767995 139774434195200 logging_writer.py:48] [102056] accumulated_eval_time=5158.299485, accumulated_logging_time=4.410747, accumulated_submission_time=46676.576291, global_step=102056, preemption_count=0, score=46676.576291, test/accuracy=0.520000, test/loss=2.172455, test/num_examples=10000, total_duration=51844.581929, train/accuracy=0.693750, train/loss=1.290024, validation/accuracy=0.638760, validation/loss=1.529425, validation/num_examples=50000
I0202 03:25:30.625402 139774417409792 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.9964958429336548, loss=3.4522545337677
I0202 03:26:14.751312 139774434195200 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.8735214471817017, loss=3.283818006515503
I0202 03:27:00.714647 139774417409792 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.2283859252929688, loss=3.0336310863494873
I0202 03:27:47.325022 139774434195200 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.2741167545318604, loss=3.017787456512451
I0202 03:28:33.452962 139774417409792 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.1216588020324707, loss=2.1601314544677734
I0202 03:29:19.918600 139774434195200 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.413986921310425, loss=2.1921887397766113
I0202 03:30:06.774137 139774417409792 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.3460309505462646, loss=2.619802236557007
I0202 03:30:52.706248 139774434195200 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.026587963104248, loss=4.769800662994385
I0202 03:31:39.168082 139774417409792 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.0984084606170654, loss=4.671360969543457
I0202 03:32:12.795121 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:32:24.831936 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:32:58.133937 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:32:59.735524 139936116377408 submission_runner.py:408] Time since start: 52311.59s, 	Step: 102974, 	{'train/accuracy': 0.7155859470367432, 'train/loss': 1.145095944404602, 'validation/accuracy': 0.6472600102424622, 'validation/loss': 1.4536617994308472, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.0942981243133545, 'test/num_examples': 10000, 'score': 47096.54494357109, 'total_duration': 52311.58908891678, 'accumulated_submission_time': 47096.54494357109, 'accumulated_eval_time': 5205.239888191223, 'accumulated_logging_time': 4.460630416870117}
I0202 03:32:59.767002 139774434195200 logging_writer.py:48] [102974] accumulated_eval_time=5205.239888, accumulated_logging_time=4.460630, accumulated_submission_time=47096.544944, global_step=102974, preemption_count=0, score=47096.544944, test/accuracy=0.522900, test/loss=2.094298, test/num_examples=10000, total_duration=52311.589089, train/accuracy=0.715586, train/loss=1.145096, validation/accuracy=0.647260, validation/loss=1.453662, validation/num_examples=50000
I0202 03:33:10.488032 139774417409792 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.931946873664856, loss=4.596810340881348
I0202 03:33:53.418226 139774434195200 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.328850269317627, loss=2.107938766479492
I0202 03:34:39.774820 139774417409792 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.8419535160064697, loss=3.6719589233398438
I0202 03:35:26.349131 139774434195200 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.510838508605957, loss=2.115673780441284
I0202 03:36:12.530136 139774417409792 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.239001512527466, loss=2.329585552215576
I0202 03:36:58.885789 139774434195200 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.05265736579895, loss=3.174785614013672
I0202 03:37:45.037355 139774417409792 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.427293539047241, loss=2.0896215438842773
I0202 03:38:31.370966 139774434195200 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.284409284591675, loss=2.1352782249450684
I0202 03:39:17.996480 139774417409792 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.0187935829162598, loss=4.541990756988525
I0202 03:40:00.197165 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:40:12.391926 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:40:45.632016 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:40:47.245187 139936116377408 submission_runner.py:408] Time since start: 52779.10s, 	Step: 103892, 	{'train/accuracy': 0.6991210579872131, 'train/loss': 1.2539443969726562, 'validation/accuracy': 0.6495400071144104, 'validation/loss': 1.4771441221237183, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1233255863189697, 'test/num_examples': 10000, 'score': 47516.917372226715, 'total_duration': 52779.09874868393, 'accumulated_submission_time': 47516.917372226715, 'accumulated_eval_time': 5252.287913560867, 'accumulated_logging_time': 4.502202272415161}
I0202 03:40:47.277552 139774434195200 logging_writer.py:48] [103892] accumulated_eval_time=5252.287914, accumulated_logging_time=4.502202, accumulated_submission_time=47516.917372, global_step=103892, preemption_count=0, score=47516.917372, test/accuracy=0.525100, test/loss=2.123326, test/num_examples=10000, total_duration=52779.098749, train/accuracy=0.699121, train/loss=1.253944, validation/accuracy=0.649540, validation/loss=1.477144, validation/num_examples=50000
I0202 03:40:50.848093 139774417409792 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.053062915802002, loss=4.0261359214782715
I0202 03:41:32.682610 139774434195200 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.261439561843872, loss=2.151827573776245
I0202 03:42:18.915966 139774417409792 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.094020366668701, loss=2.4675650596618652
I0202 03:43:05.155926 139774434195200 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.9942243099212646, loss=4.435074806213379
I0202 03:43:51.228386 139774417409792 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.1141843795776367, loss=2.179393768310547
I0202 03:44:37.355849 139774434195200 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.2439987659454346, loss=1.9435858726501465
I0202 03:45:23.357465 139774417409792 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.2195165157318115, loss=2.0949273109436035
I0202 03:46:09.508298 139774434195200 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.1560842990875244, loss=3.1616017818450928
I0202 03:46:55.591323 139774417409792 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.484417200088501, loss=2.149425745010376
I0202 03:47:41.747518 139774434195200 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.1459438800811768, loss=3.2880985736846924
I0202 03:47:47.409372 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:47:59.524966 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:48:28.119925 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:48:29.714682 139936116377408 submission_runner.py:408] Time since start: 53241.57s, 	Step: 104814, 	{'train/accuracy': 0.7024218440055847, 'train/loss': 1.2268569469451904, 'validation/accuracy': 0.6485799551010132, 'validation/loss': 1.4664111137390137, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.094282627105713, 'test/num_examples': 10000, 'score': 47936.990965127945, 'total_duration': 53241.568251132965, 'accumulated_submission_time': 47936.990965127945, 'accumulated_eval_time': 5294.593217134476, 'accumulated_logging_time': 4.5446178913116455}
I0202 03:48:29.747900 139774417409792 logging_writer.py:48] [104814] accumulated_eval_time=5294.593217, accumulated_logging_time=4.544618, accumulated_submission_time=47936.990965, global_step=104814, preemption_count=0, score=47936.990965, test/accuracy=0.525000, test/loss=2.094283, test/num_examples=10000, total_duration=53241.568251, train/accuracy=0.702422, train/loss=1.226857, validation/accuracy=0.648580, validation/loss=1.466411, validation/num_examples=50000
I0202 03:49:05.222400 139774434195200 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.3613438606262207, loss=2.199733018875122
I0202 03:49:50.953956 139774417409792 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.505934715270996, loss=2.2495694160461426
I0202 03:50:37.576459 139774434195200 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.8286638259887695, loss=2.153571128845215
I0202 03:51:23.862956 139774417409792 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.7814141511917114, loss=4.135317802429199
I0202 03:52:10.403777 139774434195200 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.410717248916626, loss=2.109812021255493
I0202 03:52:56.305106 139774417409792 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.0220139026641846, loss=3.0936102867126465
I0202 03:53:42.471476 139774434195200 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.4916083812713623, loss=2.1936676502227783
I0202 03:54:28.527981 139774417409792 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.8975647687911987, loss=4.032860279083252
I0202 03:55:14.634352 139774434195200 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.3632619380950928, loss=2.4751169681549072
I0202 03:55:29.816008 139936116377408 spec.py:321] Evaluating on the training split.
I0202 03:55:41.824660 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 03:56:13.684201 139936116377408 spec.py:349] Evaluating on the test split.
I0202 03:56:15.284213 139936116377408 submission_runner.py:408] Time since start: 53707.14s, 	Step: 105735, 	{'train/accuracy': 0.7244726419448853, 'train/loss': 1.1140944957733154, 'validation/accuracy': 0.6542800068855286, 'validation/loss': 1.4219244718551636, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.06118106842041, 'test/num_examples': 10000, 'score': 48357.001981019974, 'total_duration': 53707.13777804375, 'accumulated_submission_time': 48357.001981019974, 'accumulated_eval_time': 5340.061425924301, 'accumulated_logging_time': 4.586939811706543}
I0202 03:56:15.315880 139774417409792 logging_writer.py:48] [105735] accumulated_eval_time=5340.061426, accumulated_logging_time=4.586940, accumulated_submission_time=48357.001981, global_step=105735, preemption_count=0, score=48357.001981, test/accuracy=0.529700, test/loss=2.061181, test/num_examples=10000, total_duration=53707.137778, train/accuracy=0.724473, train/loss=1.114094, validation/accuracy=0.654280, validation/loss=1.421924, validation/num_examples=50000
I0202 03:56:41.518270 139774434195200 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.484504461288452, loss=2.1066980361938477
I0202 03:57:26.715971 139774417409792 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.3318305015563965, loss=3.744110584259033
I0202 03:58:13.131651 139774434195200 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.1691057682037354, loss=3.6616368293762207
I0202 03:58:59.563971 139774417409792 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.4229471683502197, loss=2.531867027282715
I0202 03:59:46.028334 139774434195200 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.259857177734375, loss=2.130121946334839
I0202 04:00:32.718816 139774417409792 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.026655435562134, loss=2.4488027095794678
I0202 04:01:19.462640 139774434195200 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.2682199478149414, loss=4.6880927085876465
I0202 04:02:06.077750 139774417409792 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.302318572998047, loss=2.202643871307373
I0202 04:02:52.402930 139774434195200 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.570326566696167, loss=2.1725525856018066
I0202 04:03:15.557271 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:03:27.534307 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:04:02.347160 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:04:03.969211 139936116377408 submission_runner.py:408] Time since start: 54175.82s, 	Step: 106651, 	{'train/accuracy': 0.7056054472923279, 'train/loss': 1.192114233970642, 'validation/accuracy': 0.6538000106811523, 'validation/loss': 1.427022099494934, 'validation/num_examples': 50000, 'test/accuracy': 0.5336000323295593, 'test/loss': 2.058079481124878, 'test/num_examples': 10000, 'score': 48777.18662452698, 'total_duration': 54175.822756290436, 'accumulated_submission_time': 48777.18662452698, 'accumulated_eval_time': 5388.473347187042, 'accumulated_logging_time': 4.627787828445435}
I0202 04:04:04.003409 139774417409792 logging_writer.py:48] [106651] accumulated_eval_time=5388.473347, accumulated_logging_time=4.627788, accumulated_submission_time=48777.186625, global_step=106651, preemption_count=0, score=48777.186625, test/accuracy=0.533600, test/loss=2.058079, test/num_examples=10000, total_duration=54175.822756, train/accuracy=0.705605, train/loss=1.192114, validation/accuracy=0.653800, validation/loss=1.427022, validation/num_examples=50000
I0202 04:04:23.841196 139774434195200 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.3070309162139893, loss=2.1987264156341553
I0202 04:05:08.255625 139774417409792 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.897579312324524, loss=2.8010709285736084
I0202 04:05:54.563021 139774434195200 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.112135171890259, loss=4.632312297821045
I0202 04:06:41.291470 139774417409792 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.203624725341797, loss=2.338038682937622
I0202 04:07:27.734352 139774434195200 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.1395785808563232, loss=3.7258810997009277
I0202 04:08:14.022351 139774417409792 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.082207679748535, loss=3.37589693069458
I0202 04:09:00.134451 139774434195200 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.0459256172180176, loss=4.392881393432617
I0202 04:09:46.369324 139774417409792 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.3483965396881104, loss=2.4359607696533203
I0202 04:10:33.239334 139774434195200 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.2661447525024414, loss=2.133026599884033
I0202 04:11:04.101626 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:11:16.078582 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:11:52.281213 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:11:53.885891 139936116377408 submission_runner.py:408] Time since start: 54645.74s, 	Step: 107568, 	{'train/accuracy': 0.7154492139816284, 'train/loss': 1.1752427816390991, 'validation/accuracy': 0.6553599834442139, 'validation/loss': 1.4241199493408203, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.0526721477508545, 'test/num_examples': 10000, 'score': 49197.227964401245, 'total_duration': 54645.739453077316, 'accumulated_submission_time': 49197.227964401245, 'accumulated_eval_time': 5438.257611513138, 'accumulated_logging_time': 4.671726226806641}
I0202 04:11:53.921065 139774417409792 logging_writer.py:48] [107568] accumulated_eval_time=5438.257612, accumulated_logging_time=4.671726, accumulated_submission_time=49197.227964, global_step=107568, preemption_count=0, score=49197.227964, test/accuracy=0.535600, test/loss=2.052672, test/num_examples=10000, total_duration=54645.739453, train/accuracy=0.715449, train/loss=1.175243, validation/accuracy=0.655360, validation/loss=1.424120, validation/num_examples=50000
I0202 04:12:07.018530 139774434195200 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.291564464569092, loss=2.1957991123199463
I0202 04:12:50.554242 139774417409792 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.0974552631378174, loss=2.7428369522094727
I0202 04:13:36.848192 139774434195200 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.42689847946167, loss=4.913276195526123
I0202 04:14:23.272534 139774417409792 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.8443766832351685, loss=4.427885055541992
I0202 04:15:09.339226 139774434195200 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.416541337966919, loss=2.046961784362793
I0202 04:15:55.579356 139774417409792 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.0783307552337646, loss=3.0130343437194824
I0202 04:16:41.644191 139774434195200 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.4942429065704346, loss=2.3040926456451416
I0202 04:17:27.893989 139774417409792 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.4523684978485107, loss=2.0942139625549316
I0202 04:18:14.420376 139774434195200 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.97017502784729, loss=3.213468074798584
I0202 04:18:54.032425 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:19:06.482285 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:19:41.336906 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:19:42.939266 139936116377408 submission_runner.py:408] Time since start: 55114.79s, 	Step: 108488, 	{'train/accuracy': 0.71839839220047, 'train/loss': 1.1570578813552856, 'validation/accuracy': 0.6541000008583069, 'validation/loss': 1.4482253789901733, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.080390214920044, 'test/num_examples': 10000, 'score': 49617.28289413452, 'total_duration': 55114.79280400276, 'accumulated_submission_time': 49617.28289413452, 'accumulated_eval_time': 5487.164425611496, 'accumulated_logging_time': 4.715782403945923}
I0202 04:19:42.978402 139774417409792 logging_writer.py:48] [108488] accumulated_eval_time=5487.164426, accumulated_logging_time=4.715782, accumulated_submission_time=49617.282894, global_step=108488, preemption_count=0, score=49617.282894, test/accuracy=0.531300, test/loss=2.080390, test/num_examples=10000, total_duration=55114.792804, train/accuracy=0.718398, train/loss=1.157058, validation/accuracy=0.654100, validation/loss=1.448225, validation/num_examples=50000
I0202 04:19:48.134232 139774434195200 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.346522808074951, loss=2.4472055435180664
I0202 04:20:30.403681 139774417409792 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.282059907913208, loss=2.1919546127319336
I0202 04:21:16.304434 139774434195200 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.6118524074554443, loss=2.1491870880126953
I0202 04:22:02.705031 139774417409792 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.607252359390259, loss=2.172785758972168
I0202 04:22:49.201191 139774434195200 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.1919353008270264, loss=2.3757011890411377
I0202 04:23:35.278882 139774417409792 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.342447519302368, loss=2.536386728286743
I0202 04:24:21.261490 139774434195200 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.5512585639953613, loss=2.122377395629883
I0202 04:25:07.422288 139774417409792 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.323333501815796, loss=2.0016133785247803
I0202 04:25:53.590525 139774434195200 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.5042407512664795, loss=4.329474449157715
I0202 04:26:39.679460 139774417409792 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.5204010009765625, loss=2.2512459754943848
I0202 04:26:43.144421 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:26:55.154881 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:27:26.754013 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:27:28.363026 139936116377408 submission_runner.py:408] Time since start: 55580.22s, 	Step: 109409, 	{'train/accuracy': 0.7098046541213989, 'train/loss': 1.2095328569412231, 'validation/accuracy': 0.6589800119400024, 'validation/loss': 1.4317922592163086, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.062147617340088, 'test/num_examples': 10000, 'score': 50037.39028072357, 'total_duration': 55580.21659255028, 'accumulated_submission_time': 50037.39028072357, 'accumulated_eval_time': 5532.3830144405365, 'accumulated_logging_time': 4.7658116817474365}
I0202 04:27:28.397512 139774434195200 logging_writer.py:48] [109409] accumulated_eval_time=5532.383014, accumulated_logging_time=4.765812, accumulated_submission_time=50037.390281, global_step=109409, preemption_count=0, score=50037.390281, test/accuracy=0.536400, test/loss=2.062148, test/num_examples=10000, total_duration=55580.216593, train/accuracy=0.709805, train/loss=1.209533, validation/accuracy=0.658980, validation/loss=1.431792, validation/num_examples=50000
I0202 04:28:06.392288 139774417409792 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.414217948913574, loss=2.0328330993652344
I0202 04:28:52.178967 139774434195200 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.204510450363159, loss=4.7418904304504395
I0202 04:29:38.641533 139774417409792 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.061814069747925, loss=3.176252603530884
I0202 04:30:24.969221 139774434195200 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.3302857875823975, loss=2.126272678375244
I0202 04:31:11.103307 139774417409792 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.186821937561035, loss=2.6448729038238525
I0202 04:31:57.581051 139774434195200 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.5263302326202393, loss=2.727447509765625
I0202 04:32:43.967056 139774417409792 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.13338303565979, loss=2.8124799728393555
I0202 04:33:30.372653 139774434195200 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.29552960395813, loss=2.431788921356201
I0202 04:34:16.521799 139774417409792 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.4093258380889893, loss=2.113152027130127
I0202 04:34:28.636344 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:34:40.674082 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:35:14.613392 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:35:16.216462 139936116377408 submission_runner.py:408] Time since start: 56048.07s, 	Step: 110328, 	{'train/accuracy': 0.705078125, 'train/loss': 1.2167671918869019, 'validation/accuracy': 0.6532399654388428, 'validation/loss': 1.4524482488632202, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.0938665866851807, 'test/num_examples': 10000, 'score': 50457.570643901825, 'total_duration': 56048.070026397705, 'accumulated_submission_time': 50457.570643901825, 'accumulated_eval_time': 5579.963124036789, 'accumulated_logging_time': 4.810678005218506}
I0202 04:35:16.249710 139774434195200 logging_writer.py:48] [110328] accumulated_eval_time=5579.963124, accumulated_logging_time=4.810678, accumulated_submission_time=50457.570644, global_step=110328, preemption_count=0, score=50457.570644, test/accuracy=0.529400, test/loss=2.093867, test/num_examples=10000, total_duration=56048.070026, train/accuracy=0.705078, train/loss=1.216767, validation/accuracy=0.653240, validation/loss=1.452448, validation/num_examples=50000
I0202 04:35:45.224461 139774417409792 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.8098292350769043, loss=2.1002860069274902
I0202 04:36:31.211908 139774434195200 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.183811902999878, loss=4.254715442657471
I0202 04:37:17.781111 139774417409792 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.5597925186157227, loss=2.0937561988830566
I0202 04:38:04.121832 139774434195200 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.554184675216675, loss=2.1697988510131836
I0202 04:38:50.303956 139774417409792 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.267850875854492, loss=2.2980146408081055
I0202 04:39:36.800408 139774434195200 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.1531460285186768, loss=4.697587966918945
I0202 04:40:23.074178 139774417409792 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.18314266204834, loss=2.531083106994629
I0202 04:41:09.262488 139774434195200 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.227551221847534, loss=3.4675369262695312
I0202 04:41:55.612894 139774417409792 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.132652521133423, loss=3.687300682067871
I0202 04:42:16.679230 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:42:28.844889 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:43:05.340787 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:43:06.948764 139936116377408 submission_runner.py:408] Time since start: 56518.80s, 	Step: 111247, 	{'train/accuracy': 0.7220507860183716, 'train/loss': 1.1261377334594727, 'validation/accuracy': 0.6609999537467957, 'validation/loss': 1.4034225940704346, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.035445213317871, 'test/num_examples': 10000, 'score': 50877.94385957718, 'total_duration': 56518.80230307579, 'accumulated_submission_time': 50877.94385957718, 'accumulated_eval_time': 5630.232651948929, 'accumulated_logging_time': 4.852938175201416}
I0202 04:43:06.989704 139774434195200 logging_writer.py:48] [111247] accumulated_eval_time=5630.232652, accumulated_logging_time=4.852938, accumulated_submission_time=50877.943860, global_step=111247, preemption_count=0, score=50877.943860, test/accuracy=0.535300, test/loss=2.035445, test/num_examples=10000, total_duration=56518.802303, train/accuracy=0.722051, train/loss=1.126138, validation/accuracy=0.661000, validation/loss=1.403423, validation/num_examples=50000
I0202 04:43:28.432573 139774417409792 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.313081979751587, loss=2.099696636199951
I0202 04:44:13.524121 139774434195200 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.3290836811065674, loss=4.020654201507568
I0202 04:44:59.599918 139774417409792 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.1633284091949463, loss=4.03118371963501
I0202 04:45:46.062308 139774434195200 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.568621873855591, loss=2.206575870513916
I0202 04:46:32.162469 139774417409792 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.356736183166504, loss=2.0508100986480713
I0202 04:47:18.374840 139774434195200 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.5365777015686035, loss=2.2568180561065674
I0202 04:48:04.519541 139774417409792 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.1896839141845703, loss=2.6947789192199707
I0202 04:48:50.576591 139774434195200 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.9793602228164673, loss=4.191553115844727
I0202 04:49:36.913816 139774417409792 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.094909191131592, loss=3.8928260803222656
I0202 04:50:07.192608 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:50:19.230322 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:50:54.270574 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:50:55.884879 139936116377408 submission_runner.py:408] Time since start: 56987.74s, 	Step: 112167, 	{'train/accuracy': 0.7176757454872131, 'train/loss': 1.1667044162750244, 'validation/accuracy': 0.6600599884986877, 'validation/loss': 1.4147700071334839, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.0341708660125732, 'test/num_examples': 10000, 'score': 51298.08949494362, 'total_duration': 56987.73841023445, 'accumulated_submission_time': 51298.08949494362, 'accumulated_eval_time': 5678.924888134003, 'accumulated_logging_time': 4.904115915298462}
I0202 04:50:55.923207 139774434195200 logging_writer.py:48] [112167] accumulated_eval_time=5678.924888, accumulated_logging_time=4.904116, accumulated_submission_time=51298.089495, global_step=112167, preemption_count=0, score=51298.089495, test/accuracy=0.534900, test/loss=2.034171, test/num_examples=10000, total_duration=56987.738410, train/accuracy=0.717676, train/loss=1.166704, validation/accuracy=0.660060, validation/loss=1.414770, validation/num_examples=50000
I0202 04:51:09.413758 139774417409792 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.6730854511260986, loss=2.0388991832733154
I0202 04:51:52.990123 139774434195200 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.249509811401367, loss=2.6408352851867676
I0202 04:52:39.342339 139774417409792 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.5191702842712402, loss=2.200909376144409
I0202 04:53:25.761449 139774434195200 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.46643328666687, loss=2.157764196395874
I0202 04:54:12.284218 139774417409792 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.088662624359131, loss=2.9695398807525635
I0202 04:54:58.557469 139774434195200 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.2667980194091797, loss=2.522231101989746
I0202 04:55:44.992276 139774417409792 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.7965526580810547, loss=2.1632301807403564
I0202 04:56:31.435942 139774434195200 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.4271392822265625, loss=2.2647790908813477
I0202 04:57:17.778208 139774417409792 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.4162135124206543, loss=2.0142195224761963
I0202 04:57:56.139871 139936116377408 spec.py:321] Evaluating on the training split.
I0202 04:58:08.443837 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 04:58:42.904705 139936116377408 spec.py:349] Evaluating on the test split.
I0202 04:58:44.505074 139936116377408 submission_runner.py:408] Time since start: 57456.36s, 	Step: 113085, 	{'train/accuracy': 0.7177538871765137, 'train/loss': 1.1266690492630005, 'validation/accuracy': 0.6662999987602234, 'validation/loss': 1.3710778951644897, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.005274534225464, 'test/num_examples': 10000, 'score': 51718.244585990906, 'total_duration': 57456.35862541199, 'accumulated_submission_time': 51718.244585990906, 'accumulated_eval_time': 5727.290082454681, 'accumulated_logging_time': 4.955888032913208}
I0202 04:58:44.538265 139774434195200 logging_writer.py:48] [113085] accumulated_eval_time=5727.290082, accumulated_logging_time=4.955888, accumulated_submission_time=51718.244586, global_step=113085, preemption_count=0, score=51718.244586, test/accuracy=0.545100, test/loss=2.005275, test/num_examples=10000, total_duration=57456.358625, train/accuracy=0.717754, train/loss=1.126669, validation/accuracy=0.666300, validation/loss=1.371078, validation/num_examples=50000
I0202 04:58:50.883510 139774417409792 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.3597664833068848, loss=2.060192346572876
I0202 04:59:33.282880 139774434195200 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.4061484336853027, loss=1.9988420009613037
I0202 05:00:19.270126 139774417409792 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.4718124866485596, loss=2.0837066173553467
I0202 05:01:05.503621 139774434195200 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.1557862758636475, loss=4.212306022644043
I0202 05:01:51.777455 139774417409792 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.5053231716156006, loss=2.758004665374756
I0202 05:02:38.118885 139774434195200 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8171629905700684, loss=2.250119209289551
I0202 05:03:24.614867 139774417409792 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.6361663341522217, loss=2.1769113540649414
I0202 05:04:10.752590 139774434195200 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.327496290206909, loss=2.01237416267395
I0202 05:04:57.362046 139774417409792 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.389028310775757, loss=2.2453646659851074
I0202 05:05:43.918704 139774434195200 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.1433303356170654, loss=3.177427291870117
I0202 05:05:44.960610 139936116377408 spec.py:321] Evaluating on the training split.
I0202 05:05:57.024636 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 05:06:28.094823 139936116377408 spec.py:349] Evaluating on the test split.
I0202 05:06:29.697832 139936116377408 submission_runner.py:408] Time since start: 57921.55s, 	Step: 114004, 	{'train/accuracy': 0.7226366996765137, 'train/loss': 1.1332825422286987, 'validation/accuracy': 0.6693199872970581, 'validation/loss': 1.3838796615600586, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.008410692214966, 'test/num_examples': 10000, 'score': 52138.60879659653, 'total_duration': 57921.55140066147, 'accumulated_submission_time': 52138.60879659653, 'accumulated_eval_time': 5772.0273015499115, 'accumulated_logging_time': 5.000272750854492}
I0202 05:06:29.730268 139774417409792 logging_writer.py:48] [114004] accumulated_eval_time=5772.027302, accumulated_logging_time=5.000273, accumulated_submission_time=52138.608797, global_step=114004, preemption_count=0, score=52138.608797, test/accuracy=0.543900, test/loss=2.008411, test/num_examples=10000, total_duration=57921.551401, train/accuracy=0.722637, train/loss=1.133283, validation/accuracy=0.669320, validation/loss=1.383880, validation/num_examples=50000
I0202 05:07:09.826719 139774434195200 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.741265058517456, loss=2.0405733585357666
I0202 05:07:55.576820 139774417409792 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.918992042541504, loss=2.0137016773223877
I0202 05:08:41.851295 139774434195200 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.501689910888672, loss=1.9717488288879395
I0202 05:09:28.274943 139774417409792 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.4086506366729736, loss=2.7827823162078857
I0202 05:10:14.250296 139774434195200 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.71151065826416, loss=2.1188406944274902
I0202 05:11:00.149276 139774417409792 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.4840309619903564, loss=2.3291099071502686
I0202 05:11:46.425673 139774434195200 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.6758499145507812, loss=2.1264994144439697
I0202 05:12:32.889217 139774417409792 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.5821874141693115, loss=4.677051544189453
I0202 05:13:19.740734 139774434195200 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.4334983825683594, loss=2.4760828018188477
I0202 05:13:30.213397 139936116377408 spec.py:321] Evaluating on the training split.
I0202 05:13:42.258733 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 05:14:19.108982 139936116377408 spec.py:349] Evaluating on the test split.
I0202 05:14:20.709341 139936116377408 submission_runner.py:408] Time since start: 58392.56s, 	Step: 114925, 	{'train/accuracy': 0.7480273246765137, 'train/loss': 1.0093517303466797, 'validation/accuracy': 0.6700999736785889, 'validation/loss': 1.3534212112426758, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 1.9908446073532104, 'test/num_examples': 10000, 'score': 52559.03491163254, 'total_duration': 58392.56290578842, 'accumulated_submission_time': 52559.03491163254, 'accumulated_eval_time': 5822.523226261139, 'accumulated_logging_time': 5.0422186851501465}
I0202 05:14:20.744530 139774417409792 logging_writer.py:48] [114925] accumulated_eval_time=5822.523226, accumulated_logging_time=5.042219, accumulated_submission_time=52559.034912, global_step=114925, preemption_count=0, score=52559.034912, test/accuracy=0.544000, test/loss=1.990845, test/num_examples=10000, total_duration=58392.562906, train/accuracy=0.748027, train/loss=1.009352, validation/accuracy=0.670100, validation/loss=1.353421, validation/num_examples=50000
I0202 05:14:51.125123 139774434195200 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.3707237243652344, loss=2.7154054641723633
I0202 05:15:37.173575 139774417409792 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.4924967288970947, loss=2.0148234367370605
I0202 05:16:24.052475 139774434195200 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.3808987140655518, loss=3.292057991027832
I0202 05:17:10.653373 139774417409792 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.3741142749786377, loss=2.1116373538970947
I0202 05:17:56.685866 139774434195200 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.7904603481292725, loss=2.027009963989258
I0202 05:18:43.205662 139774417409792 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.499316930770874, loss=2.117377281188965
I0202 05:19:29.649949 139774434195200 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.4649903774261475, loss=2.1697516441345215
I0202 05:20:16.121993 139774417409792 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.1362977027893066, loss=3.233309030532837
I0202 05:21:02.505010 139774434195200 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.7417361736297607, loss=2.056452989578247
I0202 05:21:21.247756 139936116377408 spec.py:321] Evaluating on the training split.
I0202 05:21:33.318204 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 05:22:05.239850 139936116377408 spec.py:349] Evaluating on the test split.
I0202 05:22:06.850245 139936116377408 submission_runner.py:408] Time since start: 58858.70s, 	Step: 115842, 	{'train/accuracy': 0.7229687571525574, 'train/loss': 1.115723729133606, 'validation/accuracy': 0.6693199872970581, 'validation/loss': 1.3546879291534424, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 1.9898380041122437, 'test/num_examples': 10000, 'score': 52979.48181724548, 'total_duration': 58858.70378828049, 'accumulated_submission_time': 52979.48181724548, 'accumulated_eval_time': 5868.125700950623, 'accumulated_logging_time': 5.087374925613403}
I0202 05:22:06.891360 139774417409792 logging_writer.py:48] [115842] accumulated_eval_time=5868.125701, accumulated_logging_time=5.087375, accumulated_submission_time=52979.481817, global_step=115842, preemption_count=0, score=52979.481817, test/accuracy=0.545900, test/loss=1.989838, test/num_examples=10000, total_duration=58858.703788, train/accuracy=0.722969, train/loss=1.115724, validation/accuracy=0.669320, validation/loss=1.354688, validation/num_examples=50000
I0202 05:22:30.326068 139774434195200 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.5252692699432373, loss=4.536237716674805
I0202 05:23:15.330912 139774417409792 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.499295949935913, loss=2.134350061416626
I0202 05:24:01.929512 139774434195200 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.13828706741333, loss=3.717463493347168
I0202 05:24:48.586823 139774417409792 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.5697638988494873, loss=2.1518101692199707
I0202 05:25:34.879746 139774434195200 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.4791295528411865, loss=2.1613354682922363
I0202 05:26:21.474838 139774417409792 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.747056484222412, loss=2.360462188720703
I0202 05:27:07.953729 139774434195200 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.098667860031128, loss=3.284660816192627
I0202 05:27:54.351856 139774417409792 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.5179991722106934, loss=2.0262069702148438
I0202 05:28:40.796870 139774434195200 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.5670437812805176, loss=2.0253384113311768
I0202 05:29:07.084170 139936116377408 spec.py:321] Evaluating on the training split.
I0202 05:29:19.241581 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 05:29:53.450377 139936116377408 spec.py:349] Evaluating on the test split.
I0202 05:29:55.061853 139936116377408 submission_runner.py:408] Time since start: 59326.92s, 	Step: 116758, 	{'train/accuracy': 0.7300195097923279, 'train/loss': 1.0898079872131348, 'validation/accuracy': 0.671019971370697, 'validation/loss': 1.3476372957229614, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 1.9787174463272095, 'test/num_examples': 10000, 'score': 53399.61646103859, 'total_duration': 59326.91538286209, 'accumulated_submission_time': 53399.61646103859, 'accumulated_eval_time': 5916.103356122971, 'accumulated_logging_time': 5.1391777992248535}
I0202 05:29:55.103809 139774417409792 logging_writer.py:48] [116758] accumulated_eval_time=5916.103356, accumulated_logging_time=5.139178, accumulated_submission_time=53399.616461, global_step=116758, preemption_count=0, score=53399.616461, test/accuracy=0.543400, test/loss=1.978717, test/num_examples=10000, total_duration=59326.915383, train/accuracy=0.730020, train/loss=1.089808, validation/accuracy=0.671020, validation/loss=1.347637, validation/num_examples=50000
I0202 05:30:12.182976 139774434195200 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.6522958278656006, loss=2.116729259490967
I0202 05:30:55.915522 139774417409792 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.5663866996765137, loss=2.1999125480651855
I0202 05:31:42.238502 139774434195200 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.5282626152038574, loss=2.141558885574341
I0202 05:32:29.069422 139774417409792 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.628906726837158, loss=2.1533679962158203
I0202 05:33:15.207382 139774434195200 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.3124923706054688, loss=3.1416513919830322
I0202 05:34:01.140444 139774417409792 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.7052624225616455, loss=2.002185344696045
I0202 05:34:47.534778 139774434195200 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.411228656768799, loss=4.653923511505127
I0202 05:35:33.778022 139774417409792 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.4038712978363037, loss=4.21707010269165
I0202 05:36:20.129535 139774434195200 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.381577730178833, loss=4.2082695960998535
I0202 05:36:55.510672 139936116377408 spec.py:321] Evaluating on the training split.
I0202 05:37:07.879220 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 05:37:40.630735 139936116377408 spec.py:349] Evaluating on the test split.
I0202 05:37:42.239888 139936116377408 submission_runner.py:408] Time since start: 59794.09s, 	Step: 117679, 	{'train/accuracy': 0.7401171922683716, 'train/loss': 1.0645672082901, 'validation/accuracy': 0.6676999926567078, 'validation/loss': 1.3838788270950317, 'validation/num_examples': 50000, 'test/accuracy': 0.544700026512146, 'test/loss': 2.0176634788513184, 'test/num_examples': 10000, 'score': 53819.964002370834, 'total_duration': 59794.09345436096, 'accumulated_submission_time': 53819.964002370834, 'accumulated_eval_time': 5962.832585573196, 'accumulated_logging_time': 5.19250226020813}
I0202 05:37:42.274977 139774417409792 logging_writer.py:48] [117679] accumulated_eval_time=5962.832586, accumulated_logging_time=5.192502, accumulated_submission_time=53819.964002, global_step=117679, preemption_count=0, score=53819.964002, test/accuracy=0.544700, test/loss=2.017663, test/num_examples=10000, total_duration=59794.093454, train/accuracy=0.740117, train/loss=1.064567, validation/accuracy=0.667700, validation/loss=1.383879, validation/num_examples=50000
I0202 05:37:50.991487 139774434195200 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.6296534538269043, loss=1.976027488708496
I0202 05:38:33.715062 139774417409792 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.5864784717559814, loss=2.073922634124756
I0202 05:39:20.029033 139774434195200 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.3706459999084473, loss=4.224793434143066
I0202 05:40:06.484133 139774417409792 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.437476873397827, loss=4.697402477264404
I0202 05:40:52.613321 139774434195200 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.5949807167053223, loss=4.138962268829346
I0202 05:41:39.088797 139774417409792 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.2481260299682617, loss=3.2255029678344727
I0202 05:42:25.593148 139774434195200 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.2596821784973145, loss=2.9887661933898926
I0202 05:43:11.978244 139774417409792 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.8986785411834717, loss=2.1044540405273438
I0202 05:43:58.233357 139774434195200 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.3214919567108154, loss=3.472992420196533
I0202 05:44:42.690053 139936116377408 spec.py:321] Evaluating on the training split.
I0202 05:44:55.803520 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 05:45:29.675391 139936116377408 spec.py:349] Evaluating on the test split.
I0202 05:45:31.296143 139936116377408 submission_runner.py:408] Time since start: 60263.15s, 	Step: 118597, 	{'train/accuracy': 0.7274804711341858, 'train/loss': 1.0996066331863403, 'validation/accuracy': 0.6718199849128723, 'validation/loss': 1.3471410274505615, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 1.9880914688110352, 'test/num_examples': 10000, 'score': 54240.321560144424, 'total_duration': 60263.149705171585, 'accumulated_submission_time': 54240.321560144424, 'accumulated_eval_time': 6011.4386677742, 'accumulated_logging_time': 5.236751317977905}
I0202 05:45:31.333506 139774417409792 logging_writer.py:48] [118597] accumulated_eval_time=6011.438668, accumulated_logging_time=5.236751, accumulated_submission_time=54240.321560, global_step=118597, preemption_count=0, score=54240.321560, test/accuracy=0.546900, test/loss=1.988091, test/num_examples=10000, total_duration=60263.149705, train/accuracy=0.727480, train/loss=1.099607, validation/accuracy=0.671820, validation/loss=1.347141, validation/num_examples=50000
I0202 05:45:32.927837 139774434195200 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.4509029388427734, loss=2.4653520584106445
I0202 05:46:14.595498 139774417409792 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.650125026702881, loss=2.0292906761169434
I0202 05:47:00.230227 139774434195200 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.502000331878662, loss=2.3819146156311035
I0202 05:47:46.709648 139774417409792 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.3906493186950684, loss=1.9752545356750488
I0202 05:48:33.121957 139774434195200 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.7167282104492188, loss=2.3165695667266846
I0202 05:49:19.221312 139774417409792 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.348116636276245, loss=2.947439670562744
I0202 05:50:05.399630 139774434195200 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.6860222816467285, loss=3.6457083225250244
I0202 05:50:51.476244 139774417409792 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.510998010635376, loss=1.9075647592544556
I0202 05:51:38.004687 139774434195200 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.4237306118011475, loss=3.3064310550689697
I0202 05:52:24.184646 139774417409792 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.6161789894104004, loss=2.167250871658325
I0202 05:52:31.566743 139936116377408 spec.py:321] Evaluating on the training split.
I0202 05:52:43.580198 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 05:53:18.662347 139936116377408 spec.py:349] Evaluating on the test split.
I0202 05:53:20.271154 139936116377408 submission_runner.py:408] Time since start: 60732.12s, 	Step: 119518, 	{'train/accuracy': 0.7304491996765137, 'train/loss': 1.0636045932769775, 'validation/accuracy': 0.672760009765625, 'validation/loss': 1.335329294204712, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 1.9718530178070068, 'test/num_examples': 10000, 'score': 54660.49790549278, 'total_duration': 60732.12470769882, 'accumulated_submission_time': 54660.49790549278, 'accumulated_eval_time': 6060.143052816391, 'accumulated_logging_time': 5.284197807312012}
I0202 05:53:20.306906 139774434195200 logging_writer.py:48] [119518] accumulated_eval_time=6060.143053, accumulated_logging_time=5.284198, accumulated_submission_time=54660.497905, global_step=119518, preemption_count=0, score=54660.497905, test/accuracy=0.546900, test/loss=1.971853, test/num_examples=10000, total_duration=60732.124708, train/accuracy=0.730449, train/loss=1.063605, validation/accuracy=0.672760, validation/loss=1.335329, validation/num_examples=50000
I0202 05:53:53.754980 139774417409792 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.3212099075317383, loss=3.348109245300293
I0202 05:54:39.742242 139774434195200 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.7170932292938232, loss=4.240636825561523
I0202 05:55:26.101481 139774417409792 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.2752668857574463, loss=2.8394837379455566
I0202 05:56:12.312010 139774434195200 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.63861083984375, loss=1.9925334453582764
I0202 05:56:58.284529 139774417409792 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.514098644256592, loss=2.2956457138061523
I0202 05:57:44.876983 139774434195200 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.952810764312744, loss=2.122249126434326
I0202 05:58:31.102062 139774417409792 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.4453582763671875, loss=2.3153209686279297
I0202 05:59:17.511888 139774434195200 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.3270390033721924, loss=2.6986429691314697
I0202 06:00:03.730496 139774417409792 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.722337484359741, loss=2.0443129539489746
I0202 06:00:20.459022 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:00:32.550720 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:01:07.837355 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:01:09.446719 139936116377408 submission_runner.py:408] Time since start: 61201.30s, 	Step: 120438, 	{'train/accuracy': 0.7368554472923279, 'train/loss': 1.0701631307601929, 'validation/accuracy': 0.675879955291748, 'validation/loss': 1.3530094623565674, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 1.9830008745193481, 'test/num_examples': 10000, 'score': 55080.59421658516, 'total_duration': 61201.30026555061, 'accumulated_submission_time': 55080.59421658516, 'accumulated_eval_time': 6109.130714178085, 'accumulated_logging_time': 5.328948974609375}
I0202 06:01:09.487190 139774434195200 logging_writer.py:48] [120438] accumulated_eval_time=6109.130714, accumulated_logging_time=5.328949, accumulated_submission_time=55080.594217, global_step=120438, preemption_count=0, score=55080.594217, test/accuracy=0.548300, test/loss=1.983001, test/num_examples=10000, total_duration=61201.300266, train/accuracy=0.736855, train/loss=1.070163, validation/accuracy=0.675880, validation/loss=1.353009, validation/num_examples=50000
I0202 06:01:34.499730 139774417409792 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.775714159011841, loss=2.133060932159424
I0202 06:02:20.036435 139774434195200 logging_writer.py:48] [120600] global_step=120600, grad_norm=3.122626781463623, loss=2.073122978210449
I0202 06:03:06.606388 139774417409792 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.65830397605896, loss=1.9769978523254395
I0202 06:03:52.722850 139774434195200 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.4863030910491943, loss=3.243865489959717
I0202 06:04:39.096029 139774417409792 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.7522382736206055, loss=2.2459001541137695
I0202 06:05:25.521019 139774434195200 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.6224985122680664, loss=3.9817209243774414
I0202 06:06:11.790207 139774417409792 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.676144599914551, loss=2.0382308959960938
I0202 06:06:58.347095 139774434195200 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.245082378387451, loss=3.7958381175994873
I0202 06:07:45.136435 139774417409792 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.356184959411621, loss=2.6156954765319824
I0202 06:08:09.766599 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:08:21.886505 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:08:54.650185 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:08:56.254328 139936116377408 submission_runner.py:408] Time since start: 61668.11s, 	Step: 121354, 	{'train/accuracy': 0.7291015386581421, 'train/loss': 1.0866446495056152, 'validation/accuracy': 0.6756199598312378, 'validation/loss': 1.3281562328338623, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 1.955164909362793, 'test/num_examples': 10000, 'score': 55500.81260108948, 'total_duration': 61668.1078953743, 'accumulated_submission_time': 55500.81260108948, 'accumulated_eval_time': 6155.618450164795, 'accumulated_logging_time': 5.383184432983398}
I0202 06:08:56.287856 139774434195200 logging_writer.py:48] [121354] accumulated_eval_time=6155.618450, accumulated_logging_time=5.383184, accumulated_submission_time=55500.812601, global_step=121354, preemption_count=0, score=55500.812601, test/accuracy=0.556000, test/loss=1.955165, test/num_examples=10000, total_duration=61668.107895, train/accuracy=0.729102, train/loss=1.086645, validation/accuracy=0.675620, validation/loss=1.328156, validation/num_examples=50000
I0202 06:09:14.939756 139774417409792 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.3194103240966797, loss=3.793734073638916
I0202 06:09:59.072813 139774434195200 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.946829080581665, loss=1.9479811191558838
I0202 06:10:45.650332 139774417409792 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.4638898372650146, loss=1.9432827234268188
I0202 06:11:32.141425 139774434195200 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.683227062225342, loss=2.0250866413116455
I0202 06:12:18.278555 139774417409792 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.794051170349121, loss=2.4767749309539795
I0202 06:13:04.732635 139774434195200 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.9650981426239014, loss=2.029974937438965
I0202 06:13:50.995186 139774417409792 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.947218418121338, loss=2.083171844482422
I0202 06:14:37.383355 139774434195200 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.849233627319336, loss=1.9554425477981567
I0202 06:15:23.694671 139774417409792 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.4079298973083496, loss=2.4879045486450195
I0202 06:15:56.558352 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:16:08.863310 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:16:40.927312 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:16:42.558001 139936116377408 submission_runner.py:408] Time since start: 62134.41s, 	Step: 122273, 	{'train/accuracy': 0.7377538681030273, 'train/loss': 1.0467967987060547, 'validation/accuracy': 0.6814199686050415, 'validation/loss': 1.3041400909423828, 'validation/num_examples': 50000, 'test/accuracy': 0.5575000047683716, 'test/loss': 1.926209807395935, 'test/num_examples': 10000, 'score': 55921.026826143265, 'total_duration': 62134.41156554222, 'accumulated_submission_time': 55921.026826143265, 'accumulated_eval_time': 6201.618116378784, 'accumulated_logging_time': 5.425713300704956}
I0202 06:16:42.597185 139774434195200 logging_writer.py:48] [122273] accumulated_eval_time=6201.618116, accumulated_logging_time=5.425713, accumulated_submission_time=55921.026826, global_step=122273, preemption_count=0, score=55921.026826, test/accuracy=0.557500, test/loss=1.926210, test/num_examples=10000, total_duration=62134.411566, train/accuracy=0.737754, train/loss=1.046797, validation/accuracy=0.681420, validation/loss=1.304140, validation/num_examples=50000
I0202 06:16:53.695103 139774417409792 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.751185894012451, loss=2.5210373401641846
I0202 06:17:36.949845 139774434195200 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.3927342891693115, loss=3.9614481925964355
I0202 06:18:23.344033 139774417409792 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.7466111183166504, loss=2.0534541606903076
I0202 06:19:10.249982 139774434195200 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.68412184715271, loss=1.961940050125122
I0202 06:19:56.552357 139774417409792 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.630366563796997, loss=1.9848788976669312
I0202 06:20:42.745585 139774434195200 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.094480037689209, loss=2.0197904109954834
I0202 06:21:29.120282 139774417409792 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.5396065711975098, loss=2.3226122856140137
I0202 06:22:15.811179 139774434195200 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.713160753250122, loss=2.7084062099456787
I0202 06:23:02.272369 139774417409792 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.3146588802337646, loss=3.2744719982147217
I0202 06:23:42.777996 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:23:54.936846 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:24:28.233008 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:24:29.837397 139936116377408 submission_runner.py:408] Time since start: 62601.69s, 	Step: 123189, 	{'train/accuracy': 0.7463476657867432, 'train/loss': 1.0369484424591064, 'validation/accuracy': 0.6841599941253662, 'validation/loss': 1.3181054592132568, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 1.9473214149475098, 'test/num_examples': 10000, 'score': 56341.15030384064, 'total_duration': 62601.69093823433, 'accumulated_submission_time': 56341.15030384064, 'accumulated_eval_time': 6248.677495479584, 'accumulated_logging_time': 5.474867820739746}
I0202 06:24:29.879667 139774434195200 logging_writer.py:48] [123189] accumulated_eval_time=6248.677495, accumulated_logging_time=5.474868, accumulated_submission_time=56341.150304, global_step=123189, preemption_count=0, score=56341.150304, test/accuracy=0.556700, test/loss=1.947321, test/num_examples=10000, total_duration=62601.690938, train/accuracy=0.746348, train/loss=1.036948, validation/accuracy=0.684160, validation/loss=1.318105, validation/num_examples=50000
I0202 06:24:34.640628 139774417409792 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.7685835361480713, loss=2.0527966022491455
I0202 06:25:17.002978 139774434195200 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.694617986679077, loss=2.2288291454315186
I0202 06:26:03.095619 139774417409792 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.9592950344085693, loss=1.9958820343017578
I0202 06:26:49.444931 139774434195200 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.7310619354248047, loss=2.1734321117401123
I0202 06:27:35.589046 139774417409792 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.6057255268096924, loss=4.114266395568848
I0202 06:28:21.787201 139774434195200 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.7207469940185547, loss=2.0595576763153076
I0202 06:29:08.104723 139774417409792 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.7109315395355225, loss=3.942826509475708
I0202 06:29:54.419716 139774434195200 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.6604275703430176, loss=3.613525867462158
I0202 06:30:40.781448 139774417409792 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.7940313816070557, loss=1.958885908126831
I0202 06:31:27.322158 139774434195200 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.936690092086792, loss=2.2257606983184814
I0202 06:31:30.149138 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:31:42.279567 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:32:19.352749 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:32:20.971483 139936116377408 submission_runner.py:408] Time since start: 63072.83s, 	Step: 124108, 	{'train/accuracy': 0.7437499761581421, 'train/loss': 1.0200892686843872, 'validation/accuracy': 0.6828199625015259, 'validation/loss': 1.2920786142349243, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 1.9192627668380737, 'test/num_examples': 10000, 'score': 56761.34786653519, 'total_duration': 63072.82502889633, 'accumulated_submission_time': 56761.34786653519, 'accumulated_eval_time': 6299.499836921692, 'accumulated_logging_time': 5.528214454650879}
I0202 06:32:21.014682 139774417409792 logging_writer.py:48] [124108] accumulated_eval_time=6299.499837, accumulated_logging_time=5.528214, accumulated_submission_time=56761.347867, global_step=124108, preemption_count=0, score=56761.347867, test/accuracy=0.559900, test/loss=1.919263, test/num_examples=10000, total_duration=63072.825029, train/accuracy=0.743750, train/loss=1.020089, validation/accuracy=0.682820, validation/loss=1.292079, validation/num_examples=50000
I0202 06:32:59.220837 139774434195200 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.3525917530059814, loss=2.0249404907226562
I0202 06:33:45.074217 139774417409792 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.577698230743408, loss=2.034597158432007
I0202 06:34:31.691032 139774434195200 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.5674290657043457, loss=3.062623977661133
I0202 06:35:18.291080 139774417409792 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.2945637702941895, loss=3.2325217723846436
I0202 06:36:04.731981 139774434195200 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.557119369506836, loss=2.7717835903167725
I0202 06:36:50.984694 139774417409792 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.490306854248047, loss=3.301905632019043
I0202 06:37:37.509437 139774434195200 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0630102157592773, loss=1.9181767702102661
I0202 06:38:23.977981 139774417409792 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.640578269958496, loss=1.93039870262146
I0202 06:39:10.346074 139774434195200 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.7389867305755615, loss=2.164733409881592
I0202 06:39:21.159105 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:39:33.295292 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:40:03.809831 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:40:05.418732 139936116377408 submission_runner.py:408] Time since start: 63537.27s, 	Step: 125025, 	{'train/accuracy': 0.7412695288658142, 'train/loss': 1.0353752374649048, 'validation/accuracy': 0.6846599578857422, 'validation/loss': 1.2898250818252563, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 1.9244365692138672, 'test/num_examples': 10000, 'score': 57181.43518590927, 'total_duration': 63537.27227139473, 'accumulated_submission_time': 57181.43518590927, 'accumulated_eval_time': 6343.759425878525, 'accumulated_logging_time': 5.581441879272461}
I0202 06:40:05.452329 139774417409792 logging_writer.py:48] [125025] accumulated_eval_time=6343.759426, accumulated_logging_time=5.581442, accumulated_submission_time=57181.435186, global_step=125025, preemption_count=0, score=57181.435186, test/accuracy=0.561200, test/loss=1.924437, test/num_examples=10000, total_duration=63537.272271, train/accuracy=0.741270, train/loss=1.035375, validation/accuracy=0.684660, validation/loss=1.289825, validation/num_examples=50000
I0202 06:40:36.103551 139774434195200 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.3907296657562256, loss=1.866679310798645
I0202 06:41:21.918491 139774417409792 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.720689058303833, loss=3.7132792472839355
I0202 06:42:08.314882 139774434195200 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.5451271533966064, loss=2.1291818618774414
I0202 06:42:54.905234 139774417409792 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.0808143615722656, loss=1.8881800174713135
I0202 06:43:41.118015 139774434195200 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.768265962600708, loss=1.9361088275909424
I0202 06:44:27.426131 139774417409792 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.610513210296631, loss=4.041369915008545
I0202 06:45:13.477174 139774434195200 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.6488401889801025, loss=3.252303123474121
I0202 06:45:59.573656 139774417409792 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.7382960319519043, loss=2.241389751434326
I0202 06:46:45.761443 139774434195200 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.7621712684631348, loss=4.413057804107666
I0202 06:47:05.773641 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:47:17.840672 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:47:51.694382 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:47:53.299161 139936116377408 submission_runner.py:408] Time since start: 64005.15s, 	Step: 125945, 	{'train/accuracy': 0.7484374642372131, 'train/loss': 0.9919676780700684, 'validation/accuracy': 0.6880999803543091, 'validation/loss': 1.2748708724975586, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 1.912021517753601, 'test/num_examples': 10000, 'score': 57601.69718050957, 'total_duration': 64005.15271496773, 'accumulated_submission_time': 57601.69718050957, 'accumulated_eval_time': 6391.28493309021, 'accumulated_logging_time': 5.625512361526489}
I0202 06:47:53.336082 139774417409792 logging_writer.py:48] [125945] accumulated_eval_time=6391.284933, accumulated_logging_time=5.625512, accumulated_submission_time=57601.697181, global_step=125945, preemption_count=0, score=57601.697181, test/accuracy=0.562400, test/loss=1.912022, test/num_examples=10000, total_duration=64005.152715, train/accuracy=0.748437, train/loss=0.991968, validation/accuracy=0.688100, validation/loss=1.274871, validation/num_examples=50000
I0202 06:48:15.550955 139774434195200 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.1458425521850586, loss=1.94191312789917
I0202 06:49:00.389516 139774417409792 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.5261404514312744, loss=2.365488290786743
I0202 06:49:46.807769 139774434195200 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.6966090202331543, loss=2.4654719829559326
I0202 06:50:33.240959 139774417409792 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.8397765159606934, loss=4.512784481048584
I0202 06:51:19.592525 139774434195200 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.480656147003174, loss=4.348605155944824
I0202 06:52:05.937216 139774417409792 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.9521701335906982, loss=1.8577260971069336
I0202 06:52:52.073401 139774434195200 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.6262152194976807, loss=4.032808303833008
I0202 06:53:38.459475 139774417409792 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.7408132553100586, loss=2.1324076652526855
I0202 06:54:24.527476 139774434195200 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.6202237606048584, loss=1.943809151649475
I0202 06:54:53.425608 139936116377408 spec.py:321] Evaluating on the training split.
I0202 06:55:05.684566 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 06:55:39.562028 139936116377408 spec.py:349] Evaluating on the test split.
I0202 06:55:41.165750 139936116377408 submission_runner.py:408] Time since start: 64473.02s, 	Step: 126864, 	{'train/accuracy': 0.76025390625, 'train/loss': 0.9850084185600281, 'validation/accuracy': 0.687559962272644, 'validation/loss': 1.3039485216140747, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9312610626220703, 'test/num_examples': 10000, 'score': 58021.72929406166, 'total_duration': 64473.019273757935, 'accumulated_submission_time': 58021.72929406166, 'accumulated_eval_time': 6439.025028705597, 'accumulated_logging_time': 5.6715407371521}
I0202 06:55:41.206252 139774417409792 logging_writer.py:48] [126864] accumulated_eval_time=6439.025029, accumulated_logging_time=5.671541, accumulated_submission_time=58021.729294, global_step=126864, preemption_count=0, score=58021.729294, test/accuracy=0.565400, test/loss=1.931261, test/num_examples=10000, total_duration=64473.019274, train/accuracy=0.760254, train/loss=0.985008, validation/accuracy=0.687560, validation/loss=1.303949, validation/num_examples=50000
I0202 06:55:55.922952 139774434195200 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.558915376663208, loss=3.240485429763794
I0202 06:56:39.491868 139774417409792 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.973907947540283, loss=2.110366106033325
I0202 06:57:25.723017 139774434195200 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.9223198890686035, loss=1.814204216003418
I0202 06:58:12.134400 139774417409792 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.6551036834716797, loss=2.319953203201294
I0202 06:58:58.176810 139774434195200 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.790574073791504, loss=2.0487005710601807
I0202 06:59:44.509581 139774417409792 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.0112791061401367, loss=1.858933687210083
I0202 07:00:30.524597 139774434195200 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.7218756675720215, loss=1.9025813341140747
I0202 07:01:17.184697 139774417409792 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.2918853759765625, loss=2.043832302093506
I0202 07:02:03.455437 139774434195200 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.7620418071746826, loss=2.1823811531066895
I0202 07:02:41.393918 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:02:53.329046 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:03:26.829953 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:03:28.432229 139936116377408 submission_runner.py:408] Time since start: 64940.29s, 	Step: 127784, 	{'train/accuracy': 0.7442578077316284, 'train/loss': 1.032114863395691, 'validation/accuracy': 0.6894199848175049, 'validation/loss': 1.2755590677261353, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 1.8965699672698975, 'test/num_examples': 10000, 'score': 58441.85705113411, 'total_duration': 64940.28579545021, 'accumulated_submission_time': 58441.85705113411, 'accumulated_eval_time': 6486.063357114792, 'accumulated_logging_time': 5.723682403564453}
I0202 07:03:28.468615 139774417409792 logging_writer.py:48] [127784] accumulated_eval_time=6486.063357, accumulated_logging_time=5.723682, accumulated_submission_time=58441.857051, global_step=127784, preemption_count=0, score=58441.857051, test/accuracy=0.562200, test/loss=1.896570, test/num_examples=10000, total_duration=64940.285795, train/accuracy=0.744258, train/loss=1.032115, validation/accuracy=0.689420, validation/loss=1.275559, validation/num_examples=50000
I0202 07:03:35.217472 139774434195200 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.040862560272217, loss=2.0220227241516113
I0202 07:04:17.735487 139774417409792 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.4366257190704346, loss=3.6275835037231445
I0202 07:05:03.786699 139774434195200 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.8305323123931885, loss=1.9477667808532715
I0202 07:05:50.370301 139774417409792 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.7754886150360107, loss=2.557908058166504
I0202 07:06:36.549300 139774434195200 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.804485321044922, loss=2.0020408630371094
I0202 07:07:22.858760 139774417409792 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.1670172214508057, loss=2.0142312049865723
I0202 07:08:09.111043 139774434195200 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.7063262462615967, loss=4.3496270179748535
I0202 07:08:55.387339 139774417409792 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.640768527984619, loss=3.425060987472534
I0202 07:09:41.776569 139774434195200 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.5418965816497803, loss=2.830326557159424
I0202 07:10:28.325274 139774417409792 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.4863085746765137, loss=3.3136825561523438
I0202 07:10:28.884087 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:10:40.893459 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:11:14.335670 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:11:15.934588 139936116377408 submission_runner.py:408] Time since start: 65407.79s, 	Step: 128703, 	{'train/accuracy': 0.7488671541213989, 'train/loss': 1.0089884996414185, 'validation/accuracy': 0.6900999546051025, 'validation/loss': 1.2736644744873047, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 1.8980499505996704, 'test/num_examples': 10000, 'score': 58862.215963602066, 'total_duration': 65407.78814959526, 'accumulated_submission_time': 58862.215963602066, 'accumulated_eval_time': 6533.1138525009155, 'accumulated_logging_time': 5.769906282424927}
I0202 07:11:15.969049 139774434195200 logging_writer.py:48] [128703] accumulated_eval_time=6533.113853, accumulated_logging_time=5.769906, accumulated_submission_time=58862.215964, global_step=128703, preemption_count=0, score=58862.215964, test/accuracy=0.562700, test/loss=1.898050, test/num_examples=10000, total_duration=65407.788150, train/accuracy=0.748867, train/loss=1.008988, validation/accuracy=0.690100, validation/loss=1.273664, validation/num_examples=50000
I0202 07:11:56.399460 139774417409792 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.0233473777770996, loss=1.863560676574707
I0202 07:12:42.801937 139774434195200 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.094539165496826, loss=2.2349636554718018
I0202 07:13:29.349329 139774417409792 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.7240219116210938, loss=3.1851024627685547
I0202 07:14:15.788352 139774434195200 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.740950584411621, loss=2.8942506313323975
I0202 07:15:01.847160 139774417409792 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.753110647201538, loss=2.6593143939971924
I0202 07:15:47.900196 139774434195200 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.0455589294433594, loss=1.944982886314392
I0202 07:16:34.498239 139774417409792 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.1258280277252197, loss=1.8767542839050293
I0202 07:17:20.702782 139774434195200 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.5650885105133057, loss=2.582392692565918
I0202 07:18:06.964163 139774417409792 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.061905860900879, loss=2.067837715148926
I0202 07:18:15.978091 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:18:28.080647 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:19:02.372210 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:19:03.964108 139936116377408 submission_runner.py:408] Time since start: 65875.82s, 	Step: 129621, 	{'train/accuracy': 0.7646874785423279, 'train/loss': 0.9320166707038879, 'validation/accuracy': 0.6924799680709839, 'validation/loss': 1.245804786682129, 'validation/num_examples': 50000, 'test/accuracy': 0.5667999982833862, 'test/loss': 1.8743077516555786, 'test/num_examples': 10000, 'score': 59282.16807126999, 'total_duration': 65875.81766748428, 'accumulated_submission_time': 59282.16807126999, 'accumulated_eval_time': 6581.099862098694, 'accumulated_logging_time': 5.814780950546265}
I0202 07:19:04.000902 139774434195200 logging_writer.py:48] [129621] accumulated_eval_time=6581.099862, accumulated_logging_time=5.814781, accumulated_submission_time=59282.168071, global_step=129621, preemption_count=0, score=59282.168071, test/accuracy=0.566800, test/loss=1.874308, test/num_examples=10000, total_duration=65875.817667, train/accuracy=0.764687, train/loss=0.932017, validation/accuracy=0.692480, validation/loss=1.245805, validation/num_examples=50000
I0202 07:19:36.243000 139774417409792 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.6064064502716064, loss=4.4643235206604
I0202 07:20:22.346243 139774434195200 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.9311671257019043, loss=1.8120472431182861
I0202 07:21:09.012412 139774417409792 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.8234081268310547, loss=4.48714017868042
I0202 07:21:55.467557 139774434195200 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.864053249359131, loss=2.569580554962158
I0202 07:22:41.617235 139774417409792 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.6872646808624268, loss=3.0814273357391357
I0202 07:23:28.169800 139774434195200 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.747135877609253, loss=3.5603199005126953
I0202 07:24:14.524215 139774417409792 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.2285218238830566, loss=1.9162107706069946
I0202 07:25:00.629976 139774434195200 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.8193087577819824, loss=3.765589475631714
I0202 07:25:46.882894 139774417409792 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.8698387145996094, loss=1.8584051132202148
I0202 07:26:04.377424 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:26:16.518410 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:26:52.224574 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:26:53.823797 139936116377408 submission_runner.py:408] Time since start: 66345.68s, 	Step: 130539, 	{'train/accuracy': 0.7518749833106995, 'train/loss': 0.9943748116493225, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.2452759742736816, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 1.879859447479248, 'test/num_examples': 10000, 'score': 59702.487728357315, 'total_duration': 66345.67736411095, 'accumulated_submission_time': 59702.487728357315, 'accumulated_eval_time': 6630.546304941177, 'accumulated_logging_time': 5.8607916831970215}
I0202 07:26:53.861654 139774434195200 logging_writer.py:48] [130539] accumulated_eval_time=6630.546305, accumulated_logging_time=5.860792, accumulated_submission_time=59702.487728, global_step=130539, preemption_count=0, score=59702.487728, test/accuracy=0.567100, test/loss=1.879859, test/num_examples=10000, total_duration=66345.677364, train/accuracy=0.751875, train/loss=0.994375, validation/accuracy=0.693720, validation/loss=1.245276, validation/num_examples=50000
I0202 07:27:18.474824 139774417409792 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.656163454055786, loss=3.7811505794525146
I0202 07:28:03.852648 139774434195200 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.9837915897369385, loss=1.884277582168579
I0202 07:28:50.112250 139774417409792 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.4890079498291016, loss=3.0796501636505127
I0202 07:29:36.706136 139774434195200 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.248410224914551, loss=3.905123710632324
I0202 07:30:22.676238 139774417409792 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.782349109649658, loss=1.851701259613037
I0202 07:31:08.792355 139774434195200 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.876370429992676, loss=3.457700252532959
I0202 07:31:55.004505 139774417409792 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.961634874343872, loss=1.9693639278411865
I0202 07:32:41.251023 139774434195200 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.16072154045105, loss=3.978379011154175
I0202 07:33:27.809479 139774417409792 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.097721815109253, loss=3.373805046081543
I0202 07:33:54.211850 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:34:06.535714 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:34:40.538941 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:34:42.136209 139936116377408 submission_runner.py:408] Time since start: 66813.99s, 	Step: 131459, 	{'train/accuracy': 0.7592187523841858, 'train/loss': 0.9559024572372437, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.223264455795288, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 1.8423492908477783, 'test/num_examples': 10000, 'score': 60122.77754402161, 'total_duration': 66813.98977065086, 'accumulated_submission_time': 60122.77754402161, 'accumulated_eval_time': 6678.470661401749, 'accumulated_logging_time': 5.9110212326049805}
I0202 07:34:42.173976 139774434195200 logging_writer.py:48] [131459] accumulated_eval_time=6678.470661, accumulated_logging_time=5.911021, accumulated_submission_time=60122.777544, global_step=131459, preemption_count=0, score=60122.777544, test/accuracy=0.573000, test/loss=1.842349, test/num_examples=10000, total_duration=66813.989771, train/accuracy=0.759219, train/loss=0.955902, validation/accuracy=0.698000, validation/loss=1.223264, validation/num_examples=50000
I0202 07:34:58.840280 139774417409792 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.9208173751831055, loss=4.309130668640137
I0202 07:35:42.583080 139774434195200 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.867274284362793, loss=2.2494959831237793
I0202 07:36:29.023964 139774417409792 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.8911595344543457, loss=1.8867863416671753
I0202 07:37:15.482385 139774434195200 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.01163649559021, loss=2.004437208175659
I0202 07:38:01.469055 139774417409792 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.0769264698028564, loss=1.8325599431991577
I0202 07:38:47.848190 139774434195200 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.3143060207366943, loss=2.0979857444763184
I0202 07:39:34.151405 139774417409792 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.128742218017578, loss=1.9653980731964111
I0202 07:40:20.323629 139774434195200 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.351997137069702, loss=4.109476089477539
I0202 07:41:06.661352 139774417409792 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.0488646030426025, loss=1.9421136379241943
I0202 07:41:42.440042 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:41:54.342279 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:42:30.458824 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:42:32.070243 139936116377408 submission_runner.py:408] Time since start: 67283.92s, 	Step: 132379, 	{'train/accuracy': 0.7625390291213989, 'train/loss': 0.9606295824050903, 'validation/accuracy': 0.6951199769973755, 'validation/loss': 1.261569857597351, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.8848967552185059, 'test/num_examples': 10000, 'score': 60542.986839056015, 'total_duration': 67283.923807621, 'accumulated_submission_time': 60542.986839056015, 'accumulated_eval_time': 6728.100863933563, 'accumulated_logging_time': 5.9576075077056885}
I0202 07:42:32.105959 139774434195200 logging_writer.py:48] [132379] accumulated_eval_time=6728.100864, accumulated_logging_time=5.957608, accumulated_submission_time=60542.986839, global_step=132379, preemption_count=0, score=60542.986839, test/accuracy=0.569900, test/loss=1.884897, test/num_examples=10000, total_duration=67283.923808, train/accuracy=0.762539, train/loss=0.960630, validation/accuracy=0.695120, validation/loss=1.261570, validation/num_examples=50000
I0202 07:42:40.861984 139774417409792 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.333085060119629, loss=1.9866100549697876
I0202 07:43:23.685257 139774434195200 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.1245479583740234, loss=2.134441614151001
I0202 07:44:09.812811 139774417409792 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.0173699855804443, loss=1.8244119882583618
I0202 07:44:56.619283 139774434195200 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.369722843170166, loss=1.8453409671783447
I0202 07:45:42.842911 139774417409792 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.775738477706909, loss=3.19309139251709
I0202 07:46:29.103519 139774434195200 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.04548716545105, loss=1.9659924507141113
I0202 07:47:15.434634 139774417409792 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.9765262603759766, loss=2.2184929847717285
I0202 07:48:01.748336 139774434195200 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.442096710205078, loss=1.9210063219070435
I0202 07:48:47.965273 139774417409792 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.13468599319458, loss=1.7897419929504395
I0202 07:49:32.367862 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:49:44.411635 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:50:18.213117 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:50:19.825509 139936116377408 submission_runner.py:408] Time since start: 67751.68s, 	Step: 133298, 	{'train/accuracy': 0.7527148127555847, 'train/loss': 0.9829521179199219, 'validation/accuracy': 0.7013199925422668, 'validation/loss': 1.2232753038406372, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.8487434387207031, 'test/num_examples': 10000, 'score': 60963.19054841995, 'total_duration': 67751.67906785011, 'accumulated_submission_time': 60963.19054841995, 'accumulated_eval_time': 6775.558528184891, 'accumulated_logging_time': 6.004410266876221}
I0202 07:50:19.864066 139774434195200 logging_writer.py:48] [133298] accumulated_eval_time=6775.558528, accumulated_logging_time=6.004410, accumulated_submission_time=60963.190548, global_step=133298, preemption_count=0, score=60963.190548, test/accuracy=0.572900, test/loss=1.848743, test/num_examples=10000, total_duration=67751.679068, train/accuracy=0.752715, train/loss=0.982952, validation/accuracy=0.701320, validation/loss=1.223275, validation/num_examples=50000
I0202 07:50:21.057996 139774417409792 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.7511935234069824, loss=3.0911712646484375
I0202 07:51:02.955859 139774434195200 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.1676223278045654, loss=1.9980864524841309
I0202 07:51:48.971447 139774417409792 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.263329029083252, loss=1.9794275760650635
I0202 07:52:35.456893 139774434195200 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.0398521423339844, loss=4.118260860443115
I0202 07:53:21.655176 139774417409792 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.0740127563476562, loss=3.5276827812194824
I0202 07:54:08.023937 139774434195200 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.25717830657959, loss=1.890881061553955
I0202 07:54:54.531223 139774417409792 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.9688613414764404, loss=1.9914183616638184
I0202 07:55:40.885949 139774434195200 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.7760376930236816, loss=2.4147229194641113
I0202 07:56:27.734793 139774417409792 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.8367197513580322, loss=2.4940237998962402
I0202 07:57:13.660156 139774434195200 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.4814491271972656, loss=2.8446500301361084
I0202 07:57:20.183456 139936116377408 spec.py:321] Evaluating on the training split.
I0202 07:57:32.290530 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 07:58:04.766448 139936116377408 spec.py:349] Evaluating on the test split.
I0202 07:58:06.368451 139936116377408 submission_runner.py:408] Time since start: 68218.22s, 	Step: 134216, 	{'train/accuracy': 0.7622656226158142, 'train/loss': 0.9357055425643921, 'validation/accuracy': 0.701259970664978, 'validation/loss': 1.2122747898101807, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.8352450132369995, 'test/num_examples': 10000, 'score': 61383.45324897766, 'total_duration': 68218.22201561928, 'accumulated_submission_time': 61383.45324897766, 'accumulated_eval_time': 6821.74352645874, 'accumulated_logging_time': 6.052812814712524}
I0202 07:58:06.407382 139774417409792 logging_writer.py:48] [134216] accumulated_eval_time=6821.743526, accumulated_logging_time=6.052813, accumulated_submission_time=61383.453249, global_step=134216, preemption_count=0, score=61383.453249, test/accuracy=0.575100, test/loss=1.835245, test/num_examples=10000, total_duration=68218.222016, train/accuracy=0.762266, train/loss=0.935706, validation/accuracy=0.701260, validation/loss=1.212275, validation/num_examples=50000
I0202 07:58:40.937881 139774434195200 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.3948917388916016, loss=4.283088207244873
I0202 07:59:26.979580 139774417409792 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.9756319522857666, loss=2.3238775730133057
I0202 08:00:13.956882 139774434195200 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.15915584564209, loss=2.1487622261047363
I0202 08:01:00.223896 139774417409792 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.2247731685638428, loss=1.843419075012207
I0202 08:01:46.897182 139774434195200 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.7718546390533447, loss=2.6637797355651855
I0202 08:02:33.380827 139774417409792 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.832493782043457, loss=2.910552978515625
I0202 08:03:20.045942 139774434195200 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.0586347579956055, loss=2.0752034187316895
I0202 08:04:06.308934 139774417409792 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.7501745223999023, loss=2.692401647567749
I0202 08:04:52.336003 139774434195200 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.201215982437134, loss=1.7973588705062866
I0202 08:05:06.379436 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:05:18.467337 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 08:05:51.779230 139936116377408 spec.py:349] Evaluating on the test split.
I0202 08:05:53.388250 139936116377408 submission_runner.py:408] Time since start: 68685.24s, 	Step: 135131, 	{'train/accuracy': 0.7727343440055847, 'train/loss': 0.8989397287368774, 'validation/accuracy': 0.7053200006484985, 'validation/loss': 1.198777675628662, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.8275467157363892, 'test/num_examples': 10000, 'score': 61803.36758232117, 'total_duration': 68685.2418153286, 'accumulated_submission_time': 61803.36758232117, 'accumulated_eval_time': 6868.752334356308, 'accumulated_logging_time': 6.101391077041626}
I0202 08:05:53.426082 139774417409792 logging_writer.py:48] [135131] accumulated_eval_time=6868.752334, accumulated_logging_time=6.101391, accumulated_submission_time=61803.367582, global_step=135131, preemption_count=0, score=61803.367582, test/accuracy=0.581500, test/loss=1.827547, test/num_examples=10000, total_duration=68685.241815, train/accuracy=0.772734, train/loss=0.898940, validation/accuracy=0.705320, validation/loss=1.198778, validation/num_examples=50000
I0202 08:06:21.245196 139774434195200 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.2844645977020264, loss=1.7502936124801636
I0202 08:07:06.821218 139774417409792 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.3209891319274902, loss=1.9829267263412476
I0202 08:07:53.349258 139774434195200 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.3415679931640625, loss=1.7598683834075928
I0202 08:08:39.999053 139774417409792 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.4811747074127197, loss=1.9568297863006592
I0202 08:09:26.051862 139774434195200 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.103369951248169, loss=2.0157670974731445
I0202 08:10:12.489950 139774417409792 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.2273151874542236, loss=2.163297176361084
I0202 08:10:58.765000 139774434195200 logging_writer.py:48] [135800] global_step=135800, grad_norm=4.190598487854004, loss=2.0061326026916504
I0202 08:11:45.113305 139774417409792 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.3219096660614014, loss=2.12288498878479
I0202 08:12:31.353570 139774434195200 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.1812551021575928, loss=2.4832663536071777
I0202 08:12:53.682044 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:13:05.988375 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 08:13:41.022797 139936116377408 spec.py:349] Evaluating on the test split.
I0202 08:13:42.643721 139936116377408 submission_runner.py:408] Time since start: 69154.50s, 	Step: 136050, 	{'train/accuracy': 0.77685546875, 'train/loss': 0.8904039859771729, 'validation/accuracy': 0.7037000060081482, 'validation/loss': 1.207800030708313, 'validation/num_examples': 50000, 'test/accuracy': 0.5791000127792358, 'test/loss': 1.836230993270874, 'test/num_examples': 10000, 'score': 62223.56702518463, 'total_duration': 69154.49729061127, 'accumulated_submission_time': 62223.56702518463, 'accumulated_eval_time': 6917.71401143074, 'accumulated_logging_time': 6.14823055267334}
I0202 08:13:42.680777 139774417409792 logging_writer.py:48] [136050] accumulated_eval_time=6917.714011, accumulated_logging_time=6.148231, accumulated_submission_time=62223.567025, global_step=136050, preemption_count=0, score=62223.567025, test/accuracy=0.579100, test/loss=1.836231, test/num_examples=10000, total_duration=69154.497291, train/accuracy=0.776855, train/loss=0.890404, validation/accuracy=0.703700, validation/loss=1.207800, validation/num_examples=50000
I0202 08:14:02.936769 139774434195200 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.0387609004974365, loss=2.447315216064453
I0202 08:14:47.381059 139774417409792 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.5132927894592285, loss=1.8551435470581055
I0202 08:15:33.593698 139774434195200 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.0772476196289062, loss=3.093696117401123
I0202 08:16:20.301321 139774417409792 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.2206203937530518, loss=1.8176615238189697
I0202 08:17:06.231917 139774434195200 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.604739189147949, loss=1.88640296459198
I0202 08:17:52.333613 139774417409792 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.083892583847046, loss=1.8083438873291016
I0202 08:18:38.705321 139774434195200 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.4542009830474854, loss=1.8503904342651367
I0202 08:19:24.986161 139774417409792 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.2662055492401123, loss=1.955428123474121
I0202 08:20:11.257631 139774434195200 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.294370174407959, loss=1.7820855379104614
I0202 08:20:42.799649 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:20:54.964026 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 08:21:29.977245 139936116377408 spec.py:349] Evaluating on the test split.
I0202 08:21:31.579191 139936116377408 submission_runner.py:408] Time since start: 69623.43s, 	Step: 136970, 	{'train/accuracy': 0.7667577862739563, 'train/loss': 0.9236660003662109, 'validation/accuracy': 0.7049799561500549, 'validation/loss': 1.1936697959899902, 'validation/num_examples': 50000, 'test/accuracy': 0.579300045967102, 'test/loss': 1.815743088722229, 'test/num_examples': 10000, 'score': 62643.629824876785, 'total_duration': 69623.43274188042, 'accumulated_submission_time': 62643.629824876785, 'accumulated_eval_time': 6966.49352145195, 'accumulated_logging_time': 6.19426703453064}
I0202 08:21:31.618624 139774417409792 logging_writer.py:48] [136970] accumulated_eval_time=6966.493521, accumulated_logging_time=6.194267, accumulated_submission_time=62643.629825, global_step=136970, preemption_count=0, score=62643.629825, test/accuracy=0.579300, test/loss=1.815743, test/num_examples=10000, total_duration=69623.432742, train/accuracy=0.766758, train/loss=0.923666, validation/accuracy=0.704980, validation/loss=1.193670, validation/num_examples=50000
I0202 08:21:43.924557 139774434195200 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.3791756629943848, loss=1.743605375289917
I0202 08:22:27.559476 139774417409792 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.9767346382141113, loss=3.611105442047119
I0202 08:23:13.292294 139774434195200 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.2603564262390137, loss=1.9917141199111938
I0202 08:23:59.357978 139774417409792 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.619051694869995, loss=1.8371334075927734
I0202 08:24:45.628725 139774434195200 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.4431941509246826, loss=1.9215956926345825
I0202 08:25:31.854323 139774417409792 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.3353917598724365, loss=3.955458641052246
I0202 08:26:18.225958 139774434195200 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2249913215637207, loss=1.9866020679473877
I0202 08:27:04.880664 139774417409792 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.1931958198547363, loss=1.7867077589035034
I0202 08:27:50.949894 139774434195200 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.5042366981506348, loss=1.7613487243652344
I0202 08:28:31.837664 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:28:43.946420 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 08:29:17.277214 139936116377408 spec.py:349] Evaluating on the test split.
I0202 08:29:18.883412 139936116377408 submission_runner.py:408] Time since start: 70090.74s, 	Step: 137890, 	{'train/accuracy': 0.772753894329071, 'train/loss': 0.9023522138595581, 'validation/accuracy': 0.706279993057251, 'validation/loss': 1.1953895092010498, 'validation/num_examples': 50000, 'test/accuracy': 0.5799000263214111, 'test/loss': 1.819128155708313, 'test/num_examples': 10000, 'score': 63063.79121661186, 'total_duration': 70090.73695373535, 'accumulated_submission_time': 63063.79121661186, 'accumulated_eval_time': 7013.5392434597015, 'accumulated_logging_time': 6.243442058563232}
I0202 08:29:18.927502 139774417409792 logging_writer.py:48] [137890] accumulated_eval_time=7013.539243, accumulated_logging_time=6.243442, accumulated_submission_time=63063.791217, global_step=137890, preemption_count=0, score=63063.791217, test/accuracy=0.579900, test/loss=1.819128, test/num_examples=10000, total_duration=70090.736954, train/accuracy=0.772754, train/loss=0.902352, validation/accuracy=0.706280, validation/loss=1.195390, validation/num_examples=50000
I0202 08:29:23.299005 139774434195200 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.327000379562378, loss=1.781899333000183
I0202 08:30:05.602727 139774417409792 logging_writer.py:48] [138000] global_step=138000, grad_norm=4.020118236541748, loss=1.8584542274475098
I0202 08:30:51.480492 139774434195200 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.118370532989502, loss=3.0574569702148438
I0202 08:31:38.070869 139774417409792 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.2880101203918457, loss=1.8074301481246948
I0202 08:32:24.489830 139774434195200 logging_writer.py:48] [138300] global_step=138300, grad_norm=4.2461137771606445, loss=1.7283143997192383
I0202 08:33:10.902106 139774417409792 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.0998318195343018, loss=3.9576892852783203
I0202 08:33:57.140022 139774434195200 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.221895933151245, loss=1.8581185340881348
I0202 08:34:43.375679 139774417409792 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.337217330932617, loss=1.8517266511917114
I0202 08:35:29.859716 139774434195200 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.140495538711548, loss=1.736415147781372
I0202 08:36:16.180230 139774417409792 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.272465229034424, loss=1.8065481185913086
I0202 08:36:19.175594 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:36:31.317911 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 08:37:06.865432 139936116377408 spec.py:349] Evaluating on the test split.
I0202 08:37:08.463072 139936116377408 submission_runner.py:408] Time since start: 70560.32s, 	Step: 138808, 	{'train/accuracy': 0.78382807970047, 'train/loss': 0.8481549024581909, 'validation/accuracy': 0.7096999883651733, 'validation/loss': 1.173912525177002, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.7924264669418335, 'test/num_examples': 10000, 'score': 63483.9806933403, 'total_duration': 70560.31663990021, 'accumulated_submission_time': 63483.9806933403, 'accumulated_eval_time': 7062.826720952988, 'accumulated_logging_time': 6.297896862030029}
I0202 08:37:08.501766 139774434195200 logging_writer.py:48] [138808] accumulated_eval_time=7062.826721, accumulated_logging_time=6.297897, accumulated_submission_time=63483.980693, global_step=138808, preemption_count=0, score=63483.980693, test/accuracy=0.578800, test/loss=1.792426, test/num_examples=10000, total_duration=70560.316640, train/accuracy=0.783828, train/loss=0.848155, validation/accuracy=0.709700, validation/loss=1.173913, validation/num_examples=50000
I0202 08:37:46.675977 139774417409792 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.5972447395324707, loss=1.8564119338989258
I0202 08:38:32.716986 139774434195200 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.3805549144744873, loss=3.365273952484131
I0202 08:39:19.393594 139774417409792 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.2053983211517334, loss=1.7659320831298828
I0202 08:40:05.783697 139774434195200 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.4638047218322754, loss=1.7858946323394775
I0202 08:40:51.737277 139774417409792 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.5944442749023438, loss=2.1603810787200928
I0202 08:41:38.311778 139774434195200 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.918123245239258, loss=3.432846784591675
I0202 08:42:24.883512 139774417409792 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.180119752883911, loss=3.881307601928711
I0202 08:43:11.154495 139774434195200 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.4924416542053223, loss=1.8586044311523438
I0202 08:43:57.225275 139774417409792 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.4492385387420654, loss=1.7871955633163452
I0202 08:44:08.542979 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:44:20.738569 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 08:44:54.125422 139936116377408 spec.py:349] Evaluating on the test split.
I0202 08:44:55.722829 139936116377408 submission_runner.py:408] Time since start: 71027.58s, 	Step: 139726, 	{'train/accuracy': 0.7791601419448853, 'train/loss': 0.8667029142379761, 'validation/accuracy': 0.7123000025749207, 'validation/loss': 1.1546515226364136, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 1.7686362266540527, 'test/num_examples': 10000, 'score': 63903.965514183044, 'total_duration': 71027.57639861107, 'accumulated_submission_time': 63903.965514183044, 'accumulated_eval_time': 7110.006555318832, 'accumulated_logging_time': 6.345866918563843}
I0202 08:44:55.762250 139774434195200 logging_writer.py:48] [139726] accumulated_eval_time=7110.006555, accumulated_logging_time=6.345867, accumulated_submission_time=63903.965514, global_step=139726, preemption_count=0, score=63903.965514, test/accuracy=0.589700, test/loss=1.768636, test/num_examples=10000, total_duration=71027.576399, train/accuracy=0.779160, train/loss=0.866703, validation/accuracy=0.712300, validation/loss=1.154652, validation/num_examples=50000
I0202 08:45:25.757099 139774417409792 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.373818874359131, loss=3.286008834838867
I0202 08:46:11.831602 139774434195200 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.249448299407959, loss=2.3187334537506104
I0202 08:46:58.629638 139774417409792 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.1214687824249268, loss=3.108238458633423
I0202 08:47:45.065925 139774434195200 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.636518955230713, loss=1.6402900218963623
I0202 08:48:31.654439 139774417409792 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.5964810848236084, loss=1.7616115808486938
I0202 08:49:17.975019 139774434195200 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.5784125328063965, loss=1.9084703922271729
I0202 08:50:04.451366 139774417409792 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.2273600101470947, loss=3.6435141563415527
I0202 08:50:50.870907 139774434195200 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.106891632080078, loss=2.778686046600342
I0202 08:51:37.240652 139774417409792 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.571978807449341, loss=1.837318778038025
I0202 08:51:56.246912 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:52:08.557260 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 08:52:43.361184 139936116377408 spec.py:349] Evaluating on the test split.
I0202 08:52:44.969675 139936116377408 submission_runner.py:408] Time since start: 71496.82s, 	Step: 140642, 	{'train/accuracy': 0.7782421708106995, 'train/loss': 0.8562284708023071, 'validation/accuracy': 0.7149199843406677, 'validation/loss': 1.147608757019043, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.7629472017288208, 'test/num_examples': 10000, 'score': 64324.39351463318, 'total_duration': 71496.82323789597, 'accumulated_submission_time': 64324.39351463318, 'accumulated_eval_time': 7158.72931265831, 'accumulated_logging_time': 6.395110607147217}
I0202 08:52:45.010529 139774434195200 logging_writer.py:48] [140642] accumulated_eval_time=7158.729313, accumulated_logging_time=6.395111, accumulated_submission_time=64324.393515, global_step=140642, preemption_count=0, score=64324.393515, test/accuracy=0.590600, test/loss=1.762947, test/num_examples=10000, total_duration=71496.823238, train/accuracy=0.778242, train/loss=0.856228, validation/accuracy=0.714920, validation/loss=1.147609, validation/num_examples=50000
I0202 08:53:08.451783 139774417409792 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.412287950515747, loss=1.8659639358520508
I0202 08:53:53.636294 139774434195200 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.5921149253845215, loss=1.8447096347808838
I0202 08:54:39.714309 139774417409792 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.7368597984313965, loss=1.8175578117370605
I0202 08:55:26.340296 139774434195200 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.2275683879852295, loss=3.8187203407287598
I0202 08:56:12.627619 139774417409792 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.0276310443878174, loss=2.2525241374969482
I0202 08:56:58.796132 139774434195200 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.570251941680908, loss=1.8181636333465576
I0202 08:57:45.322105 139774417409792 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.663586378097534, loss=1.7210451364517212
I0202 08:58:31.874252 139774434195200 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.442101240158081, loss=1.8341596126556396
I0202 08:59:18.072558 139774417409792 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.3401005268096924, loss=3.8313584327697754
I0202 08:59:45.348801 139936116377408 spec.py:321] Evaluating on the training split.
I0202 08:59:58.180304 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:00:31.545009 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:00:33.147535 139936116377408 submission_runner.py:408] Time since start: 71965.00s, 	Step: 141561, 	{'train/accuracy': 0.7863867282867432, 'train/loss': 0.8447074890136719, 'validation/accuracy': 0.7131199836730957, 'validation/loss': 1.16819167137146, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.8064215183258057, 'test/num_examples': 10000, 'score': 64744.67516851425, 'total_duration': 71965.00109434128, 'accumulated_submission_time': 64744.67516851425, 'accumulated_eval_time': 7206.5280418396, 'accumulated_logging_time': 6.445488691329956}
I0202 09:00:33.185381 139774434195200 logging_writer.py:48] [141561] accumulated_eval_time=7206.528042, accumulated_logging_time=6.445489, accumulated_submission_time=64744.675169, global_step=141561, preemption_count=0, score=64744.675169, test/accuracy=0.588200, test/loss=1.806422, test/num_examples=10000, total_duration=71965.001094, train/accuracy=0.786387, train/loss=0.844707, validation/accuracy=0.713120, validation/loss=1.168192, validation/num_examples=50000
I0202 09:00:49.084156 139774417409792 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.642706871032715, loss=1.7619547843933105
I0202 09:01:32.993285 139774434195200 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.636693000793457, loss=1.9006900787353516
I0202 09:02:19.700804 139774417409792 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.52846360206604, loss=2.731721878051758
I0202 09:03:06.450218 139774434195200 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.561645030975342, loss=1.7537590265274048
I0202 09:03:52.819952 139774417409792 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.1043200492858887, loss=3.381178617477417
I0202 09:04:39.253746 139774434195200 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.2122910022735596, loss=3.334451675415039
I0202 09:05:25.576857 139774417409792 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.5304884910583496, loss=1.8301613330841064
I0202 09:06:12.012979 139774434195200 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.8151373863220215, loss=1.8286348581314087
I0202 09:06:59.611520 139774417409792 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.753415822982788, loss=3.962430953979492
I0202 09:07:33.256249 139936116377408 spec.py:321] Evaluating on the training split.
I0202 09:07:45.480144 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:08:20.452692 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:08:22.053834 139936116377408 submission_runner.py:408] Time since start: 72433.91s, 	Step: 142474, 	{'train/accuracy': 0.7731249928474426, 'train/loss': 0.8849400281906128, 'validation/accuracy': 0.7135399580001831, 'validation/loss': 1.1557042598724365, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.762101411819458, 'test/num_examples': 10000, 'score': 65164.6888320446, 'total_duration': 72433.90739750862, 'accumulated_submission_time': 65164.6888320446, 'accumulated_eval_time': 7255.32564163208, 'accumulated_logging_time': 6.493313312530518}
I0202 09:08:22.091537 139774434195200 logging_writer.py:48] [142474] accumulated_eval_time=7255.325642, accumulated_logging_time=6.493313, accumulated_submission_time=65164.688832, global_step=142474, preemption_count=0, score=65164.688832, test/accuracy=0.591400, test/loss=1.762101, test/num_examples=10000, total_duration=72433.907398, train/accuracy=0.773125, train/loss=0.884940, validation/accuracy=0.713540, validation/loss=1.155704, validation/num_examples=50000
I0202 09:08:32.821374 139774417409792 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.3521883487701416, loss=3.084890365600586
I0202 09:09:16.059841 139774434195200 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.6284494400024414, loss=1.7534385919570923
I0202 09:10:02.644854 139774417409792 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.9023659229278564, loss=1.7152663469314575
I0202 09:10:49.158747 139774434195200 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.9252169132232666, loss=3.2487642765045166
I0202 09:11:35.542491 139774417409792 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.7694056034088135, loss=1.8684486150741577
I0202 09:12:22.001570 139774434195200 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.4207801818847656, loss=3.8118157386779785
I0202 09:13:08.572370 139774417409792 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.761739730834961, loss=2.0321640968322754
I0202 09:13:54.849313 139774434195200 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.7086539268493652, loss=3.6191484928131104
I0202 09:14:41.468593 139774417409792 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.5380446910858154, loss=2.117277145385742
I0202 09:15:22.505750 139936116377408 spec.py:321] Evaluating on the training split.
I0202 09:15:34.553819 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:16:11.026633 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:16:12.643898 139936116377408 submission_runner.py:408] Time since start: 72904.50s, 	Step: 143390, 	{'train/accuracy': 0.7816015481948853, 'train/loss': 0.868894100189209, 'validation/accuracy': 0.716219961643219, 'validation/loss': 1.1561012268066406, 'validation/num_examples': 50000, 'test/accuracy': 0.5877000093460083, 'test/loss': 1.783122181892395, 'test/num_examples': 10000, 'score': 65585.04565286636, 'total_duration': 72904.4974284172, 'accumulated_submission_time': 65585.04565286636, 'accumulated_eval_time': 7305.463745594025, 'accumulated_logging_time': 6.540487051010132}
I0202 09:16:12.688904 139774434195200 logging_writer.py:48] [143390] accumulated_eval_time=7305.463746, accumulated_logging_time=6.540487, accumulated_submission_time=65585.045653, global_step=143390, preemption_count=0, score=65585.045653, test/accuracy=0.587700, test/loss=1.783122, test/num_examples=10000, total_duration=72904.497428, train/accuracy=0.781602, train/loss=0.868894, validation/accuracy=0.716220, validation/loss=1.156101, validation/num_examples=50000
I0202 09:16:17.063755 139774417409792 logging_writer.py:48] [143400] global_step=143400, grad_norm=4.044437885284424, loss=1.7896775007247925
I0202 09:16:59.259899 139774434195200 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.8727071285247803, loss=1.790682315826416
I0202 09:17:45.189685 139774417409792 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.175410032272339, loss=2.546184778213501
I0202 09:18:31.681131 139774434195200 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.576348304748535, loss=2.029911756515503
I0202 09:19:17.916304 139774417409792 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.9817633628845215, loss=1.9335626363754272
I0202 09:20:04.399377 139774434195200 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.6819214820861816, loss=1.7024574279785156
I0202 09:20:50.693345 139774417409792 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.86590576171875, loss=1.9182136058807373
I0202 09:21:36.875205 139774434195200 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.5539228916168213, loss=3.500196695327759
I0202 09:22:23.533719 139774417409792 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.5443131923675537, loss=2.3855788707733154
I0202 09:23:09.795077 139774434195200 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.4657399654388428, loss=2.1308162212371826
I0202 09:23:12.660541 139936116377408 spec.py:321] Evaluating on the training split.
I0202 09:23:24.743287 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:23:59.694558 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:24:01.305693 139936116377408 submission_runner.py:408] Time since start: 73373.16s, 	Step: 144308, 	{'train/accuracy': 0.79212886095047, 'train/loss': 0.8323768377304077, 'validation/accuracy': 0.7192999720573425, 'validation/loss': 1.145952582359314, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.7589880228042603, 'test/num_examples': 10000, 'score': 66004.958309412, 'total_duration': 73373.15925145149, 'accumulated_submission_time': 66004.958309412, 'accumulated_eval_time': 7354.108882188797, 'accumulated_logging_time': 6.597029685974121}
I0202 09:24:01.342997 139774417409792 logging_writer.py:48] [144308] accumulated_eval_time=7354.108882, accumulated_logging_time=6.597030, accumulated_submission_time=66004.958309, global_step=144308, preemption_count=0, score=66004.958309, test/accuracy=0.592900, test/loss=1.758988, test/num_examples=10000, total_duration=73373.159251, train/accuracy=0.792129, train/loss=0.832377, validation/accuracy=0.719300, validation/loss=1.145953, validation/num_examples=50000
I0202 09:24:39.568074 139774434195200 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.6535301208496094, loss=1.683348536491394
I0202 09:25:25.440098 139774417409792 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.6887142658233643, loss=1.887420415878296
I0202 09:26:11.853864 139774434195200 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.7478959560394287, loss=3.103350877761841
I0202 09:26:57.793376 139774417409792 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.345968723297119, loss=2.434375762939453
I0202 09:27:43.978611 139774434195200 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.4578847885131836, loss=2.133045196533203
I0202 09:28:30.417815 139774417409792 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.72878360748291, loss=2.8827908039093018
I0202 09:29:16.728090 139774434195200 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.9394586086273193, loss=2.3282885551452637
I0202 09:30:02.908672 139774417409792 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.639516830444336, loss=2.0613420009613037
I0202 09:30:49.397419 139774434195200 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.973480224609375, loss=1.9166698455810547
I0202 09:31:01.828172 139936116377408 spec.py:321] Evaluating on the training split.
I0202 09:31:13.790602 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:31:47.892622 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:31:49.498605 139936116377408 submission_runner.py:408] Time since start: 73841.35s, 	Step: 145228, 	{'train/accuracy': 0.7823046445846558, 'train/loss': 0.8637334108352661, 'validation/accuracy': 0.7188799977302551, 'validation/loss': 1.1443239450454712, 'validation/num_examples': 50000, 'test/accuracy': 0.593000054359436, 'test/loss': 1.7642781734466553, 'test/num_examples': 10000, 'score': 66425.3852956295, 'total_duration': 73841.35216474533, 'accumulated_submission_time': 66425.3852956295, 'accumulated_eval_time': 7401.779304265976, 'accumulated_logging_time': 6.644883871078491}
I0202 09:31:49.535474 139774417409792 logging_writer.py:48] [145228] accumulated_eval_time=7401.779304, accumulated_logging_time=6.644884, accumulated_submission_time=66425.385296, global_step=145228, preemption_count=0, score=66425.385296, test/accuracy=0.593000, test/loss=1.764278, test/num_examples=10000, total_duration=73841.352165, train/accuracy=0.782305, train/loss=0.863733, validation/accuracy=0.718880, validation/loss=1.144324, validation/num_examples=50000
I0202 09:32:18.719007 139774434195200 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.852431535720825, loss=2.0423126220703125
I0202 09:33:04.664882 139774417409792 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.618041515350342, loss=3.5588173866271973
I0202 09:33:50.704245 139774434195200 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.578580856323242, loss=2.204042673110962
I0202 09:34:37.002816 139774417409792 logging_writer.py:48] [145600] global_step=145600, grad_norm=4.368666172027588, loss=4.197725772857666
I0202 09:35:23.013449 139774434195200 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.592496156692505, loss=1.801222562789917
I0202 09:36:09.272851 139774417409792 logging_writer.py:48] [145800] global_step=145800, grad_norm=4.129752159118652, loss=1.6668798923492432
I0202 09:36:55.448037 139774434195200 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.680180311203003, loss=1.734081745147705
I0202 09:37:41.557872 139774417409792 logging_writer.py:48] [146000] global_step=146000, grad_norm=4.029292106628418, loss=1.7259175777435303
I0202 09:38:27.831379 139774434195200 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.65040922164917, loss=2.964160919189453
I0202 09:38:49.862696 139936116377408 spec.py:321] Evaluating on the training split.
I0202 09:39:01.831041 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:39:38.526630 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:39:40.158459 139936116377408 submission_runner.py:408] Time since start: 74312.01s, 	Step: 146149, 	{'train/accuracy': 0.7882617115974426, 'train/loss': 0.8386130928993225, 'validation/accuracy': 0.7200599908828735, 'validation/loss': 1.130061388015747, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.7559378147125244, 'test/num_examples': 10000, 'score': 66845.65575814247, 'total_duration': 74312.01202607155, 'accumulated_submission_time': 66845.65575814247, 'accumulated_eval_time': 7452.07506108284, 'accumulated_logging_time': 6.691014051437378}
I0202 09:39:40.197801 139774417409792 logging_writer.py:48] [146149] accumulated_eval_time=7452.075061, accumulated_logging_time=6.691014, accumulated_submission_time=66845.655758, global_step=146149, preemption_count=0, score=66845.655758, test/accuracy=0.592300, test/loss=1.755938, test/num_examples=10000, total_duration=74312.012026, train/accuracy=0.788262, train/loss=0.838613, validation/accuracy=0.720060, validation/loss=1.130061, validation/num_examples=50000
I0202 09:40:00.844701 139774434195200 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.5997095108032227, loss=3.963993549346924
I0202 09:40:45.422405 139774417409792 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.960226058959961, loss=1.5498104095458984
I0202 09:41:32.128500 139774434195200 logging_writer.py:48] [146400] global_step=146400, grad_norm=4.079672813415527, loss=4.152149677276611
I0202 09:42:18.504564 139774417409792 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.964251756668091, loss=1.6579560041427612
I0202 09:43:04.585844 139774434195200 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.921752691268921, loss=3.892364025115967
I0202 09:43:50.546059 139774417409792 logging_writer.py:48] [146700] global_step=146700, grad_norm=4.638708114624023, loss=1.770700216293335
I0202 09:44:36.795668 139774434195200 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.2014851570129395, loss=1.6529018878936768
I0202 09:45:22.905388 139774417409792 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.705106019973755, loss=1.6287009716033936
I0202 09:46:08.985716 139774434195200 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.9755609035491943, loss=2.0117709636688232
I0202 09:46:40.394325 139936116377408 spec.py:321] Evaluating on the training split.
I0202 09:46:52.514364 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:47:27.920026 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:47:29.530122 139936116377408 submission_runner.py:408] Time since start: 74781.38s, 	Step: 147070, 	{'train/accuracy': 0.7914062142372131, 'train/loss': 0.8205158710479736, 'validation/accuracy': 0.7212799787521362, 'validation/loss': 1.122092366218567, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.7555046081542969, 'test/num_examples': 10000, 'score': 67265.79447817802, 'total_duration': 74781.38368654251, 'accumulated_submission_time': 67265.79447817802, 'accumulated_eval_time': 7501.210858821869, 'accumulated_logging_time': 6.740800857543945}
I0202 09:47:29.572785 139774417409792 logging_writer.py:48] [147070] accumulated_eval_time=7501.210859, accumulated_logging_time=6.740801, accumulated_submission_time=67265.794478, global_step=147070, preemption_count=0, score=67265.794478, test/accuracy=0.591000, test/loss=1.755505, test/num_examples=10000, total_duration=74781.383687, train/accuracy=0.791406, train/loss=0.820516, validation/accuracy=0.721280, validation/loss=1.122092, validation/num_examples=50000
I0202 09:47:41.881476 139774434195200 logging_writer.py:48] [147100] global_step=147100, grad_norm=4.117861747741699, loss=1.664947509765625
I0202 09:48:25.019899 139774417409792 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.711294412612915, loss=2.4386820793151855
I0202 09:49:11.331794 139774434195200 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.979240655899048, loss=1.640967845916748
I0202 09:49:57.532343 139774417409792 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.3311262130737305, loss=2.745150327682495
I0202 09:50:43.675582 139774434195200 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.275861740112305, loss=3.616119384765625
I0202 09:51:29.924944 139774417409792 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.6616194248199463, loss=2.702345371246338
I0202 09:52:16.671628 139774434195200 logging_writer.py:48] [147700] global_step=147700, grad_norm=4.071072101593018, loss=2.216693162918091
I0202 09:53:02.770492 139774417409792 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.9761803150177, loss=1.733608603477478
I0202 09:53:48.604063 139774434195200 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.632931709289551, loss=1.607179880142212
I0202 09:54:29.816220 139936116377408 spec.py:321] Evaluating on the training split.
I0202 09:54:41.822216 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 09:55:17.346289 139936116377408 spec.py:349] Evaluating on the test split.
I0202 09:55:18.954753 139936116377408 submission_runner.py:408] Time since start: 75250.81s, 	Step: 147991, 	{'train/accuracy': 0.8050585985183716, 'train/loss': 0.7596969604492188, 'validation/accuracy': 0.724079966545105, 'validation/loss': 1.102674126625061, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.719403624534607, 'test/num_examples': 10000, 'score': 67685.98072981834, 'total_duration': 75250.80831742287, 'accumulated_submission_time': 67685.98072981834, 'accumulated_eval_time': 7550.349389076233, 'accumulated_logging_time': 6.793242692947388}
I0202 09:55:18.996409 139774417409792 logging_writer.py:48] [147991] accumulated_eval_time=7550.349389, accumulated_logging_time=6.793243, accumulated_submission_time=67685.980730, global_step=147991, preemption_count=0, score=67685.980730, test/accuracy=0.605600, test/loss=1.719404, test/num_examples=10000, total_duration=75250.808317, train/accuracy=0.805059, train/loss=0.759697, validation/accuracy=0.724080, validation/loss=1.102674, validation/num_examples=50000
I0202 09:55:22.978337 139774434195200 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.219878673553467, loss=2.064720630645752
I0202 09:56:05.253051 139774417409792 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.944647789001465, loss=3.578049659729004
I0202 09:56:51.156195 139774434195200 logging_writer.py:48] [148200] global_step=148200, grad_norm=4.323690891265869, loss=1.6298139095306396
I0202 09:57:37.795636 139774417409792 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.6640889644622803, loss=1.7288472652435303
I0202 09:58:24.012241 139774434195200 logging_writer.py:48] [148400] global_step=148400, grad_norm=4.2917962074279785, loss=1.7237181663513184
I0202 09:59:09.994802 139774417409792 logging_writer.py:48] [148500] global_step=148500, grad_norm=4.076488494873047, loss=3.8657312393188477
I0202 09:59:56.316900 139774434195200 logging_writer.py:48] [148600] global_step=148600, grad_norm=4.3874664306640625, loss=1.812010645866394
I0202 10:00:42.760211 139774417409792 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.7566535472869873, loss=2.089517593383789
I0202 10:01:28.614212 139774434195200 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.401803970336914, loss=1.865342617034912
I0202 10:02:15.462064 139774417409792 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.9679758548736572, loss=2.215587854385376
I0202 10:02:19.338060 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:02:31.437907 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:03:07.469095 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:03:09.074319 139936116377408 submission_runner.py:408] Time since start: 75720.93s, 	Step: 148910, 	{'train/accuracy': 0.79408198595047, 'train/loss': 0.8039225339889526, 'validation/accuracy': 0.7251399755477905, 'validation/loss': 1.0975062847137451, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.7171880006790161, 'test/num_examples': 10000, 'score': 68106.26304864883, 'total_duration': 75720.92788505554, 'accumulated_submission_time': 68106.26304864883, 'accumulated_eval_time': 7600.08563709259, 'accumulated_logging_time': 6.847251892089844}
I0202 10:03:09.113361 139774434195200 logging_writer.py:48] [148910] accumulated_eval_time=7600.085637, accumulated_logging_time=6.847252, accumulated_submission_time=68106.263049, global_step=148910, preemption_count=0, score=68106.263049, test/accuracy=0.597700, test/loss=1.717188, test/num_examples=10000, total_duration=75720.927885, train/accuracy=0.794082, train/loss=0.803923, validation/accuracy=0.725140, validation/loss=1.097506, validation/num_examples=50000
I0202 10:03:46.394145 139774417409792 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.729565143585205, loss=3.9646544456481934
I0202 10:04:32.240725 139774434195200 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.8374617099761963, loss=1.734020471572876
I0202 10:05:18.830752 139774417409792 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.7590510845184326, loss=1.96781587600708
I0202 10:06:04.922513 139774434195200 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.6103856563568115, loss=2.6691231727600098
I0202 10:06:50.847189 139774417409792 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.6830241680145264, loss=2.2415363788604736
I0202 10:07:36.941614 139774434195200 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.1444621086120605, loss=3.666297197341919
I0202 10:08:23.391585 139774417409792 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.9787604808807373, loss=3.24356746673584
I0202 10:09:09.421432 139774434195200 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.44508171081543, loss=1.8441828489303589
I0202 10:09:55.713864 139774417409792 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.154763221740723, loss=1.57899010181427
I0202 10:10:09.498140 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:10:21.523357 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:10:57.048695 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:10:58.654340 139936116377408 submission_runner.py:408] Time since start: 76190.51s, 	Step: 149831, 	{'train/accuracy': 0.7937890291213989, 'train/loss': 0.8100752830505371, 'validation/accuracy': 0.7240999937057495, 'validation/loss': 1.1172319650650024, 'validation/num_examples': 50000, 'test/accuracy': 0.5993000268936157, 'test/loss': 1.7362432479858398, 'test/num_examples': 10000, 'score': 68526.58949446678, 'total_duration': 76190.50789570808, 'accumulated_submission_time': 68526.58949446678, 'accumulated_eval_time': 7649.241825342178, 'accumulated_logging_time': 6.897162199020386}
I0202 10:10:58.695883 139774434195200 logging_writer.py:48] [149831] accumulated_eval_time=7649.241825, accumulated_logging_time=6.897162, accumulated_submission_time=68526.589494, global_step=149831, preemption_count=0, score=68526.589494, test/accuracy=0.599300, test/loss=1.736243, test/num_examples=10000, total_duration=76190.507896, train/accuracy=0.793789, train/loss=0.810075, validation/accuracy=0.724100, validation/loss=1.117232, validation/num_examples=50000
I0202 10:11:26.506174 139774417409792 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.049916744232178, loss=1.741090178489685
I0202 10:12:12.717380 139774434195200 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.162110805511475, loss=1.8021661043167114
I0202 10:12:58.755725 139774417409792 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.446594715118408, loss=1.6708346605300903
I0202 10:13:45.244431 139774434195200 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.41873836517334, loss=3.583740472793579
I0202 10:14:31.259437 139774417409792 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.550783157348633, loss=3.9946885108947754
I0202 10:15:17.571109 139774434195200 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.181277275085449, loss=1.8424979448318481
I0202 10:16:03.565800 139774417409792 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.007715225219727, loss=1.7202777862548828
I0202 10:16:49.593108 139774434195200 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.129706859588623, loss=1.9085769653320312
I0202 10:17:35.705171 139774417409792 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.358757495880127, loss=3.598065137863159
I0202 10:17:58.880142 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:18:11.102271 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:18:45.850114 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:18:47.462275 139936116377408 submission_runner.py:408] Time since start: 76659.32s, 	Step: 150752, 	{'train/accuracy': 0.8075780868530273, 'train/loss': 0.7453005909919739, 'validation/accuracy': 0.7288599610328674, 'validation/loss': 1.078874945640564, 'validation/num_examples': 50000, 'test/accuracy': 0.6023000478744507, 'test/loss': 1.6958317756652832, 'test/num_examples': 10000, 'score': 68946.71733403206, 'total_duration': 76659.31583476067, 'accumulated_submission_time': 68946.71733403206, 'accumulated_eval_time': 7697.823964357376, 'accumulated_logging_time': 6.947785139083862}
I0202 10:18:47.503618 139774434195200 logging_writer.py:48] [150752] accumulated_eval_time=7697.823964, accumulated_logging_time=6.947785, accumulated_submission_time=68946.717334, global_step=150752, preemption_count=0, score=68946.717334, test/accuracy=0.602300, test/loss=1.695832, test/num_examples=10000, total_duration=76659.315835, train/accuracy=0.807578, train/loss=0.745301, validation/accuracy=0.728860, validation/loss=1.078875, validation/num_examples=50000
I0202 10:19:06.962313 139774417409792 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.110403537750244, loss=2.0728111267089844
I0202 10:19:50.954236 139774434195200 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.370633125305176, loss=3.491039991378784
I0202 10:20:37.096990 139774417409792 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.404199600219727, loss=1.655625820159912
I0202 10:21:23.348911 139774434195200 logging_writer.py:48] [151100] global_step=151100, grad_norm=4.214128494262695, loss=1.6086281538009644
I0202 10:22:09.918923 139774417409792 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.238746643066406, loss=1.8117420673370361
I0202 10:22:56.221777 139774434195200 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.272322654724121, loss=1.972100853919983
I0202 10:23:42.756814 139774417409792 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.108684062957764, loss=1.5650776624679565
I0202 10:24:28.817498 139774434195200 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.270523548126221, loss=2.0797040462493896
I0202 10:25:14.977971 139774417409792 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.355883598327637, loss=2.4455697536468506
I0202 10:25:47.495622 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:25:59.774976 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:26:35.245459 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:26:36.841399 139936116377408 submission_runner.py:408] Time since start: 77128.69s, 	Step: 151672, 	{'train/accuracy': 0.7996875047683716, 'train/loss': 0.7857888340950012, 'validation/accuracy': 0.7291399836540222, 'validation/loss': 1.0955058336257935, 'validation/num_examples': 50000, 'test/accuracy': 0.6053000092506409, 'test/loss': 1.7195252180099487, 'test/num_examples': 10000, 'score': 69366.65239262581, 'total_duration': 77128.69496536255, 'accumulated_submission_time': 69366.65239262581, 'accumulated_eval_time': 7747.169746875763, 'accumulated_logging_time': 6.998458623886108}
I0202 10:26:36.885515 139774434195200 logging_writer.py:48] [151672] accumulated_eval_time=7747.169747, accumulated_logging_time=6.998459, accumulated_submission_time=69366.652393, global_step=151672, preemption_count=0, score=69366.652393, test/accuracy=0.605300, test/loss=1.719525, test/num_examples=10000, total_duration=77128.694965, train/accuracy=0.799688, train/loss=0.785789, validation/accuracy=0.729140, validation/loss=1.095506, validation/num_examples=50000
I0202 10:26:48.420440 139774417409792 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.970442295074463, loss=2.2894768714904785
I0202 10:27:31.909488 139774434195200 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.26165771484375, loss=1.8941491842269897
I0202 10:28:18.089680 139774417409792 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.797516107559204, loss=3.291677951812744
I0202 10:29:04.631469 139774434195200 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.138699531555176, loss=1.6003105640411377
I0202 10:29:50.609438 139774417409792 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.916546106338501, loss=2.617877960205078
I0202 10:30:36.785116 139774434195200 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.508776664733887, loss=3.4633636474609375
I0202 10:31:23.183643 139774417409792 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.271554946899414, loss=1.800239086151123
I0202 10:32:09.983620 139774434195200 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.49683141708374, loss=1.5625972747802734
I0202 10:32:56.110674 139774417409792 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.721170902252197, loss=1.5791360139846802
I0202 10:33:37.346334 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:33:49.318969 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:34:26.614999 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:34:28.213990 139936116377408 submission_runner.py:408] Time since start: 77600.07s, 	Step: 152590, 	{'train/accuracy': 0.8040820360183716, 'train/loss': 0.7654426693916321, 'validation/accuracy': 0.7314599752426147, 'validation/loss': 1.0745552778244019, 'validation/num_examples': 50000, 'test/accuracy': 0.6079000234603882, 'test/loss': 1.6825690269470215, 'test/num_examples': 10000, 'score': 69787.05628466606, 'total_duration': 77600.06755399704, 'accumulated_submission_time': 69787.05628466606, 'accumulated_eval_time': 7798.037398815155, 'accumulated_logging_time': 7.052371025085449}
I0202 10:34:28.256215 139774434195200 logging_writer.py:48] [152590] accumulated_eval_time=7798.037399, accumulated_logging_time=7.052371, accumulated_submission_time=69787.056285, global_step=152590, preemption_count=0, score=69787.056285, test/accuracy=0.607900, test/loss=1.682569, test/num_examples=10000, total_duration=77600.067554, train/accuracy=0.804082, train/loss=0.765443, validation/accuracy=0.731460, validation/loss=1.074555, validation/num_examples=50000
I0202 10:34:32.627750 139774417409792 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.565183162689209, loss=1.701425552368164
I0202 10:35:14.805753 139774434195200 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.672942638397217, loss=3.726069927215576
I0202 10:36:00.822595 139774417409792 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.7875044345855713, loss=2.3935484886169434
I0202 10:36:47.015025 139774434195200 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.357506275177002, loss=1.6340837478637695
I0202 10:37:33.460042 139774417409792 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.150306224822998, loss=2.3907666206359863
I0202 10:38:19.584357 139774434195200 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.667872428894043, loss=1.6441574096679688
I0202 10:39:05.922646 139774417409792 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.257735729217529, loss=1.5803172588348389
I0202 10:39:51.902762 139774434195200 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.347925662994385, loss=1.8215317726135254
I0202 10:40:38.085159 139774417409792 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.335201263427734, loss=1.7121046781539917
I0202 10:41:24.146099 139774434195200 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.33765983581543, loss=1.7343837022781372
I0202 10:41:28.380112 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:41:40.723339 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:42:19.947099 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:42:21.571861 139936116377408 submission_runner.py:408] Time since start: 78073.43s, 	Step: 153511, 	{'train/accuracy': 0.8078711032867432, 'train/loss': 0.7358626127243042, 'validation/accuracy': 0.7321999669075012, 'validation/loss': 1.064021110534668, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.678421974182129, 'test/num_examples': 10000, 'score': 70207.12128043175, 'total_duration': 78073.42539596558, 'accumulated_submission_time': 70207.12128043175, 'accumulated_eval_time': 7851.229133844376, 'accumulated_logging_time': 7.106428384780884}
I0202 10:42:21.619430 139774417409792 logging_writer.py:48] [153511] accumulated_eval_time=7851.229134, accumulated_logging_time=7.106428, accumulated_submission_time=70207.121280, global_step=153511, preemption_count=0, score=70207.121280, test/accuracy=0.604500, test/loss=1.678422, test/num_examples=10000, total_duration=78073.425396, train/accuracy=0.807871, train/loss=0.735863, validation/accuracy=0.732200, validation/loss=1.064021, validation/num_examples=50000
I0202 10:42:58.284786 139774434195200 logging_writer.py:48] [153600] global_step=153600, grad_norm=5.070888042449951, loss=1.6096405982971191
I0202 10:43:44.138415 139774417409792 logging_writer.py:48] [153700] global_step=153700, grad_norm=5.084940433502197, loss=3.8157403469085693
I0202 10:44:30.288759 139774434195200 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.961453437805176, loss=3.5175604820251465
I0202 10:45:16.903697 139774417409792 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.456961631774902, loss=1.953587293624878
I0202 10:46:03.123991 139774434195200 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.875654697418213, loss=1.6953028440475464
I0202 10:46:49.110001 139774417409792 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.469783306121826, loss=1.6205484867095947
I0202 10:47:35.624119 139774434195200 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.390603065490723, loss=1.794735312461853
I0202 10:48:21.680336 139774417409792 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.46894645690918, loss=1.6521480083465576
I0202 10:49:07.944711 139774434195200 logging_writer.py:48] [154400] global_step=154400, grad_norm=5.036850452423096, loss=2.4150850772857666
I0202 10:49:21.915663 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:49:34.057369 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:50:10.716232 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:50:12.319319 139936116377408 submission_runner.py:408] Time since start: 78544.17s, 	Step: 154432, 	{'train/accuracy': 0.8040429353713989, 'train/loss': 0.7815302014350891, 'validation/accuracy': 0.7331399917602539, 'validation/loss': 1.0862562656402588, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.7023465633392334, 'test/num_examples': 10000, 'score': 70627.35976624489, 'total_duration': 78544.17288303375, 'accumulated_submission_time': 70627.35976624489, 'accumulated_eval_time': 7901.632784366608, 'accumulated_logging_time': 7.164010286331177}
I0202 10:50:12.357830 139774417409792 logging_writer.py:48] [154432] accumulated_eval_time=7901.632784, accumulated_logging_time=7.164010, accumulated_submission_time=70627.359766, global_step=154432, preemption_count=0, score=70627.359766, test/accuracy=0.610700, test/loss=1.702347, test/num_examples=10000, total_duration=78544.172883, train/accuracy=0.804043, train/loss=0.781530, validation/accuracy=0.733140, validation/loss=1.086256, validation/num_examples=50000
I0202 10:50:39.754702 139774434195200 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.0939812660217285, loss=2.9932174682617188
I0202 10:51:25.680120 139774417409792 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.393245220184326, loss=3.2266016006469727
I0202 10:52:12.048828 139774434195200 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.711644649505615, loss=1.507280945777893
I0202 10:52:58.499141 139774417409792 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.176490783691406, loss=2.0396971702575684
I0202 10:53:44.703161 139774434195200 logging_writer.py:48] [154900] global_step=154900, grad_norm=5.438753604888916, loss=1.7556935548782349
I0202 10:54:30.963340 139774417409792 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.4563164710998535, loss=1.5796473026275635
I0202 10:55:17.137802 139774434195200 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.497779846191406, loss=1.6226849555969238
I0202 10:56:03.663595 139774417409792 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.312437534332275, loss=1.8022258281707764
I0202 10:56:49.597274 139774434195200 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.983904838562012, loss=3.9327149391174316
I0202 10:57:12.535717 139936116377408 spec.py:321] Evaluating on the training split.
I0202 10:57:24.738107 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 10:58:03.037900 139936116377408 spec.py:349] Evaluating on the test split.
I0202 10:58:04.649156 139936116377408 submission_runner.py:408] Time since start: 79016.50s, 	Step: 155351, 	{'train/accuracy': 0.804492175579071, 'train/loss': 0.760875940322876, 'validation/accuracy': 0.7349199652671814, 'validation/loss': 1.0727885961532593, 'validation/num_examples': 50000, 'test/accuracy': 0.6113000512123108, 'test/loss': 1.6814213991165161, 'test/num_examples': 10000, 'score': 71047.48124408722, 'total_duration': 79016.50269341469, 'accumulated_submission_time': 71047.48124408722, 'accumulated_eval_time': 7953.746181964874, 'accumulated_logging_time': 7.211568593978882}
I0202 10:58:04.696684 139774417409792 logging_writer.py:48] [155351] accumulated_eval_time=7953.746182, accumulated_logging_time=7.211569, accumulated_submission_time=71047.481244, global_step=155351, preemption_count=0, score=71047.481244, test/accuracy=0.611300, test/loss=1.681421, test/num_examples=10000, total_duration=79016.502693, train/accuracy=0.804492, train/loss=0.760876, validation/accuracy=0.734920, validation/loss=1.072789, validation/num_examples=50000
I0202 10:58:24.557275 139774434195200 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.444030284881592, loss=1.542689323425293
I0202 10:59:09.220519 139774417409792 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.225620269775391, loss=2.1898999214172363
I0202 10:59:55.378123 139774434195200 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.944977283477783, loss=3.9882376194000244
I0202 11:00:42.008031 139774417409792 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.9046096801757812, loss=2.6643402576446533
I0202 11:01:28.240449 139774434195200 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.565971851348877, loss=3.4293553829193115
I0202 11:02:14.897943 139774417409792 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.5454301834106445, loss=2.2373948097229004
I0202 11:03:00.982291 139774434195200 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.503825664520264, loss=2.000720739364624
I0202 11:03:47.144258 139774417409792 logging_writer.py:48] [156100] global_step=156100, grad_norm=5.28330135345459, loss=1.62757408618927
I0202 11:04:33.474062 139774434195200 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.593780994415283, loss=1.4983633756637573
I0202 11:05:04.772610 139936116377408 spec.py:321] Evaluating on the training split.
I0202 11:05:16.865042 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 11:05:52.723149 139936116377408 spec.py:349] Evaluating on the test split.
I0202 11:05:54.330460 139936116377408 submission_runner.py:408] Time since start: 79486.18s, 	Step: 156269, 	{'train/accuracy': 0.8096483945846558, 'train/loss': 0.7489880323410034, 'validation/accuracy': 0.73499995470047, 'validation/loss': 1.0660606622695923, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.6834540367126465, 'test/num_examples': 10000, 'score': 71467.49888920784, 'total_duration': 79486.18399739265, 'accumulated_submission_time': 71467.49888920784, 'accumulated_eval_time': 8003.304003477097, 'accumulated_logging_time': 7.269599676132202}
I0202 11:05:54.378375 139774417409792 logging_writer.py:48] [156269] accumulated_eval_time=8003.304003, accumulated_logging_time=7.269600, accumulated_submission_time=71467.498889, global_step=156269, preemption_count=0, score=71467.498889, test/accuracy=0.606700, test/loss=1.683454, test/num_examples=10000, total_duration=79486.183997, train/accuracy=0.809648, train/loss=0.748988, validation/accuracy=0.735000, validation/loss=1.066061, validation/num_examples=50000
I0202 11:06:07.104371 139774434195200 logging_writer.py:48] [156300] global_step=156300, grad_norm=5.139183044433594, loss=1.5339607000350952
I0202 11:06:50.781847 139774417409792 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.610487461090088, loss=1.6908838748931885
I0202 11:07:36.817951 139774434195200 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.6521124839782715, loss=2.9627530574798584
I0202 11:08:23.144136 139774417409792 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.609350681304932, loss=2.1453652381896973
I0202 11:09:09.407686 139774434195200 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.906397819519043, loss=1.924402117729187
I0202 11:09:55.395169 139774417409792 logging_writer.py:48] [156800] global_step=156800, grad_norm=5.12064790725708, loss=3.8301515579223633
I0202 11:10:41.644655 139774434195200 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.7562761306762695, loss=1.5261801481246948
I0202 11:11:27.811572 139774417409792 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.343424320220947, loss=2.112015724182129
I0202 11:12:14.355038 139774434195200 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.310486316680908, loss=1.6585791110992432
I0202 11:12:54.407458 139936116377408 spec.py:321] Evaluating on the training split.
I0202 11:13:06.856884 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 11:13:42.214293 139936116377408 spec.py:349] Evaluating on the test split.
I0202 11:13:43.821182 139936116377408 submission_runner.py:408] Time since start: 79955.67s, 	Step: 157189, 	{'train/accuracy': 0.8141406178474426, 'train/loss': 0.7198129296302795, 'validation/accuracy': 0.7380399703979492, 'validation/loss': 1.0487289428710938, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.6608270406723022, 'test/num_examples': 10000, 'score': 71887.46869874, 'total_duration': 79955.67472600937, 'accumulated_submission_time': 71887.46869874, 'accumulated_eval_time': 8052.717717885971, 'accumulated_logging_time': 7.328284025192261}
I0202 11:13:43.867971 139774417409792 logging_writer.py:48] [157189] accumulated_eval_time=8052.717718, accumulated_logging_time=7.328284, accumulated_submission_time=71887.468699, global_step=157189, preemption_count=0, score=71887.468699, test/accuracy=0.617400, test/loss=1.660827, test/num_examples=10000, total_duration=79955.674726, train/accuracy=0.814141, train/loss=0.719813, validation/accuracy=0.738040, validation/loss=1.048729, validation/num_examples=50000
I0202 11:13:48.640364 139774434195200 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.914104461669922, loss=2.510077953338623
I0202 11:14:30.827826 139774417409792 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.74744176864624, loss=1.5844752788543701
I0202 11:15:16.768259 139774434195200 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.804096698760986, loss=2.5790224075317383
I0202 11:16:03.508776 139774417409792 logging_writer.py:48] [157500] global_step=157500, grad_norm=5.539790153503418, loss=1.615738034248352
I0202 11:16:49.906249 139774434195200 logging_writer.py:48] [157600] global_step=157600, grad_norm=5.108628749847412, loss=3.4141502380371094
I0202 11:17:36.699283 139774417409792 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.8019280433654785, loss=1.5683996677398682
I0202 11:18:23.228312 139774434195200 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.893815040588379, loss=1.6409149169921875
I0202 11:19:09.574717 139774417409792 logging_writer.py:48] [157900] global_step=157900, grad_norm=5.094947814941406, loss=1.473541259765625
I0202 11:19:55.733949 139774434195200 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.802480220794678, loss=1.465798020362854
I0202 11:20:42.278145 139774417409792 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.568572521209717, loss=2.879854202270508
I0202 11:20:43.875824 139936116377408 spec.py:321] Evaluating on the training split.
I0202 11:20:56.009099 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 11:21:29.345938 139936116377408 spec.py:349] Evaluating on the test split.
I0202 11:21:30.968121 139936116377408 submission_runner.py:408] Time since start: 80422.82s, 	Step: 158105, 	{'train/accuracy': 0.813769519329071, 'train/loss': 0.7237128019332886, 'validation/accuracy': 0.7416200041770935, 'validation/loss': 1.0368276834487915, 'validation/num_examples': 50000, 'test/accuracy': 0.6190000176429749, 'test/loss': 1.6595739126205444, 'test/num_examples': 10000, 'score': 72307.4182343483, 'total_duration': 80422.82166051865, 'accumulated_submission_time': 72307.4182343483, 'accumulated_eval_time': 8099.809978485107, 'accumulated_logging_time': 7.386165380477905}
I0202 11:21:31.011100 139774434195200 logging_writer.py:48] [158105] accumulated_eval_time=8099.809978, accumulated_logging_time=7.386165, accumulated_submission_time=72307.418234, global_step=158105, preemption_count=0, score=72307.418234, test/accuracy=0.619000, test/loss=1.659574, test/num_examples=10000, total_duration=80422.821661, train/accuracy=0.813770, train/loss=0.723713, validation/accuracy=0.741620, validation/loss=1.036828, validation/num_examples=50000
I0202 11:22:10.848548 139774417409792 logging_writer.py:48] [158200] global_step=158200, grad_norm=5.169500350952148, loss=1.7427226305007935
I0202 11:22:56.725946 139774434195200 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.470932960510254, loss=1.5614948272705078
I0202 11:23:43.175674 139774417409792 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.844629764556885, loss=1.5665669441223145
I0202 11:24:29.360375 139774434195200 logging_writer.py:48] [158500] global_step=158500, grad_norm=5.390205383300781, loss=3.8480520248413086
I0202 11:25:15.615696 139774417409792 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.691292762756348, loss=1.9050861597061157
I0202 11:26:01.893444 139774434195200 logging_writer.py:48] [158700] global_step=158700, grad_norm=5.338916301727295, loss=3.5656158924102783
I0202 11:26:47.781418 139774417409792 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.498363018035889, loss=2.170111656188965
I0202 11:27:34.364349 139774434195200 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.575174808502197, loss=1.4399340152740479
I0202 11:28:20.593429 139774417409792 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.758037567138672, loss=2.7688918113708496
I0202 11:28:31.300381 139936116377408 spec.py:321] Evaluating on the training split.
I0202 11:28:43.468678 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 11:29:19.112579 139936116377408 spec.py:349] Evaluating on the test split.
I0202 11:29:20.735481 139936116377408 submission_runner.py:408] Time since start: 80892.59s, 	Step: 159025, 	{'train/accuracy': 0.8179101347923279, 'train/loss': 0.6971430778503418, 'validation/accuracy': 0.7404400110244751, 'validation/loss': 1.028631567955017, 'validation/num_examples': 50000, 'test/accuracy': 0.6154000163078308, 'test/loss': 1.6334420442581177, 'test/num_examples': 10000, 'score': 72727.65055274963, 'total_duration': 80892.5890173912, 'accumulated_submission_time': 72727.65055274963, 'accumulated_eval_time': 8149.245040655136, 'accumulated_logging_time': 7.438138723373413}
I0202 11:29:20.783153 139774434195200 logging_writer.py:48] [159025] accumulated_eval_time=8149.245041, accumulated_logging_time=7.438139, accumulated_submission_time=72727.650553, global_step=159025, preemption_count=0, score=72727.650553, test/accuracy=0.615400, test/loss=1.633442, test/num_examples=10000, total_duration=80892.589017, train/accuracy=0.817910, train/loss=0.697143, validation/accuracy=0.740440, validation/loss=1.028632, validation/num_examples=50000
I0202 11:29:51.136669 139774417409792 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.742897987365723, loss=3.193624258041382
I0202 11:30:37.062109 139774434195200 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.8263421058654785, loss=1.702365756034851
I0202 11:31:23.286474 139774417409792 logging_writer.py:48] [159300] global_step=159300, grad_norm=5.020297050476074, loss=1.6174166202545166
I0202 11:32:09.890422 139774434195200 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.845442771911621, loss=1.98977792263031
I0202 11:32:55.991200 139774417409792 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.941909313201904, loss=1.4619245529174805
I0202 11:33:42.299352 139774434195200 logging_writer.py:48] [159600] global_step=159600, grad_norm=5.488005638122559, loss=1.5185009241104126
I0202 11:34:28.442439 139774417409792 logging_writer.py:48] [159700] global_step=159700, grad_norm=5.387670993804932, loss=1.5671335458755493
I0202 11:35:14.735232 139774434195200 logging_writer.py:48] [159800] global_step=159800, grad_norm=5.256957530975342, loss=1.6352003812789917
I0202 11:36:00.898436 139774417409792 logging_writer.py:48] [159900] global_step=159900, grad_norm=5.667787075042725, loss=2.98866605758667
I0202 11:36:21.068382 139936116377408 spec.py:321] Evaluating on the training split.
I0202 11:36:33.189026 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 11:37:09.054666 139936116377408 spec.py:349] Evaluating on the test split.
I0202 11:37:10.661779 139936116377408 submission_runner.py:408] Time since start: 81362.52s, 	Step: 159945, 	{'train/accuracy': 0.8225781321525574, 'train/loss': 0.6903254389762878, 'validation/accuracy': 0.7412799596786499, 'validation/loss': 1.0446081161499023, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.6675461530685425, 'test/num_examples': 10000, 'score': 73147.87590813637, 'total_duration': 81362.51534724236, 'accumulated_submission_time': 73147.87590813637, 'accumulated_eval_time': 8198.83842921257, 'accumulated_logging_time': 7.497429847717285}
I0202 11:37:10.709221 139774434195200 logging_writer.py:48] [159945] accumulated_eval_time=8198.838429, accumulated_logging_time=7.497430, accumulated_submission_time=73147.875908, global_step=159945, preemption_count=0, score=73147.875908, test/accuracy=0.616600, test/loss=1.667546, test/num_examples=10000, total_duration=81362.515347, train/accuracy=0.822578, train/loss=0.690325, validation/accuracy=0.741280, validation/loss=1.044608, validation/num_examples=50000
I0202 11:37:32.960515 139774417409792 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.853367805480957, loss=2.4854698181152344
I0202 11:38:17.752936 139774434195200 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.951931476593018, loss=1.57863187789917
I0202 11:39:04.825764 139774417409792 logging_writer.py:48] [160200] global_step=160200, grad_norm=5.23139762878418, loss=2.122652530670166
I0202 11:39:51.957146 139774434195200 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.810092926025391, loss=3.19124436378479
I0202 11:40:38.520812 139774417409792 logging_writer.py:48] [160400] global_step=160400, grad_norm=5.358608722686768, loss=1.5038495063781738
I0202 11:41:25.194797 139774434195200 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.6497578620910645, loss=2.3148059844970703
I0202 11:42:12.009966 139774417409792 logging_writer.py:48] [160600] global_step=160600, grad_norm=5.102747440338135, loss=2.5289812088012695
I0202 11:42:58.954685 139774434195200 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.958439350128174, loss=2.9857709407806396
I0202 11:43:45.706312 139774417409792 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.8694987297058105, loss=3.3780245780944824
I0202 11:44:11.129887 139936116377408 spec.py:321] Evaluating on the training split.
I0202 11:44:23.390471 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 11:44:59.082069 139936116377408 spec.py:349] Evaluating on the test split.
I0202 11:45:00.682952 139936116377408 submission_runner.py:408] Time since start: 81832.54s, 	Step: 160856, 	{'train/accuracy': 0.8190624713897705, 'train/loss': 0.7006577253341675, 'validation/accuracy': 0.7416999936103821, 'validation/loss': 1.023085355758667, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.641365885734558, 'test/num_examples': 10000, 'score': 73568.24109864235, 'total_duration': 81832.5364947319, 'accumulated_submission_time': 73568.24109864235, 'accumulated_eval_time': 8248.3914706707, 'accumulated_logging_time': 7.553764581680298}
I0202 11:45:00.729957 139774434195200 logging_writer.py:48] [160856] accumulated_eval_time=8248.391471, accumulated_logging_time=7.553765, accumulated_submission_time=73568.241099, global_step=160856, preemption_count=0, score=73568.241099, test/accuracy=0.622700, test/loss=1.641366, test/num_examples=10000, total_duration=81832.536495, train/accuracy=0.819062, train/loss=0.700658, validation/accuracy=0.741700, validation/loss=1.023085, validation/num_examples=50000
I0202 11:45:18.620481 139774417409792 logging_writer.py:48] [160900] global_step=160900, grad_norm=5.631312370300293, loss=1.4962443113327026
I0202 11:46:02.836684 139774434195200 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.726449012756348, loss=2.555771827697754
I0202 11:46:49.008924 139774417409792 logging_writer.py:48] [161100] global_step=161100, grad_norm=6.247956275939941, loss=1.6316064596176147
I0202 11:47:35.634997 139774434195200 logging_writer.py:48] [161200] global_step=161200, grad_norm=5.278184413909912, loss=1.443583607673645
I0202 11:48:21.891935 139774417409792 logging_writer.py:48] [161300] global_step=161300, grad_norm=5.370471477508545, loss=1.6136376857757568
I0202 11:49:08.823631 139774434195200 logging_writer.py:48] [161400] global_step=161400, grad_norm=5.157618045806885, loss=3.165249824523926
I0202 11:49:54.972130 139774417409792 logging_writer.py:48] [161500] global_step=161500, grad_norm=5.625899791717529, loss=1.4329068660736084
I0202 11:50:41.226969 139774434195200 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.963748931884766, loss=1.5229997634887695
I0202 11:51:27.610864 139774417409792 logging_writer.py:48] [161700] global_step=161700, grad_norm=5.490807056427002, loss=1.9100619554519653
I0202 11:52:01.080323 139936116377408 spec.py:321] Evaluating on the training split.
I0202 11:52:13.371152 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 11:52:50.911317 139936116377408 spec.py:349] Evaluating on the test split.
I0202 11:52:52.514895 139936116377408 submission_runner.py:408] Time since start: 82304.37s, 	Step: 161774, 	{'train/accuracy': 0.8226171731948853, 'train/loss': 0.693584680557251, 'validation/accuracy': 0.7432399988174438, 'validation/loss': 1.023402214050293, 'validation/num_examples': 50000, 'test/accuracy': 0.6225000023841858, 'test/loss': 1.6433871984481812, 'test/num_examples': 10000, 'score': 73988.53427219391, 'total_duration': 82304.36846089363, 'accumulated_submission_time': 73988.53427219391, 'accumulated_eval_time': 8299.826081991196, 'accumulated_logging_time': 7.611063718795776}
I0202 11:52:52.555874 139774434195200 logging_writer.py:48] [161774] accumulated_eval_time=8299.826082, accumulated_logging_time=7.611064, accumulated_submission_time=73988.534272, global_step=161774, preemption_count=0, score=73988.534272, test/accuracy=0.622500, test/loss=1.643387, test/num_examples=10000, total_duration=82304.368461, train/accuracy=0.822617, train/loss=0.693585, validation/accuracy=0.743240, validation/loss=1.023402, validation/num_examples=50000
I0202 11:53:03.270709 139774417409792 logging_writer.py:48] [161800] global_step=161800, grad_norm=5.434680938720703, loss=3.2391419410705566
I0202 11:53:46.363014 139774434195200 logging_writer.py:48] [161900] global_step=161900, grad_norm=5.294898986816406, loss=1.5566076040267944
I0202 11:54:32.532063 139774417409792 logging_writer.py:48] [162000] global_step=162000, grad_norm=5.252085208892822, loss=2.4990341663360596
I0202 11:55:18.898539 139774434195200 logging_writer.py:48] [162100] global_step=162100, grad_norm=5.366401195526123, loss=1.4539068937301636
I0202 11:56:05.246574 139774417409792 logging_writer.py:48] [162200] global_step=162200, grad_norm=5.509292125701904, loss=1.570213794708252
I0202 11:56:51.589402 139774434195200 logging_writer.py:48] [162300] global_step=162300, grad_norm=5.949921607971191, loss=3.661104440689087
I0202 11:57:37.960961 139774417409792 logging_writer.py:48] [162400] global_step=162400, grad_norm=5.899906158447266, loss=1.5692315101623535
I0202 11:58:24.356934 139774434195200 logging_writer.py:48] [162500] global_step=162500, grad_norm=5.444803237915039, loss=1.4318338632583618
I0202 11:59:10.698241 139774417409792 logging_writer.py:48] [162600] global_step=162600, grad_norm=5.4576096534729, loss=1.7462178468704224
I0202 11:59:52.522696 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:00:04.752242 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:00:41.238632 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:00:42.843305 139936116377408 submission_runner.py:408] Time since start: 82774.70s, 	Step: 162692, 	{'train/accuracy': 0.8324609398841858, 'train/loss': 0.6406580805778503, 'validation/accuracy': 0.7445999979972839, 'validation/loss': 1.0042402744293213, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.6267750263214111, 'test/num_examples': 10000, 'score': 74408.44389462471, 'total_duration': 82774.69685792923, 'accumulated_submission_time': 74408.44389462471, 'accumulated_eval_time': 8350.1466858387, 'accumulated_logging_time': 7.6614062786102295}
I0202 12:00:42.886725 139774434195200 logging_writer.py:48] [162692] accumulated_eval_time=8350.146686, accumulated_logging_time=7.661406, accumulated_submission_time=74408.443895, global_step=162692, preemption_count=0, score=74408.443895, test/accuracy=0.624700, test/loss=1.626775, test/num_examples=10000, total_duration=82774.696858, train/accuracy=0.832461, train/loss=0.640658, validation/accuracy=0.744600, validation/loss=1.004240, validation/num_examples=50000
I0202 12:00:46.460853 139774417409792 logging_writer.py:48] [162700] global_step=162700, grad_norm=5.310147285461426, loss=2.11564564704895
I0202 12:01:28.451325 139774434195200 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.952077865600586, loss=3.9481945037841797
I0202 12:02:14.628012 139774417409792 logging_writer.py:48] [162900] global_step=162900, grad_norm=5.402364253997803, loss=1.4867515563964844
I0202 12:03:00.875737 139774434195200 logging_writer.py:48] [163000] global_step=163000, grad_norm=5.618138313293457, loss=1.5717157125473022
I0202 12:03:47.319432 139774417409792 logging_writer.py:48] [163100] global_step=163100, grad_norm=5.322709083557129, loss=1.4971728324890137
I0202 12:04:33.558480 139774434195200 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.794698238372803, loss=1.7714698314666748
I0202 12:05:19.611280 139774417409792 logging_writer.py:48] [163300] global_step=163300, grad_norm=5.372727870941162, loss=2.5813443660736084
I0202 12:06:05.899622 139774434195200 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.811973571777344, loss=1.9534649848937988
I0202 12:06:51.818186 139774417409792 logging_writer.py:48] [163500] global_step=163500, grad_norm=5.168303489685059, loss=1.3652085065841675
I0202 12:07:38.194102 139774434195200 logging_writer.py:48] [163600] global_step=163600, grad_norm=6.0394392013549805, loss=3.7590384483337402
I0202 12:07:42.972692 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:07:55.232442 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:08:31.015278 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:08:32.617484 139936116377408 submission_runner.py:408] Time since start: 83244.47s, 	Step: 163612, 	{'train/accuracy': 0.8236718773841858, 'train/loss': 0.6884275078773499, 'validation/accuracy': 0.7479199767112732, 'validation/loss': 1.008457899093628, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.6180665493011475, 'test/num_examples': 10000, 'score': 74828.47229576111, 'total_duration': 83244.47105193138, 'accumulated_submission_time': 74828.47229576111, 'accumulated_eval_time': 8399.791483402252, 'accumulated_logging_time': 7.7152698040008545}
I0202 12:08:32.660588 139774417409792 logging_writer.py:48] [163612] accumulated_eval_time=8399.791483, accumulated_logging_time=7.715270, accumulated_submission_time=74828.472296, global_step=163612, preemption_count=0, score=74828.472296, test/accuracy=0.628400, test/loss=1.618067, test/num_examples=10000, total_duration=83244.471052, train/accuracy=0.823672, train/loss=0.688428, validation/accuracy=0.747920, validation/loss=1.008458, validation/num_examples=50000
I0202 12:09:09.250435 139774434195200 logging_writer.py:48] [163700] global_step=163700, grad_norm=5.180854797363281, loss=1.505626916885376
I0202 12:09:55.163470 139774417409792 logging_writer.py:48] [163800] global_step=163800, grad_norm=5.33865213394165, loss=1.4982434511184692
I0202 12:10:41.831673 139774434195200 logging_writer.py:48] [163900] global_step=163900, grad_norm=5.099720478057861, loss=1.486722707748413
I0202 12:11:28.078781 139774417409792 logging_writer.py:48] [164000] global_step=164000, grad_norm=5.366918087005615, loss=1.5309518575668335
I0202 12:12:14.129123 139774434195200 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.864492416381836, loss=2.7809948921203613
I0202 12:13:00.086499 139774417409792 logging_writer.py:48] [164200] global_step=164200, grad_norm=5.782103061676025, loss=1.6977312564849854
I0202 12:13:46.310447 139774434195200 logging_writer.py:48] [164300] global_step=164300, grad_norm=5.178328037261963, loss=1.5036816596984863
I0202 12:14:32.308427 139774417409792 logging_writer.py:48] [164400] global_step=164400, grad_norm=5.203118801116943, loss=2.9028825759887695
I0202 12:15:18.455868 139774434195200 logging_writer.py:48] [164500] global_step=164500, grad_norm=5.679123401641846, loss=1.4839346408843994
I0202 12:15:32.970335 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:15:46.141407 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:16:21.807424 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:16:23.413611 139936116377408 submission_runner.py:408] Time since start: 83715.27s, 	Step: 164533, 	{'train/accuracy': 0.8290624618530273, 'train/loss': 0.662371039390564, 'validation/accuracy': 0.7500999569892883, 'validation/loss': 1.0033659934997559, 'validation/num_examples': 50000, 'test/accuracy': 0.6251000165939331, 'test/loss': 1.6135456562042236, 'test/num_examples': 10000, 'score': 75248.7256937027, 'total_duration': 83715.26717567444, 'accumulated_submission_time': 75248.7256937027, 'accumulated_eval_time': 8450.2347574234, 'accumulated_logging_time': 7.7678821086883545}
I0202 12:16:23.456710 139774417409792 logging_writer.py:48] [164533] accumulated_eval_time=8450.234757, accumulated_logging_time=7.767882, accumulated_submission_time=75248.725694, global_step=164533, preemption_count=0, score=75248.725694, test/accuracy=0.625100, test/loss=1.613546, test/num_examples=10000, total_duration=83715.267176, train/accuracy=0.829062, train/loss=0.662371, validation/accuracy=0.750100, validation/loss=1.003366, validation/num_examples=50000
I0202 12:16:50.439946 139774434195200 logging_writer.py:48] [164600] global_step=164600, grad_norm=5.625045299530029, loss=1.5131025314331055
I0202 12:17:36.032363 139774417409792 logging_writer.py:48] [164700] global_step=164700, grad_norm=5.64534854888916, loss=1.5277154445648193
I0202 12:18:22.664332 139774434195200 logging_writer.py:48] [164800] global_step=164800, grad_norm=5.651285648345947, loss=1.459910273551941
I0202 12:19:09.125607 139774417409792 logging_writer.py:48] [164900] global_step=164900, grad_norm=5.574113368988037, loss=1.6827391386032104
I0202 12:19:54.949096 139774434195200 logging_writer.py:48] [165000] global_step=165000, grad_norm=5.036253929138184, loss=1.491017460823059
I0202 12:20:41.373879 139774417409792 logging_writer.py:48] [165100] global_step=165100, grad_norm=5.50095796585083, loss=1.5008143186569214
I0202 12:21:27.708929 139774434195200 logging_writer.py:48] [165200] global_step=165200, grad_norm=5.633877754211426, loss=1.5170990228652954
I0202 12:22:14.324876 139774417409792 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.335957050323486, loss=3.868572235107422
I0202 12:23:00.475552 139774434195200 logging_writer.py:48] [165400] global_step=165400, grad_norm=5.836195468902588, loss=1.4874703884124756
I0202 12:23:23.866048 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:23:36.075751 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:24:11.698545 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:24:13.305934 139936116377408 submission_runner.py:408] Time since start: 84185.16s, 	Step: 165452, 	{'train/accuracy': 0.8309765458106995, 'train/loss': 0.6480200886726379, 'validation/accuracy': 0.7496799826622009, 'validation/loss': 0.9904419183731079, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.5892136096954346, 'test/num_examples': 10000, 'score': 75669.07854604721, 'total_duration': 84185.15949583054, 'accumulated_submission_time': 75669.07854604721, 'accumulated_eval_time': 8499.674641370773, 'accumulated_logging_time': 7.819833278656006}
I0202 12:24:13.348956 139774417409792 logging_writer.py:48] [165452] accumulated_eval_time=8499.674641, accumulated_logging_time=7.819833, accumulated_submission_time=75669.078546, global_step=165452, preemption_count=0, score=75669.078546, test/accuracy=0.627800, test/loss=1.589214, test/num_examples=10000, total_duration=84185.159496, train/accuracy=0.830977, train/loss=0.648020, validation/accuracy=0.749680, validation/loss=0.990442, validation/num_examples=50000
I0202 12:24:32.817595 139774434195200 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.670806884765625, loss=1.9734162092208862
I0202 12:25:17.457921 139774417409792 logging_writer.py:48] [165600] global_step=165600, grad_norm=5.878897666931152, loss=1.60190749168396
I0202 12:26:03.408382 139774434195200 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.774631023406982, loss=1.4413366317749023
I0202 12:26:49.857398 139774417409792 logging_writer.py:48] [165800] global_step=165800, grad_norm=6.052481651306152, loss=3.5523462295532227
I0202 12:27:36.077984 139774434195200 logging_writer.py:48] [165900] global_step=165900, grad_norm=6.063551425933838, loss=3.8856959342956543
I0202 12:28:22.306493 139774417409792 logging_writer.py:48] [166000] global_step=166000, grad_norm=5.798530101776123, loss=1.457458734512329
I0202 12:29:08.622115 139774434195200 logging_writer.py:48] [166100] global_step=166100, grad_norm=5.1992692947387695, loss=1.6865571737289429
I0202 12:29:54.822607 139774417409792 logging_writer.py:48] [166200] global_step=166200, grad_norm=5.828548431396484, loss=1.4776026010513306
I0202 12:30:41.272101 139774434195200 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.484957218170166, loss=1.6094776391983032
I0202 12:31:13.402327 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:31:25.378194 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:32:04.083919 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:32:05.684188 139936116377408 submission_runner.py:408] Time since start: 84657.54s, 	Step: 166372, 	{'train/accuracy': 0.8332812190055847, 'train/loss': 0.6423313617706299, 'validation/accuracy': 0.7530800104141235, 'validation/loss': 0.9780200719833374, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.583455204963684, 'test/num_examples': 10000, 'score': 76089.07453298569, 'total_duration': 84657.53774857521, 'accumulated_submission_time': 76089.07453298569, 'accumulated_eval_time': 8551.956509113312, 'accumulated_logging_time': 7.872750520706177}
I0202 12:32:05.729708 139774417409792 logging_writer.py:48] [166372] accumulated_eval_time=8551.956509, accumulated_logging_time=7.872751, accumulated_submission_time=76089.074533, global_step=166372, preemption_count=0, score=76089.074533, test/accuracy=0.630200, test/loss=1.583455, test/num_examples=10000, total_duration=84657.537749, train/accuracy=0.833281, train/loss=0.642331, validation/accuracy=0.753080, validation/loss=0.978020, validation/num_examples=50000
I0202 12:32:17.229900 139774434195200 logging_writer.py:48] [166400] global_step=166400, grad_norm=6.034083843231201, loss=3.4913699626922607
I0202 12:33:00.223124 139774417409792 logging_writer.py:48] [166500] global_step=166500, grad_norm=6.603238105773926, loss=1.594423770904541
I0202 12:33:46.418930 139774434195200 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.022950649261475, loss=3.8647594451904297
I0202 12:34:33.049666 139774417409792 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.866922378540039, loss=1.3547574281692505
I0202 12:35:19.500441 139774434195200 logging_writer.py:48] [166800] global_step=166800, grad_norm=5.820766925811768, loss=2.182746410369873
I0202 12:36:05.919740 139774417409792 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.632283687591553, loss=2.2594754695892334
I0202 12:36:52.007829 139774434195200 logging_writer.py:48] [167000] global_step=167000, grad_norm=5.407513618469238, loss=1.4137532711029053
I0202 12:37:38.363300 139774417409792 logging_writer.py:48] [167100] global_step=167100, grad_norm=6.067582130432129, loss=1.939937949180603
I0202 12:38:24.697275 139774434195200 logging_writer.py:48] [167200] global_step=167200, grad_norm=5.975131511688232, loss=1.5322798490524292
I0202 12:39:06.152041 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:39:18.264108 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:39:53.701061 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:39:55.303304 139936116377408 submission_runner.py:408] Time since start: 85127.16s, 	Step: 167291, 	{'train/accuracy': 0.8354296684265137, 'train/loss': 0.6281799077987671, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 0.9751542210578918, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.580045223236084, 'test/num_examples': 10000, 'score': 76509.43893957138, 'total_duration': 85127.15686106682, 'accumulated_submission_time': 76509.43893957138, 'accumulated_eval_time': 8601.107754945755, 'accumulated_logging_time': 7.928579807281494}
I0202 12:39:55.347230 139774417409792 logging_writer.py:48] [167291] accumulated_eval_time=8601.107755, accumulated_logging_time=7.928580, accumulated_submission_time=76509.438940, global_step=167291, preemption_count=0, score=76509.438940, test/accuracy=0.634500, test/loss=1.580045, test/num_examples=10000, total_duration=85127.156861, train/accuracy=0.835430, train/loss=0.628180, validation/accuracy=0.752320, validation/loss=0.975154, validation/num_examples=50000
I0202 12:39:59.321420 139774434195200 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.004929542541504, loss=2.2618861198425293
I0202 12:40:41.339670 139774417409792 logging_writer.py:48] [167400] global_step=167400, grad_norm=5.184629440307617, loss=2.5065741539001465
I0202 12:41:27.324563 139774434195200 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.648643970489502, loss=1.36991286277771
I0202 12:42:13.949590 139774417409792 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.827777862548828, loss=1.516085147857666
I0202 12:43:00.355858 139774434195200 logging_writer.py:48] [167700] global_step=167700, grad_norm=6.176541328430176, loss=3.860836982727051
I0202 12:43:46.419574 139774417409792 logging_writer.py:48] [167800] global_step=167800, grad_norm=5.766125679016113, loss=1.4542977809906006
I0202 12:44:32.622569 139774434195200 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.874929428100586, loss=1.6155364513397217
I0202 12:45:18.889403 139774417409792 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.447125434875488, loss=1.3859241008758545
I0202 12:46:05.179856 139774434195200 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.595221042633057, loss=1.853088617324829
I0202 12:46:51.235977 139774417409792 logging_writer.py:48] [168200] global_step=168200, grad_norm=6.14576530456543, loss=1.4583920240402222
I0202 12:46:55.533726 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:47:07.850485 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:47:44.575970 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:47:46.185382 139936116377408 submission_runner.py:408] Time since start: 85598.04s, 	Step: 168211, 	{'train/accuracy': 0.8359179496765137, 'train/loss': 0.6287804841995239, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 0.9704552888870239, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.5775206089019775, 'test/num_examples': 10000, 'score': 76929.56784844398, 'total_duration': 85598.03895044327, 'accumulated_submission_time': 76929.56784844398, 'accumulated_eval_time': 8651.759412050247, 'accumulated_logging_time': 7.982419967651367}
I0202 12:47:46.229568 139774434195200 logging_writer.py:48] [168211] accumulated_eval_time=8651.759412, accumulated_logging_time=7.982420, accumulated_submission_time=76929.567848, global_step=168211, preemption_count=0, score=76929.567848, test/accuracy=0.634100, test/loss=1.577521, test/num_examples=10000, total_duration=85598.038950, train/accuracy=0.835918, train/loss=0.628780, validation/accuracy=0.756260, validation/loss=0.970455, validation/num_examples=50000
I0202 12:48:22.984328 139774417409792 logging_writer.py:48] [168300] global_step=168300, grad_norm=5.4759087562561035, loss=1.3829772472381592
I0202 12:49:09.002403 139774434195200 logging_writer.py:48] [168400] global_step=168400, grad_norm=6.810848712921143, loss=3.6147115230560303
I0202 12:49:55.185473 139774417409792 logging_writer.py:48] [168500] global_step=168500, grad_norm=5.943124771118164, loss=1.6024450063705444
I0202 12:50:41.517438 139774434195200 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.916317462921143, loss=1.610066294670105
I0202 12:51:27.697015 139774417409792 logging_writer.py:48] [168700] global_step=168700, grad_norm=6.288059234619141, loss=3.1232590675354004
I0202 12:52:14.220303 139774434195200 logging_writer.py:48] [168800] global_step=168800, grad_norm=6.331787109375, loss=3.7075138092041016
I0202 12:53:00.682627 139774417409792 logging_writer.py:48] [168900] global_step=168900, grad_norm=6.341095924377441, loss=1.387861967086792
I0202 12:53:46.794909 139774434195200 logging_writer.py:48] [169000] global_step=169000, grad_norm=6.59193754196167, loss=3.5644471645355225
I0202 12:54:33.038616 139774417409792 logging_writer.py:48] [169100] global_step=169100, grad_norm=6.674571990966797, loss=3.680823802947998
I0202 12:54:46.678989 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:54:58.882041 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 12:55:30.841735 139936116377408 spec.py:349] Evaluating on the test split.
I0202 12:55:32.442936 139936116377408 submission_runner.py:408] Time since start: 86064.30s, 	Step: 169131, 	{'train/accuracy': 0.8357812166213989, 'train/loss': 0.6224377751350403, 'validation/accuracy': 0.7562800049781799, 'validation/loss': 0.9682385921478271, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.56676185131073, 'test/num_examples': 10000, 'score': 77349.9605910778, 'total_duration': 86064.29647517204, 'accumulated_submission_time': 77349.9605910778, 'accumulated_eval_time': 8697.523322582245, 'accumulated_logging_time': 8.035626888275146}
I0202 12:55:32.494657 139774434195200 logging_writer.py:48] [169131] accumulated_eval_time=8697.523323, accumulated_logging_time=8.035627, accumulated_submission_time=77349.960591, global_step=169131, preemption_count=0, score=77349.960591, test/accuracy=0.638100, test/loss=1.566762, test/num_examples=10000, total_duration=86064.296475, train/accuracy=0.835781, train/loss=0.622438, validation/accuracy=0.756280, validation/loss=0.968239, validation/num_examples=50000
I0202 12:56:00.310065 139774417409792 logging_writer.py:48] [169200] global_step=169200, grad_norm=6.278534412384033, loss=1.3916951417922974
I0202 12:56:45.645041 139774434195200 logging_writer.py:48] [169300] global_step=169300, grad_norm=6.146412372589111, loss=1.5603892803192139
I0202 12:57:32.042716 139774417409792 logging_writer.py:48] [169400] global_step=169400, grad_norm=6.3848371505737305, loss=1.3706755638122559
I0202 12:58:18.625762 139774434195200 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.902369976043701, loss=2.6905343532562256
I0202 12:58:23.006223 139774417409792 logging_writer.py:48] [169511] global_step=169511, preemption_count=0, score=77520.383802
I0202 12:58:23.650291 139936116377408 checkpoints.py:490] Saving checkpoint at step: 169511
I0202 12:58:24.852596 139936116377408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_3/checkpoint_169511
I0202 12:58:24.873151 139936116377408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_3/checkpoint_169511.
I0202 12:58:25.597890 139936116377408 submission_runner.py:583] Tuning trial 3/5
I0202 12:58:25.598121 139936116377408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0202 12:58:25.606599 139936116377408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009765625, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 37.03280329704285, 'total_duration': 66.09364891052246, 'accumulated_submission_time': 37.03280329704285, 'accumulated_eval_time': 29.060741901397705, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (860, {'train/accuracy': 0.012519530951976776, 'train/loss': 6.41873836517334, 'validation/accuracy': 0.012719999998807907, 'validation/loss': 6.431779384613037, 'validation/num_examples': 50000, 'test/accuracy': 0.01080000028014183, 'test/loss': 6.47079610824585, 'test/num_examples': 10000, 'score': 457.21945309638977, 'total_duration': 530.0452859401703, 'accumulated_submission_time': 457.21945309638977, 'accumulated_eval_time': 72.75973510742188, 'accumulated_logging_time': 0.01935410499572754, 'global_step': 860, 'preemption_count': 0}), (1773, {'train/accuracy': 0.03847656026482582, 'train/loss': 5.859482288360596, 'validation/accuracy': 0.03543999791145325, 'validation/loss': 5.89152193069458, 'validation/num_examples': 50000, 'test/accuracy': 0.030300000682473183, 'test/loss': 6.007748603820801, 'test/num_examples': 10000, 'score': 877.4289219379425, 'total_duration': 1000.501962184906, 'accumulated_submission_time': 877.4289219379425, 'accumulated_eval_time': 122.92927050590515, 'accumulated_logging_time': 0.04998922348022461, 'global_step': 1773, 'preemption_count': 0}), (2692, {'train/accuracy': 0.06236327812075615, 'train/loss': 5.426368713378906, 'validation/accuracy': 0.06021999940276146, 'validation/loss': 5.460067272186279, 'validation/num_examples': 50000, 'test/accuracy': 0.044600002467632294, 'test/loss': 5.647171974182129, 'test/num_examples': 10000, 'score': 1297.60599899292, 'total_duration': 1467.5219156742096, 'accumulated_submission_time': 1297.60599899292, 'accumulated_eval_time': 169.69586300849915, 'accumulated_logging_time': 0.07787775993347168, 'global_step': 2692, 'preemption_count': 0}), (3612, {'train/accuracy': 0.09099609404802322, 'train/loss': 5.120298385620117, 'validation/accuracy': 0.08423999696969986, 'validation/loss': 5.156952857971191, 'validation/num_examples': 50000, 'test/accuracy': 0.0625, 'test/loss': 5.394311428070068, 'test/num_examples': 10000, 'score': 1717.723210811615, 'total_duration': 1922.2475941181183, 'accumulated_submission_time': 1717.723210811615, 'accumulated_eval_time': 204.23260712623596, 'accumulated_logging_time': 0.10209107398986816, 'global_step': 3612, 'preemption_count': 0}), (4511, {'train/accuracy': 0.1286914050579071, 'train/loss': 4.747590065002441, 'validation/accuracy': 0.11927999556064606, 'validation/loss': 4.806865692138672, 'validation/num_examples': 50000, 'test/accuracy': 0.08970000594854355, 'test/loss': 5.0951080322265625, 'test/num_examples': 10000, 'score': 2138.045210123062, 'total_duration': 2388.123862028122, 'accumulated_submission_time': 2138.045210123062, 'accumulated_eval_time': 249.70977568626404, 'accumulated_logging_time': 0.13258743286132812, 'global_step': 4511, 'preemption_count': 0}), (5431, {'train/accuracy': 0.16908203065395355, 'train/loss': 4.369624137878418, 'validation/accuracy': 0.15493999421596527, 'validation/loss': 4.4540791511535645, 'validation/num_examples': 50000, 'test/accuracy': 0.11290000379085541, 'test/loss': 4.7985992431640625, 'test/num_examples': 10000, 'score': 2558.279573202133, 'total_duration': 2855.2203080654144, 'accumulated_submission_time': 2558.279573202133, 'accumulated_eval_time': 296.49737071990967, 'accumulated_logging_time': 0.1598045825958252, 'global_step': 5431, 'preemption_count': 0}), (6353, {'train/accuracy': 0.20998045802116394, 'train/loss': 4.027159690856934, 'validation/accuracy': 0.19377999007701874, 'validation/loss': 4.129310607910156, 'validation/num_examples': 50000, 'test/accuracy': 0.14410001039505005, 'test/loss': 4.515932083129883, 'test/num_examples': 10000, 'score': 2978.6933076381683, 'total_duration': 3314.668624162674, 'accumulated_submission_time': 2978.6933076381683, 'accumulated_eval_time': 335.4525935649872, 'accumulated_logging_time': 0.19125723838806152, 'global_step': 6353, 'preemption_count': 0}), (7274, {'train/accuracy': 0.24982421100139618, 'train/loss': 3.7537617683410645, 'validation/accuracy': 0.23155999183654785, 'validation/loss': 3.8504300117492676, 'validation/num_examples': 50000, 'test/accuracy': 0.1762000024318695, 'test/loss': 4.282338619232178, 'test/num_examples': 10000, 'score': 3398.987615585327, 'total_duration': 3780.519155740738, 'accumulated_submission_time': 3398.987615585327, 'accumulated_eval_time': 380.9338798522949, 'accumulated_logging_time': 0.21874260902404785, 'global_step': 7274, 'preemption_count': 0}), (8196, {'train/accuracy': 0.2862304747104645, 'train/loss': 3.5035860538482666, 'validation/accuracy': 0.2628999948501587, 'validation/loss': 3.6230437755584717, 'validation/num_examples': 50000, 'test/accuracy': 0.20350000262260437, 'test/loss': 4.090365886688232, 'test/num_examples': 10000, 'score': 3819.009551525116, 'total_duration': 4246.702522277832, 'accumulated_submission_time': 3819.009551525116, 'accumulated_eval_time': 427.0207488536835, 'accumulated_logging_time': 0.24491047859191895, 'global_step': 8196, 'preemption_count': 0}), (9114, {'train/accuracy': 0.33785155415534973, 'train/loss': 3.160243511199951, 'validation/accuracy': 0.30473998188972473, 'validation/loss': 3.333974599838257, 'validation/num_examples': 50000, 'test/accuracy': 0.2339000105857849, 'test/loss': 3.833681344985962, 'test/num_examples': 10000, 'score': 4239.054158687592, 'total_duration': 4712.58321595192, 'accumulated_submission_time': 4239.054158687592, 'accumulated_eval_time': 472.77794551849365, 'accumulated_logging_time': 0.27527666091918945, 'global_step': 9114, 'preemption_count': 0}), (10037, {'train/accuracy': 0.35658201575279236, 'train/loss': 3.0313377380371094, 'validation/accuracy': 0.3318600058555603, 'validation/loss': 3.1532087326049805, 'validation/num_examples': 50000, 'test/accuracy': 0.2568000257015228, 'test/loss': 3.6938228607177734, 'test/num_examples': 10000, 'score': 4659.012718915939, 'total_duration': 5179.155911445618, 'accumulated_submission_time': 4659.012718915939, 'accumulated_eval_time': 519.3172528743744, 'accumulated_logging_time': 0.30214810371398926, 'global_step': 10037, 'preemption_count': 0}), (10960, {'train/accuracy': 0.3778125047683716, 'train/loss': 2.9234459400177, 'validation/accuracy': 0.34775999188423157, 'validation/loss': 3.085547685623169, 'validation/num_examples': 50000, 'test/accuracy': 0.2639000117778778, 'test/loss': 3.631510019302368, 'test/num_examples': 10000, 'score': 5079.202866315842, 'total_duration': 5645.115402698517, 'accumulated_submission_time': 5079.202866315842, 'accumulated_eval_time': 565.0101907253265, 'accumulated_logging_time': 0.33082032203674316, 'global_step': 10960, 'preemption_count': 0}), (11881, {'train/accuracy': 0.41001951694488525, 'train/loss': 2.7307326793670654, 'validation/accuracy': 0.37143999338150024, 'validation/loss': 2.922337532043457, 'validation/num_examples': 50000, 'test/accuracy': 0.2891000211238861, 'test/loss': 3.4765114784240723, 'test/num_examples': 10000, 'score': 5499.275423049927, 'total_duration': 6107.204093456268, 'accumulated_submission_time': 5499.275423049927, 'accumulated_eval_time': 606.9534866809845, 'accumulated_logging_time': 0.3557870388031006, 'global_step': 11881, 'preemption_count': 0}), (12800, {'train/accuracy': 0.42429685592651367, 'train/loss': 2.650355815887451, 'validation/accuracy': 0.3928999900817871, 'validation/loss': 2.803166151046753, 'validation/num_examples': 50000, 'test/accuracy': 0.29920002818107605, 'test/loss': 3.3898091316223145, 'test/num_examples': 10000, 'score': 5919.309090852737, 'total_duration': 6573.585469007492, 'accumulated_submission_time': 5919.309090852737, 'accumulated_eval_time': 653.2269532680511, 'accumulated_logging_time': 0.38291049003601074, 'global_step': 12800, 'preemption_count': 0}), (13720, {'train/accuracy': 0.4307031035423279, 'train/loss': 2.6292409896850586, 'validation/accuracy': 0.3979800045490265, 'validation/loss': 2.789198398590088, 'validation/num_examples': 50000, 'test/accuracy': 0.3100000023841858, 'test/loss': 3.3641951084136963, 'test/num_examples': 10000, 'score': 6338.901314973831, 'total_duration': 7037.623072147369, 'accumulated_submission_time': 6338.901314973831, 'accumulated_eval_time': 697.1185910701752, 'accumulated_logging_time': 0.8888986110687256, 'global_step': 13720, 'preemption_count': 0}), (14641, {'train/accuracy': 0.44679686427116394, 'train/loss': 2.5055689811706543, 'validation/accuracy': 0.4116399884223938, 'validation/loss': 2.6887874603271484, 'validation/num_examples': 50000, 'test/accuracy': 0.31620001792907715, 'test/loss': 3.266141653060913, 'test/num_examples': 10000, 'score': 6759.135899543762, 'total_duration': 7494.290160655975, 'accumulated_submission_time': 6759.135899543762, 'accumulated_eval_time': 733.4783155918121, 'accumulated_logging_time': 0.914440393447876, 'global_step': 14641, 'preemption_count': 0}), (15556, {'train/accuracy': 0.4633203148841858, 'train/loss': 2.4123597145080566, 'validation/accuracy': 0.43077999353408813, 'validation/loss': 2.577693462371826, 'validation/num_examples': 50000, 'test/accuracy': 0.3327000141143799, 'test/loss': 3.1715259552001953, 'test/num_examples': 10000, 'score': 7179.2973692417145, 'total_duration': 7951.254838228226, 'accumulated_submission_time': 7179.2973692417145, 'accumulated_eval_time': 770.1984448432922, 'accumulated_logging_time': 0.9485998153686523, 'global_step': 15556, 'preemption_count': 0}), (16474, {'train/accuracy': 0.4699999988079071, 'train/loss': 2.36291766166687, 'validation/accuracy': 0.43789997696876526, 'validation/loss': 2.5307412147521973, 'validation/num_examples': 50000, 'test/accuracy': 0.3393000066280365, 'test/loss': 3.1288845539093018, 'test/num_examples': 10000, 'score': 7599.65398979187, 'total_duration': 8420.844632863998, 'accumulated_submission_time': 7599.65398979187, 'accumulated_eval_time': 819.353661775589, 'accumulated_logging_time': 0.9791200160980225, 'global_step': 16474, 'preemption_count': 0}), (17397, {'train/accuracy': 0.47802734375, 'train/loss': 2.3279898166656494, 'validation/accuracy': 0.4379799962043762, 'validation/loss': 2.5215203762054443, 'validation/num_examples': 50000, 'test/accuracy': 0.3379000127315521, 'test/loss': 3.118520498275757, 'test/num_examples': 10000, 'score': 8019.693618297577, 'total_duration': 8882.96993303299, 'accumulated_submission_time': 8019.693618297577, 'accumulated_eval_time': 861.3643939495087, 'accumulated_logging_time': 1.0059688091278076, 'global_step': 17397, 'preemption_count': 0}), (18317, {'train/accuracy': 0.4999414086341858, 'train/loss': 2.242021322250366, 'validation/accuracy': 0.44091999530792236, 'validation/loss': 2.529330015182495, 'validation/num_examples': 50000, 'test/accuracy': 0.33970001339912415, 'test/loss': 3.14664888381958, 'test/num_examples': 10000, 'score': 8439.754869222641, 'total_duration': 9346.944638490677, 'accumulated_submission_time': 8439.754869222641, 'accumulated_eval_time': 905.2010207176208, 'accumulated_logging_time': 1.0355210304260254, 'global_step': 18317, 'preemption_count': 0}), (19236, {'train/accuracy': 0.48960936069488525, 'train/loss': 2.2664780616760254, 'validation/accuracy': 0.4534199833869934, 'validation/loss': 2.4464144706726074, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.072669744491577, 'test/num_examples': 10000, 'score': 8860.098046064377, 'total_duration': 9812.855647802353, 'accumulated_submission_time': 8860.098046064377, 'accumulated_eval_time': 950.6946420669556, 'accumulated_logging_time': 1.0625567436218262, 'global_step': 19236, 'preemption_count': 0}), (20159, {'train/accuracy': 0.4975976347923279, 'train/loss': 2.2221951484680176, 'validation/accuracy': 0.461139976978302, 'validation/loss': 2.4042558670043945, 'validation/num_examples': 50000, 'test/accuracy': 0.35190001130104065, 'test/loss': 3.0374844074249268, 'test/num_examples': 10000, 'score': 9280.475875854492, 'total_duration': 10274.733105897903, 'accumulated_submission_time': 9280.475875854492, 'accumulated_eval_time': 992.1189947128296, 'accumulated_logging_time': 1.0902252197265625, 'global_step': 20159, 'preemption_count': 0}), (21080, {'train/accuracy': 0.5208203196525574, 'train/loss': 2.0787808895111084, 'validation/accuracy': 0.4711199998855591, 'validation/loss': 2.326721429824829, 'validation/num_examples': 50000, 'test/accuracy': 0.36650002002716064, 'test/loss': 2.948438882827759, 'test/num_examples': 10000, 'score': 9700.750081539154, 'total_duration': 10741.280704021454, 'accumulated_submission_time': 9700.750081539154, 'accumulated_eval_time': 1038.3150515556335, 'accumulated_logging_time': 1.1201841831207275, 'global_step': 21080, 'preemption_count': 0}), (22001, {'train/accuracy': 0.5149999856948853, 'train/loss': 2.116328239440918, 'validation/accuracy': 0.47957998514175415, 'validation/loss': 2.289320707321167, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 2.8965604305267334, 'test/num_examples': 10000, 'score': 10121.196420192719, 'total_duration': 11208.20026922226, 'accumulated_submission_time': 10121.196420192719, 'accumulated_eval_time': 1084.7075653076172, 'accumulated_logging_time': 1.152724266052246, 'global_step': 22001, 'preemption_count': 0}), (22922, {'train/accuracy': 0.521191418170929, 'train/loss': 2.095719814300537, 'validation/accuracy': 0.48517999053001404, 'validation/loss': 2.2728850841522217, 'validation/num_examples': 50000, 'test/accuracy': 0.37610000371932983, 'test/loss': 2.9281606674194336, 'test/num_examples': 10000, 'score': 10541.374686002731, 'total_duration': 11673.316876888275, 'accumulated_submission_time': 10541.374686002731, 'accumulated_eval_time': 1129.569720506668, 'accumulated_logging_time': 1.1803789138793945, 'global_step': 22922, 'preemption_count': 0}), (23841, {'train/accuracy': 0.5325781106948853, 'train/loss': 2.0169644355773926, 'validation/accuracy': 0.4902399778366089, 'validation/loss': 2.243086338043213, 'validation/num_examples': 50000, 'test/accuracy': 0.38450002670288086, 'test/loss': 2.868086576461792, 'test/num_examples': 10000, 'score': 10961.407228469849, 'total_duration': 12138.750408172607, 'accumulated_submission_time': 10961.407228469849, 'accumulated_eval_time': 1174.8933689594269, 'accumulated_logging_time': 1.210268497467041, 'global_step': 23841, 'preemption_count': 0}), (24762, {'train/accuracy': 0.5335351228713989, 'train/loss': 2.0236427783966064, 'validation/accuracy': 0.4941200017929077, 'validation/loss': 2.203388214111328, 'validation/num_examples': 50000, 'test/accuracy': 0.3880000114440918, 'test/loss': 2.834979295730591, 'test/num_examples': 10000, 'score': 11381.509302854538, 'total_duration': 12606.17445731163, 'accumulated_submission_time': 11381.509302854538, 'accumulated_eval_time': 1222.1330163478851, 'accumulated_logging_time': 1.244988203048706, 'global_step': 24762, 'preemption_count': 0}), (25685, {'train/accuracy': 0.5403710603713989, 'train/loss': 2.0134925842285156, 'validation/accuracy': 0.5018999576568604, 'validation/loss': 2.1954407691955566, 'validation/num_examples': 50000, 'test/accuracy': 0.3906000256538391, 'test/loss': 2.8306357860565186, 'test/num_examples': 10000, 'score': 11801.537393569946, 'total_duration': 13068.69039440155, 'accumulated_submission_time': 11801.537393569946, 'accumulated_eval_time': 1264.5435791015625, 'accumulated_logging_time': 1.27274751663208, 'global_step': 25685, 'preemption_count': 0}), (26604, {'train/accuracy': 0.5479491949081421, 'train/loss': 1.9419466257095337, 'validation/accuracy': 0.5062199831008911, 'validation/loss': 2.1494569778442383, 'validation/num_examples': 50000, 'test/accuracy': 0.3939000070095062, 'test/loss': 2.7954912185668945, 'test/num_examples': 10000, 'score': 12221.882104635239, 'total_duration': 13534.276743412018, 'accumulated_submission_time': 12221.882104635239, 'accumulated_eval_time': 1309.7079060077667, 'accumulated_logging_time': 1.3029565811157227, 'global_step': 26604, 'preemption_count': 0}), (27527, {'train/accuracy': 0.5578905940055847, 'train/loss': 1.9007580280303955, 'validation/accuracy': 0.51419997215271, 'validation/loss': 2.116635322570801, 'validation/num_examples': 50000, 'test/accuracy': 0.40630000829696655, 'test/loss': 2.749185562133789, 'test/num_examples': 10000, 'score': 12642.213397979736, 'total_duration': 13994.85052037239, 'accumulated_submission_time': 12642.213397979736, 'accumulated_eval_time': 1349.8739371299744, 'accumulated_logging_time': 1.3320088386535645, 'global_step': 27527, 'preemption_count': 0}), (28448, {'train/accuracy': 0.5541796684265137, 'train/loss': 1.9155114889144897, 'validation/accuracy': 0.5175999999046326, 'validation/loss': 2.0890777111053467, 'validation/num_examples': 50000, 'test/accuracy': 0.4115000069141388, 'test/loss': 2.720576763153076, 'test/num_examples': 10000, 'score': 13062.25780081749, 'total_duration': 14460.51496386528, 'accumulated_submission_time': 13062.25780081749, 'accumulated_eval_time': 1395.4143552780151, 'accumulated_logging_time': 1.3635451793670654, 'global_step': 28448, 'preemption_count': 0}), (29369, {'train/accuracy': 0.5678515434265137, 'train/loss': 1.865001916885376, 'validation/accuracy': 0.5196200013160706, 'validation/loss': 2.0869064331054688, 'validation/num_examples': 50000, 'test/accuracy': 0.4068000316619873, 'test/loss': 2.7158117294311523, 'test/num_examples': 10000, 'score': 13482.429998636246, 'total_duration': 14925.807185173035, 'accumulated_submission_time': 13482.429998636246, 'accumulated_eval_time': 1440.4584770202637, 'accumulated_logging_time': 1.391657829284668, 'global_step': 29369, 'preemption_count': 0}), (30290, {'train/accuracy': 0.5793359279632568, 'train/loss': 1.8263520002365112, 'validation/accuracy': 0.5206999778747559, 'validation/loss': 2.103342294692993, 'validation/num_examples': 50000, 'test/accuracy': 0.41290003061294556, 'test/loss': 2.727597951889038, 'test/num_examples': 10000, 'score': 13902.37486410141, 'total_duration': 15392.34959602356, 'accumulated_submission_time': 13902.37486410141, 'accumulated_eval_time': 1486.9760718345642, 'accumulated_logging_time': 1.4241070747375488, 'global_step': 30290, 'preemption_count': 0}), (31212, {'train/accuracy': 0.564160168170929, 'train/loss': 1.8538535833358765, 'validation/accuracy': 0.527899980545044, 'validation/loss': 2.041722297668457, 'validation/num_examples': 50000, 'test/accuracy': 0.4142000079154968, 'test/loss': 2.687079906463623, 'test/num_examples': 10000, 'score': 14322.354754447937, 'total_duration': 15855.711465358734, 'accumulated_submission_time': 14322.354754447937, 'accumulated_eval_time': 1530.2789916992188, 'accumulated_logging_time': 1.4560437202453613, 'global_step': 31212, 'preemption_count': 0}), (32134, {'train/accuracy': 0.5700390338897705, 'train/loss': 1.8478858470916748, 'validation/accuracy': 0.531279981136322, 'validation/loss': 2.034501314163208, 'validation/num_examples': 50000, 'test/accuracy': 0.41930001974105835, 'test/loss': 2.6696832180023193, 'test/num_examples': 10000, 'score': 14742.48341703415, 'total_duration': 16321.563993692398, 'accumulated_submission_time': 14742.48341703415, 'accumulated_eval_time': 1575.9266781806946, 'accumulated_logging_time': 1.4843404293060303, 'global_step': 32134, 'preemption_count': 0}), (33052, {'train/accuracy': 0.5787890553474426, 'train/loss': 1.8214852809906006, 'validation/accuracy': 0.5297799706459045, 'validation/loss': 2.06502366065979, 'validation/num_examples': 50000, 'test/accuracy': 0.4118000268936157, 'test/loss': 2.7029848098754883, 'test/num_examples': 10000, 'score': 15162.777776002884, 'total_duration': 16788.576245307922, 'accumulated_submission_time': 15162.777776002884, 'accumulated_eval_time': 1622.5681648254395, 'accumulated_logging_time': 1.513521432876587, 'global_step': 33052, 'preemption_count': 0}), (33973, {'train/accuracy': 0.5707616806030273, 'train/loss': 1.8383631706237793, 'validation/accuracy': 0.5380600094795227, 'validation/loss': 2.0063157081604004, 'validation/num_examples': 50000, 'test/accuracy': 0.4244000315666199, 'test/loss': 2.666775941848755, 'test/num_examples': 10000, 'score': 15582.711037397385, 'total_duration': 17254.40386199951, 'accumulated_submission_time': 15582.711037397385, 'accumulated_eval_time': 1668.3824818134308, 'accumulated_logging_time': 1.5456140041351318, 'global_step': 33973, 'preemption_count': 0}), (34894, {'train/accuracy': 0.5763476490974426, 'train/loss': 1.8116711378097534, 'validation/accuracy': 0.5345199704170227, 'validation/loss': 2.015291452407837, 'validation/num_examples': 50000, 'test/accuracy': 0.4220000207424164, 'test/loss': 2.628847360610962, 'test/num_examples': 10000, 'score': 16003.069641828537, 'total_duration': 17720.966168165207, 'accumulated_submission_time': 16003.069641828537, 'accumulated_eval_time': 1714.5101835727692, 'accumulated_logging_time': 1.5746049880981445, 'global_step': 34894, 'preemption_count': 0}), (35814, {'train/accuracy': 0.581250011920929, 'train/loss': 1.7759336233139038, 'validation/accuracy': 0.5402399897575378, 'validation/loss': 1.991693377494812, 'validation/num_examples': 50000, 'test/accuracy': 0.42170003056526184, 'test/loss': 2.6568825244903564, 'test/num_examples': 10000, 'score': 16423.170471429825, 'total_duration': 18189.147683382034, 'accumulated_submission_time': 16423.170471429825, 'accumulated_eval_time': 1762.5099685192108, 'accumulated_logging_time': 1.608259916305542, 'global_step': 35814, 'preemption_count': 0}), (36737, {'train/accuracy': 0.5838086009025574, 'train/loss': 1.7920989990234375, 'validation/accuracy': 0.5418800115585327, 'validation/loss': 1.9881590604782104, 'validation/num_examples': 50000, 'test/accuracy': 0.4239000082015991, 'test/loss': 2.632972478866577, 'test/num_examples': 10000, 'score': 16843.52310347557, 'total_duration': 18653.83980345726, 'accumulated_submission_time': 16843.52310347557, 'accumulated_eval_time': 1806.7723808288574, 'accumulated_logging_time': 1.6376621723175049, 'global_step': 36737, 'preemption_count': 0}), (37657, {'train/accuracy': 0.5879687070846558, 'train/loss': 1.7495322227478027, 'validation/accuracy': 0.5480200052261353, 'validation/loss': 1.946392297744751, 'validation/num_examples': 50000, 'test/accuracy': 0.4320000112056732, 'test/loss': 2.6023566722869873, 'test/num_examples': 10000, 'score': 17263.79153227806, 'total_duration': 19120.8027510643, 'accumulated_submission_time': 17263.79153227806, 'accumulated_eval_time': 1853.3759117126465, 'accumulated_logging_time': 1.6815898418426514, 'global_step': 37657, 'preemption_count': 0}), (38580, {'train/accuracy': 0.5942773222923279, 'train/loss': 1.714832067489624, 'validation/accuracy': 0.5480599999427795, 'validation/loss': 1.926216721534729, 'validation/num_examples': 50000, 'test/accuracy': 0.4335000216960907, 'test/loss': 2.5650787353515625, 'test/num_examples': 10000, 'score': 17684.12568449974, 'total_duration': 19585.754362106323, 'accumulated_submission_time': 17684.12568449974, 'accumulated_eval_time': 1897.9108562469482, 'accumulated_logging_time': 1.71612548828125, 'global_step': 38580, 'preemption_count': 0}), (39501, {'train/accuracy': 0.6201366782188416, 'train/loss': 1.614801287651062, 'validation/accuracy': 0.5507599711418152, 'validation/loss': 1.94295072555542, 'validation/num_examples': 50000, 'test/accuracy': 0.43650001287460327, 'test/loss': 2.5802359580993652, 'test/num_examples': 10000, 'score': 18104.34602546692, 'total_duration': 20052.413615226746, 'accumulated_submission_time': 18104.34602546692, 'accumulated_eval_time': 1944.2633888721466, 'accumulated_logging_time': 1.7545132637023926, 'global_step': 39501, 'preemption_count': 0}), (40422, {'train/accuracy': 0.5883983969688416, 'train/loss': 1.758009433746338, 'validation/accuracy': 0.5488199591636658, 'validation/loss': 1.9514132738113403, 'validation/num_examples': 50000, 'test/accuracy': 0.430400013923645, 'test/loss': 2.604454517364502, 'test/num_examples': 10000, 'score': 18524.727516174316, 'total_duration': 20521.472969293594, 'accumulated_submission_time': 18524.727516174316, 'accumulated_eval_time': 1992.8587412834167, 'accumulated_logging_time': 1.7895410060882568, 'global_step': 40422, 'preemption_count': 0}), (41344, {'train/accuracy': 0.6026757955551147, 'train/loss': 1.6814193725585938, 'validation/accuracy': 0.5562199950218201, 'validation/loss': 1.8999054431915283, 'validation/num_examples': 50000, 'test/accuracy': 0.4415000081062317, 'test/loss': 2.5421695709228516, 'test/num_examples': 10000, 'score': 18944.705970048904, 'total_duration': 20985.605131864548, 'accumulated_submission_time': 18944.705970048904, 'accumulated_eval_time': 2036.9278779029846, 'accumulated_logging_time': 1.8252899646759033, 'global_step': 41344, 'preemption_count': 0}), (42267, {'train/accuracy': 0.6179882884025574, 'train/loss': 1.5960801839828491, 'validation/accuracy': 0.560479998588562, 'validation/loss': 1.867920994758606, 'validation/num_examples': 50000, 'test/accuracy': 0.442300021648407, 'test/loss': 2.526688814163208, 'test/num_examples': 10000, 'score': 19364.844741344452, 'total_duration': 21454.487444639206, 'accumulated_submission_time': 19364.844741344452, 'accumulated_eval_time': 2085.58514547348, 'accumulated_logging_time': 1.8638558387756348, 'global_step': 42267, 'preemption_count': 0}), (43187, {'train/accuracy': 0.5931640267372131, 'train/loss': 1.7232056856155396, 'validation/accuracy': 0.5553199648857117, 'validation/loss': 1.9169193506240845, 'validation/num_examples': 50000, 'test/accuracy': 0.44120001792907715, 'test/loss': 2.5503687858581543, 'test/num_examples': 10000, 'score': 19785.094200372696, 'total_duration': 21920.119605779648, 'accumulated_submission_time': 19785.094200372696, 'accumulated_eval_time': 2130.889495611191, 'accumulated_logging_time': 1.8945605754852295, 'global_step': 43187, 'preemption_count': 0}), (44108, {'train/accuracy': 0.6003515720367432, 'train/loss': 1.6775927543640137, 'validation/accuracy': 0.5586400032043457, 'validation/loss': 1.8800852298736572, 'validation/num_examples': 50000, 'test/accuracy': 0.4472000300884247, 'test/loss': 2.5132040977478027, 'test/num_examples': 10000, 'score': 20205.310017347336, 'total_duration': 22388.537200927734, 'accumulated_submission_time': 20205.310017347336, 'accumulated_eval_time': 2179.0115325450897, 'accumulated_logging_time': 1.9270873069763184, 'global_step': 44108, 'preemption_count': 0}), (45030, {'train/accuracy': 0.6131640672683716, 'train/loss': 1.6342071294784546, 'validation/accuracy': 0.5626999735832214, 'validation/loss': 1.8697541952133179, 'validation/num_examples': 50000, 'test/accuracy': 0.44200003147125244, 'test/loss': 2.5277490615844727, 'test/num_examples': 10000, 'score': 20625.409114599228, 'total_duration': 22855.010596990585, 'accumulated_submission_time': 20625.409114599228, 'accumulated_eval_time': 2225.3049223423004, 'accumulated_logging_time': 1.9600763320922852, 'global_step': 45030, 'preemption_count': 0}), (45952, {'train/accuracy': 0.6003320217132568, 'train/loss': 1.708828091621399, 'validation/accuracy': 0.5588200092315674, 'validation/loss': 1.8941869735717773, 'validation/num_examples': 50000, 'test/accuracy': 0.43790000677108765, 'test/loss': 2.5504677295684814, 'test/num_examples': 10000, 'score': 21045.712456464767, 'total_duration': 23322.01553273201, 'accumulated_submission_time': 21045.712456464767, 'accumulated_eval_time': 2271.9245131015778, 'accumulated_logging_time': 1.9946684837341309, 'global_step': 45952, 'preemption_count': 0}), (46874, {'train/accuracy': 0.6097265481948853, 'train/loss': 1.6467933654785156, 'validation/accuracy': 0.5633599758148193, 'validation/loss': 1.8516621589660645, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.517657518386841, 'test/num_examples': 10000, 'score': 21465.933866262436, 'total_duration': 23790.59200644493, 'accumulated_submission_time': 21465.933866262436, 'accumulated_eval_time': 2320.2013940811157, 'accumulated_logging_time': 2.025162935256958, 'global_step': 46874, 'preemption_count': 0}), (47793, {'train/accuracy': 0.6098241806030273, 'train/loss': 1.677674412727356, 'validation/accuracy': 0.5647000074386597, 'validation/loss': 1.8973495960235596, 'validation/num_examples': 50000, 'test/accuracy': 0.44510000944137573, 'test/loss': 2.5339388847351074, 'test/num_examples': 10000, 'score': 21886.19406223297, 'total_duration': 24254.984174966812, 'accumulated_submission_time': 21886.19406223297, 'accumulated_eval_time': 2364.2496979236603, 'accumulated_logging_time': 2.061084032058716, 'global_step': 47793, 'preemption_count': 0}), (48711, {'train/accuracy': 0.6181249618530273, 'train/loss': 1.6265902519226074, 'validation/accuracy': 0.5595600008964539, 'validation/loss': 1.886021375656128, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.5331673622131348, 'test/num_examples': 10000, 'score': 22306.34263277054, 'total_duration': 24723.15763783455, 'accumulated_submission_time': 22306.34263277054, 'accumulated_eval_time': 2412.188717842102, 'accumulated_logging_time': 2.0992519855499268, 'global_step': 48711, 'preemption_count': 0}), (49631, {'train/accuracy': 0.6079882383346558, 'train/loss': 1.6691138744354248, 'validation/accuracy': 0.5669800043106079, 'validation/loss': 1.862606406211853, 'validation/num_examples': 50000, 'test/accuracy': 0.444100022315979, 'test/loss': 2.522622585296631, 'test/num_examples': 10000, 'score': 22726.509202957153, 'total_duration': 25191.15389060974, 'accumulated_submission_time': 22726.509202957153, 'accumulated_eval_time': 2459.936644077301, 'accumulated_logging_time': 2.1330649852752686, 'global_step': 49631, 'preemption_count': 0}), (50554, {'train/accuracy': 0.6143358945846558, 'train/loss': 1.6266354322433472, 'validation/accuracy': 0.5703999996185303, 'validation/loss': 1.8336740732192993, 'validation/num_examples': 50000, 'test/accuracy': 0.4585000276565552, 'test/loss': 2.471269369125366, 'test/num_examples': 10000, 'score': 23146.882725715637, 'total_duration': 25658.474204063416, 'accumulated_submission_time': 23146.882725715637, 'accumulated_eval_time': 2506.8026852607727, 'accumulated_logging_time': 2.1660990715026855, 'global_step': 50554, 'preemption_count': 0}), (51474, {'train/accuracy': 0.6382616758346558, 'train/loss': 1.5179091691970825, 'validation/accuracy': 0.5744999647140503, 'validation/loss': 1.8161741495132446, 'validation/num_examples': 50000, 'test/accuracy': 0.4506000280380249, 'test/loss': 2.467916250228882, 'test/num_examples': 10000, 'score': 23566.82369351387, 'total_duration': 26122.45309472084, 'accumulated_submission_time': 23566.82369351387, 'accumulated_eval_time': 2550.7530856132507, 'accumulated_logging_time': 2.205763101577759, 'global_step': 51474, 'preemption_count': 0}), (52393, {'train/accuracy': 0.615527331829071, 'train/loss': 1.6107457876205444, 'validation/accuracy': 0.5764999985694885, 'validation/loss': 1.7930006980895996, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.4591097831726074, 'test/num_examples': 10000, 'score': 23987.047943353653, 'total_duration': 26589.09598493576, 'accumulated_submission_time': 23987.047943353653, 'accumulated_eval_time': 2597.088151693344, 'accumulated_logging_time': 2.242366075515747, 'global_step': 52393, 'preemption_count': 0}), (53314, {'train/accuracy': 0.6172460913658142, 'train/loss': 1.6188424825668335, 'validation/accuracy': 0.5728799700737, 'validation/loss': 1.8209742307662964, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.461902141571045, 'test/num_examples': 10000, 'score': 24407.272254943848, 'total_duration': 27058.116228818893, 'accumulated_submission_time': 24407.272254943848, 'accumulated_eval_time': 2645.788686990738, 'accumulated_logging_time': 2.2893104553222656, 'global_step': 53314, 'preemption_count': 0}), (54234, {'train/accuracy': 0.6295117139816284, 'train/loss': 1.57720947265625, 'validation/accuracy': 0.572219967842102, 'validation/loss': 1.8321927785873413, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.464775800704956, 'test/num_examples': 10000, 'score': 24827.344648122787, 'total_duration': 27526.469512939453, 'accumulated_submission_time': 24827.344648122787, 'accumulated_eval_time': 2693.9905047416687, 'accumulated_logging_time': 2.3210394382476807, 'global_step': 54234, 'preemption_count': 0}), (55152, {'train/accuracy': 0.6147655844688416, 'train/loss': 1.6305612325668335, 'validation/accuracy': 0.5796599984169006, 'validation/loss': 1.788099765777588, 'validation/num_examples': 50000, 'test/accuracy': 0.46670001745224, 'test/loss': 2.4314701557159424, 'test/num_examples': 10000, 'score': 25247.712097883224, 'total_duration': 27995.004477739334, 'accumulated_submission_time': 25247.712097883224, 'accumulated_eval_time': 2742.075278520584, 'accumulated_logging_time': 2.35662841796875, 'global_step': 55152, 'preemption_count': 0}), (56070, {'train/accuracy': 0.6223242282867432, 'train/loss': 1.5823334455490112, 'validation/accuracy': 0.5771600008010864, 'validation/loss': 1.789430856704712, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.429868221282959, 'test/num_examples': 10000, 'score': 25667.8112885952, 'total_duration': 28459.00898051262, 'accumulated_submission_time': 25667.8112885952, 'accumulated_eval_time': 2785.8978073596954, 'accumulated_logging_time': 2.3918521404266357, 'global_step': 56070, 'preemption_count': 0}), (56989, {'train/accuracy': 0.6273437142372131, 'train/loss': 1.5826400518417358, 'validation/accuracy': 0.5783799886703491, 'validation/loss': 1.807153582572937, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.4639346599578857, 'test/num_examples': 10000, 'score': 26088.068242549896, 'total_duration': 28927.74728178978, 'accumulated_submission_time': 26088.068242549896, 'accumulated_eval_time': 2834.294125318527, 'accumulated_logging_time': 2.4296727180480957, 'global_step': 56989, 'preemption_count': 0}), (57909, {'train/accuracy': 0.6226171851158142, 'train/loss': 1.5738970041275024, 'validation/accuracy': 0.578719973564148, 'validation/loss': 1.7816462516784668, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.410048246383667, 'test/num_examples': 10000, 'score': 26508.355674743652, 'total_duration': 29394.660277605057, 'accumulated_submission_time': 26508.355674743652, 'accumulated_eval_time': 2880.826591491699, 'accumulated_logging_time': 2.475525379180908, 'global_step': 57909, 'preemption_count': 0}), (58827, {'train/accuracy': 0.6302929520606995, 'train/loss': 1.5352495908737183, 'validation/accuracy': 0.5891000032424927, 'validation/loss': 1.726948857307434, 'validation/num_examples': 50000, 'test/accuracy': 0.4725000262260437, 'test/loss': 2.381237268447876, 'test/num_examples': 10000, 'score': 26928.282299280167, 'total_duration': 29861.34847187996, 'accumulated_submission_time': 26928.282299280167, 'accumulated_eval_time': 2927.505961894989, 'accumulated_logging_time': 2.509864330291748, 'global_step': 58827, 'preemption_count': 0}), (59747, {'train/accuracy': 0.6289257407188416, 'train/loss': 1.5401300191879272, 'validation/accuracy': 0.581559956073761, 'validation/loss': 1.7661179304122925, 'validation/num_examples': 50000, 'test/accuracy': 0.4588000178337097, 'test/loss': 2.400780200958252, 'test/num_examples': 10000, 'score': 27348.234403848648, 'total_duration': 30329.973765850067, 'accumulated_submission_time': 27348.234403848648, 'accumulated_eval_time': 2976.0984501838684, 'accumulated_logging_time': 2.5432207584381104, 'global_step': 59747, 'preemption_count': 0}), (60667, {'train/accuracy': 0.6617382764816284, 'train/loss': 1.412854790687561, 'validation/accuracy': 0.5889999866485596, 'validation/loss': 1.731900691986084, 'validation/num_examples': 50000, 'test/accuracy': 0.4660000205039978, 'test/loss': 2.385641574859619, 'test/num_examples': 10000, 'score': 27768.30704021454, 'total_duration': 30796.253998041153, 'accumulated_submission_time': 27768.30704021454, 'accumulated_eval_time': 3022.2227504253387, 'accumulated_logging_time': 2.578721761703491, 'global_step': 60667, 'preemption_count': 0}), (61583, {'train/accuracy': 0.6290038824081421, 'train/loss': 1.5284565687179565, 'validation/accuracy': 0.5866400003433228, 'validation/loss': 1.7359672784805298, 'validation/num_examples': 50000, 'test/accuracy': 0.47290003299713135, 'test/loss': 2.3630123138427734, 'test/num_examples': 10000, 'score': 28188.32996058464, 'total_duration': 31264.258637428284, 'accumulated_submission_time': 28188.32996058464, 'accumulated_eval_time': 3070.1183915138245, 'accumulated_logging_time': 2.6172053813934326, 'global_step': 61583, 'preemption_count': 0}), (62501, {'train/accuracy': 0.6339648365974426, 'train/loss': 1.5344921350479126, 'validation/accuracy': 0.5881800055503845, 'validation/loss': 1.743263602256775, 'validation/num_examples': 50000, 'test/accuracy': 0.4690000116825104, 'test/loss': 2.3870527744293213, 'test/num_examples': 10000, 'score': 28608.93250131607, 'total_duration': 31731.734622240067, 'accumulated_submission_time': 28608.93250131607, 'accumulated_eval_time': 3116.9010808467865, 'accumulated_logging_time': 2.6603078842163086, 'global_step': 62501, 'preemption_count': 0}), (63421, {'train/accuracy': 0.6504101157188416, 'train/loss': 1.453066349029541, 'validation/accuracy': 0.5853399634361267, 'validation/loss': 1.746640682220459, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.4003686904907227, 'test/num_examples': 10000, 'score': 29029.071456193924, 'total_duration': 32200.69903111458, 'accumulated_submission_time': 29029.071456193924, 'accumulated_eval_time': 3165.640180826187, 'accumulated_logging_time': 2.6990957260131836, 'global_step': 63421, 'preemption_count': 0}), (64341, {'train/accuracy': 0.6320703029632568, 'train/loss': 1.5366462469100952, 'validation/accuracy': 0.588979959487915, 'validation/loss': 1.7273471355438232, 'validation/num_examples': 50000, 'test/accuracy': 0.4739000201225281, 'test/loss': 2.3589258193969727, 'test/num_examples': 10000, 'score': 29449.20278072357, 'total_duration': 32667.884612083435, 'accumulated_submission_time': 29449.20278072357, 'accumulated_eval_time': 3212.6139874458313, 'accumulated_logging_time': 2.7327611446380615, 'global_step': 64341, 'preemption_count': 0}), (65261, {'train/accuracy': 0.6381250023841858, 'train/loss': 1.5205624103546143, 'validation/accuracy': 0.5915200114250183, 'validation/loss': 1.7391788959503174, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.375885486602783, 'test/num_examples': 10000, 'score': 29869.367889881134, 'total_duration': 33134.08188343048, 'accumulated_submission_time': 29869.367889881134, 'accumulated_eval_time': 3258.5594029426575, 'accumulated_logging_time': 2.7715091705322266, 'global_step': 65261, 'preemption_count': 0}), (66184, {'train/accuracy': 0.6523046493530273, 'train/loss': 1.447373628616333, 'validation/accuracy': 0.595259964466095, 'validation/loss': 1.7073568105697632, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.3564023971557617, 'test/num_examples': 10000, 'score': 30289.691901922226, 'total_duration': 33600.01197576523, 'accumulated_submission_time': 30289.691901922226, 'accumulated_eval_time': 3304.079433441162, 'accumulated_logging_time': 2.809138774871826, 'global_step': 66184, 'preemption_count': 0}), (67101, {'train/accuracy': 0.6419531106948853, 'train/loss': 1.4803026914596558, 'validation/accuracy': 0.6005600094795227, 'validation/loss': 1.6780091524124146, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.3071646690368652, 'test/num_examples': 10000, 'score': 30710.141847133636, 'total_duration': 34067.99260735512, 'accumulated_submission_time': 30710.141847133636, 'accumulated_eval_time': 3351.5239536762238, 'accumulated_logging_time': 2.8487424850463867, 'global_step': 67101, 'preemption_count': 0}), (68020, {'train/accuracy': 0.6422265768051147, 'train/loss': 1.4910420179367065, 'validation/accuracy': 0.5968599915504456, 'validation/loss': 1.6952730417251587, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.322556495666504, 'test/num_examples': 10000, 'score': 31130.390008687973, 'total_duration': 34534.2478351593, 'accumulated_submission_time': 31130.390008687973, 'accumulated_eval_time': 3397.4480471611023, 'accumulated_logging_time': 2.8839402198791504, 'global_step': 68020, 'preemption_count': 0}), (68940, {'train/accuracy': 0.646289050579071, 'train/loss': 1.4966928958892822, 'validation/accuracy': 0.5961999893188477, 'validation/loss': 1.7258793115615845, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.361830711364746, 'test/num_examples': 10000, 'score': 31550.531436681747, 'total_duration': 34999.48217225075, 'accumulated_submission_time': 31550.531436681747, 'accumulated_eval_time': 3442.4561920166016, 'accumulated_logging_time': 2.9215943813323975, 'global_step': 68940, 'preemption_count': 0}), (69862, {'train/accuracy': 0.6507031321525574, 'train/loss': 1.4504530429840088, 'validation/accuracy': 0.6015200018882751, 'validation/loss': 1.6813979148864746, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.3317134380340576, 'test/num_examples': 10000, 'score': 31970.809402942657, 'total_duration': 35468.415583610535, 'accumulated_submission_time': 31970.809402942657, 'accumulated_eval_time': 3491.027411222458, 'accumulated_logging_time': 2.9576942920684814, 'global_step': 69862, 'preemption_count': 0}), (70783, {'train/accuracy': 0.6480273008346558, 'train/loss': 1.50771164894104, 'validation/accuracy': 0.600879967212677, 'validation/loss': 1.7230969667434692, 'validation/num_examples': 50000, 'test/accuracy': 0.48030000925064087, 'test/loss': 2.354217767715454, 'test/num_examples': 10000, 'score': 32390.825472831726, 'total_duration': 35935.01786708832, 'accumulated_submission_time': 32390.825472831726, 'accumulated_eval_time': 3537.5296635627747, 'accumulated_logging_time': 2.9942171573638916, 'global_step': 70783, 'preemption_count': 0}), (71703, {'train/accuracy': 0.656054675579071, 'train/loss': 1.4267289638519287, 'validation/accuracy': 0.6021999716758728, 'validation/loss': 1.6627541780471802, 'validation/num_examples': 50000, 'test/accuracy': 0.4830000102519989, 'test/loss': 2.303077459335327, 'test/num_examples': 10000, 'score': 32810.79419326782, 'total_duration': 36402.86121177673, 'accumulated_submission_time': 32810.79419326782, 'accumulated_eval_time': 3585.3168003559113, 'accumulated_logging_time': 3.033379316329956, 'global_step': 71703, 'preemption_count': 0}), (72620, {'train/accuracy': 0.6702929735183716, 'train/loss': 1.3864797353744507, 'validation/accuracy': 0.605679988861084, 'validation/loss': 1.6824146509170532, 'validation/num_examples': 50000, 'test/accuracy': 0.49060001969337463, 'test/loss': 2.331911087036133, 'test/num_examples': 10000, 'score': 33231.07704329491, 'total_duration': 36870.469790935516, 'accumulated_submission_time': 33231.07704329491, 'accumulated_eval_time': 3632.5534229278564, 'accumulated_logging_time': 3.0752978324890137, 'global_step': 72620, 'preemption_count': 0}), (73540, {'train/accuracy': 0.6487694978713989, 'train/loss': 1.4816597700119019, 'validation/accuracy': 0.6056999564170837, 'validation/loss': 1.6786365509033203, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.3274142742156982, 'test/num_examples': 10000, 'score': 33651.26502633095, 'total_duration': 37337.673639297485, 'accumulated_submission_time': 33651.26502633095, 'accumulated_eval_time': 3679.4828877449036, 'accumulated_logging_time': 3.113532781600952, 'global_step': 73540, 'preemption_count': 0}), (74460, {'train/accuracy': 0.650585949420929, 'train/loss': 1.4430195093154907, 'validation/accuracy': 0.6067399978637695, 'validation/loss': 1.657494306564331, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.2823092937469482, 'test/num_examples': 10000, 'score': 34071.27830410004, 'total_duration': 37807.56340956688, 'accumulated_submission_time': 34071.27830410004, 'accumulated_eval_time': 3729.273057460785, 'accumulated_logging_time': 3.151975631713867, 'global_step': 74460, 'preemption_count': 0}), (75382, {'train/accuracy': 0.6629296541213989, 'train/loss': 1.3945977687835693, 'validation/accuracy': 0.6041199564933777, 'validation/loss': 1.6651949882507324, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3012421131134033, 'test/num_examples': 10000, 'score': 34491.6043074131, 'total_duration': 38275.6949763298, 'accumulated_submission_time': 34491.6043074131, 'accumulated_eval_time': 3776.9945294857025, 'accumulated_logging_time': 3.188735008239746, 'global_step': 75382, 'preemption_count': 0}), (76303, {'train/accuracy': 0.6522851586341858, 'train/loss': 1.4518017768859863, 'validation/accuracy': 0.6080399751663208, 'validation/loss': 1.6486347913742065, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.312741279602051, 'test/num_examples': 10000, 'score': 34911.860114097595, 'total_duration': 38743.56843471527, 'accumulated_submission_time': 34911.860114097595, 'accumulated_eval_time': 3824.521594762802, 'accumulated_logging_time': 3.2313475608825684, 'global_step': 76303, 'preemption_count': 0}), (77223, {'train/accuracy': 0.6526171565055847, 'train/loss': 1.4243426322937012, 'validation/accuracy': 0.605459988117218, 'validation/loss': 1.6445677280426025, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.2839086055755615, 'test/num_examples': 10000, 'score': 35331.959324359894, 'total_duration': 39210.669605731964, 'accumulated_submission_time': 35331.959324359894, 'accumulated_eval_time': 3871.440359354019, 'accumulated_logging_time': 3.267023801803589, 'global_step': 77223, 'preemption_count': 0}), (78143, {'train/accuracy': 0.6633203029632568, 'train/loss': 1.4052098989486694, 'validation/accuracy': 0.6102399826049805, 'validation/loss': 1.643385887145996, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.2711172103881836, 'test/num_examples': 10000, 'score': 35752.245290756226, 'total_duration': 39680.146780490875, 'accumulated_submission_time': 35752.245290756226, 'accumulated_eval_time': 3920.543641090393, 'accumulated_logging_time': 3.3071439266204834, 'global_step': 78143, 'preemption_count': 0}), (79065, {'train/accuracy': 0.6604882478713989, 'train/loss': 1.4064991474151611, 'validation/accuracy': 0.6160799860954285, 'validation/loss': 1.6271092891693115, 'validation/num_examples': 50000, 'test/accuracy': 0.4962000250816345, 'test/loss': 2.260464668273926, 'test/num_examples': 10000, 'score': 36172.618824243546, 'total_duration': 40148.349724531174, 'accumulated_submission_time': 36172.618824243546, 'accumulated_eval_time': 3968.2896530628204, 'accumulated_logging_time': 3.3433430194854736, 'global_step': 79065, 'preemption_count': 0}), (79985, {'train/accuracy': 0.6540429592132568, 'train/loss': 1.444997787475586, 'validation/accuracy': 0.6112200021743774, 'validation/loss': 1.641564965248108, 'validation/num_examples': 50000, 'test/accuracy': 0.49230003356933594, 'test/loss': 2.274275779724121, 'test/num_examples': 10000, 'score': 36592.68895483017, 'total_duration': 40617.90951514244, 'accumulated_submission_time': 36592.68895483017, 'accumulated_eval_time': 4017.691013813019, 'accumulated_logging_time': 3.384563684463501, 'global_step': 79985, 'preemption_count': 0}), (80905, {'train/accuracy': 0.6605077981948853, 'train/loss': 1.3962000608444214, 'validation/accuracy': 0.6113799810409546, 'validation/loss': 1.6233501434326172, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.255664110183716, 'test/num_examples': 10000, 'score': 37012.93760061264, 'total_duration': 41086.63825082779, 'accumulated_submission_time': 37012.93760061264, 'accumulated_eval_time': 4066.0741584300995, 'accumulated_logging_time': 3.4339022636413574, 'global_step': 80905, 'preemption_count': 0}), (81826, {'train/accuracy': 0.6890624761581421, 'train/loss': 1.2909141778945923, 'validation/accuracy': 0.6201599836349487, 'validation/loss': 1.59787118434906, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.2272136211395264, 'test/num_examples': 10000, 'score': 37433.077839136124, 'total_duration': 41555.29655838013, 'accumulated_submission_time': 37433.077839136124, 'accumulated_eval_time': 4114.506975889206, 'accumulated_logging_time': 3.4718174934387207, 'global_step': 81826, 'preemption_count': 0}), (82746, {'train/accuracy': 0.6591405868530273, 'train/loss': 1.4403164386749268, 'validation/accuracy': 0.6159799695014954, 'validation/loss': 1.641635537147522, 'validation/num_examples': 50000, 'test/accuracy': 0.4896000325679779, 'test/loss': 2.2967171669006348, 'test/num_examples': 10000, 'score': 37853.40079331398, 'total_duration': 42018.43554711342, 'accumulated_submission_time': 37853.40079331398, 'accumulated_eval_time': 4157.234240293503, 'accumulated_logging_time': 3.51223087310791, 'global_step': 82746, 'preemption_count': 0}), (83666, {'train/accuracy': 0.663769543170929, 'train/loss': 1.3689274787902832, 'validation/accuracy': 0.616320013999939, 'validation/loss': 1.5968698263168335, 'validation/num_examples': 50000, 'test/accuracy': 0.49890002608299255, 'test/loss': 2.235353469848633, 'test/num_examples': 10000, 'score': 38273.69846391678, 'total_duration': 42488.097521305084, 'accumulated_submission_time': 38273.69846391678, 'accumulated_eval_time': 4206.504290103912, 'accumulated_logging_time': 3.5590643882751465, 'global_step': 83666, 'preemption_count': 0}), (84587, {'train/accuracy': 0.6851366758346558, 'train/loss': 1.2849160432815552, 'validation/accuracy': 0.6199399828910828, 'validation/loss': 1.5707097053527832, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.218472480773926, 'test/num_examples': 10000, 'score': 38693.85767745972, 'total_duration': 42955.73055052757, 'accumulated_submission_time': 38693.85767745972, 'accumulated_eval_time': 4253.889084339142, 'accumulated_logging_time': 3.6006920337677, 'global_step': 84587, 'preemption_count': 0}), (85507, {'train/accuracy': 0.6639453172683716, 'train/loss': 1.3985720872879028, 'validation/accuracy': 0.6188200116157532, 'validation/loss': 1.608765721321106, 'validation/num_examples': 50000, 'test/accuracy': 0.4985000193119049, 'test/loss': 2.248077869415283, 'test/num_examples': 10000, 'score': 39114.19425010681, 'total_duration': 43423.79737305641, 'accumulated_submission_time': 39114.19425010681, 'accumulated_eval_time': 4301.529177188873, 'accumulated_logging_time': 3.6426284313201904, 'global_step': 85507, 'preemption_count': 0}), (86427, {'train/accuracy': 0.673535168170929, 'train/loss': 1.353287935256958, 'validation/accuracy': 0.6249600052833557, 'validation/loss': 1.5730277299880981, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.218822717666626, 'test/num_examples': 10000, 'score': 39534.38470721245, 'total_duration': 43892.334481716156, 'accumulated_submission_time': 39534.38470721245, 'accumulated_eval_time': 4349.79114151001, 'accumulated_logging_time': 3.6794145107269287, 'global_step': 86427, 'preemption_count': 0}), (87345, {'train/accuracy': 0.6821679472923279, 'train/loss': 1.2937649488449097, 'validation/accuracy': 0.6266599893569946, 'validation/loss': 1.55876624584198, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.2032463550567627, 'test/num_examples': 10000, 'score': 39954.312836647034, 'total_duration': 44361.49378180504, 'accumulated_submission_time': 39954.312836647034, 'accumulated_eval_time': 4398.92951130867, 'accumulated_logging_time': 3.7220330238342285, 'global_step': 87345, 'preemption_count': 0}), (88262, {'train/accuracy': 0.6767578125, 'train/loss': 1.3330546617507935, 'validation/accuracy': 0.6308199763298035, 'validation/loss': 1.5420633554458618, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.181927442550659, 'test/num_examples': 10000, 'score': 40374.37486696243, 'total_duration': 44829.66952109337, 'accumulated_submission_time': 40374.37486696243, 'accumulated_eval_time': 4446.9581387043, 'accumulated_logging_time': 3.759573459625244, 'global_step': 88262, 'preemption_count': 0}), (89181, {'train/accuracy': 0.6698632836341858, 'train/loss': 1.3340215682983398, 'validation/accuracy': 0.6282399892807007, 'validation/loss': 1.5469225645065308, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.215367317199707, 'test/num_examples': 10000, 'score': 40794.69830536842, 'total_duration': 45298.46662902832, 'accumulated_submission_time': 40794.69830536842, 'accumulated_eval_time': 4495.342826843262, 'accumulated_logging_time': 3.800304889678955, 'global_step': 89181, 'preemption_count': 0}), (90099, {'train/accuracy': 0.68359375, 'train/loss': 1.305981159210205, 'validation/accuracy': 0.6265999674797058, 'validation/loss': 1.5628786087036133, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2054131031036377, 'test/num_examples': 10000, 'score': 41214.659896850586, 'total_duration': 45763.786215782166, 'accumulated_submission_time': 41214.659896850586, 'accumulated_eval_time': 4540.608873128891, 'accumulated_logging_time': 3.844949960708618, 'global_step': 90099, 'preemption_count': 0}), (91018, {'train/accuracy': 0.6847070455551147, 'train/loss': 1.3044037818908691, 'validation/accuracy': 0.6290199756622314, 'validation/loss': 1.5476787090301514, 'validation/num_examples': 50000, 'test/accuracy': 0.5110000371932983, 'test/loss': 2.183170795440674, 'test/num_examples': 10000, 'score': 41635.006526470184, 'total_duration': 46232.89801955223, 'accumulated_submission_time': 41635.006526470184, 'accumulated_eval_time': 4589.280221223831, 'accumulated_logging_time': 3.8913192749023438, 'global_step': 91018, 'preemption_count': 0}), (91936, {'train/accuracy': 0.6799609065055847, 'train/loss': 1.3173344135284424, 'validation/accuracy': 0.6291599869728088, 'validation/loss': 1.534510850906372, 'validation/num_examples': 50000, 'test/accuracy': 0.5093000531196594, 'test/loss': 2.1739141941070557, 'test/num_examples': 10000, 'score': 42055.1633245945, 'total_duration': 46700.18941116333, 'accumulated_submission_time': 42055.1633245945, 'accumulated_eval_time': 4636.324400186539, 'accumulated_logging_time': 3.9344964027404785, 'global_step': 91936, 'preemption_count': 0}), (92858, {'train/accuracy': 0.6845703125, 'train/loss': 1.30815589427948, 'validation/accuracy': 0.6358000040054321, 'validation/loss': 1.5391371250152588, 'validation/num_examples': 50000, 'test/accuracy': 0.5106000304222107, 'test/loss': 2.1962108612060547, 'test/num_examples': 10000, 'score': 42475.191940784454, 'total_duration': 47169.319345235825, 'accumulated_submission_time': 42475.191940784454, 'accumulated_eval_time': 4685.3404994010925, 'accumulated_logging_time': 3.972641944885254, 'global_step': 92858, 'preemption_count': 0}), (93780, {'train/accuracy': 0.7090038657188416, 'train/loss': 1.1813215017318726, 'validation/accuracy': 0.6354999542236328, 'validation/loss': 1.5112653970718384, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.147897481918335, 'test/num_examples': 10000, 'score': 42895.57374858856, 'total_duration': 47636.26103925705, 'accumulated_submission_time': 42895.57374858856, 'accumulated_eval_time': 4731.815329551697, 'accumulated_logging_time': 4.010065317153931, 'global_step': 93780, 'preemption_count': 0}), (94699, {'train/accuracy': 0.6853711009025574, 'train/loss': 1.2983242273330688, 'validation/accuracy': 0.634939968585968, 'validation/loss': 1.5220338106155396, 'validation/num_examples': 50000, 'test/accuracy': 0.516700029373169, 'test/loss': 2.158723831176758, 'test/num_examples': 10000, 'score': 43315.627083063126, 'total_duration': 48105.22648501396, 'accumulated_submission_time': 43315.627083063126, 'accumulated_eval_time': 4780.6331214904785, 'accumulated_logging_time': 4.056293964385986, 'global_step': 94699, 'preemption_count': 0}), (95619, {'train/accuracy': 0.6918359398841858, 'train/loss': 1.3312885761260986, 'validation/accuracy': 0.6355999708175659, 'validation/loss': 1.5759185552597046, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.203728199005127, 'test/num_examples': 10000, 'score': 43735.60219955444, 'total_duration': 48575.65565729141, 'accumulated_submission_time': 43735.60219955444, 'accumulated_eval_time': 4830.998948812485, 'accumulated_logging_time': 4.0974836349487305, 'global_step': 95619, 'preemption_count': 0}), (96541, {'train/accuracy': 0.6978124976158142, 'train/loss': 1.2508113384246826, 'validation/accuracy': 0.6395999789237976, 'validation/loss': 1.5194545984268188, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.156029462814331, 'test/num_examples': 10000, 'score': 44155.92951416969, 'total_duration': 49041.48910880089, 'accumulated_submission_time': 44155.92951416969, 'accumulated_eval_time': 4876.413905143738, 'accumulated_logging_time': 4.138431787490845, 'global_step': 96541, 'preemption_count': 0}), (97461, {'train/accuracy': 0.6885156035423279, 'train/loss': 1.2628841400146484, 'validation/accuracy': 0.642579972743988, 'validation/loss': 1.4694026708602905, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.106327772140503, 'test/num_examples': 10000, 'score': 44575.93083691597, 'total_duration': 49509.01126098633, 'accumulated_submission_time': 44575.93083691597, 'accumulated_eval_time': 4923.843191146851, 'accumulated_logging_time': 4.1819233894348145, 'global_step': 97461, 'preemption_count': 0}), (98381, {'train/accuracy': 0.6943945288658142, 'train/loss': 1.2435778379440308, 'validation/accuracy': 0.6418399810791016, 'validation/loss': 1.479137659072876, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.118631362915039, 'test/num_examples': 10000, 'score': 44995.94634652138, 'total_duration': 49977.063490867615, 'accumulated_submission_time': 44995.94634652138, 'accumulated_eval_time': 4971.78942322731, 'accumulated_logging_time': 4.224483251571655, 'global_step': 98381, 'preemption_count': 0}), (99296, {'train/accuracy': 0.6937890648841858, 'train/loss': 1.2605602741241455, 'validation/accuracy': 0.6408799886703491, 'validation/loss': 1.5083057880401611, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.170624017715454, 'test/num_examples': 10000, 'score': 45416.24973320961, 'total_duration': 50444.14269065857, 'accumulated_submission_time': 45416.24973320961, 'accumulated_eval_time': 5018.474480867386, 'accumulated_logging_time': 4.268015146255493, 'global_step': 99296, 'preemption_count': 0}), (100214, {'train/accuracy': 0.6874804496765137, 'train/loss': 1.2632285356521606, 'validation/accuracy': 0.6426999568939209, 'validation/loss': 1.4662885665893555, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.1122829914093018, 'test/num_examples': 10000, 'score': 45836.359743356705, 'total_duration': 50912.906153678894, 'accumulated_submission_time': 45836.359743356705, 'accumulated_eval_time': 5067.025764942169, 'accumulated_logging_time': 4.322429895401001, 'global_step': 100214, 'preemption_count': 0}), (101134, {'train/accuracy': 0.6952343583106995, 'train/loss': 1.2298552989959717, 'validation/accuracy': 0.646399974822998, 'validation/loss': 1.4554753303527832, 'validation/num_examples': 50000, 'test/accuracy': 0.5227000117301941, 'test/loss': 2.1116693019866943, 'test/num_examples': 10000, 'score': 46256.391678094864, 'total_duration': 51377.74165701866, 'accumulated_submission_time': 46256.391678094864, 'accumulated_eval_time': 5111.740765094757, 'accumulated_logging_time': 4.362881422042847, 'global_step': 101134, 'preemption_count': 0}), (102056, {'train/accuracy': 0.6937499642372131, 'train/loss': 1.2900238037109375, 'validation/accuracy': 0.638759970664978, 'validation/loss': 1.5294249057769775, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.172455310821533, 'test/num_examples': 10000, 'score': 46676.57629132271, 'total_duration': 51844.58192944527, 'accumulated_submission_time': 46676.57629132271, 'accumulated_eval_time': 5158.299484729767, 'accumulated_logging_time': 4.4107465744018555, 'global_step': 102056, 'preemption_count': 0}), (102974, {'train/accuracy': 0.7155859470367432, 'train/loss': 1.145095944404602, 'validation/accuracy': 0.6472600102424622, 'validation/loss': 1.4536617994308472, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.0942981243133545, 'test/num_examples': 10000, 'score': 47096.54494357109, 'total_duration': 52311.58908891678, 'accumulated_submission_time': 47096.54494357109, 'accumulated_eval_time': 5205.239888191223, 'accumulated_logging_time': 4.460630416870117, 'global_step': 102974, 'preemption_count': 0}), (103892, {'train/accuracy': 0.6991210579872131, 'train/loss': 1.2539443969726562, 'validation/accuracy': 0.6495400071144104, 'validation/loss': 1.4771441221237183, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1233255863189697, 'test/num_examples': 10000, 'score': 47516.917372226715, 'total_duration': 52779.09874868393, 'accumulated_submission_time': 47516.917372226715, 'accumulated_eval_time': 5252.287913560867, 'accumulated_logging_time': 4.502202272415161, 'global_step': 103892, 'preemption_count': 0}), (104814, {'train/accuracy': 0.7024218440055847, 'train/loss': 1.2268569469451904, 'validation/accuracy': 0.6485799551010132, 'validation/loss': 1.4664111137390137, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.094282627105713, 'test/num_examples': 10000, 'score': 47936.990965127945, 'total_duration': 53241.568251132965, 'accumulated_submission_time': 47936.990965127945, 'accumulated_eval_time': 5294.593217134476, 'accumulated_logging_time': 4.5446178913116455, 'global_step': 104814, 'preemption_count': 0}), (105735, {'train/accuracy': 0.7244726419448853, 'train/loss': 1.1140944957733154, 'validation/accuracy': 0.6542800068855286, 'validation/loss': 1.4219244718551636, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.06118106842041, 'test/num_examples': 10000, 'score': 48357.001981019974, 'total_duration': 53707.13777804375, 'accumulated_submission_time': 48357.001981019974, 'accumulated_eval_time': 5340.061425924301, 'accumulated_logging_time': 4.586939811706543, 'global_step': 105735, 'preemption_count': 0}), (106651, {'train/accuracy': 0.7056054472923279, 'train/loss': 1.192114233970642, 'validation/accuracy': 0.6538000106811523, 'validation/loss': 1.427022099494934, 'validation/num_examples': 50000, 'test/accuracy': 0.5336000323295593, 'test/loss': 2.058079481124878, 'test/num_examples': 10000, 'score': 48777.18662452698, 'total_duration': 54175.822756290436, 'accumulated_submission_time': 48777.18662452698, 'accumulated_eval_time': 5388.473347187042, 'accumulated_logging_time': 4.627787828445435, 'global_step': 106651, 'preemption_count': 0}), (107568, {'train/accuracy': 0.7154492139816284, 'train/loss': 1.1752427816390991, 'validation/accuracy': 0.6553599834442139, 'validation/loss': 1.4241199493408203, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.0526721477508545, 'test/num_examples': 10000, 'score': 49197.227964401245, 'total_duration': 54645.739453077316, 'accumulated_submission_time': 49197.227964401245, 'accumulated_eval_time': 5438.257611513138, 'accumulated_logging_time': 4.671726226806641, 'global_step': 107568, 'preemption_count': 0}), (108488, {'train/accuracy': 0.71839839220047, 'train/loss': 1.1570578813552856, 'validation/accuracy': 0.6541000008583069, 'validation/loss': 1.4482253789901733, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.080390214920044, 'test/num_examples': 10000, 'score': 49617.28289413452, 'total_duration': 55114.79280400276, 'accumulated_submission_time': 49617.28289413452, 'accumulated_eval_time': 5487.164425611496, 'accumulated_logging_time': 4.715782403945923, 'global_step': 108488, 'preemption_count': 0}), (109409, {'train/accuracy': 0.7098046541213989, 'train/loss': 1.2095328569412231, 'validation/accuracy': 0.6589800119400024, 'validation/loss': 1.4317922592163086, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.062147617340088, 'test/num_examples': 10000, 'score': 50037.39028072357, 'total_duration': 55580.21659255028, 'accumulated_submission_time': 50037.39028072357, 'accumulated_eval_time': 5532.3830144405365, 'accumulated_logging_time': 4.7658116817474365, 'global_step': 109409, 'preemption_count': 0}), (110328, {'train/accuracy': 0.705078125, 'train/loss': 1.2167671918869019, 'validation/accuracy': 0.6532399654388428, 'validation/loss': 1.4524482488632202, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.0938665866851807, 'test/num_examples': 10000, 'score': 50457.570643901825, 'total_duration': 56048.070026397705, 'accumulated_submission_time': 50457.570643901825, 'accumulated_eval_time': 5579.963124036789, 'accumulated_logging_time': 4.810678005218506, 'global_step': 110328, 'preemption_count': 0}), (111247, {'train/accuracy': 0.7220507860183716, 'train/loss': 1.1261377334594727, 'validation/accuracy': 0.6609999537467957, 'validation/loss': 1.4034225940704346, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.035445213317871, 'test/num_examples': 10000, 'score': 50877.94385957718, 'total_duration': 56518.80230307579, 'accumulated_submission_time': 50877.94385957718, 'accumulated_eval_time': 5630.232651948929, 'accumulated_logging_time': 4.852938175201416, 'global_step': 111247, 'preemption_count': 0}), (112167, {'train/accuracy': 0.7176757454872131, 'train/loss': 1.1667044162750244, 'validation/accuracy': 0.6600599884986877, 'validation/loss': 1.4147700071334839, 'validation/num_examples': 50000, 'test/accuracy': 0.5349000096321106, 'test/loss': 2.0341708660125732, 'test/num_examples': 10000, 'score': 51298.08949494362, 'total_duration': 56987.73841023445, 'accumulated_submission_time': 51298.08949494362, 'accumulated_eval_time': 5678.924888134003, 'accumulated_logging_time': 4.904115915298462, 'global_step': 112167, 'preemption_count': 0}), (113085, {'train/accuracy': 0.7177538871765137, 'train/loss': 1.1266690492630005, 'validation/accuracy': 0.6662999987602234, 'validation/loss': 1.3710778951644897, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.005274534225464, 'test/num_examples': 10000, 'score': 51718.244585990906, 'total_duration': 57456.35862541199, 'accumulated_submission_time': 51718.244585990906, 'accumulated_eval_time': 5727.290082454681, 'accumulated_logging_time': 4.955888032913208, 'global_step': 113085, 'preemption_count': 0}), (114004, {'train/accuracy': 0.7226366996765137, 'train/loss': 1.1332825422286987, 'validation/accuracy': 0.6693199872970581, 'validation/loss': 1.3838796615600586, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.008410692214966, 'test/num_examples': 10000, 'score': 52138.60879659653, 'total_duration': 57921.55140066147, 'accumulated_submission_time': 52138.60879659653, 'accumulated_eval_time': 5772.0273015499115, 'accumulated_logging_time': 5.000272750854492, 'global_step': 114004, 'preemption_count': 0}), (114925, {'train/accuracy': 0.7480273246765137, 'train/loss': 1.0093517303466797, 'validation/accuracy': 0.6700999736785889, 'validation/loss': 1.3534212112426758, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 1.9908446073532104, 'test/num_examples': 10000, 'score': 52559.03491163254, 'total_duration': 58392.56290578842, 'accumulated_submission_time': 52559.03491163254, 'accumulated_eval_time': 5822.523226261139, 'accumulated_logging_time': 5.0422186851501465, 'global_step': 114925, 'preemption_count': 0}), (115842, {'train/accuracy': 0.7229687571525574, 'train/loss': 1.115723729133606, 'validation/accuracy': 0.6693199872970581, 'validation/loss': 1.3546879291534424, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 1.9898380041122437, 'test/num_examples': 10000, 'score': 52979.48181724548, 'total_duration': 58858.70378828049, 'accumulated_submission_time': 52979.48181724548, 'accumulated_eval_time': 5868.125700950623, 'accumulated_logging_time': 5.087374925613403, 'global_step': 115842, 'preemption_count': 0}), (116758, {'train/accuracy': 0.7300195097923279, 'train/loss': 1.0898079872131348, 'validation/accuracy': 0.671019971370697, 'validation/loss': 1.3476372957229614, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 1.9787174463272095, 'test/num_examples': 10000, 'score': 53399.61646103859, 'total_duration': 59326.91538286209, 'accumulated_submission_time': 53399.61646103859, 'accumulated_eval_time': 5916.103356122971, 'accumulated_logging_time': 5.1391777992248535, 'global_step': 116758, 'preemption_count': 0}), (117679, {'train/accuracy': 0.7401171922683716, 'train/loss': 1.0645672082901, 'validation/accuracy': 0.6676999926567078, 'validation/loss': 1.3838788270950317, 'validation/num_examples': 50000, 'test/accuracy': 0.544700026512146, 'test/loss': 2.0176634788513184, 'test/num_examples': 10000, 'score': 53819.964002370834, 'total_duration': 59794.09345436096, 'accumulated_submission_time': 53819.964002370834, 'accumulated_eval_time': 5962.832585573196, 'accumulated_logging_time': 5.19250226020813, 'global_step': 117679, 'preemption_count': 0}), (118597, {'train/accuracy': 0.7274804711341858, 'train/loss': 1.0996066331863403, 'validation/accuracy': 0.6718199849128723, 'validation/loss': 1.3471410274505615, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 1.9880914688110352, 'test/num_examples': 10000, 'score': 54240.321560144424, 'total_duration': 60263.149705171585, 'accumulated_submission_time': 54240.321560144424, 'accumulated_eval_time': 6011.4386677742, 'accumulated_logging_time': 5.236751317977905, 'global_step': 118597, 'preemption_count': 0}), (119518, {'train/accuracy': 0.7304491996765137, 'train/loss': 1.0636045932769775, 'validation/accuracy': 0.672760009765625, 'validation/loss': 1.335329294204712, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 1.9718530178070068, 'test/num_examples': 10000, 'score': 54660.49790549278, 'total_duration': 60732.12470769882, 'accumulated_submission_time': 54660.49790549278, 'accumulated_eval_time': 6060.143052816391, 'accumulated_logging_time': 5.284197807312012, 'global_step': 119518, 'preemption_count': 0}), (120438, {'train/accuracy': 0.7368554472923279, 'train/loss': 1.0701631307601929, 'validation/accuracy': 0.675879955291748, 'validation/loss': 1.3530094623565674, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 1.9830008745193481, 'test/num_examples': 10000, 'score': 55080.59421658516, 'total_duration': 61201.30026555061, 'accumulated_submission_time': 55080.59421658516, 'accumulated_eval_time': 6109.130714178085, 'accumulated_logging_time': 5.328948974609375, 'global_step': 120438, 'preemption_count': 0}), (121354, {'train/accuracy': 0.7291015386581421, 'train/loss': 1.0866446495056152, 'validation/accuracy': 0.6756199598312378, 'validation/loss': 1.3281562328338623, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 1.955164909362793, 'test/num_examples': 10000, 'score': 55500.81260108948, 'total_duration': 61668.1078953743, 'accumulated_submission_time': 55500.81260108948, 'accumulated_eval_time': 6155.618450164795, 'accumulated_logging_time': 5.383184432983398, 'global_step': 121354, 'preemption_count': 0}), (122273, {'train/accuracy': 0.7377538681030273, 'train/loss': 1.0467967987060547, 'validation/accuracy': 0.6814199686050415, 'validation/loss': 1.3041400909423828, 'validation/num_examples': 50000, 'test/accuracy': 0.5575000047683716, 'test/loss': 1.926209807395935, 'test/num_examples': 10000, 'score': 55921.026826143265, 'total_duration': 62134.41156554222, 'accumulated_submission_time': 55921.026826143265, 'accumulated_eval_time': 6201.618116378784, 'accumulated_logging_time': 5.425713300704956, 'global_step': 122273, 'preemption_count': 0}), (123189, {'train/accuracy': 0.7463476657867432, 'train/loss': 1.0369484424591064, 'validation/accuracy': 0.6841599941253662, 'validation/loss': 1.3181054592132568, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 1.9473214149475098, 'test/num_examples': 10000, 'score': 56341.15030384064, 'total_duration': 62601.69093823433, 'accumulated_submission_time': 56341.15030384064, 'accumulated_eval_time': 6248.677495479584, 'accumulated_logging_time': 5.474867820739746, 'global_step': 123189, 'preemption_count': 0}), (124108, {'train/accuracy': 0.7437499761581421, 'train/loss': 1.0200892686843872, 'validation/accuracy': 0.6828199625015259, 'validation/loss': 1.2920786142349243, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 1.9192627668380737, 'test/num_examples': 10000, 'score': 56761.34786653519, 'total_duration': 63072.82502889633, 'accumulated_submission_time': 56761.34786653519, 'accumulated_eval_time': 6299.499836921692, 'accumulated_logging_time': 5.528214454650879, 'global_step': 124108, 'preemption_count': 0}), (125025, {'train/accuracy': 0.7412695288658142, 'train/loss': 1.0353752374649048, 'validation/accuracy': 0.6846599578857422, 'validation/loss': 1.2898250818252563, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 1.9244365692138672, 'test/num_examples': 10000, 'score': 57181.43518590927, 'total_duration': 63537.27227139473, 'accumulated_submission_time': 57181.43518590927, 'accumulated_eval_time': 6343.759425878525, 'accumulated_logging_time': 5.581441879272461, 'global_step': 125025, 'preemption_count': 0}), (125945, {'train/accuracy': 0.7484374642372131, 'train/loss': 0.9919676780700684, 'validation/accuracy': 0.6880999803543091, 'validation/loss': 1.2748708724975586, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 1.912021517753601, 'test/num_examples': 10000, 'score': 57601.69718050957, 'total_duration': 64005.15271496773, 'accumulated_submission_time': 57601.69718050957, 'accumulated_eval_time': 6391.28493309021, 'accumulated_logging_time': 5.625512361526489, 'global_step': 125945, 'preemption_count': 0}), (126864, {'train/accuracy': 0.76025390625, 'train/loss': 0.9850084185600281, 'validation/accuracy': 0.687559962272644, 'validation/loss': 1.3039485216140747, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9312610626220703, 'test/num_examples': 10000, 'score': 58021.72929406166, 'total_duration': 64473.019273757935, 'accumulated_submission_time': 58021.72929406166, 'accumulated_eval_time': 6439.025028705597, 'accumulated_logging_time': 5.6715407371521, 'global_step': 126864, 'preemption_count': 0}), (127784, {'train/accuracy': 0.7442578077316284, 'train/loss': 1.032114863395691, 'validation/accuracy': 0.6894199848175049, 'validation/loss': 1.2755590677261353, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 1.8965699672698975, 'test/num_examples': 10000, 'score': 58441.85705113411, 'total_duration': 64940.28579545021, 'accumulated_submission_time': 58441.85705113411, 'accumulated_eval_time': 6486.063357114792, 'accumulated_logging_time': 5.723682403564453, 'global_step': 127784, 'preemption_count': 0}), (128703, {'train/accuracy': 0.7488671541213989, 'train/loss': 1.0089884996414185, 'validation/accuracy': 0.6900999546051025, 'validation/loss': 1.2736644744873047, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 1.8980499505996704, 'test/num_examples': 10000, 'score': 58862.215963602066, 'total_duration': 65407.78814959526, 'accumulated_submission_time': 58862.215963602066, 'accumulated_eval_time': 6533.1138525009155, 'accumulated_logging_time': 5.769906282424927, 'global_step': 128703, 'preemption_count': 0}), (129621, {'train/accuracy': 0.7646874785423279, 'train/loss': 0.9320166707038879, 'validation/accuracy': 0.6924799680709839, 'validation/loss': 1.245804786682129, 'validation/num_examples': 50000, 'test/accuracy': 0.5667999982833862, 'test/loss': 1.8743077516555786, 'test/num_examples': 10000, 'score': 59282.16807126999, 'total_duration': 65875.81766748428, 'accumulated_submission_time': 59282.16807126999, 'accumulated_eval_time': 6581.099862098694, 'accumulated_logging_time': 5.814780950546265, 'global_step': 129621, 'preemption_count': 0}), (130539, {'train/accuracy': 0.7518749833106995, 'train/loss': 0.9943748116493225, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.2452759742736816, 'validation/num_examples': 50000, 'test/accuracy': 0.5671000480651855, 'test/loss': 1.879859447479248, 'test/num_examples': 10000, 'score': 59702.487728357315, 'total_duration': 66345.67736411095, 'accumulated_submission_time': 59702.487728357315, 'accumulated_eval_time': 6630.546304941177, 'accumulated_logging_time': 5.8607916831970215, 'global_step': 130539, 'preemption_count': 0}), (131459, {'train/accuracy': 0.7592187523841858, 'train/loss': 0.9559024572372437, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.223264455795288, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 1.8423492908477783, 'test/num_examples': 10000, 'score': 60122.77754402161, 'total_duration': 66813.98977065086, 'accumulated_submission_time': 60122.77754402161, 'accumulated_eval_time': 6678.470661401749, 'accumulated_logging_time': 5.9110212326049805, 'global_step': 131459, 'preemption_count': 0}), (132379, {'train/accuracy': 0.7625390291213989, 'train/loss': 0.9606295824050903, 'validation/accuracy': 0.6951199769973755, 'validation/loss': 1.261569857597351, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.8848967552185059, 'test/num_examples': 10000, 'score': 60542.986839056015, 'total_duration': 67283.923807621, 'accumulated_submission_time': 60542.986839056015, 'accumulated_eval_time': 6728.100863933563, 'accumulated_logging_time': 5.9576075077056885, 'global_step': 132379, 'preemption_count': 0}), (133298, {'train/accuracy': 0.7527148127555847, 'train/loss': 0.9829521179199219, 'validation/accuracy': 0.7013199925422668, 'validation/loss': 1.2232753038406372, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.8487434387207031, 'test/num_examples': 10000, 'score': 60963.19054841995, 'total_duration': 67751.67906785011, 'accumulated_submission_time': 60963.19054841995, 'accumulated_eval_time': 6775.558528184891, 'accumulated_logging_time': 6.004410266876221, 'global_step': 133298, 'preemption_count': 0}), (134216, {'train/accuracy': 0.7622656226158142, 'train/loss': 0.9357055425643921, 'validation/accuracy': 0.701259970664978, 'validation/loss': 1.2122747898101807, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.8352450132369995, 'test/num_examples': 10000, 'score': 61383.45324897766, 'total_duration': 68218.22201561928, 'accumulated_submission_time': 61383.45324897766, 'accumulated_eval_time': 6821.74352645874, 'accumulated_logging_time': 6.052812814712524, 'global_step': 134216, 'preemption_count': 0}), (135131, {'train/accuracy': 0.7727343440055847, 'train/loss': 0.8989397287368774, 'validation/accuracy': 0.7053200006484985, 'validation/loss': 1.198777675628662, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.8275467157363892, 'test/num_examples': 10000, 'score': 61803.36758232117, 'total_duration': 68685.2418153286, 'accumulated_submission_time': 61803.36758232117, 'accumulated_eval_time': 6868.752334356308, 'accumulated_logging_time': 6.101391077041626, 'global_step': 135131, 'preemption_count': 0}), (136050, {'train/accuracy': 0.77685546875, 'train/loss': 0.8904039859771729, 'validation/accuracy': 0.7037000060081482, 'validation/loss': 1.207800030708313, 'validation/num_examples': 50000, 'test/accuracy': 0.5791000127792358, 'test/loss': 1.836230993270874, 'test/num_examples': 10000, 'score': 62223.56702518463, 'total_duration': 69154.49729061127, 'accumulated_submission_time': 62223.56702518463, 'accumulated_eval_time': 6917.71401143074, 'accumulated_logging_time': 6.14823055267334, 'global_step': 136050, 'preemption_count': 0}), (136970, {'train/accuracy': 0.7667577862739563, 'train/loss': 0.9236660003662109, 'validation/accuracy': 0.7049799561500549, 'validation/loss': 1.1936697959899902, 'validation/num_examples': 50000, 'test/accuracy': 0.579300045967102, 'test/loss': 1.815743088722229, 'test/num_examples': 10000, 'score': 62643.629824876785, 'total_duration': 69623.43274188042, 'accumulated_submission_time': 62643.629824876785, 'accumulated_eval_time': 6966.49352145195, 'accumulated_logging_time': 6.19426703453064, 'global_step': 136970, 'preemption_count': 0}), (137890, {'train/accuracy': 0.772753894329071, 'train/loss': 0.9023522138595581, 'validation/accuracy': 0.706279993057251, 'validation/loss': 1.1953895092010498, 'validation/num_examples': 50000, 'test/accuracy': 0.5799000263214111, 'test/loss': 1.819128155708313, 'test/num_examples': 10000, 'score': 63063.79121661186, 'total_duration': 70090.73695373535, 'accumulated_submission_time': 63063.79121661186, 'accumulated_eval_time': 7013.5392434597015, 'accumulated_logging_time': 6.243442058563232, 'global_step': 137890, 'preemption_count': 0}), (138808, {'train/accuracy': 0.78382807970047, 'train/loss': 0.8481549024581909, 'validation/accuracy': 0.7096999883651733, 'validation/loss': 1.173912525177002, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.7924264669418335, 'test/num_examples': 10000, 'score': 63483.9806933403, 'total_duration': 70560.31663990021, 'accumulated_submission_time': 63483.9806933403, 'accumulated_eval_time': 7062.826720952988, 'accumulated_logging_time': 6.297896862030029, 'global_step': 138808, 'preemption_count': 0}), (139726, {'train/accuracy': 0.7791601419448853, 'train/loss': 0.8667029142379761, 'validation/accuracy': 0.7123000025749207, 'validation/loss': 1.1546515226364136, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 1.7686362266540527, 'test/num_examples': 10000, 'score': 63903.965514183044, 'total_duration': 71027.57639861107, 'accumulated_submission_time': 63903.965514183044, 'accumulated_eval_time': 7110.006555318832, 'accumulated_logging_time': 6.345866918563843, 'global_step': 139726, 'preemption_count': 0}), (140642, {'train/accuracy': 0.7782421708106995, 'train/loss': 0.8562284708023071, 'validation/accuracy': 0.7149199843406677, 'validation/loss': 1.147608757019043, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.7629472017288208, 'test/num_examples': 10000, 'score': 64324.39351463318, 'total_duration': 71496.82323789597, 'accumulated_submission_time': 64324.39351463318, 'accumulated_eval_time': 7158.72931265831, 'accumulated_logging_time': 6.395110607147217, 'global_step': 140642, 'preemption_count': 0}), (141561, {'train/accuracy': 0.7863867282867432, 'train/loss': 0.8447074890136719, 'validation/accuracy': 0.7131199836730957, 'validation/loss': 1.16819167137146, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.8064215183258057, 'test/num_examples': 10000, 'score': 64744.67516851425, 'total_duration': 71965.00109434128, 'accumulated_submission_time': 64744.67516851425, 'accumulated_eval_time': 7206.5280418396, 'accumulated_logging_time': 6.445488691329956, 'global_step': 141561, 'preemption_count': 0}), (142474, {'train/accuracy': 0.7731249928474426, 'train/loss': 0.8849400281906128, 'validation/accuracy': 0.7135399580001831, 'validation/loss': 1.1557042598724365, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.762101411819458, 'test/num_examples': 10000, 'score': 65164.6888320446, 'total_duration': 72433.90739750862, 'accumulated_submission_time': 65164.6888320446, 'accumulated_eval_time': 7255.32564163208, 'accumulated_logging_time': 6.493313312530518, 'global_step': 142474, 'preemption_count': 0}), (143390, {'train/accuracy': 0.7816015481948853, 'train/loss': 0.868894100189209, 'validation/accuracy': 0.716219961643219, 'validation/loss': 1.1561012268066406, 'validation/num_examples': 50000, 'test/accuracy': 0.5877000093460083, 'test/loss': 1.783122181892395, 'test/num_examples': 10000, 'score': 65585.04565286636, 'total_duration': 72904.4974284172, 'accumulated_submission_time': 65585.04565286636, 'accumulated_eval_time': 7305.463745594025, 'accumulated_logging_time': 6.540487051010132, 'global_step': 143390, 'preemption_count': 0}), (144308, {'train/accuracy': 0.79212886095047, 'train/loss': 0.8323768377304077, 'validation/accuracy': 0.7192999720573425, 'validation/loss': 1.145952582359314, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.7589880228042603, 'test/num_examples': 10000, 'score': 66004.958309412, 'total_duration': 73373.15925145149, 'accumulated_submission_time': 66004.958309412, 'accumulated_eval_time': 7354.108882188797, 'accumulated_logging_time': 6.597029685974121, 'global_step': 144308, 'preemption_count': 0}), (145228, {'train/accuracy': 0.7823046445846558, 'train/loss': 0.8637334108352661, 'validation/accuracy': 0.7188799977302551, 'validation/loss': 1.1443239450454712, 'validation/num_examples': 50000, 'test/accuracy': 0.593000054359436, 'test/loss': 1.7642781734466553, 'test/num_examples': 10000, 'score': 66425.3852956295, 'total_duration': 73841.35216474533, 'accumulated_submission_time': 66425.3852956295, 'accumulated_eval_time': 7401.779304265976, 'accumulated_logging_time': 6.644883871078491, 'global_step': 145228, 'preemption_count': 0}), (146149, {'train/accuracy': 0.7882617115974426, 'train/loss': 0.8386130928993225, 'validation/accuracy': 0.7200599908828735, 'validation/loss': 1.130061388015747, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.7559378147125244, 'test/num_examples': 10000, 'score': 66845.65575814247, 'total_duration': 74312.01202607155, 'accumulated_submission_time': 66845.65575814247, 'accumulated_eval_time': 7452.07506108284, 'accumulated_logging_time': 6.691014051437378, 'global_step': 146149, 'preemption_count': 0}), (147070, {'train/accuracy': 0.7914062142372131, 'train/loss': 0.8205158710479736, 'validation/accuracy': 0.7212799787521362, 'validation/loss': 1.122092366218567, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.7555046081542969, 'test/num_examples': 10000, 'score': 67265.79447817802, 'total_duration': 74781.38368654251, 'accumulated_submission_time': 67265.79447817802, 'accumulated_eval_time': 7501.210858821869, 'accumulated_logging_time': 6.740800857543945, 'global_step': 147070, 'preemption_count': 0}), (147991, {'train/accuracy': 0.8050585985183716, 'train/loss': 0.7596969604492188, 'validation/accuracy': 0.724079966545105, 'validation/loss': 1.102674126625061, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.719403624534607, 'test/num_examples': 10000, 'score': 67685.98072981834, 'total_duration': 75250.80831742287, 'accumulated_submission_time': 67685.98072981834, 'accumulated_eval_time': 7550.349389076233, 'accumulated_logging_time': 6.793242692947388, 'global_step': 147991, 'preemption_count': 0}), (148910, {'train/accuracy': 0.79408198595047, 'train/loss': 0.8039225339889526, 'validation/accuracy': 0.7251399755477905, 'validation/loss': 1.0975062847137451, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.7171880006790161, 'test/num_examples': 10000, 'score': 68106.26304864883, 'total_duration': 75720.92788505554, 'accumulated_submission_time': 68106.26304864883, 'accumulated_eval_time': 7600.08563709259, 'accumulated_logging_time': 6.847251892089844, 'global_step': 148910, 'preemption_count': 0}), (149831, {'train/accuracy': 0.7937890291213989, 'train/loss': 0.8100752830505371, 'validation/accuracy': 0.7240999937057495, 'validation/loss': 1.1172319650650024, 'validation/num_examples': 50000, 'test/accuracy': 0.5993000268936157, 'test/loss': 1.7362432479858398, 'test/num_examples': 10000, 'score': 68526.58949446678, 'total_duration': 76190.50789570808, 'accumulated_submission_time': 68526.58949446678, 'accumulated_eval_time': 7649.241825342178, 'accumulated_logging_time': 6.897162199020386, 'global_step': 149831, 'preemption_count': 0}), (150752, {'train/accuracy': 0.8075780868530273, 'train/loss': 0.7453005909919739, 'validation/accuracy': 0.7288599610328674, 'validation/loss': 1.078874945640564, 'validation/num_examples': 50000, 'test/accuracy': 0.6023000478744507, 'test/loss': 1.6958317756652832, 'test/num_examples': 10000, 'score': 68946.71733403206, 'total_duration': 76659.31583476067, 'accumulated_submission_time': 68946.71733403206, 'accumulated_eval_time': 7697.823964357376, 'accumulated_logging_time': 6.947785139083862, 'global_step': 150752, 'preemption_count': 0}), (151672, {'train/accuracy': 0.7996875047683716, 'train/loss': 0.7857888340950012, 'validation/accuracy': 0.7291399836540222, 'validation/loss': 1.0955058336257935, 'validation/num_examples': 50000, 'test/accuracy': 0.6053000092506409, 'test/loss': 1.7195252180099487, 'test/num_examples': 10000, 'score': 69366.65239262581, 'total_duration': 77128.69496536255, 'accumulated_submission_time': 69366.65239262581, 'accumulated_eval_time': 7747.169746875763, 'accumulated_logging_time': 6.998458623886108, 'global_step': 151672, 'preemption_count': 0}), (152590, {'train/accuracy': 0.8040820360183716, 'train/loss': 0.7654426693916321, 'validation/accuracy': 0.7314599752426147, 'validation/loss': 1.0745552778244019, 'validation/num_examples': 50000, 'test/accuracy': 0.6079000234603882, 'test/loss': 1.6825690269470215, 'test/num_examples': 10000, 'score': 69787.05628466606, 'total_duration': 77600.06755399704, 'accumulated_submission_time': 69787.05628466606, 'accumulated_eval_time': 7798.037398815155, 'accumulated_logging_time': 7.052371025085449, 'global_step': 152590, 'preemption_count': 0}), (153511, {'train/accuracy': 0.8078711032867432, 'train/loss': 0.7358626127243042, 'validation/accuracy': 0.7321999669075012, 'validation/loss': 1.064021110534668, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.678421974182129, 'test/num_examples': 10000, 'score': 70207.12128043175, 'total_duration': 78073.42539596558, 'accumulated_submission_time': 70207.12128043175, 'accumulated_eval_time': 7851.229133844376, 'accumulated_logging_time': 7.106428384780884, 'global_step': 153511, 'preemption_count': 0}), (154432, {'train/accuracy': 0.8040429353713989, 'train/loss': 0.7815302014350891, 'validation/accuracy': 0.7331399917602539, 'validation/loss': 1.0862562656402588, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.7023465633392334, 'test/num_examples': 10000, 'score': 70627.35976624489, 'total_duration': 78544.17288303375, 'accumulated_submission_time': 70627.35976624489, 'accumulated_eval_time': 7901.632784366608, 'accumulated_logging_time': 7.164010286331177, 'global_step': 154432, 'preemption_count': 0}), (155351, {'train/accuracy': 0.804492175579071, 'train/loss': 0.760875940322876, 'validation/accuracy': 0.7349199652671814, 'validation/loss': 1.0727885961532593, 'validation/num_examples': 50000, 'test/accuracy': 0.6113000512123108, 'test/loss': 1.6814213991165161, 'test/num_examples': 10000, 'score': 71047.48124408722, 'total_duration': 79016.50269341469, 'accumulated_submission_time': 71047.48124408722, 'accumulated_eval_time': 7953.746181964874, 'accumulated_logging_time': 7.211568593978882, 'global_step': 155351, 'preemption_count': 0}), (156269, {'train/accuracy': 0.8096483945846558, 'train/loss': 0.7489880323410034, 'validation/accuracy': 0.73499995470047, 'validation/loss': 1.0660606622695923, 'validation/num_examples': 50000, 'test/accuracy': 0.6067000031471252, 'test/loss': 1.6834540367126465, 'test/num_examples': 10000, 'score': 71467.49888920784, 'total_duration': 79486.18399739265, 'accumulated_submission_time': 71467.49888920784, 'accumulated_eval_time': 8003.304003477097, 'accumulated_logging_time': 7.269599676132202, 'global_step': 156269, 'preemption_count': 0}), (157189, {'train/accuracy': 0.8141406178474426, 'train/loss': 0.7198129296302795, 'validation/accuracy': 0.7380399703979492, 'validation/loss': 1.0487289428710938, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.6608270406723022, 'test/num_examples': 10000, 'score': 71887.46869874, 'total_duration': 79955.67472600937, 'accumulated_submission_time': 71887.46869874, 'accumulated_eval_time': 8052.717717885971, 'accumulated_logging_time': 7.328284025192261, 'global_step': 157189, 'preemption_count': 0}), (158105, {'train/accuracy': 0.813769519329071, 'train/loss': 0.7237128019332886, 'validation/accuracy': 0.7416200041770935, 'validation/loss': 1.0368276834487915, 'validation/num_examples': 50000, 'test/accuracy': 0.6190000176429749, 'test/loss': 1.6595739126205444, 'test/num_examples': 10000, 'score': 72307.4182343483, 'total_duration': 80422.82166051865, 'accumulated_submission_time': 72307.4182343483, 'accumulated_eval_time': 8099.809978485107, 'accumulated_logging_time': 7.386165380477905, 'global_step': 158105, 'preemption_count': 0}), (159025, {'train/accuracy': 0.8179101347923279, 'train/loss': 0.6971430778503418, 'validation/accuracy': 0.7404400110244751, 'validation/loss': 1.028631567955017, 'validation/num_examples': 50000, 'test/accuracy': 0.6154000163078308, 'test/loss': 1.6334420442581177, 'test/num_examples': 10000, 'score': 72727.65055274963, 'total_duration': 80892.5890173912, 'accumulated_submission_time': 72727.65055274963, 'accumulated_eval_time': 8149.245040655136, 'accumulated_logging_time': 7.438138723373413, 'global_step': 159025, 'preemption_count': 0}), (159945, {'train/accuracy': 0.8225781321525574, 'train/loss': 0.6903254389762878, 'validation/accuracy': 0.7412799596786499, 'validation/loss': 1.0446081161499023, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.6675461530685425, 'test/num_examples': 10000, 'score': 73147.87590813637, 'total_duration': 81362.51534724236, 'accumulated_submission_time': 73147.87590813637, 'accumulated_eval_time': 8198.83842921257, 'accumulated_logging_time': 7.497429847717285, 'global_step': 159945, 'preemption_count': 0}), (160856, {'train/accuracy': 0.8190624713897705, 'train/loss': 0.7006577253341675, 'validation/accuracy': 0.7416999936103821, 'validation/loss': 1.023085355758667, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.641365885734558, 'test/num_examples': 10000, 'score': 73568.24109864235, 'total_duration': 81832.5364947319, 'accumulated_submission_time': 73568.24109864235, 'accumulated_eval_time': 8248.3914706707, 'accumulated_logging_time': 7.553764581680298, 'global_step': 160856, 'preemption_count': 0}), (161774, {'train/accuracy': 0.8226171731948853, 'train/loss': 0.693584680557251, 'validation/accuracy': 0.7432399988174438, 'validation/loss': 1.023402214050293, 'validation/num_examples': 50000, 'test/accuracy': 0.6225000023841858, 'test/loss': 1.6433871984481812, 'test/num_examples': 10000, 'score': 73988.53427219391, 'total_duration': 82304.36846089363, 'accumulated_submission_time': 73988.53427219391, 'accumulated_eval_time': 8299.826081991196, 'accumulated_logging_time': 7.611063718795776, 'global_step': 161774, 'preemption_count': 0}), (162692, {'train/accuracy': 0.8324609398841858, 'train/loss': 0.6406580805778503, 'validation/accuracy': 0.7445999979972839, 'validation/loss': 1.0042402744293213, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.6267750263214111, 'test/num_examples': 10000, 'score': 74408.44389462471, 'total_duration': 82774.69685792923, 'accumulated_submission_time': 74408.44389462471, 'accumulated_eval_time': 8350.1466858387, 'accumulated_logging_time': 7.6614062786102295, 'global_step': 162692, 'preemption_count': 0}), (163612, {'train/accuracy': 0.8236718773841858, 'train/loss': 0.6884275078773499, 'validation/accuracy': 0.7479199767112732, 'validation/loss': 1.008457899093628, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.6180665493011475, 'test/num_examples': 10000, 'score': 74828.47229576111, 'total_duration': 83244.47105193138, 'accumulated_submission_time': 74828.47229576111, 'accumulated_eval_time': 8399.791483402252, 'accumulated_logging_time': 7.7152698040008545, 'global_step': 163612, 'preemption_count': 0}), (164533, {'train/accuracy': 0.8290624618530273, 'train/loss': 0.662371039390564, 'validation/accuracy': 0.7500999569892883, 'validation/loss': 1.0033659934997559, 'validation/num_examples': 50000, 'test/accuracy': 0.6251000165939331, 'test/loss': 1.6135456562042236, 'test/num_examples': 10000, 'score': 75248.7256937027, 'total_duration': 83715.26717567444, 'accumulated_submission_time': 75248.7256937027, 'accumulated_eval_time': 8450.2347574234, 'accumulated_logging_time': 7.7678821086883545, 'global_step': 164533, 'preemption_count': 0}), (165452, {'train/accuracy': 0.8309765458106995, 'train/loss': 0.6480200886726379, 'validation/accuracy': 0.7496799826622009, 'validation/loss': 0.9904419183731079, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.5892136096954346, 'test/num_examples': 10000, 'score': 75669.07854604721, 'total_duration': 84185.15949583054, 'accumulated_submission_time': 75669.07854604721, 'accumulated_eval_time': 8499.674641370773, 'accumulated_logging_time': 7.819833278656006, 'global_step': 165452, 'preemption_count': 0}), (166372, {'train/accuracy': 0.8332812190055847, 'train/loss': 0.6423313617706299, 'validation/accuracy': 0.7530800104141235, 'validation/loss': 0.9780200719833374, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.583455204963684, 'test/num_examples': 10000, 'score': 76089.07453298569, 'total_duration': 84657.53774857521, 'accumulated_submission_time': 76089.07453298569, 'accumulated_eval_time': 8551.956509113312, 'accumulated_logging_time': 7.872750520706177, 'global_step': 166372, 'preemption_count': 0}), (167291, {'train/accuracy': 0.8354296684265137, 'train/loss': 0.6281799077987671, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 0.9751542210578918, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.580045223236084, 'test/num_examples': 10000, 'score': 76509.43893957138, 'total_duration': 85127.15686106682, 'accumulated_submission_time': 76509.43893957138, 'accumulated_eval_time': 8601.107754945755, 'accumulated_logging_time': 7.928579807281494, 'global_step': 167291, 'preemption_count': 0}), (168211, {'train/accuracy': 0.8359179496765137, 'train/loss': 0.6287804841995239, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 0.9704552888870239, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.5775206089019775, 'test/num_examples': 10000, 'score': 76929.56784844398, 'total_duration': 85598.03895044327, 'accumulated_submission_time': 76929.56784844398, 'accumulated_eval_time': 8651.759412050247, 'accumulated_logging_time': 7.982419967651367, 'global_step': 168211, 'preemption_count': 0}), (169131, {'train/accuracy': 0.8357812166213989, 'train/loss': 0.6224377751350403, 'validation/accuracy': 0.7562800049781799, 'validation/loss': 0.9682385921478271, 'validation/num_examples': 50000, 'test/accuracy': 0.6381000280380249, 'test/loss': 1.56676185131073, 'test/num_examples': 10000, 'score': 77349.9605910778, 'total_duration': 86064.29647517204, 'accumulated_submission_time': 77349.9605910778, 'accumulated_eval_time': 8697.523322582245, 'accumulated_logging_time': 8.035626888275146, 'global_step': 169131, 'preemption_count': 0})], 'global_step': 169511}
I0202 12:58:25.607438 139936116377408 submission_runner.py:586] Timing: 77520.38380241394
I0202 12:58:25.607529 139936116377408 submission_runner.py:588] Total number of evals: 185
I0202 12:58:25.607580 139936116377408 submission_runner.py:589] ====================
I0202 12:58:25.607632 139936116377408 submission_runner.py:542] Using RNG seed 2064292405
I0202 12:58:25.609097 139936116377408 submission_runner.py:551] --- Tuning run 4/5 ---
I0202 12:58:25.609216 139936116377408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_4.
I0202 12:58:25.612170 139936116377408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_4/hparams.json.
I0202 12:58:25.613047 139936116377408 submission_runner.py:206] Initializing dataset.
I0202 12:58:25.622514 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0202 12:58:25.633197 139936116377408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0202 12:58:25.833759 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0202 12:58:30.235041 139936116377408 submission_runner.py:213] Initializing model.
I0202 12:58:37.495278 139936116377408 submission_runner.py:255] Initializing optimizer.
I0202 12:58:38.007262 139936116377408 submission_runner.py:262] Initializing metrics bundle.
I0202 12:58:38.007503 139936116377408 submission_runner.py:280] Initializing checkpoint and logger.
I0202 12:58:38.023672 139936116377408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_4 with prefix checkpoint_
I0202 12:58:38.023843 139936116377408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0202 12:58:54.454058 139936116377408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0202 12:59:09.843090 139936116377408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_4/flags_0.json.
I0202 12:59:09.848896 139936116377408 submission_runner.py:314] Starting training loop.
I0202 12:59:45.985594 139774392231680 logging_writer.py:48] [0] global_step=0, grad_norm=0.3655499815940857, loss=6.9077558517456055
I0202 12:59:46.000180 139936116377408 spec.py:321] Evaluating on the training split.
I0202 12:59:54.426258 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:00:14.018139 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:00:15.620604 139936116377408 submission_runner.py:408] Time since start: 65.77s, 	Step: 1, 	{'train/accuracy': 0.0009570312104187906, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.151187896728516, 'total_duration': 65.77166390419006, 'accumulated_submission_time': 36.151187896728516, 'accumulated_eval_time': 29.620367527008057, 'accumulated_logging_time': 0}
I0202 13:00:15.629016 139774400624384 logging_writer.py:48] [1] accumulated_eval_time=29.620368, accumulated_logging_time=0, accumulated_submission_time=36.151188, global_step=1, preemption_count=0, score=36.151188, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=65.771664, train/accuracy=0.000957, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0202 13:01:22.186255 139774434195200 logging_writer.py:48] [100] global_step=100, grad_norm=0.6376575231552124, loss=6.847419261932373
I0202 13:02:08.253534 139774417409792 logging_writer.py:48] [200] global_step=200, grad_norm=0.9279347062110901, loss=6.702489852905273
I0202 13:02:54.601952 139774434195200 logging_writer.py:48] [300] global_step=300, grad_norm=1.362532138824463, loss=6.573473930358887
I0202 13:03:41.164736 139774417409792 logging_writer.py:48] [400] global_step=400, grad_norm=1.0964962244033813, loss=6.534063816070557
I0202 13:04:27.752948 139774434195200 logging_writer.py:48] [500] global_step=500, grad_norm=0.8318119049072266, loss=6.685945987701416
I0202 13:05:14.605063 139774417409792 logging_writer.py:48] [600] global_step=600, grad_norm=0.9151761531829834, loss=6.43905782699585
I0202 13:06:01.138991 139774434195200 logging_writer.py:48] [700] global_step=700, grad_norm=0.9871315360069275, loss=6.285146713256836
I0202 13:06:48.081890 139774417409792 logging_writer.py:48] [800] global_step=800, grad_norm=0.9208433032035828, loss=6.621548652648926
I0202 13:07:15.641079 139936116377408 spec.py:321] Evaluating on the training split.
I0202 13:07:28.776270 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:08:00.310896 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:08:01.924654 139936116377408 submission_runner.py:408] Time since start: 532.08s, 	Step: 861, 	{'train/accuracy': 0.033964842557907104, 'train/loss': 5.889985084533691, 'validation/accuracy': 0.031219998374581337, 'validation/loss': 5.931602478027344, 'validation/num_examples': 50000, 'test/accuracy': 0.026500001549720764, 'test/loss': 6.062020778656006, 'test/num_examples': 10000, 'score': 456.1083743572235, 'total_duration': 532.0756976604462, 'accumulated_submission_time': 456.1083743572235, 'accumulated_eval_time': 75.90391826629639, 'accumulated_logging_time': 0.017279624938964844}
I0202 13:08:01.948900 139774434195200 logging_writer.py:48] [861] accumulated_eval_time=75.903918, accumulated_logging_time=0.017280, accumulated_submission_time=456.108374, global_step=861, preemption_count=0, score=456.108374, test/accuracy=0.026500, test/loss=6.062021, test/num_examples=10000, total_duration=532.075698, train/accuracy=0.033965, train/loss=5.889985, validation/accuracy=0.031220, validation/loss=5.931602, validation/num_examples=50000
I0202 13:08:17.891899 139774417409792 logging_writer.py:48] [900] global_step=900, grad_norm=0.6161331534385681, loss=6.4734954833984375
I0202 13:09:01.740677 139774434195200 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6501962542533875, loss=6.0994977951049805
I0202 13:09:48.237632 139774417409792 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6537315249443054, loss=6.756523132324219
I0202 13:10:34.863284 139774434195200 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6039871573448181, loss=6.091100692749023
I0202 13:11:21.495777 139774417409792 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.923751950263977, loss=6.799857139587402
I0202 13:12:08.455059 139774434195200 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.739203929901123, loss=6.235940933227539
I0202 13:12:54.831588 139774417409792 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6416047811508179, loss=6.285388946533203
I0202 13:13:41.608626 139774434195200 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.62485271692276, loss=5.901751518249512
I0202 13:14:28.213629 139774417409792 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6894649863243103, loss=5.8360276222229
I0202 13:15:02.021214 139936116377408 spec.py:321] Evaluating on the training split.
I0202 13:15:14.314523 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:15:48.622661 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:15:50.229547 139936116377408 submission_runner.py:408] Time since start: 1000.38s, 	Step: 1774, 	{'train/accuracy': 0.07195312529802322, 'train/loss': 5.346263408660889, 'validation/accuracy': 0.06695999950170517, 'validation/loss': 5.416959285736084, 'validation/num_examples': 50000, 'test/accuracy': 0.05350000411272049, 'test/loss': 5.638273239135742, 'test/num_examples': 10000, 'score': 876.1209945678711, 'total_duration': 1000.3806068897247, 'accumulated_submission_time': 876.1209945678711, 'accumulated_eval_time': 124.11227488517761, 'accumulated_logging_time': 0.05303072929382324}
I0202 13:15:50.245011 139774434195200 logging_writer.py:48] [1774] accumulated_eval_time=124.112275, accumulated_logging_time=0.053031, accumulated_submission_time=876.120995, global_step=1774, preemption_count=0, score=876.120995, test/accuracy=0.053500, test/loss=5.638273, test/num_examples=10000, total_duration=1000.380607, train/accuracy=0.071953, train/loss=5.346263, validation/accuracy=0.066960, validation/loss=5.416959, validation/num_examples=50000
I0202 13:16:00.977374 139774417409792 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6179776787757874, loss=5.966546058654785
I0202 13:16:44.102185 139774434195200 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.48420342803001404, loss=6.413880348205566
I0202 13:17:30.460629 139774417409792 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.4727131128311157, loss=6.534823417663574
I0202 13:18:16.737733 139774434195200 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.486954003572464, loss=5.756412982940674
I0202 13:19:02.518011 139774417409792 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3861386477947235, loss=6.222630500793457
I0202 13:19:48.945613 139774434195200 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5189235210418701, loss=5.791855812072754
I0202 13:20:35.087321 139774417409792 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5956610441207886, loss=5.601197242736816
I0202 13:21:21.632057 139774434195200 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.4276297688484192, loss=6.416365623474121
I0202 13:22:07.862227 139774417409792 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.4380909204483032, loss=6.123587608337402
I0202 13:22:50.337502 139936116377408 spec.py:321] Evaluating on the training split.
I0202 13:23:02.948358 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:23:42.580995 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:23:44.196515 139936116377408 submission_runner.py:408] Time since start: 1474.35s, 	Step: 2694, 	{'train/accuracy': 0.1066601574420929, 'train/loss': 4.961225986480713, 'validation/accuracy': 0.09883999824523926, 'validation/loss': 5.016429424285889, 'validation/num_examples': 50000, 'test/accuracy': 0.07660000026226044, 'test/loss': 5.27877950668335, 'test/num_examples': 10000, 'score': 1296.1530268192291, 'total_duration': 1474.3475806713104, 'accumulated_submission_time': 1296.1530268192291, 'accumulated_eval_time': 177.97129225730896, 'accumulated_logging_time': 0.08097434043884277}
I0202 13:23:44.212380 139774434195200 logging_writer.py:48] [2694] accumulated_eval_time=177.971292, accumulated_logging_time=0.080974, accumulated_submission_time=1296.153027, global_step=2694, preemption_count=0, score=1296.153027, test/accuracy=0.076600, test/loss=5.278780, test/num_examples=10000, total_duration=1474.347581, train/accuracy=0.106660, train/loss=4.961226, validation/accuracy=0.098840, validation/loss=5.016429, validation/num_examples=50000
I0202 13:23:46.999869 139774417409792 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4862789511680603, loss=5.894248962402344
I0202 13:24:29.274372 139774434195200 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.4877201020717621, loss=5.537540912628174
I0202 13:25:15.451334 139774417409792 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.4989551305770874, loss=5.608197212219238
I0202 13:26:01.627989 139774434195200 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5830240845680237, loss=5.443195343017578
I0202 13:26:47.791380 139774417409792 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.5260329246520996, loss=5.360604286193848
I0202 13:27:33.971008 139774434195200 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.38235047459602356, loss=6.497609615325928
I0202 13:28:20.053682 139774417409792 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5685756802558899, loss=5.42667293548584
I0202 13:29:06.371802 139774434195200 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6085950136184692, loss=5.845279216766357
I0202 13:29:52.586805 139774417409792 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6693229675292969, loss=5.449257850646973
I0202 13:30:38.694750 139774434195200 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.508020281791687, loss=5.394008159637451
I0202 13:30:44.399959 139936116377408 spec.py:321] Evaluating on the training split.
I0202 13:30:56.623822 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:31:31.176679 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:31:32.794170 139936116377408 submission_runner.py:408] Time since start: 1942.95s, 	Step: 3614, 	{'train/accuracy': 0.12578125298023224, 'train/loss': 4.762739181518555, 'validation/accuracy': 0.1183599978685379, 'validation/loss': 4.825972557067871, 'validation/num_examples': 50000, 'test/accuracy': 0.0958000048995018, 'test/loss': 5.121191024780273, 'test/num_examples': 10000, 'score': 1716.2821543216705, 'total_duration': 1942.9452345371246, 'accumulated_submission_time': 1716.2821543216705, 'accumulated_eval_time': 226.36552715301514, 'accumulated_logging_time': 0.10770201683044434}
I0202 13:31:32.809579 139774417409792 logging_writer.py:48] [3614] accumulated_eval_time=226.365527, accumulated_logging_time=0.107702, accumulated_submission_time=1716.282154, global_step=3614, preemption_count=0, score=1716.282154, test/accuracy=0.095800, test/loss=5.121191, test/num_examples=10000, total_duration=1942.945235, train/accuracy=0.125781, train/loss=4.762739, validation/accuracy=0.118360, validation/loss=4.825973, validation/num_examples=50000
I0202 13:32:08.699913 139774434195200 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6386221647262573, loss=5.367698669433594
I0202 13:32:55.104332 139774417409792 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.4736885130405426, loss=5.293456554412842
I0202 13:33:41.801847 139774434195200 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8007697463035583, loss=5.2207560539245605
I0202 13:34:28.267428 139774417409792 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5677149891853333, loss=6.382204532623291
I0202 13:35:14.695900 139774434195200 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7429167032241821, loss=6.398656368255615
I0202 13:36:00.977912 139774417409792 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6139580011367798, loss=5.331984519958496
I0202 13:36:47.515006 139774434195200 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5605512261390686, loss=6.336531162261963
I0202 13:37:33.851797 139774417409792 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7766953706741333, loss=5.160114288330078
I0202 13:38:20.150968 139774434195200 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5421901345252991, loss=5.993781089782715
I0202 13:38:32.822255 139936116377408 spec.py:321] Evaluating on the training split.
I0202 13:38:44.486722 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:39:15.879414 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:39:17.486794 139936116377408 submission_runner.py:408] Time since start: 2407.64s, 	Step: 4529, 	{'train/accuracy': 0.16820311546325684, 'train/loss': 4.37608528137207, 'validation/accuracy': 0.1546199917793274, 'validation/loss': 4.472805976867676, 'validation/num_examples': 50000, 'test/accuracy': 0.11570000648498535, 'test/loss': 4.830338478088379, 'test/num_examples': 10000, 'score': 2136.238264322281, 'total_duration': 2407.637862443924, 'accumulated_submission_time': 2136.238264322281, 'accumulated_eval_time': 271.0300600528717, 'accumulated_logging_time': 0.1323871612548828}
I0202 13:39:17.502112 139774417409792 logging_writer.py:48] [4529] accumulated_eval_time=271.030060, accumulated_logging_time=0.132387, accumulated_submission_time=2136.238264, global_step=4529, preemption_count=0, score=2136.238264, test/accuracy=0.115700, test/loss=4.830338, test/num_examples=10000, total_duration=2407.637862, train/accuracy=0.168203, train/loss=4.376085, validation/accuracy=0.154620, validation/loss=4.472806, validation/num_examples=50000
I0202 13:39:46.427234 139774434195200 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.779306173324585, loss=5.079288482666016
I0202 13:40:32.659106 139774417409792 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6101513504981995, loss=5.17583703994751
I0202 13:41:19.143190 139774434195200 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6775107383728027, loss=4.907220840454102
I0202 13:42:05.807416 139774417409792 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6693233251571655, loss=5.769416809082031
I0202 13:42:52.427682 139774434195200 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7370275259017944, loss=5.597152233123779
I0202 13:43:38.708833 139774417409792 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7471035122871399, loss=5.181201934814453
I0202 13:44:25.249940 139774434195200 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.654471755027771, loss=4.9436163902282715
I0202 13:45:11.546848 139774417409792 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.749295175075531, loss=4.949343681335449
I0202 13:45:57.781798 139774434195200 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.6197304129600525, loss=4.926572799682617
I0202 13:46:17.942052 139936116377408 spec.py:321] Evaluating on the training split.
I0202 13:46:28.857542 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:47:05.622290 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:47:07.229804 139936116377408 submission_runner.py:408] Time since start: 2877.38s, 	Step: 5445, 	{'train/accuracy': 0.18382811546325684, 'train/loss': 4.257326126098633, 'validation/accuracy': 0.1714800000190735, 'validation/loss': 4.334697246551514, 'validation/num_examples': 50000, 'test/accuracy': 0.13300000131130219, 'test/loss': 4.718831539154053, 'test/num_examples': 10000, 'score': 2556.621108531952, 'total_duration': 2877.380870580673, 'accumulated_submission_time': 2556.621108531952, 'accumulated_eval_time': 320.31780982017517, 'accumulated_logging_time': 0.15650534629821777}
I0202 13:47:07.245106 139774417409792 logging_writer.py:48] [5445] accumulated_eval_time=320.317810, accumulated_logging_time=0.156505, accumulated_submission_time=2556.621109, global_step=5445, preemption_count=0, score=2556.621109, test/accuracy=0.133000, test/loss=4.718832, test/num_examples=10000, total_duration=2877.380871, train/accuracy=0.183828, train/loss=4.257326, validation/accuracy=0.171480, validation/loss=4.334697, validation/num_examples=50000
I0202 13:47:29.500948 139774434195200 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6934272050857544, loss=4.836215019226074
I0202 13:48:14.437209 139774417409792 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5734598636627197, loss=6.252729892730713
I0202 13:49:00.637446 139774434195200 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5569326281547546, loss=6.269748210906982
I0202 13:49:46.841860 139774417409792 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9343687891960144, loss=5.101519584655762
I0202 13:50:33.161552 139774434195200 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8220937252044678, loss=4.687013626098633
I0202 13:51:19.672691 139774417409792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.870904266834259, loss=4.9526262283325195
I0202 13:52:05.853888 139774434195200 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8615638017654419, loss=4.830051898956299
I0202 13:52:51.816968 139774417409792 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8438535928726196, loss=5.1938982009887695
I0202 13:53:38.322004 139774434195200 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8845248818397522, loss=6.313832759857178
I0202 13:54:07.316122 139936116377408 spec.py:321] Evaluating on the training split.
I0202 13:54:19.077361 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 13:54:55.670386 139936116377408 spec.py:349] Evaluating on the test split.
I0202 13:54:57.279914 139936116377408 submission_runner.py:408] Time since start: 3347.43s, 	Step: 6364, 	{'train/accuracy': 0.22624999284744263, 'train/loss': 3.872288942337036, 'validation/accuracy': 0.21079999208450317, 'validation/loss': 3.9770443439483643, 'validation/num_examples': 50000, 'test/accuracy': 0.1616000086069107, 'test/loss': 4.43961763381958, 'test/num_examples': 10000, 'score': 2976.629909515381, 'total_duration': 3347.430968284607, 'accumulated_submission_time': 2976.629909515381, 'accumulated_eval_time': 370.2815811634064, 'accumulated_logging_time': 0.1853024959564209}
I0202 13:54:57.295891 139774417409792 logging_writer.py:48] [6364] accumulated_eval_time=370.281581, accumulated_logging_time=0.185302, accumulated_submission_time=2976.629910, global_step=6364, preemption_count=0, score=2976.629910, test/accuracy=0.161600, test/loss=4.439618, test/num_examples=10000, total_duration=3347.430968, train/accuracy=0.226250, train/loss=3.872289, validation/accuracy=0.210800, validation/loss=3.977044, validation/num_examples=50000
I0202 13:55:11.992835 139774434195200 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8292651772499084, loss=5.724515914916992
I0202 13:55:55.724264 139774417409792 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8148916363716125, loss=4.970717906951904
I0202 13:56:41.895695 139774434195200 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7192457914352417, loss=4.909964084625244
I0202 13:57:28.254086 139774417409792 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7408788204193115, loss=5.146170616149902
I0202 13:58:16.352700 139774434195200 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8055777549743652, loss=4.953671932220459
I0202 13:59:02.524863 139774417409792 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.6969496607780457, loss=4.706662178039551
I0202 13:59:48.571879 139774434195200 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.0280601978302002, loss=4.829719066619873
I0202 14:00:34.827250 139774417409792 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8235598206520081, loss=4.5850629806518555
I0202 14:01:21.277733 139774434195200 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8880839347839355, loss=4.772223472595215
I0202 14:01:57.642564 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:02:10.163900 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:02:45.913388 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:02:47.530062 139936116377408 submission_runner.py:408] Time since start: 3817.68s, 	Step: 7280, 	{'train/accuracy': 0.2319921851158142, 'train/loss': 3.833322286605835, 'validation/accuracy': 0.21235999464988708, 'validation/loss': 3.9645304679870605, 'validation/num_examples': 50000, 'test/accuracy': 0.17000000178813934, 'test/loss': 4.424200534820557, 'test/num_examples': 10000, 'score': 3396.913996696472, 'total_duration': 3817.68110537529, 'accumulated_submission_time': 3396.913996696472, 'accumulated_eval_time': 420.16905403137207, 'accumulated_logging_time': 0.21693873405456543}
I0202 14:02:47.550956 139774417409792 logging_writer.py:48] [7280] accumulated_eval_time=420.169054, accumulated_logging_time=0.216939, accumulated_submission_time=3396.913997, global_step=7280, preemption_count=0, score=3396.913997, test/accuracy=0.170000, test/loss=4.424201, test/num_examples=10000, total_duration=3817.681105, train/accuracy=0.231992, train/loss=3.833322, validation/accuracy=0.212360, validation/loss=3.964530, validation/num_examples=50000
I0202 14:02:55.886787 139774434195200 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7799285650253296, loss=6.336007595062256
I0202 14:03:38.986105 139774417409792 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0178779363632202, loss=4.734283447265625
I0202 14:04:25.291615 139774434195200 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.995811402797699, loss=4.585123538970947
I0202 14:05:11.622859 139774417409792 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6923847794532776, loss=4.833370685577393
I0202 14:05:57.553771 139774434195200 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7708591222763062, loss=4.6020355224609375
I0202 14:06:43.745595 139774417409792 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8069159388542175, loss=4.607520580291748
I0202 14:07:29.902635 139774434195200 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7939661741256714, loss=5.004179000854492
I0202 14:08:16.273881 139774417409792 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9527177810668945, loss=6.245468616485596
I0202 14:09:02.529938 139774434195200 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8378159999847412, loss=4.759934425354004
I0202 14:09:47.553004 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:09:59.517678 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:10:34.389662 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:10:35.997224 139936116377408 submission_runner.py:408] Time since start: 4286.15s, 	Step: 8200, 	{'train/accuracy': 0.25533202290534973, 'train/loss': 3.758265256881714, 'validation/accuracy': 0.22007998824119568, 'validation/loss': 3.949136257171631, 'validation/num_examples': 50000, 'test/accuracy': 0.17180000245571136, 'test/loss': 4.393675327301025, 'test/num_examples': 10000, 'score': 3816.8563516139984, 'total_duration': 4286.148283243179, 'accumulated_submission_time': 3816.8563516139984, 'accumulated_eval_time': 468.6132698059082, 'accumulated_logging_time': 0.24872469902038574}
I0202 14:10:36.016798 139774417409792 logging_writer.py:48] [8200] accumulated_eval_time=468.613270, accumulated_logging_time=0.248725, accumulated_submission_time=3816.856352, global_step=8200, preemption_count=0, score=3816.856352, test/accuracy=0.171800, test/loss=4.393675, test/num_examples=10000, total_duration=4286.148283, train/accuracy=0.255332, train/loss=3.758265, validation/accuracy=0.220080, validation/loss=3.949136, validation/num_examples=50000
I0202 14:10:36.422147 139774434195200 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7801799774169922, loss=5.4983625411987305
I0202 14:11:18.381484 139774417409792 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9451500773429871, loss=4.677331447601318
I0202 14:12:04.594498 139774434195200 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7718849778175354, loss=4.53889274597168
I0202 14:12:50.871108 139774417409792 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9796625375747681, loss=4.553781509399414
I0202 14:13:37.078112 139774434195200 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7314033508300781, loss=5.334152698516846
I0202 14:14:23.337930 139774417409792 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7407140135765076, loss=5.9810638427734375
I0202 14:15:09.885551 139774434195200 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.7572294473648071, loss=4.4553632736206055
I0202 14:15:55.945853 139774417409792 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6553694605827332, loss=5.871369361877441
I0202 14:16:42.197792 139774434195200 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6928794980049133, loss=4.784287452697754
I0202 14:17:28.647968 139774417409792 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9834628701210022, loss=4.551712989807129
I0202 14:17:36.053585 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:17:46.714418 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:18:20.243275 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:18:21.863024 139936116377408 submission_runner.py:408] Time since start: 4752.01s, 	Step: 9118, 	{'train/accuracy': 0.26787108182907104, 'train/loss': 3.58646821975708, 'validation/accuracy': 0.2466599941253662, 'validation/loss': 3.7260239124298096, 'validation/num_examples': 50000, 'test/accuracy': 0.19190001487731934, 'test/loss': 4.206649303436279, 'test/num_examples': 10000, 'score': 4236.834023237228, 'total_duration': 4752.014056444168, 'accumulated_submission_time': 4236.834023237228, 'accumulated_eval_time': 514.4226930141449, 'accumulated_logging_time': 0.2796754837036133}
I0202 14:18:21.880474 139774434195200 logging_writer.py:48] [9118] accumulated_eval_time=514.422693, accumulated_logging_time=0.279675, accumulated_submission_time=4236.834023, global_step=9118, preemption_count=0, score=4236.834023, test/accuracy=0.191900, test/loss=4.206649, test/num_examples=10000, total_duration=4752.014056, train/accuracy=0.267871, train/loss=3.586468, validation/accuracy=0.246660, validation/loss=3.726024, validation/num_examples=50000
I0202 14:18:55.810375 139774417409792 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9847379326820374, loss=4.524820327758789
I0202 14:19:41.920602 139774434195200 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6402597427368164, loss=5.6616973876953125
I0202 14:20:28.141113 139774417409792 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0258625745773315, loss=4.4380292892456055
I0202 14:21:14.450869 139774434195200 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7587928175926208, loss=6.20313024520874
I0202 14:22:00.939434 139774417409792 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6402052044868469, loss=5.919223308563232
I0202 14:22:47.536083 139774434195200 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7487326264381409, loss=6.0509233474731445
I0202 14:23:34.039223 139774417409792 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9539966583251953, loss=4.4019670486450195
I0202 14:24:20.459324 139774434195200 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6849119663238525, loss=6.107555389404297
I0202 14:25:06.957817 139774417409792 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9275405406951904, loss=4.387159824371338
I0202 14:25:21.937967 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:25:33.605406 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:26:08.606982 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:26:10.210465 139936116377408 submission_runner.py:408] Time since start: 5220.36s, 	Step: 10034, 	{'train/accuracy': 0.2818945348262787, 'train/loss': 3.467654228210449, 'validation/accuracy': 0.26075997948646545, 'validation/loss': 3.6061902046203613, 'validation/num_examples': 50000, 'test/accuracy': 0.2006000131368637, 'test/loss': 4.112731456756592, 'test/num_examples': 10000, 'score': 4656.83242058754, 'total_duration': 5220.361520528793, 'accumulated_submission_time': 4656.83242058754, 'accumulated_eval_time': 562.69517993927, 'accumulated_logging_time': 0.3083176612854004}
I0202 14:26:10.228280 139774434195200 logging_writer.py:48] [10034] accumulated_eval_time=562.695180, accumulated_logging_time=0.308318, accumulated_submission_time=4656.832421, global_step=10034, preemption_count=0, score=4656.832421, test/accuracy=0.200600, test/loss=4.112731, test/num_examples=10000, total_duration=5220.361521, train/accuracy=0.281895, train/loss=3.467654, validation/accuracy=0.260760, validation/loss=3.606190, validation/num_examples=50000
I0202 14:26:36.881125 139774417409792 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.8426501750946045, loss=6.123910427093506
I0202 14:27:22.798070 139774434195200 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6376595497131348, loss=5.5052971839904785
I0202 14:28:09.489860 139774417409792 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.8334407210350037, loss=5.470473289489746
I0202 14:28:55.818431 139774434195200 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.8166477680206299, loss=5.090119361877441
I0202 14:29:42.076294 139774417409792 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8156838417053223, loss=4.976376533508301
I0202 14:30:28.306084 139774434195200 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7932195663452148, loss=4.40079402923584
I0202 14:31:14.529831 139774417409792 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.2955294847488403, loss=4.557458877563477
I0202 14:32:00.809147 139774434195200 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.9925715923309326, loss=4.407249450683594
I0202 14:32:47.149796 139774417409792 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6858540177345276, loss=5.360805511474609
I0202 14:33:10.426665 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:33:20.857664 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:33:55.759952 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:33:57.367240 139936116377408 submission_runner.py:408] Time since start: 5687.52s, 	Step: 10952, 	{'train/accuracy': 0.29472655057907104, 'train/loss': 3.4312899112701416, 'validation/accuracy': 0.259660005569458, 'validation/loss': 3.6437604427337646, 'validation/num_examples': 50000, 'test/accuracy': 0.19820000231266022, 'test/loss': 4.1670637130737305, 'test/num_examples': 10000, 'score': 5076.973096132278, 'total_duration': 5687.518299818039, 'accumulated_submission_time': 5076.973096132278, 'accumulated_eval_time': 609.6357562541962, 'accumulated_logging_time': 0.3355536460876465}
I0202 14:33:57.384626 139774434195200 logging_writer.py:48] [10952] accumulated_eval_time=609.635756, accumulated_logging_time=0.335554, accumulated_submission_time=5076.973096, global_step=10952, preemption_count=0, score=5076.973096, test/accuracy=0.198200, test/loss=4.167064, test/num_examples=10000, total_duration=5687.518300, train/accuracy=0.294727, train/loss=3.431290, validation/accuracy=0.259660, validation/loss=3.643760, validation/num_examples=50000
I0202 14:34:16.879847 139774417409792 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8645801544189453, loss=4.305268287658691
I0202 14:35:01.385684 139774434195200 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9735541343688965, loss=4.309907913208008
I0202 14:35:47.886455 139774417409792 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8681838512420654, loss=4.5307183265686035
I0202 14:36:34.445175 139774434195200 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.76340651512146, loss=4.351643085479736
I0202 14:37:20.604361 139774417409792 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6709948182106018, loss=6.029845714569092
I0202 14:38:07.077846 139774434195200 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9221733808517456, loss=5.085607528686523
I0202 14:38:53.409942 139774417409792 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9248375296592712, loss=4.357784748077393
I0202 14:39:39.721924 139774434195200 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8079268932342529, loss=5.466876029968262
I0202 14:40:25.981726 139774417409792 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9284422993659973, loss=4.22813606262207
I0202 14:40:57.685521 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:41:08.585726 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:41:45.234241 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:41:46.833450 139936116377408 submission_runner.py:408] Time since start: 6156.98s, 	Step: 11870, 	{'train/accuracy': 0.28761717677116394, 'train/loss': 3.4422073364257812, 'validation/accuracy': 0.2721000015735626, 'validation/loss': 3.556260108947754, 'validation/num_examples': 50000, 'test/accuracy': 0.20550000667572021, 'test/loss': 4.087541103363037, 'test/num_examples': 10000, 'score': 5497.2170152664185, 'total_duration': 6156.984508275986, 'accumulated_submission_time': 5497.2170152664185, 'accumulated_eval_time': 658.7836654186249, 'accumulated_logging_time': 0.36228036880493164}
I0202 14:41:46.851564 139774434195200 logging_writer.py:48] [11870] accumulated_eval_time=658.783665, accumulated_logging_time=0.362280, accumulated_submission_time=5497.217015, global_step=11870, preemption_count=0, score=5497.217015, test/accuracy=0.205500, test/loss=4.087541, test/num_examples=10000, total_duration=6156.984508, train/accuracy=0.287617, train/loss=3.442207, validation/accuracy=0.272100, validation/loss=3.556260, validation/num_examples=50000
I0202 14:41:59.183229 139774417409792 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6396518349647522, loss=6.012582778930664
I0202 14:42:42.571408 139774434195200 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9309958219528198, loss=4.202846050262451
I0202 14:43:28.609193 139774417409792 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.030448317527771, loss=4.866006374359131
I0202 14:44:14.797520 139774434195200 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7830262780189514, loss=4.385378837585449
I0202 14:45:00.769508 139774417409792 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.974238932132721, loss=4.289024829864502
I0202 14:45:47.278915 139774434195200 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8531801104545593, loss=4.419563293457031
I0202 14:46:33.952832 139774417409792 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8989340662956238, loss=4.226126670837402
I0202 14:47:20.405549 139774434195200 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0137416124343872, loss=4.746884346008301
I0202 14:48:06.698955 139774417409792 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.09075129032135, loss=4.359266757965088
I0202 14:48:47.017601 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:48:58.962664 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:49:28.831881 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:49:30.440544 139936116377408 submission_runner.py:408] Time since start: 6620.59s, 	Step: 12789, 	{'train/accuracy': 0.29289060831069946, 'train/loss': 3.4402520656585693, 'validation/accuracy': 0.26941999793052673, 'validation/loss': 3.5773630142211914, 'validation/num_examples': 50000, 'test/accuracy': 0.2110000103712082, 'test/loss': 4.108162879943848, 'test/num_examples': 10000, 'score': 5917.322836399078, 'total_duration': 6620.591611146927, 'accumulated_submission_time': 5917.322836399078, 'accumulated_eval_time': 702.2066161632538, 'accumulated_logging_time': 0.39301395416259766}
I0202 14:49:30.461015 139774434195200 logging_writer.py:48] [12789] accumulated_eval_time=702.206616, accumulated_logging_time=0.393014, accumulated_submission_time=5917.322836, global_step=12789, preemption_count=0, score=5917.322836, test/accuracy=0.211000, test/loss=4.108163, test/num_examples=10000, total_duration=6620.591611, train/accuracy=0.292891, train/loss=3.440252, validation/accuracy=0.269420, validation/loss=3.577363, validation/num_examples=50000
I0202 14:49:35.227449 139774417409792 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.8860733509063721, loss=5.517330169677734
I0202 14:50:17.667337 139774434195200 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.776970386505127, loss=4.15479850769043
I0202 14:51:03.814601 139774417409792 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.0288946628570557, loss=4.281942844390869
I0202 14:51:50.305084 139774434195200 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.8052310347557068, loss=4.306689262390137
I0202 14:52:36.230541 139774417409792 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.0475564002990723, loss=4.339219093322754
I0202 14:53:22.828958 139774434195200 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.8405435085296631, loss=4.195071220397949
I0202 14:54:09.369436 139774417409792 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8268879652023315, loss=6.18255615234375
I0202 14:54:55.509269 139774434195200 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.848872184753418, loss=4.227752208709717
I0202 14:55:41.858289 139774417409792 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8164215683937073, loss=5.587923526763916
I0202 14:56:28.282213 139774434195200 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6264162063598633, loss=5.418201446533203
I0202 14:56:30.701751 139936116377408 spec.py:321] Evaluating on the training split.
I0202 14:56:41.733840 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 14:57:17.219298 139936116377408 spec.py:349] Evaluating on the test split.
I0202 14:57:18.823171 139936116377408 submission_runner.py:408] Time since start: 7088.97s, 	Step: 13707, 	{'train/accuracy': 0.30980467796325684, 'train/loss': 3.3494856357574463, 'validation/accuracy': 0.28251999616622925, 'validation/loss': 3.5177979469299316, 'validation/num_examples': 50000, 'test/accuracy': 0.21400001645088196, 'test/loss': 4.0196003913879395, 'test/num_examples': 10000, 'score': 6337.505370855331, 'total_duration': 7088.974200248718, 'accumulated_submission_time': 6337.505370855331, 'accumulated_eval_time': 750.3280100822449, 'accumulated_logging_time': 0.42362475395202637}
I0202 14:57:18.839672 139774417409792 logging_writer.py:48] [13707] accumulated_eval_time=750.328010, accumulated_logging_time=0.423625, accumulated_submission_time=6337.505371, global_step=13707, preemption_count=0, score=6337.505371, test/accuracy=0.214000, test/loss=4.019600, test/num_examples=10000, total_duration=7088.974200, train/accuracy=0.309805, train/loss=3.349486, validation/accuracy=0.282520, validation/loss=3.517798, validation/num_examples=50000
I0202 14:57:57.651045 139774434195200 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.8537352085113525, loss=5.10450553894043
I0202 14:58:43.714423 139774417409792 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6541436910629272, loss=5.841220855712891
I0202 14:59:30.471959 139774434195200 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9575192928314209, loss=4.238018989562988
I0202 15:00:16.874304 139774417409792 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.1056448221206665, loss=4.341225624084473
I0202 15:01:03.290108 139774434195200 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9542787671089172, loss=4.223213195800781
I0202 15:01:49.557264 139774417409792 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7688453197479248, loss=5.354739665985107
I0202 15:02:35.979732 139774434195200 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.9318185448646545, loss=4.418490409851074
I0202 15:03:22.257811 139774417409792 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.897992730140686, loss=4.094773769378662
I0202 15:04:08.475448 139774434195200 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8695027232170105, loss=4.208683490753174
I0202 15:04:19.163090 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:04:31.120412 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:05:06.044894 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:05:07.654368 139936116377408 submission_runner.py:408] Time since start: 7557.81s, 	Step: 14625, 	{'train/accuracy': 0.3052343726158142, 'train/loss': 3.417556047439575, 'validation/accuracy': 0.2822999954223633, 'validation/loss': 3.541224956512451, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.021054744720459, 'test/num_examples': 10000, 'score': 6757.771499872208, 'total_duration': 7557.805434465408, 'accumulated_submission_time': 6757.771499872208, 'accumulated_eval_time': 798.8193001747131, 'accumulated_logging_time': 0.4494767189025879}
I0202 15:05:07.672975 139774417409792 logging_writer.py:48] [14625] accumulated_eval_time=798.819300, accumulated_logging_time=0.449477, accumulated_submission_time=6757.771500, global_step=14625, preemption_count=0, score=6757.771500, test/accuracy=0.220100, test/loss=4.021055, test/num_examples=10000, total_duration=7557.805434, train/accuracy=0.305234, train/loss=3.417556, validation/accuracy=0.282300, validation/loss=3.541225, validation/num_examples=50000
I0202 15:05:38.270664 139774434195200 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.781490683555603, loss=4.041310787200928
I0202 15:06:24.660026 139774417409792 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8513028621673584, loss=4.24498176574707
I0202 15:07:10.989202 139774434195200 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7253838181495667, loss=6.224412441253662
I0202 15:07:57.718014 139774417409792 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8322832584381104, loss=4.940280437469482
I0202 15:08:44.118781 139774434195200 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8544487953186035, loss=4.133040904998779
I0202 15:09:30.597290 139774417409792 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6899705529212952, loss=4.6237664222717285
I0202 15:10:17.146584 139774434195200 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.788459300994873, loss=4.5076093673706055
I0202 15:11:03.679353 139774417409792 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.8385950922966003, loss=4.09429931640625
I0202 15:11:50.290338 139774434195200 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7507326602935791, loss=5.818882942199707
I0202 15:12:07.674145 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:12:18.855107 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:12:53.917200 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:12:55.537492 139936116377408 submission_runner.py:408] Time since start: 8025.69s, 	Step: 15539, 	{'train/accuracy': 0.31101560592651367, 'train/loss': 3.333627939224243, 'validation/accuracy': 0.29071998596191406, 'validation/loss': 3.459397315979004, 'validation/num_examples': 50000, 'test/accuracy': 0.226500004529953, 'test/loss': 3.976407527923584, 'test/num_examples': 10000, 'score': 7177.714713335037, 'total_duration': 8025.688539028168, 'accumulated_submission_time': 7177.714713335037, 'accumulated_eval_time': 846.682626247406, 'accumulated_logging_time': 0.4787595272064209}
I0202 15:12:55.556639 139774417409792 logging_writer.py:48] [15539] accumulated_eval_time=846.682626, accumulated_logging_time=0.478760, accumulated_submission_time=7177.714713, global_step=15539, preemption_count=0, score=7177.714713, test/accuracy=0.226500, test/loss=3.976408, test/num_examples=10000, total_duration=8025.688539, train/accuracy=0.311016, train/loss=3.333628, validation/accuracy=0.290720, validation/loss=3.459397, validation/num_examples=50000
I0202 15:13:20.215237 139774434195200 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7698874473571777, loss=5.1039557456970215
I0202 15:14:05.644486 139774417409792 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.8093230128288269, loss=4.1284661293029785
I0202 15:14:51.903769 139774434195200 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7799510359764099, loss=6.005168914794922
I0202 15:15:38.310704 139774417409792 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8801515698432922, loss=4.557085990905762
I0202 15:16:24.581506 139774434195200 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.8360378742218018, loss=4.8813371658325195
I0202 15:17:10.764292 139774417409792 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.678485631942749, loss=5.987008094787598
I0202 15:17:56.733964 139774434195200 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8881145715713501, loss=4.272724151611328
I0202 15:18:43.272293 139774417409792 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.846125602722168, loss=4.361800670623779
I0202 15:19:29.513486 139774434195200 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.316481351852417, loss=4.2978901863098145
I0202 15:19:55.655018 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:20:07.600780 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:20:43.562329 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:20:45.176578 139936116377408 submission_runner.py:408] Time since start: 8495.33s, 	Step: 16458, 	{'train/accuracy': 0.3293554484844208, 'train/loss': 3.1784443855285645, 'validation/accuracy': 0.30295997858047485, 'validation/loss': 3.3324337005615234, 'validation/num_examples': 50000, 'test/accuracy': 0.23240001499652863, 'test/loss': 3.8720884323120117, 'test/num_examples': 10000, 'score': 7597.7491154670715, 'total_duration': 8495.327637910843, 'accumulated_submission_time': 7597.7491154670715, 'accumulated_eval_time': 896.20419049263, 'accumulated_logging_time': 0.5077810287475586}
I0202 15:20:45.194155 139774417409792 logging_writer.py:48] [16458] accumulated_eval_time=896.204190, accumulated_logging_time=0.507781, accumulated_submission_time=7597.749115, global_step=16458, preemption_count=0, score=7597.749115, test/accuracy=0.232400, test/loss=3.872088, test/num_examples=10000, total_duration=8495.327638, train/accuracy=0.329355, train/loss=3.178444, validation/accuracy=0.302960, validation/loss=3.332434, validation/num_examples=50000
I0202 15:21:02.290642 139774434195200 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.8452360033988953, loss=4.013406753540039
I0202 15:21:46.790253 139774417409792 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1483453512191772, loss=4.283595085144043
I0202 15:22:33.012903 139774434195200 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.8213644027709961, loss=4.0484466552734375
I0202 15:23:19.330473 139774417409792 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.0365736484527588, loss=4.333249092102051
I0202 15:24:05.544741 139774434195200 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7653563618659973, loss=5.212038993835449
I0202 15:24:51.631578 139774417409792 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.8831250071525574, loss=4.193964958190918
I0202 15:25:37.933813 139774434195200 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9606233239173889, loss=4.049722671508789
I0202 15:26:24.138150 139774417409792 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9626181721687317, loss=4.346749305725098
I0202 15:27:10.500788 139774434195200 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.142301082611084, loss=4.126546859741211
I0202 15:27:45.232358 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:27:57.513426 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:28:33.420102 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:28:35.074017 139936116377408 submission_runner.py:408] Time since start: 8965.23s, 	Step: 17377, 	{'train/accuracy': 0.32539060711860657, 'train/loss': 3.243004083633423, 'validation/accuracy': 0.30079999566078186, 'validation/loss': 3.385110855102539, 'validation/num_examples': 50000, 'test/accuracy': 0.2331000119447708, 'test/loss': 3.922811985015869, 'test/num_examples': 10000, 'score': 8017.729243755341, 'total_duration': 8965.22508430481, 'accumulated_submission_time': 8017.729243755341, 'accumulated_eval_time': 946.0458581447601, 'accumulated_logging_time': 0.5358819961547852}
I0202 15:28:35.093204 139774417409792 logging_writer.py:48] [17377] accumulated_eval_time=946.045858, accumulated_logging_time=0.535882, accumulated_submission_time=8017.729244, global_step=17377, preemption_count=0, score=8017.729244, test/accuracy=0.233100, test/loss=3.922812, test/num_examples=10000, total_duration=8965.225084, train/accuracy=0.325391, train/loss=3.243004, validation/accuracy=0.300800, validation/loss=3.385111, validation/num_examples=50000
I0202 15:28:44.629160 139774434195200 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.8956443667411804, loss=4.860317707061768
I0202 15:29:27.936480 139774417409792 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0819706916809082, loss=4.128632068634033
I0202 15:30:14.146301 139774434195200 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8433223962783813, loss=4.130456447601318
I0202 15:31:00.565784 139774417409792 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9610574245452881, loss=4.202442169189453
I0202 15:31:47.216077 139774434195200 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.1958410739898682, loss=4.383274555206299
I0202 15:32:33.622724 139774417409792 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8539857864379883, loss=4.20557975769043
I0202 15:33:19.843113 139774434195200 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.9481638073921204, loss=4.232809543609619
I0202 15:34:06.018485 139774417409792 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7551279067993164, loss=6.008915901184082
I0202 15:34:52.084879 139774434195200 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.138336420059204, loss=4.107272148132324
I0202 15:35:35.445482 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:35:45.805948 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:36:22.841012 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:36:24.442983 139936116377408 submission_runner.py:408] Time since start: 9434.59s, 	Step: 18295, 	{'train/accuracy': 0.34615233540534973, 'train/loss': 3.063856363296509, 'validation/accuracy': 0.3191399872303009, 'validation/loss': 3.214115619659424, 'validation/num_examples': 50000, 'test/accuracy': 0.24240000545978546, 'test/loss': 3.7857863903045654, 'test/num_examples': 10000, 'score': 8438.02011179924, 'total_duration': 9434.594047546387, 'accumulated_submission_time': 8438.02011179924, 'accumulated_eval_time': 995.0433971881866, 'accumulated_logging_time': 0.5683629512786865}
I0202 15:36:24.461463 139774417409792 logging_writer.py:48] [18295] accumulated_eval_time=995.043397, accumulated_logging_time=0.568363, accumulated_submission_time=8438.020112, global_step=18295, preemption_count=0, score=8438.020112, test/accuracy=0.242400, test/loss=3.785786, test/num_examples=10000, total_duration=9434.594048, train/accuracy=0.346152, train/loss=3.063856, validation/accuracy=0.319140, validation/loss=3.214116, validation/num_examples=50000
I0202 15:36:26.857752 139774434195200 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9387980103492737, loss=4.21707820892334
I0202 15:37:09.048276 139774417409792 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8834777474403381, loss=6.06361198425293
I0202 15:37:54.853330 139774434195200 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0941256284713745, loss=4.21221399307251
I0202 15:38:41.197509 139774417409792 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.9992114305496216, loss=4.097373962402344
I0202 15:39:27.397844 139774434195200 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.938470721244812, loss=4.264561653137207
I0202 15:40:13.995118 139774417409792 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.156968593597412, loss=4.101234436035156
I0202 15:41:00.205115 139774434195200 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9095814228057861, loss=4.0822319984436035
I0202 15:41:46.754341 139774417409792 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8614799976348877, loss=3.9459352493286133
I0202 15:42:32.877972 139774434195200 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9296886920928955, loss=5.564722537994385
I0202 15:43:19.307972 139774417409792 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8618876338005066, loss=4.307027339935303
I0202 15:43:24.530313 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:43:35.999514 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:44:14.866321 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:44:16.472064 139936116377408 submission_runner.py:408] Time since start: 9906.62s, 	Step: 19213, 	{'train/accuracy': 0.3359765410423279, 'train/loss': 3.1254475116729736, 'validation/accuracy': 0.30570000410079956, 'validation/loss': 3.2792983055114746, 'validation/num_examples': 50000, 'test/accuracy': 0.23990000784397125, 'test/loss': 3.8324697017669678, 'test/num_examples': 10000, 'score': 8858.02958726883, 'total_duration': 9906.623125314713, 'accumulated_submission_time': 8858.02958726883, 'accumulated_eval_time': 1046.9851393699646, 'accumulated_logging_time': 0.5980954170227051}
I0202 15:44:16.489816 139774434195200 logging_writer.py:48] [19213] accumulated_eval_time=1046.985139, accumulated_logging_time=0.598095, accumulated_submission_time=8858.029587, global_step=19213, preemption_count=0, score=8858.029587, test/accuracy=0.239900, test/loss=3.832470, test/num_examples=10000, total_duration=9906.623125, train/accuracy=0.335977, train/loss=3.125448, validation/accuracy=0.305700, validation/loss=3.279298, validation/num_examples=50000
I0202 15:44:52.514614 139774417409792 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.9042681455612183, loss=4.096585750579834
I0202 15:45:38.659509 139774434195200 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9184406399726868, loss=4.476840019226074
I0202 15:46:25.179371 139774417409792 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.307935357093811, loss=4.175025463104248
I0202 15:47:11.411635 139774434195200 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.157171368598938, loss=4.276083946228027
I0202 15:47:57.370870 139774417409792 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9095282554626465, loss=4.0777974128723145
I0202 15:48:43.657954 139774434195200 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.7957578301429749, loss=4.247961521148682
I0202 15:49:30.048819 139774417409792 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9693549871444702, loss=4.268596649169922
I0202 15:50:16.167479 139774434195200 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9314014315605164, loss=3.9859585762023926
I0202 15:51:02.671635 139774417409792 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.8857657313346863, loss=4.928349018096924
I0202 15:51:16.722952 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:51:28.031817 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:52:04.708065 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:52:06.317249 139936116377408 submission_runner.py:408] Time since start: 10376.47s, 	Step: 20132, 	{'train/accuracy': 0.3737109303474426, 'train/loss': 2.931885004043579, 'validation/accuracy': 0.3187199831008911, 'validation/loss': 3.236717939376831, 'validation/num_examples': 50000, 'test/accuracy': 0.241100013256073, 'test/loss': 3.7928264141082764, 'test/num_examples': 10000, 'score': 9278.196017742157, 'total_duration': 10376.468310594559, 'accumulated_submission_time': 9278.196017742157, 'accumulated_eval_time': 1096.579419374466, 'accumulated_logging_time': 0.625751256942749}
I0202 15:52:06.334776 139774434195200 logging_writer.py:48] [20132] accumulated_eval_time=1096.579419, accumulated_logging_time=0.625751, accumulated_submission_time=9278.196018, global_step=20132, preemption_count=0, score=9278.196018, test/accuracy=0.241100, test/loss=3.792826, test/num_examples=10000, total_duration=10376.468311, train/accuracy=0.373711, train/loss=2.931885, validation/accuracy=0.318720, validation/loss=3.236718, validation/num_examples=50000
I0202 15:52:33.772174 139774417409792 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7425704598426819, loss=6.0954413414001465
I0202 15:53:19.532455 139774434195200 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8788214325904846, loss=4.768693923950195
I0202 15:54:05.824263 139774417409792 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7791445851325989, loss=6.045347690582275
I0202 15:54:52.034082 139774434195200 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6605991721153259, loss=5.992079257965088
I0202 15:55:38.065663 139774417409792 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.880062997341156, loss=4.035149097442627
I0202 15:56:24.363976 139774434195200 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.8505324125289917, loss=3.9588236808776855
I0202 15:57:10.714756 139774417409792 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8164956569671631, loss=5.59770393371582
I0202 15:57:56.911834 139774434195200 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7902499437332153, loss=4.780725955963135
I0202 15:58:43.096917 139774417409792 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7250906229019165, loss=5.535123348236084
I0202 15:59:06.631476 139936116377408 spec.py:321] Evaluating on the training split.
I0202 15:59:17.927031 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 15:59:56.437320 139936116377408 spec.py:349] Evaluating on the test split.
I0202 15:59:58.057602 139936116377408 submission_runner.py:408] Time since start: 10848.21s, 	Step: 21052, 	{'train/accuracy': 0.3433007597923279, 'train/loss': 3.070847988128662, 'validation/accuracy': 0.31977999210357666, 'validation/loss': 3.210434913635254, 'validation/num_examples': 50000, 'test/accuracy': 0.24820001423358917, 'test/loss': 3.769993543624878, 'test/num_examples': 10000, 'score': 9698.436318159103, 'total_duration': 10848.208649396896, 'accumulated_submission_time': 9698.436318159103, 'accumulated_eval_time': 1148.0055141448975, 'accumulated_logging_time': 0.6522841453552246}
I0202 15:59:58.074949 139774434195200 logging_writer.py:48] [21052] accumulated_eval_time=1148.005514, accumulated_logging_time=0.652284, accumulated_submission_time=9698.436318, global_step=21052, preemption_count=0, score=9698.436318, test/accuracy=0.248200, test/loss=3.769994, test/num_examples=10000, total_duration=10848.208649, train/accuracy=0.343301, train/loss=3.070848, validation/accuracy=0.319780, validation/loss=3.210435, validation/num_examples=50000
I0202 16:00:17.542782 139774417409792 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.951534628868103, loss=4.043994903564453
I0202 16:01:02.263110 139774434195200 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.9010857343673706, loss=4.20847225189209
I0202 16:01:49.398861 139774417409792 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6634567379951477, loss=6.110937595367432
I0202 16:02:36.418343 139774434195200 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8220499753952026, loss=4.7006049156188965
I0202 16:03:22.980706 139774417409792 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.760535717010498, loss=6.015901565551758
I0202 16:04:09.451749 139774434195200 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7031478881835938, loss=5.896293640136719
I0202 16:04:56.082725 139774417409792 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.6686966419219971, loss=5.793374061584473
I0202 16:05:42.714071 139774434195200 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.9158124923706055, loss=4.191145420074463
I0202 16:06:29.233404 139774417409792 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.0263909101486206, loss=4.276330947875977
I0202 16:06:58.402229 139936116377408 spec.py:321] Evaluating on the training split.
I0202 16:07:10.343645 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 16:07:45.981738 139936116377408 spec.py:349] Evaluating on the test split.
I0202 16:07:47.586410 139936116377408 submission_runner.py:408] Time since start: 11317.74s, 	Step: 21964, 	{'train/accuracy': 0.3420703113079071, 'train/loss': 3.094113349914551, 'validation/accuracy': 0.32277998328208923, 'validation/loss': 3.2260701656341553, 'validation/num_examples': 50000, 'test/accuracy': 0.24150000512599945, 'test/loss': 3.7991697788238525, 'test/num_examples': 10000, 'score': 10118.706419706345, 'total_duration': 11317.737473249435, 'accumulated_submission_time': 10118.706419706345, 'accumulated_eval_time': 1197.1896917819977, 'accumulated_logging_time': 0.6798815727233887}
I0202 16:07:47.610766 139774434195200 logging_writer.py:48] [21964] accumulated_eval_time=1197.189692, accumulated_logging_time=0.679882, accumulated_submission_time=10118.706420, global_step=21964, preemption_count=0, score=10118.706420, test/accuracy=0.241500, test/loss=3.799170, test/num_examples=10000, total_duration=11317.737473, train/accuracy=0.342070, train/loss=3.094113, validation/accuracy=0.322780, validation/loss=3.226070, validation/num_examples=50000
I0202 16:08:02.328065 139774417409792 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8872069120407104, loss=4.082846641540527
I0202 16:08:46.279271 139774434195200 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9381546378135681, loss=4.245523929595947
I0202 16:09:32.423640 139774417409792 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7837553024291992, loss=4.868762493133545
I0202 16:10:18.930030 139774434195200 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0074723958969116, loss=4.001990795135498
I0202 16:11:05.109457 139774417409792 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7902895212173462, loss=5.679680347442627
I0202 16:11:51.706451 139774434195200 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8702884316444397, loss=4.681882381439209
I0202 16:12:38.404436 139774417409792 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7784166932106018, loss=4.381738662719727
I0202 16:13:24.860683 139774434195200 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9006351232528687, loss=4.00185489654541
I0202 16:14:11.474743 139774417409792 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8595796823501587, loss=4.480433464050293
I0202 16:14:47.788968 139936116377408 spec.py:321] Evaluating on the training split.
I0202 16:14:58.263899 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 16:15:34.882982 139936116377408 spec.py:349] Evaluating on the test split.
I0202 16:15:36.493529 139936116377408 submission_runner.py:408] Time since start: 11786.64s, 	Step: 22880, 	{'train/accuracy': 0.36865234375, 'train/loss': 2.973356008529663, 'validation/accuracy': 0.3253600001335144, 'validation/loss': 3.211477041244507, 'validation/num_examples': 50000, 'test/accuracy': 0.2574000060558319, 'test/loss': 3.756904125213623, 'test/num_examples': 10000, 'score': 10538.827253580093, 'total_duration': 11786.644594669342, 'accumulated_submission_time': 10538.827253580093, 'accumulated_eval_time': 1245.8942565917969, 'accumulated_logging_time': 0.7139434814453125}
I0202 16:15:36.518038 139774434195200 logging_writer.py:48] [22880] accumulated_eval_time=1245.894257, accumulated_logging_time=0.713943, accumulated_submission_time=10538.827254, global_step=22880, preemption_count=0, score=10538.827254, test/accuracy=0.257400, test/loss=3.756904, test/num_examples=10000, total_duration=11786.644595, train/accuracy=0.368652, train/loss=2.973356, validation/accuracy=0.325360, validation/loss=3.211477, validation/num_examples=50000
I0202 16:15:44.875442 139774417409792 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8665456771850586, loss=4.258938789367676
I0202 16:16:27.982162 139774434195200 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9754269123077393, loss=4.009838581085205
I0202 16:17:14.462848 139774417409792 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.9364333152770996, loss=4.014606475830078
I0202 16:18:00.955418 139774434195200 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.9273936748504639, loss=4.058566093444824
I0202 16:18:47.256601 139774417409792 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.748832106590271, loss=5.035691738128662
I0202 16:19:33.832094 139774434195200 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.1181094646453857, loss=4.07346773147583
I0202 16:20:20.171943 139774417409792 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0191234350204468, loss=5.966430187225342
I0202 16:21:06.516177 139774434195200 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.9660938382148743, loss=4.628369331359863
I0202 16:21:53.070398 139774417409792 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.953302800655365, loss=3.864932060241699
I0202 16:22:36.568472 139936116377408 spec.py:321] Evaluating on the training split.
I0202 16:22:48.333890 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 16:23:24.338870 139936116377408 spec.py:349] Evaluating on the test split.
I0202 16:23:25.952788 139936116377408 submission_runner.py:408] Time since start: 12256.10s, 	Step: 23795, 	{'train/accuracy': 0.3464648425579071, 'train/loss': 3.0866899490356445, 'validation/accuracy': 0.3257800042629242, 'validation/loss': 3.2104380130767822, 'validation/num_examples': 50000, 'test/accuracy': 0.24940000474452972, 'test/loss': 3.7716026306152344, 'test/num_examples': 10000, 'score': 10958.82033610344, 'total_duration': 12256.103848218918, 'accumulated_submission_time': 10958.82033610344, 'accumulated_eval_time': 1295.278584241867, 'accumulated_logging_time': 0.7482728958129883}
I0202 16:23:25.975522 139774434195200 logging_writer.py:48] [23795] accumulated_eval_time=1295.278584, accumulated_logging_time=0.748273, accumulated_submission_time=10958.820336, global_step=23795, preemption_count=0, score=10958.820336, test/accuracy=0.249400, test/loss=3.771603, test/num_examples=10000, total_duration=12256.103848, train/accuracy=0.346465, train/loss=3.086690, validation/accuracy=0.325780, validation/loss=3.210438, validation/num_examples=50000
I0202 16:23:28.366662 139774417409792 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.953172504901886, loss=4.291996002197266
I0202 16:24:10.620345 139774434195200 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9207655191421509, loss=4.084734916687012
I0202 16:24:56.697111 139774417409792 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8794354796409607, loss=5.058531761169434
I0202 16:25:42.987244 139774434195200 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9446994662284851, loss=4.217494964599609
I0202 16:26:29.148615 139774417409792 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9941466450691223, loss=4.769602298736572
I0202 16:27:15.482176 139774434195200 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9532766938209534, loss=4.023890495300293
I0202 16:28:01.735658 139774417409792 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7135677933692932, loss=5.42401647567749
I0202 16:28:47.775959 139774434195200 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.1119446754455566, loss=4.069540500640869
I0202 16:29:34.220283 139774417409792 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.8757878541946411, loss=3.9665277004241943
I0202 16:30:20.612263 139774434195200 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.2010821104049683, loss=5.227945327758789
I0202 16:30:26.173225 139936116377408 spec.py:321] Evaluating on the training split.
I0202 16:30:36.999401 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 16:31:13.922138 139936116377408 spec.py:349] Evaluating on the test split.
I0202 16:31:15.539370 139936116377408 submission_runner.py:408] Time since start: 12725.69s, 	Step: 24713, 	{'train/accuracy': 0.35957029461860657, 'train/loss': 2.9951577186584473, 'validation/accuracy': 0.333079993724823, 'validation/loss': 3.153878688812256, 'validation/num_examples': 50000, 'test/accuracy': 0.2598000168800354, 'test/loss': 3.7074122428894043, 'test/num_examples': 10000, 'score': 11378.957997083664, 'total_duration': 12725.690438747406, 'accumulated_submission_time': 11378.957997083664, 'accumulated_eval_time': 1344.6447319984436, 'accumulated_logging_time': 0.7822833061218262}
I0202 16:31:15.560546 139774417409792 logging_writer.py:48] [24713] accumulated_eval_time=1344.644732, accumulated_logging_time=0.782283, accumulated_submission_time=11378.957997, global_step=24713, preemption_count=0, score=11378.957997, test/accuracy=0.259800, test/loss=3.707412, test/num_examples=10000, total_duration=12725.690439, train/accuracy=0.359570, train/loss=2.995158, validation/accuracy=0.333080, validation/loss=3.153879, validation/num_examples=50000
I0202 16:31:51.678841 139774434195200 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7712962031364441, loss=6.011900901794434
I0202 16:32:37.789467 139774417409792 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.8492194414138794, loss=5.756259918212891
I0202 16:33:24.494359 139774434195200 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7061143517494202, loss=6.113511085510254
I0202 16:34:11.212796 139774417409792 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8942258358001709, loss=4.534548759460449
I0202 16:34:57.642812 139774434195200 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.8161852955818176, loss=5.025295734405518
I0202 16:35:43.963557 139774417409792 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.7684934735298157, loss=5.824777126312256
I0202 16:36:30.378202 139774434195200 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.9887978434562683, loss=4.021472930908203
I0202 16:37:16.909583 139774417409792 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.107936978340149, loss=4.061855316162109
I0202 16:38:03.306444 139774434195200 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7509744167327881, loss=5.223918437957764
I0202 16:38:15.685854 139936116377408 spec.py:321] Evaluating on the training split.
I0202 16:38:27.107715 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 16:39:03.763563 139936116377408 spec.py:349] Evaluating on the test split.
I0202 16:39:05.370529 139936116377408 submission_runner.py:408] Time since start: 13195.52s, 	Step: 25628, 	{'train/accuracy': 0.37269529700279236, 'train/loss': 2.9379703998565674, 'validation/accuracy': 0.34002000093460083, 'validation/loss': 3.1157100200653076, 'validation/num_examples': 50000, 'test/accuracy': 0.25760000944137573, 'test/loss': 3.7011423110961914, 'test/num_examples': 10000, 'score': 11799.026058912277, 'total_duration': 13195.52159357071, 'accumulated_submission_time': 11799.026058912277, 'accumulated_eval_time': 1394.3293986320496, 'accumulated_logging_time': 0.8127717971801758}
I0202 16:39:05.389931 139774417409792 logging_writer.py:48] [25628] accumulated_eval_time=1394.329399, accumulated_logging_time=0.812772, accumulated_submission_time=11799.026059, global_step=25628, preemption_count=0, score=11799.026059, test/accuracy=0.257600, test/loss=3.701142, test/num_examples=10000, total_duration=13195.521594, train/accuracy=0.372695, train/loss=2.937970, validation/accuracy=0.340020, validation/loss=3.115710, validation/num_examples=50000
I0202 16:39:34.914730 139774434195200 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.9697219133377075, loss=4.214751243591309
I0202 16:40:20.960168 139774417409792 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9553982615470886, loss=4.1109795570373535
I0202 16:41:07.708155 139774434195200 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.9415758848190308, loss=4.2202558517456055
I0202 16:41:54.084576 139774417409792 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0481058359146118, loss=3.848191976547241
I0202 16:42:40.451900 139774434195200 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.8834505081176758, loss=6.088267803192139
I0202 16:43:27.020371 139774417409792 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.029239535331726, loss=4.306362628936768
I0202 16:44:13.563055 139774434195200 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0025582313537598, loss=3.8942863941192627
I0202 16:45:00.084388 139774417409792 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8437461256980896, loss=4.649503707885742
I0202 16:45:46.659539 139774434195200 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.0411756038665771, loss=4.020228385925293
I0202 16:46:05.460277 139936116377408 spec.py:321] Evaluating on the training split.
I0202 16:46:16.083831 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 16:46:53.890193 139936116377408 spec.py:349] Evaluating on the test split.
I0202 16:46:55.500135 139936116377408 submission_runner.py:408] Time since start: 13665.65s, 	Step: 26542, 	{'train/accuracy': 0.36146482825279236, 'train/loss': 2.9840753078460693, 'validation/accuracy': 0.3368600010871887, 'validation/loss': 3.117708444595337, 'validation/num_examples': 50000, 'test/accuracy': 0.2600000202655792, 'test/loss': 3.6868534088134766, 'test/num_examples': 10000, 'score': 12219.03768491745, 'total_duration': 13665.651197195053, 'accumulated_submission_time': 12219.03768491745, 'accumulated_eval_time': 1444.3692378997803, 'accumulated_logging_time': 0.843348503112793}
I0202 16:46:55.519523 139774417409792 logging_writer.py:48] [26542] accumulated_eval_time=1444.369238, accumulated_logging_time=0.843349, accumulated_submission_time=12219.037685, global_step=26542, preemption_count=0, score=12219.037685, test/accuracy=0.260000, test/loss=3.686853, test/num_examples=10000, total_duration=13665.651197, train/accuracy=0.361465, train/loss=2.984075, validation/accuracy=0.336860, validation/loss=3.117708, validation/num_examples=50000
I0202 16:47:18.966532 139774434195200 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.9338967800140381, loss=3.8980960845947266
I0202 16:48:04.182514 139774417409792 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9989989995956421, loss=4.006573677062988
I0202 16:48:50.292419 139774434195200 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.7876775860786438, loss=5.852534294128418
I0202 16:49:36.661612 139774417409792 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7659978270530701, loss=5.1948771476745605
I0202 16:50:22.826337 139774434195200 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.9518101215362549, loss=3.795456647872925
I0202 16:51:08.966458 139774417409792 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9505633115768433, loss=4.300940990447998
I0202 16:51:55.307928 139774434195200 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8777510523796082, loss=4.1513471603393555
I0202 16:52:41.544447 139774417409792 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.1292098760604858, loss=3.8847134113311768
I0202 16:53:27.800494 139774434195200 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.818062961101532, loss=5.284587860107422
I0202 16:53:55.640699 139936116377408 spec.py:321] Evaluating on the training split.
I0202 16:54:06.324641 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 16:54:43.677987 139936116377408 spec.py:349] Evaluating on the test split.
I0202 16:54:45.283438 139936116377408 submission_runner.py:408] Time since start: 14135.43s, 	Step: 27462, 	{'train/accuracy': 0.3483007848262787, 'train/loss': 3.1170191764831543, 'validation/accuracy': 0.32401999831199646, 'validation/loss': 3.26419997215271, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 3.807725191116333, 'test/num_examples': 10000, 'score': 12639.100955963135, 'total_duration': 14135.43448138237, 'accumulated_submission_time': 12639.100955963135, 'accumulated_eval_time': 1494.0119626522064, 'accumulated_logging_time': 0.8733620643615723}
I0202 16:54:45.305906 139774417409792 logging_writer.py:48] [27462] accumulated_eval_time=1494.011963, accumulated_logging_time=0.873362, accumulated_submission_time=12639.100956, global_step=27462, preemption_count=0, score=12639.100956, test/accuracy=0.249700, test/loss=3.807725, test/num_examples=10000, total_duration=14135.434481, train/accuracy=0.348301, train/loss=3.117019, validation/accuracy=0.324020, validation/loss=3.264200, validation/num_examples=50000
I0202 16:55:00.805345 139774434195200 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8627828359603882, loss=4.044943332672119
I0202 16:55:45.021621 139774417409792 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7739109396934509, loss=5.969719886779785
I0202 16:56:31.224688 139774434195200 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.8827666640281677, loss=5.231978893280029
I0202 16:57:17.822485 139774417409792 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.8438078165054321, loss=6.030823707580566
I0202 16:58:04.156228 139774434195200 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.9802266955375671, loss=4.0634918212890625
I0202 16:58:50.301158 139774417409792 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.9636325240135193, loss=3.938197135925293
I0202 16:59:36.829845 139774434195200 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.2063130140304565, loss=4.06374979019165
I0202 17:00:23.010461 139774417409792 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.8105347156524658, loss=4.730583190917969
I0202 17:01:09.360231 139774434195200 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.9839040040969849, loss=3.9198718070983887
I0202 17:01:45.647575 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:01:56.110658 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:02:35.411322 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:02:37.019593 139936116377408 submission_runner.py:408] Time since start: 14607.17s, 	Step: 28380, 	{'train/accuracy': 0.36894530057907104, 'train/loss': 2.9343044757843018, 'validation/accuracy': 0.33861997723579407, 'validation/loss': 3.1066977977752686, 'validation/num_examples': 50000, 'test/accuracy': 0.2630999982357025, 'test/loss': 3.679342269897461, 'test/num_examples': 10000, 'score': 13059.384540557861, 'total_duration': 14607.170650720596, 'accumulated_submission_time': 13059.384540557861, 'accumulated_eval_time': 1545.383987903595, 'accumulated_logging_time': 0.9063313007354736}
I0202 17:02:37.042727 139774417409792 logging_writer.py:48] [28380] accumulated_eval_time=1545.383988, accumulated_logging_time=0.906331, accumulated_submission_time=13059.384541, global_step=28380, preemption_count=0, score=13059.384541, test/accuracy=0.263100, test/loss=3.679342, test/num_examples=10000, total_duration=14607.170651, train/accuracy=0.368945, train/loss=2.934304, validation/accuracy=0.338620, validation/loss=3.106698, validation/num_examples=50000
I0202 17:02:45.401553 139774434195200 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.9707431197166443, loss=3.933516025543213
I0202 17:03:28.675923 139774417409792 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8583581447601318, loss=4.367570877075195
I0202 17:04:14.788915 139774434195200 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.9580548405647278, loss=4.112112045288086
I0202 17:05:01.195377 139774417409792 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.8087836503982544, loss=5.1526875495910645
I0202 17:05:47.644724 139774434195200 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7729742527008057, loss=5.331375598907471
I0202 17:06:33.815702 139774417409792 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.8894184827804565, loss=4.400491237640381
I0202 17:07:20.254834 139774434195200 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9330434799194336, loss=3.98353910446167
I0202 17:08:06.600448 139774417409792 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.8681721091270447, loss=5.124178409576416
I0202 17:08:52.849344 139774434195200 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.8249555826187134, loss=4.358348846435547
I0202 17:09:37.203872 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:09:47.542846 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:10:26.225438 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:10:27.845018 139936116377408 submission_runner.py:408] Time since start: 15078.00s, 	Step: 29298, 	{'train/accuracy': 0.37068358063697815, 'train/loss': 2.9274771213531494, 'validation/accuracy': 0.3446199893951416, 'validation/loss': 3.0712943077087402, 'validation/num_examples': 50000, 'test/accuracy': 0.2648000121116638, 'test/loss': 3.661438465118408, 'test/num_examples': 10000, 'score': 13479.487174987793, 'total_duration': 15077.99607181549, 'accumulated_submission_time': 13479.487174987793, 'accumulated_eval_time': 1596.0251424312592, 'accumulated_logging_time': 0.9394416809082031}
I0202 17:10:27.870643 139774417409792 logging_writer.py:48] [29298] accumulated_eval_time=1596.025142, accumulated_logging_time=0.939442, accumulated_submission_time=13479.487175, global_step=29298, preemption_count=0, score=13479.487175, test/accuracy=0.264800, test/loss=3.661438, test/num_examples=10000, total_duration=15077.996072, train/accuracy=0.370684, train/loss=2.927477, validation/accuracy=0.344620, validation/loss=3.071294, validation/num_examples=50000
I0202 17:10:29.065310 139774434195200 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.9782757759094238, loss=4.124069690704346
I0202 17:11:11.123727 139774417409792 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.8969645500183105, loss=4.012604236602783
I0202 17:11:57.211682 139774434195200 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8928115963935852, loss=4.018537998199463
I0202 17:12:43.473036 139774417409792 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.2665272951126099, loss=3.9084129333496094
I0202 17:13:29.664622 139774434195200 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.96427321434021, loss=3.9757273197174072
I0202 17:14:15.953611 139774417409792 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.153543472290039, loss=3.950908899307251
I0202 17:15:02.187705 139774434195200 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.0521589517593384, loss=3.9608564376831055
I0202 17:15:48.363934 139774417409792 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9759207367897034, loss=3.8988494873046875
I0202 17:16:34.855546 139774434195200 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.1700959205627441, loss=4.330267429351807
I0202 17:17:21.101630 139774417409792 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.7794524431228638, loss=4.6394429206848145
I0202 17:17:28.138386 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:17:38.698341 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:18:16.484639 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:18:18.100014 139936116377408 submission_runner.py:408] Time since start: 15548.25s, 	Step: 30217, 	{'train/accuracy': 0.3766210973262787, 'train/loss': 2.902115821838379, 'validation/accuracy': 0.3519800007343292, 'validation/loss': 3.029473066329956, 'validation/num_examples': 50000, 'test/accuracy': 0.2742000222206116, 'test/loss': 3.6279361248016357, 'test/num_examples': 10000, 'score': 13899.69462966919, 'total_duration': 15548.25105714798, 'accumulated_submission_time': 13899.69462966919, 'accumulated_eval_time': 1645.9867305755615, 'accumulated_logging_time': 0.9761998653411865}
I0202 17:18:18.122606 139774434195200 logging_writer.py:48] [30217] accumulated_eval_time=1645.986731, accumulated_logging_time=0.976200, accumulated_submission_time=13899.694630, global_step=30217, preemption_count=0, score=13899.694630, test/accuracy=0.274200, test/loss=3.627936, test/num_examples=10000, total_duration=15548.251057, train/accuracy=0.376621, train/loss=2.902116, validation/accuracy=0.351980, validation/loss=3.029473, validation/num_examples=50000
I0202 17:18:52.448571 139774417409792 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.9956648349761963, loss=3.935164451599121
I0202 17:19:38.398085 139774434195200 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7964514493942261, loss=5.784069061279297
I0202 17:20:24.731306 139774417409792 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.9268369078636169, loss=4.217522144317627
I0202 17:21:11.075770 139774434195200 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.9298226833343506, loss=3.9458348751068115
I0202 17:21:57.552202 139774417409792 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8062129020690918, loss=4.490538120269775
I0202 17:22:43.755629 139774434195200 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.763249933719635, loss=5.457474708557129
I0202 17:23:29.808643 139774417409792 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.9053555130958557, loss=4.728033542633057
I0202 17:24:16.355417 139774434195200 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.9252262711524963, loss=3.746828317642212
I0202 17:25:02.744481 139774417409792 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.8670486211776733, loss=3.856708526611328
I0202 17:25:18.287548 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:25:28.914649 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:26:06.452673 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:26:08.063505 139936116377408 submission_runner.py:408] Time since start: 16018.21s, 	Step: 31135, 	{'train/accuracy': 0.3855859339237213, 'train/loss': 2.8547158241271973, 'validation/accuracy': 0.3534599840641022, 'validation/loss': 3.0370798110961914, 'validation/num_examples': 50000, 'test/accuracy': 0.2705000042915344, 'test/loss': 3.6340298652648926, 'test/num_examples': 10000, 'score': 14319.800573825836, 'total_duration': 16018.214548110962, 'accumulated_submission_time': 14319.800573825836, 'accumulated_eval_time': 1695.7626497745514, 'accumulated_logging_time': 1.0101087093353271}
I0202 17:26:08.085742 139774434195200 logging_writer.py:48] [31135] accumulated_eval_time=1695.762650, accumulated_logging_time=1.010109, accumulated_submission_time=14319.800574, global_step=31135, preemption_count=0, score=14319.800574, test/accuracy=0.270500, test/loss=3.634030, test/num_examples=10000, total_duration=16018.214548, train/accuracy=0.385586, train/loss=2.854716, validation/accuracy=0.353460, validation/loss=3.037080, validation/num_examples=50000
I0202 17:26:34.340322 139774417409792 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9114245772361755, loss=5.280365943908691
I0202 17:27:20.325627 139774434195200 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8228166103363037, loss=5.038290977478027
I0202 17:28:06.559240 139774417409792 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.082495927810669, loss=3.8467257022857666
I0202 17:28:52.644970 139774434195200 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.9362341165542603, loss=4.3543806076049805
I0202 17:29:38.920843 139774417409792 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0498281717300415, loss=3.872877836227417
I0202 17:30:25.070734 139774434195200 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9556360840797424, loss=3.9720306396484375
I0202 17:31:11.147826 139774417409792 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.937751054763794, loss=3.982332468032837
I0202 17:31:57.395948 139774434195200 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.1863237619400024, loss=3.9079766273498535
I0202 17:32:43.525868 139774417409792 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.9263902306556702, loss=3.9355671405792236
I0202 17:33:08.154342 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:33:18.830513 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:33:54.689002 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:33:56.297760 139936116377408 submission_runner.py:408] Time since start: 16486.45s, 	Step: 32055, 	{'train/accuracy': 0.4124804735183716, 'train/loss': 2.7808024883270264, 'validation/accuracy': 0.3519199788570404, 'validation/loss': 3.0841052532196045, 'validation/num_examples': 50000, 'test/accuracy': 0.2671000063419342, 'test/loss': 3.660425901412964, 'test/num_examples': 10000, 'score': 14739.811116695404, 'total_duration': 16486.448800325394, 'accumulated_submission_time': 14739.811116695404, 'accumulated_eval_time': 1743.906052350998, 'accumulated_logging_time': 1.0428571701049805}
I0202 17:33:56.323617 139774434195200 logging_writer.py:48] [32055] accumulated_eval_time=1743.906052, accumulated_logging_time=1.042857, accumulated_submission_time=14739.811117, global_step=32055, preemption_count=0, score=14739.811117, test/accuracy=0.267100, test/loss=3.660426, test/num_examples=10000, total_duration=16486.448800, train/accuracy=0.412480, train/loss=2.780802, validation/accuracy=0.351920, validation/loss=3.084105, validation/num_examples=50000
I0202 17:34:14.622332 139774417409792 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8441300392150879, loss=6.046339511871338
I0202 17:34:59.332832 139774434195200 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.8531855940818787, loss=6.041004180908203
I0202 17:35:45.889330 139774417409792 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.90912926197052, loss=4.062684535980225
I0202 17:36:32.049436 139774434195200 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.2668826580047607, loss=4.066220283508301
I0202 17:37:18.183742 139774417409792 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7503442168235779, loss=5.912662982940674
I0202 17:38:05.248279 139774434195200 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.004333734512329, loss=3.9163477420806885
I0202 17:38:51.369565 139774417409792 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9433439373970032, loss=3.892446756362915
I0202 17:39:37.698786 139774434195200 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0250753164291382, loss=3.801751136779785
I0202 17:40:24.296340 139774417409792 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.077848196029663, loss=3.9262633323669434
I0202 17:40:56.704415 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:41:07.731819 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:41:44.157985 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:41:45.759856 139936116377408 submission_runner.py:408] Time since start: 16955.91s, 	Step: 32972, 	{'train/accuracy': 0.38671875, 'train/loss': 2.8052151203155518, 'validation/accuracy': 0.36127999424934387, 'validation/loss': 2.94804048538208, 'validation/num_examples': 50000, 'test/accuracy': 0.27730000019073486, 'test/loss': 3.5375256538391113, 'test/num_examples': 10000, 'score': 15160.133778810501, 'total_duration': 16955.91092300415, 'accumulated_submission_time': 15160.133778810501, 'accumulated_eval_time': 1792.961481332779, 'accumulated_logging_time': 1.0791571140289307}
I0202 17:41:45.783776 139774434195200 logging_writer.py:48] [32972] accumulated_eval_time=1792.961481, accumulated_logging_time=1.079157, accumulated_submission_time=15160.133779, global_step=32972, preemption_count=0, score=15160.133779, test/accuracy=0.277300, test/loss=3.537526, test/num_examples=10000, total_duration=16955.910923, train/accuracy=0.386719, train/loss=2.805215, validation/accuracy=0.361280, validation/loss=2.948040, validation/num_examples=50000
I0202 17:41:57.317023 139774417409792 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0820218324661255, loss=5.142107963562012
I0202 17:42:40.783047 139774434195200 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.0119479894638062, loss=3.8401341438293457
I0202 17:43:26.991559 139774417409792 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.0804526805877686, loss=3.8721704483032227
I0202 17:44:13.085157 139774434195200 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.0887839794158936, loss=3.956533432006836
I0202 17:44:59.294565 139774417409792 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6999450922012329, loss=5.090441703796387
I0202 17:45:45.686136 139774434195200 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7343977689743042, loss=5.332822322845459
I0202 17:46:32.181831 139774417409792 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.0347521305084229, loss=3.81430983543396
I0202 17:47:18.466794 139774434195200 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.0165183544158936, loss=3.9084668159484863
I0202 17:48:04.991213 139774417409792 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.8804476261138916, loss=3.791806697845459
I0202 17:48:46.259478 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:48:56.848241 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:49:31.112088 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:49:32.734006 139936116377408 submission_runner.py:408] Time since start: 17422.89s, 	Step: 33891, 	{'train/accuracy': 0.3903124928474426, 'train/loss': 2.8002209663391113, 'validation/accuracy': 0.36399999260902405, 'validation/loss': 2.969886541366577, 'validation/num_examples': 50000, 'test/accuracy': 0.2771000266075134, 'test/loss': 3.5811426639556885, 'test/num_examples': 10000, 'score': 15580.551282167435, 'total_duration': 17422.885044813156, 'accumulated_submission_time': 15580.551282167435, 'accumulated_eval_time': 1839.4359893798828, 'accumulated_logging_time': 1.1127994060516357}
I0202 17:49:32.761027 139774434195200 logging_writer.py:48] [33891] accumulated_eval_time=1839.435989, accumulated_logging_time=1.112799, accumulated_submission_time=15580.551282, global_step=33891, preemption_count=0, score=15580.551282, test/accuracy=0.277100, test/loss=3.581143, test/num_examples=10000, total_duration=17422.885045, train/accuracy=0.390312, train/loss=2.800221, validation/accuracy=0.364000, validation/loss=2.969887, validation/num_examples=50000
I0202 17:49:36.742871 139774417409792 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.98260098695755, loss=3.913644313812256
I0202 17:50:19.151811 139774434195200 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0864766836166382, loss=3.9332005977630615
I0202 17:51:05.307633 139774417409792 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.9256942272186279, loss=3.8367817401885986
I0202 17:51:51.906304 139774434195200 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.915521502494812, loss=4.042435646057129
I0202 17:52:38.124963 139774417409792 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.9350360631942749, loss=5.897776126861572
I0202 17:53:24.534655 139774434195200 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.0211271047592163, loss=3.9315788745880127
I0202 17:54:10.792634 139774417409792 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.2764599323272705, loss=4.043079853057861
I0202 17:54:56.931824 139774434195200 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.3106836080551147, loss=3.758139133453369
I0202 17:55:43.197343 139774417409792 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.1402143239974976, loss=3.807454824447632
I0202 17:56:29.613443 139774434195200 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.7273474335670471, loss=4.960704803466797
I0202 17:56:32.987486 139936116377408 spec.py:321] Evaluating on the training split.
I0202 17:56:43.699985 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 17:57:20.622428 139936116377408 spec.py:349] Evaluating on the test split.
I0202 17:57:22.227172 139936116377408 submission_runner.py:408] Time since start: 17892.38s, 	Step: 34809, 	{'train/accuracy': 0.3956054747104645, 'train/loss': 2.79667067527771, 'validation/accuracy': 0.35711997747421265, 'validation/loss': 3.022451400756836, 'validation/num_examples': 50000, 'test/accuracy': 0.26840001344680786, 'test/loss': 3.636655569076538, 'test/num_examples': 10000, 'score': 16000.717082977295, 'total_duration': 17892.378203630447, 'accumulated_submission_time': 16000.717082977295, 'accumulated_eval_time': 1888.6756381988525, 'accumulated_logging_time': 1.1519043445587158}
I0202 17:57:22.247830 139774417409792 logging_writer.py:48] [34809] accumulated_eval_time=1888.675638, accumulated_logging_time=1.151904, accumulated_submission_time=16000.717083, global_step=34809, preemption_count=0, score=16000.717083, test/accuracy=0.268400, test/loss=3.636656, test/num_examples=10000, total_duration=17892.378204, train/accuracy=0.395605, train/loss=2.796671, validation/accuracy=0.357120, validation/loss=3.022451, validation/num_examples=50000
I0202 17:58:00.434061 139774434195200 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8565785884857178, loss=4.913135051727295
I0202 17:58:46.386467 139774417409792 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7735865116119385, loss=5.849767684936523
I0202 17:59:33.210173 139774434195200 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.8737716674804688, loss=5.655754089355469
I0202 18:00:19.941347 139774417409792 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.911119818687439, loss=3.8337903022766113
I0202 18:01:06.312402 139774434195200 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.0074416399002075, loss=4.147769451141357
I0202 18:01:52.727846 139774417409792 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.3028621673583984, loss=3.7858364582061768
I0202 18:02:39.039422 139774434195200 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.3409085273742676, loss=3.9245340824127197
I0202 18:03:25.403562 139774417409792 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.0373164415359497, loss=5.629124164581299
I0202 18:04:11.781084 139774434195200 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.00845205783844, loss=3.9095420837402344
I0202 18:04:22.608387 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:04:33.181242 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 18:05:10.847041 139936116377408 spec.py:349] Evaluating on the test split.
I0202 18:05:12.456908 139936116377408 submission_runner.py:408] Time since start: 18362.61s, 	Step: 35725, 	{'train/accuracy': 0.390937477350235, 'train/loss': 2.7831339836120605, 'validation/accuracy': 0.3679399788379669, 'validation/loss': 2.916210174560547, 'validation/num_examples': 50000, 'test/accuracy': 0.2833000123500824, 'test/loss': 3.526043176651001, 'test/num_examples': 10000, 'score': 16421.02041387558, 'total_duration': 18362.607944726944, 'accumulated_submission_time': 16421.02041387558, 'accumulated_eval_time': 1938.5241289138794, 'accumulated_logging_time': 1.1824181079864502}
I0202 18:05:12.482774 139774417409792 logging_writer.py:48] [35725] accumulated_eval_time=1938.524129, accumulated_logging_time=1.182418, accumulated_submission_time=16421.020414, global_step=35725, preemption_count=0, score=16421.020414, test/accuracy=0.283300, test/loss=3.526043, test/num_examples=10000, total_duration=18362.607945, train/accuracy=0.390937, train/loss=2.783134, validation/accuracy=0.367940, validation/loss=2.916210, validation/num_examples=50000
I0202 18:05:43.215347 139774434195200 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.9786957502365112, loss=5.98689603805542
I0202 18:06:29.198238 139774417409792 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.9822187423706055, loss=3.826493740081787
I0202 18:07:15.810761 139774434195200 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.125726342201233, loss=3.8754525184631348
I0202 18:08:02.443973 139774417409792 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.9562946557998657, loss=3.6614043712615967
I0202 18:08:48.750949 139774434195200 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1214574575424194, loss=3.776069402694702
I0202 18:09:35.409938 139774417409792 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1432321071624756, loss=3.7868337631225586
I0202 18:10:21.933771 139774434195200 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.0547884702682495, loss=4.1702470779418945
I0202 18:11:08.577799 139774417409792 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8570073843002319, loss=4.673226356506348
I0202 18:11:55.162475 139774434195200 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0241061449050903, loss=3.82456636428833
I0202 18:12:12.749219 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:12:23.455804 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 18:13:00.516326 139936116377408 spec.py:349] Evaluating on the test split.
I0202 18:13:02.126305 139936116377408 submission_runner.py:408] Time since start: 18832.28s, 	Step: 36639, 	{'train/accuracy': 0.38343748450279236, 'train/loss': 2.84735369682312, 'validation/accuracy': 0.3583599925041199, 'validation/loss': 2.9957029819488525, 'validation/num_examples': 50000, 'test/accuracy': 0.2743000090122223, 'test/loss': 3.578441619873047, 'test/num_examples': 10000, 'score': 16841.228150367737, 'total_duration': 18832.27735710144, 'accumulated_submission_time': 16841.228150367737, 'accumulated_eval_time': 1987.9012160301208, 'accumulated_logging_time': 1.2189600467681885}
I0202 18:13:02.159242 139774417409792 logging_writer.py:48] [36639] accumulated_eval_time=1987.901216, accumulated_logging_time=1.218960, accumulated_submission_time=16841.228150, global_step=36639, preemption_count=0, score=16841.228150, test/accuracy=0.274300, test/loss=3.578442, test/num_examples=10000, total_duration=18832.277357, train/accuracy=0.383437, train/loss=2.847354, validation/accuracy=0.358360, validation/loss=2.995703, validation/num_examples=50000
I0202 18:13:26.801270 139774434195200 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.9675542116165161, loss=3.7961597442626953
I0202 18:14:12.443675 139774417409792 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.9675787091255188, loss=3.7458114624023438
I0202 18:14:58.780327 139774434195200 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.834248960018158, loss=5.488601207733154
I0202 18:15:45.315956 139774417409792 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0058205127716064, loss=3.9183833599090576
I0202 18:16:31.701878 139774434195200 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.9400394558906555, loss=3.8636913299560547
I0202 18:17:18.086373 139774417409792 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.9465392231941223, loss=4.174614429473877
I0202 18:18:04.492929 139774434195200 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.2983146905899048, loss=3.718806266784668
I0202 18:18:50.798686 139774417409792 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0360902547836304, loss=3.671182632446289
I0202 18:19:37.236799 139774434195200 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.0637861490249634, loss=3.91688871383667
I0202 18:20:02.388832 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:20:12.877385 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 18:20:52.818380 139936116377408 spec.py:349] Evaluating on the test split.
I0202 18:20:54.420783 139936116377408 submission_runner.py:408] Time since start: 19304.57s, 	Step: 37555, 	{'train/accuracy': 0.3958398401737213, 'train/loss': 2.8110783100128174, 'validation/accuracy': 0.36098000407218933, 'validation/loss': 3.0116312503814697, 'validation/num_examples': 50000, 'test/accuracy': 0.27650001645088196, 'test/loss': 3.5944924354553223, 'test/num_examples': 10000, 'score': 17261.399400949478, 'total_duration': 19304.57182574272, 'accumulated_submission_time': 17261.399400949478, 'accumulated_eval_time': 2039.933144569397, 'accumulated_logging_time': 1.262007474899292}
I0202 18:20:54.447814 139774417409792 logging_writer.py:48] [37555] accumulated_eval_time=2039.933145, accumulated_logging_time=1.262007, accumulated_submission_time=17261.399401, global_step=37555, preemption_count=0, score=17261.399401, test/accuracy=0.276500, test/loss=3.594492, test/num_examples=10000, total_duration=19304.571826, train/accuracy=0.395840, train/loss=2.811078, validation/accuracy=0.360980, validation/loss=3.011631, validation/num_examples=50000
I0202 18:21:12.741695 139774434195200 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.093049168586731, loss=3.8426263332366943
I0202 18:21:57.660658 139774417409792 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.6219700574874878, loss=4.277348518371582
I0202 18:22:43.810417 139774434195200 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.9042304754257202, loss=4.904512882232666
I0202 18:23:30.311459 139774417409792 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8433195948600769, loss=5.06956672668457
I0202 18:24:16.658786 139774434195200 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9663763642311096, loss=3.7652199268341064
I0202 18:25:02.844900 139774417409792 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.8453654050827026, loss=5.96683406829834
I0202 18:25:49.039676 139774434195200 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.2040265798568726, loss=4.36240291595459
I0202 18:26:35.435055 139774417409792 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.8964481353759766, loss=3.8555891513824463
I0202 18:27:21.802007 139774434195200 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.8598058223724365, loss=4.630358695983887
I0202 18:27:54.540918 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:28:05.219893 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 18:28:41.270641 139936116377408 spec.py:349] Evaluating on the test split.
I0202 18:28:42.873583 139936116377408 submission_runner.py:408] Time since start: 19773.02s, 	Step: 38473, 	{'train/accuracy': 0.4010546803474426, 'train/loss': 2.771629571914673, 'validation/accuracy': 0.37567999958992004, 'validation/loss': 2.9098598957061768, 'validation/num_examples': 50000, 'test/accuracy': 0.29420000314712524, 'test/loss': 3.5065810680389404, 'test/num_examples': 10000, 'score': 17681.434158086777, 'total_duration': 19773.024648189545, 'accumulated_submission_time': 17681.434158086777, 'accumulated_eval_time': 2088.265805721283, 'accumulated_logging_time': 1.299102783203125}
I0202 18:28:42.897323 139774417409792 logging_writer.py:48] [38473] accumulated_eval_time=2088.265806, accumulated_logging_time=1.299103, accumulated_submission_time=17681.434158, global_step=38473, preemption_count=0, score=17681.434158, test/accuracy=0.294200, test/loss=3.506581, test/num_examples=10000, total_duration=19773.024648, train/accuracy=0.401055, train/loss=2.771630, validation/accuracy=0.375680, validation/loss=2.909860, validation/num_examples=50000
I0202 18:28:54.021628 139774434195200 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.095565915107727, loss=3.640324831008911
I0202 18:29:37.890520 139774417409792 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.0507705211639404, loss=3.824650526046753
I0202 18:30:23.746128 139774434195200 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.091478705406189, loss=4.900393962860107
I0202 18:31:10.492238 139774417409792 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.1072299480438232, loss=3.818284034729004
I0202 18:31:56.903385 139774434195200 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.1294575929641724, loss=3.8721654415130615
I0202 18:32:43.069869 139774417409792 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1138200759887695, loss=4.227872371673584
I0202 18:33:29.526885 139774434195200 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.9821470379829407, loss=3.805428981781006
I0202 18:34:15.771206 139774417409792 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.8384758234024048, loss=4.073676109313965
I0202 18:35:02.002016 139774434195200 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.8149645924568176, loss=4.59514856338501
I0202 18:35:43.023785 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:35:53.649913 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 18:36:30.984297 139936116377408 spec.py:349] Evaluating on the test split.
I0202 18:36:32.596690 139936116377408 submission_runner.py:408] Time since start: 20242.75s, 	Step: 39390, 	{'train/accuracy': 0.39265623688697815, 'train/loss': 2.793609142303467, 'validation/accuracy': 0.36813998222351074, 'validation/loss': 2.933591842651367, 'validation/num_examples': 50000, 'test/accuracy': 0.28300002217292786, 'test/loss': 3.5438942909240723, 'test/num_examples': 10000, 'score': 18101.503092050552, 'total_duration': 20242.74775648117, 'accumulated_submission_time': 18101.503092050552, 'accumulated_eval_time': 2137.8387155532837, 'accumulated_logging_time': 1.3325514793395996}
I0202 18:36:32.621214 139774417409792 logging_writer.py:48] [39390] accumulated_eval_time=2137.838716, accumulated_logging_time=1.332551, accumulated_submission_time=18101.503092, global_step=39390, preemption_count=0, score=18101.503092, test/accuracy=0.283000, test/loss=3.543894, test/num_examples=10000, total_duration=20242.747756, train/accuracy=0.392656, train/loss=2.793609, validation/accuracy=0.368140, validation/loss=2.933592, validation/num_examples=50000
I0202 18:36:37.000822 139774434195200 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.8570480346679688, loss=4.579313278198242
I0202 18:37:19.360397 139774417409792 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.039642333984375, loss=3.8725504875183105
I0202 18:38:05.467781 139774434195200 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.0260252952575684, loss=3.7535738945007324
I0202 18:38:51.799178 139774417409792 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.9587211012840271, loss=3.8968241214752197
I0202 18:39:37.935656 139774434195200 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.1329134702682495, loss=3.731536865234375
I0202 18:40:24.421778 139774417409792 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.9480438828468323, loss=3.7083990573883057
I0202 18:41:10.778271 139774434195200 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7817128300666809, loss=5.441061019897461
I0202 18:41:57.647587 139774417409792 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.588753342628479, loss=3.9496891498565674
I0202 18:42:43.986234 139774434195200 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.9978263974189758, loss=3.6994335651397705
I0202 18:43:30.347237 139774417409792 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.1549879312515259, loss=3.827453136444092
I0202 18:43:32.869168 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:43:44.632721 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 18:44:20.610264 139936116377408 spec.py:349] Evaluating on the test split.
I0202 18:44:22.229835 139936116377408 submission_runner.py:408] Time since start: 20712.38s, 	Step: 40307, 	{'train/accuracy': 0.4039843678474426, 'train/loss': 2.782717227935791, 'validation/accuracy': 0.3682200014591217, 'validation/loss': 2.971782684326172, 'validation/num_examples': 50000, 'test/accuracy': 0.28610000014305115, 'test/loss': 3.549136161804199, 'test/num_examples': 10000, 'score': 18521.691387176514, 'total_duration': 20712.3809030056, 'accumulated_submission_time': 18521.691387176514, 'accumulated_eval_time': 2187.1993803977966, 'accumulated_logging_time': 1.3684730529785156}
I0202 18:44:22.250757 139774434195200 logging_writer.py:48] [40307] accumulated_eval_time=2187.199380, accumulated_logging_time=1.368473, accumulated_submission_time=18521.691387, global_step=40307, preemption_count=0, score=18521.691387, test/accuracy=0.286100, test/loss=3.549136, test/num_examples=10000, total_duration=20712.380903, train/accuracy=0.403984, train/loss=2.782717, validation/accuracy=0.368220, validation/loss=2.971783, validation/num_examples=50000
I0202 18:45:01.198720 139774417409792 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.9718903303146362, loss=3.704033851623535
I0202 18:45:47.259196 139774434195200 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.107265591621399, loss=3.653127670288086
I0202 18:46:33.790182 139774417409792 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8023402690887451, loss=4.42512845993042
I0202 18:47:19.926276 139774434195200 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.1414172649383545, loss=3.7393317222595215
I0202 18:48:06.082458 139774417409792 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.8619407415390015, loss=3.843705415725708
I0202 18:48:52.313254 139774434195200 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.7308984398841858, loss=5.8814568519592285
I0202 18:49:38.418394 139774417409792 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.089554786682129, loss=4.223517417907715
I0202 18:50:24.525863 139774434195200 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1635264158248901, loss=3.9232215881347656
I0202 18:51:10.678311 139774417409792 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.069293737411499, loss=3.668100357055664
I0202 18:51:22.342255 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:51:33.141818 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 18:52:13.962367 139936116377408 spec.py:349] Evaluating on the test split.
I0202 18:52:15.567763 139936116377408 submission_runner.py:408] Time since start: 21185.72s, 	Step: 41227, 	{'train/accuracy': 0.4065038859844208, 'train/loss': 2.7331039905548096, 'validation/accuracy': 0.3854199945926666, 'validation/loss': 2.870001792907715, 'validation/num_examples': 50000, 'test/accuracy': 0.29590001702308655, 'test/loss': 3.476227045059204, 'test/num_examples': 10000, 'score': 18941.72615456581, 'total_duration': 21185.718816757202, 'accumulated_submission_time': 18941.72615456581, 'accumulated_eval_time': 2240.424861431122, 'accumulated_logging_time': 1.3985400199890137}
I0202 18:52:15.590308 139774434195200 logging_writer.py:48] [41227] accumulated_eval_time=2240.424861, accumulated_logging_time=1.398540, accumulated_submission_time=18941.726155, global_step=41227, preemption_count=0, score=18941.726155, test/accuracy=0.295900, test/loss=3.476227, test/num_examples=10000, total_duration=21185.718817, train/accuracy=0.406504, train/loss=2.733104, validation/accuracy=0.385420, validation/loss=2.870002, validation/num_examples=50000
I0202 18:52:45.579740 139774417409792 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.9968706965446472, loss=4.060798645019531
I0202 18:53:31.557990 139774434195200 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.082901954650879, loss=3.7270493507385254
I0202 18:54:17.902247 139774417409792 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7098314762115479, loss=5.262602806091309
I0202 18:55:04.582939 139774434195200 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7057035565376282, loss=5.379621505737305
I0202 18:55:50.655761 139774417409792 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7802072763442993, loss=5.95853328704834
I0202 18:56:36.999305 139774434195200 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.0230334997177124, loss=3.659165143966675
I0202 18:57:23.444950 139774417409792 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.7649221420288086, loss=5.557551383972168
I0202 18:58:09.778676 139774434195200 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0568219423294067, loss=3.771800994873047
I0202 18:58:55.786173 139774417409792 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.9224294424057007, loss=5.023393154144287
I0202 18:59:15.948879 139936116377408 spec.py:321] Evaluating on the training split.
I0202 18:59:26.692362 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:00:03.584861 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:00:05.186670 139936116377408 submission_runner.py:408] Time since start: 21655.34s, 	Step: 42145, 	{'train/accuracy': 0.41316404938697815, 'train/loss': 2.6345086097717285, 'validation/accuracy': 0.38152000308036804, 'validation/loss': 2.8117265701293945, 'validation/num_examples': 50000, 'test/accuracy': 0.29250001907348633, 'test/loss': 3.437443971633911, 'test/num_examples': 10000, 'score': 19362.027856588364, 'total_duration': 21655.337735414505, 'accumulated_submission_time': 19362.027856588364, 'accumulated_eval_time': 2289.662645339966, 'accumulated_logging_time': 1.4311163425445557}
I0202 19:00:05.211547 139774434195200 logging_writer.py:48] [42145] accumulated_eval_time=2289.662645, accumulated_logging_time=1.431116, accumulated_submission_time=19362.027857, global_step=42145, preemption_count=0, score=19362.027857, test/accuracy=0.292500, test/loss=3.437444, test/num_examples=10000, total_duration=21655.337735, train/accuracy=0.413164, train/loss=2.634509, validation/accuracy=0.381520, validation/loss=2.811727, validation/num_examples=50000
I0202 19:00:27.476329 139774417409792 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.9650082588195801, loss=3.665544271469116
I0202 19:01:12.614840 139774434195200 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1569039821624756, loss=3.7976927757263184
I0202 19:01:59.434020 139774417409792 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.9873917698860168, loss=4.207203388214111
I0202 19:02:45.955094 139774434195200 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0563005208969116, loss=3.745614528656006
I0202 19:03:32.630544 139774417409792 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7339749932289124, loss=4.62690544128418
I0202 19:04:18.985100 139774434195200 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0887353420257568, loss=3.7235026359558105
I0202 19:05:05.231433 139774417409792 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.0351910591125488, loss=3.7134923934936523
I0202 19:05:51.695382 139774434195200 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.9948729276657104, loss=3.710371971130371
I0202 19:06:37.826384 139774417409792 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0079150199890137, loss=3.887911319732666
I0202 19:07:05.380512 139936116377408 spec.py:321] Evaluating on the training split.
I0202 19:07:16.061637 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:07:52.342501 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:07:53.953577 139936116377408 submission_runner.py:408] Time since start: 22124.10s, 	Step: 43061, 	{'train/accuracy': 0.4229101538658142, 'train/loss': 2.6399965286254883, 'validation/accuracy': 0.3895399868488312, 'validation/loss': 2.8090786933898926, 'validation/num_examples': 50000, 'test/accuracy': 0.30640000104904175, 'test/loss': 3.3924434185028076, 'test/num_examples': 10000, 'score': 19782.140295267105, 'total_duration': 22124.104640245438, 'accumulated_submission_time': 19782.140295267105, 'accumulated_eval_time': 2338.235716342926, 'accumulated_logging_time': 1.4649834632873535}
I0202 19:07:53.982370 139774434195200 logging_writer.py:48] [43061] accumulated_eval_time=2338.235716, accumulated_logging_time=1.464983, accumulated_submission_time=19782.140295, global_step=43061, preemption_count=0, score=19782.140295, test/accuracy=0.306400, test/loss=3.392443, test/num_examples=10000, total_duration=22124.104640, train/accuracy=0.422910, train/loss=2.639997, validation/accuracy=0.389540, validation/loss=2.809079, validation/num_examples=50000
I0202 19:08:09.887032 139774417409792 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.0849100351333618, loss=3.8070223331451416
I0202 19:08:53.971063 139774434195200 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.111765742301941, loss=3.8247997760772705
I0202 19:09:40.284659 139774417409792 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.9495938420295715, loss=3.962989091873169
I0202 19:10:26.797503 139774434195200 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.1631540060043335, loss=3.6696224212646484
I0202 19:11:13.091147 139774417409792 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.8054071068763733, loss=4.185935020446777
I0202 19:11:59.510473 139774434195200 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.1199053525924683, loss=3.7744641304016113
I0202 19:12:45.787463 139774417409792 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.8343867659568787, loss=4.257913589477539
I0202 19:13:32.467818 139774434195200 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.0273189544677734, loss=5.082136154174805
I0202 19:14:18.840912 139774417409792 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7959333658218384, loss=5.405799865722656
I0202 19:14:54.183528 139936116377408 spec.py:321] Evaluating on the training split.
I0202 19:15:04.515953 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:15:42.550940 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:15:44.159159 139936116377408 submission_runner.py:408] Time since start: 22594.31s, 	Step: 43978, 	{'train/accuracy': 0.44771483540534973, 'train/loss': 2.5128729343414307, 'validation/accuracy': 0.38387998938560486, 'validation/loss': 2.8571627140045166, 'validation/num_examples': 50000, 'test/accuracy': 0.29500001668930054, 'test/loss': 3.4772708415985107, 'test/num_examples': 10000, 'score': 20202.28423190117, 'total_duration': 22594.31020140648, 'accumulated_submission_time': 20202.28423190117, 'accumulated_eval_time': 2388.2113218307495, 'accumulated_logging_time': 1.503532886505127}
I0202 19:15:44.186061 139774434195200 logging_writer.py:48] [43978] accumulated_eval_time=2388.211322, accumulated_logging_time=1.503533, accumulated_submission_time=20202.284232, global_step=43978, preemption_count=0, score=20202.284232, test/accuracy=0.295000, test/loss=3.477271, test/num_examples=10000, total_duration=22594.310201, train/accuracy=0.447715, train/loss=2.512873, validation/accuracy=0.383880, validation/loss=2.857163, validation/num_examples=50000
I0202 19:15:53.319031 139774417409792 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1889886856079102, loss=3.7708616256713867
I0202 19:16:36.655758 139774434195200 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.8986952304840088, loss=3.6683640480041504
I0202 19:17:22.805525 139774417409792 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.1068286895751953, loss=3.930006504058838
I0202 19:18:09.450972 139774434195200 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.1325103044509888, loss=3.822877883911133
I0202 19:18:55.476709 139774417409792 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.94203782081604, loss=3.6813766956329346
I0202 19:19:41.884103 139774434195200 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.1455494165420532, loss=3.8120906352996826
I0202 19:20:28.194556 139774417409792 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.9252730011940002, loss=3.8947973251342773
I0202 19:21:14.462826 139774434195200 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.9246753454208374, loss=3.6268210411071777
I0202 19:22:00.902114 139774417409792 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.9179140329360962, loss=4.02100133895874
I0202 19:22:44.241653 139936116377408 spec.py:321] Evaluating on the training split.
I0202 19:22:54.650910 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:23:33.394948 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:23:35.002206 139936116377408 submission_runner.py:408] Time since start: 23065.15s, 	Step: 44895, 	{'train/accuracy': 0.42130857706069946, 'train/loss': 2.619121551513672, 'validation/accuracy': 0.3991200029850006, 'validation/loss': 2.7532565593719482, 'validation/num_examples': 50000, 'test/accuracy': 0.3067000210285187, 'test/loss': 3.367946147918701, 'test/num_examples': 10000, 'score': 20622.277943134308, 'total_duration': 23065.153266191483, 'accumulated_submission_time': 20622.277943134308, 'accumulated_eval_time': 2438.9718708992004, 'accumulated_logging_time': 1.5404622554779053}
I0202 19:23:35.027966 139774434195200 logging_writer.py:48] [44895] accumulated_eval_time=2438.971871, accumulated_logging_time=1.540462, accumulated_submission_time=20622.277943, global_step=44895, preemption_count=0, score=20622.277943, test/accuracy=0.306700, test/loss=3.367946, test/num_examples=10000, total_duration=23065.153266, train/accuracy=0.421309, train/loss=2.619122, validation/accuracy=0.399120, validation/loss=2.753257, validation/num_examples=50000
I0202 19:23:37.417841 139774417409792 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.044262170791626, loss=3.7462613582611084
I0202 19:24:19.573959 139774434195200 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.809095025062561, loss=3.933529853820801
I0202 19:25:06.043263 139774417409792 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.0147615671157837, loss=3.6850671768188477
I0202 19:25:52.236109 139774434195200 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.8652926683425903, loss=5.797097206115723
I0202 19:26:38.565480 139774417409792 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.1113877296447754, loss=3.7320332527160645
I0202 19:27:24.837056 139774434195200 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.0591970682144165, loss=3.6494510173797607
I0202 19:28:11.015049 139774417409792 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0338701009750366, loss=3.63450288772583
I0202 19:28:57.235785 139774434195200 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9559274911880493, loss=4.248223781585693
I0202 19:29:43.466688 139774417409792 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0826311111450195, loss=3.7097342014312744
I0202 19:30:29.620496 139774434195200 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.102663516998291, loss=3.7243785858154297
I0202 19:30:35.348268 139936116377408 spec.py:321] Evaluating on the training split.
I0202 19:30:45.895533 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:31:24.581474 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:31:26.187101 139936116377408 submission_runner.py:408] Time since start: 23536.34s, 	Step: 45814, 	{'train/accuracy': 0.41191405057907104, 'train/loss': 2.6856353282928467, 'validation/accuracy': 0.38457998633384705, 'validation/loss': 2.8403830528259277, 'validation/num_examples': 50000, 'test/accuracy': 0.29260000586509705, 'test/loss': 3.459824323654175, 'test/num_examples': 10000, 'score': 21042.53992986679, 'total_duration': 23536.338141679764, 'accumulated_submission_time': 21042.53992986679, 'accumulated_eval_time': 2489.8106729984283, 'accumulated_logging_time': 1.576117992401123}
I0202 19:31:26.214684 139774417409792 logging_writer.py:48] [45814] accumulated_eval_time=2489.810673, accumulated_logging_time=1.576118, accumulated_submission_time=21042.539930, global_step=45814, preemption_count=0, score=21042.539930, test/accuracy=0.292600, test/loss=3.459824, test/num_examples=10000, total_duration=23536.338142, train/accuracy=0.411914, train/loss=2.685635, validation/accuracy=0.384580, validation/loss=2.840383, validation/num_examples=50000
I0202 19:32:02.428009 139774434195200 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.9167608618736267, loss=4.180902004241943
I0202 19:32:48.587027 139774417409792 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.93083655834198, loss=3.9750630855560303
I0202 19:33:35.030685 139774434195200 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.8445712924003601, loss=4.1097092628479
I0202 19:34:21.464868 139774417409792 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.8611043691635132, loss=4.829353332519531
I0202 19:35:08.301416 139774434195200 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0285472869873047, loss=3.7068347930908203
I0202 19:35:54.561197 139774417409792 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.951583743095398, loss=4.69630241394043
I0202 19:36:41.524078 139774434195200 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1324940919876099, loss=4.20225715637207
I0202 19:37:28.133388 139774417409792 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1510556936264038, loss=3.6083059310913086
I0202 19:38:14.653913 139774434195200 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.9868086576461792, loss=3.6204731464385986
I0202 19:38:26.389576 139936116377408 spec.py:321] Evaluating on the training split.
I0202 19:38:36.788381 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:39:15.561263 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:39:17.160427 139936116377408 submission_runner.py:408] Time since start: 24007.31s, 	Step: 46727, 	{'train/accuracy': 0.4382031261920929, 'train/loss': 2.527977466583252, 'validation/accuracy': 0.3956199884414673, 'validation/loss': 2.7620689868927, 'validation/num_examples': 50000, 'test/accuracy': 0.3050000071525574, 'test/loss': 3.409043550491333, 'test/num_examples': 10000, 'score': 21462.657836198807, 'total_duration': 24007.31149339676, 'accumulated_submission_time': 21462.657836198807, 'accumulated_eval_time': 2540.581508398056, 'accumulated_logging_time': 1.6131682395935059}
I0202 19:39:17.185466 139774417409792 logging_writer.py:48] [46727] accumulated_eval_time=2540.581508, accumulated_logging_time=1.613168, accumulated_submission_time=21462.657836, global_step=46727, preemption_count=0, score=21462.657836, test/accuracy=0.305000, test/loss=3.409044, test/num_examples=10000, total_duration=24007.311493, train/accuracy=0.438203, train/loss=2.527977, validation/accuracy=0.395620, validation/loss=2.762069, validation/num_examples=50000
I0202 19:39:46.990025 139774434195200 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.9585590362548828, loss=3.805093288421631
I0202 19:40:32.815781 139774417409792 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.9524562954902649, loss=5.627774238586426
I0202 19:41:19.510617 139774434195200 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.180786371231079, loss=3.5753910541534424
I0202 19:42:06.130284 139774417409792 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.193557620048523, loss=3.617353677749634
I0202 19:42:52.442441 139774434195200 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.0798218250274658, loss=5.62065315246582
I0202 19:43:38.756441 139774417409792 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.1688897609710693, loss=3.7726192474365234
I0202 19:44:25.153316 139774434195200 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.020700454711914, loss=4.332317352294922
I0202 19:45:11.719515 139774417409792 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.215410590171814, loss=3.69171404838562
I0202 19:45:58.212117 139774434195200 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.145751714706421, loss=3.7733116149902344
I0202 19:46:17.475632 139936116377408 spec.py:321] Evaluating on the training split.
I0202 19:46:28.170572 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:47:06.738393 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:47:08.355380 139936116377408 submission_runner.py:408] Time since start: 24478.51s, 	Step: 47643, 	{'train/accuracy': 0.4313085973262787, 'train/loss': 2.561516046524048, 'validation/accuracy': 0.40535998344421387, 'validation/loss': 2.7094690799713135, 'validation/num_examples': 50000, 'test/accuracy': 0.31050002574920654, 'test/loss': 3.3404970169067383, 'test/num_examples': 10000, 'score': 21882.891329288483, 'total_duration': 24478.50642466545, 'accumulated_submission_time': 21882.891329288483, 'accumulated_eval_time': 2591.4612271785736, 'accumulated_logging_time': 1.6478898525238037}
I0202 19:47:08.380874 139774417409792 logging_writer.py:48] [47643] accumulated_eval_time=2591.461227, accumulated_logging_time=1.647890, accumulated_submission_time=21882.891329, global_step=47643, preemption_count=0, score=21882.891329, test/accuracy=0.310500, test/loss=3.340497, test/num_examples=10000, total_duration=24478.506425, train/accuracy=0.431309, train/loss=2.561516, validation/accuracy=0.405360, validation/loss=2.709469, validation/num_examples=50000
I0202 19:47:31.443975 139774434195200 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.0219799280166626, loss=3.6209964752197266
I0202 19:48:16.826649 139774417409792 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.8964606523513794, loss=5.215052127838135
I0202 19:49:03.123746 139774434195200 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.0670937299728394, loss=3.546337604522705
I0202 19:49:49.458784 139774417409792 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.88400799036026, loss=4.606067180633545
I0202 19:50:35.602516 139774434195200 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.9478784799575806, loss=5.171317100524902
I0202 19:51:21.869544 139774417409792 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.0785783529281616, loss=3.7618181705474854
I0202 19:52:08.450820 139774434195200 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.3522366285324097, loss=3.8797926902770996
I0202 19:52:54.586764 139774417409792 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.0459377765655518, loss=3.6231765747070312
I0202 19:53:40.939611 139774434195200 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0100082159042358, loss=3.7124428749084473
I0202 19:54:08.512670 139936116377408 spec.py:321] Evaluating on the training split.
I0202 19:54:19.321056 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 19:54:57.018576 139936116377408 spec.py:349] Evaluating on the test split.
I0202 19:54:58.641871 139936116377408 submission_runner.py:408] Time since start: 24948.79s, 	Step: 48561, 	{'train/accuracy': 0.4389062523841858, 'train/loss': 2.5233049392700195, 'validation/accuracy': 0.41290000081062317, 'validation/loss': 2.669914960861206, 'validation/num_examples': 50000, 'test/accuracy': 0.31790000200271606, 'test/loss': 3.2796595096588135, 'test/num_examples': 10000, 'score': 22302.96504020691, 'total_duration': 24948.79293680191, 'accumulated_submission_time': 22302.96504020691, 'accumulated_eval_time': 2641.5904109477997, 'accumulated_logging_time': 1.6836578845977783}
I0202 19:54:58.668641 139774417409792 logging_writer.py:48] [48561] accumulated_eval_time=2641.590411, accumulated_logging_time=1.683658, accumulated_submission_time=22302.965040, global_step=48561, preemption_count=0, score=22302.965040, test/accuracy=0.317900, test/loss=3.279660, test/num_examples=10000, total_duration=24948.792937, train/accuracy=0.438906, train/loss=2.523305, validation/accuracy=0.412900, validation/loss=2.669915, validation/num_examples=50000
I0202 19:55:14.575010 139774434195200 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0273363590240479, loss=3.4523046016693115
I0202 19:55:59.261789 139774417409792 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.9594687819480896, loss=3.5930395126342773
I0202 19:56:45.796689 139774434195200 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.179595708847046, loss=3.5857200622558594
I0202 19:57:32.888802 139774417409792 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.9485995173454285, loss=3.518239736557007
I0202 19:58:19.365423 139774434195200 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8645697832107544, loss=5.512331008911133
I0202 19:59:05.854565 139774417409792 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.7482712864875793, loss=5.992534637451172
I0202 19:59:52.309423 139774434195200 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.1868928670883179, loss=3.634014129638672
I0202 20:00:38.744605 139774417409792 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9169357419013977, loss=3.9384536743164062
I0202 20:01:25.194051 139774434195200 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.1453802585601807, loss=3.654630422592163
I0202 20:01:59.052593 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:02:09.467210 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:02:47.939685 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:02:49.546092 139936116377408 submission_runner.py:408] Time since start: 25419.70s, 	Step: 49474, 	{'train/accuracy': 0.449531227350235, 'train/loss': 2.4620165824890137, 'validation/accuracy': 0.4113599956035614, 'validation/loss': 2.6755542755126953, 'validation/num_examples': 50000, 'test/accuracy': 0.3184000253677368, 'test/loss': 3.2931008338928223, 'test/num_examples': 10000, 'score': 22723.290951013565, 'total_duration': 25419.697157859802, 'accumulated_submission_time': 22723.290951013565, 'accumulated_eval_time': 2692.083906888962, 'accumulated_logging_time': 1.7213480472564697}
I0202 20:02:49.569174 139774417409792 logging_writer.py:48] [49474] accumulated_eval_time=2692.083907, accumulated_logging_time=1.721348, accumulated_submission_time=22723.290951, global_step=49474, preemption_count=0, score=22723.290951, test/accuracy=0.318400, test/loss=3.293101, test/num_examples=10000, total_duration=25419.697158, train/accuracy=0.449531, train/loss=2.462017, validation/accuracy=0.411360, validation/loss=2.675554, validation/num_examples=50000
I0202 20:03:00.303635 139774434195200 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.9650322198867798, loss=3.9765281677246094
I0202 20:03:43.528568 139774417409792 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.0264273881912231, loss=3.5417842864990234
I0202 20:04:30.024829 139774434195200 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0349295139312744, loss=3.5607118606567383
I0202 20:05:16.584456 139774417409792 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.9259341955184937, loss=5.918178081512451
I0202 20:06:02.990854 139774434195200 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.9152613878250122, loss=5.283084869384766
I0202 20:06:50.637315 139774417409792 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.0679086446762085, loss=3.575674533843994
I0202 20:07:37.369055 139774434195200 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9845308661460876, loss=4.8474321365356445
I0202 20:08:24.068220 139774417409792 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.9069005250930786, loss=3.7960870265960693
I0202 20:09:10.803767 139774434195200 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.9877577424049377, loss=4.91682767868042
I0202 20:09:49.857019 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:10:00.286133 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:10:34.738817 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:10:36.345615 139936116377408 submission_runner.py:408] Time since start: 25886.50s, 	Step: 50386, 	{'train/accuracy': 0.4435351490974426, 'train/loss': 2.5222740173339844, 'validation/accuracy': 0.4126800000667572, 'validation/loss': 2.6682076454162598, 'validation/num_examples': 50000, 'test/accuracy': 0.31390002369880676, 'test/loss': 3.3136818408966064, 'test/num_examples': 10000, 'score': 23143.52159333229, 'total_duration': 25886.496681451797, 'accumulated_submission_time': 23143.52159333229, 'accumulated_eval_time': 2738.5725288391113, 'accumulated_logging_time': 1.7537884712219238}
I0202 20:10:36.373373 139774417409792 logging_writer.py:48] [50386] accumulated_eval_time=2738.572529, accumulated_logging_time=1.753788, accumulated_submission_time=23143.521593, global_step=50386, preemption_count=0, score=23143.521593, test/accuracy=0.313900, test/loss=3.313682, test/num_examples=10000, total_duration=25886.496681, train/accuracy=0.443535, train/loss=2.522274, validation/accuracy=0.412680, validation/loss=2.668208, validation/num_examples=50000
I0202 20:10:42.341633 139774434195200 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.9750499129295349, loss=4.222784519195557
I0202 20:11:25.196300 139774417409792 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.0497729778289795, loss=3.5462284088134766
I0202 20:12:11.714541 139774434195200 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.8408425450325012, loss=5.811545372009277
I0202 20:12:58.082460 139774417409792 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.9375982880592346, loss=4.149450778961182
I0202 20:13:44.588837 139774434195200 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.4055743217468262, loss=3.536893844604492
I0202 20:14:31.140064 139774417409792 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.267935872077942, loss=3.630450487136841
I0202 20:15:17.816797 139774434195200 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0491961240768433, loss=4.183882713317871
I0202 20:16:04.210630 139774417409792 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.021594524383545, loss=3.7104744911193848
I0202 20:16:50.634217 139774434195200 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.1164233684539795, loss=3.5490360260009766
I0202 20:17:36.737130 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:17:47.413806 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:18:24.699978 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:18:26.308210 139936116377408 submission_runner.py:408] Time since start: 26356.46s, 	Step: 51300, 	{'train/accuracy': 0.4445898234844208, 'train/loss': 2.4773788452148438, 'validation/accuracy': 0.4203200042247772, 'validation/loss': 2.636164903640747, 'validation/num_examples': 50000, 'test/accuracy': 0.3272000253200531, 'test/loss': 3.2604527473449707, 'test/num_examples': 10000, 'score': 23563.827215909958, 'total_duration': 26356.45927453041, 'accumulated_submission_time': 23563.827215909958, 'accumulated_eval_time': 2788.143606901169, 'accumulated_logging_time': 1.7920589447021484}
I0202 20:18:26.337028 139774417409792 logging_writer.py:48] [51300] accumulated_eval_time=2788.143607, accumulated_logging_time=1.792059, accumulated_submission_time=23563.827216, global_step=51300, preemption_count=0, score=23563.827216, test/accuracy=0.327200, test/loss=3.260453, test/num_examples=10000, total_duration=26356.459275, train/accuracy=0.444590, train/loss=2.477379, validation/accuracy=0.420320, validation/loss=2.636165, validation/num_examples=50000
I0202 20:18:26.746429 139774434195200 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.9376375675201416, loss=3.471848726272583
I0202 20:19:08.793421 139774417409792 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.904626190662384, loss=3.9666130542755127
I0202 20:19:54.943881 139774434195200 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.0000324249267578, loss=3.6353325843811035
I0202 20:20:41.417113 139774417409792 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.9738178849220276, loss=5.216350555419922
I0202 20:21:28.110851 139774434195200 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.0748291015625, loss=3.5988118648529053
I0202 20:22:14.609055 139774417409792 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.0129597187042236, loss=4.43308687210083
I0202 20:23:00.568105 139774434195200 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9540542364120483, loss=4.266916751861572
I0202 20:23:47.143918 139774417409792 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.9769445657730103, loss=3.9734811782836914
I0202 20:24:33.353717 139774434195200 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8589651584625244, loss=4.5926923751831055
I0202 20:25:19.915936 139774417409792 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.2854095697402954, loss=3.7383904457092285
I0202 20:25:26.473904 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:25:37.090181 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:26:13.663041 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:26:15.277085 139936116377408 submission_runner.py:408] Time since start: 26825.43s, 	Step: 52216, 	{'train/accuracy': 0.438789039850235, 'train/loss': 2.5263781547546387, 'validation/accuracy': 0.4053199887275696, 'validation/loss': 2.713552474975586, 'validation/num_examples': 50000, 'test/accuracy': 0.31220000982284546, 'test/loss': 3.3333580493927, 'test/num_examples': 10000, 'score': 23983.90644145012, 'total_duration': 26825.42811512947, 'accumulated_submission_time': 23983.90644145012, 'accumulated_eval_time': 2836.9467310905457, 'accumulated_logging_time': 1.830620288848877}
I0202 20:26:15.307476 139774434195200 logging_writer.py:48] [52216] accumulated_eval_time=2836.946731, accumulated_logging_time=1.830620, accumulated_submission_time=23983.906441, global_step=52216, preemption_count=0, score=23983.906441, test/accuracy=0.312200, test/loss=3.333358, test/num_examples=10000, total_duration=26825.428115, train/accuracy=0.438789, train/loss=2.526378, validation/accuracy=0.405320, validation/loss=2.713552, validation/num_examples=50000
I0202 20:26:50.189632 139774417409792 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.961949348449707, loss=3.786874294281006
I0202 20:27:36.076525 139774434195200 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8762143850326538, loss=3.655691385269165
I0202 20:28:22.797204 139774417409792 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2301299571990967, loss=3.704775333404541
I0202 20:29:09.437735 139774434195200 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.9340440630912781, loss=4.120457649230957
I0202 20:29:55.481671 139774417409792 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.9055395126342773, loss=3.5172674655914307
I0202 20:30:42.177579 139774434195200 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.056570291519165, loss=3.5832443237304688
I0202 20:31:28.458976 139774417409792 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.8652561902999878, loss=4.72251033782959
I0202 20:32:14.880897 139774434195200 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9364522695541382, loss=5.650798797607422
I0202 20:33:00.926151 139774417409792 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2774500846862793, loss=3.653778314590454
I0202 20:33:15.475989 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:33:26.083779 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:34:02.167187 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:34:03.780239 139936116377408 submission_runner.py:408] Time since start: 27293.93s, 	Step: 53133, 	{'train/accuracy': 0.4564843773841858, 'train/loss': 2.438621759414673, 'validation/accuracy': 0.4238399863243103, 'validation/loss': 2.607288122177124, 'validation/num_examples': 50000, 'test/accuracy': 0.32520002126693726, 'test/loss': 3.2333431243896484, 'test/num_examples': 10000, 'score': 24404.016840696335, 'total_duration': 27293.931302309036, 'accumulated_submission_time': 24404.016840696335, 'accumulated_eval_time': 2885.250978946686, 'accumulated_logging_time': 1.871053695678711}
I0202 20:34:03.803283 139774434195200 logging_writer.py:48] [53133] accumulated_eval_time=2885.250979, accumulated_logging_time=1.871054, accumulated_submission_time=24404.016841, global_step=53133, preemption_count=0, score=24404.016841, test/accuracy=0.325200, test/loss=3.233343, test/num_examples=10000, total_duration=27293.931302, train/accuracy=0.456484, train/loss=2.438622, validation/accuracy=0.423840, validation/loss=2.607288, validation/num_examples=50000
I0202 20:34:30.925001 139774417409792 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.1767264604568481, loss=3.636183738708496
I0202 20:35:16.647865 139774434195200 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8301804065704346, loss=5.763504505157471
I0202 20:36:02.936760 139774417409792 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.0884875059127808, loss=3.62483549118042
I0202 20:36:49.127088 139774434195200 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.9830501079559326, loss=3.825982093811035
I0202 20:37:35.398464 139774417409792 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.2126169204711914, loss=3.6503653526306152
I0202 20:38:21.534385 139774434195200 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.9410669207572937, loss=4.100764274597168
I0202 20:39:07.847746 139774417409792 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.0341850519180298, loss=4.1870527267456055
I0202 20:39:53.758594 139774434195200 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8874249458312988, loss=5.068784236907959
I0202 20:40:39.969437 139774417409792 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6857883930206299, loss=4.957224369049072
I0202 20:41:04.213489 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:41:14.599400 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:41:53.863609 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:41:55.469599 139936116377408 submission_runner.py:408] Time since start: 27765.62s, 	Step: 54054, 	{'train/accuracy': 0.4422265589237213, 'train/loss': 2.535531759262085, 'validation/accuracy': 0.412200003862381, 'validation/loss': 2.7084314823150635, 'validation/num_examples': 50000, 'test/accuracy': 0.3118000030517578, 'test/loss': 3.3063266277313232, 'test/num_examples': 10000, 'score': 24824.366614103317, 'total_duration': 27765.620665311813, 'accumulated_submission_time': 24824.366614103317, 'accumulated_eval_time': 2936.507098197937, 'accumulated_logging_time': 1.9068207740783691}
I0202 20:41:55.497110 139774434195200 logging_writer.py:48] [54054] accumulated_eval_time=2936.507098, accumulated_logging_time=1.906821, accumulated_submission_time=24824.366614, global_step=54054, preemption_count=0, score=24824.366614, test/accuracy=0.311800, test/loss=3.306327, test/num_examples=10000, total_duration=27765.620665, train/accuracy=0.442227, train/loss=2.535532, validation/accuracy=0.412200, validation/loss=2.708431, validation/num_examples=50000
I0202 20:42:14.196763 139774417409792 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.9163448810577393, loss=4.240704536437988
I0202 20:42:58.675040 139774434195200 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.9615289568901062, loss=3.5239129066467285
I0202 20:43:44.944774 139774417409792 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9789556264877319, loss=4.535841941833496
I0202 20:44:31.402028 139774434195200 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.1825731992721558, loss=3.5781850814819336
I0202 20:45:17.622749 139774417409792 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2866848707199097, loss=3.748809814453125
I0202 20:46:04.121908 139774434195200 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.020159363746643, loss=4.104615688323975
I0202 20:46:50.292385 139774417409792 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.160489797592163, loss=3.654127597808838
I0202 20:47:36.391972 139774434195200 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.8356115221977234, loss=5.021778583526611
I0202 20:48:22.623959 139774417409792 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.0746852159500122, loss=3.5173568725585938
I0202 20:48:55.647552 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:49:06.201131 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:49:43.932011 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:49:45.547369 139936116377408 submission_runner.py:408] Time since start: 28235.70s, 	Step: 54973, 	{'train/accuracy': 0.44408202171325684, 'train/loss': 2.5492329597473145, 'validation/accuracy': 0.41053998470306396, 'validation/loss': 2.712507724761963, 'validation/num_examples': 50000, 'test/accuracy': 0.31690001487731934, 'test/loss': 3.3160958290100098, 'test/num_examples': 10000, 'score': 25244.459810972214, 'total_duration': 28235.698424577713, 'accumulated_submission_time': 25244.459810972214, 'accumulated_eval_time': 2986.406905412674, 'accumulated_logging_time': 1.9439399242401123}
I0202 20:49:45.573766 139774434195200 logging_writer.py:48] [54973] accumulated_eval_time=2986.406905, accumulated_logging_time=1.943940, accumulated_submission_time=25244.459811, global_step=54973, preemption_count=0, score=25244.459811, test/accuracy=0.316900, test/loss=3.316096, test/num_examples=10000, total_duration=28235.698425, train/accuracy=0.444082, train/loss=2.549233, validation/accuracy=0.410540, validation/loss=2.712508, validation/num_examples=50000
I0202 20:49:56.710679 139774417409792 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.0170683860778809, loss=3.5656867027282715
I0202 20:50:40.254468 139774434195200 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.3540458679199219, loss=3.5202269554138184
I0202 20:51:26.584777 139774417409792 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.8973910808563232, loss=3.6201674938201904
I0202 20:52:13.429571 139774434195200 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1110479831695557, loss=3.6575920581817627
I0202 20:52:59.551796 139774417409792 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.0549877882003784, loss=3.4045307636260986
I0202 20:53:45.990453 139774434195200 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.0929323434829712, loss=3.466519832611084
I0202 20:54:32.439342 139774417409792 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.3540949821472168, loss=3.6606533527374268
I0202 20:55:18.739428 139774434195200 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.1669384241104126, loss=3.526136875152588
I0202 20:56:05.184669 139774417409792 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0022928714752197, loss=3.598331928253174
I0202 20:56:45.605715 139936116377408 spec.py:321] Evaluating on the training split.
I0202 20:56:56.247120 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 20:57:33.930117 139936116377408 spec.py:349] Evaluating on the test split.
I0202 20:57:35.542868 139936116377408 submission_runner.py:408] Time since start: 28705.69s, 	Step: 55889, 	{'train/accuracy': 0.47236326336860657, 'train/loss': 2.383117198944092, 'validation/accuracy': 0.4037799835205078, 'validation/loss': 2.7402095794677734, 'validation/num_examples': 50000, 'test/accuracy': 0.31610000133514404, 'test/loss': 3.364776611328125, 'test/num_examples': 10000, 'score': 25664.43350338936, 'total_duration': 28705.693928956985, 'accumulated_submission_time': 25664.43350338936, 'accumulated_eval_time': 3036.3440520763397, 'accumulated_logging_time': 1.9801304340362549}
I0202 20:57:35.570176 139774434195200 logging_writer.py:48] [55889] accumulated_eval_time=3036.344052, accumulated_logging_time=1.980130, accumulated_submission_time=25664.433503, global_step=55889, preemption_count=0, score=25664.433503, test/accuracy=0.316100, test/loss=3.364777, test/num_examples=10000, total_duration=28705.693929, train/accuracy=0.472363, train/loss=2.383117, validation/accuracy=0.403780, validation/loss=2.740210, validation/num_examples=50000
I0202 20:57:40.352699 139774417409792 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.1322563886642456, loss=3.6073408126831055
I0202 20:58:22.969269 139774434195200 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0212265253067017, loss=3.765850782394409
I0202 20:59:09.067791 139774417409792 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0047818422317505, loss=3.949666738510132
I0202 20:59:55.096772 139774434195200 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9929473400115967, loss=3.5028505325317383
I0202 21:00:41.640920 139774417409792 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.1289036273956299, loss=3.418246269226074
I0202 21:01:27.810213 139774434195200 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2843434810638428, loss=3.7238709926605225
I0202 21:02:14.233991 139774417409792 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9718157052993774, loss=3.6078808307647705
I0202 21:03:00.511079 139774434195200 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.8449167013168335, loss=5.872050762176514
I0202 21:03:46.823810 139774417409792 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.101215124130249, loss=3.422628164291382
I0202 21:04:33.218189 139774434195200 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.7585948705673218, loss=5.280716419219971
I0202 21:04:35.700356 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:04:46.180726 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 21:05:24.088855 139936116377408 spec.py:349] Evaluating on the test split.
I0202 21:05:25.691536 139936116377408 submission_runner.py:408] Time since start: 29175.84s, 	Step: 56807, 	{'train/accuracy': 0.4465624988079071, 'train/loss': 2.5229108333587646, 'validation/accuracy': 0.4216800034046173, 'validation/loss': 2.661752700805664, 'validation/num_examples': 50000, 'test/accuracy': 0.3288000226020813, 'test/loss': 3.2641468048095703, 'test/num_examples': 10000, 'score': 26084.50500845909, 'total_duration': 29175.84260368347, 'accumulated_submission_time': 26084.50500845909, 'accumulated_eval_time': 3086.3352172374725, 'accumulated_logging_time': 2.0175254344940186}
I0202 21:05:25.715250 139774417409792 logging_writer.py:48] [56807] accumulated_eval_time=3086.335217, accumulated_logging_time=2.017525, accumulated_submission_time=26084.505008, global_step=56807, preemption_count=0, score=26084.505008, test/accuracy=0.328800, test/loss=3.264147, test/num_examples=10000, total_duration=29175.842604, train/accuracy=0.446562, train/loss=2.522911, validation/accuracy=0.421680, validation/loss=2.661753, validation/num_examples=50000
I0202 21:06:04.658682 139774434195200 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7723488807678223, loss=5.236701488494873
I0202 21:06:50.500964 139774417409792 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.6744967103004456, loss=5.28227424621582
I0202 21:07:36.618991 139774434195200 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.833109438419342, loss=5.2586259841918945
I0202 21:08:22.584543 139774417409792 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.7692938446998596, loss=5.1776885986328125
I0202 21:09:08.772754 139774434195200 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1738358736038208, loss=4.238490104675293
I0202 21:09:54.933558 139774417409792 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.0745049715042114, loss=3.4521307945251465
I0202 21:10:41.158794 139774434195200 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.9165003299713135, loss=3.661773681640625
I0202 21:11:27.757532 139774417409792 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.8850001096725464, loss=4.370293140411377
I0202 21:12:13.870916 139774434195200 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.9838125109672546, loss=3.5403382778167725
I0202 21:12:25.973316 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:12:36.704826 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 21:13:13.092596 139936116377408 spec.py:349] Evaluating on the test split.
I0202 21:13:14.698173 139936116377408 submission_runner.py:408] Time since start: 29644.85s, 	Step: 57728, 	{'train/accuracy': 0.45119139552116394, 'train/loss': 2.4399993419647217, 'validation/accuracy': 0.42455998063087463, 'validation/loss': 2.5960423946380615, 'validation/num_examples': 50000, 'test/accuracy': 0.33420002460479736, 'test/loss': 3.2178361415863037, 'test/num_examples': 10000, 'score': 26504.705290555954, 'total_duration': 29644.84923171997, 'accumulated_submission_time': 26504.705290555954, 'accumulated_eval_time': 3135.060056447983, 'accumulated_logging_time': 2.0510499477386475}
I0202 21:13:14.721875 139774417409792 logging_writer.py:48] [57728] accumulated_eval_time=3135.060056, accumulated_logging_time=2.051050, accumulated_submission_time=26504.705291, global_step=57728, preemption_count=0, score=26504.705291, test/accuracy=0.334200, test/loss=3.217836, test/num_examples=10000, total_duration=29644.849232, train/accuracy=0.451191, train/loss=2.439999, validation/accuracy=0.424560, validation/loss=2.596042, validation/num_examples=50000
I0202 21:13:44.129512 139774434195200 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.9428274035453796, loss=4.004897117614746
I0202 21:14:30.039947 139774417409792 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0292696952819824, loss=3.62359356880188
I0202 21:15:16.476869 139774434195200 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.3167396783828735, loss=3.601513147354126
I0202 21:16:02.900710 139774417409792 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.9983832836151123, loss=4.849958896636963
I0202 21:16:49.185888 139774434195200 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.7214663028717041, loss=4.986674785614014
I0202 21:17:35.657156 139774417409792 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.23199462890625, loss=3.450868844985962
I0202 21:18:22.029249 139774434195200 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.1693170070648193, loss=3.54709529876709
I0202 21:19:08.493308 139774417409792 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8453194499015808, loss=4.441425800323486
I0202 21:19:54.867310 139774434195200 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.0975555181503296, loss=3.846308708190918
I0202 21:20:14.984113 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:20:25.860924 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 21:21:03.935278 139936116377408 spec.py:349] Evaluating on the test split.
I0202 21:21:05.544595 139936116377408 submission_runner.py:408] Time since start: 30115.70s, 	Step: 58645, 	{'train/accuracy': 0.47408202290534973, 'train/loss': 2.372753143310547, 'validation/accuracy': 0.42541998624801636, 'validation/loss': 2.6239047050476074, 'validation/num_examples': 50000, 'test/accuracy': 0.33310002088546753, 'test/loss': 3.236710786819458, 'test/num_examples': 10000, 'score': 26924.908395767212, 'total_duration': 30115.69566130638, 'accumulated_submission_time': 26924.908395767212, 'accumulated_eval_time': 3185.6205384731293, 'accumulated_logging_time': 2.085714817047119}
I0202 21:21:05.571371 139774417409792 logging_writer.py:48] [58645] accumulated_eval_time=3185.620538, accumulated_logging_time=2.085715, accumulated_submission_time=26924.908396, global_step=58645, preemption_count=0, score=26924.908396, test/accuracy=0.333100, test/loss=3.236711, test/num_examples=10000, total_duration=30115.695661, train/accuracy=0.474082, train/loss=2.372753, validation/accuracy=0.425420, validation/loss=2.623905, validation/num_examples=50000
I0202 21:21:27.842790 139774434195200 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2048207521438599, loss=3.6426291465759277
I0202 21:22:13.667528 139774417409792 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1154327392578125, loss=3.5566999912261963
I0202 21:23:00.451053 139774434195200 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7813885807991028, loss=4.763134956359863
I0202 21:23:46.878525 139774417409792 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.0615708827972412, loss=3.447110652923584
I0202 21:24:33.378219 139774434195200 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.1912751197814941, loss=4.131692886352539
I0202 21:25:20.008557 139774417409792 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1537808179855347, loss=3.475794553756714
I0202 21:26:06.722141 139774434195200 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.7342795133590698, loss=5.761666297912598
I0202 21:26:52.816623 139774417409792 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.7277457118034363, loss=5.888978004455566
I0202 21:27:39.279268 139774434195200 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.010478138923645, loss=5.8270721435546875
I0202 21:28:05.605260 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:28:16.026200 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 21:28:53.949265 139936116377408 spec.py:349] Evaluating on the test split.
I0202 21:28:55.565155 139936116377408 submission_runner.py:408] Time since start: 30585.72s, 	Step: 59558, 	{'train/accuracy': 0.4577734172344208, 'train/loss': 2.429400682449341, 'validation/accuracy': 0.42837998270988464, 'validation/loss': 2.5803024768829346, 'validation/num_examples': 50000, 'test/accuracy': 0.3296000063419342, 'test/loss': 3.227254867553711, 'test/num_examples': 10000, 'score': 27344.886063098907, 'total_duration': 30585.716195106506, 'accumulated_submission_time': 27344.886063098907, 'accumulated_eval_time': 3235.580410003662, 'accumulated_logging_time': 2.121338367462158}
I0202 21:28:55.595548 139774417409792 logging_writer.py:48] [59558] accumulated_eval_time=3235.580410, accumulated_logging_time=2.121338, accumulated_submission_time=27344.886063, global_step=59558, preemption_count=0, score=27344.886063, test/accuracy=0.329600, test/loss=3.227255, test/num_examples=10000, total_duration=30585.716195, train/accuracy=0.457773, train/loss=2.429401, validation/accuracy=0.428380, validation/loss=2.580302, validation/num_examples=50000
I0202 21:29:12.709862 139774434195200 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.0729278326034546, loss=3.4745330810546875
I0202 21:29:56.758540 139774417409792 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7152780890464783, loss=5.522533416748047
I0202 21:30:43.043915 139774434195200 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.1819416284561157, loss=3.7963695526123047
I0202 21:31:29.831934 139774417409792 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.0713380575180054, loss=3.4518795013427734
I0202 21:32:16.301808 139774434195200 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0234169960021973, loss=4.270244598388672
I0202 21:33:02.969788 139774417409792 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.020098090171814, loss=3.438638210296631
I0202 21:33:49.363392 139774434195200 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.98030024766922, loss=3.4543328285217285
I0202 21:34:35.692436 139774417409792 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0198595523834229, loss=4.741507053375244
I0202 21:35:22.154805 139774434195200 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0701276063919067, loss=3.396101474761963
I0202 21:35:55.631552 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:36:06.323072 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 21:36:42.530016 139936116377408 spec.py:349] Evaluating on the test split.
I0202 21:36:44.139393 139936116377408 submission_runner.py:408] Time since start: 31054.29s, 	Step: 60474, 	{'train/accuracy': 0.46486327052116394, 'train/loss': 2.3890140056610107, 'validation/accuracy': 0.4313199818134308, 'validation/loss': 2.562873363494873, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.1996138095855713, 'test/num_examples': 10000, 'score': 27764.862775564194, 'total_duration': 31054.29045343399, 'accumulated_submission_time': 27764.862775564194, 'accumulated_eval_time': 3284.088232278824, 'accumulated_logging_time': 2.162022352218628}
I0202 21:36:44.167103 139774417409792 logging_writer.py:48] [60474] accumulated_eval_time=3284.088232, accumulated_logging_time=2.162022, accumulated_submission_time=27764.862776, global_step=60474, preemption_count=0, score=27764.862776, test/accuracy=0.331500, test/loss=3.199614, test/num_examples=10000, total_duration=31054.290453, train/accuracy=0.464863, train/loss=2.389014, validation/accuracy=0.431320, validation/loss=2.562873, validation/num_examples=50000
I0202 21:36:55.111402 139774434195200 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.3301504850387573, loss=3.668910264968872
I0202 21:37:38.417222 139774417409792 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.056984305381775, loss=3.8455777168273926
I0202 21:38:24.518000 139774434195200 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.0138945579528809, loss=3.557920217514038
I0202 21:39:10.984412 139774417409792 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.2916760444641113, loss=3.5554347038269043
I0202 21:39:56.900542 139774434195200 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.9903095364570618, loss=4.460951805114746
I0202 21:40:43.206080 139774417409792 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.8799930214881897, loss=5.1397705078125
I0202 21:41:29.610781 139774434195200 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0328471660614014, loss=3.642575979232788
I0202 21:42:16.039729 139774417409792 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.8867705464363098, loss=4.828836441040039
I0202 21:43:02.614070 139774434195200 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.9841761589050293, loss=3.3812124729156494
I0202 21:43:44.426492 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:43:55.148220 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 21:44:31.798211 139936116377408 spec.py:349] Evaluating on the test split.
I0202 21:44:33.408319 139936116377408 submission_runner.py:408] Time since start: 31523.56s, 	Step: 61391, 	{'train/accuracy': 0.46708983182907104, 'train/loss': 2.3804032802581787, 'validation/accuracy': 0.42931997776031494, 'validation/loss': 2.585036277770996, 'validation/num_examples': 50000, 'test/accuracy': 0.3352000117301941, 'test/loss': 3.2316999435424805, 'test/num_examples': 10000, 'score': 28185.063593387604, 'total_duration': 31523.559364318848, 'accumulated_submission_time': 28185.063593387604, 'accumulated_eval_time': 3333.0700438022614, 'accumulated_logging_time': 2.200597047805786}
I0202 21:44:33.439110 139774417409792 logging_writer.py:48] [61391] accumulated_eval_time=3333.070044, accumulated_logging_time=2.200597, accumulated_submission_time=28185.063593, global_step=61391, preemption_count=0, score=28185.063593, test/accuracy=0.335200, test/loss=3.231700, test/num_examples=10000, total_duration=31523.559364, train/accuracy=0.467090, train/loss=2.380403, validation/accuracy=0.429320, validation/loss=2.585036, validation/num_examples=50000
I0202 21:44:37.416693 139774434195200 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.172654390335083, loss=3.4843976497650146
I0202 21:45:19.889464 139774417409792 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9426984190940857, loss=3.4121580123901367
I0202 21:46:06.085426 139774434195200 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1353535652160645, loss=3.7285139560699463
I0202 21:46:52.258838 139774417409792 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.9266849756240845, loss=4.7019944190979
I0202 21:47:38.406152 139774434195200 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1157960891723633, loss=3.5241830348968506
I0202 21:48:24.759831 139774417409792 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0040597915649414, loss=3.4015684127807617
I0202 21:49:10.911224 139774434195200 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0683132410049438, loss=3.5675740242004395
I0202 21:49:56.727712 139774417409792 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.9576160907745361, loss=3.5483715534210205
I0202 21:50:43.100714 139774434195200 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0786787271499634, loss=3.472425937652588
I0202 21:51:29.535192 139774417409792 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.9419004321098328, loss=5.820980072021484
I0202 21:51:33.416830 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:51:44.267941 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 21:52:22.128530 139936116377408 spec.py:349] Evaluating on the test split.
I0202 21:52:23.735941 139936116377408 submission_runner.py:408] Time since start: 31993.89s, 	Step: 62310, 	{'train/accuracy': 0.46433591842651367, 'train/loss': 2.393752336502075, 'validation/accuracy': 0.43361997604370117, 'validation/loss': 2.5538320541381836, 'validation/num_examples': 50000, 'test/accuracy': 0.3335000276565552, 'test/loss': 3.2018165588378906, 'test/num_examples': 10000, 'score': 28604.98014640808, 'total_duration': 31993.887003183365, 'accumulated_submission_time': 28604.98014640808, 'accumulated_eval_time': 3383.3891365528107, 'accumulated_logging_time': 2.243925094604492}
I0202 21:52:23.763906 139774434195200 logging_writer.py:48] [62310] accumulated_eval_time=3383.389137, accumulated_logging_time=2.243925, accumulated_submission_time=28604.980146, global_step=62310, preemption_count=0, score=28604.980146, test/accuracy=0.333500, test/loss=3.201817, test/num_examples=10000, total_duration=31993.887003, train/accuracy=0.464336, train/loss=2.393752, validation/accuracy=0.433620, validation/loss=2.553832, validation/num_examples=50000
I0202 21:53:01.448926 139774417409792 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.1419192552566528, loss=3.5667672157287598
I0202 21:53:47.563773 139774434195200 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.0354493856430054, loss=4.075564384460449
I0202 21:54:34.561824 139774417409792 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.051620602607727, loss=3.5994057655334473
I0202 21:55:20.843967 139774434195200 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.9322253465652466, loss=4.836580276489258
I0202 21:56:07.116702 139774417409792 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.9302377700805664, loss=5.615428924560547
I0202 21:56:53.331891 139774434195200 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1161941289901733, loss=3.375417709350586
I0202 21:57:39.660605 139774417409792 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.039476752281189, loss=3.5142595767974854
I0202 21:58:26.059797 139774434195200 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.0676332712173462, loss=3.427262783050537
I0202 21:59:12.184920 139774417409792 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.7814021706581116, loss=5.017117500305176
I0202 21:59:24.001603 139936116377408 spec.py:321] Evaluating on the training split.
I0202 21:59:35.647810 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:00:11.978975 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:00:13.597527 139936116377408 submission_runner.py:408] Time since start: 32463.75s, 	Step: 63227, 	{'train/accuracy': 0.4670117199420929, 'train/loss': 2.3691554069519043, 'validation/accuracy': 0.4369799792766571, 'validation/loss': 2.5394680500030518, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.135342836380005, 'test/num_examples': 10000, 'score': 29025.158362150192, 'total_duration': 32463.748566627502, 'accumulated_submission_time': 29025.158362150192, 'accumulated_eval_time': 3432.9850244522095, 'accumulated_logging_time': 2.2831294536590576}
I0202 22:00:13.629119 139774434195200 logging_writer.py:48] [63227] accumulated_eval_time=3432.985024, accumulated_logging_time=2.283129, accumulated_submission_time=29025.158362, global_step=63227, preemption_count=0, score=29025.158362, test/accuracy=0.342700, test/loss=3.135343, test/num_examples=10000, total_duration=32463.748567, train/accuracy=0.467012, train/loss=2.369155, validation/accuracy=0.436980, validation/loss=2.539468, validation/num_examples=50000
I0202 22:00:43.405668 139774417409792 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.9625909924507141, loss=3.8334856033325195
I0202 22:01:29.379731 139774434195200 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.8355870246887207, loss=5.751251220703125
I0202 22:02:16.172385 139774417409792 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.969696044921875, loss=4.299367904663086
I0202 22:03:02.581592 139774434195200 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.7601577043533325, loss=5.126893997192383
I0202 22:03:48.537016 139774417409792 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0148732662200928, loss=3.4227395057678223
I0202 22:04:35.193158 139774434195200 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.0299090147018433, loss=5.108992099761963
I0202 22:05:21.459097 139774417409792 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.9713529348373413, loss=5.285234451293945
I0202 22:06:08.033737 139774434195200 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.9952103495597839, loss=3.41404128074646
I0202 22:06:53.965815 139774417409792 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0645346641540527, loss=3.736041784286499
I0202 22:07:13.803761 139936116377408 spec.py:321] Evaluating on the training split.
I0202 22:07:24.252650 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:08:02.285483 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:08:03.897643 139936116377408 submission_runner.py:408] Time since start: 32934.05s, 	Step: 64144, 	{'train/accuracy': 0.4650976359844208, 'train/loss': 2.3967294692993164, 'validation/accuracy': 0.4300599992275238, 'validation/loss': 2.5977704524993896, 'validation/num_examples': 50000, 'test/accuracy': 0.3273000121116638, 'test/loss': 3.223234176635742, 'test/num_examples': 10000, 'score': 29445.27424812317, 'total_duration': 32934.04869699478, 'accumulated_submission_time': 29445.27424812317, 'accumulated_eval_time': 3483.0788888931274, 'accumulated_logging_time': 2.325901746749878}
I0202 22:08:03.929680 139774434195200 logging_writer.py:48] [64144] accumulated_eval_time=3483.078889, accumulated_logging_time=2.325902, accumulated_submission_time=29445.274248, global_step=64144, preemption_count=0, score=29445.274248, test/accuracy=0.327300, test/loss=3.223234, test/num_examples=10000, total_duration=32934.048697, train/accuracy=0.465098, train/loss=2.396729, validation/accuracy=0.430060, validation/loss=2.597770, validation/num_examples=50000
I0202 22:08:26.602990 139774417409792 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.8235905766487122, loss=5.678535461425781
I0202 22:09:12.086189 139774434195200 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1026084423065186, loss=3.322058916091919
I0202 22:09:58.158520 139774417409792 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.7303030490875244, loss=5.533782482147217
I0202 22:10:44.269035 139774434195200 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1819003820419312, loss=3.5145113468170166
I0202 22:11:30.463429 139774417409792 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.976273775100708, loss=3.7165558338165283
I0202 22:12:16.933674 139774434195200 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1495009660720825, loss=3.4374752044677734
I0202 22:13:03.331695 139774417409792 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.150794267654419, loss=3.7896552085876465
I0202 22:13:49.554311 139774434195200 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.9192911386489868, loss=4.288971900939941
I0202 22:14:35.931243 139774417409792 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.8135850429534912, loss=5.434525966644287
I0202 22:15:04.167381 139936116377408 spec.py:321] Evaluating on the training split.
I0202 22:15:14.834710 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:15:53.329785 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:15:54.931651 139936116377408 submission_runner.py:408] Time since start: 33405.08s, 	Step: 65062, 	{'train/accuracy': 0.4678320288658142, 'train/loss': 2.389435291290283, 'validation/accuracy': 0.4369399845600128, 'validation/loss': 2.5648996829986572, 'validation/num_examples': 50000, 'test/accuracy': 0.3362000286579132, 'test/loss': 3.1964430809020996, 'test/num_examples': 10000, 'score': 29865.45224094391, 'total_duration': 33405.08270573616, 'accumulated_submission_time': 29865.45224094391, 'accumulated_eval_time': 3533.843167066574, 'accumulated_logging_time': 2.3687186241149902}
I0202 22:15:54.959939 139774434195200 logging_writer.py:48] [65062] accumulated_eval_time=3533.843167, accumulated_logging_time=2.368719, accumulated_submission_time=29865.452241, global_step=65062, preemption_count=0, score=29865.452241, test/accuracy=0.336200, test/loss=3.196443, test/num_examples=10000, total_duration=33405.082706, train/accuracy=0.467832, train/loss=2.389435, validation/accuracy=0.436940, validation/loss=2.564900, validation/num_examples=50000
I0202 22:16:10.468579 139774417409792 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.9151859283447266, loss=4.372520446777344
I0202 22:16:54.396193 139774434195200 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.026764988899231, loss=3.424150228500366
I0202 22:17:40.674028 139774417409792 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.8568124771118164, loss=5.771650791168213
I0202 22:18:27.037156 139774434195200 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.0276119709014893, loss=3.469595193862915
I0202 22:19:13.210842 139774417409792 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0473301410675049, loss=3.553882122039795
I0202 22:19:59.246744 139774434195200 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.9869903922080994, loss=3.6937880516052246
I0202 22:20:45.427450 139774417409792 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.0024504661560059, loss=3.416926383972168
I0202 22:21:31.690386 139774434195200 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.5408307313919067, loss=3.563354969024658
I0202 22:22:18.143778 139774417409792 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.0014550685882568, loss=3.9543495178222656
I0202 22:22:54.975016 139936116377408 spec.py:321] Evaluating on the training split.
I0202 22:23:05.638309 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:23:44.386424 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:23:45.995508 139936116377408 submission_runner.py:408] Time since start: 33876.15s, 	Step: 65982, 	{'train/accuracy': 0.4744921624660492, 'train/loss': 2.419340133666992, 'validation/accuracy': 0.43595999479293823, 'validation/loss': 2.5876500606536865, 'validation/num_examples': 50000, 'test/accuracy': 0.3450000286102295, 'test/loss': 3.2141637802124023, 'test/num_examples': 10000, 'score': 30285.408737182617, 'total_duration': 33876.14655208588, 'accumulated_submission_time': 30285.408737182617, 'accumulated_eval_time': 3584.863637447357, 'accumulated_logging_time': 2.4079158306121826}
I0202 22:23:46.028891 139774434195200 logging_writer.py:48] [65982] accumulated_eval_time=3584.863637, accumulated_logging_time=2.407916, accumulated_submission_time=30285.408737, global_step=65982, preemption_count=0, score=30285.408737, test/accuracy=0.345000, test/loss=3.214164, test/num_examples=10000, total_duration=33876.146552, train/accuracy=0.474492, train/loss=2.419340, validation/accuracy=0.435960, validation/loss=2.587650, validation/num_examples=50000
I0202 22:23:53.578762 139774417409792 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.9955453276634216, loss=3.234168767929077
I0202 22:24:36.557134 139774434195200 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.9474114775657654, loss=5.225495338439941
I0202 22:25:22.613116 139774417409792 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0315953493118286, loss=3.614755630493164
I0202 22:26:08.952463 139774434195200 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3364332914352417, loss=3.39094877243042
I0202 22:26:55.159435 139774417409792 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.9534170627593994, loss=3.7407805919647217
I0202 22:27:41.494250 139774434195200 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9942095279693604, loss=3.3522801399230957
I0202 22:28:27.749652 139774417409792 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.9992720484733582, loss=3.4635181427001953
I0202 22:29:13.886282 139774434195200 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.9254899621009827, loss=5.717332363128662
I0202 22:30:00.011288 139774417409792 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.8721764087677002, loss=3.814887046813965
I0202 22:30:46.251671 139774434195200 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.9407872557640076, loss=4.873849391937256
I0202 22:30:46.267626 139936116377408 spec.py:321] Evaluating on the training split.
I0202 22:30:56.821179 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:31:35.179452 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:31:36.799419 139936116377408 submission_runner.py:408] Time since start: 34346.95s, 	Step: 66901, 	{'train/accuracy': 0.47376951575279236, 'train/loss': 2.3729798793792725, 'validation/accuracy': 0.44200000166893005, 'validation/loss': 2.548090696334839, 'validation/num_examples': 50000, 'test/accuracy': 0.340800017118454, 'test/loss': 3.17934513092041, 'test/num_examples': 10000, 'score': 30705.5859375, 'total_duration': 34346.95048165321, 'accumulated_submission_time': 30705.5859375, 'accumulated_eval_time': 3635.3954322338104, 'accumulated_logging_time': 2.4538557529449463}
I0202 22:31:36.824569 139774417409792 logging_writer.py:48] [66901] accumulated_eval_time=3635.395432, accumulated_logging_time=2.453856, accumulated_submission_time=30705.585938, global_step=66901, preemption_count=0, score=30705.585938, test/accuracy=0.340800, test/loss=3.179345, test/num_examples=10000, total_duration=34346.950482, train/accuracy=0.473770, train/loss=2.372980, validation/accuracy=0.442000, validation/loss=2.548091, validation/num_examples=50000
I0202 22:32:18.742544 139774434195200 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.9274898767471313, loss=5.584845542907715
I0202 22:33:04.547109 139774417409792 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.147262454032898, loss=3.367541790008545
I0202 22:33:51.053223 139774434195200 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.8101528286933899, loss=4.6389851570129395
I0202 22:34:37.136822 139774417409792 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.045674443244934, loss=4.844603061676025
I0202 22:35:23.350182 139774434195200 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.0879735946655273, loss=3.7102010250091553
I0202 22:36:09.623390 139774417409792 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1316478252410889, loss=3.4205212593078613
I0202 22:36:56.072677 139774434195200 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9577944874763489, loss=3.474280595779419
I0202 22:37:42.502430 139774417409792 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1444672346115112, loss=3.5168673992156982
I0202 22:38:28.955643 139774434195200 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.098583459854126, loss=3.3113396167755127
I0202 22:38:37.053577 139936116377408 spec.py:321] Evaluating on the training split.
I0202 22:38:47.401128 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:39:22.600511 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:39:24.209616 139936116377408 submission_runner.py:408] Time since start: 34814.36s, 	Step: 67819, 	{'train/accuracy': 0.5081835985183716, 'train/loss': 2.1930553913116455, 'validation/accuracy': 0.4444599747657776, 'validation/loss': 2.5196900367736816, 'validation/num_examples': 50000, 'test/accuracy': 0.3449000120162964, 'test/loss': 3.149144172668457, 'test/num_examples': 10000, 'score': 31125.757450580597, 'total_duration': 34814.36068201065, 'accumulated_submission_time': 31125.757450580597, 'accumulated_eval_time': 3682.5514616966248, 'accumulated_logging_time': 2.488304853439331}
I0202 22:39:24.234955 139774417409792 logging_writer.py:48] [67819] accumulated_eval_time=3682.551462, accumulated_logging_time=2.488305, accumulated_submission_time=31125.757451, global_step=67819, preemption_count=0, score=31125.757451, test/accuracy=0.344900, test/loss=3.149144, test/num_examples=10000, total_duration=34814.360682, train/accuracy=0.508184, train/loss=2.193055, validation/accuracy=0.444460, validation/loss=2.519690, validation/num_examples=50000
I0202 22:39:57.807897 139774434195200 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.8976304531097412, loss=5.7572736740112305
I0202 22:40:43.636076 139774417409792 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.9915775656700134, loss=4.050591945648193
I0202 22:41:30.332203 139774434195200 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.0223599672317505, loss=3.9193804264068604
I0202 22:42:16.593610 139774417409792 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9468494057655334, loss=3.7839910984039307
I0202 22:43:02.884334 139774434195200 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.967497706413269, loss=3.647657632827759
I0202 22:43:49.050127 139774417409792 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.947945773601532, loss=4.391161918640137
I0202 22:44:35.215692 139774434195200 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.019044041633606, loss=3.4398131370544434
I0202 22:45:21.426946 139774417409792 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.9955613613128662, loss=3.3388071060180664
I0202 22:46:07.838329 139774434195200 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.0194507837295532, loss=3.2513999938964844
I0202 22:46:24.517638 139936116377408 spec.py:321] Evaluating on the training split.
I0202 22:46:35.060375 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:47:11.804590 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:47:13.412683 139936116377408 submission_runner.py:408] Time since start: 35283.56s, 	Step: 68738, 	{'train/accuracy': 0.47441405057907104, 'train/loss': 2.358579397201538, 'validation/accuracy': 0.44589999318122864, 'validation/loss': 2.502725124359131, 'validation/num_examples': 50000, 'test/accuracy': 0.3493000268936157, 'test/loss': 3.140624761581421, 'test/num_examples': 10000, 'score': 31545.980580091476, 'total_duration': 35283.56374049187, 'accumulated_submission_time': 31545.980580091476, 'accumulated_eval_time': 3731.446517467499, 'accumulated_logging_time': 2.525161027908325}
I0202 22:47:13.443436 139774417409792 logging_writer.py:48] [68738] accumulated_eval_time=3731.446517, accumulated_logging_time=2.525161, accumulated_submission_time=31545.980580, global_step=68738, preemption_count=0, score=31545.980580, test/accuracy=0.349300, test/loss=3.140625, test/num_examples=10000, total_duration=35283.563740, train/accuracy=0.474414, train/loss=2.358579, validation/accuracy=0.445900, validation/loss=2.502725, validation/num_examples=50000
I0202 22:47:38.496585 139774434195200 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.8169252276420593, loss=5.603935241699219
I0202 22:48:24.349409 139774417409792 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.003971815109253, loss=3.411597490310669
I0202 22:49:10.664141 139774434195200 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1054985523223877, loss=3.2617509365081787
I0202 22:49:56.737200 139774417409792 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.0886296033859253, loss=3.3213205337524414
I0202 22:50:43.195902 139774434195200 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0365411043167114, loss=3.2845940589904785
I0202 22:51:29.653356 139774417409792 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.1785435676574707, loss=3.442986249923706
I0202 22:52:16.234013 139774434195200 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.0728089809417725, loss=3.4039926528930664
I0202 22:53:02.620249 139774417409792 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.187492847442627, loss=3.5902099609375
I0202 22:53:49.009171 139774434195200 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1358697414398193, loss=3.368616819381714
I0202 22:54:13.732702 139936116377408 spec.py:321] Evaluating on the training split.
I0202 22:54:24.490721 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 22:55:02.177384 139936116377408 spec.py:349] Evaluating on the test split.
I0202 22:55:03.819086 139936116377408 submission_runner.py:408] Time since start: 35753.97s, 	Step: 69655, 	{'train/accuracy': 0.48808592557907104, 'train/loss': 2.284952402114868, 'validation/accuracy': 0.45097997784614563, 'validation/loss': 2.4512698650360107, 'validation/num_examples': 50000, 'test/accuracy': 0.35630002617836, 'test/loss': 3.0855069160461426, 'test/num_examples': 10000, 'score': 31966.211909532547, 'total_duration': 35753.97013711929, 'accumulated_submission_time': 31966.211909532547, 'accumulated_eval_time': 3781.5328879356384, 'accumulated_logging_time': 2.565286159515381}
I0202 22:55:03.848985 139774417409792 logging_writer.py:48] [69655] accumulated_eval_time=3781.532888, accumulated_logging_time=2.565286, accumulated_submission_time=31966.211910, global_step=69655, preemption_count=0, score=31966.211910, test/accuracy=0.356300, test/loss=3.085507, test/num_examples=10000, total_duration=35753.970137, train/accuracy=0.488086, train/loss=2.284952, validation/accuracy=0.450980, validation/loss=2.451270, validation/num_examples=50000
I0202 22:55:22.127783 139774434195200 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.976041853427887, loss=3.338550567626953
I0202 22:56:06.627863 139774417409792 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.9275117516517639, loss=4.354225158691406
I0202 22:56:52.698587 139774434195200 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.0929468870162964, loss=3.588340997695923
I0202 22:57:39.226155 139774417409792 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.520551323890686, loss=3.385962963104248
I0202 22:58:25.708261 139774434195200 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1502838134765625, loss=3.4716415405273438
I0202 22:59:12.196135 139774417409792 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.053309679031372, loss=3.7408478260040283
I0202 22:59:58.592456 139774434195200 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.8901036381721497, loss=4.4139323234558105
I0202 23:00:45.120897 139774417409792 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1087946891784668, loss=3.5132946968078613
I0202 23:01:31.568686 139774434195200 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.2098921537399292, loss=3.303696632385254
I0202 23:02:04.037283 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:02:14.704783 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:02:50.729161 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:02:52.332793 139936116377408 submission_runner.py:408] Time since start: 36222.48s, 	Step: 70571, 	{'train/accuracy': 0.5066015720367432, 'train/loss': 2.156141519546509, 'validation/accuracy': 0.45743998885154724, 'validation/loss': 2.423490285873413, 'validation/num_examples': 50000, 'test/accuracy': 0.36260002851486206, 'test/loss': 3.076205253601074, 'test/num_examples': 10000, 'score': 32386.341804027557, 'total_duration': 36222.48383450508, 'accumulated_submission_time': 32386.341804027557, 'accumulated_eval_time': 3829.8283665180206, 'accumulated_logging_time': 2.6060354709625244}
I0202 23:02:52.370871 139774417409792 logging_writer.py:48] [70571] accumulated_eval_time=3829.828367, accumulated_logging_time=2.606035, accumulated_submission_time=32386.341804, global_step=70571, preemption_count=0, score=32386.341804, test/accuracy=0.362600, test/loss=3.076205, test/num_examples=10000, total_duration=36222.483835, train/accuracy=0.506602, train/loss=2.156142, validation/accuracy=0.457440, validation/loss=2.423490, validation/num_examples=50000
I0202 23:03:04.309174 139774434195200 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0256675481796265, loss=3.4704971313476562
I0202 23:03:48.133558 139774417409792 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1288920640945435, loss=3.425556182861328
I0202 23:04:34.326717 139774434195200 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.107556939125061, loss=3.285374879837036
I0202 23:05:20.824140 139774417409792 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8943685293197632, loss=4.6543779373168945
I0202 23:06:07.025939 139774434195200 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.9787408709526062, loss=5.780637264251709
I0202 23:06:53.215049 139774417409792 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.1737251281738281, loss=3.3855667114257812
I0202 23:07:39.699711 139774434195200 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.2731515169143677, loss=3.560253143310547
I0202 23:08:26.429472 139774417409792 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.220608115196228, loss=3.520681381225586
I0202 23:09:12.657936 139774434195200 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.937969982624054, loss=4.402555465698242
I0202 23:09:52.646910 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:10:03.416731 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:10:40.444374 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:10:42.043999 139936116377408 submission_runner.py:408] Time since start: 36692.20s, 	Step: 71488, 	{'train/accuracy': 0.4865429699420929, 'train/loss': 2.26175856590271, 'validation/accuracy': 0.45819997787475586, 'validation/loss': 2.418018102645874, 'validation/num_examples': 50000, 'test/accuracy': 0.35910001397132874, 'test/loss': 3.0786125659942627, 'test/num_examples': 10000, 'score': 32806.55907249451, 'total_duration': 36692.19506430626, 'accumulated_submission_time': 32806.55907249451, 'accumulated_eval_time': 3879.22545838356, 'accumulated_logging_time': 2.655019521713257}
I0202 23:10:42.071256 139774417409792 logging_writer.py:48] [71488] accumulated_eval_time=3879.225458, accumulated_logging_time=2.655020, accumulated_submission_time=32806.559072, global_step=71488, preemption_count=0, score=32806.559072, test/accuracy=0.359100, test/loss=3.078613, test/num_examples=10000, total_duration=36692.195064, train/accuracy=0.486543, train/loss=2.261759, validation/accuracy=0.458200, validation/loss=2.418018, validation/num_examples=50000
I0202 23:10:47.244872 139774434195200 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.8734413385391235, loss=3.622671604156494
I0202 23:11:29.930813 139774417409792 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.9880126118659973, loss=4.353975296020508
I0202 23:12:16.013123 139774434195200 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.061719298362732, loss=3.2334389686584473
I0202 23:13:02.354609 139774417409792 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.0126988887786865, loss=5.088786602020264
I0202 23:13:48.647150 139774434195200 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.4851303100585938, loss=3.2981204986572266
I0202 23:14:34.772722 139774417409792 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.9722480177879333, loss=3.6495518684387207
I0202 23:15:21.179057 139774434195200 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.813896119594574, loss=5.547950744628906
I0202 23:16:07.555098 139774417409792 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.2610455751419067, loss=3.3637890815734863
I0202 23:16:53.857849 139774434195200 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.0007214546203613, loss=3.957867383956909
I0202 23:17:40.355041 139774417409792 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.8676602840423584, loss=4.970248222351074
I0202 23:17:42.295411 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:17:52.968155 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:18:31.065320 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:18:32.664377 139936116377408 submission_runner.py:408] Time since start: 37162.82s, 	Step: 72406, 	{'train/accuracy': 0.4870898425579071, 'train/loss': 2.2414968013763428, 'validation/accuracy': 0.4551199972629547, 'validation/loss': 2.416773796081543, 'validation/num_examples': 50000, 'test/accuracy': 0.35440000891685486, 'test/loss': 3.063166618347168, 'test/num_examples': 10000, 'score': 33226.725727796555, 'total_duration': 37162.81544137001, 'accumulated_submission_time': 33226.725727796555, 'accumulated_eval_time': 3929.594414949417, 'accumulated_logging_time': 2.6921746730804443}
I0202 23:18:32.690157 139774434195200 logging_writer.py:48] [72406] accumulated_eval_time=3929.594415, accumulated_logging_time=2.692175, accumulated_submission_time=33226.725728, global_step=72406, preemption_count=0, score=33226.725728, test/accuracy=0.354400, test/loss=3.063167, test/num_examples=10000, total_duration=37162.815441, train/accuracy=0.487090, train/loss=2.241497, validation/accuracy=0.455120, validation/loss=2.416774, validation/num_examples=50000
I0202 23:19:12.239953 139774417409792 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.009521245956421, loss=3.263387680053711
I0202 23:19:58.319922 139774434195200 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.973020613193512, loss=4.231147766113281
I0202 23:20:44.918469 139774417409792 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.0410079956054688, loss=3.5261449813842773
I0202 23:21:31.502532 139774434195200 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.9326080083847046, loss=5.268604278564453
I0202 23:22:17.928771 139774417409792 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.8053774833679199, loss=5.695488929748535
I0202 23:23:04.257555 139774434195200 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.9221475124359131, loss=4.675357341766357
I0202 23:23:50.784406 139774417409792 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.3364472389221191, loss=3.3502426147460938
I0202 23:24:37.163236 139774434195200 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.8273031711578369, loss=5.742820739746094
I0202 23:25:23.525203 139774417409792 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.1159718036651611, loss=3.4904112815856934
I0202 23:25:33.022524 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:25:43.345311 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:26:22.029636 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:26:23.633418 139936116377408 submission_runner.py:408] Time since start: 37633.78s, 	Step: 73322, 	{'train/accuracy': 0.48949217796325684, 'train/loss': 2.2488605976104736, 'validation/accuracy': 0.45325997471809387, 'validation/loss': 2.4476559162139893, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.1173243522644043, 'test/num_examples': 10000, 'score': 33647.000801324844, 'total_duration': 37633.78448009491, 'accumulated_submission_time': 33647.000801324844, 'accumulated_eval_time': 3980.205307483673, 'accumulated_logging_time': 2.726961135864258}
I0202 23:26:23.659327 139774434195200 logging_writer.py:48] [73322] accumulated_eval_time=3980.205307, accumulated_logging_time=2.726961, accumulated_submission_time=33647.000801, global_step=73322, preemption_count=0, score=33647.000801, test/accuracy=0.347300, test/loss=3.117324, test/num_examples=10000, total_duration=37633.784480, train/accuracy=0.489492, train/loss=2.248861, validation/accuracy=0.453260, validation/loss=2.447656, validation/num_examples=50000
I0202 23:26:55.975359 139774417409792 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.1331605911254883, loss=3.4660534858703613
I0202 23:27:41.973937 139774434195200 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.1117058992385864, loss=3.426499605178833
I0202 23:28:28.294296 139774417409792 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.0571918487548828, loss=3.342888116836548
I0202 23:29:14.554845 139774434195200 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0741815567016602, loss=3.5455403327941895
I0202 23:30:00.867091 139774417409792 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3647273778915405, loss=3.358961582183838
I0202 23:30:47.390690 139774434195200 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.909431517124176, loss=5.570395469665527
I0202 23:31:33.780696 139774417409792 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.0656927824020386, loss=3.3295161724090576
I0202 23:32:20.295333 139774434195200 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.1140327453613281, loss=5.863956928253174
I0202 23:33:06.804225 139774417409792 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.2157888412475586, loss=3.4252564907073975
I0202 23:33:23.998944 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:33:34.768947 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:34:12.459727 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:34:14.062809 139936116377408 submission_runner.py:408] Time since start: 38104.21s, 	Step: 74239, 	{'train/accuracy': 0.49150389432907104, 'train/loss': 2.291461944580078, 'validation/accuracy': 0.4583199918270111, 'validation/loss': 2.455028772354126, 'validation/num_examples': 50000, 'test/accuracy': 0.3549000024795532, 'test/loss': 3.087660074234009, 'test/num_examples': 10000, 'score': 34067.28379058838, 'total_duration': 38104.21386909485, 'accumulated_submission_time': 34067.28379058838, 'accumulated_eval_time': 4030.269171476364, 'accumulated_logging_time': 2.76204776763916}
I0202 23:34:14.089646 139774434195200 logging_writer.py:48] [74239] accumulated_eval_time=4030.269171, accumulated_logging_time=2.762048, accumulated_submission_time=34067.283791, global_step=74239, preemption_count=0, score=34067.283791, test/accuracy=0.354900, test/loss=3.087660, test/num_examples=10000, total_duration=38104.213869, train/accuracy=0.491504, train/loss=2.291462, validation/accuracy=0.458320, validation/loss=2.455029, validation/num_examples=50000
I0202 23:34:38.705105 139774417409792 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.0971550941467285, loss=3.2522366046905518
I0202 23:35:24.744077 139774434195200 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.9518643617630005, loss=4.643263339996338
I0202 23:36:11.139681 139774417409792 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.0624573230743408, loss=3.2608726024627686
I0202 23:36:57.144065 139774434195200 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.976513683795929, loss=3.7127044200897217
I0202 23:37:43.284985 139774417409792 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.9789596199989319, loss=5.751251220703125
I0202 23:38:29.676027 139774434195200 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1233805418014526, loss=3.2460265159606934
I0202 23:39:15.754341 139774417409792 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.9053175449371338, loss=4.826362609863281
I0202 23:40:01.792538 139774434195200 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.8212043642997742, loss=5.360828399658203
I0202 23:40:48.382048 139774417409792 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.0218801498413086, loss=3.3398663997650146
I0202 23:41:14.372778 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:41:24.756301 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:42:03.555008 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:42:05.162081 139936116377408 submission_runner.py:408] Time since start: 38575.31s, 	Step: 75158, 	{'train/accuracy': 0.5007616877555847, 'train/loss': 2.170815944671631, 'validation/accuracy': 0.46995997428894043, 'validation/loss': 2.3507161140441895, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 3.005544662475586, 'test/num_examples': 10000, 'score': 34487.50742006302, 'total_duration': 38575.3131480217, 'accumulated_submission_time': 34487.50742006302, 'accumulated_eval_time': 4081.058485507965, 'accumulated_logging_time': 2.7998476028442383}
I0202 23:42:05.190575 139774434195200 logging_writer.py:48] [75158] accumulated_eval_time=4081.058486, accumulated_logging_time=2.799848, accumulated_submission_time=34487.507420, global_step=75158, preemption_count=0, score=34487.507420, test/accuracy=0.368000, test/loss=3.005545, test/num_examples=10000, total_duration=38575.313148, train/accuracy=0.500762, train/loss=2.170816, validation/accuracy=0.469960, validation/loss=2.350716, validation/num_examples=50000
I0202 23:42:22.288049 139774417409792 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1087509393692017, loss=3.2202706336975098
I0202 23:43:06.831535 139774434195200 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.354456901550293, loss=3.359071731567383
I0202 23:43:52.806930 139774417409792 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.2044039964675903, loss=3.3857316970825195
I0202 23:44:39.061290 139774434195200 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.0497660636901855, loss=3.2357451915740967
I0202 23:45:25.444501 139774417409792 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.9363752603530884, loss=3.823784828186035
I0202 23:46:11.758372 139774434195200 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.864098846912384, loss=4.4182305335998535
I0202 23:46:58.158197 139774417409792 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.0761916637420654, loss=3.433669090270996
I0202 23:47:44.502024 139774434195200 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.2978285551071167, loss=3.2371456623077393
I0202 23:48:30.876430 139774417409792 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.113060712814331, loss=3.553950786590576
I0202 23:49:05.363279 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:49:15.812235 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:49:55.024492 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:49:56.627052 139936116377408 submission_runner.py:408] Time since start: 39046.78s, 	Step: 76076, 	{'train/accuracy': 0.5032812356948853, 'train/loss': 2.190779447555542, 'validation/accuracy': 0.464819997549057, 'validation/loss': 2.4005751609802246, 'validation/num_examples': 50000, 'test/accuracy': 0.36090001463890076, 'test/loss': 3.055748462677002, 'test/num_examples': 10000, 'score': 34907.619475364685, 'total_duration': 39046.77811384201, 'accumulated_submission_time': 34907.619475364685, 'accumulated_eval_time': 4132.322255373001, 'accumulated_logging_time': 2.8405230045318604}
I0202 23:49:56.657893 139774434195200 logging_writer.py:48] [76076] accumulated_eval_time=4132.322255, accumulated_logging_time=2.840523, accumulated_submission_time=34907.619475, global_step=76076, preemption_count=0, score=34907.619475, test/accuracy=0.360900, test/loss=3.055748, test/num_examples=10000, total_duration=39046.778114, train/accuracy=0.503281, train/loss=2.190779, validation/accuracy=0.464820, validation/loss=2.400575, validation/num_examples=50000
I0202 23:50:06.594096 139774417409792 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1103475093841553, loss=3.220853328704834
I0202 23:50:49.884117 139774434195200 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.0958914756774902, loss=3.327836036682129
I0202 23:51:36.271003 139774417409792 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.227662205696106, loss=3.4426932334899902
I0202 23:52:23.704669 139774434195200 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.8689345717430115, loss=5.281260013580322
I0202 23:53:10.258211 139774417409792 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.8647136688232422, loss=4.652398586273193
I0202 23:53:56.598394 139774434195200 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.2396327257156372, loss=5.718405723571777
I0202 23:54:42.848225 139774417409792 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.1546190977096558, loss=3.3945326805114746
I0202 23:55:29.116077 139774434195200 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.8295888900756836, loss=4.613745212554932
I0202 23:56:15.380355 139774417409792 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.973828136920929, loss=5.350559234619141
I0202 23:56:57.024725 139936116377408 spec.py:321] Evaluating on the training split.
I0202 23:57:07.636257 139936116377408 spec.py:333] Evaluating on the validation split.
I0202 23:57:44.266018 139936116377408 spec.py:349] Evaluating on the test split.
I0202 23:57:45.876439 139936116377408 submission_runner.py:408] Time since start: 39516.03s, 	Step: 76992, 	{'train/accuracy': 0.5121288895606995, 'train/loss': 2.185343027114868, 'validation/accuracy': 0.46901997923851013, 'validation/loss': 2.402076244354248, 'validation/num_examples': 50000, 'test/accuracy': 0.36640000343322754, 'test/loss': 3.04853892326355, 'test/num_examples': 10000, 'score': 35327.92823219299, 'total_duration': 39516.02749085426, 'accumulated_submission_time': 35327.92823219299, 'accumulated_eval_time': 4181.173963069916, 'accumulated_logging_time': 2.8811960220336914}
I0202 23:57:45.904782 139774434195200 logging_writer.py:48] [76992] accumulated_eval_time=4181.173963, accumulated_logging_time=2.881196, accumulated_submission_time=35327.928232, global_step=76992, preemption_count=0, score=35327.928232, test/accuracy=0.366400, test/loss=3.048539, test/num_examples=10000, total_duration=39516.027491, train/accuracy=0.512129, train/loss=2.185343, validation/accuracy=0.469020, validation/loss=2.402076, validation/num_examples=50000
I0202 23:57:49.488866 139774417409792 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4623943567276, loss=3.3831305503845215
I0202 23:58:31.754676 139774434195200 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.056802749633789, loss=3.1733078956604004
I0202 23:59:18.029292 139774417409792 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.1462267637252808, loss=3.198521852493286
I0203 00:00:04.458337 139774434195200 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.1163426637649536, loss=3.2855870723724365
I0203 00:00:50.596061 139774417409792 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.0257782936096191, loss=3.4072465896606445
I0203 00:01:37.334042 139774434195200 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.0953595638275146, loss=3.2225823402404785
I0203 00:02:24.215167 139774417409792 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.9867945909500122, loss=3.5639615058898926
I0203 00:03:10.637601 139774434195200 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.084670066833496, loss=3.3757898807525635
I0203 00:03:56.925044 139774417409792 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.9358855485916138, loss=5.587441921234131
I0203 00:04:43.425003 139774434195200 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.9761744737625122, loss=3.783435821533203
I0203 00:04:45.890869 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:04:56.300011 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 00:05:32.978927 139936116377408 spec.py:349] Evaluating on the test split.
I0203 00:05:34.584455 139936116377408 submission_runner.py:408] Time since start: 39984.74s, 	Step: 77907, 	{'train/accuracy': 0.5077343583106995, 'train/loss': 2.158487558364868, 'validation/accuracy': 0.4768199920654297, 'validation/loss': 2.3143656253814697, 'validation/num_examples': 50000, 'test/accuracy': 0.36890003085136414, 'test/loss': 2.9804775714874268, 'test/num_examples': 10000, 'score': 35747.85677528381, 'total_duration': 39984.73551940918, 'accumulated_submission_time': 35747.85677528381, 'accumulated_eval_time': 4229.867544412613, 'accumulated_logging_time': 2.919562578201294}
I0203 00:05:34.615392 139774417409792 logging_writer.py:48] [77907] accumulated_eval_time=4229.867544, accumulated_logging_time=2.919563, accumulated_submission_time=35747.856775, global_step=77907, preemption_count=0, score=35747.856775, test/accuracy=0.368900, test/loss=2.980478, test/num_examples=10000, total_duration=39984.735519, train/accuracy=0.507734, train/loss=2.158488, validation/accuracy=0.476820, validation/loss=2.314366, validation/num_examples=50000
I0203 00:06:14.055752 139774434195200 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.1380362510681152, loss=3.213151693344116
I0203 00:06:59.949853 139774417409792 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.0817389488220215, loss=3.252304792404175
I0203 00:07:46.634635 139774434195200 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.150366187095642, loss=3.452578544616699
I0203 00:08:33.193783 139774417409792 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.7757407426834106, loss=4.738755702972412
I0203 00:09:19.410585 139774434195200 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.033247709274292, loss=3.171128511428833
I0203 00:10:05.589741 139774417409792 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.2521928548812866, loss=3.2800488471984863
I0203 00:10:52.056277 139774434195200 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.0212039947509766, loss=3.7522106170654297
I0203 00:11:38.634562 139774417409792 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.1531314849853516, loss=3.2006373405456543
I0203 00:12:25.098803 139774434195200 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.8230473399162292, loss=5.382747650146484
I0203 00:12:34.861715 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:12:45.290597 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 00:13:28.746114 139936116377408 spec.py:349] Evaluating on the test split.
I0203 00:13:30.356513 139936116377408 submission_runner.py:408] Time since start: 40460.51s, 	Step: 78822, 	{'train/accuracy': 0.5114843845367432, 'train/loss': 2.1231250762939453, 'validation/accuracy': 0.4710799753665924, 'validation/loss': 2.3353586196899414, 'validation/num_examples': 50000, 'test/accuracy': 0.3671000301837921, 'test/loss': 3.0031578540802, 'test/num_examples': 10000, 'score': 36168.04482078552, 'total_duration': 40460.507573604584, 'accumulated_submission_time': 36168.04482078552, 'accumulated_eval_time': 4285.362320184708, 'accumulated_logging_time': 2.961005449295044}
I0203 00:13:30.386586 139774417409792 logging_writer.py:48] [78822] accumulated_eval_time=4285.362320, accumulated_logging_time=2.961005, accumulated_submission_time=36168.044821, global_step=78822, preemption_count=0, score=36168.044821, test/accuracy=0.367100, test/loss=3.003158, test/num_examples=10000, total_duration=40460.507574, train/accuracy=0.511484, train/loss=2.123125, validation/accuracy=0.471080, validation/loss=2.335359, validation/num_examples=50000
I0203 00:14:02.522942 139774434195200 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.8450279831886292, loss=5.661175727844238
I0203 00:14:48.361161 139774417409792 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.8881686925888062, loss=4.603715896606445
I0203 00:15:34.964326 139774434195200 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.103686809539795, loss=3.3372116088867188
I0203 00:16:21.182544 139774417409792 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.7685313820838928, loss=5.640655994415283
I0203 00:17:07.516543 139774434195200 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.0837864875793457, loss=3.3536577224731445
I0203 00:17:53.648047 139774417409792 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.9816596508026123, loss=3.214428186416626
I0203 00:18:40.022729 139774434195200 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.2038735151290894, loss=3.1905996799468994
I0203 00:19:26.373020 139774417409792 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.8414791226387024, loss=4.913557052612305
I0203 00:20:12.709054 139774434195200 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.8349085450172424, loss=5.566793441772461
I0203 00:20:30.370486 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:20:41.044274 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 00:21:17.818016 139936116377408 spec.py:349] Evaluating on the test split.
I0203 00:21:19.423301 139936116377408 submission_runner.py:408] Time since start: 40929.57s, 	Step: 79740, 	{'train/accuracy': 0.5294140577316284, 'train/loss': 2.0676496028900146, 'validation/accuracy': 0.465859979391098, 'validation/loss': 2.3905460834503174, 'validation/num_examples': 50000, 'test/accuracy': 0.3686000108718872, 'test/loss': 3.0116066932678223, 'test/num_examples': 10000, 'score': 36587.97202754021, 'total_duration': 40929.57435941696, 'accumulated_submission_time': 36587.97202754021, 'accumulated_eval_time': 4334.415132522583, 'accumulated_logging_time': 3.0005173683166504}
I0203 00:21:19.453469 139774417409792 logging_writer.py:48] [79740] accumulated_eval_time=4334.415133, accumulated_logging_time=3.000517, accumulated_submission_time=36587.972028, global_step=79740, preemption_count=0, score=36587.972028, test/accuracy=0.368600, test/loss=3.011607, test/num_examples=10000, total_duration=40929.574359, train/accuracy=0.529414, train/loss=2.067650, validation/accuracy=0.465860, validation/loss=2.390546, validation/num_examples=50000
I0203 00:21:43.709711 139774434195200 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.0554792881011963, loss=3.431821346282959
I0203 00:22:29.533818 139774417409792 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.0025248527526855, loss=3.2633731365203857
I0203 00:23:16.000763 139774434195200 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.1337854862213135, loss=3.2185475826263428
I0203 00:24:02.703323 139774417409792 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.4429391622543335, loss=3.4772403240203857
I0203 00:24:48.802125 139774434195200 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.8801782727241516, loss=3.957127571105957
I0203 00:25:35.093077 139774417409792 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4040271043777466, loss=3.366440534591675
I0203 00:26:21.643310 139774434195200 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.8631924390792847, loss=4.706699848175049
I0203 00:27:08.151674 139774417409792 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.8817946314811707, loss=5.48845100402832
I0203 00:27:54.238091 139774434195200 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.1385493278503418, loss=3.2473273277282715
I0203 00:28:19.469183 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:28:30.322111 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 00:29:08.567222 139936116377408 spec.py:349] Evaluating on the test split.
I0203 00:29:10.177231 139936116377408 submission_runner.py:408] Time since start: 41400.33s, 	Step: 80656, 	{'train/accuracy': 0.4966796636581421, 'train/loss': 2.2336039543151855, 'validation/accuracy': 0.4697999954223633, 'validation/loss': 2.378688335418701, 'validation/num_examples': 50000, 'test/accuracy': 0.3669000267982483, 'test/loss': 3.037405252456665, 'test/num_examples': 10000, 'score': 37007.93005943298, 'total_duration': 41400.328288793564, 'accumulated_submission_time': 37007.93005943298, 'accumulated_eval_time': 4385.123164892197, 'accumulated_logging_time': 3.0408191680908203}
I0203 00:29:10.207479 139774417409792 logging_writer.py:48] [80656] accumulated_eval_time=4385.123165, accumulated_logging_time=3.040819, accumulated_submission_time=37007.930059, global_step=80656, preemption_count=0, score=37007.930059, test/accuracy=0.366900, test/loss=3.037405, test/num_examples=10000, total_duration=41400.328289, train/accuracy=0.496680, train/loss=2.233604, validation/accuracy=0.469800, validation/loss=2.378688, validation/num_examples=50000
I0203 00:29:28.345078 139774434195200 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.37384831905365, loss=3.4414236545562744
I0203 00:30:12.730770 139774417409792 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.9239011406898499, loss=4.448929786682129
I0203 00:30:59.050679 139774434195200 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.0625238418579102, loss=3.11342191696167
I0203 00:31:45.745137 139774417409792 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.085368037223816, loss=3.5981853008270264
I0203 00:32:32.164632 139774434195200 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.854846715927124, loss=3.124561071395874
I0203 00:33:18.580222 139774417409792 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.9700866341590881, loss=5.52195930480957
I0203 00:34:05.042735 139774434195200 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.2615479230880737, loss=3.1869325637817383
I0203 00:34:51.455080 139774417409792 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.2213075160980225, loss=3.2747881412506104
I0203 00:35:37.549263 139774434195200 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.1839548349380493, loss=3.408942699432373
I0203 00:36:10.231767 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:36:20.979446 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 00:36:58.470371 139936116377408 spec.py:349] Evaluating on the test split.
I0203 00:37:00.078356 139936116377408 submission_runner.py:408] Time since start: 41870.23s, 	Step: 81572, 	{'train/accuracy': 0.512890636920929, 'train/loss': 2.137596368789673, 'validation/accuracy': 0.47655999660491943, 'validation/loss': 2.3293349742889404, 'validation/num_examples': 50000, 'test/accuracy': 0.37210002541542053, 'test/loss': 2.9698870182037354, 'test/num_examples': 10000, 'score': 37427.89789867401, 'total_duration': 41870.229408979416, 'accumulated_submission_time': 37427.89789867401, 'accumulated_eval_time': 4434.969736337662, 'accumulated_logging_time': 3.0801961421966553}
I0203 00:37:00.111173 139774417409792 logging_writer.py:48] [81572] accumulated_eval_time=4434.969736, accumulated_logging_time=3.080196, accumulated_submission_time=37427.897899, global_step=81572, preemption_count=0, score=37427.897899, test/accuracy=0.372100, test/loss=2.969887, test/num_examples=10000, total_duration=41870.229409, train/accuracy=0.512891, train/loss=2.137596, validation/accuracy=0.476560, validation/loss=2.329335, validation/num_examples=50000
I0203 00:37:11.641073 139774434195200 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.9760923981666565, loss=5.045351505279541
I0203 00:37:55.340256 139774417409792 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.2166779041290283, loss=5.79153299331665
I0203 00:38:41.501292 139774434195200 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.7296897768974304, loss=5.559358596801758
I0203 00:39:27.975527 139774417409792 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.0357741117477417, loss=3.2766003608703613
I0203 00:40:14.087393 139774434195200 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.1578333377838135, loss=5.089150428771973
I0203 00:41:00.049680 139774417409792 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.3044312000274658, loss=3.352505922317505
I0203 00:41:46.508668 139774434195200 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.1951613426208496, loss=5.7819976806640625
I0203 00:42:32.701407 139774417409792 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.0836564302444458, loss=3.2718546390533447
I0203 00:43:18.983687 139774434195200 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.1624908447265625, loss=3.1946048736572266
I0203 00:44:00.249521 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:44:11.365966 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 00:44:48.773198 139936116377408 spec.py:349] Evaluating on the test split.
I0203 00:44:50.375148 139936116377408 submission_runner.py:408] Time since start: 42340.53s, 	Step: 82491, 	{'train/accuracy': 0.531054675579071, 'train/loss': 2.0348589420318604, 'validation/accuracy': 0.4846400022506714, 'validation/loss': 2.2830276489257812, 'validation/num_examples': 50000, 'test/accuracy': 0.37310001254081726, 'test/loss': 2.9452946186065674, 'test/num_examples': 10000, 'score': 37847.978172302246, 'total_duration': 42340.52619123459, 'accumulated_submission_time': 37847.978172302246, 'accumulated_eval_time': 4485.095364332199, 'accumulated_logging_time': 3.1229937076568604}
I0203 00:44:50.404080 139774417409792 logging_writer.py:48] [82491] accumulated_eval_time=4485.095364, accumulated_logging_time=3.122994, accumulated_submission_time=37847.978172, global_step=82491, preemption_count=0, score=37847.978172, test/accuracy=0.373100, test/loss=2.945295, test/num_examples=10000, total_duration=42340.526191, train/accuracy=0.531055, train/loss=2.034859, validation/accuracy=0.484640, validation/loss=2.283028, validation/num_examples=50000
I0203 00:44:54.376938 139774434195200 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.056107759475708, loss=4.058467864990234
I0203 00:45:36.942265 139774417409792 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.9524168968200684, loss=5.237010955810547
I0203 00:46:23.295122 139774434195200 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.0288636684417725, loss=3.1125941276550293
I0203 00:47:09.783319 139774417409792 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.1767948865890503, loss=3.2792038917541504
I0203 00:47:55.856746 139774434195200 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.0354325771331787, loss=3.164137840270996
I0203 00:48:42.427209 139774417409792 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.888318657875061, loss=4.3189873695373535
I0203 00:49:28.824071 139774434195200 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.1931418180465698, loss=3.195906639099121
I0203 00:50:15.073852 139774417409792 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.0403934717178345, loss=3.316976308822632
I0203 00:51:01.184801 139774434195200 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.7855537533760071, loss=5.430136203765869
I0203 00:51:47.770256 139774417409792 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.3519591093063354, loss=5.702781677246094
I0203 00:51:50.564121 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:52:01.304708 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 00:52:36.712395 139936116377408 spec.py:349] Evaluating on the test split.
I0203 00:52:38.311922 139936116377408 submission_runner.py:408] Time since start: 42808.46s, 	Step: 83408, 	{'train/accuracy': 0.519726574420929, 'train/loss': 2.0835025310516357, 'validation/accuracy': 0.4893999993801117, 'validation/loss': 2.2483296394348145, 'validation/num_examples': 50000, 'test/accuracy': 0.38360002636909485, 'test/loss': 2.9029996395111084, 'test/num_examples': 10000, 'score': 38268.08063173294, 'total_duration': 42808.462988615036, 'accumulated_submission_time': 38268.08063173294, 'accumulated_eval_time': 4532.843173027039, 'accumulated_logging_time': 3.1619906425476074}
I0203 00:52:38.342351 139774434195200 logging_writer.py:48] [83408] accumulated_eval_time=4532.843173, accumulated_logging_time=3.161991, accumulated_submission_time=38268.080632, global_step=83408, preemption_count=0, score=38268.080632, test/accuracy=0.383600, test/loss=2.903000, test/num_examples=10000, total_duration=42808.462989, train/accuracy=0.519727, train/loss=2.083503, validation/accuracy=0.489400, validation/loss=2.248330, validation/num_examples=50000
I0203 00:53:17.338967 139774417409792 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2220076322555542, loss=3.182363510131836
I0203 00:54:03.527798 139774434195200 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.001823902130127, loss=3.984088182449341
I0203 00:54:50.083496 139774417409792 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.8359048366546631, loss=5.049480438232422
I0203 00:55:36.361274 139774434195200 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.1131830215454102, loss=3.501215934753418
I0203 00:56:23.016299 139774417409792 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.0964536666870117, loss=3.156616687774658
I0203 00:57:09.357013 139774434195200 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.144006371498108, loss=3.6312503814697266
I0203 00:57:55.592500 139774417409792 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.2567224502563477, loss=3.2522761821746826
I0203 00:58:41.944826 139774434195200 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.0460119247436523, loss=3.1345438957214355
I0203 00:59:28.349255 139774417409792 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.9325708150863647, loss=4.3059468269348145
I0203 00:59:38.577544 139936116377408 spec.py:321] Evaluating on the training split.
I0203 00:59:49.205101 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:00:25.576611 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:00:27.184124 139936116377408 submission_runner.py:408] Time since start: 43277.34s, 	Step: 84324, 	{'train/accuracy': 0.5274999737739563, 'train/loss': 2.044956684112549, 'validation/accuracy': 0.4887399971485138, 'validation/loss': 2.2427096366882324, 'validation/num_examples': 50000, 'test/accuracy': 0.38020002841949463, 'test/loss': 2.9330248832702637, 'test/num_examples': 10000, 'score': 38688.25997066498, 'total_duration': 43277.33517932892, 'accumulated_submission_time': 38688.25997066498, 'accumulated_eval_time': 4581.449735164642, 'accumulated_logging_time': 3.20169997215271}
I0203 01:00:27.213454 139774434195200 logging_writer.py:48] [84324] accumulated_eval_time=4581.449735, accumulated_logging_time=3.201700, accumulated_submission_time=38688.259971, global_step=84324, preemption_count=0, score=38688.259971, test/accuracy=0.380200, test/loss=2.933025, test/num_examples=10000, total_duration=43277.335179, train/accuracy=0.527500, train/loss=2.044957, validation/accuracy=0.488740, validation/loss=2.242710, validation/num_examples=50000
I0203 01:00:58.374397 139774417409792 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.0140475034713745, loss=3.300133228302002
I0203 01:01:44.460759 139774434195200 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.073227047920227, loss=3.3185739517211914
I0203 01:02:31.063990 139774417409792 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.1399266719818115, loss=3.3868672847747803
I0203 01:03:17.184273 139774434195200 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.2483506202697754, loss=3.246598243713379
I0203 01:04:03.483003 139774417409792 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.107673168182373, loss=3.1679251194000244
I0203 01:04:49.826118 139774434195200 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.043067216873169, loss=5.588085174560547
I0203 01:05:36.054572 139774417409792 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.1048651933670044, loss=3.4883947372436523
I0203 01:06:22.664601 139774434195200 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.1222105026245117, loss=3.228085517883301
I0203 01:07:09.320477 139774417409792 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.086264729499817, loss=3.12296986579895
I0203 01:07:27.593319 139936116377408 spec.py:321] Evaluating on the training split.
I0203 01:07:38.216297 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:08:15.896094 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:08:17.514970 139936116377408 submission_runner.py:408] Time since start: 43747.67s, 	Step: 85241, 	{'train/accuracy': 0.5252148509025574, 'train/loss': 2.1249876022338867, 'validation/accuracy': 0.48159998655319214, 'validation/loss': 2.3384721279144287, 'validation/num_examples': 50000, 'test/accuracy': 0.3765000104904175, 'test/loss': 2.978649139404297, 'test/num_examples': 10000, 'score': 39108.579641819, 'total_duration': 43747.666029930115, 'accumulated_submission_time': 39108.579641819, 'accumulated_eval_time': 4631.371387481689, 'accumulated_logging_time': 3.2427725791931152}
I0203 01:08:17.543079 139774434195200 logging_writer.py:48] [85241] accumulated_eval_time=4631.371387, accumulated_logging_time=3.242773, accumulated_submission_time=39108.579642, global_step=85241, preemption_count=0, score=39108.579642, test/accuracy=0.376500, test/loss=2.978649, test/num_examples=10000, total_duration=43747.666030, train/accuracy=0.525215, train/loss=2.124988, validation/accuracy=0.481600, validation/loss=2.338472, validation/num_examples=50000
I0203 01:08:41.382328 139774417409792 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.8194467425346375, loss=5.5719685554504395
I0203 01:09:26.875800 139774434195200 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.1249624490737915, loss=3.1473548412323
I0203 01:10:13.289965 139774417409792 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.1134291887283325, loss=3.293787956237793
I0203 01:10:59.544641 139774434195200 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.1291645765304565, loss=3.15214204788208
I0203 01:11:45.989573 139774417409792 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.2231906652450562, loss=3.2597928047180176
I0203 01:12:32.362668 139774434195200 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.0574226379394531, loss=3.384800434112549
I0203 01:13:18.740959 139774417409792 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.1441595554351807, loss=3.2740097045898438
I0203 01:14:05.141846 139774434195200 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.2047791481018066, loss=3.202754020690918
I0203 01:14:51.286081 139774417409792 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.2038205862045288, loss=3.478153944015503
I0203 01:15:17.629971 139936116377408 spec.py:321] Evaluating on the training split.
I0203 01:15:29.184485 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:16:09.396825 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:16:11.003542 139936116377408 submission_runner.py:408] Time since start: 44221.15s, 	Step: 86158, 	{'train/accuracy': 0.515820324420929, 'train/loss': 2.156745672225952, 'validation/accuracy': 0.4821399748325348, 'validation/loss': 2.332691192626953, 'validation/num_examples': 50000, 'test/accuracy': 0.3702000081539154, 'test/loss': 2.974332332611084, 'test/num_examples': 10000, 'score': 39528.60836338997, 'total_duration': 44221.15460038185, 'accumulated_submission_time': 39528.60836338997, 'accumulated_eval_time': 4684.744953393936, 'accumulated_logging_time': 3.2811779975891113}
I0203 01:16:11.036650 139774434195200 logging_writer.py:48] [86158] accumulated_eval_time=4684.744953, accumulated_logging_time=3.281178, accumulated_submission_time=39528.608363, global_step=86158, preemption_count=0, score=39528.608363, test/accuracy=0.370200, test/loss=2.974332, test/num_examples=10000, total_duration=44221.154600, train/accuracy=0.515820, train/loss=2.156746, validation/accuracy=0.482140, validation/loss=2.332691, validation/num_examples=50000
I0203 01:16:28.123997 139774417409792 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.9057426452636719, loss=5.4382429122924805
I0203 01:17:12.740927 139774434195200 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.1186786890029907, loss=5.086745262145996
I0203 01:17:59.283331 139774417409792 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.9029754996299744, loss=5.496128082275391
I0203 01:18:45.652623 139774434195200 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.1039636135101318, loss=3.033524990081787
I0203 01:19:31.950728 139774417409792 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.9218927621841431, loss=4.083867073059082
I0203 01:20:18.378572 139774434195200 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.0564897060394287, loss=3.161797523498535
I0203 01:21:04.685239 139774417409792 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.1312412023544312, loss=3.6582162380218506
I0203 01:21:50.849478 139774434195200 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.1361145973205566, loss=3.9267866611480713
I0203 01:22:37.174865 139774417409792 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.104054570198059, loss=3.3079710006713867
I0203 01:23:11.180437 139936116377408 spec.py:321] Evaluating on the training split.
I0203 01:23:21.785933 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:23:59.071964 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:24:00.674748 139936116377408 submission_runner.py:408] Time since start: 44690.83s, 	Step: 87075, 	{'train/accuracy': 0.5296288728713989, 'train/loss': 2.0581154823303223, 'validation/accuracy': 0.4989999830722809, 'validation/loss': 2.2154085636138916, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.8767480850219727, 'test/num_examples': 10000, 'score': 39948.69438958168, 'total_duration': 44690.825795173645, 'accumulated_submission_time': 39948.69438958168, 'accumulated_eval_time': 4734.239243745804, 'accumulated_logging_time': 3.324136257171631}
I0203 01:24:00.706998 139774434195200 logging_writer.py:48] [87075] accumulated_eval_time=4734.239244, accumulated_logging_time=3.324136, accumulated_submission_time=39948.694390, global_step=87075, preemption_count=0, score=39948.694390, test/accuracy=0.392400, test/loss=2.876748, test/num_examples=10000, total_duration=44690.825795, train/accuracy=0.529629, train/loss=2.058115, validation/accuracy=0.499000, validation/loss=2.215409, validation/num_examples=50000
I0203 01:24:11.045104 139774417409792 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.0270678997039795, loss=3.15783429145813
I0203 01:24:54.556286 139774434195200 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.1782578229904175, loss=3.1027956008911133
I0203 01:25:40.787990 139774417409792 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.0639705657958984, loss=3.4089462757110596
I0203 01:26:27.254817 139774434195200 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.1513704061508179, loss=3.1866493225097656
I0203 01:27:13.696162 139774417409792 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.1828913688659668, loss=3.1308884620666504
I0203 01:28:00.129981 139774434195200 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.8846023678779602, loss=4.464771270751953
I0203 01:28:46.618944 139774417409792 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.9650232195854187, loss=4.377612590789795
I0203 01:29:33.240383 139774434195200 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.9578976035118103, loss=4.74133825302124
I0203 01:30:19.520128 139774417409792 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.0709973573684692, loss=3.12357497215271
I0203 01:31:00.944878 139936116377408 spec.py:321] Evaluating on the training split.
I0203 01:31:11.696361 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:31:51.104222 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:31:52.706664 139936116377408 submission_runner.py:408] Time since start: 45162.86s, 	Step: 87991, 	{'train/accuracy': 0.5358788967132568, 'train/loss': 1.9901221990585327, 'validation/accuracy': 0.5009599924087524, 'validation/loss': 2.1941733360290527, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.863675594329834, 'test/num_examples': 10000, 'score': 40368.87440466881, 'total_duration': 45162.857728004456, 'accumulated_submission_time': 40368.87440466881, 'accumulated_eval_time': 4786.001025438309, 'accumulated_logging_time': 3.3661766052246094}
I0203 01:31:52.738350 139774434195200 logging_writer.py:48] [87991] accumulated_eval_time=4786.001025, accumulated_logging_time=3.366177, accumulated_submission_time=40368.874405, global_step=87991, preemption_count=0, score=40368.874405, test/accuracy=0.392100, test/loss=2.863676, test/num_examples=10000, total_duration=45162.857728, train/accuracy=0.535879, train/loss=1.990122, validation/accuracy=0.500960, validation/loss=2.194173, validation/num_examples=50000
I0203 01:31:56.717906 139774417409792 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.0438668727874756, loss=4.668968677520752
I0203 01:32:39.168532 139774434195200 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.3486448526382446, loss=3.262974739074707
I0203 01:33:25.457725 139774417409792 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.8478060364723206, loss=5.251248359680176
I0203 01:34:11.921082 139774434195200 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.2790517807006836, loss=3.173062324523926
I0203 01:34:58.145931 139774417409792 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.8452546000480652, loss=5.408543586730957
I0203 01:35:44.501752 139774434195200 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.9449562430381775, loss=4.19917631149292
I0203 01:36:31.066938 139774417409792 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.050005316734314, loss=3.176359176635742
I0203 01:37:17.504477 139774434195200 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.212851881980896, loss=3.1183207035064697
I0203 01:38:03.872261 139774417409792 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.1469842195510864, loss=3.235323429107666
I0203 01:38:50.300087 139774434195200 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.0261341333389282, loss=3.400573968887329
I0203 01:38:52.774860 139936116377408 spec.py:321] Evaluating on the training split.
I0203 01:39:03.276000 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:39:40.347214 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:39:41.960502 139936116377408 submission_runner.py:408] Time since start: 45632.11s, 	Step: 88907, 	{'train/accuracy': 0.5342382788658142, 'train/loss': 2.0812058448791504, 'validation/accuracy': 0.48997998237609863, 'validation/loss': 2.29469895362854, 'validation/num_examples': 50000, 'test/accuracy': 0.3799000084400177, 'test/loss': 2.949227809906006, 'test/num_examples': 10000, 'score': 40788.85287356377, 'total_duration': 45632.111568689346, 'accumulated_submission_time': 40788.85287356377, 'accumulated_eval_time': 4835.1866590976715, 'accumulated_logging_time': 3.407886266708374}
I0203 01:39:41.991107 139774417409792 logging_writer.py:48] [88907] accumulated_eval_time=4835.186659, accumulated_logging_time=3.407886, accumulated_submission_time=40788.852874, global_step=88907, preemption_count=0, score=40788.852874, test/accuracy=0.379900, test/loss=2.949228, test/num_examples=10000, total_duration=45632.111569, train/accuracy=0.534238, train/loss=2.081206, validation/accuracy=0.489980, validation/loss=2.294699, validation/num_examples=50000
I0203 01:40:20.988598 139774434195200 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.2759363651275635, loss=3.126399040222168
I0203 01:41:07.122713 139774417409792 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.1223613023757935, loss=3.2956929206848145
I0203 01:41:53.728497 139774434195200 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.0684771537780762, loss=3.397704839706421
I0203 01:42:40.465845 139774417409792 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.0494410991668701, loss=3.5052578449249268
I0203 01:43:26.875556 139774434195200 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.8888845443725586, loss=4.991280555725098
I0203 01:44:13.410814 139774417409792 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.0620012283325195, loss=4.229353427886963
I0203 01:45:00.134795 139774434195200 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.1335402727127075, loss=3.184980869293213
I0203 01:45:46.562246 139774417409792 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.0203111171722412, loss=3.432192087173462
I0203 01:46:33.247956 139774434195200 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.0140308141708374, loss=5.422461032867432
I0203 01:46:42.252769 139936116377408 spec.py:321] Evaluating on the training split.
I0203 01:46:52.870354 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:47:29.362926 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:47:30.972231 139936116377408 submission_runner.py:408] Time since start: 46101.12s, 	Step: 89821, 	{'train/accuracy': 0.5366601347923279, 'train/loss': 2.041024923324585, 'validation/accuracy': 0.5012800097465515, 'validation/loss': 2.2048683166503906, 'validation/num_examples': 50000, 'test/accuracy': 0.394400030374527, 'test/loss': 2.8692476749420166, 'test/num_examples': 10000, 'score': 41209.05537772179, 'total_duration': 46101.123297929764, 'accumulated_submission_time': 41209.05537772179, 'accumulated_eval_time': 4883.906123161316, 'accumulated_logging_time': 3.450183391571045}
I0203 01:47:31.000795 139774417409792 logging_writer.py:48] [89821] accumulated_eval_time=4883.906123, accumulated_logging_time=3.450183, accumulated_submission_time=41209.055378, global_step=89821, preemption_count=0, score=41209.055378, test/accuracy=0.394400, test/loss=2.869248, test/num_examples=10000, total_duration=46101.123298, train/accuracy=0.536660, train/loss=2.041025, validation/accuracy=0.501280, validation/loss=2.204868, validation/num_examples=50000
I0203 01:48:04.151907 139774434195200 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.149412751197815, loss=2.9717259407043457
I0203 01:48:50.124681 139774417409792 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.3229261636734009, loss=3.1810977458953857
I0203 01:49:36.713039 139774434195200 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.8395103216171265, loss=5.496222496032715
I0203 01:50:23.367340 139774417409792 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.9553161859512329, loss=3.828873634338379
I0203 01:51:09.370672 139774434195200 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.0111576318740845, loss=3.7379462718963623
I0203 01:51:55.920653 139774417409792 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.3403822183609009, loss=3.135605812072754
I0203 01:52:42.223672 139774434195200 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.0391616821289062, loss=3.1122922897338867
I0203 01:53:28.761451 139774417409792 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.0874072313308716, loss=4.525238037109375
I0203 01:54:15.169415 139774434195200 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.1869161128997803, loss=3.0408101081848145
I0203 01:54:31.111387 139936116377408 spec.py:321] Evaluating on the training split.
I0203 01:54:41.794240 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 01:55:19.203410 139936116377408 spec.py:349] Evaluating on the test split.
I0203 01:55:20.808041 139936116377408 submission_runner.py:408] Time since start: 46570.96s, 	Step: 90736, 	{'train/accuracy': 0.5470117330551147, 'train/loss': 1.9687862396240234, 'validation/accuracy': 0.5073800086975098, 'validation/loss': 2.1694068908691406, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.8334524631500244, 'test/num_examples': 10000, 'score': 41629.10823345184, 'total_duration': 46570.9590845108, 'accumulated_submission_time': 41629.10823345184, 'accumulated_eval_time': 4933.602746248245, 'accumulated_logging_time': 3.4877588748931885}
I0203 01:55:20.838096 139774417409792 logging_writer.py:48] [90736] accumulated_eval_time=4933.602746, accumulated_logging_time=3.487759, accumulated_submission_time=41629.108233, global_step=90736, preemption_count=0, score=41629.108233, test/accuracy=0.398900, test/loss=2.833452, test/num_examples=10000, total_duration=46570.959085, train/accuracy=0.547012, train/loss=1.968786, validation/accuracy=0.507380, validation/loss=2.169407, validation/num_examples=50000
I0203 01:55:46.730232 139774434195200 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.0591362714767456, loss=3.586223840713501
I0203 01:56:32.710160 139774417409792 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.1687934398651123, loss=3.32080340385437
I0203 01:57:19.151517 139774434195200 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.1156020164489746, loss=3.924377202987671
I0203 01:58:05.894671 139774417409792 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.270675778388977, loss=3.115070343017578
I0203 01:58:52.200308 139774434195200 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.1384282112121582, loss=2.9819633960723877
I0203 01:59:39.055216 139774417409792 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.071442723274231, loss=3.4001083374023438
I0203 02:00:25.902900 139774434195200 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.2958065271377563, loss=2.976400375366211
I0203 02:01:12.318887 139774417409792 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.9233898520469666, loss=5.256343364715576
I0203 02:01:59.110694 139774434195200 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.1922892332077026, loss=3.082019805908203
I0203 02:02:21.274857 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:02:32.197790 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:03:09.809042 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:03:11.416594 139936116377408 submission_runner.py:408] Time since start: 47041.57s, 	Step: 91649, 	{'train/accuracy': 0.5741991996765137, 'train/loss': 1.8185863494873047, 'validation/accuracy': 0.5086199641227722, 'validation/loss': 2.1520354747772217, 'validation/num_examples': 50000, 'test/accuracy': 0.39730000495910645, 'test/loss': 2.799119234085083, 'test/num_examples': 10000, 'score': 42049.48864984512, 'total_duration': 47041.567656993866, 'accumulated_submission_time': 42049.48864984512, 'accumulated_eval_time': 4983.74448466301, 'accumulated_logging_time': 3.526984214782715}
I0203 02:03:11.448265 139774417409792 logging_writer.py:48] [91649] accumulated_eval_time=4983.744485, accumulated_logging_time=3.526984, accumulated_submission_time=42049.488650, global_step=91649, preemption_count=0, score=42049.488650, test/accuracy=0.397300, test/loss=2.799119, test/num_examples=10000, total_duration=47041.567657, train/accuracy=0.574199, train/loss=1.818586, validation/accuracy=0.508620, validation/loss=2.152035, validation/num_examples=50000
I0203 02:03:32.126471 139774434195200 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.1937941312789917, loss=3.115568161010742
I0203 02:04:17.380199 139774417409792 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.8257889151573181, loss=5.241747856140137
I0203 02:05:03.775593 139774434195200 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.0652498006820679, loss=3.365034341812134
I0203 02:05:50.068989 139774417409792 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.2571871280670166, loss=3.28418231010437
I0203 02:06:36.316956 139774434195200 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.063041090965271, loss=5.319708824157715
I0203 02:07:22.878746 139774417409792 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.1719563007354736, loss=3.170907974243164
I0203 02:08:09.030476 139774434195200 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.9257778525352478, loss=5.463155269622803
I0203 02:08:55.473891 139774417409792 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.9301708340644836, loss=4.735107898712158
I0203 02:09:41.808493 139774434195200 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.8849050402641296, loss=4.345511436462402
I0203 02:10:11.743828 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:10:22.129707 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:11:00.554030 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:11:02.173449 139936116377408 submission_runner.py:408] Time since start: 47512.32s, 	Step: 92566, 	{'train/accuracy': 0.5272265672683716, 'train/loss': 2.0880000591278076, 'validation/accuracy': 0.5004400014877319, 'validation/loss': 2.219738483428955, 'validation/num_examples': 50000, 'test/accuracy': 0.39430001378059387, 'test/loss': 2.86732816696167, 'test/num_examples': 10000, 'score': 42469.72681379318, 'total_duration': 47512.32449412346, 'accumulated_submission_time': 42469.72681379318, 'accumulated_eval_time': 5034.174078941345, 'accumulated_logging_time': 3.568467617034912}
I0203 02:11:02.216375 139774417409792 logging_writer.py:48] [92566] accumulated_eval_time=5034.174079, accumulated_logging_time=3.568468, accumulated_submission_time=42469.726814, global_step=92566, preemption_count=0, score=42469.726814, test/accuracy=0.394300, test/loss=2.867328, test/num_examples=10000, total_duration=47512.324494, train/accuracy=0.527227, train/loss=2.088000, validation/accuracy=0.500440, validation/loss=2.219738, validation/num_examples=50000
I0203 02:11:16.348437 139774434195200 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.9895268678665161, loss=3.085131883621216
I0203 02:12:00.363179 139774417409792 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.8800757527351379, loss=5.091713905334473
I0203 02:12:46.737635 139774434195200 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.2023179531097412, loss=3.2090470790863037
I0203 02:13:33.271687 139774417409792 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.9836429357528687, loss=4.221822261810303
I0203 02:14:19.714054 139774434195200 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.0808035135269165, loss=3.915480852127075
I0203 02:15:05.927825 139774417409792 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.4494552612304688, loss=3.1102707386016846
I0203 02:15:53.401578 139774434195200 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.0307426452636719, loss=3.8108415603637695
I0203 02:16:39.826243 139774417409792 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.0709540843963623, loss=3.1411216259002686
I0203 02:17:26.060059 139774434195200 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.971265435218811, loss=3.7505393028259277
I0203 02:18:02.254022 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:18:12.902988 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:18:50.296808 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:18:51.900949 139936116377408 submission_runner.py:408] Time since start: 47982.05s, 	Step: 93480, 	{'train/accuracy': 0.5433398485183716, 'train/loss': 2.01934814453125, 'validation/accuracy': 0.5062999725341797, 'validation/loss': 2.205066442489624, 'validation/num_examples': 50000, 'test/accuracy': 0.39320001006126404, 'test/loss': 2.861738681793213, 'test/num_examples': 10000, 'score': 42889.70433783531, 'total_duration': 47982.05200815201, 'accumulated_submission_time': 42889.70433783531, 'accumulated_eval_time': 5083.820999860764, 'accumulated_logging_time': 3.623884439468384}
I0203 02:18:51.933231 139774417409792 logging_writer.py:48] [93480] accumulated_eval_time=5083.821000, accumulated_logging_time=3.623884, accumulated_submission_time=42889.704338, global_step=93480, preemption_count=0, score=42889.704338, test/accuracy=0.393200, test/loss=2.861739, test/num_examples=10000, total_duration=47982.052008, train/accuracy=0.543340, train/loss=2.019348, validation/accuracy=0.506300, validation/loss=2.205066, validation/num_examples=50000
I0203 02:19:00.290091 139774434195200 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.034590482711792, loss=4.1042256355285645
I0203 02:19:43.598380 139774417409792 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.2099334001541138, loss=3.3727164268493652
I0203 02:20:29.623032 139774434195200 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.0818291902542114, loss=3.122454881668091
I0203 02:21:15.964771 139774417409792 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.1642719507217407, loss=3.2174665927886963
I0203 02:22:02.634973 139774434195200 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.9850777387619019, loss=4.50173282623291
I0203 02:22:48.905128 139774417409792 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.2578991651535034, loss=2.992656946182251
I0203 02:23:35.369941 139774434195200 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.0460668802261353, loss=5.454090118408203
I0203 02:24:21.778191 139774417409792 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.0397733449935913, loss=5.190366744995117
I0203 02:25:08.287867 139774434195200 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.8826733827590942, loss=5.21936559677124
I0203 02:25:51.998469 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:26:02.838429 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:26:41.409176 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:26:43.012891 139936116377408 submission_runner.py:408] Time since start: 48453.16s, 	Step: 94396, 	{'train/accuracy': 0.5606250166893005, 'train/loss': 1.9081244468688965, 'validation/accuracy': 0.5089399814605713, 'validation/loss': 2.1560139656066895, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.820587158203125, 'test/num_examples': 10000, 'score': 43309.709506988525, 'total_duration': 48453.16395688057, 'accumulated_submission_time': 43309.709506988525, 'accumulated_eval_time': 5134.8354160785675, 'accumulated_logging_time': 3.6679365634918213}
I0203 02:26:43.046003 139774417409792 logging_writer.py:48] [94396] accumulated_eval_time=5134.835416, accumulated_logging_time=3.667937, accumulated_submission_time=43309.709507, global_step=94396, preemption_count=0, score=43309.709507, test/accuracy=0.397600, test/loss=2.820587, test/num_examples=10000, total_duration=48453.163957, train/accuracy=0.560625, train/loss=1.908124, validation/accuracy=0.508940, validation/loss=2.156014, validation/num_examples=50000
I0203 02:26:45.039900 139774434195200 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.1654151678085327, loss=3.221792459487915
I0203 02:27:27.441810 139774417409792 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.1437629461288452, loss=3.3957409858703613
I0203 02:28:13.612396 139774434195200 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.1694303750991821, loss=3.131803512573242
I0203 02:28:59.890183 139774417409792 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.1177196502685547, loss=3.368558406829834
I0203 02:29:46.194003 139774434195200 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.1348985433578491, loss=3.2948050498962402
I0203 02:30:32.725356 139774417409792 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.0782724618911743, loss=5.525537967681885
I0203 02:31:19.111213 139774434195200 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.9508217573165894, loss=4.524353981018066
I0203 02:32:06.163254 139774417409792 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.2437286376953125, loss=3.037473201751709
I0203 02:32:52.637020 139774434195200 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.1203882694244385, loss=3.0683655738830566
I0203 02:33:39.085216 139774417409792 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.2565537691116333, loss=3.1311354637145996
I0203 02:33:43.376464 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:33:54.040393 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:34:32.517671 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:34:34.120187 139936116377408 submission_runner.py:408] Time since start: 48924.27s, 	Step: 95311, 	{'train/accuracy': 0.5488671660423279, 'train/loss': 1.9369208812713623, 'validation/accuracy': 0.5136199593544006, 'validation/loss': 2.113115072250366, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.7761130332946777, 'test/num_examples': 10000, 'score': 43729.9824256897, 'total_duration': 48924.271253585815, 'accumulated_submission_time': 43729.9824256897, 'accumulated_eval_time': 5185.579132556915, 'accumulated_logging_time': 3.7109029293060303}
I0203 02:34:34.150017 139774434195200 logging_writer.py:48] [95311] accumulated_eval_time=5185.579133, accumulated_logging_time=3.710903, accumulated_submission_time=43729.982426, global_step=95311, preemption_count=0, score=43729.982426, test/accuracy=0.402800, test/loss=2.776113, test/num_examples=10000, total_duration=48924.271254, train/accuracy=0.548867, train/loss=1.936921, validation/accuracy=0.513620, validation/loss=2.113115, validation/num_examples=50000
I0203 02:35:11.637193 139774417409792 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.0792407989501953, loss=5.168097972869873
I0203 02:35:57.454532 139774434195200 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.9359835386276245, loss=5.52092981338501
I0203 02:36:44.139382 139774417409792 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.1716643571853638, loss=3.155984878540039
I0203 02:37:30.432302 139774434195200 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.9506558179855347, loss=3.8500142097473145
I0203 02:38:16.906377 139774417409792 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.9136772155761719, loss=5.274949073791504
I0203 02:39:03.095645 139774434195200 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.2033625841140747, loss=3.337210178375244
I0203 02:39:49.629714 139774417409792 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.1882433891296387, loss=3.0353848934173584
I0203 02:40:35.896983 139774434195200 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.9197937250137329, loss=5.365866184234619
I0203 02:41:22.236153 139774417409792 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.2181766033172607, loss=3.441046714782715
I0203 02:41:34.427274 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:41:45.080277 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:42:20.421465 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:42:22.032670 139936116377408 submission_runner.py:408] Time since start: 49392.18s, 	Step: 96228, 	{'train/accuracy': 0.5465624928474426, 'train/loss': 1.977414846420288, 'validation/accuracy': 0.5113599896430969, 'validation/loss': 2.1627683639526367, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.8213248252868652, 'test/num_examples': 10000, 'score': 44150.20173883438, 'total_duration': 49392.18372750282, 'accumulated_submission_time': 44150.20173883438, 'accumulated_eval_time': 5233.184512376785, 'accumulated_logging_time': 3.750455856323242}
I0203 02:42:22.065553 139774434195200 logging_writer.py:48] [96228] accumulated_eval_time=5233.184512, accumulated_logging_time=3.750456, accumulated_submission_time=44150.201739, global_step=96228, preemption_count=0, score=44150.201739, test/accuracy=0.402800, test/loss=2.821325, test/num_examples=10000, total_duration=49392.183728, train/accuracy=0.546562, train/loss=1.977415, validation/accuracy=0.511360, validation/loss=2.162768, validation/num_examples=50000
I0203 02:42:51.578430 139774417409792 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.273586630821228, loss=3.0072388648986816
I0203 02:43:37.818412 139774434195200 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.8352648019790649, loss=5.481723785400391
I0203 02:44:24.193705 139774417409792 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.1615862846374512, loss=3.044125556945801
I0203 02:45:10.412711 139774434195200 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.192933440208435, loss=3.0098087787628174
I0203 02:45:56.669580 139774417409792 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.0778213739395142, loss=2.8638904094696045
I0203 02:46:42.892682 139774434195200 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.2536050081253052, loss=3.0031888484954834
I0203 02:47:29.222366 139774417409792 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.137669563293457, loss=3.1265668869018555
I0203 02:48:15.539027 139774434195200 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.0244327783584595, loss=3.183669090270996
I0203 02:49:01.887601 139774417409792 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.2314014434814453, loss=5.4560017585754395
I0203 02:49:22.403154 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:49:33.390113 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:50:10.488558 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:50:12.093083 139936116377408 submission_runner.py:408] Time since start: 49862.24s, 	Step: 97146, 	{'train/accuracy': 0.5612499713897705, 'train/loss': 1.8850711584091187, 'validation/accuracy': 0.5123400092124939, 'validation/loss': 2.129922866821289, 'validation/num_examples': 50000, 'test/accuracy': 0.4019000232219696, 'test/loss': 2.7724106311798096, 'test/num_examples': 10000, 'score': 44570.48270201683, 'total_duration': 49862.24414396286, 'accumulated_submission_time': 44570.48270201683, 'accumulated_eval_time': 5282.874450683594, 'accumulated_logging_time': 3.7924487590789795}
I0203 02:50:12.122529 139774434195200 logging_writer.py:48] [97146] accumulated_eval_time=5282.874451, accumulated_logging_time=3.792449, accumulated_submission_time=44570.482702, global_step=97146, preemption_count=0, score=44570.482702, test/accuracy=0.401900, test/loss=2.772411, test/num_examples=10000, total_duration=49862.244144, train/accuracy=0.561250, train/loss=1.885071, validation/accuracy=0.512340, validation/loss=2.129923, validation/num_examples=50000
I0203 02:50:33.978288 139774417409792 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.0208889245986938, loss=3.727046012878418
I0203 02:51:19.219934 139774434195200 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.1474581956863403, loss=5.055503845214844
I0203 02:52:05.955927 139774417409792 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.0105377435684204, loss=5.4257283210754395
I0203 02:52:52.205763 139774434195200 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.1708521842956543, loss=3.2764697074890137
I0203 02:53:39.048681 139774417409792 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.1257075071334839, loss=3.0584793090820312
I0203 02:54:26.237591 139774434195200 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.0644221305847168, loss=3.2711849212646484
I0203 02:55:12.729454 139774417409792 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.0257203578948975, loss=4.433080673217773
I0203 02:55:59.233682 139774434195200 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.2209100723266602, loss=3.0116119384765625
I0203 02:56:46.096907 139774417409792 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.3043158054351807, loss=3.068800687789917
I0203 02:57:12.508987 139936116377408 spec.py:321] Evaluating on the training split.
I0203 02:57:23.147804 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 02:57:56.984694 139936116377408 spec.py:349] Evaluating on the test split.
I0203 02:57:58.590393 139936116377408 submission_runner.py:408] Time since start: 50328.74s, 	Step: 98058, 	{'train/accuracy': 0.5537304282188416, 'train/loss': 1.9444429874420166, 'validation/accuracy': 0.524179995059967, 'validation/loss': 2.0987751483917236, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.7410056591033936, 'test/num_examples': 10000, 'score': 44990.81266307831, 'total_duration': 50328.74145245552, 'accumulated_submission_time': 44990.81266307831, 'accumulated_eval_time': 5328.955847263336, 'accumulated_logging_time': 3.8312745094299316}
I0203 02:57:58.623581 139774434195200 logging_writer.py:48] [98058] accumulated_eval_time=5328.955847, accumulated_logging_time=3.831275, accumulated_submission_time=44990.812663, global_step=98058, preemption_count=0, score=44990.812663, test/accuracy=0.417000, test/loss=2.741006, test/num_examples=10000, total_duration=50328.741452, train/accuracy=0.553730, train/loss=1.944443, validation/accuracy=0.524180, validation/loss=2.098775, validation/num_examples=50000
I0203 02:58:15.784568 139774417409792 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.2284011840820312, loss=3.201364517211914
I0203 02:59:00.528157 139774434195200 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.2646567821502686, loss=2.9757907390594482
I0203 02:59:46.892753 139774417409792 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.1358309984207153, loss=3.13458514213562
I0203 03:00:33.295288 139774434195200 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.8838611841201782, loss=5.4355549812316895
I0203 03:01:19.731998 139774417409792 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.06610906124115, loss=5.495360851287842
I0203 03:02:06.539784 139774434195200 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.022557258605957, loss=5.410952568054199
I0203 03:02:52.999797 139774417409792 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.1819027662277222, loss=2.891803503036499
I0203 03:03:39.415893 139774434195200 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.174208164215088, loss=2.9612553119659424
I0203 03:04:26.184243 139774417409792 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.1565120220184326, loss=3.113410472869873
I0203 03:04:58.757489 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:05:09.505697 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 03:05:47.867066 139936116377408 spec.py:349] Evaluating on the test split.
I0203 03:05:49.466967 139936116377408 submission_runner.py:408] Time since start: 50799.62s, 	Step: 98972, 	{'train/accuracy': 0.56494140625, 'train/loss': 1.8888856172561646, 'validation/accuracy': 0.5273999571800232, 'validation/loss': 2.0798420906066895, 'validation/num_examples': 50000, 'test/accuracy': 0.4125000238418579, 'test/loss': 2.7286581993103027, 'test/num_examples': 10000, 'score': 45410.88789892197, 'total_duration': 50799.61802506447, 'accumulated_submission_time': 45410.88789892197, 'accumulated_eval_time': 5379.665320634842, 'accumulated_logging_time': 3.8756015300750732}
I0203 03:05:49.500065 139774434195200 logging_writer.py:48] [98972] accumulated_eval_time=5379.665321, accumulated_logging_time=3.875602, accumulated_submission_time=45410.887899, global_step=98972, preemption_count=0, score=45410.887899, test/accuracy=0.412500, test/loss=2.728658, test/num_examples=10000, total_duration=50799.618025, train/accuracy=0.564941, train/loss=1.888886, validation/accuracy=0.527400, validation/loss=2.079842, validation/num_examples=50000
I0203 03:06:01.029570 139774417409792 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.9908819198608398, loss=5.430577754974365
I0203 03:06:44.587419 139774434195200 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.202618956565857, loss=3.039121627807617
I0203 03:07:30.653778 139774417409792 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.0351210832595825, loss=5.310262203216553
I0203 03:08:17.222982 139774434195200 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.2022608518600464, loss=3.0147829055786133
I0203 03:09:03.801329 139774417409792 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.1405349969863892, loss=2.886951446533203
I0203 03:09:50.128332 139774434195200 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.8892378807067871, loss=5.281888484954834
I0203 03:10:36.277212 139774417409792 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.1225004196166992, loss=3.4164767265319824
I0203 03:11:22.982256 139774434195200 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.8941560387611389, loss=4.65643310546875
I0203 03:12:09.530745 139774417409792 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.2055491209030151, loss=4.020896911621094
I0203 03:12:49.923966 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:13:00.826339 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 03:13:37.665232 139936116377408 spec.py:349] Evaluating on the test split.
I0203 03:13:39.263726 139936116377408 submission_runner.py:408] Time since start: 51269.41s, 	Step: 99889, 	{'train/accuracy': 0.5637304782867432, 'train/loss': 1.909096121788025, 'validation/accuracy': 0.5228599905967712, 'validation/loss': 2.1150269508361816, 'validation/num_examples': 50000, 'test/accuracy': 0.4125000238418579, 'test/loss': 2.7483208179473877, 'test/num_examples': 10000, 'score': 45831.25173521042, 'total_duration': 51269.41478252411, 'accumulated_submission_time': 45831.25173521042, 'accumulated_eval_time': 5429.00506401062, 'accumulated_logging_time': 3.9204533100128174}
I0203 03:13:39.297807 139774434195200 logging_writer.py:48] [99889] accumulated_eval_time=5429.005064, accumulated_logging_time=3.920453, accumulated_submission_time=45831.251735, global_step=99889, preemption_count=0, score=45831.251735, test/accuracy=0.412500, test/loss=2.748321, test/num_examples=10000, total_duration=51269.414783, train/accuracy=0.563730, train/loss=1.909096, validation/accuracy=0.522860, validation/loss=2.115027, validation/num_examples=50000
I0203 03:13:44.131821 139774417409792 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.8096775412559509, loss=4.779551982879639
I0203 03:14:26.966637 139774434195200 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.157870888710022, loss=2.784881114959717
I0203 03:15:13.318881 139774417409792 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.0193660259246826, loss=5.434597015380859
I0203 03:15:59.631440 139774434195200 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.0700573921203613, loss=5.173403739929199
I0203 03:16:46.107911 139774417409792 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.0057791471481323, loss=4.798853874206543
I0203 03:17:32.546797 139774434195200 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.1789714097976685, loss=3.0309903621673584
I0203 03:18:18.853621 139774417409792 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.2561304569244385, loss=2.8980908393859863
I0203 03:19:05.213515 139774434195200 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.967258095741272, loss=4.019271373748779
I0203 03:19:51.429782 139774417409792 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.1056791543960571, loss=2.846951723098755
I0203 03:20:37.675055 139774434195200 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.261771321296692, loss=2.958268642425537
I0203 03:20:39.312560 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:20:49.859196 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 03:21:25.978461 139936116377408 spec.py:349] Evaluating on the test split.
I0203 03:21:27.582411 139936116377408 submission_runner.py:408] Time since start: 51737.73s, 	Step: 100805, 	{'train/accuracy': 0.5720898509025574, 'train/loss': 1.8558332920074463, 'validation/accuracy': 0.532260000705719, 'validation/loss': 2.0450856685638428, 'validation/num_examples': 50000, 'test/accuracy': 0.42330002784729004, 'test/loss': 2.701802968978882, 'test/num_examples': 10000, 'score': 46251.209023952484, 'total_duration': 51737.73347878456, 'accumulated_submission_time': 46251.209023952484, 'accumulated_eval_time': 5477.274906158447, 'accumulated_logging_time': 3.9645845890045166}
I0203 03:21:27.612753 139774417409792 logging_writer.py:48] [100805] accumulated_eval_time=5477.274906, accumulated_logging_time=3.964585, accumulated_submission_time=46251.209024, global_step=100805, preemption_count=0, score=46251.209024, test/accuracy=0.423300, test/loss=2.701803, test/num_examples=10000, total_duration=51737.733479, train/accuracy=0.572090, train/loss=1.855833, validation/accuracy=0.532260, validation/loss=2.045086, validation/num_examples=50000
I0203 03:22:07.935216 139774434195200 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.8668299913406372, loss=4.82263708114624
I0203 03:22:53.820570 139774417409792 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.2240617275238037, loss=2.8222789764404297
I0203 03:23:40.586407 139774434195200 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.062219500541687, loss=3.6002540588378906
I0203 03:24:26.883903 139774417409792 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.3167147636413574, loss=3.2626137733459473
I0203 03:25:13.216818 139774434195200 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.3536016941070557, loss=2.8802831172943115
I0203 03:25:59.956646 139774417409792 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.1298515796661377, loss=5.500334739685059
I0203 03:26:46.350975 139774434195200 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.2484349012374878, loss=2.9563729763031006
I0203 03:27:32.379454 139774417409792 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.0123482942581177, loss=4.373562812805176
I0203 03:28:18.913503 139774434195200 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.2257004976272583, loss=3.154672622680664
I0203 03:28:27.869743 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:28:38.362379 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 03:29:14.332342 139936116377408 spec.py:349] Evaluating on the test split.
I0203 03:29:15.933392 139936116377408 submission_runner.py:408] Time since start: 52206.08s, 	Step: 101721, 	{'train/accuracy': 0.5673046708106995, 'train/loss': 1.848193645477295, 'validation/accuracy': 0.5283600091934204, 'validation/loss': 2.050722360610962, 'validation/num_examples': 50000, 'test/accuracy': 0.40950003266334534, 'test/loss': 2.745497703552246, 'test/num_examples': 10000, 'score': 46671.40980911255, 'total_duration': 52206.08445620537, 'accumulated_submission_time': 46671.40980911255, 'accumulated_eval_time': 5525.338563919067, 'accumulated_logging_time': 4.004071235656738}
I0203 03:29:15.967962 139774417409792 logging_writer.py:48] [101721] accumulated_eval_time=5525.338564, accumulated_logging_time=4.004071, accumulated_submission_time=46671.409809, global_step=101721, preemption_count=0, score=46671.409809, test/accuracy=0.409500, test/loss=2.745498, test/num_examples=10000, total_duration=52206.084456, train/accuracy=0.567305, train/loss=1.848194, validation/accuracy=0.528360, validation/loss=2.050722, validation/num_examples=50000
I0203 03:29:48.559986 139774434195200 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.9736076593399048, loss=4.785726547241211
I0203 03:30:34.533793 139774417409792 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.17459237575531, loss=3.0710489749908447
I0203 03:31:20.831182 139774434195200 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.20142662525177, loss=2.9482433795928955
I0203 03:32:07.412089 139774417409792 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.1621768474578857, loss=4.054964065551758
I0203 03:32:53.336431 139774434195200 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.9650763273239136, loss=3.9200456142425537
I0203 03:33:39.498564 139774417409792 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.1215581893920898, loss=3.6578872203826904
I0203 03:34:25.910386 139774434195200 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.0319827795028687, loss=3.676968574523926
I0203 03:35:12.050581 139774417409792 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.2271676063537598, loss=2.8809969425201416
I0203 03:35:58.245405 139774434195200 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.2325419187545776, loss=2.8791394233703613
I0203 03:36:16.000089 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:36:26.626202 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 03:37:03.261498 139936116377408 spec.py:349] Evaluating on the test split.
I0203 03:37:04.875818 139936116377408 submission_runner.py:408] Time since start: 52675.03s, 	Step: 102640, 	{'train/accuracy': 0.5785741806030273, 'train/loss': 1.8186800479888916, 'validation/accuracy': 0.5371400117874146, 'validation/loss': 2.024113655090332, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.689002752304077, 'test/num_examples': 10000, 'score': 47091.384001493454, 'total_duration': 52675.02685260773, 'accumulated_submission_time': 47091.384001493454, 'accumulated_eval_time': 5574.214259624481, 'accumulated_logging_time': 4.048406600952148}
I0203 03:37:04.914392 139774417409792 logging_writer.py:48] [102640] accumulated_eval_time=5574.214260, accumulated_logging_time=4.048407, accumulated_submission_time=47091.384001, global_step=102640, preemption_count=0, score=47091.384001, test/accuracy=0.422700, test/loss=2.689003, test/num_examples=10000, total_duration=52675.026853, train/accuracy=0.578574, train/loss=1.818680, validation/accuracy=0.537140, validation/loss=2.024114, validation/num_examples=50000
I0203 03:37:29.166830 139774434195200 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.2614452838897705, loss=3.2480218410491943
I0203 03:38:14.705268 139774417409792 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.8837617635726929, loss=5.411154747009277
I0203 03:39:00.946728 139774434195200 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.9373168349266052, loss=5.370173454284668
I0203 03:39:47.261488 139774417409792 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.9589336514472961, loss=5.211374282836914
I0203 03:40:33.582676 139774434195200 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.268285870552063, loss=2.8808889389038086
I0203 03:41:19.817284 139774417409792 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.1404039859771729, loss=4.306111812591553
I0203 03:42:06.529403 139774434195200 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.2809053659439087, loss=2.8066799640655518
I0203 03:42:52.715978 139774417409792 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.1589819192886353, loss=3.013192653656006
I0203 03:43:38.816443 139774434195200 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.9837534427642822, loss=3.6796865463256836
I0203 03:44:04.926071 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:44:15.350716 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 03:44:52.213995 139936116377408 spec.py:349] Evaluating on the test split.
I0203 03:44:53.819814 139936116377408 submission_runner.py:408] Time since start: 53143.97s, 	Step: 103558, 	{'train/accuracy': 0.5989648103713989, 'train/loss': 1.7452768087387085, 'validation/accuracy': 0.5286200046539307, 'validation/loss': 2.0765881538391113, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.7206692695617676, 'test/num_examples': 10000, 'score': 47511.33321595192, 'total_duration': 53143.97084522247, 'accumulated_submission_time': 47511.33321595192, 'accumulated_eval_time': 5623.107983827591, 'accumulated_logging_time': 4.101131916046143}
I0203 03:44:53.852662 139774417409792 logging_writer.py:48] [103558] accumulated_eval_time=5623.107984, accumulated_logging_time=4.101132, accumulated_submission_time=47511.333216, global_step=103558, preemption_count=0, score=47511.333216, test/accuracy=0.418800, test/loss=2.720669, test/num_examples=10000, total_duration=53143.970845, train/accuracy=0.598965, train/loss=1.745277, validation/accuracy=0.528620, validation/loss=2.076588, validation/num_examples=50000
I0203 03:45:10.955515 139774434195200 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.3987922668457031, loss=2.7621071338653564
I0203 03:45:55.612405 139774417409792 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.1154834032058716, loss=2.8148369789123535
I0203 03:46:41.690376 139774434195200 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.0858209133148193, loss=5.23593282699585
I0203 03:47:28.336884 139774417409792 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.0091739892959595, loss=4.651586055755615
I0203 03:48:14.560298 139774434195200 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.6202824115753174, loss=2.8831417560577393
I0203 03:49:00.765138 139774417409792 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.1778734922409058, loss=3.174377918243408
I0203 03:49:47.031053 139774434195200 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.0835199356079102, loss=5.1456685066223145
I0203 03:50:33.105780 139774417409792 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.2547229528427124, loss=2.8846323490142822
I0203 03:51:19.356431 139774434195200 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.1789608001708984, loss=2.7017617225646973
I0203 03:51:54.142397 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:52:04.970437 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 03:52:41.632593 139936116377408 spec.py:349] Evaluating on the test split.
I0203 03:52:43.250733 139936116377408 submission_runner.py:408] Time since start: 53613.40s, 	Step: 104477, 	{'train/accuracy': 0.5718554854393005, 'train/loss': 1.8921626806259155, 'validation/accuracy': 0.5343199968338013, 'validation/loss': 2.0755221843719482, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.7130990028381348, 'test/num_examples': 10000, 'score': 47931.566383600235, 'total_duration': 53613.40177822113, 'accumulated_submission_time': 47931.566383600235, 'accumulated_eval_time': 5672.216294765472, 'accumulated_logging_time': 4.143033742904663}
I0203 03:52:43.287203 139774417409792 logging_writer.py:48] [104477] accumulated_eval_time=5672.216295, accumulated_logging_time=4.143034, accumulated_submission_time=47931.566384, global_step=104477, preemption_count=0, score=47931.566384, test/accuracy=0.422800, test/loss=2.713099, test/num_examples=10000, total_duration=53613.401778, train/accuracy=0.571855, train/loss=1.892163, validation/accuracy=0.534320, validation/loss=2.075522, validation/num_examples=50000
I0203 03:52:52.831181 139774434195200 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.3395370244979858, loss=2.7562742233276367
I0203 03:53:36.184919 139774417409792 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.1085773706436157, loss=3.7372632026672363
I0203 03:54:22.307482 139774434195200 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.303056001663208, loss=2.90822696685791
I0203 03:55:08.787676 139774417409792 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.0716108083724976, loss=3.8611114025115967
I0203 03:55:55.037735 139774434195200 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.2122716903686523, loss=2.8643274307250977
I0203 03:56:41.374922 139774417409792 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.2679715156555176, loss=2.8521056175231934
I0203 03:57:28.157143 139774434195200 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.2825753688812256, loss=2.867842674255371
I0203 03:58:14.738131 139774417409792 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.937961220741272, loss=4.680741786956787
I0203 03:59:01.269743 139774434195200 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.2352174520492554, loss=2.8874635696411133
I0203 03:59:43.706880 139936116377408 spec.py:321] Evaluating on the training split.
I0203 03:59:54.244112 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:00:33.637509 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:00:35.250181 139936116377408 submission_runner.py:408] Time since start: 54085.40s, 	Step: 105394, 	{'train/accuracy': 0.5820507407188416, 'train/loss': 1.8374958038330078, 'validation/accuracy': 0.5375800132751465, 'validation/loss': 2.045927047729492, 'validation/num_examples': 50000, 'test/accuracy': 0.4191000163555145, 'test/loss': 2.7071774005889893, 'test/num_examples': 10000, 'score': 48351.92719125748, 'total_duration': 54085.40121340752, 'accumulated_submission_time': 48351.92719125748, 'accumulated_eval_time': 5723.759583473206, 'accumulated_logging_time': 4.190583944320679}
I0203 04:00:35.289597 139774417409792 logging_writer.py:48] [105394] accumulated_eval_time=5723.759583, accumulated_logging_time=4.190584, accumulated_submission_time=48351.927191, global_step=105394, preemption_count=0, score=48351.927191, test/accuracy=0.419100, test/loss=2.707177, test/num_examples=10000, total_duration=54085.401213, train/accuracy=0.582051, train/loss=1.837496, validation/accuracy=0.537580, validation/loss=2.045927, validation/num_examples=50000
I0203 04:00:38.072806 139774434195200 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.0193320512771606, loss=3.733362913131714
I0203 04:01:20.560537 139774417409792 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.2361661195755005, loss=2.9178805351257324
I0203 04:02:06.570547 139774434195200 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.0434678792953491, loss=4.64080810546875
I0203 04:02:53.184735 139774417409792 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.2516494989395142, loss=3.1650500297546387
I0203 04:03:39.227750 139774434195200 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.2783761024475098, loss=2.757434129714966
I0203 04:04:25.474423 139774417409792 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.0783963203430176, loss=4.2665863037109375
I0203 04:05:11.684902 139774434195200 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.0601348876953125, loss=4.214995861053467
I0203 04:05:58.020347 139774417409792 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.1914780139923096, loss=3.140303134918213
I0203 04:06:44.429588 139774434195200 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.3016823530197144, loss=2.814723014831543
I0203 04:07:30.691015 139774417409792 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.1706417798995972, loss=3.0862224102020264
I0203 04:07:35.427653 139936116377408 spec.py:321] Evaluating on the training split.
I0203 04:07:45.734025 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:08:24.632215 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:08:26.238444 139936116377408 submission_runner.py:408] Time since start: 54556.39s, 	Step: 106312, 	{'train/accuracy': 0.5904687643051147, 'train/loss': 1.7310149669647217, 'validation/accuracy': 0.5409199595451355, 'validation/loss': 1.9848403930664062, 'validation/num_examples': 50000, 'test/accuracy': 0.4317000210285187, 'test/loss': 2.6487910747528076, 'test/num_examples': 10000, 'score': 48772.00656723976, 'total_duration': 54556.38950634003, 'accumulated_submission_time': 48772.00656723976, 'accumulated_eval_time': 5774.5703637599945, 'accumulated_logging_time': 4.240931749343872}
I0203 04:08:26.273330 139774434195200 logging_writer.py:48] [106312] accumulated_eval_time=5774.570364, accumulated_logging_time=4.240932, accumulated_submission_time=48772.006567, global_step=106312, preemption_count=0, score=48772.006567, test/accuracy=0.431700, test/loss=2.648791, test/num_examples=10000, total_duration=54556.389506, train/accuracy=0.590469, train/loss=1.731015, validation/accuracy=0.540920, validation/loss=1.984840, validation/num_examples=50000
I0203 04:09:03.308016 139774417409792 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.1293599605560303, loss=5.386484146118164
I0203 04:09:48.899901 139774434195200 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.2670434713363647, loss=2.854090690612793
I0203 04:10:35.412481 139774417409792 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.320427656173706, loss=2.8620219230651855
I0203 04:11:21.906889 139774434195200 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.2959665060043335, loss=2.8421905040740967
I0203 04:12:08.511831 139774417409792 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.149717926979065, loss=3.409346103668213
I0203 04:12:54.761393 139774434195200 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.9499329328536987, loss=5.2535810470581055
I0203 04:13:40.899691 139774417409792 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.1165494918823242, loss=2.9344568252563477
I0203 04:14:27.501559 139774434195200 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.1545541286468506, loss=4.301124095916748
I0203 04:15:14.069480 139774417409792 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.0355955362319946, loss=3.938019275665283
I0203 04:15:26.383369 139936116377408 spec.py:321] Evaluating on the training split.
I0203 04:15:36.875377 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:16:13.681227 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:16:15.288913 139936116377408 submission_runner.py:408] Time since start: 55025.44s, 	Step: 107228, 	{'train/accuracy': 0.5759570002555847, 'train/loss': 1.833008050918579, 'validation/accuracy': 0.537880003452301, 'validation/loss': 2.0171401500701904, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.690009593963623, 'test/num_examples': 10000, 'score': 49192.05952215195, 'total_duration': 55025.439949035645, 'accumulated_submission_time': 49192.05952215195, 'accumulated_eval_time': 5823.475866556168, 'accumulated_logging_time': 4.285040616989136}
I0203 04:16:15.320312 139774434195200 logging_writer.py:48] [107228] accumulated_eval_time=5823.475867, accumulated_logging_time=4.285041, accumulated_submission_time=49192.059522, global_step=107228, preemption_count=0, score=49192.059522, test/accuracy=0.425100, test/loss=2.690010, test/num_examples=10000, total_duration=55025.439949, train/accuracy=0.575957, train/loss=1.833008, validation/accuracy=0.537880, validation/loss=2.017140, validation/num_examples=50000
I0203 04:16:44.860817 139774417409792 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.9594739079475403, loss=4.956927299499512
I0203 04:17:30.462126 139774434195200 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.1775261163711548, loss=3.1087186336517334
I0203 04:18:17.183163 139774417409792 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.3139806985855103, loss=2.861393928527832
I0203 04:19:03.812602 139774434195200 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.1995882987976074, loss=2.8461010456085205
I0203 04:19:49.814608 139774417409792 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.111349105834961, loss=3.407409906387329
I0203 04:20:36.215619 139774434195200 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.2200980186462402, loss=5.545695781707764
I0203 04:21:22.427701 139774417409792 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.8540976047515869, loss=4.988098621368408
I0203 04:22:08.958840 139774434195200 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.2984044551849365, loss=2.844855546951294
I0203 04:22:55.084732 139774417409792 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.050531268119812, loss=3.5480642318725586
I0203 04:23:15.650804 139936116377408 spec.py:321] Evaluating on the training split.
I0203 04:23:26.446832 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:24:03.756559 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:24:05.367150 139936116377408 submission_runner.py:408] Time since start: 55495.52s, 	Step: 108146, 	{'train/accuracy': 0.5908398032188416, 'train/loss': 1.7584213018417358, 'validation/accuracy': 0.5523999929428101, 'validation/loss': 1.949994444847107, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.611959934234619, 'test/num_examples': 10000, 'score': 49612.333136081696, 'total_duration': 55495.518181562424, 'accumulated_submission_time': 49612.333136081696, 'accumulated_eval_time': 5873.19217467308, 'accumulated_logging_time': 4.326111793518066}
I0203 04:24:05.400492 139774434195200 logging_writer.py:48] [108146] accumulated_eval_time=5873.192175, accumulated_logging_time=4.326112, accumulated_submission_time=49612.333136, global_step=108146, preemption_count=0, score=49612.333136, test/accuracy=0.438800, test/loss=2.611960, test/num_examples=10000, total_duration=55495.518182, train/accuracy=0.590840, train/loss=1.758421, validation/accuracy=0.552400, validation/loss=1.949994, validation/num_examples=50000
I0203 04:24:27.264344 139774417409792 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.1550568342208862, loss=2.815079689025879
I0203 04:25:12.521118 139774434195200 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.1608645915985107, loss=2.70940899848938
I0203 04:25:59.078316 139774417409792 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.132672667503357, loss=3.7669196128845215
I0203 04:26:45.573236 139774434195200 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.5107872486114502, loss=3.138148307800293
I0203 04:27:32.028219 139774417409792 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.2910634279251099, loss=2.8517963886260986
I0203 04:28:18.462519 139774434195200 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.2421196699142456, loss=2.7844271659851074
I0203 04:29:04.917617 139774417409792 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.273503303527832, loss=2.7834858894348145
I0203 04:29:51.518415 139774434195200 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.2158557176589966, loss=2.95705509185791
I0203 04:30:38.059082 139774417409792 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.173639178276062, loss=3.083314895629883
I0203 04:31:05.508558 139936116377408 spec.py:321] Evaluating on the training split.
I0203 04:31:17.162621 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:31:53.663089 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:31:55.265433 139936116377408 submission_runner.py:408] Time since start: 55965.42s, 	Step: 109061, 	{'train/accuracy': 0.6046093702316284, 'train/loss': 1.7196390628814697, 'validation/accuracy': 0.5508399605751038, 'validation/loss': 1.968966007232666, 'validation/num_examples': 50000, 'test/accuracy': 0.43540000915527344, 'test/loss': 2.6368744373321533, 'test/num_examples': 10000, 'score': 50032.38190603256, 'total_duration': 55965.41650009155, 'accumulated_submission_time': 50032.38190603256, 'accumulated_eval_time': 5922.949047088623, 'accumulated_logging_time': 4.369261980056763}
I0203 04:31:55.297248 139774434195200 logging_writer.py:48] [109061] accumulated_eval_time=5922.949047, accumulated_logging_time=4.369262, accumulated_submission_time=50032.381906, global_step=109061, preemption_count=0, score=50032.381906, test/accuracy=0.435400, test/loss=2.636874, test/num_examples=10000, total_duration=55965.416500, train/accuracy=0.604609, train/loss=1.719639, validation/accuracy=0.550840, validation/loss=1.968966, validation/num_examples=50000
I0203 04:32:11.184425 139774417409792 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.2398015260696411, loss=2.76023006439209
I0203 04:32:55.274269 139774434195200 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.2488350868225098, loss=2.69923996925354
I0203 04:33:41.601343 139774417409792 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.066214680671692, loss=4.88057804107666
I0203 04:34:28.259239 139774434195200 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.2777067422866821, loss=2.8839619159698486
I0203 04:35:14.359027 139774417409792 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.2639918327331543, loss=2.680699110031128
I0203 04:36:00.346626 139774434195200 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.139567255973816, loss=5.412228584289551
I0203 04:36:46.513178 139774417409792 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.0622787475585938, loss=3.7176897525787354
I0203 04:37:32.875356 139774434195200 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.3189752101898193, loss=2.829172372817993
I0203 04:38:19.198131 139774417409792 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.098460078239441, loss=3.2325587272644043
I0203 04:38:55.685690 139936116377408 spec.py:321] Evaluating on the training split.
I0203 04:39:06.487038 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:39:43.039225 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:39:44.647430 139936116377408 submission_runner.py:408] Time since start: 56434.80s, 	Step: 109981, 	{'train/accuracy': 0.5870702862739563, 'train/loss': 1.7547787427902222, 'validation/accuracy': 0.5485199689865112, 'validation/loss': 1.9464657306671143, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.6147043704986572, 'test/num_examples': 10000, 'score': 50452.712889909744, 'total_duration': 56434.7984893322, 'accumulated_submission_time': 50452.712889909744, 'accumulated_eval_time': 5971.910776138306, 'accumulated_logging_time': 4.410284757614136}
I0203 04:39:44.681024 139774434195200 logging_writer.py:48] [109981] accumulated_eval_time=5971.910776, accumulated_logging_time=4.410285, accumulated_submission_time=50452.712890, global_step=109981, preemption_count=0, score=50452.712890, test/accuracy=0.437400, test/loss=2.614704, test/num_examples=10000, total_duration=56434.798489, train/accuracy=0.587070, train/loss=1.754779, validation/accuracy=0.548520, validation/loss=1.946466, validation/num_examples=50000
I0203 04:39:52.626353 139774417409792 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.1473361253738403, loss=3.317387819290161
I0203 04:40:36.244285 139774434195200 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.047211766242981, loss=3.347883462905884
I0203 04:41:22.745587 139774417409792 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.1336884498596191, loss=3.0391321182250977
I0203 04:42:09.648430 139774434195200 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.3406202793121338, loss=2.8193881511688232
I0203 04:42:56.075684 139774417409792 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.2988892793655396, loss=2.7770166397094727
I0203 04:43:42.546446 139774434195200 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.0041582584381104, loss=4.839522361755371
I0203 04:44:28.952086 139774417409792 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.1907413005828857, loss=2.709261655807495
I0203 04:45:15.307396 139774434195200 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.4104198217391968, loss=2.780550479888916
I0203 04:46:01.548891 139774417409792 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.3045064210891724, loss=3.0757625102996826
I0203 04:46:45.017948 139936116377408 spec.py:321] Evaluating on the training split.
I0203 04:46:55.708291 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:47:34.463983 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:47:36.076322 139936116377408 submission_runner.py:408] Time since start: 56906.23s, 	Step: 110896, 	{'train/accuracy': 0.6010546684265137, 'train/loss': 1.6854904890060425, 'validation/accuracy': 0.556879997253418, 'validation/loss': 1.8993265628814697, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.5637691020965576, 'test/num_examples': 10000, 'score': 50872.9928958416, 'total_duration': 56906.227376937866, 'accumulated_submission_time': 50872.9928958416, 'accumulated_eval_time': 6022.969138383865, 'accumulated_logging_time': 4.453753709793091}
I0203 04:47:36.121678 139774434195200 logging_writer.py:48] [110896] accumulated_eval_time=6022.969138, accumulated_logging_time=4.453754, accumulated_submission_time=50872.992896, global_step=110896, preemption_count=0, score=50872.992896, test/accuracy=0.443800, test/loss=2.563769, test/num_examples=10000, total_duration=56906.227377, train/accuracy=0.601055, train/loss=1.685490, validation/accuracy=0.556880, validation/loss=1.899327, validation/num_examples=50000
I0203 04:47:38.111704 139774417409792 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.9786993265151978, loss=5.358777046203613
I0203 04:48:20.548097 139774434195200 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.063563346862793, loss=3.099452257156372
I0203 04:49:06.529094 139774417409792 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.0058865547180176, loss=3.9149813652038574
I0203 04:49:52.701869 139774434195200 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.0080541372299194, loss=4.239709854125977
I0203 04:50:38.890002 139774417409792 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.304551601409912, loss=2.6784322261810303
I0203 04:51:25.428622 139774434195200 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.0089173316955566, loss=4.598020553588867
I0203 04:52:12.028368 139774417409792 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.9906366467475891, loss=4.562961578369141
I0203 04:52:58.370931 139774434195200 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.2086998224258423, loss=2.9033005237579346
I0203 04:53:44.759346 139774417409792 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.3622289896011353, loss=2.7150936126708984
I0203 04:54:31.317449 139774434195200 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.2535536289215088, loss=2.929840087890625
I0203 04:54:36.425875 139936116377408 spec.py:321] Evaluating on the training split.
I0203 04:54:46.907225 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 04:55:25.198751 139936116377408 spec.py:349] Evaluating on the test split.
I0203 04:55:26.806363 139936116377408 submission_runner.py:408] Time since start: 57376.96s, 	Step: 111813, 	{'train/accuracy': 0.606640636920929, 'train/loss': 1.6578865051269531, 'validation/accuracy': 0.5602999925613403, 'validation/loss': 1.8957290649414062, 'validation/num_examples': 50000, 'test/accuracy': 0.4487000107765198, 'test/loss': 2.552631378173828, 'test/num_examples': 10000, 'score': 51293.23732328415, 'total_duration': 57376.957426548004, 'accumulated_submission_time': 51293.23732328415, 'accumulated_eval_time': 6073.349613189697, 'accumulated_logging_time': 4.510779142379761}
I0203 04:55:26.841317 139774417409792 logging_writer.py:48] [111813] accumulated_eval_time=6073.349613, accumulated_logging_time=4.510779, accumulated_submission_time=51293.237323, global_step=111813, preemption_count=0, score=51293.237323, test/accuracy=0.448700, test/loss=2.552631, test/num_examples=10000, total_duration=57376.957427, train/accuracy=0.606641, train/loss=1.657887, validation/accuracy=0.560300, validation/loss=1.895729, validation/num_examples=50000
I0203 04:56:03.431962 139774434195200 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.1202179193496704, loss=3.214033603668213
I0203 04:56:49.393967 139774417409792 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.9753631353378296, loss=4.833217144012451
I0203 04:57:35.837621 139774434195200 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.025683045387268, loss=4.458942890167236
I0203 04:58:22.004951 139774417409792 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.4140924215316772, loss=2.768559217453003
I0203 04:59:08.291174 139774434195200 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.1858792304992676, loss=3.1981003284454346
I0203 04:59:54.448432 139774417409792 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.133746862411499, loss=2.866878032684326
I0203 05:00:40.677386 139774434195200 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.3953860998153687, loss=2.855827808380127
I0203 05:01:27.265207 139774417409792 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.2391602993011475, loss=3.609571933746338
I0203 05:02:13.772849 139774434195200 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.2260982990264893, loss=3.1086204051971436
I0203 05:02:27.243783 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:02:37.775768 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:03:14.673016 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:03:16.273131 139936116377408 submission_runner.py:408] Time since start: 57846.42s, 	Step: 112731, 	{'train/accuracy': 0.6123046875, 'train/loss': 1.6747193336486816, 'validation/accuracy': 0.5610399842262268, 'validation/loss': 1.8994790315628052, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.5727462768554688, 'test/num_examples': 10000, 'score': 51713.58249878883, 'total_duration': 57846.424187898636, 'accumulated_submission_time': 51713.58249878883, 'accumulated_eval_time': 6122.37894487381, 'accumulated_logging_time': 4.555319547653198}
I0203 05:03:16.308388 139774417409792 logging_writer.py:48] [112731] accumulated_eval_time=6122.378945, accumulated_logging_time=4.555320, accumulated_submission_time=51713.582499, global_step=112731, preemption_count=0, score=51713.582499, test/accuracy=0.441000, test/loss=2.572746, test/num_examples=10000, total_duration=57846.424188, train/accuracy=0.612305, train/loss=1.674719, validation/accuracy=0.561040, validation/loss=1.899479, validation/num_examples=50000
I0203 05:03:44.552629 139774434195200 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.4300075769424438, loss=2.8920676708221436
I0203 05:04:30.670289 139774417409792 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.2701295614242554, loss=2.8746602535247803
I0203 05:05:17.080502 139774434195200 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.190285325050354, loss=2.646507978439331
I0203 05:06:03.835052 139774417409792 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.2725261449813843, loss=2.7248446941375732
I0203 05:06:49.840607 139774434195200 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.2494635581970215, loss=2.6107115745544434
I0203 05:07:36.280125 139774417409792 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.3495666980743408, loss=2.8226113319396973
I0203 05:08:22.768535 139774434195200 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.261255145072937, loss=4.93078088760376
I0203 05:09:09.076048 139774417409792 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.1287672519683838, loss=3.2738773822784424
I0203 05:09:55.495084 139774434195200 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.217926263809204, loss=2.8906915187835693
I0203 05:10:16.645823 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:10:27.042321 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:11:05.530598 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:11:07.129197 139936116377408 submission_runner.py:408] Time since start: 58317.28s, 	Step: 113647, 	{'train/accuracy': 0.6024609208106995, 'train/loss': 1.6888262033462524, 'validation/accuracy': 0.5626800060272217, 'validation/loss': 1.876355528831482, 'validation/num_examples': 50000, 'test/accuracy': 0.44460001587867737, 'test/loss': 2.5511436462402344, 'test/num_examples': 10000, 'score': 52133.86265182495, 'total_duration': 58317.28026175499, 'accumulated_submission_time': 52133.86265182495, 'accumulated_eval_time': 6172.862322568893, 'accumulated_logging_time': 4.599631071090698}
I0203 05:11:07.164442 139774417409792 logging_writer.py:48] [113647] accumulated_eval_time=6172.862323, accumulated_logging_time=4.599631, accumulated_submission_time=52133.862652, global_step=113647, preemption_count=0, score=52133.862652, test/accuracy=0.444600, test/loss=2.551144, test/num_examples=10000, total_duration=58317.280262, train/accuracy=0.602461, train/loss=1.688826, validation/accuracy=0.562680, validation/loss=1.876356, validation/num_examples=50000
I0203 05:11:28.620235 139774434195200 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.2938683032989502, loss=2.7360668182373047
I0203 05:12:14.272340 139774417409792 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.4164094924926758, loss=2.77339506149292
I0203 05:13:00.678198 139774434195200 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.2617661952972412, loss=2.866917133331299
I0203 05:13:47.055768 139774417409792 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.0596323013305664, loss=3.766732692718506
I0203 05:14:33.225940 139774434195200 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.2632153034210205, loss=2.6792798042297363
I0203 05:15:19.413230 139774417409792 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.310912013053894, loss=2.7081339359283447
I0203 05:16:05.798452 139774434195200 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.23222815990448, loss=2.5940449237823486
I0203 05:16:52.064471 139774417409792 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.1398965120315552, loss=3.368394613265991
I0203 05:17:37.950717 139774434195200 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.3368970155715942, loss=2.7242465019226074
I0203 05:18:07.369652 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:18:18.182988 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:18:54.017266 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:18:55.623312 139936116377408 submission_runner.py:408] Time since start: 58785.77s, 	Step: 114565, 	{'train/accuracy': 0.5999413728713989, 'train/loss': 1.7239134311676025, 'validation/accuracy': 0.5591199994087219, 'validation/loss': 1.9196312427520752, 'validation/num_examples': 50000, 'test/accuracy': 0.4391000270843506, 'test/loss': 2.578800678253174, 'test/num_examples': 10000, 'score': 52554.008450984955, 'total_duration': 58785.77437710762, 'accumulated_submission_time': 52554.008450984955, 'accumulated_eval_time': 6221.115981340408, 'accumulated_logging_time': 4.645940542221069}
I0203 05:18:55.659377 139774417409792 logging_writer.py:48] [114565] accumulated_eval_time=6221.115981, accumulated_logging_time=4.645941, accumulated_submission_time=52554.008451, global_step=114565, preemption_count=0, score=52554.008451, test/accuracy=0.439100, test/loss=2.578801, test/num_examples=10000, total_duration=58785.774377, train/accuracy=0.599941, train/loss=1.723913, validation/accuracy=0.559120, validation/loss=1.919631, validation/num_examples=50000
I0203 05:19:09.958290 139774434195200 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.3007991313934326, loss=2.9523115158081055
I0203 05:19:53.720021 139774417409792 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.374594807624817, loss=2.7477290630340576
I0203 05:20:40.247951 139774434195200 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.180171251296997, loss=5.294642925262451
I0203 05:21:26.856653 139774417409792 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.234926462173462, loss=3.110816240310669
I0203 05:22:13.148430 139774434195200 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.1030877828598022, loss=3.331799268722534
I0203 05:22:59.705775 139774417409792 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.4021553993225098, loss=2.6613078117370605
I0203 05:23:45.925728 139774434195200 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.0726674795150757, loss=3.789771556854248
I0203 05:24:32.082963 139774417409792 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.3925544023513794, loss=2.7441823482513428
I0203 05:25:18.273696 139774434195200 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.4511137008666992, loss=2.657740592956543
I0203 05:25:55.801985 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:26:06.644552 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:26:43.559806 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:26:45.173012 139936116377408 submission_runner.py:408] Time since start: 59255.32s, 	Step: 115483, 	{'train/accuracy': 0.6393163800239563, 'train/loss': 1.5017653703689575, 'validation/accuracy': 0.5700199604034424, 'validation/loss': 1.8257381916046143, 'validation/num_examples': 50000, 'test/accuracy': 0.4597000181674957, 'test/loss': 2.50107741355896, 'test/num_examples': 10000, 'score': 52974.09374523163, 'total_duration': 59255.32407426834, 'accumulated_submission_time': 52974.09374523163, 'accumulated_eval_time': 6270.487004041672, 'accumulated_logging_time': 4.691897869110107}
I0203 05:26:45.207477 139774417409792 logging_writer.py:48] [115483] accumulated_eval_time=6270.487004, accumulated_logging_time=4.691898, accumulated_submission_time=52974.093745, global_step=115483, preemption_count=0, score=52974.093745, test/accuracy=0.459700, test/loss=2.501077, test/num_examples=10000, total_duration=59255.324074, train/accuracy=0.639316, train/loss=1.501765, validation/accuracy=0.570020, validation/loss=1.825738, validation/num_examples=50000
I0203 05:26:52.343566 139774434195200 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.3895975351333618, loss=2.7467470169067383
I0203 05:27:35.337567 139774417409792 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.232822060585022, loss=2.8213412761688232
I0203 05:28:21.489552 139774434195200 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.0733362436294556, loss=3.7561447620391846
I0203 05:29:07.881627 139774417409792 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.3463900089263916, loss=2.662551164627075
I0203 05:29:54.014450 139774434195200 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.1786662340164185, loss=5.200047492980957
I0203 05:30:40.424271 139774417409792 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.4711312055587769, loss=2.764376401901245
I0203 05:31:26.968823 139774434195200 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.2300212383270264, loss=4.2591400146484375
I0203 05:32:13.495933 139774417409792 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.5645028352737427, loss=2.8394997119903564
I0203 05:32:59.700394 139774434195200 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.2477080821990967, loss=2.7280354499816895
I0203 05:33:45.640713 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:33:56.336227 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:34:37.709699 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:34:39.314457 139936116377408 submission_runner.py:408] Time since start: 59729.47s, 	Step: 116400, 	{'train/accuracy': 0.6116992235183716, 'train/loss': 1.6490353345870972, 'validation/accuracy': 0.5755999684333801, 'validation/loss': 1.843803882598877, 'validation/num_examples': 50000, 'test/accuracy': 0.4602000117301941, 'test/loss': 2.501185655593872, 'test/num_examples': 10000, 'score': 53394.46939063072, 'total_duration': 59729.46551704407, 'accumulated_submission_time': 53394.46939063072, 'accumulated_eval_time': 6324.160755395889, 'accumulated_logging_time': 4.736301422119141}
I0203 05:34:39.350365 139774417409792 logging_writer.py:48] [116400] accumulated_eval_time=6324.160755, accumulated_logging_time=4.736301, accumulated_submission_time=53394.469391, global_step=116400, preemption_count=0, score=53394.469391, test/accuracy=0.460200, test/loss=2.501186, test/num_examples=10000, total_duration=59729.465517, train/accuracy=0.611699, train/loss=1.649035, validation/accuracy=0.575600, validation/loss=1.843804, validation/num_examples=50000
I0203 05:34:39.750962 139774434195200 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.4901093244552612, loss=3.0000648498535156
I0203 05:35:21.606027 139774417409792 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.1624562740325928, loss=3.7815022468566895
I0203 05:36:07.677399 139774434195200 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.389491319656372, loss=2.6548662185668945
I0203 05:36:53.872312 139774417409792 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.3024157285690308, loss=2.6560590267181396
I0203 05:37:40.077453 139774434195200 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.384381651878357, loss=2.6570205688476562
I0203 05:38:26.242018 139774417409792 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.231145977973938, loss=2.759575843811035
I0203 05:39:12.439430 139774434195200 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.354866862297058, loss=2.6889467239379883
I0203 05:39:58.732533 139774417409792 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.3149700164794922, loss=2.8135557174682617
I0203 05:40:45.263121 139774434195200 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.1314305067062378, loss=3.7008724212646484
I0203 05:41:31.392733 139774417409792 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.3520416021347046, loss=2.6145665645599365
I0203 05:41:39.612310 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:41:50.467470 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:42:29.669590 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:42:31.289721 139936116377408 submission_runner.py:408] Time since start: 60201.44s, 	Step: 117319, 	{'train/accuracy': 0.6172265410423279, 'train/loss': 1.627258539199829, 'validation/accuracy': 0.5745399594306946, 'validation/loss': 1.8362805843353271, 'validation/num_examples': 50000, 'test/accuracy': 0.45510002970695496, 'test/loss': 2.4943559169769287, 'test/num_examples': 10000, 'score': 53814.67329096794, 'total_duration': 60201.44076490402, 'accumulated_submission_time': 53814.67329096794, 'accumulated_eval_time': 6375.838165998459, 'accumulated_logging_time': 4.7819108963012695}
I0203 05:42:31.331691 139774434195200 logging_writer.py:48] [117319] accumulated_eval_time=6375.838166, accumulated_logging_time=4.781911, accumulated_submission_time=53814.673291, global_step=117319, preemption_count=0, score=53814.673291, test/accuracy=0.455100, test/loss=2.494356, test/num_examples=10000, total_duration=60201.440765, train/accuracy=0.617227, train/loss=1.627259, validation/accuracy=0.574540, validation/loss=1.836281, validation/num_examples=50000
I0203 05:43:05.115433 139774417409792 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.2856262922286987, loss=5.301509857177734
I0203 05:43:50.873448 139774434195200 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.1623119115829468, loss=4.875555992126465
I0203 05:44:37.436911 139774417409792 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.1075570583343506, loss=4.805809497833252
I0203 05:45:23.803909 139774434195200 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.3477634191513062, loss=2.5751872062683105
I0203 05:46:10.511524 139774417409792 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.4065473079681396, loss=2.7039031982421875
I0203 05:46:56.456390 139774434195200 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.1093915700912476, loss=4.8545331954956055
I0203 05:47:42.797761 139774417409792 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.0302739143371582, loss=5.2268266677856445
I0203 05:48:28.959527 139774434195200 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.1593645811080933, loss=4.7395782470703125
I0203 05:49:15.513266 139774417409792 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.1242997646331787, loss=3.65563702583313
I0203 05:49:31.569957 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:49:42.250783 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:50:19.819837 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:50:21.427406 139936116377408 submission_runner.py:408] Time since start: 60671.58s, 	Step: 118236, 	{'train/accuracy': 0.6234374642372131, 'train/loss': 1.591255784034729, 'validation/accuracy': 0.5689799785614014, 'validation/loss': 1.8540898561477661, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.519318103790283, 'test/num_examples': 10000, 'score': 54234.853268146515, 'total_duration': 60671.578468084335, 'accumulated_submission_time': 54234.853268146515, 'accumulated_eval_time': 6425.695611715317, 'accumulated_logging_time': 4.8346474170684814}
I0203 05:50:21.463902 139774434195200 logging_writer.py:48] [118236] accumulated_eval_time=6425.695612, accumulated_logging_time=4.834647, accumulated_submission_time=54234.853268, global_step=118236, preemption_count=0, score=54234.853268, test/accuracy=0.459300, test/loss=2.519318, test/num_examples=10000, total_duration=60671.578468, train/accuracy=0.623437, train/loss=1.591256, validation/accuracy=0.568980, validation/loss=1.854090, validation/num_examples=50000
I0203 05:50:47.517802 139774417409792 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.159940242767334, loss=3.503107786178589
I0203 05:51:33.536567 139774434195200 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.4212428331375122, loss=2.6948037147521973
I0203 05:52:20.247203 139774417409792 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.0592275857925415, loss=3.957275152206421
I0203 05:53:06.728474 139774434195200 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.2728064060211182, loss=3.00766658782959
I0203 05:53:53.164906 139774417409792 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.5191363096237183, loss=2.668546438217163
I0203 05:54:39.736293 139774434195200 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.2559599876403809, loss=2.981983184814453
I0203 05:55:26.399384 139774417409792 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.415308952331543, loss=2.548574447631836
I0203 05:56:12.820620 139774434195200 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.3494607210159302, loss=2.9102134704589844
I0203 05:56:59.239715 139774417409792 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.1112486124038696, loss=3.385044813156128
I0203 05:57:21.791094 139936116377408 spec.py:321] Evaluating on the training split.
I0203 05:57:32.259814 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 05:58:10.308496 139936116377408 spec.py:349] Evaluating on the test split.
I0203 05:58:11.910343 139936116377408 submission_runner.py:408] Time since start: 61142.06s, 	Step: 119150, 	{'train/accuracy': 0.6201952695846558, 'train/loss': 1.594254732131958, 'validation/accuracy': 0.5825200080871582, 'validation/loss': 1.778774380683899, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.4543488025665283, 'test/num_examples': 10000, 'score': 54655.12280344963, 'total_duration': 61142.06141138077, 'accumulated_submission_time': 54655.12280344963, 'accumulated_eval_time': 6475.814856529236, 'accumulated_logging_time': 4.881854772567749}
I0203 05:58:11.945546 139774434195200 logging_writer.py:48] [119150] accumulated_eval_time=6475.814857, accumulated_logging_time=4.881855, accumulated_submission_time=54655.122803, global_step=119150, preemption_count=0, score=54655.122803, test/accuracy=0.462200, test/loss=2.454349, test/num_examples=10000, total_duration=61142.061411, train/accuracy=0.620195, train/loss=1.594255, validation/accuracy=0.582520, validation/loss=1.778774, validation/num_examples=50000
I0203 05:58:32.190361 139774417409792 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.0881738662719727, loss=4.2263102531433105
I0203 05:59:16.865558 139774434195200 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.3383963108062744, loss=2.616736888885498
I0203 06:00:03.309011 139774417409792 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.086999773979187, loss=3.90252685546875
I0203 06:00:49.726644 139774434195200 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.4313310384750366, loss=2.807821750640869
I0203 06:01:35.816190 139774417409792 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.1512569189071655, loss=3.876213788986206
I0203 06:02:22.384882 139774434195200 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.1088576316833496, loss=4.824132442474365
I0203 06:03:08.681148 139774417409792 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.134331464767456, loss=3.364413022994995
I0203 06:03:55.181232 139774434195200 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.3398278951644897, loss=2.555492401123047
I0203 06:04:41.187175 139774417409792 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.3486706018447876, loss=2.9297595024108887
I0203 06:05:12.318374 139936116377408 spec.py:321] Evaluating on the training split.
I0203 06:05:23.327739 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 06:05:58.310391 139936116377408 spec.py:349] Evaluating on the test split.
I0203 06:05:59.919118 139936116377408 submission_runner.py:408] Time since start: 61610.07s, 	Step: 120069, 	{'train/accuracy': 0.6280273199081421, 'train/loss': 1.5842721462249756, 'validation/accuracy': 0.5796799659729004, 'validation/loss': 1.7919082641601562, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.4546687602996826, 'test/num_examples': 10000, 'score': 55075.437239170074, 'total_duration': 61610.07016038895, 'accumulated_submission_time': 55075.437239170074, 'accumulated_eval_time': 6523.415584564209, 'accumulated_logging_time': 4.927415132522583}
I0203 06:05:59.958073 139774434195200 logging_writer.py:48] [120069] accumulated_eval_time=6523.415585, accumulated_logging_time=4.927415, accumulated_submission_time=55075.437239, global_step=120069, preemption_count=0, score=55075.437239, test/accuracy=0.464300, test/loss=2.454669, test/num_examples=10000, total_duration=61610.070160, train/accuracy=0.628027, train/loss=1.584272, validation/accuracy=0.579680, validation/loss=1.791908, validation/num_examples=50000
I0203 06:06:12.667955 139774417409792 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.5330944061279297, loss=2.729846954345703
I0203 06:06:56.545233 139774434195200 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.2491135597229004, loss=2.8530805110931396
I0203 06:07:43.005225 139774417409792 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.1705657243728638, loss=3.193021535873413
I0203 06:08:29.370162 139774434195200 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.4049887657165527, loss=2.7247910499572754
I0203 06:09:15.708789 139774417409792 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.2884167432785034, loss=2.714991569519043
I0203 06:10:01.932696 139774434195200 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.469169020652771, loss=2.627467155456543
I0203 06:10:48.145398 139774417409792 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.3473563194274902, loss=2.600926399230957
I0203 06:11:34.398727 139774434195200 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.2135882377624512, loss=3.771979808807373
I0203 06:12:20.816367 139774417409792 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.4513798952102661, loss=2.8786773681640625
I0203 06:13:00.279635 139936116377408 spec.py:321] Evaluating on the training split.
I0203 06:13:11.229157 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 06:13:48.631438 139936116377408 spec.py:349] Evaluating on the test split.
I0203 06:13:50.228571 139936116377408 submission_runner.py:408] Time since start: 62080.38s, 	Step: 120987, 	{'train/accuracy': 0.6349999904632568, 'train/loss': 1.535889744758606, 'validation/accuracy': 0.584879994392395, 'validation/loss': 1.7815570831298828, 'validation/num_examples': 50000, 'test/accuracy': 0.4634000360965729, 'test/loss': 2.4435477256774902, 'test/num_examples': 10000, 'score': 55495.7010948658, 'total_duration': 62080.379637002945, 'accumulated_submission_time': 55495.7010948658, 'accumulated_eval_time': 6573.364508867264, 'accumulated_logging_time': 4.976737022399902}
I0203 06:13:50.270698 139774434195200 logging_writer.py:48] [120987] accumulated_eval_time=6573.364509, accumulated_logging_time=4.976737, accumulated_submission_time=55495.701095, global_step=120987, preemption_count=0, score=55495.701095, test/accuracy=0.463400, test/loss=2.443548, test/num_examples=10000, total_duration=62080.379637, train/accuracy=0.635000, train/loss=1.535890, validation/accuracy=0.584880, validation/loss=1.781557, validation/num_examples=50000
I0203 06:13:55.823878 139774417409792 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.22421395778656, loss=4.520016193389893
I0203 06:14:38.726728 139774434195200 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.4349225759506226, loss=2.6079769134521484
I0203 06:15:25.109789 139774417409792 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.1008104085922241, loss=4.397360801696777
I0203 06:16:11.680681 139774434195200 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.3028771877288818, loss=3.1402530670166016
I0203 06:16:58.138079 139774417409792 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.3150672912597656, loss=4.351728439331055
I0203 06:17:44.698402 139774434195200 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.323195457458496, loss=2.5217056274414062
I0203 06:18:30.947844 139774417409792 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.5511751174926758, loss=2.62817645072937
I0203 06:19:17.221915 139774434195200 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.3336409330368042, loss=2.6356918811798096
I0203 06:20:03.524674 139774417409792 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.4033958911895752, loss=2.947695255279541
I0203 06:20:50.007804 139774434195200 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.5758976936340332, loss=2.670938014984131
I0203 06:20:50.613158 139936116377408 spec.py:321] Evaluating on the training split.
I0203 06:21:01.289779 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 06:21:37.344838 139936116377408 spec.py:349] Evaluating on the test split.
I0203 06:21:38.956656 139936116377408 submission_runner.py:408] Time since start: 62549.11s, 	Step: 121903, 	{'train/accuracy': 0.6355859041213989, 'train/loss': 1.5495035648345947, 'validation/accuracy': 0.5896599888801575, 'validation/loss': 1.7570979595184326, 'validation/num_examples': 50000, 'test/accuracy': 0.47040003538131714, 'test/loss': 2.42789888381958, 'test/num_examples': 10000, 'score': 55915.98540139198, 'total_duration': 62549.107691049576, 'accumulated_submission_time': 55915.98540139198, 'accumulated_eval_time': 6621.707966804504, 'accumulated_logging_time': 5.029550790786743}
I0203 06:21:38.995642 139774417409792 logging_writer.py:48] [121903] accumulated_eval_time=6621.707967, accumulated_logging_time=5.029551, accumulated_submission_time=55915.985401, global_step=121903, preemption_count=0, score=55915.985401, test/accuracy=0.470400, test/loss=2.427899, test/num_examples=10000, total_duration=62549.107691, train/accuracy=0.635586, train/loss=1.549504, validation/accuracy=0.589660, validation/loss=1.757098, validation/num_examples=50000
I0203 06:22:20.082927 139774434195200 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.4940263032913208, loss=2.6937248706817627
I0203 06:23:06.470040 139774417409792 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.3822596073150635, loss=2.602940559387207
I0203 06:23:52.656709 139774434195200 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.2474231719970703, loss=2.99397349357605
I0203 06:24:39.093123 139774417409792 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.3351938724517822, loss=3.0932939052581787
I0203 06:25:25.596850 139774434195200 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.1080317497253418, loss=4.456364154815674
I0203 06:26:11.971876 139774417409792 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.4539068937301636, loss=2.6359241008758545
I0203 06:26:58.414021 139774434195200 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.3679819107055664, loss=2.5335333347320557
I0203 06:27:44.715527 139774417409792 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.736250877380371, loss=2.5765762329101562
I0203 06:28:31.127445 139774434195200 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.3996201753616333, loss=2.6749396324157715
I0203 06:28:39.341901 139936116377408 spec.py:321] Evaluating on the training split.
I0203 06:28:49.862722 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 06:29:27.694304 139936116377408 spec.py:349] Evaluating on the test split.
I0203 06:29:29.297775 139936116377408 submission_runner.py:408] Time since start: 63019.45s, 	Step: 122819, 	{'train/accuracy': 0.6341015696525574, 'train/loss': 1.5348260402679443, 'validation/accuracy': 0.5896399617195129, 'validation/loss': 1.748339056968689, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.4274723529815674, 'test/num_examples': 10000, 'score': 56336.27511167526, 'total_duration': 63019.44883728027, 'accumulated_submission_time': 56336.27511167526, 'accumulated_eval_time': 6671.663818836212, 'accumulated_logging_time': 5.077637434005737}
I0203 06:29:29.333417 139774417409792 logging_writer.py:48] [122819] accumulated_eval_time=6671.663819, accumulated_logging_time=5.077637, accumulated_submission_time=56336.275112, global_step=122819, preemption_count=0, score=56336.275112, test/accuracy=0.469300, test/loss=2.427472, test/num_examples=10000, total_duration=63019.448837, train/accuracy=0.634102, train/loss=1.534826, validation/accuracy=0.589640, validation/loss=1.748339, validation/num_examples=50000
I0203 06:30:03.011875 139774434195200 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.2705918550491333, loss=2.8008100986480713
I0203 06:30:48.945806 139774417409792 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.217738151550293, loss=3.189771890640259
I0203 06:31:35.720358 139774434195200 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.2180681228637695, loss=3.7475390434265137
I0203 06:32:22.117810 139774417409792 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.350107192993164, loss=2.5917177200317383
I0203 06:33:08.256499 139774434195200 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.4340736865997314, loss=2.729194164276123
I0203 06:33:54.420783 139774417409792 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.4147173166275024, loss=2.5622382164001465
I0203 06:34:40.836453 139774434195200 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.3999357223510742, loss=2.686013698577881
I0203 06:35:27.029838 139774417409792 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.285791039466858, loss=4.739410400390625
I0203 06:36:13.434123 139774434195200 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.4280962944030762, loss=2.571352481842041
I0203 06:36:29.339984 139936116377408 spec.py:321] Evaluating on the training split.
I0203 06:36:39.888347 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 06:37:18.118875 139936116377408 spec.py:349] Evaluating on the test split.
I0203 06:37:19.728732 139936116377408 submission_runner.py:408] Time since start: 63489.88s, 	Step: 123736, 	{'train/accuracy': 0.6442577838897705, 'train/loss': 1.493449330329895, 'validation/accuracy': 0.594539999961853, 'validation/loss': 1.7315579652786255, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.365442991256714, 'test/num_examples': 10000, 'score': 56756.223601818085, 'total_duration': 63489.87978386879, 'accumulated_submission_time': 56756.223601818085, 'accumulated_eval_time': 6722.052555799484, 'accumulated_logging_time': 5.1239094734191895}
I0203 06:37:19.766277 139774417409792 logging_writer.py:48] [123736] accumulated_eval_time=6722.052556, accumulated_logging_time=5.123909, accumulated_submission_time=56756.223602, global_step=123736, preemption_count=0, score=56756.223602, test/accuracy=0.479200, test/loss=2.365443, test/num_examples=10000, total_duration=63489.879784, train/accuracy=0.644258, train/loss=1.493449, validation/accuracy=0.594540, validation/loss=1.731558, validation/num_examples=50000
I0203 06:37:45.650117 139774434195200 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.1703299283981323, loss=4.492271423339844
I0203 06:38:31.806462 139774417409792 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.2493362426757812, loss=4.132817268371582
I0203 06:39:18.098855 139774434195200 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.5102721452713013, loss=2.5626487731933594
I0203 06:40:04.703178 139774417409792 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.3803157806396484, loss=2.7263245582580566
I0203 06:40:50.907931 139774434195200 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.423285961151123, loss=2.6988182067871094
I0203 06:41:37.509064 139774417409792 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.4038044214248657, loss=2.624113082885742
I0203 06:42:23.714233 139774434195200 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.2546205520629883, loss=3.595313549041748
I0203 06:43:10.082771 139774417409792 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.1684622764587402, loss=3.772611618041992
I0203 06:43:56.219320 139774434195200 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.2307336330413818, loss=3.2928953170776367
I0203 06:44:19.912054 139936116377408 spec.py:321] Evaluating on the training split.
I0203 06:44:30.438150 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 06:45:09.729293 139936116377408 spec.py:349] Evaluating on the test split.
I0203 06:45:11.334848 139936116377408 submission_runner.py:408] Time since start: 63961.49s, 	Step: 124653, 	{'train/accuracy': 0.6554492115974426, 'train/loss': 1.4739404916763306, 'validation/accuracy': 0.5974400043487549, 'validation/loss': 1.7426960468292236, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.39933180809021, 'test/num_examples': 10000, 'score': 57176.31259250641, 'total_duration': 63961.48591351509, 'accumulated_submission_time': 57176.31259250641, 'accumulated_eval_time': 6773.475342273712, 'accumulated_logging_time': 5.171019077301025}
I0203 06:45:11.372792 139774417409792 logging_writer.py:48] [124653] accumulated_eval_time=6773.475342, accumulated_logging_time=5.171019, accumulated_submission_time=57176.312593, global_step=124653, preemption_count=0, score=57176.312593, test/accuracy=0.478000, test/loss=2.399332, test/num_examples=10000, total_duration=63961.485914, train/accuracy=0.655449, train/loss=1.473940, validation/accuracy=0.597440, validation/loss=1.742696, validation/num_examples=50000
I0203 06:45:30.568274 139774434195200 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.1558492183685303, loss=3.8397769927978516
I0203 06:46:15.736321 139774417409792 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.572559118270874, loss=2.51552152633667
I0203 06:47:01.749999 139774434195200 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.3999475240707397, loss=2.521327018737793
I0203 06:47:48.211187 139774417409792 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.472090482711792, loss=2.6518537998199463
I0203 06:48:34.770993 139774434195200 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.4115351438522339, loss=2.4811384677886963
I0203 06:49:21.363626 139774417409792 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.237199068069458, loss=4.195069313049316
I0203 06:50:07.728556 139774434195200 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.52278470993042, loss=2.7357711791992188
I0203 06:50:54.088484 139774417409792 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.4913365840911865, loss=2.4758362770080566
I0203 06:51:40.753888 139774434195200 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.464648962020874, loss=2.5911097526550293
I0203 06:52:11.641265 139936116377408 spec.py:321] Evaluating on the training split.
I0203 06:52:22.267076 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 06:52:58.662240 139936116377408 spec.py:349] Evaluating on the test split.
I0203 06:53:00.268352 139936116377408 submission_runner.py:408] Time since start: 64430.42s, 	Step: 125568, 	{'train/accuracy': 0.6419726610183716, 'train/loss': 1.5183041095733643, 'validation/accuracy': 0.5974000096321106, 'validation/loss': 1.7342219352722168, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.396895408630371, 'test/num_examples': 10000, 'score': 57596.52352762222, 'total_duration': 64430.41939020157, 'accumulated_submission_time': 57596.52352762222, 'accumulated_eval_time': 6822.102419376373, 'accumulated_logging_time': 5.218945741653442}
I0203 06:53:00.320318 139774417409792 logging_writer.py:48] [125568] accumulated_eval_time=6822.102419, accumulated_logging_time=5.218946, accumulated_submission_time=57596.523528, global_step=125568, preemption_count=0, score=57596.523528, test/accuracy=0.476300, test/loss=2.396895, test/num_examples=10000, total_duration=64430.419390, train/accuracy=0.641973, train/loss=1.518304, validation/accuracy=0.597400, validation/loss=1.734222, validation/num_examples=50000
I0203 06:53:13.417892 139774434195200 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.2119284868240356, loss=4.663386821746826
I0203 06:53:57.472668 139774417409792 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.2059051990509033, loss=3.763615131378174
I0203 06:54:44.001897 139774434195200 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.4370753765106201, loss=2.814335823059082
I0203 06:55:30.590952 139774417409792 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.2888267040252686, loss=5.090456485748291
I0203 06:56:16.920315 139774434195200 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.7576662302017212, loss=2.4141290187835693
I0203 06:57:03.551809 139774417409792 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.2768720388412476, loss=2.9092557430267334
I0203 06:57:50.140194 139774434195200 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.3840508460998535, loss=2.991419792175293
I0203 06:58:36.673901 139774417409792 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.2546396255493164, loss=5.01604700088501
I0203 06:59:23.276477 139774434195200 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.1597676277160645, loss=4.924248695373535
I0203 07:00:00.648435 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:00:11.751502 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:00:47.222873 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:00:48.831483 139936116377408 submission_runner.py:408] Time since start: 64898.98s, 	Step: 126482, 	{'train/accuracy': 0.6576171517372131, 'train/loss': 1.45563805103302, 'validation/accuracy': 0.6087200045585632, 'validation/loss': 1.6954721212387085, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.356315851211548, 'test/num_examples': 10000, 'score': 58016.79306435585, 'total_duration': 64898.98254442215, 'accumulated_submission_time': 58016.79306435585, 'accumulated_eval_time': 6870.285463809967, 'accumulated_logging_time': 5.281857252120972}
I0203 07:00:48.869083 139774417409792 logging_writer.py:48] [126482] accumulated_eval_time=6870.285464, accumulated_logging_time=5.281857, accumulated_submission_time=58016.793064, global_step=126482, preemption_count=0, score=58016.793064, test/accuracy=0.483600, test/loss=2.356316, test/num_examples=10000, total_duration=64898.982544, train/accuracy=0.657617, train/loss=1.455638, validation/accuracy=0.608720, validation/loss=1.695472, validation/num_examples=50000
I0203 07:00:56.424921 139774434195200 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.5908304452896118, loss=2.441030502319336
I0203 07:01:39.532023 139774417409792 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.1936070919036865, loss=4.56118631362915
I0203 07:02:25.658866 139774434195200 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.3983415365219116, loss=2.6421151161193848
I0203 07:03:12.269544 139774417409792 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.4528878927230835, loss=2.5068485736846924
I0203 07:03:58.413190 139774434195200 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.2580304145812988, loss=3.725450038909912
I0203 07:04:44.969322 139774417409792 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.3754768371582031, loss=2.62587308883667
I0203 07:05:31.371019 139774434195200 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.475394368171692, loss=2.4154200553894043
I0203 07:06:17.345802 139774417409792 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.4612635374069214, loss=2.80926513671875
I0203 07:07:03.737564 139774434195200 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.5060410499572754, loss=2.5655908584594727
I0203 07:07:49.234582 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:07:59.915437 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:08:34.989563 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:08:36.592466 139936116377408 submission_runner.py:408] Time since start: 65366.74s, 	Step: 127400, 	{'train/accuracy': 0.6700586080551147, 'train/loss': 1.3599334955215454, 'validation/accuracy': 0.6076599955558777, 'validation/loss': 1.6682051420211792, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.3263015747070312, 'test/num_examples': 10000, 'score': 58437.100652217865, 'total_duration': 65366.74352836609, 'accumulated_submission_time': 58437.100652217865, 'accumulated_eval_time': 6917.643349409103, 'accumulated_logging_time': 5.328955173492432}
I0203 07:08:36.636563 139774417409792 logging_writer.py:48] [127400] accumulated_eval_time=6917.643349, accumulated_logging_time=5.328955, accumulated_submission_time=58437.100652, global_step=127400, preemption_count=0, score=58437.100652, test/accuracy=0.488100, test/loss=2.326302, test/num_examples=10000, total_duration=65366.743528, train/accuracy=0.670059, train/loss=1.359933, validation/accuracy=0.607660, validation/loss=1.668205, validation/num_examples=50000
I0203 07:08:37.037314 139774434195200 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.4510836601257324, loss=2.397432565689087
I0203 07:09:19.182251 139774417409792 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.5092989206314087, loss=2.566032648086548
I0203 07:10:05.394598 139774434195200 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.564618468284607, loss=2.60500431060791
I0203 07:10:51.728327 139774417409792 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.270176887512207, loss=2.6745829582214355
I0203 07:11:38.205147 139774434195200 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.3691892623901367, loss=2.539628267288208
I0203 07:12:24.566399 139774417409792 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.200088381767273, loss=4.186803817749023
I0203 07:13:11.016156 139774434195200 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.420282006263733, loss=2.510512113571167
I0203 07:13:57.456928 139774417409792 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.280776023864746, loss=3.0164411067962646
I0203 07:14:43.727808 139774434195200 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.6476531028747559, loss=2.6052896976470947
I0203 07:15:30.260030 139774417409792 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.5364959239959717, loss=2.5319156646728516
I0203 07:15:36.763878 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:15:47.735828 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:16:23.685637 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:16:25.302238 139936116377408 submission_runner.py:408] Time since start: 65835.45s, 	Step: 128316, 	{'train/accuracy': 0.6534960865974426, 'train/loss': 1.4608618021011353, 'validation/accuracy': 0.6089000105857849, 'validation/loss': 1.6636296510696411, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.318061113357544, 'test/num_examples': 10000, 'score': 58857.16923952103, 'total_duration': 65835.45329618454, 'accumulated_submission_time': 58857.16923952103, 'accumulated_eval_time': 6966.18169260025, 'accumulated_logging_time': 5.3842246532440186}
I0203 07:16:25.339756 139774434195200 logging_writer.py:48] [128316] accumulated_eval_time=6966.181693, accumulated_logging_time=5.384225, accumulated_submission_time=58857.169240, global_step=128316, preemption_count=0, score=58857.169240, test/accuracy=0.486400, test/loss=2.318061, test/num_examples=10000, total_duration=65835.453296, train/accuracy=0.653496, train/loss=1.460862, validation/accuracy=0.608900, validation/loss=1.663630, validation/num_examples=50000
I0203 07:17:00.345529 139774417409792 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.2608805894851685, loss=4.892590522766113
I0203 07:17:46.383063 139774434195200 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.2023767232894897, loss=3.9128103256225586
I0203 07:18:32.801326 139774417409792 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.304215908050537, loss=3.3590803146362305
I0203 07:19:19.166440 139774434195200 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.2223988771438599, loss=3.834157943725586
I0203 07:20:05.253582 139774417409792 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.4357833862304688, loss=2.4278857707977295
I0203 07:20:51.569320 139774434195200 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.5060383081436157, loss=2.7600133419036865
I0203 07:21:38.147839 139774417409792 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.3489753007888794, loss=3.6857895851135254
I0203 07:22:24.720720 139774434195200 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.3120110034942627, loss=3.413288116455078
I0203 07:23:10.912190 139774417409792 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.4094899892807007, loss=3.2457430362701416
I0203 07:23:25.652462 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:23:36.089101 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:24:14.149212 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:24:15.751655 139936116377408 submission_runner.py:408] Time since start: 66305.90s, 	Step: 129234, 	{'train/accuracy': 0.6603124737739563, 'train/loss': 1.4079570770263672, 'validation/accuracy': 0.6110399961471558, 'validation/loss': 1.6467926502227783, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.2942965030670166, 'test/num_examples': 10000, 'score': 59277.42556810379, 'total_duration': 66305.90271615982, 'accumulated_submission_time': 59277.42556810379, 'accumulated_eval_time': 7016.280867099762, 'accumulated_logging_time': 5.430881500244141}
I0203 07:24:15.788987 139774434195200 logging_writer.py:48] [129234] accumulated_eval_time=7016.280867, accumulated_logging_time=5.430882, accumulated_submission_time=59277.425568, global_step=129234, preemption_count=0, score=59277.425568, test/accuracy=0.495000, test/loss=2.294297, test/num_examples=10000, total_duration=66305.902716, train/accuracy=0.660312, train/loss=1.407957, validation/accuracy=0.611040, validation/loss=1.646793, validation/num_examples=50000
I0203 07:24:42.422262 139774417409792 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.580521583557129, loss=2.4227747917175293
I0203 07:25:28.509755 139774434195200 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.5066088438034058, loss=2.389937400817871
I0203 07:26:14.768964 139774417409792 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.3317818641662598, loss=3.040414333343506
I0203 07:27:00.930704 139774434195200 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.5417944192886353, loss=2.548802137374878
I0203 07:27:47.197676 139774417409792 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.1467465162277222, loss=5.05061149597168
I0203 07:28:33.416698 139774434195200 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.4835933446884155, loss=2.2989492416381836
I0203 07:29:19.567571 139774417409792 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.241991639137268, loss=5.056920528411865
I0203 07:30:05.607115 139774434195200 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.4377119541168213, loss=3.0155436992645264
I0203 07:30:51.765295 139774417409792 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.4189475774765015, loss=3.564077854156494
I0203 07:31:16.096054 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:31:26.747577 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:32:02.656662 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:32:04.259669 139936116377408 submission_runner.py:408] Time since start: 66774.41s, 	Step: 130154, 	{'train/accuracy': 0.6708202958106995, 'train/loss': 1.3997917175292969, 'validation/accuracy': 0.6119999885559082, 'validation/loss': 1.674870252609253, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.3398818969726562, 'test/num_examples': 10000, 'score': 59697.674666404724, 'total_duration': 66774.41073536873, 'accumulated_submission_time': 59697.674666404724, 'accumulated_eval_time': 7064.444483995438, 'accumulated_logging_time': 5.477983713150024}
I0203 07:32:04.294246 139774434195200 logging_writer.py:48] [130154] accumulated_eval_time=7064.444484, accumulated_logging_time=5.477984, accumulated_submission_time=59697.674666, global_step=130154, preemption_count=0, score=59697.674666, test/accuracy=0.490800, test/loss=2.339882, test/num_examples=10000, total_duration=66774.410735, train/accuracy=0.670820, train/loss=1.399792, validation/accuracy=0.612000, validation/loss=1.674870, validation/num_examples=50000
I0203 07:32:23.045096 139774417409792 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.3032411336898804, loss=4.110016345977783
I0203 07:33:07.682461 139774434195200 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.6511985063552856, loss=2.4291481971740723
I0203 07:33:53.772301 139774417409792 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.4345893859863281, loss=4.320044994354248
I0203 07:34:39.685346 139774434195200 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.6009200811386108, loss=2.3546106815338135
I0203 07:35:25.932374 139774417409792 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.2116508483886719, loss=4.3538923263549805
I0203 07:36:12.013586 139774434195200 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.8145437240600586, loss=2.418771266937256
I0203 07:36:58.109503 139774417409792 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.358963131904602, loss=3.6104848384857178
I0203 07:37:44.367022 139774434195200 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.400948405265808, loss=4.553178310394287
I0203 07:38:30.874498 139774417409792 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.4388456344604492, loss=2.345268726348877
I0203 07:39:04.472243 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:39:15.065369 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:39:51.106928 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:39:52.721720 139936116377408 submission_runner.py:408] Time since start: 67242.87s, 	Step: 131074, 	{'train/accuracy': 0.6654687523841858, 'train/loss': 1.3992410898208618, 'validation/accuracy': 0.6183599829673767, 'validation/loss': 1.6063157320022583, 'validation/num_examples': 50000, 'test/accuracy': 0.49470001459121704, 'test/loss': 2.2643074989318848, 'test/num_examples': 10000, 'score': 60117.79187011719, 'total_duration': 67242.87277555466, 'accumulated_submission_time': 60117.79187011719, 'accumulated_eval_time': 7112.693949460983, 'accumulated_logging_time': 5.525101900100708}
I0203 07:39:52.757876 139774434195200 logging_writer.py:48] [131074] accumulated_eval_time=7112.693949, accumulated_logging_time=5.525102, accumulated_submission_time=60117.791870, global_step=131074, preemption_count=0, score=60117.791870, test/accuracy=0.494700, test/loss=2.264307, test/num_examples=10000, total_duration=67242.872776, train/accuracy=0.665469, train/loss=1.399241, validation/accuracy=0.618360, validation/loss=1.606316, validation/num_examples=50000
I0203 07:40:03.474082 139774417409792 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.3974382877349854, loss=3.8781814575195312
I0203 07:40:47.152292 139774434195200 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.5674159526824951, loss=2.500866413116455
I0203 07:41:33.433758 139774417409792 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.2342246770858765, loss=4.5215864181518555
I0203 07:42:20.200338 139774434195200 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.3090605735778809, loss=3.8436665534973145
I0203 07:43:06.621295 139774417409792 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.279935359954834, loss=4.939577579498291
I0203 07:43:52.709553 139774434195200 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.5833625793457031, loss=2.761107921600342
I0203 07:44:38.891112 139774417409792 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.521702766418457, loss=2.471388339996338
I0203 07:45:25.249231 139774434195200 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.4910950660705566, loss=2.5252749919891357
I0203 07:46:11.757514 139774417409792 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.52657151222229, loss=2.3510475158691406
I0203 07:46:52.926611 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:47:04.728475 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:47:40.150279 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:47:41.757845 139936116377408 submission_runner.py:408] Time since start: 67711.91s, 	Step: 131991, 	{'train/accuracy': 0.6669335961341858, 'train/loss': 1.3916473388671875, 'validation/accuracy': 0.6188600063323975, 'validation/loss': 1.6234492063522339, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.2747392654418945, 'test/num_examples': 10000, 'score': 60537.90219092369, 'total_duration': 67711.90890932083, 'accumulated_submission_time': 60537.90219092369, 'accumulated_eval_time': 7161.525184869766, 'accumulated_logging_time': 5.570846319198608}
I0203 07:47:41.801174 139774434195200 logging_writer.py:48] [131991] accumulated_eval_time=7161.525185, accumulated_logging_time=5.570846, accumulated_submission_time=60537.902191, global_step=131991, preemption_count=0, score=60537.902191, test/accuracy=0.497300, test/loss=2.274739, test/num_examples=10000, total_duration=67711.908909, train/accuracy=0.666934, train/loss=1.391647, validation/accuracy=0.618860, validation/loss=1.623449, validation/num_examples=50000
I0203 07:47:45.778986 139774417409792 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.569303035736084, loss=2.5551023483276367
I0203 07:48:28.365618 139774434195200 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.5565239191055298, loss=2.4561996459960938
I0203 07:49:14.381598 139774417409792 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.42753005027771, loss=4.666904449462891
I0203 07:50:00.409282 139774434195200 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.478426218032837, loss=2.4250237941741943
I0203 07:50:46.870851 139774417409792 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.5797693729400635, loss=2.5567638874053955
I0203 07:51:32.967839 139774434195200 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.4376201629638672, loss=2.645918607711792
I0203 07:52:19.416649 139774417409792 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.6203956604003906, loss=2.3099260330200195
I0203 07:53:05.738821 139774434195200 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.6489012241363525, loss=2.4093799591064453
I0203 07:53:51.737227 139774417409792 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.3438290357589722, loss=3.7207725048065186
I0203 07:54:38.006572 139774434195200 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.5815489292144775, loss=2.550931215286255
I0203 07:54:41.905969 139936116377408 spec.py:321] Evaluating on the training split.
I0203 07:54:52.842702 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 07:55:29.150985 139936116377408 spec.py:349] Evaluating on the test split.
I0203 07:55:30.757496 139936116377408 submission_runner.py:408] Time since start: 68180.91s, 	Step: 132910, 	{'train/accuracy': 0.6772265434265137, 'train/loss': 1.3613171577453613, 'validation/accuracy': 0.620959997177124, 'validation/loss': 1.627536654472351, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.2841172218322754, 'test/num_examples': 10000, 'score': 60957.94920253754, 'total_duration': 68180.9085547924, 'accumulated_submission_time': 60957.94920253754, 'accumulated_eval_time': 7210.376707792282, 'accumulated_logging_time': 5.624185085296631}
I0203 07:55:30.795667 139774417409792 logging_writer.py:48] [132910] accumulated_eval_time=7210.376708, accumulated_logging_time=5.624185, accumulated_submission_time=60957.949203, global_step=132910, preemption_count=0, score=60957.949203, test/accuracy=0.497400, test/loss=2.284117, test/num_examples=10000, total_duration=68180.908555, train/accuracy=0.677227, train/loss=1.361317, validation/accuracy=0.620960, validation/loss=1.627537, validation/num_examples=50000
I0203 07:56:08.712091 139774434195200 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.5043282508850098, loss=2.687019109725952
I0203 07:56:54.717843 139774417409792 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.6626142263412476, loss=2.43420672416687
I0203 07:57:40.951722 139774434195200 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.7479177713394165, loss=2.276548385620117
I0203 07:58:27.394454 139774417409792 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.2740039825439453, loss=3.4897854328155518
I0203 07:59:13.586811 139774434195200 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.7646287679672241, loss=2.5890252590179443
I0203 07:59:59.749280 139774417409792 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.5622355937957764, loss=2.579923391342163
I0203 08:00:46.142318 139774434195200 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.4796791076660156, loss=4.703372001647949
I0203 08:01:32.295028 139774417409792 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.348327875137329, loss=4.049994468688965
I0203 08:02:18.908893 139774434195200 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.6576439142227173, loss=2.4289495944976807
I0203 08:02:31.222445 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:02:42.022311 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:03:20.334871 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:03:21.945624 139936116377408 submission_runner.py:408] Time since start: 68652.10s, 	Step: 133828, 	{'train/accuracy': 0.6712890267372131, 'train/loss': 1.365814447402954, 'validation/accuracy': 0.6225999593734741, 'validation/loss': 1.5908536911010742, 'validation/num_examples': 50000, 'test/accuracy': 0.501300036907196, 'test/loss': 2.24080491065979, 'test/num_examples': 10000, 'score': 61378.31641602516, 'total_duration': 68652.09668803215, 'accumulated_submission_time': 61378.31641602516, 'accumulated_eval_time': 7261.099878549576, 'accumulated_logging_time': 5.673567771911621}
I0203 08:03:21.984599 139774417409792 logging_writer.py:48] [133828] accumulated_eval_time=7261.099879, accumulated_logging_time=5.673568, accumulated_submission_time=61378.316416, global_step=133828, preemption_count=0, score=61378.316416, test/accuracy=0.501300, test/loss=2.240805, test/num_examples=10000, total_duration=68652.096688, train/accuracy=0.671289, train/loss=1.365814, validation/accuracy=0.622600, validation/loss=1.590854, validation/num_examples=50000
I0203 08:03:51.811470 139774434195200 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.5523890256881714, loss=2.5312376022338867
I0203 08:04:37.900293 139774417409792 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.4469186067581177, loss=2.8506157398223877
I0203 08:05:24.822371 139774434195200 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.3413448333740234, loss=2.8882784843444824
I0203 08:06:11.214108 139774417409792 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.2326476573944092, loss=3.2916226387023926
I0203 08:06:57.545185 139774434195200 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.531199336051941, loss=4.837669849395752
I0203 08:07:44.030862 139774417409792 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.4550228118896484, loss=2.7933967113494873
I0203 08:08:30.570108 139774434195200 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.4773249626159668, loss=2.6226589679718018
I0203 08:09:16.762998 139774417409792 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.5929063558578491, loss=2.3939335346221924
I0203 08:10:03.344696 139774434195200 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.4283829927444458, loss=3.1036038398742676
I0203 08:10:22.321815 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:10:32.951732 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:11:12.123817 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:11:13.723356 139936116377408 submission_runner.py:408] Time since start: 69123.87s, 	Step: 134743, 	{'train/accuracy': 0.6692187190055847, 'train/loss': 1.3907537460327148, 'validation/accuracy': 0.6218199729919434, 'validation/loss': 1.610404133796692, 'validation/num_examples': 50000, 'test/accuracy': 0.49870002269744873, 'test/loss': 2.2690556049346924, 'test/num_examples': 10000, 'score': 61798.59408092499, 'total_duration': 69123.8744187355, 'accumulated_submission_time': 61798.59408092499, 'accumulated_eval_time': 7312.5014128685, 'accumulated_logging_time': 5.723673582077026}
I0203 08:11:13.759875 139774417409792 logging_writer.py:48] [134743] accumulated_eval_time=7312.501413, accumulated_logging_time=5.723674, accumulated_submission_time=61798.594081, global_step=134743, preemption_count=0, score=61798.594081, test/accuracy=0.498700, test/loss=2.269056, test/num_examples=10000, total_duration=69123.874419, train/accuracy=0.669219, train/loss=1.390754, validation/accuracy=0.621820, validation/loss=1.610404, validation/num_examples=50000
I0203 08:11:36.817571 139774434195200 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.378891944885254, loss=3.3457415103912354
I0203 08:12:22.531698 139774417409792 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.4585007429122925, loss=2.4628875255584717
I0203 08:13:08.848634 139774434195200 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.4335870742797852, loss=3.1253507137298584
I0203 08:13:55.090904 139774417409792 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.5732632875442505, loss=2.338850259780884
I0203 08:14:59.429007 139774434195200 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.6800563335418701, loss=2.289520025253296
I0203 08:15:45.130412 139774417409792 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.7614918947219849, loss=2.470529317855835
I0203 08:16:31.633464 139774434195200 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.6488701105117798, loss=2.316188097000122
I0203 08:17:18.028973 139774417409792 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.5530027151107788, loss=2.398777723312378
I0203 08:18:04.401644 139774434195200 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.838187336921692, loss=2.5006072521209717
I0203 08:18:13.912618 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:18:24.553675 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:19:00.256689 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:19:01.862950 139936116377408 submission_runner.py:408] Time since start: 69592.01s, 	Step: 135622, 	{'train/accuracy': 0.6849804520606995, 'train/loss': 1.3148226737976074, 'validation/accuracy': 0.6287999749183655, 'validation/loss': 1.5665632486343384, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.21661114692688, 'test/num_examples': 10000, 'score': 62218.69186377525, 'total_duration': 69592.01400113106, 'accumulated_submission_time': 62218.69186377525, 'accumulated_eval_time': 7360.451724529266, 'accumulated_logging_time': 5.769453287124634}
I0203 08:19:01.905808 139774417409792 logging_writer.py:48] [135622] accumulated_eval_time=7360.451725, accumulated_logging_time=5.769453, accumulated_submission_time=62218.691864, global_step=135622, preemption_count=0, score=62218.691864, test/accuracy=0.505700, test/loss=2.216611, test/num_examples=10000, total_duration=69592.014001, train/accuracy=0.684980, train/loss=1.314823, validation/accuracy=0.628800, validation/loss=1.566563, validation/num_examples=50000
I0203 08:19:34.442760 139774434195200 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.499363660812378, loss=2.639158248901367
I0203 08:20:20.591916 139774417409792 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.4863702058792114, loss=2.454680919647217
I0203 08:21:06.939371 139774434195200 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.5957142114639282, loss=2.6743931770324707
I0203 08:21:53.250419 139774417409792 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.6259297132492065, loss=2.9383389949798584
I0203 08:22:39.905441 139774434195200 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.4932467937469482, loss=2.9563655853271484
I0203 08:23:26.406008 139774417409792 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.6908855438232422, loss=2.3262174129486084
I0203 08:24:14.077763 139774434195200 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.5099526643753052, loss=3.5020949840545654
I0203 08:25:00.888135 139774417409792 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.6154650449752808, loss=2.318018674850464
I0203 08:25:47.268372 139774434195200 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.6417522430419922, loss=2.4486374855041504
I0203 08:26:02.256935 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:26:12.602057 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:26:50.906499 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:26:52.522271 139936116377408 submission_runner.py:408] Time since start: 70062.67s, 	Step: 136534, 	{'train/accuracy': 0.6810937523841858, 'train/loss': 1.350616216659546, 'validation/accuracy': 0.6312800049781799, 'validation/loss': 1.5826791524887085, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2302193641662598, 'test/num_examples': 10000, 'score': 62638.98313713074, 'total_duration': 70062.6733353138, 'accumulated_submission_time': 62638.98313713074, 'accumulated_eval_time': 7410.717103242874, 'accumulated_logging_time': 5.824112415313721}
I0203 08:26:52.562367 139774417409792 logging_writer.py:48] [136534] accumulated_eval_time=7410.717103, accumulated_logging_time=5.824112, accumulated_submission_time=62638.983137, global_step=136534, preemption_count=0, score=62638.983137, test/accuracy=0.505900, test/loss=2.230219, test/num_examples=10000, total_duration=70062.673335, train/accuracy=0.681094, train/loss=1.350616, validation/accuracy=0.631280, validation/loss=1.582679, validation/num_examples=50000
I0203 08:27:19.514395 139774434195200 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.635045051574707, loss=2.313966751098633
I0203 08:28:05.862107 139774417409792 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.8165051937103271, loss=2.3770511150360107
I0203 08:28:52.428945 139774434195200 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.6457043886184692, loss=2.3897554874420166
I0203 08:29:39.160688 139774417409792 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.6415348052978516, loss=2.254082202911377
I0203 08:30:25.582820 139774434195200 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.5716251134872437, loss=2.264970302581787
I0203 08:31:12.324853 139774417409792 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.2471160888671875, loss=4.141319274902344
I0203 08:31:58.618936 139774434195200 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.6954644918441772, loss=2.486081123352051
I0203 08:32:45.271356 139774417409792 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.7818433046340942, loss=2.352017879486084
I0203 08:33:31.947832 139774434195200 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.511564016342163, loss=2.479477882385254
I0203 08:33:52.891504 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:34:03.697202 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:34:39.888094 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:34:41.505171 139936116377408 submission_runner.py:408] Time since start: 70531.66s, 	Step: 137447, 	{'train/accuracy': 0.6771484017372131, 'train/loss': 1.363439917564392, 'validation/accuracy': 0.6318599581718445, 'validation/loss': 1.5701099634170532, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2094790935516357, 'test/num_examples': 10000, 'score': 63059.255123615265, 'total_duration': 70531.656188488, 'accumulated_submission_time': 63059.255123615265, 'accumulated_eval_time': 7459.330714464188, 'accumulated_logging_time': 5.873344898223877}
I0203 08:34:41.549976 139774417409792 logging_writer.py:48] [137447] accumulated_eval_time=7459.330714, accumulated_logging_time=5.873345, accumulated_submission_time=63059.255124, global_step=137447, preemption_count=0, score=63059.255124, test/accuracy=0.511900, test/loss=2.209479, test/num_examples=10000, total_duration=70531.656188, train/accuracy=0.677148, train/loss=1.363440, validation/accuracy=0.631860, validation/loss=1.570110, validation/num_examples=50000
I0203 08:35:03.022563 139774434195200 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.4433971643447876, loss=4.424236297607422
I0203 08:35:48.423796 139774417409792 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.6585661172866821, loss=2.443276882171631
I0203 08:36:35.266299 139774434195200 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.6467806100845337, loss=2.3082973957061768
I0203 08:37:22.094984 139774417409792 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.8648440837860107, loss=2.243069887161255
I0203 08:38:08.495859 139774434195200 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.7503303289413452, loss=2.2150473594665527
I0203 08:38:54.867352 139774417409792 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.7204540967941284, loss=2.3663992881774902
I0203 08:39:41.416470 139774434195200 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.4996799230575562, loss=3.4533445835113525
I0203 08:40:28.280993 139774417409792 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.7166047096252441, loss=2.3986079692840576
I0203 08:41:15.126126 139774434195200 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.7999780178070068, loss=2.2292027473449707
I0203 08:41:41.761902 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:41:52.337523 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:42:29.360288 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:42:30.972422 139936116377408 submission_runner.py:408] Time since start: 71001.12s, 	Step: 138358, 	{'train/accuracy': 0.6902929544448853, 'train/loss': 1.3037259578704834, 'validation/accuracy': 0.6375799775123596, 'validation/loss': 1.5405707359313965, 'validation/num_examples': 50000, 'test/accuracy': 0.51500004529953, 'test/loss': 2.1850335597991943, 'test/num_examples': 10000, 'score': 63479.41015410423, 'total_duration': 71001.12347507477, 'accumulated_submission_time': 63479.41015410423, 'accumulated_eval_time': 7508.541209936142, 'accumulated_logging_time': 5.9278342723846436}
I0203 08:42:31.016977 139774417409792 logging_writer.py:48] [138358] accumulated_eval_time=7508.541210, accumulated_logging_time=5.927834, accumulated_submission_time=63479.410154, global_step=138358, preemption_count=0, score=63479.410154, test/accuracy=0.515000, test/loss=2.185034, test/num_examples=10000, total_duration=71001.123475, train/accuracy=0.690293, train/loss=1.303726, validation/accuracy=0.637580, validation/loss=1.540571, validation/num_examples=50000
I0203 08:42:48.109570 139774434195200 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.41597580909729, loss=4.476234436035156
I0203 08:43:32.692277 139774417409792 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.6952636241912842, loss=2.3426403999328613
I0203 08:44:19.112541 139774434195200 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.6706774234771729, loss=2.3066344261169434
I0203 08:45:05.828699 139774417409792 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.5538491010665894, loss=2.1595654487609863
I0203 08:45:52.268028 139774434195200 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.704788088798523, loss=2.344485282897949
I0203 08:46:39.030269 139774417409792 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.6487443447113037, loss=2.261777639389038
I0203 08:47:25.525787 139774434195200 logging_writer.py:48] [139000] global_step=139000, grad_norm=1.4270493984222412, loss=3.792006492614746
I0203 08:48:12.102261 139774417409792 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.8645161390304565, loss=2.283311605453491
I0203 08:48:58.491324 139774434195200 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.7095445394515991, loss=2.3568596839904785
I0203 08:49:31.167915 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:49:41.819522 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:50:21.437004 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:50:23.047515 139936116377408 submission_runner.py:408] Time since start: 71473.20s, 	Step: 139272, 	{'train/accuracy': 0.7201562523841858, 'train/loss': 1.162137746810913, 'validation/accuracy': 0.6441999673843384, 'validation/loss': 1.5044505596160889, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.161705732345581, 'test/num_examples': 10000, 'score': 63899.503361940384, 'total_duration': 71473.19856882095, 'accumulated_submission_time': 63899.503361940384, 'accumulated_eval_time': 7560.420810461044, 'accumulated_logging_time': 5.9827258586883545}
I0203 08:50:23.087806 139774417409792 logging_writer.py:48] [139272] accumulated_eval_time=7560.420810, accumulated_logging_time=5.982726, accumulated_submission_time=63899.503362, global_step=139272, preemption_count=0, score=63899.503362, test/accuracy=0.520500, test/loss=2.161706, test/num_examples=10000, total_duration=71473.198569, train/accuracy=0.720156, train/loss=1.162138, validation/accuracy=0.644200, validation/loss=1.504451, validation/num_examples=50000
I0203 08:50:34.597490 139774434195200 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.6680878400802612, loss=2.587900400161743
I0203 08:51:18.413200 139774417409792 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.4054268598556519, loss=3.8967843055725098
I0203 08:52:04.754016 139774434195200 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.482803225517273, loss=4.401317596435547
I0203 08:52:51.155444 139774417409792 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.650572657585144, loss=2.338815689086914
I0203 08:53:37.624979 139774434195200 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.7600510120391846, loss=2.276998519897461
I0203 08:54:23.741840 139774417409792 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.465138554573059, loss=3.6384949684143066
I0203 08:55:10.131736 139774434195200 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.6491550207138062, loss=2.76385235786438
I0203 08:55:56.422344 139774417409792 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.4902557134628296, loss=3.4650092124938965
I0203 08:56:42.848554 139774434195200 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.7179754972457886, loss=2.0460240840911865
I0203 08:57:23.328936 139936116377408 spec.py:321] Evaluating on the training split.
I0203 08:57:34.018851 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 08:58:11.944355 139936116377408 spec.py:349] Evaluating on the test split.
I0203 08:58:13.546028 139936116377408 submission_runner.py:408] Time since start: 71943.70s, 	Step: 140189, 	{'train/accuracy': 0.6912499666213989, 'train/loss': 1.2617908716201782, 'validation/accuracy': 0.6451199650764465, 'validation/loss': 1.4823774099349976, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1347148418426514, 'test/num_examples': 10000, 'score': 64319.68514704704, 'total_duration': 71943.69709396362, 'accumulated_submission_time': 64319.68514704704, 'accumulated_eval_time': 7610.637903690338, 'accumulated_logging_time': 6.034387826919556}
I0203 08:58:13.586625 139774417409792 logging_writer.py:48] [140189] accumulated_eval_time=7610.637904, accumulated_logging_time=6.034388, accumulated_submission_time=64319.685147, global_step=140189, preemption_count=0, score=64319.685147, test/accuracy=0.525100, test/loss=2.134715, test/num_examples=10000, total_duration=71943.697094, train/accuracy=0.691250, train/loss=1.261791, validation/accuracy=0.645120, validation/loss=1.482377, validation/num_examples=50000
I0203 08:58:18.344629 139774434195200 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.6892799139022827, loss=2.293139934539795
I0203 08:59:00.687895 139774417409792 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.5506433248519897, loss=2.352701425552368
I0203 08:59:46.913574 139774434195200 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.4245579242706299, loss=4.170160293579102
I0203 09:00:33.112628 139774417409792 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.6090914011001587, loss=3.1846959590911865
I0203 09:01:19.489064 139774434195200 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.6092262268066406, loss=2.260986089706421
I0203 09:02:05.962177 139774417409792 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.676404356956482, loss=2.328895330429077
I0203 09:02:51.918820 139774434195200 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.7133384943008423, loss=2.248166799545288
I0203 09:03:38.073242 139774417409792 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.79654860496521, loss=2.293360710144043
I0203 09:04:24.192900 139774434195200 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.610581398010254, loss=4.3174591064453125
I0203 09:05:10.392984 139774417409792 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.5109573602676392, loss=2.6359219551086426
I0203 09:05:13.644595 139936116377408 spec.py:321] Evaluating on the training split.
I0203 09:05:24.214548 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 09:06:02.940244 139936116377408 spec.py:349] Evaluating on the test split.
I0203 09:06:04.546775 139936116377408 submission_runner.py:408] Time since start: 72414.70s, 	Step: 141109, 	{'train/accuracy': 0.7030664086341858, 'train/loss': 1.2258492708206177, 'validation/accuracy': 0.6487999558448792, 'validation/loss': 1.4680681228637695, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.1157939434051514, 'test/num_examples': 10000, 'score': 64739.684688806534, 'total_duration': 72414.69781684875, 'accumulated_submission_time': 64739.684688806534, 'accumulated_eval_time': 7661.540041446686, 'accumulated_logging_time': 6.084909915924072}
I0203 09:06:04.588989 139774434195200 logging_writer.py:48] [141109] accumulated_eval_time=7661.540041, accumulated_logging_time=6.084910, accumulated_submission_time=64739.684689, global_step=141109, preemption_count=0, score=64739.684689, test/accuracy=0.525500, test/loss=2.115794, test/num_examples=10000, total_duration=72414.697817, train/accuracy=0.703066, train/loss=1.225849, validation/accuracy=0.648800, validation/loss=1.468068, validation/num_examples=50000
I0203 09:06:42.942452 139774417409792 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.795249581336975, loss=2.3635945320129395
I0203 09:07:28.772162 139774434195200 logging_writer.py:48] [141300] global_step=141300, grad_norm=1.8125590085983276, loss=2.192814350128174
I0203 09:08:15.474331 139774417409792 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.5685511827468872, loss=2.2369136810302734
I0203 09:09:01.637166 139774434195200 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.4140450954437256, loss=4.296265125274658
I0203 09:09:47.978963 139774417409792 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.7660011053085327, loss=2.2097434997558594
I0203 09:10:34.275749 139774434195200 logging_writer.py:48] [141700] global_step=141700, grad_norm=1.716295599937439, loss=2.3248257637023926
I0203 09:11:20.708635 139774417409792 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.4717458486557007, loss=3.1152737140655518
I0203 09:12:07.239017 139774434195200 logging_writer.py:48] [141900] global_step=141900, grad_norm=1.6931482553482056, loss=2.21740984916687
I0203 09:12:53.376518 139774417409792 logging_writer.py:48] [142000] global_step=142000, grad_norm=1.5548089742660522, loss=3.827615261077881
I0203 09:13:04.875668 139936116377408 spec.py:321] Evaluating on the training split.
I0203 09:13:15.700976 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 09:13:52.635589 139936116377408 spec.py:349] Evaluating on the test split.
I0203 09:13:54.238002 139936116377408 submission_runner.py:408] Time since start: 72884.39s, 	Step: 142026, 	{'train/accuracy': 0.712695300579071, 'train/loss': 1.1884727478027344, 'validation/accuracy': 0.6464200019836426, 'validation/loss': 1.488558292388916, 'validation/num_examples': 50000, 'test/accuracy': 0.5259000062942505, 'test/loss': 2.140204906463623, 'test/num_examples': 10000, 'score': 65159.91322731972, 'total_duration': 72884.38906359673, 'accumulated_submission_time': 65159.91322731972, 'accumulated_eval_time': 7710.902366638184, 'accumulated_logging_time': 6.13739275932312}
I0203 09:13:54.276951 139774434195200 logging_writer.py:48] [142026] accumulated_eval_time=7710.902367, accumulated_logging_time=6.137393, accumulated_submission_time=65159.913227, global_step=142026, preemption_count=0, score=65159.913227, test/accuracy=0.525900, test/loss=2.140205, test/num_examples=10000, total_duration=72884.389064, train/accuracy=0.712695, train/loss=1.188473, validation/accuracy=0.646420, validation/loss=1.488558, validation/num_examples=50000
I0203 09:14:24.771165 139774417409792 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.4573322534561157, loss=3.7820239067077637
I0203 09:15:10.821280 139774434195200 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.7691408395767212, loss=2.2295005321502686
I0203 09:15:57.086001 139774417409792 logging_writer.py:48] [142300] global_step=142300, grad_norm=1.9485666751861572, loss=2.2362864017486572
I0203 09:16:43.508262 139774434195200 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.4586573839187622, loss=4.416420936584473
I0203 09:17:29.849432 139774417409792 logging_writer.py:48] [142500] global_step=142500, grad_norm=1.5074965953826904, loss=3.5056607723236084
I0203 09:18:16.174833 139774434195200 logging_writer.py:48] [142600] global_step=142600, grad_norm=1.8610427379608154, loss=2.297988176345825
I0203 09:19:02.846442 139774417409792 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.7719365358352661, loss=2.1611087322235107
I0203 09:19:48.994432 139774434195200 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.5347795486450195, loss=3.659170627593994
I0203 09:20:35.428097 139774417409792 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.7530043125152588, loss=2.3179097175598145
I0203 09:20:54.670908 139936116377408 spec.py:321] Evaluating on the training split.
I0203 09:21:05.429050 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 09:21:41.560964 139936116377408 spec.py:349] Evaluating on the test split.
I0203 09:21:43.185618 139936116377408 submission_runner.py:408] Time since start: 73353.34s, 	Step: 142943, 	{'train/accuracy': 0.7005664110183716, 'train/loss': 1.2247933149337769, 'validation/accuracy': 0.6492999792098999, 'validation/loss': 1.4643806219100952, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.109903573989868, 'test/num_examples': 10000, 'score': 65580.24869775772, 'total_duration': 73353.33665847778, 'accumulated_submission_time': 65580.24869775772, 'accumulated_eval_time': 7759.4170508384705, 'accumulated_logging_time': 6.186307430267334}
I0203 09:21:43.228619 139774434195200 logging_writer.py:48] [142943] accumulated_eval_time=7759.417051, accumulated_logging_time=6.186307, accumulated_submission_time=65580.248698, global_step=142943, preemption_count=0, score=65580.248698, test/accuracy=0.523200, test/loss=2.109904, test/num_examples=10000, total_duration=73353.336658, train/accuracy=0.700566, train/loss=1.224793, validation/accuracy=0.649300, validation/loss=1.464381, validation/num_examples=50000
I0203 09:22:06.273176 139774417409792 logging_writer.py:48] [143000] global_step=143000, grad_norm=1.5424360036849976, loss=4.374678134918213
I0203 09:22:51.907196 139774434195200 logging_writer.py:48] [143100] global_step=143100, grad_norm=1.718727469444275, loss=2.4621124267578125
I0203 09:23:38.236830 139774417409792 logging_writer.py:48] [143200] global_step=143200, grad_norm=1.7759262323379517, loss=4.202399730682373
I0203 09:24:24.642211 139774434195200 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.7397048473358154, loss=2.4905240535736084
I0203 09:25:10.924557 139774417409792 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.768052577972412, loss=2.2328641414642334
I0203 09:25:57.229217 139774434195200 logging_writer.py:48] [143500] global_step=143500, grad_norm=1.9069355726242065, loss=2.232253074645996
I0203 09:26:43.558681 139774417409792 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.6474435329437256, loss=2.9386179447174072
I0203 09:27:30.258447 139774434195200 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.846985936164856, loss=2.473964214324951
I0203 09:28:17.143017 139774417409792 logging_writer.py:48] [143800] global_step=143800, grad_norm=1.7131608724594116, loss=2.369736909866333
I0203 09:28:43.426217 139936116377408 spec.py:321] Evaluating on the training split.
I0203 09:28:53.613028 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 09:29:30.273442 139936116377408 spec.py:349] Evaluating on the test split.
I0203 09:29:31.896224 139936116377408 submission_runner.py:408] Time since start: 73822.05s, 	Step: 143859, 	{'train/accuracy': 0.7032226324081421, 'train/loss': 1.2361243963241577, 'validation/accuracy': 0.6534000039100647, 'validation/loss': 1.4610118865966797, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.113292932510376, 'test/num_examples': 10000, 'score': 66000.38722920418, 'total_duration': 73822.04728531837, 'accumulated_submission_time': 66000.38722920418, 'accumulated_eval_time': 7807.887068033218, 'accumulated_logging_time': 6.239961385726929}
I0203 09:29:31.937973 139774434195200 logging_writer.py:48] [143859] accumulated_eval_time=7807.887068, accumulated_logging_time=6.239961, accumulated_submission_time=66000.387229, global_step=143859, preemption_count=0, score=66000.387229, test/accuracy=0.529700, test/loss=2.113293, test/num_examples=10000, total_duration=73822.047285, train/accuracy=0.703223, train/loss=1.236124, validation/accuracy=0.653400, validation/loss=1.461012, validation/num_examples=50000
I0203 09:29:48.627417 139774417409792 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.8333228826522827, loss=2.210956573486328
I0203 09:30:33.044168 139774434195200 logging_writer.py:48] [144000] global_step=144000, grad_norm=1.782671332359314, loss=2.3018927574157715
I0203 09:31:19.273414 139774417409792 logging_writer.py:48] [144100] global_step=144100, grad_norm=1.5714970827102661, loss=3.9174070358276367
I0203 09:32:05.918866 139774434195200 logging_writer.py:48] [144200] global_step=144200, grad_norm=1.6389775276184082, loss=2.8082809448242188
I0203 09:32:52.269839 139774417409792 logging_writer.py:48] [144300] global_step=144300, grad_norm=1.7526146173477173, loss=2.6518733501434326
I0203 09:33:38.636593 139774434195200 logging_writer.py:48] [144400] global_step=144400, grad_norm=1.874990701675415, loss=2.181184768676758
I0203 09:34:24.795896 139774417409792 logging_writer.py:48] [144500] global_step=144500, grad_norm=1.8871914148330688, loss=2.3317785263061523
I0203 09:35:11.512216 139774434195200 logging_writer.py:48] [144600] global_step=144600, grad_norm=1.624950647354126, loss=3.566054105758667
I0203 09:35:57.862971 139774417409792 logging_writer.py:48] [144700] global_step=144700, grad_norm=1.7051666975021362, loss=2.8018863201141357
I0203 09:36:32.288698 139936116377408 spec.py:321] Evaluating on the training split.
I0203 09:36:43.146636 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 09:37:20.850426 139936116377408 spec.py:349] Evaluating on the test split.
I0203 09:37:22.455992 139936116377408 submission_runner.py:408] Time since start: 74292.61s, 	Step: 144776, 	{'train/accuracy': 0.7193945050239563, 'train/loss': 1.1607738733291626, 'validation/accuracy': 0.654259979724884, 'validation/loss': 1.447080135345459, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.1155529022216797, 'test/num_examples': 10000, 'score': 66420.68016719818, 'total_duration': 74292.60705327988, 'accumulated_submission_time': 66420.68016719818, 'accumulated_eval_time': 7858.054362535477, 'accumulated_logging_time': 6.291686773300171}
I0203 09:37:22.495507 139774434195200 logging_writer.py:48] [144776] accumulated_eval_time=7858.054363, accumulated_logging_time=6.291687, accumulated_submission_time=66420.680167, global_step=144776, preemption_count=0, score=66420.680167, test/accuracy=0.529900, test/loss=2.115553, test/num_examples=10000, total_duration=74292.607053, train/accuracy=0.719395, train/loss=1.160774, validation/accuracy=0.654260, validation/loss=1.447080, validation/num_examples=50000
I0203 09:37:32.423462 139774417409792 logging_writer.py:48] [144800] global_step=144800, grad_norm=1.7326534986495972, loss=2.5361597537994385
I0203 09:38:15.674323 139774434195200 logging_writer.py:48] [144900] global_step=144900, grad_norm=1.5756741762161255, loss=3.1872384548187256
I0203 09:39:02.114431 139774417409792 logging_writer.py:48] [145000] global_step=145000, grad_norm=1.7272752523422241, loss=2.664708375930786
I0203 09:39:48.813862 139774434195200 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.712478756904602, loss=2.510951519012451
I0203 09:40:35.559154 139774417409792 logging_writer.py:48] [145200] global_step=145200, grad_norm=1.69097101688385, loss=2.2719368934631348
I0203 09:41:22.066961 139774434195200 logging_writer.py:48] [145300] global_step=145300, grad_norm=1.6808048486709595, loss=2.4535818099975586
I0203 09:42:08.820045 139774417409792 logging_writer.py:48] [145400] global_step=145400, grad_norm=1.5255935192108154, loss=3.9729719161987305
I0203 09:42:55.109475 139774434195200 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.6594871282577515, loss=2.5495541095733643
I0203 09:43:41.621233 139774417409792 logging_writer.py:48] [145600] global_step=145600, grad_norm=1.8011345863342285, loss=4.678149700164795
I0203 09:44:22.724599 139936116377408 spec.py:321] Evaluating on the training split.
I0203 09:44:33.498221 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 09:45:08.479857 139936116377408 spec.py:349] Evaluating on the test split.
I0203 09:45:10.085354 139936116377408 submission_runner.py:408] Time since start: 74760.24s, 	Step: 145690, 	{'train/accuracy': 0.713671863079071, 'train/loss': 1.1829522848129272, 'validation/accuracy': 0.6609799861907959, 'validation/loss': 1.417777180671692, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.0482189655303955, 'test/num_examples': 10000, 'score': 66840.85182523727, 'total_duration': 74760.23641347885, 'accumulated_submission_time': 66840.85182523727, 'accumulated_eval_time': 7905.415101289749, 'accumulated_logging_time': 6.340944766998291}
I0203 09:45:10.127835 139774434195200 logging_writer.py:48] [145690] accumulated_eval_time=7905.415101, accumulated_logging_time=6.340945, accumulated_submission_time=66840.851825, global_step=145690, preemption_count=0, score=66840.851825, test/accuracy=0.541500, test/loss=2.048219, test/num_examples=10000, total_duration=74760.236413, train/accuracy=0.713672, train/loss=1.182952, validation/accuracy=0.660980, validation/loss=1.417777, validation/num_examples=50000
I0203 09:45:14.503196 139774417409792 logging_writer.py:48] [145700] global_step=145700, grad_norm=1.9216594696044922, loss=2.2113399505615234
I0203 09:45:57.306649 139774434195200 logging_writer.py:48] [145800] global_step=145800, grad_norm=1.7971878051757812, loss=2.148365020751953
I0203 09:46:43.482029 139774417409792 logging_writer.py:48] [145900] global_step=145900, grad_norm=1.8079607486724854, loss=2.150994300842285
I0203 09:47:29.897563 139774434195200 logging_writer.py:48] [146000] global_step=146000, grad_norm=1.837550401687622, loss=2.1902835369110107
I0203 09:48:16.352110 139774417409792 logging_writer.py:48] [146100] global_step=146100, grad_norm=1.5422650575637817, loss=3.3352508544921875
I0203 09:49:02.981070 139774434195200 logging_writer.py:48] [146200] global_step=146200, grad_norm=1.7758547067642212, loss=4.503374099731445
I0203 09:49:49.280430 139774417409792 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.8682982921600342, loss=1.9348530769348145
I0203 09:50:35.746974 139774434195200 logging_writer.py:48] [146400] global_step=146400, grad_norm=1.6997889280319214, loss=4.651105880737305
I0203 09:51:22.203633 139774417409792 logging_writer.py:48] [146500] global_step=146500, grad_norm=1.8586608171463013, loss=2.067258596420288
I0203 09:52:08.826636 139774434195200 logging_writer.py:48] [146600] global_step=146600, grad_norm=1.7018725872039795, loss=4.394954204559326
I0203 09:52:10.330703 139936116377408 spec.py:321] Evaluating on the training split.
I0203 09:52:20.908347 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 09:52:58.786415 139936116377408 spec.py:349] Evaluating on the test split.
I0203 09:53:00.396070 139936116377408 submission_runner.py:408] Time since start: 75230.55s, 	Step: 146605, 	{'train/accuracy': 0.7144335508346558, 'train/loss': 1.191360592842102, 'validation/accuracy': 0.6621999740600586, 'validation/loss': 1.4254510402679443, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.0617923736572266, 'test/num_examples': 10000, 'score': 67260.99752354622, 'total_duration': 75230.54712605476, 'accumulated_submission_time': 67260.99752354622, 'accumulated_eval_time': 7955.480447292328, 'accumulated_logging_time': 6.393074035644531}
I0203 09:53:00.433976 139774417409792 logging_writer.py:48] [146605] accumulated_eval_time=7955.480447, accumulated_logging_time=6.393074, accumulated_submission_time=67260.997524, global_step=146605, preemption_count=0, score=67260.997524, test/accuracy=0.542900, test/loss=2.061792, test/num_examples=10000, total_duration=75230.547126, train/accuracy=0.714434, train/loss=1.191361, validation/accuracy=0.662200, validation/loss=1.425451, validation/num_examples=50000
I0203 09:53:40.489686 139774434195200 logging_writer.py:48] [146700] global_step=146700, grad_norm=1.9462831020355225, loss=2.2377309799194336
I0203 09:54:26.635302 139774417409792 logging_writer.py:48] [146800] global_step=146800, grad_norm=1.8603585958480835, loss=2.0468063354492188
I0203 09:55:13.213349 139774434195200 logging_writer.py:48] [146900] global_step=146900, grad_norm=1.9500576257705688, loss=2.0384793281555176
I0203 09:55:59.404094 139774417409792 logging_writer.py:48] [147000] global_step=147000, grad_norm=1.7724189758300781, loss=2.382134199142456
I0203 09:56:45.856340 139774434195200 logging_writer.py:48] [147100] global_step=147100, grad_norm=1.9538878202438354, loss=2.153318166732788
I0203 09:57:32.257050 139774417409792 logging_writer.py:48] [147200] global_step=147200, grad_norm=1.7466384172439575, loss=2.8046820163726807
I0203 09:58:18.775985 139774434195200 logging_writer.py:48] [147300] global_step=147300, grad_norm=1.9111618995666504, loss=2.0035245418548584
I0203 09:59:05.115859 139774417409792 logging_writer.py:48] [147400] global_step=147400, grad_norm=1.8186057806015015, loss=3.162712574005127
I0203 09:59:51.277576 139774434195200 logging_writer.py:48] [147500] global_step=147500, grad_norm=1.9161747694015503, loss=4.151925086975098
I0203 10:00:00.562807 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:00:11.150753 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:00:47.016957 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:00:48.629146 139936116377408 submission_runner.py:408] Time since start: 75698.78s, 	Step: 147522, 	{'train/accuracy': 0.7241796851158142, 'train/loss': 1.1309359073638916, 'validation/accuracy': 0.6642000079154968, 'validation/loss': 1.4086828231811523, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.0465872287750244, 'test/num_examples': 10000, 'score': 67681.06917715073, 'total_duration': 75698.780200243, 'accumulated_submission_time': 67681.06917715073, 'accumulated_eval_time': 8003.546766996384, 'accumulated_logging_time': 6.440312385559082}
I0203 10:00:48.667025 139774417409792 logging_writer.py:48] [147522] accumulated_eval_time=8003.546767, accumulated_logging_time=6.440312, accumulated_submission_time=67681.069177, global_step=147522, preemption_count=0, score=67681.069177, test/accuracy=0.544800, test/loss=2.046587, test/num_examples=10000, total_duration=75698.780200, train/accuracy=0.724180, train/loss=1.130936, validation/accuracy=0.664200, validation/loss=1.408683, validation/num_examples=50000
I0203 10:01:21.082624 139774434195200 logging_writer.py:48] [147600] global_step=147600, grad_norm=1.5756622552871704, loss=3.04793119430542
I0203 10:02:07.514385 139774417409792 logging_writer.py:48] [147700] global_step=147700, grad_norm=1.8218706846237183, loss=2.5736875534057617
I0203 10:02:53.654005 139774434195200 logging_writer.py:48] [147800] global_step=147800, grad_norm=1.9177823066711426, loss=2.248044490814209
I0203 10:03:39.960444 139774417409792 logging_writer.py:48] [147900] global_step=147900, grad_norm=1.8528515100479126, loss=2.0817501544952393
I0203 10:04:26.181924 139774434195200 logging_writer.py:48] [148000] global_step=148000, grad_norm=1.872464656829834, loss=2.4977941513061523
I0203 10:05:12.639637 139774417409792 logging_writer.py:48] [148100] global_step=148100, grad_norm=1.6925796270370483, loss=4.084610462188721
I0203 10:05:58.689202 139774434195200 logging_writer.py:48] [148200] global_step=148200, grad_norm=1.8952405452728271, loss=2.1195363998413086
I0203 10:06:44.832390 139774417409792 logging_writer.py:48] [148300] global_step=148300, grad_norm=1.836676836013794, loss=2.1234288215637207
I0203 10:07:31.265829 139774434195200 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.0132970809936523, loss=2.172322988510132
I0203 10:07:48.806564 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:07:59.296313 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:08:36.963561 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:08:38.575419 139936116377408 submission_runner.py:408] Time since start: 76168.73s, 	Step: 148440, 	{'train/accuracy': 0.7292773127555847, 'train/loss': 1.1139354705810547, 'validation/accuracy': 0.6719399690628052, 'validation/loss': 1.3779354095458984, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.010650396347046, 'test/num_examples': 10000, 'score': 68101.14979958534, 'total_duration': 76168.72648119926, 'accumulated_submission_time': 68101.14979958534, 'accumulated_eval_time': 8053.31559920311, 'accumulated_logging_time': 6.489897012710571}
I0203 10:08:38.616269 139774417409792 logging_writer.py:48] [148440] accumulated_eval_time=8053.315599, accumulated_logging_time=6.489897, accumulated_submission_time=68101.149800, global_step=148440, preemption_count=0, score=68101.149800, test/accuracy=0.548200, test/loss=2.010650, test/num_examples=10000, total_duration=76168.726481, train/accuracy=0.729277, train/loss=1.113935, validation/accuracy=0.671940, validation/loss=1.377935, validation/num_examples=50000
I0203 10:09:02.960271 139774434195200 logging_writer.py:48] [148500] global_step=148500, grad_norm=1.691635251045227, loss=4.258816719055176
I0203 10:09:49.007427 139774417409792 logging_writer.py:48] [148600] global_step=148600, grad_norm=1.878600001335144, loss=2.167217254638672
I0203 10:10:35.309559 139774434195200 logging_writer.py:48] [148700] global_step=148700, grad_norm=1.959860920906067, loss=2.4563324451446533
I0203 10:11:21.488335 139774417409792 logging_writer.py:48] [148800] global_step=148800, grad_norm=1.9772263765335083, loss=2.2111175060272217
I0203 10:12:08.390113 139774434195200 logging_writer.py:48] [148900] global_step=148900, grad_norm=1.683538556098938, loss=2.574465036392212
I0203 10:12:54.754220 139774417409792 logging_writer.py:48] [149000] global_step=149000, grad_norm=1.904072880744934, loss=4.4124956130981445
I0203 10:13:41.168935 139774434195200 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.047988176345825, loss=2.2032697200775146
I0203 10:14:27.393116 139774417409792 logging_writer.py:48] [149200] global_step=149200, grad_norm=1.7243154048919678, loss=2.4050517082214355
I0203 10:15:13.559801 139774434195200 logging_writer.py:48] [149300] global_step=149300, grad_norm=1.6089696884155273, loss=3.0339748859405518
I0203 10:15:38.654082 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:15:49.409163 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:16:27.239172 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:16:28.865539 139936116377408 submission_runner.py:408] Time since start: 76639.02s, 	Step: 149356, 	{'train/accuracy': 0.7234765291213989, 'train/loss': 1.1463912725448608, 'validation/accuracy': 0.6699000000953674, 'validation/loss': 1.3762966394424438, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.032515525817871, 'test/num_examples': 10000, 'score': 68521.11470293999, 'total_duration': 76639.01660203934, 'accumulated_submission_time': 68521.11470293999, 'accumulated_eval_time': 8103.5270392894745, 'accumulated_logging_time': 6.53973126411438}
I0203 10:16:28.906446 139774417409792 logging_writer.py:48] [149356] accumulated_eval_time=8103.527039, accumulated_logging_time=6.539731, accumulated_submission_time=68521.114703, global_step=149356, preemption_count=0, score=68521.114703, test/accuracy=0.542700, test/loss=2.032516, test/num_examples=10000, total_duration=76639.016602, train/accuracy=0.723477, train/loss=1.146391, validation/accuracy=0.669900, validation/loss=1.376297, validation/num_examples=50000
I0203 10:16:46.794673 139774434195200 logging_writer.py:48] [149400] global_step=149400, grad_norm=1.71788489818573, loss=2.6109743118286133
I0203 10:17:31.560691 139774417409792 logging_writer.py:48] [149500] global_step=149500, grad_norm=1.7166130542755127, loss=4.11282205581665
I0203 10:18:17.958418 139774434195200 logging_writer.py:48] [149600] global_step=149600, grad_norm=1.6489741802215576, loss=3.6201860904693604
I0203 10:19:04.322432 139774417409792 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.1593515872955322, loss=2.214057445526123
I0203 10:19:50.662277 139774434195200 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.0262765884399414, loss=1.9362114667892456
I0203 10:20:37.495025 139774417409792 logging_writer.py:48] [149900] global_step=149900, grad_norm=1.923957347869873, loss=2.151031732559204
I0203 10:21:23.887933 139774434195200 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.059194564819336, loss=2.197218656539917
I0203 10:22:10.397852 139774417409792 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.038388252258301, loss=2.0796990394592285
I0203 10:22:57.076699 139774434195200 logging_writer.py:48] [150200] global_step=150200, grad_norm=1.9483622312545776, loss=4.081066131591797
I0203 10:23:28.933547 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:23:39.362321 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:24:19.313145 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:24:20.919067 139936116377408 submission_runner.py:408] Time since start: 77111.07s, 	Step: 150270, 	{'train/accuracy': 0.7346875071525574, 'train/loss': 1.091256856918335, 'validation/accuracy': 0.6761800050735474, 'validation/loss': 1.3612442016601562, 'validation/num_examples': 50000, 'test/accuracy': 0.5507000088691711, 'test/loss': 1.992889165878296, 'test/num_examples': 10000, 'score': 68941.08154058456, 'total_duration': 77111.07012796402, 'accumulated_submission_time': 68941.08154058456, 'accumulated_eval_time': 8155.512540578842, 'accumulated_logging_time': 6.593385457992554}
I0203 10:24:20.966819 139774417409792 logging_writer.py:48] [150270] accumulated_eval_time=8155.512541, accumulated_logging_time=6.593385, accumulated_submission_time=68941.081541, global_step=150270, preemption_count=0, score=68941.081541, test/accuracy=0.550700, test/loss=1.992889, test/num_examples=10000, total_duration=77111.070128, train/accuracy=0.734688, train/loss=1.091257, validation/accuracy=0.676180, validation/loss=1.361244, validation/num_examples=50000
I0203 10:24:33.272791 139774434195200 logging_writer.py:48] [150300] global_step=150300, grad_norm=1.9259124994277954, loss=4.545466899871826
I0203 10:25:17.044938 139774417409792 logging_writer.py:48] [150400] global_step=150400, grad_norm=1.9179543256759644, loss=2.213595390319824
I0203 10:26:03.075460 139774434195200 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.1565780639648438, loss=2.1294710636138916
I0203 10:26:49.524690 139774417409792 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.0541887283325195, loss=2.2600414752960205
I0203 10:27:35.717141 139774434195200 logging_writer.py:48] [150700] global_step=150700, grad_norm=1.8633060455322266, loss=4.063326358795166
I0203 10:28:22.249636 139774417409792 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.199457883834839, loss=2.48107647895813
I0203 10:29:08.719892 139774434195200 logging_writer.py:48] [150900] global_step=150900, grad_norm=1.8612329959869385, loss=3.938431739807129
I0203 10:29:54.827000 139774417409792 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.0244884490966797, loss=2.1483685970306396
I0203 10:30:40.966464 139774434195200 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.168039321899414, loss=2.025951623916626
I0203 10:31:21.142926 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:31:31.852024 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:32:10.507061 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:32:12.110407 139936116377408 submission_runner.py:408] Time since start: 77582.26s, 	Step: 151188, 	{'train/accuracy': 0.75537109375, 'train/loss': 1.024010181427002, 'validation/accuracy': 0.6801199913024902, 'validation/loss': 1.350976824760437, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 1.9849852323532104, 'test/num_examples': 10000, 'score': 69361.19805550575, 'total_duration': 77582.26147294044, 'accumulated_submission_time': 69361.19805550575, 'accumulated_eval_time': 8206.480012178421, 'accumulated_logging_time': 6.650951862335205}
I0203 10:32:12.152266 139774417409792 logging_writer.py:48] [151188] accumulated_eval_time=8206.480012, accumulated_logging_time=6.650952, accumulated_submission_time=69361.198056, global_step=151188, preemption_count=0, score=69361.198056, test/accuracy=0.557400, test/loss=1.984985, test/num_examples=10000, total_duration=77582.261473, train/accuracy=0.755371, train/loss=1.024010, validation/accuracy=0.680120, validation/loss=1.350977, validation/num_examples=50000
I0203 10:32:17.317546 139774434195200 logging_writer.py:48] [151200] global_step=151200, grad_norm=1.9955968856811523, loss=2.2082467079162598
I0203 10:33:00.036987 139774417409792 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.1594064235687256, loss=2.3073863983154297
I0203 10:33:46.502568 139774434195200 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.245217800140381, loss=2.0265703201293945
I0203 10:34:33.436082 139774417409792 logging_writer.py:48] [151500] global_step=151500, grad_norm=1.9530775547027588, loss=2.424088954925537
I0203 10:35:19.551286 139774434195200 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.0175459384918213, loss=2.8310563564300537
I0203 10:36:06.070884 139774417409792 logging_writer.py:48] [151700] global_step=151700, grad_norm=1.7468668222427368, loss=2.6408045291900635
I0203 10:36:52.449418 139774434195200 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.0565359592437744, loss=2.2280149459838867
I0203 10:37:38.775235 139774417409792 logging_writer.py:48] [151900] global_step=151900, grad_norm=1.8367688655853271, loss=3.7118613719940186
I0203 10:38:24.901352 139774434195200 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.0519630908966064, loss=1.972719430923462
I0203 10:39:11.201699 139774417409792 logging_writer.py:48] [152100] global_step=152100, grad_norm=1.8322598934173584, loss=3.0029141902923584
I0203 10:39:12.242938 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:39:22.908008 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:39:59.255962 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:40:00.858267 139936116377408 submission_runner.py:408] Time since start: 78051.01s, 	Step: 152104, 	{'train/accuracy': 0.7370507717132568, 'train/loss': 1.0689661502838135, 'validation/accuracy': 0.681659996509552, 'validation/loss': 1.3200819492340088, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 1.947913408279419, 'test/num_examples': 10000, 'score': 69781.2301557064, 'total_duration': 78051.009329319, 'accumulated_submission_time': 69781.2301557064, 'accumulated_eval_time': 8255.09532880783, 'accumulated_logging_time': 6.703702688217163}
I0203 10:40:00.898725 139774434195200 logging_writer.py:48] [152104] accumulated_eval_time=8255.095329, accumulated_logging_time=6.703703, accumulated_submission_time=69781.230156, global_step=152104, preemption_count=0, score=69781.230156, test/accuracy=0.556600, test/loss=1.947913, test/num_examples=10000, total_duration=78051.009329, train/accuracy=0.737051, train/loss=1.068966, validation/accuracy=0.681660, validation/loss=1.320082, validation/num_examples=50000
I0203 10:40:41.434997 139774417409792 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.1543405055999756, loss=3.86093807220459
I0203 10:41:27.533113 139774434195200 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.0210726261138916, loss=2.102229118347168
I0203 10:42:14.283106 139774417409792 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.1970841884613037, loss=1.9742600917816162
I0203 10:43:00.505002 139774434195200 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.1114983558654785, loss=1.9566879272460938
I0203 10:43:46.889451 139774417409792 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.17938232421875, loss=2.055515766143799
I0203 10:44:33.466324 139774434195200 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.012075901031494, loss=4.195868492126465
I0203 10:45:19.736869 139774417409792 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.019785165786743, loss=2.725126266479492
I0203 10:46:05.985291 139774434195200 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.30619740486145, loss=2.016822338104248
I0203 10:46:52.468544 139774417409792 logging_writer.py:48] [153000] global_step=153000, grad_norm=1.860062837600708, loss=2.72713041305542
I0203 10:47:00.892131 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:47:11.654689 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:47:47.562797 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:47:49.163666 139936116377408 submission_runner.py:408] Time since start: 78519.31s, 	Step: 153020, 	{'train/accuracy': 0.7427343726158142, 'train/loss': 1.0332081317901611, 'validation/accuracy': 0.6832399964332581, 'validation/loss': 1.3128416538238525, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 1.9452199935913086, 'test/num_examples': 10000, 'score': 70201.16539907455, 'total_duration': 78519.31473040581, 'accumulated_submission_time': 70201.16539907455, 'accumulated_eval_time': 8303.366862535477, 'accumulated_logging_time': 6.754567861557007}
I0203 10:47:49.202071 139774434195200 logging_writer.py:48] [153020] accumulated_eval_time=8303.366863, accumulated_logging_time=6.754568, accumulated_submission_time=70201.165399, global_step=153020, preemption_count=0, score=70201.165399, test/accuracy=0.561400, test/loss=1.945220, test/num_examples=10000, total_duration=78519.314730, train/accuracy=0.742734, train/loss=1.033208, validation/accuracy=0.683240, validation/loss=1.312842, validation/num_examples=50000
I0203 10:48:22.603317 139774417409792 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.0646109580993652, loss=1.9903056621551514
I0203 10:49:08.628317 139774434195200 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.11000919342041, loss=2.0521106719970703
I0203 10:49:55.137058 139774417409792 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.02237606048584, loss=2.20646071434021
I0203 10:50:41.704742 139774434195200 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.0734035968780518, loss=2.0762107372283936
I0203 10:51:28.223466 139774417409792 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.123859167098999, loss=2.1100480556488037
I0203 10:52:15.051693 139774434195200 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.172034978866577, loss=1.999629020690918
I0203 10:53:01.607656 139774417409792 logging_writer.py:48] [153700] global_step=153700, grad_norm=1.961032509803772, loss=4.294488430023193
I0203 10:53:48.109464 139774434195200 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.0107293128967285, loss=3.977041006088257
I0203 10:54:34.993324 139774417409792 logging_writer.py:48] [153900] global_step=153900, grad_norm=1.919715166091919, loss=2.2922635078430176
I0203 10:54:49.512577 139936116377408 spec.py:321] Evaluating on the training split.
I0203 10:54:59.975828 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 10:55:37.786365 139936116377408 spec.py:349] Evaluating on the test split.
I0203 10:55:39.398081 139936116377408 submission_runner.py:408] Time since start: 78989.55s, 	Step: 153933, 	{'train/accuracy': 0.7536913752555847, 'train/loss': 1.0018309354782104, 'validation/accuracy': 0.6841399669647217, 'validation/loss': 1.3143833875656128, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9567975997924805, 'test/num_examples': 10000, 'score': 70621.41844844818, 'total_duration': 78989.54914736748, 'accumulated_submission_time': 70621.41844844818, 'accumulated_eval_time': 8353.252356290817, 'accumulated_logging_time': 6.802517414093018}
I0203 10:55:39.438451 139774434195200 logging_writer.py:48] [153933] accumulated_eval_time=8353.252356, accumulated_logging_time=6.802517, accumulated_submission_time=70621.418448, global_step=153933, preemption_count=0, score=70621.418448, test/accuracy=0.555000, test/loss=1.956798, test/num_examples=10000, total_duration=78989.549147, train/accuracy=0.753691, train/loss=1.001831, validation/accuracy=0.684140, validation/loss=1.314383, validation/num_examples=50000
I0203 10:56:06.516966 139774417409792 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.3386189937591553, loss=2.082265615463257
I0203 10:56:52.566169 139774434195200 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.2076711654663086, loss=1.973477840423584
I0203 10:57:38.806722 139774417409792 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.0161893367767334, loss=2.1139345169067383
I0203 10:58:25.070268 139774434195200 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.1954259872436523, loss=2.0428123474121094
I0203 10:59:11.432967 139774417409792 logging_writer.py:48] [154400] global_step=154400, grad_norm=1.899778127670288, loss=2.706209659576416
I0203 10:59:57.707007 139774434195200 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.029991388320923, loss=3.2974660396575928
I0203 11:00:44.268811 139774417409792 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.018509864807129, loss=3.588444948196411
I0203 11:01:30.632653 139774434195200 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.1430089473724365, loss=1.9162079095840454
I0203 11:02:17.214391 139774417409792 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.0177652835845947, loss=2.31870174407959
I0203 11:02:39.708098 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:02:51.524171 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:03:27.165209 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:03:28.787888 139936116377408 submission_runner.py:408] Time since start: 79458.94s, 	Step: 154850, 	{'train/accuracy': 0.74916011095047, 'train/loss': 1.0158547163009644, 'validation/accuracy': 0.688539981842041, 'validation/loss': 1.2876722812652588, 'validation/num_examples': 50000, 'test/accuracy': 0.5651000142097473, 'test/loss': 1.925018310546875, 'test/num_examples': 10000, 'score': 71041.63038349152, 'total_duration': 79458.93891525269, 'accumulated_submission_time': 71041.63038349152, 'accumulated_eval_time': 8402.332137584686, 'accumulated_logging_time': 6.852922439575195}
I0203 11:03:28.838380 139774434195200 logging_writer.py:48] [154850] accumulated_eval_time=8402.332138, accumulated_logging_time=6.852922, accumulated_submission_time=71041.630383, global_step=154850, preemption_count=0, score=71041.630383, test/accuracy=0.565100, test/loss=1.925018, test/num_examples=10000, total_duration=79458.938915, train/accuracy=0.749160, train/loss=1.015855, validation/accuracy=0.688540, validation/loss=1.287672, validation/num_examples=50000
I0203 11:03:49.099093 139774417409792 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.150493621826172, loss=2.1258034706115723
I0203 11:04:34.326146 139774434195200 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.270897388458252, loss=1.9578999280929565
I0203 11:05:20.966452 139774417409792 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.1568708419799805, loss=1.947441816329956
I0203 11:06:07.630949 139774434195200 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.060298204421997, loss=2.1890692710876465
I0203 11:06:53.918813 139774417409792 logging_writer.py:48] [155300] global_step=155300, grad_norm=1.978134274482727, loss=4.326418399810791
I0203 11:07:40.492164 139774434195200 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.272954225540161, loss=1.9490360021591187
I0203 11:08:26.694896 139774417409792 logging_writer.py:48] [155500] global_step=155500, grad_norm=1.960792064666748, loss=2.569483757019043
I0203 11:09:13.209537 139774434195200 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.1583242416381836, loss=4.466098785400391
I0203 11:09:59.597111 139774417409792 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.164872884750366, loss=2.9762024879455566
I0203 11:10:29.036819 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:10:39.548399 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:11:17.705255 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:11:19.314838 139936116377408 submission_runner.py:408] Time since start: 79929.47s, 	Step: 155765, 	{'train/accuracy': 0.7540234327316284, 'train/loss': 0.9922083616256714, 'validation/accuracy': 0.6906799674034119, 'validation/loss': 1.2663934230804443, 'validation/num_examples': 50000, 'test/accuracy': 0.5700000524520874, 'test/loss': 1.9121655225753784, 'test/num_examples': 10000, 'score': 71461.77147507668, 'total_duration': 79929.46589922905, 'accumulated_submission_time': 71461.77147507668, 'accumulated_eval_time': 8452.610144615173, 'accumulated_logging_time': 6.913311243057251}
I0203 11:11:19.359019 139774434195200 logging_writer.py:48] [155765] accumulated_eval_time=8452.610145, accumulated_logging_time=6.913311, accumulated_submission_time=71461.771475, global_step=155765, preemption_count=0, score=71461.771475, test/accuracy=0.570000, test/loss=1.912166, test/num_examples=10000, total_duration=79929.465899, train/accuracy=0.754023, train/loss=0.992208, validation/accuracy=0.690680, validation/loss=1.266393, validation/num_examples=50000
I0203 11:11:33.674368 139774417409792 logging_writer.py:48] [155800] global_step=155800, grad_norm=1.9731978178024292, loss=3.77767276763916
I0203 11:12:18.164168 139774434195200 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.1786091327667236, loss=2.522301435470581
I0203 11:13:04.475085 139774417409792 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.1202430725097656, loss=2.3370521068573
I0203 11:13:50.846813 139774434195200 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.420506238937378, loss=1.9789363145828247
I0203 11:14:37.257730 139774417409792 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.1787936687469482, loss=1.82879638671875
I0203 11:15:23.679221 139774434195200 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.2353785037994385, loss=1.9557956457138062
I0203 11:16:10.396212 139774417409792 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.2185425758361816, loss=2.0524046421051025
I0203 11:16:56.646430 139774434195200 logging_writer.py:48] [156500] global_step=156500, grad_norm=1.8581360578536987, loss=3.257678747177124
I0203 11:17:43.119426 139774417409792 logging_writer.py:48] [156600] global_step=156600, grad_norm=1.9433621168136597, loss=2.4913930892944336
I0203 11:18:19.467002 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:18:30.005084 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:19:06.663095 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:19:08.270168 139936116377408 submission_runner.py:408] Time since start: 80398.42s, 	Step: 156680, 	{'train/accuracy': 0.7607421875, 'train/loss': 0.9670121073722839, 'validation/accuracy': 0.6918999552726746, 'validation/loss': 1.2740281820297241, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.89507257938385, 'test/num_examples': 10000, 'score': 71881.82072901726, 'total_duration': 80398.42122769356, 'accumulated_submission_time': 71881.82072901726, 'accumulated_eval_time': 8501.413291931152, 'accumulated_logging_time': 6.9684062004089355}
I0203 11:19:08.314479 139774434195200 logging_writer.py:48] [156680] accumulated_eval_time=8501.413292, accumulated_logging_time=6.968406, accumulated_submission_time=71881.820729, global_step=156680, preemption_count=0, score=71881.820729, test/accuracy=0.569900, test/loss=1.895073, test/num_examples=10000, total_duration=80398.421228, train/accuracy=0.760742, train/loss=0.967012, validation/accuracy=0.691900, validation/loss=1.274028, validation/num_examples=50000
I0203 11:19:16.649028 139774417409792 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.239082098007202, loss=2.2425198554992676
I0203 11:19:59.760783 139774434195200 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.2860288619995117, loss=4.326155185699463
I0203 11:20:46.141393 139774417409792 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.2644455432891846, loss=1.9381177425384521
I0203 11:21:32.588453 139774434195200 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.154916286468506, loss=2.3907923698425293
I0203 11:22:19.138665 139774417409792 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.2622077465057373, loss=1.9932060241699219
I0203 11:23:05.840065 139774434195200 logging_writer.py:48] [157200] global_step=157200, grad_norm=1.9629714488983154, loss=2.8672850131988525
I0203 11:23:52.065735 139774417409792 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.3773815631866455, loss=1.9288374185562134
I0203 11:24:38.564199 139774434195200 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.0232598781585693, loss=2.8239364624023438
I0203 11:25:25.287639 139774417409792 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.2247331142425537, loss=1.9453517198562622
I0203 11:26:08.514127 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:26:19.214373 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:26:56.029267 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:26:57.625263 139936116377408 submission_runner.py:408] Time since start: 80867.78s, 	Step: 157595, 	{'train/accuracy': 0.7591796517372131, 'train/loss': 0.9821126461029053, 'validation/accuracy': 0.6990199685096741, 'validation/loss': 1.2495800256729126, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.8638801574707031, 'test/num_examples': 10000, 'score': 72301.9625506401, 'total_duration': 80867.77632331848, 'accumulated_submission_time': 72301.9625506401, 'accumulated_eval_time': 8550.524432182312, 'accumulated_logging_time': 7.022604942321777}
I0203 11:26:57.669325 139774434195200 logging_writer.py:48] [157595] accumulated_eval_time=8550.524432, accumulated_logging_time=7.022605, accumulated_submission_time=72301.962551, global_step=157595, preemption_count=0, score=72301.962551, test/accuracy=0.583700, test/loss=1.863880, test/num_examples=10000, total_duration=80867.776323, train/accuracy=0.759180, train/loss=0.982113, validation/accuracy=0.699020, validation/loss=1.249580, validation/num_examples=50000
I0203 11:27:00.053747 139774417409792 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.344774007797241, loss=3.8397717475891113
I0203 11:27:42.706923 139774434195200 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.481499671936035, loss=1.8979068994522095
I0203 11:28:28.825752 139774417409792 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.1480963230133057, loss=1.9477914571762085
I0203 11:29:15.145231 139774434195200 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.1371774673461914, loss=1.7156566381454468
I0203 11:30:01.310082 139774417409792 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.2380383014678955, loss=1.8834799528121948
I0203 11:30:47.926305 139774434195200 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.105607271194458, loss=3.2300381660461426
I0203 11:31:34.453988 139774417409792 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.1818268299102783, loss=2.096195936203003
I0203 11:32:21.257015 139774434195200 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.497386932373047, loss=1.878559947013855
I0203 11:33:07.769575 139774417409792 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.5479302406311035, loss=1.9599298238754272
I0203 11:33:54.039182 139774434195200 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.461087465286255, loss=4.306502342224121
I0203 11:33:57.975958 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:34:08.581297 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:34:45.790748 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:34:47.399611 139936116377408 submission_runner.py:408] Time since start: 81337.55s, 	Step: 158510, 	{'train/accuracy': 0.7641015648841858, 'train/loss': 0.9493613243103027, 'validation/accuracy': 0.7014600038528442, 'validation/loss': 1.2222778797149658, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 1.8460190296173096, 'test/num_examples': 10000, 'score': 72722.21183228493, 'total_duration': 81337.55066609383, 'accumulated_submission_time': 72722.21183228493, 'accumulated_eval_time': 8599.948066473007, 'accumulated_logging_time': 7.076277017593384}
I0203 11:34:47.440196 139774417409792 logging_writer.py:48] [158510] accumulated_eval_time=8599.948066, accumulated_logging_time=7.076277, accumulated_submission_time=72722.211832, global_step=158510, preemption_count=0, score=72722.211832, test/accuracy=0.577500, test/loss=1.846019, test/num_examples=10000, total_duration=81337.550666, train/accuracy=0.764102, train/loss=0.949361, validation/accuracy=0.701460, validation/loss=1.222278, validation/num_examples=50000
I0203 11:35:25.441196 139774434195200 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.2743425369262695, loss=2.15327525138855
I0203 11:36:11.521807 139774417409792 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.665266513824463, loss=3.9452459812164307
I0203 11:36:57.634470 139774434195200 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.208944320678711, loss=2.4250450134277344
I0203 11:37:44.597081 139774417409792 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.297156572341919, loss=1.8829606771469116
I0203 11:38:31.117317 139774434195200 logging_writer.py:48] [159000] global_step=159000, grad_norm=1.911950945854187, loss=3.079139232635498
I0203 11:39:17.201138 139774417409792 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.088953971862793, loss=3.523144245147705
I0203 11:40:03.811907 139774434195200 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.305649518966675, loss=1.9921976327896118
I0203 11:40:50.003120 139774417409792 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.500135660171509, loss=1.98968505859375
I0203 11:41:36.731848 139774434195200 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.3263871669769287, loss=2.310399293899536
I0203 11:41:47.622912 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:41:58.193505 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:42:37.076447 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:42:38.677877 139936116377408 submission_runner.py:408] Time since start: 81808.83s, 	Step: 159425, 	{'train/accuracy': 0.7665234208106995, 'train/loss': 0.9387437701225281, 'validation/accuracy': 0.7026199698448181, 'validation/loss': 1.223870873451233, 'validation/num_examples': 50000, 'test/accuracy': 0.5818000435829163, 'test/loss': 1.855764389038086, 'test/num_examples': 10000, 'score': 73142.33656525612, 'total_duration': 81808.82894182205, 'accumulated_submission_time': 73142.33656525612, 'accumulated_eval_time': 8651.003014564514, 'accumulated_logging_time': 7.126868724822998}
I0203 11:42:38.720451 139774417409792 logging_writer.py:48] [159425] accumulated_eval_time=8651.003015, accumulated_logging_time=7.126869, accumulated_submission_time=73142.336565, global_step=159425, preemption_count=0, score=73142.336565, test/accuracy=0.581800, test/loss=1.855764, test/num_examples=10000, total_duration=81808.828942, train/accuracy=0.766523, train/loss=0.938744, validation/accuracy=0.702620, validation/loss=1.223871, validation/num_examples=50000
I0203 11:43:09.820488 139774434195200 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.485551595687866, loss=1.8219990730285645
I0203 11:43:55.738400 139774417409792 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.3883416652679443, loss=1.8415791988372803
I0203 11:44:42.351214 139774434195200 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.596334457397461, loss=1.840641975402832
I0203 11:45:29.056547 139774417409792 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.3073205947875977, loss=1.9390507936477661
I0203 11:46:15.741667 139774434195200 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.2190768718719482, loss=3.307614803314209
I0203 11:47:02.370265 139774417409792 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.0771830081939697, loss=2.7465834617614746
I0203 11:47:48.878407 139774434195200 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.477447986602783, loss=1.883130669593811
I0203 11:48:35.397144 139774417409792 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.282010793685913, loss=2.366070032119751
I0203 11:49:22.071656 139774434195200 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.2615251541137695, loss=3.535597085952759
I0203 11:49:39.019048 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:49:49.715439 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:50:26.728363 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:50:28.342888 139936116377408 submission_runner.py:408] Time since start: 82278.49s, 	Step: 160338, 	{'train/accuracy': 0.7646874785423279, 'train/loss': 0.9522948861122131, 'validation/accuracy': 0.7034400105476379, 'validation/loss': 1.2250497341156006, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.84183931350708, 'test/num_examples': 10000, 'score': 73562.57827568054, 'total_duration': 82278.49391198158, 'accumulated_submission_time': 73562.57827568054, 'accumulated_eval_time': 8700.326808214188, 'accumulated_logging_time': 7.178597688674927}
I0203 11:50:28.390977 139774417409792 logging_writer.py:48] [160338] accumulated_eval_time=8700.326808, accumulated_logging_time=7.178598, accumulated_submission_time=73562.578276, global_step=160338, preemption_count=0, score=73562.578276, test/accuracy=0.584800, test/loss=1.841839, test/num_examples=10000, total_duration=82278.493912, train/accuracy=0.764687, train/loss=0.952295, validation/accuracy=0.703440, validation/loss=1.225050, validation/num_examples=50000
I0203 11:50:53.523192 139774434195200 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.3869435787200928, loss=1.8079535961151123
I0203 11:51:39.650712 139774417409792 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.0026419162750244, loss=2.5271737575531006
I0203 11:52:26.059597 139774434195200 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.235400676727295, loss=2.7744405269622803
I0203 11:53:12.481323 139774417409792 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.4132261276245117, loss=3.3483221530914307
I0203 11:53:58.623635 139774434195200 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.377683162689209, loss=3.7518510818481445
I0203 11:54:44.917436 139774417409792 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.5507917404174805, loss=1.8181654214859009
I0203 11:55:31.475685 139774434195200 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.1496777534484863, loss=2.7776567935943604
I0203 11:56:17.921505 139774417409792 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.6456801891326904, loss=1.9726136922836304
I0203 11:57:04.386396 139774434195200 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.3060708045959473, loss=1.6925959587097168
I0203 11:57:28.596108 139936116377408 spec.py:321] Evaluating on the training split.
I0203 11:57:39.292310 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 11:58:17.658623 139936116377408 spec.py:349] Evaluating on the test split.
I0203 11:58:19.264861 139936116377408 submission_runner.py:408] Time since start: 82749.42s, 	Step: 161254, 	{'train/accuracy': 0.7704296708106995, 'train/loss': 0.934205174446106, 'validation/accuracy': 0.7053999900817871, 'validation/loss': 1.2130885124206543, 'validation/num_examples': 50000, 'test/accuracy': 0.5820000171661377, 'test/loss': 1.8352934122085571, 'test/num_examples': 10000, 'score': 73982.7239575386, 'total_duration': 82749.41592168808, 'accumulated_submission_time': 73982.7239575386, 'accumulated_eval_time': 8750.995544433594, 'accumulated_logging_time': 7.237669229507446}
I0203 11:58:19.308696 139774417409792 logging_writer.py:48] [161254] accumulated_eval_time=8750.995544, accumulated_logging_time=7.237669, accumulated_submission_time=73982.723958, global_step=161254, preemption_count=0, score=73982.723958, test/accuracy=0.582000, test/loss=1.835293, test/num_examples=10000, total_duration=82749.415922, train/accuracy=0.770430, train/loss=0.934205, validation/accuracy=0.705400, validation/loss=1.213089, validation/num_examples=50000
I0203 11:58:37.966280 139774434195200 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.6037890911102295, loss=1.984147071838379
I0203 11:59:22.783405 139774417409792 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.188589572906494, loss=3.518993377685547
I0203 12:00:09.123801 139774434195200 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.3953487873077393, loss=1.7725322246551514
I0203 12:00:55.547008 139774417409792 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.6210875511169434, loss=1.8749332427978516
I0203 12:01:42.099975 139774434195200 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.472059965133667, loss=2.2723076343536377
I0203 12:02:28.537789 139774417409792 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.2118618488311768, loss=3.6320290565490723
I0203 12:03:15.028877 139774434195200 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.275965929031372, loss=1.7896623611450195
I0203 12:04:01.381126 139774417409792 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.41878080368042, loss=2.7748899459838867
I0203 12:04:47.569875 139774434195200 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.6049957275390625, loss=1.786550760269165
I0203 12:05:19.308799 139936116377408 spec.py:321] Evaluating on the training split.
I0203 12:05:30.039991 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 12:06:07.973274 139936116377408 spec.py:349] Evaluating on the test split.
I0203 12:06:09.578354 139936116377408 submission_runner.py:408] Time since start: 83219.73s, 	Step: 162170, 	{'train/accuracy': 0.7777734398841858, 'train/loss': 0.8995881676673889, 'validation/accuracy': 0.7101399898529053, 'validation/loss': 1.1974650621414185, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8181536197662354, 'test/num_examples': 10000, 'score': 74402.66515946388, 'total_duration': 83219.72941589355, 'accumulated_submission_time': 74402.66515946388, 'accumulated_eval_time': 8801.265083551407, 'accumulated_logging_time': 7.292815208435059}
I0203 12:06:09.619686 139774417409792 logging_writer.py:48] [162170] accumulated_eval_time=8801.265084, accumulated_logging_time=7.292815, accumulated_submission_time=74402.665159, global_step=162170, preemption_count=0, score=74402.665159, test/accuracy=0.590700, test/loss=1.818154, test/num_examples=10000, total_duration=83219.729416, train/accuracy=0.777773, train/loss=0.899588, validation/accuracy=0.710140, validation/loss=1.197465, validation/num_examples=50000
I0203 12:06:21.935772 139774434195200 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.732844352722168, loss=1.8779494762420654
I0203 12:07:05.888729 139774417409792 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.5166733264923096, loss=4.063758373260498
I0203 12:07:52.013876 139774434195200 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.456894874572754, loss=1.8321458101272583
I0203 12:08:38.481302 139774417409792 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.478297710418701, loss=1.7157905101776123
I0203 12:09:24.817589 139774434195200 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.3491768836975098, loss=1.9937334060668945
I0203 12:10:11.550200 139774417409792 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.3457045555114746, loss=2.337894916534424
I0203 12:10:57.786404 139774434195200 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.6845815181732178, loss=4.362857818603516
I0203 12:11:44.539687 139774417409792 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.8051061630249023, loss=1.7626681327819824
I0203 12:12:31.088645 139774434195200 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.6295337677001953, loss=1.890368938446045
I0203 12:13:09.666107 139936116377408 spec.py:321] Evaluating on the training split.
I0203 12:13:20.306638 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 12:13:56.496168 139936116377408 spec.py:349] Evaluating on the test split.
I0203 12:13:58.101450 139936116377408 submission_runner.py:408] Time since start: 83688.25s, 	Step: 163085, 	{'train/accuracy': 0.7899999618530273, 'train/loss': 0.8373759984970093, 'validation/accuracy': 0.7140799760818481, 'validation/loss': 1.1738150119781494, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.8106621503829956, 'test/num_examples': 10000, 'score': 74822.65410661697, 'total_duration': 83688.25251555443, 'accumulated_submission_time': 74822.65410661697, 'accumulated_eval_time': 8849.700415611267, 'accumulated_logging_time': 7.344009160995483}
I0203 12:13:58.142483 139774417409792 logging_writer.py:48] [163085] accumulated_eval_time=8849.700416, accumulated_logging_time=7.344009, accumulated_submission_time=74822.654107, global_step=163085, preemption_count=0, score=74822.654107, test/accuracy=0.590300, test/loss=1.810662, test/num_examples=10000, total_duration=83688.252516, train/accuracy=0.790000, train/loss=0.837376, validation/accuracy=0.714080, validation/loss=1.173815, validation/num_examples=50000
I0203 12:14:04.518504 139774434195200 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.417398691177368, loss=1.7284380197525024
I0203 12:14:47.477404 139774417409792 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.34104585647583, loss=2.0820648670196533
I0203 12:15:33.456404 139774434195200 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.3262691497802734, loss=2.8973567485809326
I0203 12:16:20.023540 139774417409792 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.3286893367767334, loss=2.2034223079681396
I0203 12:17:06.292421 139774434195200 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.615786075592041, loss=1.7027504444122314
I0203 12:17:52.699181 139774417409792 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.716904878616333, loss=4.199619293212891
I0203 12:18:39.243032 139774434195200 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.5941734313964844, loss=1.743250846862793
I0203 12:19:25.908962 139774417409792 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.7388477325439453, loss=1.734527349472046
I0203 12:20:12.338397 139774434195200 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.5951879024505615, loss=1.8034138679504395
I0203 12:20:58.883007 139774417409792 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.5957491397857666, loss=1.824265956878662
I0203 12:20:58.894891 139936116377408 spec.py:321] Evaluating on the training split.
I0203 12:21:09.769771 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 12:21:45.666028 139936116377408 spec.py:349] Evaluating on the test split.
I0203 12:21:47.269731 139936116377408 submission_runner.py:408] Time since start: 84157.42s, 	Step: 164001, 	{'train/accuracy': 0.7776171565055847, 'train/loss': 0.8835697174072266, 'validation/accuracy': 0.7133199572563171, 'validation/loss': 1.1664485931396484, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.779520034790039, 'test/num_examples': 10000, 'score': 75243.34757304192, 'total_duration': 84157.42079758644, 'accumulated_submission_time': 75243.34757304192, 'accumulated_eval_time': 8898.0752389431, 'accumulated_logging_time': 7.395813226699829}
I0203 12:21:47.309588 139774434195200 logging_writer.py:48] [164001] accumulated_eval_time=8898.075239, accumulated_logging_time=7.395813, accumulated_submission_time=75243.347573, global_step=164001, preemption_count=0, score=75243.347573, test/accuracy=0.592400, test/loss=1.779520, test/num_examples=10000, total_duration=84157.420798, train/accuracy=0.777617, train/loss=0.883570, validation/accuracy=0.713320, validation/loss=1.166449, validation/num_examples=50000
I0203 12:22:29.196609 139774417409792 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.458078622817993, loss=3.07521390914917
I0203 12:23:15.235681 139774434195200 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.4625978469848633, loss=1.9291108846664429
I0203 12:24:01.517364 139774417409792 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.666435480117798, loss=1.8038339614868164
I0203 12:24:47.758296 139774434195200 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.2992913722991943, loss=3.2541451454162598
I0203 12:25:34.232821 139774417409792 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.625480890274048, loss=1.7995038032531738
I0203 12:26:20.450172 139774434195200 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.669624090194702, loss=1.7616262435913086
I0203 12:27:06.694320 139774417409792 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.708395481109619, loss=1.7645769119262695
I0203 12:27:52.820422 139774434195200 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.6546645164489746, loss=1.7401800155639648
I0203 12:28:38.980461 139774417409792 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.48213529586792, loss=1.9370834827423096
I0203 12:28:47.375851 139936116377408 spec.py:321] Evaluating on the training split.
I0203 12:28:58.273521 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 12:29:35.222777 139936116377408 spec.py:349] Evaluating on the test split.
I0203 12:29:36.836073 139936116377408 submission_runner.py:408] Time since start: 84626.99s, 	Step: 164920, 	{'train/accuracy': 0.7840234041213989, 'train/loss': 0.8810456395149231, 'validation/accuracy': 0.7141799926757812, 'validation/loss': 1.1800652742385864, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.803371548652649, 'test/num_examples': 10000, 'score': 75663.35647845268, 'total_duration': 84626.98712992668, 'accumulated_submission_time': 75663.35647845268, 'accumulated_eval_time': 8947.53544473648, 'accumulated_logging_time': 7.44543981552124}
I0203 12:29:36.878473 139774434195200 logging_writer.py:48] [164920] accumulated_eval_time=8947.535445, accumulated_logging_time=7.445440, accumulated_submission_time=75663.356478, global_step=164920, preemption_count=0, score=75663.356478, test/accuracy=0.591400, test/loss=1.803372, test/num_examples=10000, total_duration=84626.987130, train/accuracy=0.784023, train/loss=0.881046, validation/accuracy=0.714180, validation/loss=1.180065, validation/num_examples=50000
I0203 12:30:10.301338 139774417409792 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.503915548324585, loss=1.814022183418274
I0203 12:30:56.167419 139774434195200 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.7497801780700684, loss=1.8208723068237305
I0203 12:31:42.990710 139774417409792 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.4946813583374023, loss=1.8018124103546143
I0203 12:32:29.445310 139774434195200 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.9181506633758545, loss=4.268771171569824
I0203 12:33:15.813576 139774417409792 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.9361929893493652, loss=1.7350972890853882
I0203 12:34:02.163236 139774434195200 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.4518439769744873, loss=2.2142672538757324
I0203 12:34:48.472791 139774417409792 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.6992006301879883, loss=1.8301937580108643
I0203 12:35:34.761113 139774434195200 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.0182628631591797, loss=1.7496001720428467
I0203 12:36:21.032414 139774417409792 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.739567995071411, loss=3.9283390045166016
I0203 12:36:37.128885 139936116377408 spec.py:321] Evaluating on the training split.
I0203 12:36:47.936278 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 12:37:25.759902 139936116377408 spec.py:349] Evaluating on the test split.
I0203 12:37:27.374892 139936116377408 submission_runner.py:408] Time since start: 85097.53s, 	Step: 165837, 	{'train/accuracy': 0.7928906083106995, 'train/loss': 0.8252041339874268, 'validation/accuracy': 0.7172600030899048, 'validation/loss': 1.1588102579116821, 'validation/num_examples': 50000, 'test/accuracy': 0.5946000218391418, 'test/loss': 1.7701854705810547, 'test/num_examples': 10000, 'score': 76083.54992222786, 'total_duration': 85097.52592563629, 'accumulated_submission_time': 76083.54992222786, 'accumulated_eval_time': 8997.781407117844, 'accumulated_logging_time': 7.497044086456299}
I0203 12:37:27.421126 139774434195200 logging_writer.py:48] [165837] accumulated_eval_time=8997.781407, accumulated_logging_time=7.497044, accumulated_submission_time=76083.549922, global_step=165837, preemption_count=0, score=76083.549922, test/accuracy=0.594600, test/loss=1.770185, test/num_examples=10000, total_duration=85097.525926, train/accuracy=0.792891, train/loss=0.825204, validation/accuracy=0.717260, validation/loss=1.158810, validation/num_examples=50000
I0203 12:37:52.930032 139774417409792 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.041755437850952, loss=4.309826374053955
I0203 12:38:39.005070 139774434195200 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.8189218044281006, loss=1.764512062072754
I0203 12:39:25.603735 139774417409792 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.664053201675415, loss=1.9032894372940063
I0203 12:40:12.256691 139774434195200 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.7459769248962402, loss=1.7108056545257568
I0203 12:40:58.340640 139774417409792 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.7436623573303223, loss=1.8433700799942017
I0203 12:41:45.018256 139774434195200 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.6472418308258057, loss=3.7585811614990234
I0203 12:42:31.268995 139774417409792 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.882904052734375, loss=1.8402681350708008
I0203 12:43:17.543374 139774434195200 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.916198492050171, loss=4.251949310302734
I0203 12:44:04.035910 139774417409792 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.772416353225708, loss=1.6322400569915771
I0203 12:44:27.718723 139936116377408 spec.py:321] Evaluating on the training split.
I0203 12:44:38.267785 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 12:45:16.419285 139936116377408 spec.py:349] Evaluating on the test split.
I0203 12:45:18.048565 139936116377408 submission_runner.py:408] Time since start: 85568.20s, 	Step: 166753, 	{'train/accuracy': 0.7876757383346558, 'train/loss': 0.8453723788261414, 'validation/accuracy': 0.7234599590301514, 'validation/loss': 1.131539225578308, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.7537925243377686, 'test/num_examples': 10000, 'score': 76503.78957104683, 'total_duration': 85568.19960737228, 'accumulated_submission_time': 76503.78957104683, 'accumulated_eval_time': 9048.111221551895, 'accumulated_logging_time': 7.553990364074707}
I0203 12:45:18.097136 139774434195200 logging_writer.py:48] [166753] accumulated_eval_time=9048.111222, accumulated_logging_time=7.553990, accumulated_submission_time=76503.789571, global_step=166753, preemption_count=0, score=76503.789571, test/accuracy=0.604600, test/loss=1.753793, test/num_examples=10000, total_duration=85568.199607, train/accuracy=0.787676, train/loss=0.845372, validation/accuracy=0.723460, validation/loss=1.131539, validation/num_examples=50000
I0203 12:45:37.165175 139774417409792 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.78562068939209, loss=2.3786447048187256
I0203 12:46:22.058872 139774434195200 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.60280179977417, loss=2.5234265327453613
I0203 12:47:08.290621 139774417409792 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.7219808101654053, loss=1.7480859756469727
I0203 12:47:54.704077 139774434195200 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.7990550994873047, loss=2.175506591796875
I0203 12:48:40.784643 139774417409792 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.952012538909912, loss=1.7964938879013062
I0203 12:49:27.162274 139774434195200 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.5082151889801025, loss=2.4738762378692627
I0203 12:50:13.466006 139774417409792 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.6229307651519775, loss=2.8202273845672607
I0203 12:50:59.740563 139774434195200 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.939654588699341, loss=1.6282716989517212
I0203 12:51:46.170368 139774417409792 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.7808170318603516, loss=1.7512973546981812
I0203 12:52:18.500355 139936116377408 spec.py:321] Evaluating on the training split.
I0203 12:52:29.310886 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 12:53:07.383909 139936116377408 spec.py:349] Evaluating on the test split.
I0203 12:53:08.988046 139936116377408 submission_runner.py:408] Time since start: 86039.14s, 	Step: 167671, 	{'train/accuracy': 0.7952343821525574, 'train/loss': 0.8193172812461853, 'validation/accuracy': 0.7249599695205688, 'validation/loss': 1.1146950721740723, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.7299996614456177, 'test/num_examples': 10000, 'score': 76924.13359379768, 'total_duration': 86039.13908457756, 'accumulated_submission_time': 76924.13359379768, 'accumulated_eval_time': 9098.598886728287, 'accumulated_logging_time': 7.613273620605469}
I0203 12:53:09.034461 139774434195200 logging_writer.py:48] [167671] accumulated_eval_time=9098.598887, accumulated_logging_time=7.613274, accumulated_submission_time=76924.133594, global_step=167671, preemption_count=0, score=76924.133594, test/accuracy=0.604100, test/loss=1.730000, test/num_examples=10000, total_duration=86039.139085, train/accuracy=0.795234, train/loss=0.819317, validation/accuracy=0.724960, validation/loss=1.114695, validation/num_examples=50000
I0203 12:53:20.958211 139774417409792 logging_writer.py:48] [167700] global_step=167700, grad_norm=3.090907573699951, loss=4.264145374298096
I0203 12:54:04.490913 139774434195200 logging_writer.py:48] [167800] global_step=167800, grad_norm=3.0642662048339844, loss=1.762660026550293
I0203 12:54:50.595179 139774417409792 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.960336923599243, loss=1.8919020891189575
I0203 12:55:37.183511 139774434195200 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.7200615406036377, loss=1.6400017738342285
I0203 12:56:23.574667 139774417409792 logging_writer.py:48] [168100] global_step=168100, grad_norm=2.852094888687134, loss=2.1125059127807617
I0203 12:57:10.029473 139774434195200 logging_writer.py:48] [168200] global_step=168200, grad_norm=3.0655598640441895, loss=1.7800003290176392
I0203 12:57:56.242122 139774417409792 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.8241946697235107, loss=1.6759241819381714
I0203 12:58:42.477244 139774434195200 logging_writer.py:48] [168400] global_step=168400, grad_norm=2.979506254196167, loss=3.957226276397705
I0203 12:59:29.001842 139774417409792 logging_writer.py:48] [168500] global_step=168500, grad_norm=2.6633434295654297, loss=1.8270599842071533
I0203 13:00:09.461084 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:00:20.126621 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:00:58.812459 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:01:00.420375 139936116377408 submission_runner.py:408] Time since start: 86510.57s, 	Step: 168589, 	{'train/accuracy': 0.8006640672683716, 'train/loss': 0.809798002243042, 'validation/accuracy': 0.7245999574661255, 'validation/loss': 1.1315559148788452, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.7418795824050903, 'test/num_examples': 10000, 'score': 77344.50248265266, 'total_duration': 86510.57143831253, 'accumulated_submission_time': 77344.50248265266, 'accumulated_eval_time': 9149.558176994324, 'accumulated_logging_time': 7.669828414916992}
I0203 13:01:00.465466 139774434195200 logging_writer.py:48] [168589] accumulated_eval_time=9149.558177, accumulated_logging_time=7.669828, accumulated_submission_time=77344.502483, global_step=168589, preemption_count=0, score=77344.502483, test/accuracy=0.604500, test/loss=1.741880, test/num_examples=10000, total_duration=86510.571438, train/accuracy=0.800664, train/loss=0.809798, validation/accuracy=0.724600, validation/loss=1.131556, validation/num_examples=50000
I0203 13:01:05.230811 139774417409792 logging_writer.py:48] [168600] global_step=168600, grad_norm=3.1019253730773926, loss=1.8952466249465942
I0203 13:01:48.221457 139774434195200 logging_writer.py:48] [168700] global_step=168700, grad_norm=2.6455652713775635, loss=3.4057235717773438
I0203 13:02:34.088094 139774417409792 logging_writer.py:48] [168800] global_step=168800, grad_norm=3.0682666301727295, loss=4.076567649841309
I0203 13:03:20.748362 139774434195200 logging_writer.py:48] [168900] global_step=168900, grad_norm=3.0344133377075195, loss=1.58297860622406
I0203 13:03:56.087856 139774417409792 logging_writer.py:48] [168978] global_step=168978, preemption_count=0, score=77520.032673
I0203 13:03:56.699771 139936116377408 checkpoints.py:490] Saving checkpoint at step: 168978
I0203 13:03:58.038730 139936116377408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_4/checkpoint_168978
I0203 13:03:58.059475 139936116377408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_4/checkpoint_168978.
I0203 13:03:58.871665 139936116377408 submission_runner.py:583] Tuning trial 4/5
I0203 13:03:58.871896 139936116377408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0203 13:03:58.887280 139936116377408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009570312104187906, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.151187896728516, 'total_duration': 65.77166390419006, 'accumulated_submission_time': 36.151187896728516, 'accumulated_eval_time': 29.620367527008057, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (861, {'train/accuracy': 0.033964842557907104, 'train/loss': 5.889985084533691, 'validation/accuracy': 0.031219998374581337, 'validation/loss': 5.931602478027344, 'validation/num_examples': 50000, 'test/accuracy': 0.026500001549720764, 'test/loss': 6.062020778656006, 'test/num_examples': 10000, 'score': 456.1083743572235, 'total_duration': 532.0756976604462, 'accumulated_submission_time': 456.1083743572235, 'accumulated_eval_time': 75.90391826629639, 'accumulated_logging_time': 0.017279624938964844, 'global_step': 861, 'preemption_count': 0}), (1774, {'train/accuracy': 0.07195312529802322, 'train/loss': 5.346263408660889, 'validation/accuracy': 0.06695999950170517, 'validation/loss': 5.416959285736084, 'validation/num_examples': 50000, 'test/accuracy': 0.05350000411272049, 'test/loss': 5.638273239135742, 'test/num_examples': 10000, 'score': 876.1209945678711, 'total_duration': 1000.3806068897247, 'accumulated_submission_time': 876.1209945678711, 'accumulated_eval_time': 124.11227488517761, 'accumulated_logging_time': 0.05303072929382324, 'global_step': 1774, 'preemption_count': 0}), (2694, {'train/accuracy': 0.1066601574420929, 'train/loss': 4.961225986480713, 'validation/accuracy': 0.09883999824523926, 'validation/loss': 5.016429424285889, 'validation/num_examples': 50000, 'test/accuracy': 0.07660000026226044, 'test/loss': 5.27877950668335, 'test/num_examples': 10000, 'score': 1296.1530268192291, 'total_duration': 1474.3475806713104, 'accumulated_submission_time': 1296.1530268192291, 'accumulated_eval_time': 177.97129225730896, 'accumulated_logging_time': 0.08097434043884277, 'global_step': 2694, 'preemption_count': 0}), (3614, {'train/accuracy': 0.12578125298023224, 'train/loss': 4.762739181518555, 'validation/accuracy': 0.1183599978685379, 'validation/loss': 4.825972557067871, 'validation/num_examples': 50000, 'test/accuracy': 0.0958000048995018, 'test/loss': 5.121191024780273, 'test/num_examples': 10000, 'score': 1716.2821543216705, 'total_duration': 1942.9452345371246, 'accumulated_submission_time': 1716.2821543216705, 'accumulated_eval_time': 226.36552715301514, 'accumulated_logging_time': 0.10770201683044434, 'global_step': 3614, 'preemption_count': 0}), (4529, {'train/accuracy': 0.16820311546325684, 'train/loss': 4.37608528137207, 'validation/accuracy': 0.1546199917793274, 'validation/loss': 4.472805976867676, 'validation/num_examples': 50000, 'test/accuracy': 0.11570000648498535, 'test/loss': 4.830338478088379, 'test/num_examples': 10000, 'score': 2136.238264322281, 'total_duration': 2407.637862443924, 'accumulated_submission_time': 2136.238264322281, 'accumulated_eval_time': 271.0300600528717, 'accumulated_logging_time': 0.1323871612548828, 'global_step': 4529, 'preemption_count': 0}), (5445, {'train/accuracy': 0.18382811546325684, 'train/loss': 4.257326126098633, 'validation/accuracy': 0.1714800000190735, 'validation/loss': 4.334697246551514, 'validation/num_examples': 50000, 'test/accuracy': 0.13300000131130219, 'test/loss': 4.718831539154053, 'test/num_examples': 10000, 'score': 2556.621108531952, 'total_duration': 2877.380870580673, 'accumulated_submission_time': 2556.621108531952, 'accumulated_eval_time': 320.31780982017517, 'accumulated_logging_time': 0.15650534629821777, 'global_step': 5445, 'preemption_count': 0}), (6364, {'train/accuracy': 0.22624999284744263, 'train/loss': 3.872288942337036, 'validation/accuracy': 0.21079999208450317, 'validation/loss': 3.9770443439483643, 'validation/num_examples': 50000, 'test/accuracy': 0.1616000086069107, 'test/loss': 4.43961763381958, 'test/num_examples': 10000, 'score': 2976.629909515381, 'total_duration': 3347.430968284607, 'accumulated_submission_time': 2976.629909515381, 'accumulated_eval_time': 370.2815811634064, 'accumulated_logging_time': 0.1853024959564209, 'global_step': 6364, 'preemption_count': 0}), (7280, {'train/accuracy': 0.2319921851158142, 'train/loss': 3.833322286605835, 'validation/accuracy': 0.21235999464988708, 'validation/loss': 3.9645304679870605, 'validation/num_examples': 50000, 'test/accuracy': 0.17000000178813934, 'test/loss': 4.424200534820557, 'test/num_examples': 10000, 'score': 3396.913996696472, 'total_duration': 3817.68110537529, 'accumulated_submission_time': 3396.913996696472, 'accumulated_eval_time': 420.16905403137207, 'accumulated_logging_time': 0.21693873405456543, 'global_step': 7280, 'preemption_count': 0}), (8200, {'train/accuracy': 0.25533202290534973, 'train/loss': 3.758265256881714, 'validation/accuracy': 0.22007998824119568, 'validation/loss': 3.949136257171631, 'validation/num_examples': 50000, 'test/accuracy': 0.17180000245571136, 'test/loss': 4.393675327301025, 'test/num_examples': 10000, 'score': 3816.8563516139984, 'total_duration': 4286.148283243179, 'accumulated_submission_time': 3816.8563516139984, 'accumulated_eval_time': 468.6132698059082, 'accumulated_logging_time': 0.24872469902038574, 'global_step': 8200, 'preemption_count': 0}), (9118, {'train/accuracy': 0.26787108182907104, 'train/loss': 3.58646821975708, 'validation/accuracy': 0.2466599941253662, 'validation/loss': 3.7260239124298096, 'validation/num_examples': 50000, 'test/accuracy': 0.19190001487731934, 'test/loss': 4.206649303436279, 'test/num_examples': 10000, 'score': 4236.834023237228, 'total_duration': 4752.014056444168, 'accumulated_submission_time': 4236.834023237228, 'accumulated_eval_time': 514.4226930141449, 'accumulated_logging_time': 0.2796754837036133, 'global_step': 9118, 'preemption_count': 0}), (10034, {'train/accuracy': 0.2818945348262787, 'train/loss': 3.467654228210449, 'validation/accuracy': 0.26075997948646545, 'validation/loss': 3.6061902046203613, 'validation/num_examples': 50000, 'test/accuracy': 0.2006000131368637, 'test/loss': 4.112731456756592, 'test/num_examples': 10000, 'score': 4656.83242058754, 'total_duration': 5220.361520528793, 'accumulated_submission_time': 4656.83242058754, 'accumulated_eval_time': 562.69517993927, 'accumulated_logging_time': 0.3083176612854004, 'global_step': 10034, 'preemption_count': 0}), (10952, {'train/accuracy': 0.29472655057907104, 'train/loss': 3.4312899112701416, 'validation/accuracy': 0.259660005569458, 'validation/loss': 3.6437604427337646, 'validation/num_examples': 50000, 'test/accuracy': 0.19820000231266022, 'test/loss': 4.1670637130737305, 'test/num_examples': 10000, 'score': 5076.973096132278, 'total_duration': 5687.518299818039, 'accumulated_submission_time': 5076.973096132278, 'accumulated_eval_time': 609.6357562541962, 'accumulated_logging_time': 0.3355536460876465, 'global_step': 10952, 'preemption_count': 0}), (11870, {'train/accuracy': 0.28761717677116394, 'train/loss': 3.4422073364257812, 'validation/accuracy': 0.2721000015735626, 'validation/loss': 3.556260108947754, 'validation/num_examples': 50000, 'test/accuracy': 0.20550000667572021, 'test/loss': 4.087541103363037, 'test/num_examples': 10000, 'score': 5497.2170152664185, 'total_duration': 6156.984508275986, 'accumulated_submission_time': 5497.2170152664185, 'accumulated_eval_time': 658.7836654186249, 'accumulated_logging_time': 0.36228036880493164, 'global_step': 11870, 'preemption_count': 0}), (12789, {'train/accuracy': 0.29289060831069946, 'train/loss': 3.4402520656585693, 'validation/accuracy': 0.26941999793052673, 'validation/loss': 3.5773630142211914, 'validation/num_examples': 50000, 'test/accuracy': 0.2110000103712082, 'test/loss': 4.108162879943848, 'test/num_examples': 10000, 'score': 5917.322836399078, 'total_duration': 6620.591611146927, 'accumulated_submission_time': 5917.322836399078, 'accumulated_eval_time': 702.2066161632538, 'accumulated_logging_time': 0.39301395416259766, 'global_step': 12789, 'preemption_count': 0}), (13707, {'train/accuracy': 0.30980467796325684, 'train/loss': 3.3494856357574463, 'validation/accuracy': 0.28251999616622925, 'validation/loss': 3.5177979469299316, 'validation/num_examples': 50000, 'test/accuracy': 0.21400001645088196, 'test/loss': 4.0196003913879395, 'test/num_examples': 10000, 'score': 6337.505370855331, 'total_duration': 7088.974200248718, 'accumulated_submission_time': 6337.505370855331, 'accumulated_eval_time': 750.3280100822449, 'accumulated_logging_time': 0.42362475395202637, 'global_step': 13707, 'preemption_count': 0}), (14625, {'train/accuracy': 0.3052343726158142, 'train/loss': 3.417556047439575, 'validation/accuracy': 0.2822999954223633, 'validation/loss': 3.541224956512451, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.021054744720459, 'test/num_examples': 10000, 'score': 6757.771499872208, 'total_duration': 7557.805434465408, 'accumulated_submission_time': 6757.771499872208, 'accumulated_eval_time': 798.8193001747131, 'accumulated_logging_time': 0.4494767189025879, 'global_step': 14625, 'preemption_count': 0}), (15539, {'train/accuracy': 0.31101560592651367, 'train/loss': 3.333627939224243, 'validation/accuracy': 0.29071998596191406, 'validation/loss': 3.459397315979004, 'validation/num_examples': 50000, 'test/accuracy': 0.226500004529953, 'test/loss': 3.976407527923584, 'test/num_examples': 10000, 'score': 7177.714713335037, 'total_duration': 8025.688539028168, 'accumulated_submission_time': 7177.714713335037, 'accumulated_eval_time': 846.682626247406, 'accumulated_logging_time': 0.4787595272064209, 'global_step': 15539, 'preemption_count': 0}), (16458, {'train/accuracy': 0.3293554484844208, 'train/loss': 3.1784443855285645, 'validation/accuracy': 0.30295997858047485, 'validation/loss': 3.3324337005615234, 'validation/num_examples': 50000, 'test/accuracy': 0.23240001499652863, 'test/loss': 3.8720884323120117, 'test/num_examples': 10000, 'score': 7597.7491154670715, 'total_duration': 8495.327637910843, 'accumulated_submission_time': 7597.7491154670715, 'accumulated_eval_time': 896.20419049263, 'accumulated_logging_time': 0.5077810287475586, 'global_step': 16458, 'preemption_count': 0}), (17377, {'train/accuracy': 0.32539060711860657, 'train/loss': 3.243004083633423, 'validation/accuracy': 0.30079999566078186, 'validation/loss': 3.385110855102539, 'validation/num_examples': 50000, 'test/accuracy': 0.2331000119447708, 'test/loss': 3.922811985015869, 'test/num_examples': 10000, 'score': 8017.729243755341, 'total_duration': 8965.22508430481, 'accumulated_submission_time': 8017.729243755341, 'accumulated_eval_time': 946.0458581447601, 'accumulated_logging_time': 0.5358819961547852, 'global_step': 17377, 'preemption_count': 0}), (18295, {'train/accuracy': 0.34615233540534973, 'train/loss': 3.063856363296509, 'validation/accuracy': 0.3191399872303009, 'validation/loss': 3.214115619659424, 'validation/num_examples': 50000, 'test/accuracy': 0.24240000545978546, 'test/loss': 3.7857863903045654, 'test/num_examples': 10000, 'score': 8438.02011179924, 'total_duration': 9434.594047546387, 'accumulated_submission_time': 8438.02011179924, 'accumulated_eval_time': 995.0433971881866, 'accumulated_logging_time': 0.5683629512786865, 'global_step': 18295, 'preemption_count': 0}), (19213, {'train/accuracy': 0.3359765410423279, 'train/loss': 3.1254475116729736, 'validation/accuracy': 0.30570000410079956, 'validation/loss': 3.2792983055114746, 'validation/num_examples': 50000, 'test/accuracy': 0.23990000784397125, 'test/loss': 3.8324697017669678, 'test/num_examples': 10000, 'score': 8858.02958726883, 'total_duration': 9906.623125314713, 'accumulated_submission_time': 8858.02958726883, 'accumulated_eval_time': 1046.9851393699646, 'accumulated_logging_time': 0.5980954170227051, 'global_step': 19213, 'preemption_count': 0}), (20132, {'train/accuracy': 0.3737109303474426, 'train/loss': 2.931885004043579, 'validation/accuracy': 0.3187199831008911, 'validation/loss': 3.236717939376831, 'validation/num_examples': 50000, 'test/accuracy': 0.241100013256073, 'test/loss': 3.7928264141082764, 'test/num_examples': 10000, 'score': 9278.196017742157, 'total_duration': 10376.468310594559, 'accumulated_submission_time': 9278.196017742157, 'accumulated_eval_time': 1096.579419374466, 'accumulated_logging_time': 0.625751256942749, 'global_step': 20132, 'preemption_count': 0}), (21052, {'train/accuracy': 0.3433007597923279, 'train/loss': 3.070847988128662, 'validation/accuracy': 0.31977999210357666, 'validation/loss': 3.210434913635254, 'validation/num_examples': 50000, 'test/accuracy': 0.24820001423358917, 'test/loss': 3.769993543624878, 'test/num_examples': 10000, 'score': 9698.436318159103, 'total_duration': 10848.208649396896, 'accumulated_submission_time': 9698.436318159103, 'accumulated_eval_time': 1148.0055141448975, 'accumulated_logging_time': 0.6522841453552246, 'global_step': 21052, 'preemption_count': 0}), (21964, {'train/accuracy': 0.3420703113079071, 'train/loss': 3.094113349914551, 'validation/accuracy': 0.32277998328208923, 'validation/loss': 3.2260701656341553, 'validation/num_examples': 50000, 'test/accuracy': 0.24150000512599945, 'test/loss': 3.7991697788238525, 'test/num_examples': 10000, 'score': 10118.706419706345, 'total_duration': 11317.737473249435, 'accumulated_submission_time': 10118.706419706345, 'accumulated_eval_time': 1197.1896917819977, 'accumulated_logging_time': 0.6798815727233887, 'global_step': 21964, 'preemption_count': 0}), (22880, {'train/accuracy': 0.36865234375, 'train/loss': 2.973356008529663, 'validation/accuracy': 0.3253600001335144, 'validation/loss': 3.211477041244507, 'validation/num_examples': 50000, 'test/accuracy': 0.2574000060558319, 'test/loss': 3.756904125213623, 'test/num_examples': 10000, 'score': 10538.827253580093, 'total_duration': 11786.644594669342, 'accumulated_submission_time': 10538.827253580093, 'accumulated_eval_time': 1245.8942565917969, 'accumulated_logging_time': 0.7139434814453125, 'global_step': 22880, 'preemption_count': 0}), (23795, {'train/accuracy': 0.3464648425579071, 'train/loss': 3.0866899490356445, 'validation/accuracy': 0.3257800042629242, 'validation/loss': 3.2104380130767822, 'validation/num_examples': 50000, 'test/accuracy': 0.24940000474452972, 'test/loss': 3.7716026306152344, 'test/num_examples': 10000, 'score': 10958.82033610344, 'total_duration': 12256.103848218918, 'accumulated_submission_time': 10958.82033610344, 'accumulated_eval_time': 1295.278584241867, 'accumulated_logging_time': 0.7482728958129883, 'global_step': 23795, 'preemption_count': 0}), (24713, {'train/accuracy': 0.35957029461860657, 'train/loss': 2.9951577186584473, 'validation/accuracy': 0.333079993724823, 'validation/loss': 3.153878688812256, 'validation/num_examples': 50000, 'test/accuracy': 0.2598000168800354, 'test/loss': 3.7074122428894043, 'test/num_examples': 10000, 'score': 11378.957997083664, 'total_duration': 12725.690438747406, 'accumulated_submission_time': 11378.957997083664, 'accumulated_eval_time': 1344.6447319984436, 'accumulated_logging_time': 0.7822833061218262, 'global_step': 24713, 'preemption_count': 0}), (25628, {'train/accuracy': 0.37269529700279236, 'train/loss': 2.9379703998565674, 'validation/accuracy': 0.34002000093460083, 'validation/loss': 3.1157100200653076, 'validation/num_examples': 50000, 'test/accuracy': 0.25760000944137573, 'test/loss': 3.7011423110961914, 'test/num_examples': 10000, 'score': 11799.026058912277, 'total_duration': 13195.52159357071, 'accumulated_submission_time': 11799.026058912277, 'accumulated_eval_time': 1394.3293986320496, 'accumulated_logging_time': 0.8127717971801758, 'global_step': 25628, 'preemption_count': 0}), (26542, {'train/accuracy': 0.36146482825279236, 'train/loss': 2.9840753078460693, 'validation/accuracy': 0.3368600010871887, 'validation/loss': 3.117708444595337, 'validation/num_examples': 50000, 'test/accuracy': 0.2600000202655792, 'test/loss': 3.6868534088134766, 'test/num_examples': 10000, 'score': 12219.03768491745, 'total_duration': 13665.651197195053, 'accumulated_submission_time': 12219.03768491745, 'accumulated_eval_time': 1444.3692378997803, 'accumulated_logging_time': 0.843348503112793, 'global_step': 26542, 'preemption_count': 0}), (27462, {'train/accuracy': 0.3483007848262787, 'train/loss': 3.1170191764831543, 'validation/accuracy': 0.32401999831199646, 'validation/loss': 3.26419997215271, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 3.807725191116333, 'test/num_examples': 10000, 'score': 12639.100955963135, 'total_duration': 14135.43448138237, 'accumulated_submission_time': 12639.100955963135, 'accumulated_eval_time': 1494.0119626522064, 'accumulated_logging_time': 0.8733620643615723, 'global_step': 27462, 'preemption_count': 0}), (28380, {'train/accuracy': 0.36894530057907104, 'train/loss': 2.9343044757843018, 'validation/accuracy': 0.33861997723579407, 'validation/loss': 3.1066977977752686, 'validation/num_examples': 50000, 'test/accuracy': 0.2630999982357025, 'test/loss': 3.679342269897461, 'test/num_examples': 10000, 'score': 13059.384540557861, 'total_duration': 14607.170650720596, 'accumulated_submission_time': 13059.384540557861, 'accumulated_eval_time': 1545.383987903595, 'accumulated_logging_time': 0.9063313007354736, 'global_step': 28380, 'preemption_count': 0}), (29298, {'train/accuracy': 0.37068358063697815, 'train/loss': 2.9274771213531494, 'validation/accuracy': 0.3446199893951416, 'validation/loss': 3.0712943077087402, 'validation/num_examples': 50000, 'test/accuracy': 0.2648000121116638, 'test/loss': 3.661438465118408, 'test/num_examples': 10000, 'score': 13479.487174987793, 'total_duration': 15077.99607181549, 'accumulated_submission_time': 13479.487174987793, 'accumulated_eval_time': 1596.0251424312592, 'accumulated_logging_time': 0.9394416809082031, 'global_step': 29298, 'preemption_count': 0}), (30217, {'train/accuracy': 0.3766210973262787, 'train/loss': 2.902115821838379, 'validation/accuracy': 0.3519800007343292, 'validation/loss': 3.029473066329956, 'validation/num_examples': 50000, 'test/accuracy': 0.2742000222206116, 'test/loss': 3.6279361248016357, 'test/num_examples': 10000, 'score': 13899.69462966919, 'total_duration': 15548.25105714798, 'accumulated_submission_time': 13899.69462966919, 'accumulated_eval_time': 1645.9867305755615, 'accumulated_logging_time': 0.9761998653411865, 'global_step': 30217, 'preemption_count': 0}), (31135, {'train/accuracy': 0.3855859339237213, 'train/loss': 2.8547158241271973, 'validation/accuracy': 0.3534599840641022, 'validation/loss': 3.0370798110961914, 'validation/num_examples': 50000, 'test/accuracy': 0.2705000042915344, 'test/loss': 3.6340298652648926, 'test/num_examples': 10000, 'score': 14319.800573825836, 'total_duration': 16018.214548110962, 'accumulated_submission_time': 14319.800573825836, 'accumulated_eval_time': 1695.7626497745514, 'accumulated_logging_time': 1.0101087093353271, 'global_step': 31135, 'preemption_count': 0}), (32055, {'train/accuracy': 0.4124804735183716, 'train/loss': 2.7808024883270264, 'validation/accuracy': 0.3519199788570404, 'validation/loss': 3.0841052532196045, 'validation/num_examples': 50000, 'test/accuracy': 0.2671000063419342, 'test/loss': 3.660425901412964, 'test/num_examples': 10000, 'score': 14739.811116695404, 'total_duration': 16486.448800325394, 'accumulated_submission_time': 14739.811116695404, 'accumulated_eval_time': 1743.906052350998, 'accumulated_logging_time': 1.0428571701049805, 'global_step': 32055, 'preemption_count': 0}), (32972, {'train/accuracy': 0.38671875, 'train/loss': 2.8052151203155518, 'validation/accuracy': 0.36127999424934387, 'validation/loss': 2.94804048538208, 'validation/num_examples': 50000, 'test/accuracy': 0.27730000019073486, 'test/loss': 3.5375256538391113, 'test/num_examples': 10000, 'score': 15160.133778810501, 'total_duration': 16955.91092300415, 'accumulated_submission_time': 15160.133778810501, 'accumulated_eval_time': 1792.961481332779, 'accumulated_logging_time': 1.0791571140289307, 'global_step': 32972, 'preemption_count': 0}), (33891, {'train/accuracy': 0.3903124928474426, 'train/loss': 2.8002209663391113, 'validation/accuracy': 0.36399999260902405, 'validation/loss': 2.969886541366577, 'validation/num_examples': 50000, 'test/accuracy': 0.2771000266075134, 'test/loss': 3.5811426639556885, 'test/num_examples': 10000, 'score': 15580.551282167435, 'total_duration': 17422.885044813156, 'accumulated_submission_time': 15580.551282167435, 'accumulated_eval_time': 1839.4359893798828, 'accumulated_logging_time': 1.1127994060516357, 'global_step': 33891, 'preemption_count': 0}), (34809, {'train/accuracy': 0.3956054747104645, 'train/loss': 2.79667067527771, 'validation/accuracy': 0.35711997747421265, 'validation/loss': 3.022451400756836, 'validation/num_examples': 50000, 'test/accuracy': 0.26840001344680786, 'test/loss': 3.636655569076538, 'test/num_examples': 10000, 'score': 16000.717082977295, 'total_duration': 17892.378203630447, 'accumulated_submission_time': 16000.717082977295, 'accumulated_eval_time': 1888.6756381988525, 'accumulated_logging_time': 1.1519043445587158, 'global_step': 34809, 'preemption_count': 0}), (35725, {'train/accuracy': 0.390937477350235, 'train/loss': 2.7831339836120605, 'validation/accuracy': 0.3679399788379669, 'validation/loss': 2.916210174560547, 'validation/num_examples': 50000, 'test/accuracy': 0.2833000123500824, 'test/loss': 3.526043176651001, 'test/num_examples': 10000, 'score': 16421.02041387558, 'total_duration': 18362.607944726944, 'accumulated_submission_time': 16421.02041387558, 'accumulated_eval_time': 1938.5241289138794, 'accumulated_logging_time': 1.1824181079864502, 'global_step': 35725, 'preemption_count': 0}), (36639, {'train/accuracy': 0.38343748450279236, 'train/loss': 2.84735369682312, 'validation/accuracy': 0.3583599925041199, 'validation/loss': 2.9957029819488525, 'validation/num_examples': 50000, 'test/accuracy': 0.2743000090122223, 'test/loss': 3.578441619873047, 'test/num_examples': 10000, 'score': 16841.228150367737, 'total_duration': 18832.27735710144, 'accumulated_submission_time': 16841.228150367737, 'accumulated_eval_time': 1987.9012160301208, 'accumulated_logging_time': 1.2189600467681885, 'global_step': 36639, 'preemption_count': 0}), (37555, {'train/accuracy': 0.3958398401737213, 'train/loss': 2.8110783100128174, 'validation/accuracy': 0.36098000407218933, 'validation/loss': 3.0116312503814697, 'validation/num_examples': 50000, 'test/accuracy': 0.27650001645088196, 'test/loss': 3.5944924354553223, 'test/num_examples': 10000, 'score': 17261.399400949478, 'total_duration': 19304.57182574272, 'accumulated_submission_time': 17261.399400949478, 'accumulated_eval_time': 2039.933144569397, 'accumulated_logging_time': 1.262007474899292, 'global_step': 37555, 'preemption_count': 0}), (38473, {'train/accuracy': 0.4010546803474426, 'train/loss': 2.771629571914673, 'validation/accuracy': 0.37567999958992004, 'validation/loss': 2.9098598957061768, 'validation/num_examples': 50000, 'test/accuracy': 0.29420000314712524, 'test/loss': 3.5065810680389404, 'test/num_examples': 10000, 'score': 17681.434158086777, 'total_duration': 19773.024648189545, 'accumulated_submission_time': 17681.434158086777, 'accumulated_eval_time': 2088.265805721283, 'accumulated_logging_time': 1.299102783203125, 'global_step': 38473, 'preemption_count': 0}), (39390, {'train/accuracy': 0.39265623688697815, 'train/loss': 2.793609142303467, 'validation/accuracy': 0.36813998222351074, 'validation/loss': 2.933591842651367, 'validation/num_examples': 50000, 'test/accuracy': 0.28300002217292786, 'test/loss': 3.5438942909240723, 'test/num_examples': 10000, 'score': 18101.503092050552, 'total_duration': 20242.74775648117, 'accumulated_submission_time': 18101.503092050552, 'accumulated_eval_time': 2137.8387155532837, 'accumulated_logging_time': 1.3325514793395996, 'global_step': 39390, 'preemption_count': 0}), (40307, {'train/accuracy': 0.4039843678474426, 'train/loss': 2.782717227935791, 'validation/accuracy': 0.3682200014591217, 'validation/loss': 2.971782684326172, 'validation/num_examples': 50000, 'test/accuracy': 0.28610000014305115, 'test/loss': 3.549136161804199, 'test/num_examples': 10000, 'score': 18521.691387176514, 'total_duration': 20712.3809030056, 'accumulated_submission_time': 18521.691387176514, 'accumulated_eval_time': 2187.1993803977966, 'accumulated_logging_time': 1.3684730529785156, 'global_step': 40307, 'preemption_count': 0}), (41227, {'train/accuracy': 0.4065038859844208, 'train/loss': 2.7331039905548096, 'validation/accuracy': 0.3854199945926666, 'validation/loss': 2.870001792907715, 'validation/num_examples': 50000, 'test/accuracy': 0.29590001702308655, 'test/loss': 3.476227045059204, 'test/num_examples': 10000, 'score': 18941.72615456581, 'total_duration': 21185.718816757202, 'accumulated_submission_time': 18941.72615456581, 'accumulated_eval_time': 2240.424861431122, 'accumulated_logging_time': 1.3985400199890137, 'global_step': 41227, 'preemption_count': 0}), (42145, {'train/accuracy': 0.41316404938697815, 'train/loss': 2.6345086097717285, 'validation/accuracy': 0.38152000308036804, 'validation/loss': 2.8117265701293945, 'validation/num_examples': 50000, 'test/accuracy': 0.29250001907348633, 'test/loss': 3.437443971633911, 'test/num_examples': 10000, 'score': 19362.027856588364, 'total_duration': 21655.337735414505, 'accumulated_submission_time': 19362.027856588364, 'accumulated_eval_time': 2289.662645339966, 'accumulated_logging_time': 1.4311163425445557, 'global_step': 42145, 'preemption_count': 0}), (43061, {'train/accuracy': 0.4229101538658142, 'train/loss': 2.6399965286254883, 'validation/accuracy': 0.3895399868488312, 'validation/loss': 2.8090786933898926, 'validation/num_examples': 50000, 'test/accuracy': 0.30640000104904175, 'test/loss': 3.3924434185028076, 'test/num_examples': 10000, 'score': 19782.140295267105, 'total_duration': 22124.104640245438, 'accumulated_submission_time': 19782.140295267105, 'accumulated_eval_time': 2338.235716342926, 'accumulated_logging_time': 1.4649834632873535, 'global_step': 43061, 'preemption_count': 0}), (43978, {'train/accuracy': 0.44771483540534973, 'train/loss': 2.5128729343414307, 'validation/accuracy': 0.38387998938560486, 'validation/loss': 2.8571627140045166, 'validation/num_examples': 50000, 'test/accuracy': 0.29500001668930054, 'test/loss': 3.4772708415985107, 'test/num_examples': 10000, 'score': 20202.28423190117, 'total_duration': 22594.31020140648, 'accumulated_submission_time': 20202.28423190117, 'accumulated_eval_time': 2388.2113218307495, 'accumulated_logging_time': 1.503532886505127, 'global_step': 43978, 'preemption_count': 0}), (44895, {'train/accuracy': 0.42130857706069946, 'train/loss': 2.619121551513672, 'validation/accuracy': 0.3991200029850006, 'validation/loss': 2.7532565593719482, 'validation/num_examples': 50000, 'test/accuracy': 0.3067000210285187, 'test/loss': 3.367946147918701, 'test/num_examples': 10000, 'score': 20622.277943134308, 'total_duration': 23065.153266191483, 'accumulated_submission_time': 20622.277943134308, 'accumulated_eval_time': 2438.9718708992004, 'accumulated_logging_time': 1.5404622554779053, 'global_step': 44895, 'preemption_count': 0}), (45814, {'train/accuracy': 0.41191405057907104, 'train/loss': 2.6856353282928467, 'validation/accuracy': 0.38457998633384705, 'validation/loss': 2.8403830528259277, 'validation/num_examples': 50000, 'test/accuracy': 0.29260000586509705, 'test/loss': 3.459824323654175, 'test/num_examples': 10000, 'score': 21042.53992986679, 'total_duration': 23536.338141679764, 'accumulated_submission_time': 21042.53992986679, 'accumulated_eval_time': 2489.8106729984283, 'accumulated_logging_time': 1.576117992401123, 'global_step': 45814, 'preemption_count': 0}), (46727, {'train/accuracy': 0.4382031261920929, 'train/loss': 2.527977466583252, 'validation/accuracy': 0.3956199884414673, 'validation/loss': 2.7620689868927, 'validation/num_examples': 50000, 'test/accuracy': 0.3050000071525574, 'test/loss': 3.409043550491333, 'test/num_examples': 10000, 'score': 21462.657836198807, 'total_duration': 24007.31149339676, 'accumulated_submission_time': 21462.657836198807, 'accumulated_eval_time': 2540.581508398056, 'accumulated_logging_time': 1.6131682395935059, 'global_step': 46727, 'preemption_count': 0}), (47643, {'train/accuracy': 0.4313085973262787, 'train/loss': 2.561516046524048, 'validation/accuracy': 0.40535998344421387, 'validation/loss': 2.7094690799713135, 'validation/num_examples': 50000, 'test/accuracy': 0.31050002574920654, 'test/loss': 3.3404970169067383, 'test/num_examples': 10000, 'score': 21882.891329288483, 'total_duration': 24478.50642466545, 'accumulated_submission_time': 21882.891329288483, 'accumulated_eval_time': 2591.4612271785736, 'accumulated_logging_time': 1.6478898525238037, 'global_step': 47643, 'preemption_count': 0}), (48561, {'train/accuracy': 0.4389062523841858, 'train/loss': 2.5233049392700195, 'validation/accuracy': 0.41290000081062317, 'validation/loss': 2.669914960861206, 'validation/num_examples': 50000, 'test/accuracy': 0.31790000200271606, 'test/loss': 3.2796595096588135, 'test/num_examples': 10000, 'score': 22302.96504020691, 'total_duration': 24948.79293680191, 'accumulated_submission_time': 22302.96504020691, 'accumulated_eval_time': 2641.5904109477997, 'accumulated_logging_time': 1.6836578845977783, 'global_step': 48561, 'preemption_count': 0}), (49474, {'train/accuracy': 0.449531227350235, 'train/loss': 2.4620165824890137, 'validation/accuracy': 0.4113599956035614, 'validation/loss': 2.6755542755126953, 'validation/num_examples': 50000, 'test/accuracy': 0.3184000253677368, 'test/loss': 3.2931008338928223, 'test/num_examples': 10000, 'score': 22723.290951013565, 'total_duration': 25419.697157859802, 'accumulated_submission_time': 22723.290951013565, 'accumulated_eval_time': 2692.083906888962, 'accumulated_logging_time': 1.7213480472564697, 'global_step': 49474, 'preemption_count': 0}), (50386, {'train/accuracy': 0.4435351490974426, 'train/loss': 2.5222740173339844, 'validation/accuracy': 0.4126800000667572, 'validation/loss': 2.6682076454162598, 'validation/num_examples': 50000, 'test/accuracy': 0.31390002369880676, 'test/loss': 3.3136818408966064, 'test/num_examples': 10000, 'score': 23143.52159333229, 'total_duration': 25886.496681451797, 'accumulated_submission_time': 23143.52159333229, 'accumulated_eval_time': 2738.5725288391113, 'accumulated_logging_time': 1.7537884712219238, 'global_step': 50386, 'preemption_count': 0}), (51300, {'train/accuracy': 0.4445898234844208, 'train/loss': 2.4773788452148438, 'validation/accuracy': 0.4203200042247772, 'validation/loss': 2.636164903640747, 'validation/num_examples': 50000, 'test/accuracy': 0.3272000253200531, 'test/loss': 3.2604527473449707, 'test/num_examples': 10000, 'score': 23563.827215909958, 'total_duration': 26356.45927453041, 'accumulated_submission_time': 23563.827215909958, 'accumulated_eval_time': 2788.143606901169, 'accumulated_logging_time': 1.7920589447021484, 'global_step': 51300, 'preemption_count': 0}), (52216, {'train/accuracy': 0.438789039850235, 'train/loss': 2.5263781547546387, 'validation/accuracy': 0.4053199887275696, 'validation/loss': 2.713552474975586, 'validation/num_examples': 50000, 'test/accuracy': 0.31220000982284546, 'test/loss': 3.3333580493927, 'test/num_examples': 10000, 'score': 23983.90644145012, 'total_duration': 26825.42811512947, 'accumulated_submission_time': 23983.90644145012, 'accumulated_eval_time': 2836.9467310905457, 'accumulated_logging_time': 1.830620288848877, 'global_step': 52216, 'preemption_count': 0}), (53133, {'train/accuracy': 0.4564843773841858, 'train/loss': 2.438621759414673, 'validation/accuracy': 0.4238399863243103, 'validation/loss': 2.607288122177124, 'validation/num_examples': 50000, 'test/accuracy': 0.32520002126693726, 'test/loss': 3.2333431243896484, 'test/num_examples': 10000, 'score': 24404.016840696335, 'total_duration': 27293.931302309036, 'accumulated_submission_time': 24404.016840696335, 'accumulated_eval_time': 2885.250978946686, 'accumulated_logging_time': 1.871053695678711, 'global_step': 53133, 'preemption_count': 0}), (54054, {'train/accuracy': 0.4422265589237213, 'train/loss': 2.535531759262085, 'validation/accuracy': 0.412200003862381, 'validation/loss': 2.7084314823150635, 'validation/num_examples': 50000, 'test/accuracy': 0.3118000030517578, 'test/loss': 3.3063266277313232, 'test/num_examples': 10000, 'score': 24824.366614103317, 'total_duration': 27765.620665311813, 'accumulated_submission_time': 24824.366614103317, 'accumulated_eval_time': 2936.507098197937, 'accumulated_logging_time': 1.9068207740783691, 'global_step': 54054, 'preemption_count': 0}), (54973, {'train/accuracy': 0.44408202171325684, 'train/loss': 2.5492329597473145, 'validation/accuracy': 0.41053998470306396, 'validation/loss': 2.712507724761963, 'validation/num_examples': 50000, 'test/accuracy': 0.31690001487731934, 'test/loss': 3.3160958290100098, 'test/num_examples': 10000, 'score': 25244.459810972214, 'total_duration': 28235.698424577713, 'accumulated_submission_time': 25244.459810972214, 'accumulated_eval_time': 2986.406905412674, 'accumulated_logging_time': 1.9439399242401123, 'global_step': 54973, 'preemption_count': 0}), (55889, {'train/accuracy': 0.47236326336860657, 'train/loss': 2.383117198944092, 'validation/accuracy': 0.4037799835205078, 'validation/loss': 2.7402095794677734, 'validation/num_examples': 50000, 'test/accuracy': 0.31610000133514404, 'test/loss': 3.364776611328125, 'test/num_examples': 10000, 'score': 25664.43350338936, 'total_duration': 28705.693928956985, 'accumulated_submission_time': 25664.43350338936, 'accumulated_eval_time': 3036.3440520763397, 'accumulated_logging_time': 1.9801304340362549, 'global_step': 55889, 'preemption_count': 0}), (56807, {'train/accuracy': 0.4465624988079071, 'train/loss': 2.5229108333587646, 'validation/accuracy': 0.4216800034046173, 'validation/loss': 2.661752700805664, 'validation/num_examples': 50000, 'test/accuracy': 0.3288000226020813, 'test/loss': 3.2641468048095703, 'test/num_examples': 10000, 'score': 26084.50500845909, 'total_duration': 29175.84260368347, 'accumulated_submission_time': 26084.50500845909, 'accumulated_eval_time': 3086.3352172374725, 'accumulated_logging_time': 2.0175254344940186, 'global_step': 56807, 'preemption_count': 0}), (57728, {'train/accuracy': 0.45119139552116394, 'train/loss': 2.4399993419647217, 'validation/accuracy': 0.42455998063087463, 'validation/loss': 2.5960423946380615, 'validation/num_examples': 50000, 'test/accuracy': 0.33420002460479736, 'test/loss': 3.2178361415863037, 'test/num_examples': 10000, 'score': 26504.705290555954, 'total_duration': 29644.84923171997, 'accumulated_submission_time': 26504.705290555954, 'accumulated_eval_time': 3135.060056447983, 'accumulated_logging_time': 2.0510499477386475, 'global_step': 57728, 'preemption_count': 0}), (58645, {'train/accuracy': 0.47408202290534973, 'train/loss': 2.372753143310547, 'validation/accuracy': 0.42541998624801636, 'validation/loss': 2.6239047050476074, 'validation/num_examples': 50000, 'test/accuracy': 0.33310002088546753, 'test/loss': 3.236710786819458, 'test/num_examples': 10000, 'score': 26924.908395767212, 'total_duration': 30115.69566130638, 'accumulated_submission_time': 26924.908395767212, 'accumulated_eval_time': 3185.6205384731293, 'accumulated_logging_time': 2.085714817047119, 'global_step': 58645, 'preemption_count': 0}), (59558, {'train/accuracy': 0.4577734172344208, 'train/loss': 2.429400682449341, 'validation/accuracy': 0.42837998270988464, 'validation/loss': 2.5803024768829346, 'validation/num_examples': 50000, 'test/accuracy': 0.3296000063419342, 'test/loss': 3.227254867553711, 'test/num_examples': 10000, 'score': 27344.886063098907, 'total_duration': 30585.716195106506, 'accumulated_submission_time': 27344.886063098907, 'accumulated_eval_time': 3235.580410003662, 'accumulated_logging_time': 2.121338367462158, 'global_step': 59558, 'preemption_count': 0}), (60474, {'train/accuracy': 0.46486327052116394, 'train/loss': 2.3890140056610107, 'validation/accuracy': 0.4313199818134308, 'validation/loss': 2.562873363494873, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.1996138095855713, 'test/num_examples': 10000, 'score': 27764.862775564194, 'total_duration': 31054.29045343399, 'accumulated_submission_time': 27764.862775564194, 'accumulated_eval_time': 3284.088232278824, 'accumulated_logging_time': 2.162022352218628, 'global_step': 60474, 'preemption_count': 0}), (61391, {'train/accuracy': 0.46708983182907104, 'train/loss': 2.3804032802581787, 'validation/accuracy': 0.42931997776031494, 'validation/loss': 2.585036277770996, 'validation/num_examples': 50000, 'test/accuracy': 0.3352000117301941, 'test/loss': 3.2316999435424805, 'test/num_examples': 10000, 'score': 28185.063593387604, 'total_duration': 31523.559364318848, 'accumulated_submission_time': 28185.063593387604, 'accumulated_eval_time': 3333.0700438022614, 'accumulated_logging_time': 2.200597047805786, 'global_step': 61391, 'preemption_count': 0}), (62310, {'train/accuracy': 0.46433591842651367, 'train/loss': 2.393752336502075, 'validation/accuracy': 0.43361997604370117, 'validation/loss': 2.5538320541381836, 'validation/num_examples': 50000, 'test/accuracy': 0.3335000276565552, 'test/loss': 3.2018165588378906, 'test/num_examples': 10000, 'score': 28604.98014640808, 'total_duration': 31993.887003183365, 'accumulated_submission_time': 28604.98014640808, 'accumulated_eval_time': 3383.3891365528107, 'accumulated_logging_time': 2.243925094604492, 'global_step': 62310, 'preemption_count': 0}), (63227, {'train/accuracy': 0.4670117199420929, 'train/loss': 2.3691554069519043, 'validation/accuracy': 0.4369799792766571, 'validation/loss': 2.5394680500030518, 'validation/num_examples': 50000, 'test/accuracy': 0.3427000045776367, 'test/loss': 3.135342836380005, 'test/num_examples': 10000, 'score': 29025.158362150192, 'total_duration': 32463.748566627502, 'accumulated_submission_time': 29025.158362150192, 'accumulated_eval_time': 3432.9850244522095, 'accumulated_logging_time': 2.2831294536590576, 'global_step': 63227, 'preemption_count': 0}), (64144, {'train/accuracy': 0.4650976359844208, 'train/loss': 2.3967294692993164, 'validation/accuracy': 0.4300599992275238, 'validation/loss': 2.5977704524993896, 'validation/num_examples': 50000, 'test/accuracy': 0.3273000121116638, 'test/loss': 3.223234176635742, 'test/num_examples': 10000, 'score': 29445.27424812317, 'total_duration': 32934.04869699478, 'accumulated_submission_time': 29445.27424812317, 'accumulated_eval_time': 3483.0788888931274, 'accumulated_logging_time': 2.325901746749878, 'global_step': 64144, 'preemption_count': 0}), (65062, {'train/accuracy': 0.4678320288658142, 'train/loss': 2.389435291290283, 'validation/accuracy': 0.4369399845600128, 'validation/loss': 2.5648996829986572, 'validation/num_examples': 50000, 'test/accuracy': 0.3362000286579132, 'test/loss': 3.1964430809020996, 'test/num_examples': 10000, 'score': 29865.45224094391, 'total_duration': 33405.08270573616, 'accumulated_submission_time': 29865.45224094391, 'accumulated_eval_time': 3533.843167066574, 'accumulated_logging_time': 2.3687186241149902, 'global_step': 65062, 'preemption_count': 0}), (65982, {'train/accuracy': 0.4744921624660492, 'train/loss': 2.419340133666992, 'validation/accuracy': 0.43595999479293823, 'validation/loss': 2.5876500606536865, 'validation/num_examples': 50000, 'test/accuracy': 0.3450000286102295, 'test/loss': 3.2141637802124023, 'test/num_examples': 10000, 'score': 30285.408737182617, 'total_duration': 33876.14655208588, 'accumulated_submission_time': 30285.408737182617, 'accumulated_eval_time': 3584.863637447357, 'accumulated_logging_time': 2.4079158306121826, 'global_step': 65982, 'preemption_count': 0}), (66901, {'train/accuracy': 0.47376951575279236, 'train/loss': 2.3729798793792725, 'validation/accuracy': 0.44200000166893005, 'validation/loss': 2.548090696334839, 'validation/num_examples': 50000, 'test/accuracy': 0.340800017118454, 'test/loss': 3.17934513092041, 'test/num_examples': 10000, 'score': 30705.5859375, 'total_duration': 34346.95048165321, 'accumulated_submission_time': 30705.5859375, 'accumulated_eval_time': 3635.3954322338104, 'accumulated_logging_time': 2.4538557529449463, 'global_step': 66901, 'preemption_count': 0}), (67819, {'train/accuracy': 0.5081835985183716, 'train/loss': 2.1930553913116455, 'validation/accuracy': 0.4444599747657776, 'validation/loss': 2.5196900367736816, 'validation/num_examples': 50000, 'test/accuracy': 0.3449000120162964, 'test/loss': 3.149144172668457, 'test/num_examples': 10000, 'score': 31125.757450580597, 'total_duration': 34814.36068201065, 'accumulated_submission_time': 31125.757450580597, 'accumulated_eval_time': 3682.5514616966248, 'accumulated_logging_time': 2.488304853439331, 'global_step': 67819, 'preemption_count': 0}), (68738, {'train/accuracy': 0.47441405057907104, 'train/loss': 2.358579397201538, 'validation/accuracy': 0.44589999318122864, 'validation/loss': 2.502725124359131, 'validation/num_examples': 50000, 'test/accuracy': 0.3493000268936157, 'test/loss': 3.140624761581421, 'test/num_examples': 10000, 'score': 31545.980580091476, 'total_duration': 35283.56374049187, 'accumulated_submission_time': 31545.980580091476, 'accumulated_eval_time': 3731.446517467499, 'accumulated_logging_time': 2.525161027908325, 'global_step': 68738, 'preemption_count': 0}), (69655, {'train/accuracy': 0.48808592557907104, 'train/loss': 2.284952402114868, 'validation/accuracy': 0.45097997784614563, 'validation/loss': 2.4512698650360107, 'validation/num_examples': 50000, 'test/accuracy': 0.35630002617836, 'test/loss': 3.0855069160461426, 'test/num_examples': 10000, 'score': 31966.211909532547, 'total_duration': 35753.97013711929, 'accumulated_submission_time': 31966.211909532547, 'accumulated_eval_time': 3781.5328879356384, 'accumulated_logging_time': 2.565286159515381, 'global_step': 69655, 'preemption_count': 0}), (70571, {'train/accuracy': 0.5066015720367432, 'train/loss': 2.156141519546509, 'validation/accuracy': 0.45743998885154724, 'validation/loss': 2.423490285873413, 'validation/num_examples': 50000, 'test/accuracy': 0.36260002851486206, 'test/loss': 3.076205253601074, 'test/num_examples': 10000, 'score': 32386.341804027557, 'total_duration': 36222.48383450508, 'accumulated_submission_time': 32386.341804027557, 'accumulated_eval_time': 3829.8283665180206, 'accumulated_logging_time': 2.6060354709625244, 'global_step': 70571, 'preemption_count': 0}), (71488, {'train/accuracy': 0.4865429699420929, 'train/loss': 2.26175856590271, 'validation/accuracy': 0.45819997787475586, 'validation/loss': 2.418018102645874, 'validation/num_examples': 50000, 'test/accuracy': 0.35910001397132874, 'test/loss': 3.0786125659942627, 'test/num_examples': 10000, 'score': 32806.55907249451, 'total_duration': 36692.19506430626, 'accumulated_submission_time': 32806.55907249451, 'accumulated_eval_time': 3879.22545838356, 'accumulated_logging_time': 2.655019521713257, 'global_step': 71488, 'preemption_count': 0}), (72406, {'train/accuracy': 0.4870898425579071, 'train/loss': 2.2414968013763428, 'validation/accuracy': 0.4551199972629547, 'validation/loss': 2.416773796081543, 'validation/num_examples': 50000, 'test/accuracy': 0.35440000891685486, 'test/loss': 3.063166618347168, 'test/num_examples': 10000, 'score': 33226.725727796555, 'total_duration': 37162.81544137001, 'accumulated_submission_time': 33226.725727796555, 'accumulated_eval_time': 3929.594414949417, 'accumulated_logging_time': 2.6921746730804443, 'global_step': 72406, 'preemption_count': 0}), (73322, {'train/accuracy': 0.48949217796325684, 'train/loss': 2.2488605976104736, 'validation/accuracy': 0.45325997471809387, 'validation/loss': 2.4476559162139893, 'validation/num_examples': 50000, 'test/accuracy': 0.3473000228404999, 'test/loss': 3.1173243522644043, 'test/num_examples': 10000, 'score': 33647.000801324844, 'total_duration': 37633.78448009491, 'accumulated_submission_time': 33647.000801324844, 'accumulated_eval_time': 3980.205307483673, 'accumulated_logging_time': 2.726961135864258, 'global_step': 73322, 'preemption_count': 0}), (74239, {'train/accuracy': 0.49150389432907104, 'train/loss': 2.291461944580078, 'validation/accuracy': 0.4583199918270111, 'validation/loss': 2.455028772354126, 'validation/num_examples': 50000, 'test/accuracy': 0.3549000024795532, 'test/loss': 3.087660074234009, 'test/num_examples': 10000, 'score': 34067.28379058838, 'total_duration': 38104.21386909485, 'accumulated_submission_time': 34067.28379058838, 'accumulated_eval_time': 4030.269171476364, 'accumulated_logging_time': 2.76204776763916, 'global_step': 74239, 'preemption_count': 0}), (75158, {'train/accuracy': 0.5007616877555847, 'train/loss': 2.170815944671631, 'validation/accuracy': 0.46995997428894043, 'validation/loss': 2.3507161140441895, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 3.005544662475586, 'test/num_examples': 10000, 'score': 34487.50742006302, 'total_duration': 38575.3131480217, 'accumulated_submission_time': 34487.50742006302, 'accumulated_eval_time': 4081.058485507965, 'accumulated_logging_time': 2.7998476028442383, 'global_step': 75158, 'preemption_count': 0}), (76076, {'train/accuracy': 0.5032812356948853, 'train/loss': 2.190779447555542, 'validation/accuracy': 0.464819997549057, 'validation/loss': 2.4005751609802246, 'validation/num_examples': 50000, 'test/accuracy': 0.36090001463890076, 'test/loss': 3.055748462677002, 'test/num_examples': 10000, 'score': 34907.619475364685, 'total_duration': 39046.77811384201, 'accumulated_submission_time': 34907.619475364685, 'accumulated_eval_time': 4132.322255373001, 'accumulated_logging_time': 2.8405230045318604, 'global_step': 76076, 'preemption_count': 0}), (76992, {'train/accuracy': 0.5121288895606995, 'train/loss': 2.185343027114868, 'validation/accuracy': 0.46901997923851013, 'validation/loss': 2.402076244354248, 'validation/num_examples': 50000, 'test/accuracy': 0.36640000343322754, 'test/loss': 3.04853892326355, 'test/num_examples': 10000, 'score': 35327.92823219299, 'total_duration': 39516.02749085426, 'accumulated_submission_time': 35327.92823219299, 'accumulated_eval_time': 4181.173963069916, 'accumulated_logging_time': 2.8811960220336914, 'global_step': 76992, 'preemption_count': 0}), (77907, {'train/accuracy': 0.5077343583106995, 'train/loss': 2.158487558364868, 'validation/accuracy': 0.4768199920654297, 'validation/loss': 2.3143656253814697, 'validation/num_examples': 50000, 'test/accuracy': 0.36890003085136414, 'test/loss': 2.9804775714874268, 'test/num_examples': 10000, 'score': 35747.85677528381, 'total_duration': 39984.73551940918, 'accumulated_submission_time': 35747.85677528381, 'accumulated_eval_time': 4229.867544412613, 'accumulated_logging_time': 2.919562578201294, 'global_step': 77907, 'preemption_count': 0}), (78822, {'train/accuracy': 0.5114843845367432, 'train/loss': 2.1231250762939453, 'validation/accuracy': 0.4710799753665924, 'validation/loss': 2.3353586196899414, 'validation/num_examples': 50000, 'test/accuracy': 0.3671000301837921, 'test/loss': 3.0031578540802, 'test/num_examples': 10000, 'score': 36168.04482078552, 'total_duration': 40460.507573604584, 'accumulated_submission_time': 36168.04482078552, 'accumulated_eval_time': 4285.362320184708, 'accumulated_logging_time': 2.961005449295044, 'global_step': 78822, 'preemption_count': 0}), (79740, {'train/accuracy': 0.5294140577316284, 'train/loss': 2.0676496028900146, 'validation/accuracy': 0.465859979391098, 'validation/loss': 2.3905460834503174, 'validation/num_examples': 50000, 'test/accuracy': 0.3686000108718872, 'test/loss': 3.0116066932678223, 'test/num_examples': 10000, 'score': 36587.97202754021, 'total_duration': 40929.57435941696, 'accumulated_submission_time': 36587.97202754021, 'accumulated_eval_time': 4334.415132522583, 'accumulated_logging_time': 3.0005173683166504, 'global_step': 79740, 'preemption_count': 0}), (80656, {'train/accuracy': 0.4966796636581421, 'train/loss': 2.2336039543151855, 'validation/accuracy': 0.4697999954223633, 'validation/loss': 2.378688335418701, 'validation/num_examples': 50000, 'test/accuracy': 0.3669000267982483, 'test/loss': 3.037405252456665, 'test/num_examples': 10000, 'score': 37007.93005943298, 'total_duration': 41400.328288793564, 'accumulated_submission_time': 37007.93005943298, 'accumulated_eval_time': 4385.123164892197, 'accumulated_logging_time': 3.0408191680908203, 'global_step': 80656, 'preemption_count': 0}), (81572, {'train/accuracy': 0.512890636920929, 'train/loss': 2.137596368789673, 'validation/accuracy': 0.47655999660491943, 'validation/loss': 2.3293349742889404, 'validation/num_examples': 50000, 'test/accuracy': 0.37210002541542053, 'test/loss': 2.9698870182037354, 'test/num_examples': 10000, 'score': 37427.89789867401, 'total_duration': 41870.229408979416, 'accumulated_submission_time': 37427.89789867401, 'accumulated_eval_time': 4434.969736337662, 'accumulated_logging_time': 3.0801961421966553, 'global_step': 81572, 'preemption_count': 0}), (82491, {'train/accuracy': 0.531054675579071, 'train/loss': 2.0348589420318604, 'validation/accuracy': 0.4846400022506714, 'validation/loss': 2.2830276489257812, 'validation/num_examples': 50000, 'test/accuracy': 0.37310001254081726, 'test/loss': 2.9452946186065674, 'test/num_examples': 10000, 'score': 37847.978172302246, 'total_duration': 42340.52619123459, 'accumulated_submission_time': 37847.978172302246, 'accumulated_eval_time': 4485.095364332199, 'accumulated_logging_time': 3.1229937076568604, 'global_step': 82491, 'preemption_count': 0}), (83408, {'train/accuracy': 0.519726574420929, 'train/loss': 2.0835025310516357, 'validation/accuracy': 0.4893999993801117, 'validation/loss': 2.2483296394348145, 'validation/num_examples': 50000, 'test/accuracy': 0.38360002636909485, 'test/loss': 2.9029996395111084, 'test/num_examples': 10000, 'score': 38268.08063173294, 'total_duration': 42808.462988615036, 'accumulated_submission_time': 38268.08063173294, 'accumulated_eval_time': 4532.843173027039, 'accumulated_logging_time': 3.1619906425476074, 'global_step': 83408, 'preemption_count': 0}), (84324, {'train/accuracy': 0.5274999737739563, 'train/loss': 2.044956684112549, 'validation/accuracy': 0.4887399971485138, 'validation/loss': 2.2427096366882324, 'validation/num_examples': 50000, 'test/accuracy': 0.38020002841949463, 'test/loss': 2.9330248832702637, 'test/num_examples': 10000, 'score': 38688.25997066498, 'total_duration': 43277.33517932892, 'accumulated_submission_time': 38688.25997066498, 'accumulated_eval_time': 4581.449735164642, 'accumulated_logging_time': 3.20169997215271, 'global_step': 84324, 'preemption_count': 0}), (85241, {'train/accuracy': 0.5252148509025574, 'train/loss': 2.1249876022338867, 'validation/accuracy': 0.48159998655319214, 'validation/loss': 2.3384721279144287, 'validation/num_examples': 50000, 'test/accuracy': 0.3765000104904175, 'test/loss': 2.978649139404297, 'test/num_examples': 10000, 'score': 39108.579641819, 'total_duration': 43747.666029930115, 'accumulated_submission_time': 39108.579641819, 'accumulated_eval_time': 4631.371387481689, 'accumulated_logging_time': 3.2427725791931152, 'global_step': 85241, 'preemption_count': 0}), (86158, {'train/accuracy': 0.515820324420929, 'train/loss': 2.156745672225952, 'validation/accuracy': 0.4821399748325348, 'validation/loss': 2.332691192626953, 'validation/num_examples': 50000, 'test/accuracy': 0.3702000081539154, 'test/loss': 2.974332332611084, 'test/num_examples': 10000, 'score': 39528.60836338997, 'total_duration': 44221.15460038185, 'accumulated_submission_time': 39528.60836338997, 'accumulated_eval_time': 4684.744953393936, 'accumulated_logging_time': 3.2811779975891113, 'global_step': 86158, 'preemption_count': 0}), (87075, {'train/accuracy': 0.5296288728713989, 'train/loss': 2.0581154823303223, 'validation/accuracy': 0.4989999830722809, 'validation/loss': 2.2154085636138916, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.8767480850219727, 'test/num_examples': 10000, 'score': 39948.69438958168, 'total_duration': 44690.825795173645, 'accumulated_submission_time': 39948.69438958168, 'accumulated_eval_time': 4734.239243745804, 'accumulated_logging_time': 3.324136257171631, 'global_step': 87075, 'preemption_count': 0}), (87991, {'train/accuracy': 0.5358788967132568, 'train/loss': 1.9901221990585327, 'validation/accuracy': 0.5009599924087524, 'validation/loss': 2.1941733360290527, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.863675594329834, 'test/num_examples': 10000, 'score': 40368.87440466881, 'total_duration': 45162.857728004456, 'accumulated_submission_time': 40368.87440466881, 'accumulated_eval_time': 4786.001025438309, 'accumulated_logging_time': 3.3661766052246094, 'global_step': 87991, 'preemption_count': 0}), (88907, {'train/accuracy': 0.5342382788658142, 'train/loss': 2.0812058448791504, 'validation/accuracy': 0.48997998237609863, 'validation/loss': 2.29469895362854, 'validation/num_examples': 50000, 'test/accuracy': 0.3799000084400177, 'test/loss': 2.949227809906006, 'test/num_examples': 10000, 'score': 40788.85287356377, 'total_duration': 45632.111568689346, 'accumulated_submission_time': 40788.85287356377, 'accumulated_eval_time': 4835.1866590976715, 'accumulated_logging_time': 3.407886266708374, 'global_step': 88907, 'preemption_count': 0}), (89821, {'train/accuracy': 0.5366601347923279, 'train/loss': 2.041024923324585, 'validation/accuracy': 0.5012800097465515, 'validation/loss': 2.2048683166503906, 'validation/num_examples': 50000, 'test/accuracy': 0.394400030374527, 'test/loss': 2.8692476749420166, 'test/num_examples': 10000, 'score': 41209.05537772179, 'total_duration': 46101.123297929764, 'accumulated_submission_time': 41209.05537772179, 'accumulated_eval_time': 4883.906123161316, 'accumulated_logging_time': 3.450183391571045, 'global_step': 89821, 'preemption_count': 0}), (90736, {'train/accuracy': 0.5470117330551147, 'train/loss': 1.9687862396240234, 'validation/accuracy': 0.5073800086975098, 'validation/loss': 2.1694068908691406, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.8334524631500244, 'test/num_examples': 10000, 'score': 41629.10823345184, 'total_duration': 46570.9590845108, 'accumulated_submission_time': 41629.10823345184, 'accumulated_eval_time': 4933.602746248245, 'accumulated_logging_time': 3.4877588748931885, 'global_step': 90736, 'preemption_count': 0}), (91649, {'train/accuracy': 0.5741991996765137, 'train/loss': 1.8185863494873047, 'validation/accuracy': 0.5086199641227722, 'validation/loss': 2.1520354747772217, 'validation/num_examples': 50000, 'test/accuracy': 0.39730000495910645, 'test/loss': 2.799119234085083, 'test/num_examples': 10000, 'score': 42049.48864984512, 'total_duration': 47041.567656993866, 'accumulated_submission_time': 42049.48864984512, 'accumulated_eval_time': 4983.74448466301, 'accumulated_logging_time': 3.526984214782715, 'global_step': 91649, 'preemption_count': 0}), (92566, {'train/accuracy': 0.5272265672683716, 'train/loss': 2.0880000591278076, 'validation/accuracy': 0.5004400014877319, 'validation/loss': 2.219738483428955, 'validation/num_examples': 50000, 'test/accuracy': 0.39430001378059387, 'test/loss': 2.86732816696167, 'test/num_examples': 10000, 'score': 42469.72681379318, 'total_duration': 47512.32449412346, 'accumulated_submission_time': 42469.72681379318, 'accumulated_eval_time': 5034.174078941345, 'accumulated_logging_time': 3.568467617034912, 'global_step': 92566, 'preemption_count': 0}), (93480, {'train/accuracy': 0.5433398485183716, 'train/loss': 2.01934814453125, 'validation/accuracy': 0.5062999725341797, 'validation/loss': 2.205066442489624, 'validation/num_examples': 50000, 'test/accuracy': 0.39320001006126404, 'test/loss': 2.861738681793213, 'test/num_examples': 10000, 'score': 42889.70433783531, 'total_duration': 47982.05200815201, 'accumulated_submission_time': 42889.70433783531, 'accumulated_eval_time': 5083.820999860764, 'accumulated_logging_time': 3.623884439468384, 'global_step': 93480, 'preemption_count': 0}), (94396, {'train/accuracy': 0.5606250166893005, 'train/loss': 1.9081244468688965, 'validation/accuracy': 0.5089399814605713, 'validation/loss': 2.1560139656066895, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.820587158203125, 'test/num_examples': 10000, 'score': 43309.709506988525, 'total_duration': 48453.16395688057, 'accumulated_submission_time': 43309.709506988525, 'accumulated_eval_time': 5134.8354160785675, 'accumulated_logging_time': 3.6679365634918213, 'global_step': 94396, 'preemption_count': 0}), (95311, {'train/accuracy': 0.5488671660423279, 'train/loss': 1.9369208812713623, 'validation/accuracy': 0.5136199593544006, 'validation/loss': 2.113115072250366, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.7761130332946777, 'test/num_examples': 10000, 'score': 43729.9824256897, 'total_duration': 48924.271253585815, 'accumulated_submission_time': 43729.9824256897, 'accumulated_eval_time': 5185.579132556915, 'accumulated_logging_time': 3.7109029293060303, 'global_step': 95311, 'preemption_count': 0}), (96228, {'train/accuracy': 0.5465624928474426, 'train/loss': 1.977414846420288, 'validation/accuracy': 0.5113599896430969, 'validation/loss': 2.1627683639526367, 'validation/num_examples': 50000, 'test/accuracy': 0.4028000235557556, 'test/loss': 2.8213248252868652, 'test/num_examples': 10000, 'score': 44150.20173883438, 'total_duration': 49392.18372750282, 'accumulated_submission_time': 44150.20173883438, 'accumulated_eval_time': 5233.184512376785, 'accumulated_logging_time': 3.750455856323242, 'global_step': 96228, 'preemption_count': 0}), (97146, {'train/accuracy': 0.5612499713897705, 'train/loss': 1.8850711584091187, 'validation/accuracy': 0.5123400092124939, 'validation/loss': 2.129922866821289, 'validation/num_examples': 50000, 'test/accuracy': 0.4019000232219696, 'test/loss': 2.7724106311798096, 'test/num_examples': 10000, 'score': 44570.48270201683, 'total_duration': 49862.24414396286, 'accumulated_submission_time': 44570.48270201683, 'accumulated_eval_time': 5282.874450683594, 'accumulated_logging_time': 3.7924487590789795, 'global_step': 97146, 'preemption_count': 0}), (98058, {'train/accuracy': 0.5537304282188416, 'train/loss': 1.9444429874420166, 'validation/accuracy': 0.524179995059967, 'validation/loss': 2.0987751483917236, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.7410056591033936, 'test/num_examples': 10000, 'score': 44990.81266307831, 'total_duration': 50328.74145245552, 'accumulated_submission_time': 44990.81266307831, 'accumulated_eval_time': 5328.955847263336, 'accumulated_logging_time': 3.8312745094299316, 'global_step': 98058, 'preemption_count': 0}), (98972, {'train/accuracy': 0.56494140625, 'train/loss': 1.8888856172561646, 'validation/accuracy': 0.5273999571800232, 'validation/loss': 2.0798420906066895, 'validation/num_examples': 50000, 'test/accuracy': 0.4125000238418579, 'test/loss': 2.7286581993103027, 'test/num_examples': 10000, 'score': 45410.88789892197, 'total_duration': 50799.61802506447, 'accumulated_submission_time': 45410.88789892197, 'accumulated_eval_time': 5379.665320634842, 'accumulated_logging_time': 3.8756015300750732, 'global_step': 98972, 'preemption_count': 0}), (99889, {'train/accuracy': 0.5637304782867432, 'train/loss': 1.909096121788025, 'validation/accuracy': 0.5228599905967712, 'validation/loss': 2.1150269508361816, 'validation/num_examples': 50000, 'test/accuracy': 0.4125000238418579, 'test/loss': 2.7483208179473877, 'test/num_examples': 10000, 'score': 45831.25173521042, 'total_duration': 51269.41478252411, 'accumulated_submission_time': 45831.25173521042, 'accumulated_eval_time': 5429.00506401062, 'accumulated_logging_time': 3.9204533100128174, 'global_step': 99889, 'preemption_count': 0}), (100805, {'train/accuracy': 0.5720898509025574, 'train/loss': 1.8558332920074463, 'validation/accuracy': 0.532260000705719, 'validation/loss': 2.0450856685638428, 'validation/num_examples': 50000, 'test/accuracy': 0.42330002784729004, 'test/loss': 2.701802968978882, 'test/num_examples': 10000, 'score': 46251.209023952484, 'total_duration': 51737.73347878456, 'accumulated_submission_time': 46251.209023952484, 'accumulated_eval_time': 5477.274906158447, 'accumulated_logging_time': 3.9645845890045166, 'global_step': 100805, 'preemption_count': 0}), (101721, {'train/accuracy': 0.5673046708106995, 'train/loss': 1.848193645477295, 'validation/accuracy': 0.5283600091934204, 'validation/loss': 2.050722360610962, 'validation/num_examples': 50000, 'test/accuracy': 0.40950003266334534, 'test/loss': 2.745497703552246, 'test/num_examples': 10000, 'score': 46671.40980911255, 'total_duration': 52206.08445620537, 'accumulated_submission_time': 46671.40980911255, 'accumulated_eval_time': 5525.338563919067, 'accumulated_logging_time': 4.004071235656738, 'global_step': 101721, 'preemption_count': 0}), (102640, {'train/accuracy': 0.5785741806030273, 'train/loss': 1.8186800479888916, 'validation/accuracy': 0.5371400117874146, 'validation/loss': 2.024113655090332, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.689002752304077, 'test/num_examples': 10000, 'score': 47091.384001493454, 'total_duration': 52675.02685260773, 'accumulated_submission_time': 47091.384001493454, 'accumulated_eval_time': 5574.214259624481, 'accumulated_logging_time': 4.048406600952148, 'global_step': 102640, 'preemption_count': 0}), (103558, {'train/accuracy': 0.5989648103713989, 'train/loss': 1.7452768087387085, 'validation/accuracy': 0.5286200046539307, 'validation/loss': 2.0765881538391113, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.7206692695617676, 'test/num_examples': 10000, 'score': 47511.33321595192, 'total_duration': 53143.97084522247, 'accumulated_submission_time': 47511.33321595192, 'accumulated_eval_time': 5623.107983827591, 'accumulated_logging_time': 4.101131916046143, 'global_step': 103558, 'preemption_count': 0}), (104477, {'train/accuracy': 0.5718554854393005, 'train/loss': 1.8921626806259155, 'validation/accuracy': 0.5343199968338013, 'validation/loss': 2.0755221843719482, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.7130990028381348, 'test/num_examples': 10000, 'score': 47931.566383600235, 'total_duration': 53613.40177822113, 'accumulated_submission_time': 47931.566383600235, 'accumulated_eval_time': 5672.216294765472, 'accumulated_logging_time': 4.143033742904663, 'global_step': 104477, 'preemption_count': 0}), (105394, {'train/accuracy': 0.5820507407188416, 'train/loss': 1.8374958038330078, 'validation/accuracy': 0.5375800132751465, 'validation/loss': 2.045927047729492, 'validation/num_examples': 50000, 'test/accuracy': 0.4191000163555145, 'test/loss': 2.7071774005889893, 'test/num_examples': 10000, 'score': 48351.92719125748, 'total_duration': 54085.40121340752, 'accumulated_submission_time': 48351.92719125748, 'accumulated_eval_time': 5723.759583473206, 'accumulated_logging_time': 4.190583944320679, 'global_step': 105394, 'preemption_count': 0}), (106312, {'train/accuracy': 0.5904687643051147, 'train/loss': 1.7310149669647217, 'validation/accuracy': 0.5409199595451355, 'validation/loss': 1.9848403930664062, 'validation/num_examples': 50000, 'test/accuracy': 0.4317000210285187, 'test/loss': 2.6487910747528076, 'test/num_examples': 10000, 'score': 48772.00656723976, 'total_duration': 54556.38950634003, 'accumulated_submission_time': 48772.00656723976, 'accumulated_eval_time': 5774.5703637599945, 'accumulated_logging_time': 4.240931749343872, 'global_step': 106312, 'preemption_count': 0}), (107228, {'train/accuracy': 0.5759570002555847, 'train/loss': 1.833008050918579, 'validation/accuracy': 0.537880003452301, 'validation/loss': 2.0171401500701904, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.690009593963623, 'test/num_examples': 10000, 'score': 49192.05952215195, 'total_duration': 55025.439949035645, 'accumulated_submission_time': 49192.05952215195, 'accumulated_eval_time': 5823.475866556168, 'accumulated_logging_time': 4.285040616989136, 'global_step': 107228, 'preemption_count': 0}), (108146, {'train/accuracy': 0.5908398032188416, 'train/loss': 1.7584213018417358, 'validation/accuracy': 0.5523999929428101, 'validation/loss': 1.949994444847107, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.611959934234619, 'test/num_examples': 10000, 'score': 49612.333136081696, 'total_duration': 55495.518181562424, 'accumulated_submission_time': 49612.333136081696, 'accumulated_eval_time': 5873.19217467308, 'accumulated_logging_time': 4.326111793518066, 'global_step': 108146, 'preemption_count': 0}), (109061, {'train/accuracy': 0.6046093702316284, 'train/loss': 1.7196390628814697, 'validation/accuracy': 0.5508399605751038, 'validation/loss': 1.968966007232666, 'validation/num_examples': 50000, 'test/accuracy': 0.43540000915527344, 'test/loss': 2.6368744373321533, 'test/num_examples': 10000, 'score': 50032.38190603256, 'total_duration': 55965.41650009155, 'accumulated_submission_time': 50032.38190603256, 'accumulated_eval_time': 5922.949047088623, 'accumulated_logging_time': 4.369261980056763, 'global_step': 109061, 'preemption_count': 0}), (109981, {'train/accuracy': 0.5870702862739563, 'train/loss': 1.7547787427902222, 'validation/accuracy': 0.5485199689865112, 'validation/loss': 1.9464657306671143, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.6147043704986572, 'test/num_examples': 10000, 'score': 50452.712889909744, 'total_duration': 56434.7984893322, 'accumulated_submission_time': 50452.712889909744, 'accumulated_eval_time': 5971.910776138306, 'accumulated_logging_time': 4.410284757614136, 'global_step': 109981, 'preemption_count': 0}), (110896, {'train/accuracy': 0.6010546684265137, 'train/loss': 1.6854904890060425, 'validation/accuracy': 0.556879997253418, 'validation/loss': 1.8993265628814697, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.5637691020965576, 'test/num_examples': 10000, 'score': 50872.9928958416, 'total_duration': 56906.227376937866, 'accumulated_submission_time': 50872.9928958416, 'accumulated_eval_time': 6022.969138383865, 'accumulated_logging_time': 4.453753709793091, 'global_step': 110896, 'preemption_count': 0}), (111813, {'train/accuracy': 0.606640636920929, 'train/loss': 1.6578865051269531, 'validation/accuracy': 0.5602999925613403, 'validation/loss': 1.8957290649414062, 'validation/num_examples': 50000, 'test/accuracy': 0.4487000107765198, 'test/loss': 2.552631378173828, 'test/num_examples': 10000, 'score': 51293.23732328415, 'total_duration': 57376.957426548004, 'accumulated_submission_time': 51293.23732328415, 'accumulated_eval_time': 6073.349613189697, 'accumulated_logging_time': 4.510779142379761, 'global_step': 111813, 'preemption_count': 0}), (112731, {'train/accuracy': 0.6123046875, 'train/loss': 1.6747193336486816, 'validation/accuracy': 0.5610399842262268, 'validation/loss': 1.8994790315628052, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.5727462768554688, 'test/num_examples': 10000, 'score': 51713.58249878883, 'total_duration': 57846.424187898636, 'accumulated_submission_time': 51713.58249878883, 'accumulated_eval_time': 6122.37894487381, 'accumulated_logging_time': 4.555319547653198, 'global_step': 112731, 'preemption_count': 0}), (113647, {'train/accuracy': 0.6024609208106995, 'train/loss': 1.6888262033462524, 'validation/accuracy': 0.5626800060272217, 'validation/loss': 1.876355528831482, 'validation/num_examples': 50000, 'test/accuracy': 0.44460001587867737, 'test/loss': 2.5511436462402344, 'test/num_examples': 10000, 'score': 52133.86265182495, 'total_duration': 58317.28026175499, 'accumulated_submission_time': 52133.86265182495, 'accumulated_eval_time': 6172.862322568893, 'accumulated_logging_time': 4.599631071090698, 'global_step': 113647, 'preemption_count': 0}), (114565, {'train/accuracy': 0.5999413728713989, 'train/loss': 1.7239134311676025, 'validation/accuracy': 0.5591199994087219, 'validation/loss': 1.9196312427520752, 'validation/num_examples': 50000, 'test/accuracy': 0.4391000270843506, 'test/loss': 2.578800678253174, 'test/num_examples': 10000, 'score': 52554.008450984955, 'total_duration': 58785.77437710762, 'accumulated_submission_time': 52554.008450984955, 'accumulated_eval_time': 6221.115981340408, 'accumulated_logging_time': 4.645940542221069, 'global_step': 114565, 'preemption_count': 0}), (115483, {'train/accuracy': 0.6393163800239563, 'train/loss': 1.5017653703689575, 'validation/accuracy': 0.5700199604034424, 'validation/loss': 1.8257381916046143, 'validation/num_examples': 50000, 'test/accuracy': 0.4597000181674957, 'test/loss': 2.50107741355896, 'test/num_examples': 10000, 'score': 52974.09374523163, 'total_duration': 59255.32407426834, 'accumulated_submission_time': 52974.09374523163, 'accumulated_eval_time': 6270.487004041672, 'accumulated_logging_time': 4.691897869110107, 'global_step': 115483, 'preemption_count': 0}), (116400, {'train/accuracy': 0.6116992235183716, 'train/loss': 1.6490353345870972, 'validation/accuracy': 0.5755999684333801, 'validation/loss': 1.843803882598877, 'validation/num_examples': 50000, 'test/accuracy': 0.4602000117301941, 'test/loss': 2.501185655593872, 'test/num_examples': 10000, 'score': 53394.46939063072, 'total_duration': 59729.46551704407, 'accumulated_submission_time': 53394.46939063072, 'accumulated_eval_time': 6324.160755395889, 'accumulated_logging_time': 4.736301422119141, 'global_step': 116400, 'preemption_count': 0}), (117319, {'train/accuracy': 0.6172265410423279, 'train/loss': 1.627258539199829, 'validation/accuracy': 0.5745399594306946, 'validation/loss': 1.8362805843353271, 'validation/num_examples': 50000, 'test/accuracy': 0.45510002970695496, 'test/loss': 2.4943559169769287, 'test/num_examples': 10000, 'score': 53814.67329096794, 'total_duration': 60201.44076490402, 'accumulated_submission_time': 53814.67329096794, 'accumulated_eval_time': 6375.838165998459, 'accumulated_logging_time': 4.7819108963012695, 'global_step': 117319, 'preemption_count': 0}), (118236, {'train/accuracy': 0.6234374642372131, 'train/loss': 1.591255784034729, 'validation/accuracy': 0.5689799785614014, 'validation/loss': 1.8540898561477661, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.519318103790283, 'test/num_examples': 10000, 'score': 54234.853268146515, 'total_duration': 60671.578468084335, 'accumulated_submission_time': 54234.853268146515, 'accumulated_eval_time': 6425.695611715317, 'accumulated_logging_time': 4.8346474170684814, 'global_step': 118236, 'preemption_count': 0}), (119150, {'train/accuracy': 0.6201952695846558, 'train/loss': 1.594254732131958, 'validation/accuracy': 0.5825200080871582, 'validation/loss': 1.778774380683899, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.4543488025665283, 'test/num_examples': 10000, 'score': 54655.12280344963, 'total_duration': 61142.06141138077, 'accumulated_submission_time': 54655.12280344963, 'accumulated_eval_time': 6475.814856529236, 'accumulated_logging_time': 4.881854772567749, 'global_step': 119150, 'preemption_count': 0}), (120069, {'train/accuracy': 0.6280273199081421, 'train/loss': 1.5842721462249756, 'validation/accuracy': 0.5796799659729004, 'validation/loss': 1.7919082641601562, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.4546687602996826, 'test/num_examples': 10000, 'score': 55075.437239170074, 'total_duration': 61610.07016038895, 'accumulated_submission_time': 55075.437239170074, 'accumulated_eval_time': 6523.415584564209, 'accumulated_logging_time': 4.927415132522583, 'global_step': 120069, 'preemption_count': 0}), (120987, {'train/accuracy': 0.6349999904632568, 'train/loss': 1.535889744758606, 'validation/accuracy': 0.584879994392395, 'validation/loss': 1.7815570831298828, 'validation/num_examples': 50000, 'test/accuracy': 0.4634000360965729, 'test/loss': 2.4435477256774902, 'test/num_examples': 10000, 'score': 55495.7010948658, 'total_duration': 62080.379637002945, 'accumulated_submission_time': 55495.7010948658, 'accumulated_eval_time': 6573.364508867264, 'accumulated_logging_time': 4.976737022399902, 'global_step': 120987, 'preemption_count': 0}), (121903, {'train/accuracy': 0.6355859041213989, 'train/loss': 1.5495035648345947, 'validation/accuracy': 0.5896599888801575, 'validation/loss': 1.7570979595184326, 'validation/num_examples': 50000, 'test/accuracy': 0.47040003538131714, 'test/loss': 2.42789888381958, 'test/num_examples': 10000, 'score': 55915.98540139198, 'total_duration': 62549.107691049576, 'accumulated_submission_time': 55915.98540139198, 'accumulated_eval_time': 6621.707966804504, 'accumulated_logging_time': 5.029550790786743, 'global_step': 121903, 'preemption_count': 0}), (122819, {'train/accuracy': 0.6341015696525574, 'train/loss': 1.5348260402679443, 'validation/accuracy': 0.5896399617195129, 'validation/loss': 1.748339056968689, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.4274723529815674, 'test/num_examples': 10000, 'score': 56336.27511167526, 'total_duration': 63019.44883728027, 'accumulated_submission_time': 56336.27511167526, 'accumulated_eval_time': 6671.663818836212, 'accumulated_logging_time': 5.077637434005737, 'global_step': 122819, 'preemption_count': 0}), (123736, {'train/accuracy': 0.6442577838897705, 'train/loss': 1.493449330329895, 'validation/accuracy': 0.594539999961853, 'validation/loss': 1.7315579652786255, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.365442991256714, 'test/num_examples': 10000, 'score': 56756.223601818085, 'total_duration': 63489.87978386879, 'accumulated_submission_time': 56756.223601818085, 'accumulated_eval_time': 6722.052555799484, 'accumulated_logging_time': 5.1239094734191895, 'global_step': 123736, 'preemption_count': 0}), (124653, {'train/accuracy': 0.6554492115974426, 'train/loss': 1.4739404916763306, 'validation/accuracy': 0.5974400043487549, 'validation/loss': 1.7426960468292236, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.39933180809021, 'test/num_examples': 10000, 'score': 57176.31259250641, 'total_duration': 63961.48591351509, 'accumulated_submission_time': 57176.31259250641, 'accumulated_eval_time': 6773.475342273712, 'accumulated_logging_time': 5.171019077301025, 'global_step': 124653, 'preemption_count': 0}), (125568, {'train/accuracy': 0.6419726610183716, 'train/loss': 1.5183041095733643, 'validation/accuracy': 0.5974000096321106, 'validation/loss': 1.7342219352722168, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.396895408630371, 'test/num_examples': 10000, 'score': 57596.52352762222, 'total_duration': 64430.41939020157, 'accumulated_submission_time': 57596.52352762222, 'accumulated_eval_time': 6822.102419376373, 'accumulated_logging_time': 5.218945741653442, 'global_step': 125568, 'preemption_count': 0}), (126482, {'train/accuracy': 0.6576171517372131, 'train/loss': 1.45563805103302, 'validation/accuracy': 0.6087200045585632, 'validation/loss': 1.6954721212387085, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.356315851211548, 'test/num_examples': 10000, 'score': 58016.79306435585, 'total_duration': 64898.98254442215, 'accumulated_submission_time': 58016.79306435585, 'accumulated_eval_time': 6870.285463809967, 'accumulated_logging_time': 5.281857252120972, 'global_step': 126482, 'preemption_count': 0}), (127400, {'train/accuracy': 0.6700586080551147, 'train/loss': 1.3599334955215454, 'validation/accuracy': 0.6076599955558777, 'validation/loss': 1.6682051420211792, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.3263015747070312, 'test/num_examples': 10000, 'score': 58437.100652217865, 'total_duration': 65366.74352836609, 'accumulated_submission_time': 58437.100652217865, 'accumulated_eval_time': 6917.643349409103, 'accumulated_logging_time': 5.328955173492432, 'global_step': 127400, 'preemption_count': 0}), (128316, {'train/accuracy': 0.6534960865974426, 'train/loss': 1.4608618021011353, 'validation/accuracy': 0.6089000105857849, 'validation/loss': 1.6636296510696411, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.318061113357544, 'test/num_examples': 10000, 'score': 58857.16923952103, 'total_duration': 65835.45329618454, 'accumulated_submission_time': 58857.16923952103, 'accumulated_eval_time': 6966.18169260025, 'accumulated_logging_time': 5.3842246532440186, 'global_step': 128316, 'preemption_count': 0}), (129234, {'train/accuracy': 0.6603124737739563, 'train/loss': 1.4079570770263672, 'validation/accuracy': 0.6110399961471558, 'validation/loss': 1.6467926502227783, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.2942965030670166, 'test/num_examples': 10000, 'score': 59277.42556810379, 'total_duration': 66305.90271615982, 'accumulated_submission_time': 59277.42556810379, 'accumulated_eval_time': 7016.280867099762, 'accumulated_logging_time': 5.430881500244141, 'global_step': 129234, 'preemption_count': 0}), (130154, {'train/accuracy': 0.6708202958106995, 'train/loss': 1.3997917175292969, 'validation/accuracy': 0.6119999885559082, 'validation/loss': 1.674870252609253, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.3398818969726562, 'test/num_examples': 10000, 'score': 59697.674666404724, 'total_duration': 66774.41073536873, 'accumulated_submission_time': 59697.674666404724, 'accumulated_eval_time': 7064.444483995438, 'accumulated_logging_time': 5.477983713150024, 'global_step': 130154, 'preemption_count': 0}), (131074, {'train/accuracy': 0.6654687523841858, 'train/loss': 1.3992410898208618, 'validation/accuracy': 0.6183599829673767, 'validation/loss': 1.6063157320022583, 'validation/num_examples': 50000, 'test/accuracy': 0.49470001459121704, 'test/loss': 2.2643074989318848, 'test/num_examples': 10000, 'score': 60117.79187011719, 'total_duration': 67242.87277555466, 'accumulated_submission_time': 60117.79187011719, 'accumulated_eval_time': 7112.693949460983, 'accumulated_logging_time': 5.525101900100708, 'global_step': 131074, 'preemption_count': 0}), (131991, {'train/accuracy': 0.6669335961341858, 'train/loss': 1.3916473388671875, 'validation/accuracy': 0.6188600063323975, 'validation/loss': 1.6234492063522339, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.2747392654418945, 'test/num_examples': 10000, 'score': 60537.90219092369, 'total_duration': 67711.90890932083, 'accumulated_submission_time': 60537.90219092369, 'accumulated_eval_time': 7161.525184869766, 'accumulated_logging_time': 5.570846319198608, 'global_step': 131991, 'preemption_count': 0}), (132910, {'train/accuracy': 0.6772265434265137, 'train/loss': 1.3613171577453613, 'validation/accuracy': 0.620959997177124, 'validation/loss': 1.627536654472351, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.2841172218322754, 'test/num_examples': 10000, 'score': 60957.94920253754, 'total_duration': 68180.9085547924, 'accumulated_submission_time': 60957.94920253754, 'accumulated_eval_time': 7210.376707792282, 'accumulated_logging_time': 5.624185085296631, 'global_step': 132910, 'preemption_count': 0}), (133828, {'train/accuracy': 0.6712890267372131, 'train/loss': 1.365814447402954, 'validation/accuracy': 0.6225999593734741, 'validation/loss': 1.5908536911010742, 'validation/num_examples': 50000, 'test/accuracy': 0.501300036907196, 'test/loss': 2.24080491065979, 'test/num_examples': 10000, 'score': 61378.31641602516, 'total_duration': 68652.09668803215, 'accumulated_submission_time': 61378.31641602516, 'accumulated_eval_time': 7261.099878549576, 'accumulated_logging_time': 5.673567771911621, 'global_step': 133828, 'preemption_count': 0}), (134743, {'train/accuracy': 0.6692187190055847, 'train/loss': 1.3907537460327148, 'validation/accuracy': 0.6218199729919434, 'validation/loss': 1.610404133796692, 'validation/num_examples': 50000, 'test/accuracy': 0.49870002269744873, 'test/loss': 2.2690556049346924, 'test/num_examples': 10000, 'score': 61798.59408092499, 'total_duration': 69123.8744187355, 'accumulated_submission_time': 61798.59408092499, 'accumulated_eval_time': 7312.5014128685, 'accumulated_logging_time': 5.723673582077026, 'global_step': 134743, 'preemption_count': 0}), (135622, {'train/accuracy': 0.6849804520606995, 'train/loss': 1.3148226737976074, 'validation/accuracy': 0.6287999749183655, 'validation/loss': 1.5665632486343384, 'validation/num_examples': 50000, 'test/accuracy': 0.5057000517845154, 'test/loss': 2.21661114692688, 'test/num_examples': 10000, 'score': 62218.69186377525, 'total_duration': 69592.01400113106, 'accumulated_submission_time': 62218.69186377525, 'accumulated_eval_time': 7360.451724529266, 'accumulated_logging_time': 5.769453287124634, 'global_step': 135622, 'preemption_count': 0}), (136534, {'train/accuracy': 0.6810937523841858, 'train/loss': 1.350616216659546, 'validation/accuracy': 0.6312800049781799, 'validation/loss': 1.5826791524887085, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2302193641662598, 'test/num_examples': 10000, 'score': 62638.98313713074, 'total_duration': 70062.6733353138, 'accumulated_submission_time': 62638.98313713074, 'accumulated_eval_time': 7410.717103242874, 'accumulated_logging_time': 5.824112415313721, 'global_step': 136534, 'preemption_count': 0}), (137447, {'train/accuracy': 0.6771484017372131, 'train/loss': 1.363439917564392, 'validation/accuracy': 0.6318599581718445, 'validation/loss': 1.5701099634170532, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2094790935516357, 'test/num_examples': 10000, 'score': 63059.255123615265, 'total_duration': 70531.656188488, 'accumulated_submission_time': 63059.255123615265, 'accumulated_eval_time': 7459.330714464188, 'accumulated_logging_time': 5.873344898223877, 'global_step': 137447, 'preemption_count': 0}), (138358, {'train/accuracy': 0.6902929544448853, 'train/loss': 1.3037259578704834, 'validation/accuracy': 0.6375799775123596, 'validation/loss': 1.5405707359313965, 'validation/num_examples': 50000, 'test/accuracy': 0.51500004529953, 'test/loss': 2.1850335597991943, 'test/num_examples': 10000, 'score': 63479.41015410423, 'total_duration': 71001.12347507477, 'accumulated_submission_time': 63479.41015410423, 'accumulated_eval_time': 7508.541209936142, 'accumulated_logging_time': 5.9278342723846436, 'global_step': 138358, 'preemption_count': 0}), (139272, {'train/accuracy': 0.7201562523841858, 'train/loss': 1.162137746810913, 'validation/accuracy': 0.6441999673843384, 'validation/loss': 1.5044505596160889, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.161705732345581, 'test/num_examples': 10000, 'score': 63899.503361940384, 'total_duration': 71473.19856882095, 'accumulated_submission_time': 63899.503361940384, 'accumulated_eval_time': 7560.420810461044, 'accumulated_logging_time': 5.9827258586883545, 'global_step': 139272, 'preemption_count': 0}), (140189, {'train/accuracy': 0.6912499666213989, 'train/loss': 1.2617908716201782, 'validation/accuracy': 0.6451199650764465, 'validation/loss': 1.4823774099349976, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.1347148418426514, 'test/num_examples': 10000, 'score': 64319.68514704704, 'total_duration': 71943.69709396362, 'accumulated_submission_time': 64319.68514704704, 'accumulated_eval_time': 7610.637903690338, 'accumulated_logging_time': 6.034387826919556, 'global_step': 140189, 'preemption_count': 0}), (141109, {'train/accuracy': 0.7030664086341858, 'train/loss': 1.2258492708206177, 'validation/accuracy': 0.6487999558448792, 'validation/loss': 1.4680681228637695, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.1157939434051514, 'test/num_examples': 10000, 'score': 64739.684688806534, 'total_duration': 72414.69781684875, 'accumulated_submission_time': 64739.684688806534, 'accumulated_eval_time': 7661.540041446686, 'accumulated_logging_time': 6.084909915924072, 'global_step': 141109, 'preemption_count': 0}), (142026, {'train/accuracy': 0.712695300579071, 'train/loss': 1.1884727478027344, 'validation/accuracy': 0.6464200019836426, 'validation/loss': 1.488558292388916, 'validation/num_examples': 50000, 'test/accuracy': 0.5259000062942505, 'test/loss': 2.140204906463623, 'test/num_examples': 10000, 'score': 65159.91322731972, 'total_duration': 72884.38906359673, 'accumulated_submission_time': 65159.91322731972, 'accumulated_eval_time': 7710.902366638184, 'accumulated_logging_time': 6.13739275932312, 'global_step': 142026, 'preemption_count': 0}), (142943, {'train/accuracy': 0.7005664110183716, 'train/loss': 1.2247933149337769, 'validation/accuracy': 0.6492999792098999, 'validation/loss': 1.4643806219100952, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.109903573989868, 'test/num_examples': 10000, 'score': 65580.24869775772, 'total_duration': 73353.33665847778, 'accumulated_submission_time': 65580.24869775772, 'accumulated_eval_time': 7759.4170508384705, 'accumulated_logging_time': 6.186307430267334, 'global_step': 142943, 'preemption_count': 0}), (143859, {'train/accuracy': 0.7032226324081421, 'train/loss': 1.2361243963241577, 'validation/accuracy': 0.6534000039100647, 'validation/loss': 1.4610118865966797, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.113292932510376, 'test/num_examples': 10000, 'score': 66000.38722920418, 'total_duration': 73822.04728531837, 'accumulated_submission_time': 66000.38722920418, 'accumulated_eval_time': 7807.887068033218, 'accumulated_logging_time': 6.239961385726929, 'global_step': 143859, 'preemption_count': 0}), (144776, {'train/accuracy': 0.7193945050239563, 'train/loss': 1.1607738733291626, 'validation/accuracy': 0.654259979724884, 'validation/loss': 1.447080135345459, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.1155529022216797, 'test/num_examples': 10000, 'score': 66420.68016719818, 'total_duration': 74292.60705327988, 'accumulated_submission_time': 66420.68016719818, 'accumulated_eval_time': 7858.054362535477, 'accumulated_logging_time': 6.291686773300171, 'global_step': 144776, 'preemption_count': 0}), (145690, {'train/accuracy': 0.713671863079071, 'train/loss': 1.1829522848129272, 'validation/accuracy': 0.6609799861907959, 'validation/loss': 1.417777180671692, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.0482189655303955, 'test/num_examples': 10000, 'score': 66840.85182523727, 'total_duration': 74760.23641347885, 'accumulated_submission_time': 66840.85182523727, 'accumulated_eval_time': 7905.415101289749, 'accumulated_logging_time': 6.340944766998291, 'global_step': 145690, 'preemption_count': 0}), (146605, {'train/accuracy': 0.7144335508346558, 'train/loss': 1.191360592842102, 'validation/accuracy': 0.6621999740600586, 'validation/loss': 1.4254510402679443, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.0617923736572266, 'test/num_examples': 10000, 'score': 67260.99752354622, 'total_duration': 75230.54712605476, 'accumulated_submission_time': 67260.99752354622, 'accumulated_eval_time': 7955.480447292328, 'accumulated_logging_time': 6.393074035644531, 'global_step': 146605, 'preemption_count': 0}), (147522, {'train/accuracy': 0.7241796851158142, 'train/loss': 1.1309359073638916, 'validation/accuracy': 0.6642000079154968, 'validation/loss': 1.4086828231811523, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.0465872287750244, 'test/num_examples': 10000, 'score': 67681.06917715073, 'total_duration': 75698.780200243, 'accumulated_submission_time': 67681.06917715073, 'accumulated_eval_time': 8003.546766996384, 'accumulated_logging_time': 6.440312385559082, 'global_step': 147522, 'preemption_count': 0}), (148440, {'train/accuracy': 0.7292773127555847, 'train/loss': 1.1139354705810547, 'validation/accuracy': 0.6719399690628052, 'validation/loss': 1.3779354095458984, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.010650396347046, 'test/num_examples': 10000, 'score': 68101.14979958534, 'total_duration': 76168.72648119926, 'accumulated_submission_time': 68101.14979958534, 'accumulated_eval_time': 8053.31559920311, 'accumulated_logging_time': 6.489897012710571, 'global_step': 148440, 'preemption_count': 0}), (149356, {'train/accuracy': 0.7234765291213989, 'train/loss': 1.1463912725448608, 'validation/accuracy': 0.6699000000953674, 'validation/loss': 1.3762966394424438, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.032515525817871, 'test/num_examples': 10000, 'score': 68521.11470293999, 'total_duration': 76639.01660203934, 'accumulated_submission_time': 68521.11470293999, 'accumulated_eval_time': 8103.5270392894745, 'accumulated_logging_time': 6.53973126411438, 'global_step': 149356, 'preemption_count': 0}), (150270, {'train/accuracy': 0.7346875071525574, 'train/loss': 1.091256856918335, 'validation/accuracy': 0.6761800050735474, 'validation/loss': 1.3612442016601562, 'validation/num_examples': 50000, 'test/accuracy': 0.5507000088691711, 'test/loss': 1.992889165878296, 'test/num_examples': 10000, 'score': 68941.08154058456, 'total_duration': 77111.07012796402, 'accumulated_submission_time': 68941.08154058456, 'accumulated_eval_time': 8155.512540578842, 'accumulated_logging_time': 6.593385457992554, 'global_step': 150270, 'preemption_count': 0}), (151188, {'train/accuracy': 0.75537109375, 'train/loss': 1.024010181427002, 'validation/accuracy': 0.6801199913024902, 'validation/loss': 1.350976824760437, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 1.9849852323532104, 'test/num_examples': 10000, 'score': 69361.19805550575, 'total_duration': 77582.26147294044, 'accumulated_submission_time': 69361.19805550575, 'accumulated_eval_time': 8206.480012178421, 'accumulated_logging_time': 6.650951862335205, 'global_step': 151188, 'preemption_count': 0}), (152104, {'train/accuracy': 0.7370507717132568, 'train/loss': 1.0689661502838135, 'validation/accuracy': 0.681659996509552, 'validation/loss': 1.3200819492340088, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 1.947913408279419, 'test/num_examples': 10000, 'score': 69781.2301557064, 'total_duration': 78051.009329319, 'accumulated_submission_time': 69781.2301557064, 'accumulated_eval_time': 8255.09532880783, 'accumulated_logging_time': 6.703702688217163, 'global_step': 152104, 'preemption_count': 0}), (153020, {'train/accuracy': 0.7427343726158142, 'train/loss': 1.0332081317901611, 'validation/accuracy': 0.6832399964332581, 'validation/loss': 1.3128416538238525, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 1.9452199935913086, 'test/num_examples': 10000, 'score': 70201.16539907455, 'total_duration': 78519.31473040581, 'accumulated_submission_time': 70201.16539907455, 'accumulated_eval_time': 8303.366862535477, 'accumulated_logging_time': 6.754567861557007, 'global_step': 153020, 'preemption_count': 0}), (153933, {'train/accuracy': 0.7536913752555847, 'train/loss': 1.0018309354782104, 'validation/accuracy': 0.6841399669647217, 'validation/loss': 1.3143833875656128, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 1.9567975997924805, 'test/num_examples': 10000, 'score': 70621.41844844818, 'total_duration': 78989.54914736748, 'accumulated_submission_time': 70621.41844844818, 'accumulated_eval_time': 8353.252356290817, 'accumulated_logging_time': 6.802517414093018, 'global_step': 153933, 'preemption_count': 0}), (154850, {'train/accuracy': 0.74916011095047, 'train/loss': 1.0158547163009644, 'validation/accuracy': 0.688539981842041, 'validation/loss': 1.2876722812652588, 'validation/num_examples': 50000, 'test/accuracy': 0.5651000142097473, 'test/loss': 1.925018310546875, 'test/num_examples': 10000, 'score': 71041.63038349152, 'total_duration': 79458.93891525269, 'accumulated_submission_time': 71041.63038349152, 'accumulated_eval_time': 8402.332137584686, 'accumulated_logging_time': 6.852922439575195, 'global_step': 154850, 'preemption_count': 0}), (155765, {'train/accuracy': 0.7540234327316284, 'train/loss': 0.9922083616256714, 'validation/accuracy': 0.6906799674034119, 'validation/loss': 1.2663934230804443, 'validation/num_examples': 50000, 'test/accuracy': 0.5700000524520874, 'test/loss': 1.9121655225753784, 'test/num_examples': 10000, 'score': 71461.77147507668, 'total_duration': 79929.46589922905, 'accumulated_submission_time': 71461.77147507668, 'accumulated_eval_time': 8452.610144615173, 'accumulated_logging_time': 6.913311243057251, 'global_step': 155765, 'preemption_count': 0}), (156680, {'train/accuracy': 0.7607421875, 'train/loss': 0.9670121073722839, 'validation/accuracy': 0.6918999552726746, 'validation/loss': 1.2740281820297241, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.89507257938385, 'test/num_examples': 10000, 'score': 71881.82072901726, 'total_duration': 80398.42122769356, 'accumulated_submission_time': 71881.82072901726, 'accumulated_eval_time': 8501.413291931152, 'accumulated_logging_time': 6.9684062004089355, 'global_step': 156680, 'preemption_count': 0}), (157595, {'train/accuracy': 0.7591796517372131, 'train/loss': 0.9821126461029053, 'validation/accuracy': 0.6990199685096741, 'validation/loss': 1.2495800256729126, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.8638801574707031, 'test/num_examples': 10000, 'score': 72301.9625506401, 'total_duration': 80867.77632331848, 'accumulated_submission_time': 72301.9625506401, 'accumulated_eval_time': 8550.524432182312, 'accumulated_logging_time': 7.022604942321777, 'global_step': 157595, 'preemption_count': 0}), (158510, {'train/accuracy': 0.7641015648841858, 'train/loss': 0.9493613243103027, 'validation/accuracy': 0.7014600038528442, 'validation/loss': 1.2222778797149658, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 1.8460190296173096, 'test/num_examples': 10000, 'score': 72722.21183228493, 'total_duration': 81337.55066609383, 'accumulated_submission_time': 72722.21183228493, 'accumulated_eval_time': 8599.948066473007, 'accumulated_logging_time': 7.076277017593384, 'global_step': 158510, 'preemption_count': 0}), (159425, {'train/accuracy': 0.7665234208106995, 'train/loss': 0.9387437701225281, 'validation/accuracy': 0.7026199698448181, 'validation/loss': 1.223870873451233, 'validation/num_examples': 50000, 'test/accuracy': 0.5818000435829163, 'test/loss': 1.855764389038086, 'test/num_examples': 10000, 'score': 73142.33656525612, 'total_duration': 81808.82894182205, 'accumulated_submission_time': 73142.33656525612, 'accumulated_eval_time': 8651.003014564514, 'accumulated_logging_time': 7.126868724822998, 'global_step': 159425, 'preemption_count': 0}), (160338, {'train/accuracy': 0.7646874785423279, 'train/loss': 0.9522948861122131, 'validation/accuracy': 0.7034400105476379, 'validation/loss': 1.2250497341156006, 'validation/num_examples': 50000, 'test/accuracy': 0.5848000049591064, 'test/loss': 1.84183931350708, 'test/num_examples': 10000, 'score': 73562.57827568054, 'total_duration': 82278.49391198158, 'accumulated_submission_time': 73562.57827568054, 'accumulated_eval_time': 8700.326808214188, 'accumulated_logging_time': 7.178597688674927, 'global_step': 160338, 'preemption_count': 0}), (161254, {'train/accuracy': 0.7704296708106995, 'train/loss': 0.934205174446106, 'validation/accuracy': 0.7053999900817871, 'validation/loss': 1.2130885124206543, 'validation/num_examples': 50000, 'test/accuracy': 0.5820000171661377, 'test/loss': 1.8352934122085571, 'test/num_examples': 10000, 'score': 73982.7239575386, 'total_duration': 82749.41592168808, 'accumulated_submission_time': 73982.7239575386, 'accumulated_eval_time': 8750.995544433594, 'accumulated_logging_time': 7.237669229507446, 'global_step': 161254, 'preemption_count': 0}), (162170, {'train/accuracy': 0.7777734398841858, 'train/loss': 0.8995881676673889, 'validation/accuracy': 0.7101399898529053, 'validation/loss': 1.1974650621414185, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8181536197662354, 'test/num_examples': 10000, 'score': 74402.66515946388, 'total_duration': 83219.72941589355, 'accumulated_submission_time': 74402.66515946388, 'accumulated_eval_time': 8801.265083551407, 'accumulated_logging_time': 7.292815208435059, 'global_step': 162170, 'preemption_count': 0}), (163085, {'train/accuracy': 0.7899999618530273, 'train/loss': 0.8373759984970093, 'validation/accuracy': 0.7140799760818481, 'validation/loss': 1.1738150119781494, 'validation/num_examples': 50000, 'test/accuracy': 0.5903000235557556, 'test/loss': 1.8106621503829956, 'test/num_examples': 10000, 'score': 74822.65410661697, 'total_duration': 83688.25251555443, 'accumulated_submission_time': 74822.65410661697, 'accumulated_eval_time': 8849.700415611267, 'accumulated_logging_time': 7.344009160995483, 'global_step': 163085, 'preemption_count': 0}), (164001, {'train/accuracy': 0.7776171565055847, 'train/loss': 0.8835697174072266, 'validation/accuracy': 0.7133199572563171, 'validation/loss': 1.1664485931396484, 'validation/num_examples': 50000, 'test/accuracy': 0.5924000144004822, 'test/loss': 1.779520034790039, 'test/num_examples': 10000, 'score': 75243.34757304192, 'total_duration': 84157.42079758644, 'accumulated_submission_time': 75243.34757304192, 'accumulated_eval_time': 8898.0752389431, 'accumulated_logging_time': 7.395813226699829, 'global_step': 164001, 'preemption_count': 0}), (164920, {'train/accuracy': 0.7840234041213989, 'train/loss': 0.8810456395149231, 'validation/accuracy': 0.7141799926757812, 'validation/loss': 1.1800652742385864, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.803371548652649, 'test/num_examples': 10000, 'score': 75663.35647845268, 'total_duration': 84626.98712992668, 'accumulated_submission_time': 75663.35647845268, 'accumulated_eval_time': 8947.53544473648, 'accumulated_logging_time': 7.44543981552124, 'global_step': 164920, 'preemption_count': 0}), (165837, {'train/accuracy': 0.7928906083106995, 'train/loss': 0.8252041339874268, 'validation/accuracy': 0.7172600030899048, 'validation/loss': 1.1588102579116821, 'validation/num_examples': 50000, 'test/accuracy': 0.5946000218391418, 'test/loss': 1.7701854705810547, 'test/num_examples': 10000, 'score': 76083.54992222786, 'total_duration': 85097.52592563629, 'accumulated_submission_time': 76083.54992222786, 'accumulated_eval_time': 8997.781407117844, 'accumulated_logging_time': 7.497044086456299, 'global_step': 165837, 'preemption_count': 0}), (166753, {'train/accuracy': 0.7876757383346558, 'train/loss': 0.8453723788261414, 'validation/accuracy': 0.7234599590301514, 'validation/loss': 1.131539225578308, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.7537925243377686, 'test/num_examples': 10000, 'score': 76503.78957104683, 'total_duration': 85568.19960737228, 'accumulated_submission_time': 76503.78957104683, 'accumulated_eval_time': 9048.111221551895, 'accumulated_logging_time': 7.553990364074707, 'global_step': 166753, 'preemption_count': 0}), (167671, {'train/accuracy': 0.7952343821525574, 'train/loss': 0.8193172812461853, 'validation/accuracy': 0.7249599695205688, 'validation/loss': 1.1146950721740723, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.7299996614456177, 'test/num_examples': 10000, 'score': 76924.13359379768, 'total_duration': 86039.13908457756, 'accumulated_submission_time': 76924.13359379768, 'accumulated_eval_time': 9098.598886728287, 'accumulated_logging_time': 7.613273620605469, 'global_step': 167671, 'preemption_count': 0}), (168589, {'train/accuracy': 0.8006640672683716, 'train/loss': 0.809798002243042, 'validation/accuracy': 0.7245999574661255, 'validation/loss': 1.1315559148788452, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.7418795824050903, 'test/num_examples': 10000, 'score': 77344.50248265266, 'total_duration': 86510.57143831253, 'accumulated_submission_time': 77344.50248265266, 'accumulated_eval_time': 9149.558176994324, 'accumulated_logging_time': 7.669828414916992, 'global_step': 168589, 'preemption_count': 0})], 'global_step': 168978}
I0203 13:03:58.888613 139936116377408 submission_runner.py:586] Timing: 77520.03267264366
I0203 13:03:58.888708 139936116377408 submission_runner.py:588] Total number of evals: 185
I0203 13:03:58.888760 139936116377408 submission_runner.py:589] ====================
I0203 13:03:58.888811 139936116377408 submission_runner.py:542] Using RNG seed 2064292405
I0203 13:03:58.890352 139936116377408 submission_runner.py:551] --- Tuning run 5/5 ---
I0203 13:03:58.890461 139936116377408 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_5.
I0203 13:03:58.891744 139936116377408 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_5/hparams.json.
I0203 13:03:58.892565 139936116377408 submission_runner.py:206] Initializing dataset.
I0203 13:03:58.902018 139936116377408 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0203 13:03:58.913290 139936116377408 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0203 13:03:59.107254 139936116377408 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0203 13:04:03.979075 139936116377408 submission_runner.py:213] Initializing model.
I0203 13:04:11.375826 139936116377408 submission_runner.py:255] Initializing optimizer.
I0203 13:04:11.899476 139936116377408 submission_runner.py:262] Initializing metrics bundle.
I0203 13:04:11.899708 139936116377408 submission_runner.py:280] Initializing checkpoint and logger.
I0203 13:04:11.915197 139936116377408 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_5 with prefix checkpoint_
I0203 13:04:11.915446 139936116377408 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0203 13:04:29.596831 139936116377408 logger_utils.py:220] Unable to record git information. Continuing without it.
I0203 13:04:46.641460 139936116377408 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_5/flags_0.json.
I0203 13:04:46.647099 139936116377408 submission_runner.py:314] Starting training loop.
I0203 13:05:26.925695 139774392231680 logging_writer.py:48] [0] global_step=0, grad_norm=0.348490834236145, loss=6.9077558517456055
I0203 13:05:26.944021 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:05:35.441709 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:06:18.548296 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:06:20.146194 139936116377408 submission_runner.py:408] Time since start: 93.50s, 	Step: 1, 	{'train/accuracy': 0.0009374999790452421, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 40.29682993888855, 'total_duration': 93.49904775619507, 'accumulated_submission_time': 40.29682993888855, 'accumulated_eval_time': 53.20211386680603, 'accumulated_logging_time': 0}
I0203 13:06:20.154985 139774400624384 logging_writer.py:48] [1] accumulated_eval_time=53.202114, accumulated_logging_time=0, accumulated_submission_time=40.296830, global_step=1, preemption_count=0, score=40.296830, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=93.499048, train/accuracy=0.000937, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0203 13:07:30.892001 139774434195200 logging_writer.py:48] [100] global_step=100, grad_norm=0.4443306028842926, loss=6.882812023162842
I0203 13:08:17.367283 139774417409792 logging_writer.py:48] [200] global_step=200, grad_norm=0.7352036237716675, loss=6.807260036468506
I0203 13:09:04.126882 139774434195200 logging_writer.py:48] [300] global_step=300, grad_norm=0.7514103055000305, loss=6.675518989562988
I0203 13:09:50.749163 139774417409792 logging_writer.py:48] [400] global_step=400, grad_norm=0.9284753203392029, loss=6.5976152420043945
I0203 13:10:37.452283 139774434195200 logging_writer.py:48] [500] global_step=500, grad_norm=0.9515183568000793, loss=6.7012481689453125
I0203 13:11:24.351730 139774417409792 logging_writer.py:48] [600] global_step=600, grad_norm=1.348868727684021, loss=6.509418964385986
I0203 13:12:11.032963 139774434195200 logging_writer.py:48] [700] global_step=700, grad_norm=1.3643854856491089, loss=6.347096920013428
I0203 13:12:57.671313 139774417409792 logging_writer.py:48] [800] global_step=800, grad_norm=0.9685580134391785, loss=6.563068866729736
I0203 13:13:20.265711 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:13:33.032106 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:14:09.418283 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:14:11.018870 139936116377408 submission_runner.py:408] Time since start: 564.37s, 	Step: 850, 	{'train/accuracy': 0.034003905951976776, 'train/loss': 5.898688793182373, 'validation/accuracy': 0.029259998351335526, 'validation/loss': 5.964993476867676, 'validation/num_examples': 50000, 'test/accuracy': 0.02240000106394291, 'test/loss': 6.085418224334717, 'test/num_examples': 10000, 'score': 460.35428380966187, 'total_duration': 564.3717305660248, 'accumulated_submission_time': 460.35428380966187, 'accumulated_eval_time': 103.95526385307312, 'accumulated_logging_time': 0.018001556396484375}
I0203 13:14:11.043537 139774434195200 logging_writer.py:48] [850] accumulated_eval_time=103.955264, accumulated_logging_time=0.018002, accumulated_submission_time=460.354284, global_step=850, preemption_count=0, score=460.354284, test/accuracy=0.022400, test/loss=6.085418, test/num_examples=10000, total_duration=564.371731, train/accuracy=0.034004, train/loss=5.898689, validation/accuracy=0.029260, validation/loss=5.964993, validation/num_examples=50000
I0203 13:14:32.237899 139774417409792 logging_writer.py:48] [900] global_step=900, grad_norm=1.7616350650787354, loss=6.4907073974609375
I0203 13:15:17.410913 139774434195200 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2228091955184937, loss=6.0826826095581055
I0203 13:16:03.908122 139774417409792 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.3783550262451172, loss=6.701277256011963
I0203 13:16:50.470799 139774434195200 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2821903228759766, loss=6.041513919830322
I0203 13:17:37.295603 139774417409792 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8654741048812866, loss=6.73868989944458
I0203 13:18:26.272573 139774434195200 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0578217506408691, loss=6.137941360473633
I0203 13:19:13.002862 139774417409792 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.976412296295166, loss=6.187190532684326
I0203 13:19:59.584426 139774434195200 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9599773287773132, loss=5.7803850173950195
I0203 13:20:46.597341 139774417409792 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.037449836730957, loss=5.712041854858398
I0203 13:21:11.482813 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:21:23.812806 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:21:59.414657 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:22:01.017238 139936116377408 submission_runner.py:408] Time since start: 1034.37s, 	Step: 1755, 	{'train/accuracy': 0.06978515535593033, 'train/loss': 5.347999572753906, 'validation/accuracy': 0.06480000168085098, 'validation/loss': 5.401942253112793, 'validation/num_examples': 50000, 'test/accuracy': 0.05040000379085541, 'test/loss': 5.601574420928955, 'test/num_examples': 10000, 'score': 880.7335257530212, 'total_duration': 1034.3701009750366, 'accumulated_submission_time': 880.7335257530212, 'accumulated_eval_time': 153.4896755218506, 'accumulated_logging_time': 0.055764198303222656}
I0203 13:22:01.038181 139774434195200 logging_writer.py:48] [1755] accumulated_eval_time=153.489676, accumulated_logging_time=0.055764, accumulated_submission_time=880.733526, global_step=1755, preemption_count=0, score=880.733526, test/accuracy=0.050400, test/loss=5.601574, test/num_examples=10000, total_duration=1034.370101, train/accuracy=0.069785, train/loss=5.348000, validation/accuracy=0.064800, validation/loss=5.401942, validation/num_examples=50000
I0203 13:22:20.107339 139774417409792 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9319819808006287, loss=5.816807270050049
I0203 13:23:05.141753 139774434195200 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0327837467193604, loss=6.391375541687012
I0203 13:23:51.747614 139774417409792 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8433644771575928, loss=6.545988082885742
I0203 13:24:38.436370 139774434195200 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.091548204421997, loss=5.647472858428955
I0203 13:25:24.891714 139774417409792 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8109933733940125, loss=6.14555025100708
I0203 13:26:11.556815 139774434195200 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8435701131820679, loss=5.628754615783691
I0203 13:26:57.738523 139774417409792 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3954286575317383, loss=5.495467185974121
I0203 13:27:44.747790 139774434195200 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6258780360221863, loss=6.3627166748046875
I0203 13:28:31.129264 139774417409792 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9031124114990234, loss=6.068066120147705
I0203 13:29:01.386182 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:29:13.760277 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:29:51.304815 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:29:52.909866 139936116377408 submission_runner.py:408] Time since start: 1506.26s, 	Step: 2667, 	{'train/accuracy': 0.11636718362569809, 'train/loss': 4.784284591674805, 'validation/accuracy': 0.1093599945306778, 'validation/loss': 4.85033655166626, 'validation/num_examples': 50000, 'test/accuracy': 0.0861000046133995, 'test/loss': 5.135005950927734, 'test/num_examples': 10000, 'score': 1301.019949913025, 'total_duration': 1506.262699842453, 'accumulated_submission_time': 1301.019949913025, 'accumulated_eval_time': 205.0133285522461, 'accumulated_logging_time': 0.09068870544433594}
I0203 13:29:52.925801 139774434195200 logging_writer.py:48] [2667] accumulated_eval_time=205.013329, accumulated_logging_time=0.090689, accumulated_submission_time=1301.019950, global_step=2667, preemption_count=0, score=1301.019950, test/accuracy=0.086100, test/loss=5.135006, test/num_examples=10000, total_duration=1506.262700, train/accuracy=0.116367, train/loss=4.784285, validation/accuracy=0.109360, validation/loss=4.850337, validation/num_examples=50000
I0203 13:30:07.016775 139774417409792 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8020141124725342, loss=5.738064289093018
I0203 13:30:51.417300 139774434195200 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9835426807403564, loss=5.433894157409668
I0203 13:31:38.347903 139774417409792 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9074565172195435, loss=5.461616039276123
I0203 13:32:25.262223 139774434195200 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0494418144226074, loss=5.283935070037842
I0203 13:33:12.170054 139774417409792 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9560899138450623, loss=5.193713665008545
I0203 13:33:58.670474 139774434195200 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8134840726852417, loss=6.4985671043396
I0203 13:34:45.139928 139774417409792 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7585902214050293, loss=5.239678859710693
I0203 13:35:31.979932 139774434195200 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9399389028549194, loss=5.692850112915039
I0203 13:36:18.643257 139774417409792 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8435208201408386, loss=5.173618793487549
I0203 13:36:52.981536 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:37:05.215834 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:37:41.885273 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:37:43.490098 139936116377408 submission_runner.py:408] Time since start: 1976.84s, 	Step: 3575, 	{'train/accuracy': 0.17994140088558197, 'train/loss': 4.237734794616699, 'validation/accuracy': 0.16110000014305115, 'validation/loss': 4.358086585998535, 'validation/num_examples': 50000, 'test/accuracy': 0.12190000712871552, 'test/loss': 4.7613372802734375, 'test/num_examples': 10000, 'score': 1721.0184772014618, 'total_duration': 1976.8429546356201, 'accumulated_submission_time': 1721.0184772014618, 'accumulated_eval_time': 255.52188277244568, 'accumulated_logging_time': 0.11698031425476074}
I0203 13:37:43.507321 139774434195200 logging_writer.py:48] [3575] accumulated_eval_time=255.521883, accumulated_logging_time=0.116980, accumulated_submission_time=1721.018477, global_step=3575, preemption_count=0, score=1721.018477, test/accuracy=0.121900, test/loss=4.761337, test/num_examples=10000, total_duration=1976.842955, train/accuracy=0.179941, train/loss=4.237735, validation/accuracy=0.161100, validation/loss=4.358087, validation/num_examples=50000
I0203 13:37:54.290599 139774417409792 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.1445765495300293, loss=5.235269546508789
I0203 13:38:38.299332 139774434195200 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8547691106796265, loss=5.075927257537842
I0203 13:39:25.316943 139774417409792 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9461258053779602, loss=5.061465740203857
I0203 13:40:12.293941 139774434195200 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9297828674316406, loss=4.938865661621094
I0203 13:40:58.977253 139774417409792 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.91209477186203, loss=6.361823081970215
I0203 13:41:45.979700 139774434195200 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7192911505699158, loss=6.260058403015137
I0203 13:42:32.986680 139774417409792 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7548472881317139, loss=5.120739459991455
I0203 13:43:19.788088 139774434195200 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6368808746337891, loss=6.236339569091797
I0203 13:44:06.973854 139774417409792 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8916555643081665, loss=4.807672500610352
I0203 13:44:43.773247 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:44:55.770914 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:45:34.187801 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:45:35.860286 139936116377408 submission_runner.py:408] Time since start: 2449.21s, 	Step: 4481, 	{'train/accuracy': 0.21689452230930328, 'train/loss': 3.9525179862976074, 'validation/accuracy': 0.2021399885416031, 'validation/loss': 4.050927639007568, 'validation/num_examples': 50000, 'test/accuracy': 0.1535000056028366, 'test/loss': 4.470614910125732, 'test/num_examples': 10000, 'score': 2141.226948738098, 'total_duration': 2449.213127851486, 'accumulated_submission_time': 2141.226948738098, 'accumulated_eval_time': 307.60889863967896, 'accumulated_logging_time': 0.14408445358276367}
I0203 13:45:35.881345 139774434195200 logging_writer.py:48] [4481] accumulated_eval_time=307.608899, accumulated_logging_time=0.144084, accumulated_submission_time=2141.226949, global_step=4481, preemption_count=0, score=2141.226949, test/accuracy=0.153500, test/loss=4.470615, test/num_examples=10000, total_duration=2449.213128, train/accuracy=0.216895, train/loss=3.952518, validation/accuracy=0.202140, validation/loss=4.050928, validation/num_examples=50000
I0203 13:45:44.169395 139774417409792 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6711258292198181, loss=5.855978012084961
I0203 13:46:28.014535 139774434195200 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9121631979942322, loss=4.667150974273682
I0203 13:47:14.479746 139774417409792 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8307198286056519, loss=4.838991641998291
I0203 13:48:00.931848 139774434195200 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8443096876144409, loss=4.566877365112305
I0203 13:48:47.612917 139774417409792 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6973364353179932, loss=5.551005840301514
I0203 13:49:34.487437 139774434195200 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7179792523384094, loss=5.310868740081787
I0203 13:50:21.254711 139774417409792 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.0755372047424316, loss=4.871152400970459
I0203 13:51:08.132654 139774434195200 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7367352843284607, loss=4.642478942871094
I0203 13:51:54.869302 139774417409792 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.1470410823822021, loss=4.574643135070801
I0203 13:52:35.933885 139936116377408 spec.py:321] Evaluating on the training split.
I0203 13:52:47.991588 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 13:53:26.489306 139936116377408 spec.py:349] Evaluating on the test split.
I0203 13:53:28.094959 139936116377408 submission_runner.py:408] Time since start: 2921.45s, 	Step: 5390, 	{'train/accuracy': 0.26624998450279236, 'train/loss': 3.6010043621063232, 'validation/accuracy': 0.24743999540805817, 'validation/loss': 3.722931385040283, 'validation/num_examples': 50000, 'test/accuracy': 0.1933000087738037, 'test/loss': 4.191379547119141, 'test/num_examples': 10000, 'score': 2561.221724510193, 'total_duration': 2921.4478216171265, 'accumulated_submission_time': 2561.221724510193, 'accumulated_eval_time': 359.7699909210205, 'accumulated_logging_time': 0.17619538307189941}
I0203 13:53:28.111923 139774434195200 logging_writer.py:48] [5390] accumulated_eval_time=359.769991, accumulated_logging_time=0.176195, accumulated_submission_time=2561.221725, global_step=5390, preemption_count=0, score=2561.221725, test/accuracy=0.193300, test/loss=4.191380, test/num_examples=10000, total_duration=2921.447822, train/accuracy=0.266250, train/loss=3.601004, validation/accuracy=0.247440, validation/loss=3.722931, validation/num_examples=50000
I0203 13:53:32.676788 139774417409792 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8621528744697571, loss=4.641098976135254
I0203 13:54:15.952708 139774434195200 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8714579939842224, loss=4.462155818939209
I0203 13:55:02.370394 139774417409792 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7234157919883728, loss=6.097019195556641
I0203 13:55:48.678108 139774434195200 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5696134567260742, loss=6.1372971534729
I0203 13:56:35.229927 139774417409792 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8222704529762268, loss=4.65662145614624
I0203 13:57:21.908140 139774434195200 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8204821944236755, loss=4.161751747131348
I0203 13:58:08.414431 139774417409792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7489297986030579, loss=4.372951030731201
I0203 13:58:54.957249 139774434195200 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7880575656890869, loss=4.320786952972412
I0203 13:59:41.428399 139774417409792 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7189861536026001, loss=4.735447406768799
I0203 14:00:28.074712 139774434195200 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5178156495094299, loss=6.115748882293701
I0203 14:00:28.188004 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:00:40.142758 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:01:20.220474 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:01:21.824554 139936116377408 submission_runner.py:408] Time since start: 3395.18s, 	Step: 6302, 	{'train/accuracy': 0.3132421672344208, 'train/loss': 3.299769163131714, 'validation/accuracy': 0.2857999801635742, 'validation/loss': 3.4638993740081787, 'validation/num_examples': 50000, 'test/accuracy': 0.21870000660419464, 'test/loss': 3.9623067378997803, 'test/num_examples': 10000, 'score': 2981.236423969269, 'total_duration': 3395.177416563034, 'accumulated_submission_time': 2981.236423969269, 'accumulated_eval_time': 413.40653800964355, 'accumulated_logging_time': 0.20651817321777344}
I0203 14:01:21.841585 139774417409792 logging_writer.py:48] [6302] accumulated_eval_time=413.406538, accumulated_logging_time=0.206518, accumulated_submission_time=2981.236424, global_step=6302, preemption_count=0, score=2981.236424, test/accuracy=0.218700, test/loss=3.962307, test/num_examples=10000, total_duration=3395.177417, train/accuracy=0.313242, train/loss=3.299769, validation/accuracy=0.285800, validation/loss=3.463899, validation/num_examples=50000
I0203 14:02:04.135272 139774434195200 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5410995483398438, loss=5.335482597351074
I0203 14:02:50.433261 139774417409792 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8205316662788391, loss=4.519603252410889
I0203 14:03:36.960623 139774434195200 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8007403016090393, loss=4.4396281242370605
I0203 14:04:23.719347 139774417409792 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.718220055103302, loss=4.749431610107422
I0203 14:05:10.166281 139774434195200 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7026888132095337, loss=4.487992763519287
I0203 14:05:56.598602 139774417409792 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7931029796600342, loss=4.162724018096924
I0203 14:06:43.166836 139774434195200 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8399516940116882, loss=4.223075866699219
I0203 14:07:29.772537 139774417409792 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9443338513374329, loss=4.088662624359131
I0203 14:08:16.425107 139774434195200 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7694430947303772, loss=4.2483415603637695
I0203 14:08:21.885430 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:08:33.921492 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:09:11.085288 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:09:12.690422 139936116377408 submission_runner.py:408] Time since start: 3866.04s, 	Step: 7214, 	{'train/accuracy': 0.3377148509025574, 'train/loss': 3.1365878582000732, 'validation/accuracy': 0.3125399947166443, 'validation/loss': 3.2730464935302734, 'validation/num_examples': 50000, 'test/accuracy': 0.2492000162601471, 'test/loss': 3.8009603023529053, 'test/num_examples': 10000, 'score': 3401.2242062091827, 'total_duration': 3866.0432856082916, 'accumulated_submission_time': 3401.2242062091827, 'accumulated_eval_time': 464.2115104198456, 'accumulated_logging_time': 0.23270773887634277}
I0203 14:09:12.709206 139774417409792 logging_writer.py:48] [7214] accumulated_eval_time=464.211510, accumulated_logging_time=0.232708, accumulated_submission_time=3401.224206, global_step=7214, preemption_count=0, score=3401.224206, test/accuracy=0.249200, test/loss=3.800960, test/num_examples=10000, total_duration=3866.043286, train/accuracy=0.337715, train/loss=3.136588, validation/accuracy=0.312540, validation/loss=3.273046, validation/num_examples=50000
I0203 14:09:49.436435 139774434195200 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6358190178871155, loss=6.100253105163574
I0203 14:10:35.816772 139774417409792 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9480798244476318, loss=4.133011817932129
I0203 14:11:22.941575 139774434195200 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7407770156860352, loss=3.9600048065185547
I0203 14:12:09.426274 139774417409792 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7352216243743896, loss=4.350305557250977
I0203 14:12:55.725178 139774434195200 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7983888983726501, loss=4.061439514160156
I0203 14:13:42.437469 139774417409792 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7983882427215576, loss=4.143923759460449
I0203 14:14:28.990157 139774434195200 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8245009779930115, loss=4.577512264251709
I0203 14:15:15.623309 139774417409792 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8377622961997986, loss=5.929781913757324
I0203 14:16:01.979599 139774434195200 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8963583111763, loss=4.098382949829102
I0203 14:16:12.922996 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:16:24.962063 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:17:01.082069 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:17:02.694675 139936116377408 submission_runner.py:408] Time since start: 4336.05s, 	Step: 8125, 	{'train/accuracy': 0.37572264671325684, 'train/loss': 2.8953447341918945, 'validation/accuracy': 0.3463599979877472, 'validation/loss': 3.064788341522217, 'validation/num_examples': 50000, 'test/accuracy': 0.26820001006126404, 'test/loss': 3.6436169147491455, 'test/num_examples': 10000, 'score': 3821.3774077892303, 'total_duration': 4336.0475380420685, 'accumulated_submission_time': 3821.3774077892303, 'accumulated_eval_time': 513.9831821918488, 'accumulated_logging_time': 0.26525330543518066}
I0203 14:17:02.711913 139774417409792 logging_writer.py:48] [8125] accumulated_eval_time=513.983182, accumulated_logging_time=0.265253, accumulated_submission_time=3821.377408, global_step=8125, preemption_count=0, score=3821.377408, test/accuracy=0.268200, test/loss=3.643617, test/num_examples=10000, total_duration=4336.047538, train/accuracy=0.375723, train/loss=2.895345, validation/accuracy=0.346360, validation/loss=3.064788, validation/num_examples=50000
I0203 14:17:34.393241 139774434195200 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6258758902549744, loss=5.102156162261963
I0203 14:18:20.825640 139774417409792 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0483306646347046, loss=4.15896463394165
I0203 14:19:07.682318 139774434195200 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.9431400895118713, loss=3.9290943145751953
I0203 14:19:54.639253 139774417409792 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9462635517120361, loss=3.9367218017578125
I0203 14:20:41.483066 139774434195200 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7231335043907166, loss=4.933542728424072
I0203 14:21:28.394533 139774417409792 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.5518954992294312, loss=5.703212738037109
I0203 14:22:15.653274 139774434195200 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0394399166107178, loss=3.883988618850708
I0203 14:23:02.135581 139774417409792 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6781293153762817, loss=5.574188709259033
I0203 14:23:48.859100 139774434195200 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7666563391685486, loss=4.322621822357178
I0203 14:24:02.843561 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:24:14.866726 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:24:52.636561 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:24:54.252858 139936116377408 submission_runner.py:408] Time since start: 4807.61s, 	Step: 9031, 	{'train/accuracy': 0.3907421827316284, 'train/loss': 2.777474880218506, 'validation/accuracy': 0.35627999901771545, 'validation/loss': 2.988455295562744, 'validation/num_examples': 50000, 'test/accuracy': 0.2786000072956085, 'test/loss': 3.5704877376556396, 'test/num_examples': 10000, 'score': 4241.45069026947, 'total_duration': 4807.605703353882, 'accumulated_submission_time': 4241.45069026947, 'accumulated_eval_time': 565.3924582004547, 'accumulated_logging_time': 0.2942073345184326}
I0203 14:24:54.271352 139774417409792 logging_writer.py:48] [9031] accumulated_eval_time=565.392458, accumulated_logging_time=0.294207, accumulated_submission_time=4241.450690, global_step=9031, preemption_count=0, score=4241.450690, test/accuracy=0.278600, test/loss=3.570488, test/num_examples=10000, total_duration=4807.605703, train/accuracy=0.390742, train/loss=2.777475, validation/accuracy=0.356280, validation/loss=2.988455, validation/num_examples=50000
I0203 14:25:23.280276 139774434195200 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.0786436796188354, loss=3.8787221908569336
I0203 14:26:09.772926 139774417409792 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9016973972320557, loss=3.794140338897705
I0203 14:26:56.775655 139774434195200 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6639421582221985, loss=5.331366539001465
I0203 14:27:43.623945 139774417409792 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0742974281311035, loss=3.7233307361602783
I0203 14:28:30.957547 139774434195200 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7057135701179504, loss=5.930149555206299
I0203 14:29:17.844987 139774417409792 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6238892674446106, loss=5.602517127990723
I0203 14:30:04.672316 139774434195200 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6326237320899963, loss=5.76201057434082
I0203 14:30:51.488945 139774417409792 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8991698026657104, loss=3.726804494857788
I0203 14:31:38.597430 139774434195200 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6353771090507507, loss=5.87397575378418
I0203 14:31:54.619994 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:32:07.052273 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:32:44.896488 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:32:46.501076 139936116377408 submission_runner.py:408] Time since start: 5279.85s, 	Step: 9936, 	{'train/accuracy': 0.3995312452316284, 'train/loss': 2.739250659942627, 'validation/accuracy': 0.373879998922348, 'validation/loss': 2.8733174800872803, 'validation/num_examples': 50000, 'test/accuracy': 0.2880000174045563, 'test/loss': 3.4860610961914062, 'test/num_examples': 10000, 'score': 4661.742277622223, 'total_duration': 5279.853937387466, 'accumulated_submission_time': 4661.742277622223, 'accumulated_eval_time': 617.273538351059, 'accumulated_logging_time': 0.3234410285949707}
I0203 14:32:46.522112 139774417409792 logging_writer.py:48] [9936] accumulated_eval_time=617.273538, accumulated_logging_time=0.323441, accumulated_submission_time=4661.742278, global_step=9936, preemption_count=0, score=4661.742278, test/accuracy=0.288000, test/loss=3.486061, test/num_examples=10000, total_duration=5279.853937, train/accuracy=0.399531, train/loss=2.739251, validation/accuracy=0.373880, validation/loss=2.873317, validation/num_examples=50000
I0203 14:33:13.500481 139774434195200 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8948127031326294, loss=3.618903636932373
I0203 14:33:59.620162 139774417409792 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7826814651489258, loss=5.879865646362305
I0203 14:34:46.335262 139774434195200 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7680444717407227, loss=5.194077968597412
I0203 14:35:33.345361 139774417409792 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6528050303459167, loss=5.0339226722717285
I0203 14:36:20.148916 139774434195200 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7647078037261963, loss=4.549625396728516
I0203 14:37:07.048704 139774417409792 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7200892567634583, loss=4.498007297515869
I0203 14:37:53.745044 139774434195200 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8897404074668884, loss=3.617825984954834
I0203 14:38:40.589065 139774417409792 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9362329840660095, loss=3.739482879638672
I0203 14:39:27.379044 139774434195200 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.8519826531410217, loss=3.778923511505127
I0203 14:39:46.553923 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:39:58.541214 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:40:38.575165 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:40:40.184638 139936116377408 submission_runner.py:408] Time since start: 5753.54s, 	Step: 10843, 	{'train/accuracy': 0.4029882848262787, 'train/loss': 2.765798807144165, 'validation/accuracy': 0.3730599880218506, 'validation/loss': 2.929877519607544, 'validation/num_examples': 50000, 'test/accuracy': 0.29330000281333923, 'test/loss': 3.4927845001220703, 'test/num_examples': 10000, 'score': 5081.717084169388, 'total_duration': 5753.537490606308, 'accumulated_submission_time': 5081.717084169388, 'accumulated_eval_time': 670.9042701721191, 'accumulated_logging_time': 0.35435938835144043}
I0203 14:40:40.202622 139774417409792 logging_writer.py:48] [10843] accumulated_eval_time=670.904270, accumulated_logging_time=0.354359, accumulated_submission_time=5081.717084, global_step=10843, preemption_count=0, score=5081.717084, test/accuracy=0.293300, test/loss=3.492785, test/num_examples=10000, total_duration=5753.537491, train/accuracy=0.402988, train/loss=2.765799, validation/accuracy=0.373060, validation/loss=2.929878, validation/num_examples=50000
I0203 14:41:04.532100 139774434195200 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6436028480529785, loss=4.915124893188477
I0203 14:41:50.069660 139774417409792 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8911728858947754, loss=3.5333213806152344
I0203 14:42:37.220537 139774434195200 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.0186526775360107, loss=3.599618911743164
I0203 14:43:24.097989 139774417409792 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.8681889176368713, loss=3.798704147338867
I0203 14:44:11.038372 139774434195200 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9616858959197998, loss=3.6060938835144043
I0203 14:44:57.483966 139774417409792 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.799325704574585, loss=5.710996627807617
I0203 14:45:44.218296 139774434195200 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7886088490486145, loss=4.455024719238281
I0203 14:46:31.143079 139774417409792 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9098682403564453, loss=3.657946825027466
I0203 14:47:17.733318 139774434195200 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7039791941642761, loss=5.085369110107422
I0203 14:47:40.405313 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:47:52.464347 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:48:30.054413 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:48:31.655830 139936116377408 submission_runner.py:408] Time since start: 6225.01s, 	Step: 11750, 	{'train/accuracy': 0.4351171851158142, 'train/loss': 2.5774996280670166, 'validation/accuracy': 0.39941999316215515, 'validation/loss': 2.767735004425049, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.375816583633423, 'test/num_examples': 10000, 'score': 5501.863046884537, 'total_duration': 6225.0086896419525, 'accumulated_submission_time': 5501.863046884537, 'accumulated_eval_time': 722.1547908782959, 'accumulated_logging_time': 0.3821694850921631}
I0203 14:48:31.677495 139774417409792 logging_writer.py:48] [11750] accumulated_eval_time=722.154791, accumulated_logging_time=0.382169, accumulated_submission_time=5501.863047, global_step=11750, preemption_count=0, score=5501.863047, test/accuracy=0.311300, test/loss=3.375817, test/num_examples=10000, total_duration=6225.008690, train/accuracy=0.435117, train/loss=2.577500, validation/accuracy=0.399420, validation/loss=2.767735, validation/num_examples=50000
I0203 14:48:52.845189 139774434195200 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9690445065498352, loss=3.552724838256836
I0203 14:49:37.848580 139774417409792 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.8885082006454468, loss=5.7120795249938965
I0203 14:50:24.705968 139774434195200 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9757260084152222, loss=3.3633553981781006
I0203 14:51:11.481909 139774417409792 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9297657012939453, loss=4.153757572174072
I0203 14:51:58.285074 139774434195200 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8422112464904785, loss=3.6468591690063477
I0203 14:52:45.387974 139774417409792 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.005135178565979, loss=3.533348798751831
I0203 14:53:32.100725 139774434195200 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8560090661048889, loss=3.698824882507324
I0203 14:54:19.327611 139774417409792 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.1239744424819946, loss=3.4764010906219482
I0203 14:55:06.093641 139774434195200 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9580464363098145, loss=4.241269111633301
I0203 14:55:31.690266 139936116377408 spec.py:321] Evaluating on the training split.
I0203 14:55:43.919646 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 14:56:22.900817 139936116377408 spec.py:349] Evaluating on the test split.
I0203 14:56:24.498105 139936116377408 submission_runner.py:408] Time since start: 6697.85s, 	Step: 12656, 	{'train/accuracy': 0.4435742199420929, 'train/loss': 2.502842903137207, 'validation/accuracy': 0.4075399935245514, 'validation/loss': 2.688591718673706, 'validation/num_examples': 50000, 'test/accuracy': 0.3148000240325928, 'test/loss': 3.308043956756592, 'test/num_examples': 10000, 'score': 5921.816805839539, 'total_duration': 6697.850959300995, 'accumulated_submission_time': 5921.816805839539, 'accumulated_eval_time': 774.9626307487488, 'accumulated_logging_time': 0.41584062576293945}
I0203 14:56:24.516149 139774417409792 logging_writer.py:48] [12656] accumulated_eval_time=774.962631, accumulated_logging_time=0.415841, accumulated_submission_time=5921.816806, global_step=12656, preemption_count=0, score=5921.816806, test/accuracy=0.314800, test/loss=3.308044, test/num_examples=10000, total_duration=6697.850959, train/accuracy=0.443574, train/loss=2.502843, validation/accuracy=0.407540, validation/loss=2.688592, validation/num_examples=50000
I0203 14:56:43.186446 139774434195200 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.9138275384902954, loss=3.4857029914855957
I0203 14:57:28.426933 139774417409792 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7039282917976379, loss=5.048728942871094
I0203 14:58:15.454703 139774434195200 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9087300896644592, loss=3.448657274246216
I0203 14:59:02.207691 139774417409792 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.0020806789398193, loss=3.4589767456054688
I0203 14:59:48.872164 139774434195200 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9468750953674316, loss=3.595618486404419
I0203 15:00:35.669399 139774417409792 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9694759249687195, loss=3.4184019565582275
I0203 15:01:22.599655 139774434195200 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0300006866455078, loss=3.3732779026031494
I0203 15:02:09.847857 139774417409792 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6839878559112549, loss=5.8100481033325195
I0203 15:02:56.805522 139774434195200 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.9542970061302185, loss=3.4188084602355957
I0203 15:03:24.791336 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:03:36.876423 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:04:14.664600 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:04:16.271223 139936116377408 submission_runner.py:408] Time since start: 7169.62s, 	Step: 13561, 	{'train/accuracy': 0.4646874964237213, 'train/loss': 2.3886172771453857, 'validation/accuracy': 0.4336400032043457, 'validation/loss': 2.552778720855713, 'validation/num_examples': 50000, 'test/accuracy': 0.33830001950263977, 'test/loss': 3.179518222808838, 'test/num_examples': 10000, 'score': 6342.030628442764, 'total_duration': 7169.624086141586, 'accumulated_submission_time': 6342.030628442764, 'accumulated_eval_time': 826.442524433136, 'accumulated_logging_time': 0.4475893974304199}
I0203 15:04:16.294242 139774417409792 logging_writer.py:48] [13561] accumulated_eval_time=826.442524, accumulated_logging_time=0.447589, accumulated_submission_time=6342.030628, global_step=13561, preemption_count=0, score=6342.030628, test/accuracy=0.338300, test/loss=3.179518, test/num_examples=10000, total_duration=7169.624086, train/accuracy=0.464687, train/loss=2.388617, validation/accuracy=0.433640, validation/loss=2.552779, validation/num_examples=50000
I0203 15:04:32.872730 139774434195200 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.8066623210906982, loss=5.060335159301758
I0203 15:05:17.635498 139774417409792 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7261969447135925, loss=5.007787704467773
I0203 15:06:04.748559 139774434195200 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.764029324054718, loss=4.563602924346924
I0203 15:06:51.638686 139774417409792 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6969559192657471, loss=5.447909832000732
I0203 15:07:38.733316 139774434195200 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.0989633798599243, loss=3.449941396713257
I0203 15:08:25.575611 139774417409792 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0485280752182007, loss=3.388636350631714
I0203 15:09:12.687530 139774434195200 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.1424155235290527, loss=3.41837739944458
I0203 15:09:59.701029 139774417409792 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8973795771598816, loss=4.9426116943359375
I0203 15:10:46.555882 139774434195200 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.1225215196609497, loss=3.5171260833740234
I0203 15:11:16.350501 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:11:28.220682 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:12:07.662402 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:12:09.266436 139936116377408 submission_runner.py:408] Time since start: 7642.62s, 	Step: 14465, 	{'train/accuracy': 0.48277342319488525, 'train/loss': 2.269944429397583, 'validation/accuracy': 0.4453199803829193, 'validation/loss': 2.4657223224639893, 'validation/num_examples': 50000, 'test/accuracy': 0.34390002489089966, 'test/loss': 3.1078310012817383, 'test/num_examples': 10000, 'score': 6762.030198812485, 'total_duration': 7642.619294166565, 'accumulated_submission_time': 6762.030198812485, 'accumulated_eval_time': 879.3584513664246, 'accumulated_logging_time': 0.47969818115234375}
I0203 15:12:09.284151 139774417409792 logging_writer.py:48] [14465] accumulated_eval_time=879.358451, accumulated_logging_time=0.479698, accumulated_submission_time=6762.030199, global_step=14465, preemption_count=0, score=6762.030199, test/accuracy=0.343900, test/loss=3.107831, test/num_examples=10000, total_duration=7642.619294, train/accuracy=0.482773, train/loss=2.269944, validation/accuracy=0.445320, validation/loss=2.465722, validation/num_examples=50000
I0203 15:12:24.213520 139774434195200 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9620895385742188, loss=3.2400565147399902
I0203 15:13:08.928185 139774417409792 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0004600286483765, loss=3.3484747409820557
I0203 15:13:55.599115 139774434195200 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0232434272766113, loss=3.29201340675354
I0203 15:14:42.889005 139774417409792 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0351563692092896, loss=3.3827106952667236
I0203 15:15:29.394917 139774434195200 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.8153397440910339, loss=5.8458123207092285
I0203 15:16:16.584524 139774417409792 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8824766874313354, loss=4.297988414764404
I0203 15:17:03.359165 139774434195200 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9695208072662354, loss=3.3092222213745117
I0203 15:17:50.171764 139774417409792 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8032861351966858, loss=3.9556450843811035
I0203 15:18:37.062255 139774434195200 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8916851878166199, loss=3.758498430252075
I0203 15:19:09.531541 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:19:21.497290 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:19:58.995195 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:20:00.603556 139936116377408 submission_runner.py:408] Time since start: 8113.96s, 	Step: 15371, 	{'train/accuracy': 0.5088281035423279, 'train/loss': 2.144819974899292, 'validation/accuracy': 0.4482799768447876, 'validation/loss': 2.458869457244873, 'validation/num_examples': 50000, 'test/accuracy': 0.344400018453598, 'test/loss': 3.1176199913024902, 'test/num_examples': 10000, 'score': 7182.221163272858, 'total_duration': 8113.95642209053, 'accumulated_submission_time': 7182.221163272858, 'accumulated_eval_time': 930.4304752349854, 'accumulated_logging_time': 0.5069384574890137}
I0203 15:20:00.621945 139774417409792 logging_writer.py:48] [15371] accumulated_eval_time=930.430475, accumulated_logging_time=0.506938, accumulated_submission_time=7182.221163, global_step=15371, preemption_count=0, score=7182.221163, test/accuracy=0.344400, test/loss=3.117620, test/num_examples=10000, total_duration=8113.956422, train/accuracy=0.508828, train/loss=2.144820, validation/accuracy=0.448280, validation/loss=2.458869, validation/num_examples=50000
I0203 15:20:13.064161 139774434195200 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.937874972820282, loss=3.3005282878875732
I0203 15:20:57.587836 139774417409792 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6906818151473999, loss=5.400544166564941
I0203 15:21:44.573077 139774434195200 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7244037985801697, loss=4.530467510223389
I0203 15:22:31.549392 139774417409792 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9225301742553711, loss=3.247471332550049
I0203 15:23:18.427011 139774434195200 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7619437575340271, loss=5.642767906188965
I0203 15:24:05.369328 139774417409792 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8782641291618347, loss=3.7337212562561035
I0203 15:24:52.131799 139774434195200 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.8501717448234558, loss=4.192633152008057
I0203 15:25:38.840874 139774417409792 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.794674277305603, loss=5.624266624450684
I0203 15:26:25.627489 139774434195200 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8916633129119873, loss=3.397702932357788
I0203 15:27:00.927083 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:27:13.218504 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:27:50.635014 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:27:52.240749 139936116377408 submission_runner.py:408] Time since start: 8585.59s, 	Step: 16277, 	{'train/accuracy': 0.4963085949420929, 'train/loss': 2.2050812244415283, 'validation/accuracy': 0.45799997448921204, 'validation/loss': 2.4059407711029053, 'validation/num_examples': 50000, 'test/accuracy': 0.35430002212524414, 'test/loss': 3.049616813659668, 'test/num_examples': 10000, 'score': 7602.466048240662, 'total_duration': 8585.593609571457, 'accumulated_submission_time': 7602.466048240662, 'accumulated_eval_time': 981.7441575527191, 'accumulated_logging_time': 0.5383155345916748}
I0203 15:27:52.260565 139774417409792 logging_writer.py:48] [16277] accumulated_eval_time=981.744158, accumulated_logging_time=0.538316, accumulated_submission_time=7602.466048, global_step=16277, preemption_count=0, score=7602.466048, test/accuracy=0.354300, test/loss=3.049617, test/num_examples=10000, total_duration=8585.593610, train/accuracy=0.496309, train/loss=2.205081, validation/accuracy=0.458000, validation/loss=2.405941, validation/num_examples=50000
I0203 15:28:02.209101 139774434195200 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.1173893213272095, loss=3.457031726837158
I0203 15:28:46.299831 139774417409792 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.0941745042800903, loss=3.310044288635254
I0203 15:29:32.945058 139774434195200 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.0498027801513672, loss=3.168588638305664
I0203 15:30:20.023374 139774417409792 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.2556397914886475, loss=3.359196662902832
I0203 15:31:06.936757 139774434195200 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0421901941299438, loss=3.2026827335357666
I0203 15:31:53.830514 139774417409792 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.060359239578247, loss=3.3999292850494385
I0203 15:32:40.954273 139774434195200 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.786292552947998, loss=4.674333572387695
I0203 15:33:27.690231 139774417409792 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.0314736366271973, loss=3.3620524406433105
I0203 15:34:14.545407 139774434195200 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.076405644416809, loss=3.16862154006958
I0203 15:34:52.416158 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:35:04.852361 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:35:42.833024 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:35:44.442455 139936116377408 submission_runner.py:408] Time since start: 9057.80s, 	Step: 17183, 	{'train/accuracy': 0.49980467557907104, 'train/loss': 2.1979124546051025, 'validation/accuracy': 0.46143999695777893, 'validation/loss': 2.4079489707946777, 'validation/num_examples': 50000, 'test/accuracy': 0.36330002546310425, 'test/loss': 3.038583993911743, 'test/num_examples': 10000, 'score': 8022.563579797745, 'total_duration': 9057.795295476913, 'accumulated_submission_time': 8022.563579797745, 'accumulated_eval_time': 1033.7704393863678, 'accumulated_logging_time': 0.5695366859436035}
I0203 15:35:44.464892 139774417409792 logging_writer.py:48] [17183] accumulated_eval_time=1033.770439, accumulated_logging_time=0.569537, accumulated_submission_time=8022.563580, global_step=17183, preemption_count=0, score=8022.563580, test/accuracy=0.363300, test/loss=3.038584, test/num_examples=10000, total_duration=9057.795295, train/accuracy=0.499805, train/loss=2.197912, validation/accuracy=0.461440, validation/loss=2.407949, validation/num_examples=50000
I0203 15:35:51.928749 139774434195200 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.1675488948822021, loss=3.3498191833496094
I0203 15:36:36.076955 139774417409792 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.010764241218567, loss=3.1255977153778076
I0203 15:37:22.812329 139774434195200 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.8856310844421387, loss=4.161524772644043
I0203 15:38:10.291452 139774417409792 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.1000598669052124, loss=3.2690036296844482
I0203 15:38:57.382468 139774434195200 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.2138036489486694, loss=3.2610325813293457
I0203 15:39:44.352237 139774417409792 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.0136739015579224, loss=3.1916162967681885
I0203 15:40:31.354782 139774434195200 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.049108862876892, loss=3.498746871948242
I0203 15:41:18.330547 139774417409792 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0235662460327148, loss=3.22560977935791
I0203 15:42:05.466051 139774434195200 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.066983699798584, loss=3.266573667526245
I0203 15:42:44.654984 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:42:56.695590 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:43:35.589616 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:43:37.197442 139936116377408 submission_runner.py:408] Time since start: 9530.55s, 	Step: 18085, 	{'train/accuracy': 0.5384179353713989, 'train/loss': 1.9806084632873535, 'validation/accuracy': 0.47623997926712036, 'validation/loss': 2.3108084201812744, 'validation/num_examples': 50000, 'test/accuracy': 0.3718000054359436, 'test/loss': 2.9495246410369873, 'test/num_examples': 10000, 'score': 8442.695451498032, 'total_duration': 9530.55030632019, 'accumulated_submission_time': 8442.695451498032, 'accumulated_eval_time': 1086.3128879070282, 'accumulated_logging_time': 0.6030721664428711}
I0203 15:43:37.216736 139774417409792 logging_writer.py:48] [18085] accumulated_eval_time=1086.312888, accumulated_logging_time=0.603072, accumulated_submission_time=8442.695451, global_step=18085, preemption_count=0, score=8442.695451, test/accuracy=0.371800, test/loss=2.949525, test/num_examples=10000, total_duration=9530.550306, train/accuracy=0.538418, train/loss=1.980608, validation/accuracy=0.476240, validation/loss=2.310808, validation/num_examples=50000
I0203 15:43:43.850208 139774434195200 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7718256711959839, loss=5.601016998291016
I0203 15:44:27.444503 139774417409792 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.971406877040863, loss=3.1348507404327393
I0203 15:45:14.135623 139774434195200 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.081531047821045, loss=3.3756422996520996
I0203 15:46:00.765569 139774417409792 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.7644309401512146, loss=5.582800388336182
I0203 15:46:47.793725 139774434195200 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.9744699597358704, loss=3.261355400085449
I0203 15:47:34.722834 139774417409792 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0895863771438599, loss=3.216193437576294
I0203 15:48:21.414517 139774434195200 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.967038631439209, loss=3.3002679347991943
I0203 15:49:08.658507 139774417409792 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.2561850547790527, loss=3.141007423400879
I0203 15:49:55.360972 139774434195200 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.219014286994934, loss=3.143721342086792
I0203 15:50:37.533878 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:50:49.686904 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:51:27.051898 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:51:28.690374 139936116377408 submission_runner.py:408] Time since start: 10002.04s, 	Step: 18992, 	{'train/accuracy': 0.5228710770606995, 'train/loss': 2.079188823699951, 'validation/accuracy': 0.487419992685318, 'validation/loss': 2.272768020629883, 'validation/num_examples': 50000, 'test/accuracy': 0.380700021982193, 'test/loss': 2.9152309894561768, 'test/num_examples': 10000, 'score': 8862.955168247223, 'total_duration': 10002.043234586716, 'accumulated_submission_time': 8862.955168247223, 'accumulated_eval_time': 1137.469367980957, 'accumulated_logging_time': 0.6330897808074951}
I0203 15:51:28.714020 139774417409792 logging_writer.py:48] [18992] accumulated_eval_time=1137.469368, accumulated_logging_time=0.633090, accumulated_submission_time=8862.955168, global_step=18992, preemption_count=0, score=8862.955168, test/accuracy=0.380700, test/loss=2.915231, test/num_examples=10000, total_duration=10002.043235, train/accuracy=0.522871, train/loss=2.079189, validation/accuracy=0.487420, validation/loss=2.272768, validation/num_examples=50000
I0203 15:51:32.447199 139774434195200 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.1161316633224487, loss=3.1336147785186768
I0203 15:52:15.851557 139774417409792 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9458953142166138, loss=4.985527038574219
I0203 15:53:02.382933 139774434195200 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9173319339752197, loss=3.342160224914551
I0203 15:53:49.279685 139774417409792 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0453356504440308, loss=3.145402669906616
I0203 15:54:36.249173 139774434195200 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.8977298736572266, loss=3.611562728881836
I0203 15:55:23.119140 139774417409792 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0482838153839111, loss=3.1093697547912598
I0203 15:56:09.801492 139774434195200 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.1358375549316406, loss=3.27847957611084
I0203 15:56:56.561535 139774417409792 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0416274070739746, loss=3.136512041091919
I0203 15:57:43.559771 139774434195200 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.9959982633590698, loss=3.5061471462249756
I0203 15:58:29.067647 139936116377408 spec.py:321] Evaluating on the training split.
I0203 15:58:40.962141 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 15:59:20.176007 139936116377408 spec.py:349] Evaluating on the test split.
I0203 15:59:21.781387 139936116377408 submission_runner.py:408] Time since start: 10475.13s, 	Step: 19898, 	{'train/accuracy': 0.5240820050239563, 'train/loss': 2.092133045196533, 'validation/accuracy': 0.48131999373435974, 'validation/loss': 2.309021234512329, 'validation/num_examples': 50000, 'test/accuracy': 0.37890002131462097, 'test/loss': 2.931926727294922, 'test/num_examples': 10000, 'score': 9283.24651813507, 'total_duration': 10475.134246349335, 'accumulated_submission_time': 9283.24651813507, 'accumulated_eval_time': 1190.1830968856812, 'accumulated_logging_time': 0.6724350452423096}
I0203 15:59:21.801675 139774417409792 logging_writer.py:48] [19898] accumulated_eval_time=1190.183097, accumulated_logging_time=0.672435, accumulated_submission_time=9283.246518, global_step=19898, preemption_count=0, score=9283.246518, test/accuracy=0.378900, test/loss=2.931927, test/num_examples=10000, total_duration=10475.134246, train/accuracy=0.524082, train/loss=2.092133, validation/accuracy=0.481320, validation/loss=2.309021, validation/num_examples=50000
I0203 15:59:23.057377 139774434195200 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9270892143249512, loss=3.535987377166748
I0203 16:00:06.492658 139774417409792 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1284126043319702, loss=3.1652941703796387
I0203 16:00:53.302928 139774434195200 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0580264329910278, loss=4.266077518463135
I0203 16:01:40.110454 139774417409792 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7565706372261047, loss=5.6171441078186035
I0203 16:02:27.060352 139774434195200 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.863483190536499, loss=3.951552629470825
I0203 16:03:14.052817 139774417409792 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0710996389389038, loss=5.6195969581604
I0203 16:04:00.935170 139774434195200 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8103846907615662, loss=5.603394508361816
I0203 16:04:48.091415 139774417409792 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.067558765411377, loss=3.2406718730926514
I0203 16:05:35.344309 139774434195200 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.080289602279663, loss=3.05007266998291
I0203 16:06:22.335985 139774417409792 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0412254333496094, loss=5.015012741088867
I0203 16:06:22.351499 139936116377408 spec.py:321] Evaluating on the training split.
I0203 16:06:34.501545 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 16:07:12.503638 139936116377408 spec.py:349] Evaluating on the test split.
I0203 16:07:14.116990 139936116377408 submission_runner.py:408] Time since start: 10947.47s, 	Step: 20801, 	{'train/accuracy': 0.5454296469688416, 'train/loss': 1.9585974216461182, 'validation/accuracy': 0.4882199764251709, 'validation/loss': 2.2411868572235107, 'validation/num_examples': 50000, 'test/accuracy': 0.3815000057220459, 'test/loss': 2.893972873687744, 'test/num_examples': 10000, 'score': 9703.740409374237, 'total_duration': 10947.46983218193, 'accumulated_submission_time': 9703.740409374237, 'accumulated_eval_time': 1241.9485597610474, 'accumulated_logging_time': 0.7021317481994629}
I0203 16:07:14.141654 139774434195200 logging_writer.py:48] [20801] accumulated_eval_time=1241.948560, accumulated_logging_time=0.702132, accumulated_submission_time=9703.740409, global_step=20801, preemption_count=0, score=9703.740409, test/accuracy=0.381500, test/loss=2.893973, test/num_examples=10000, total_duration=10947.469832, train/accuracy=0.545430, train/loss=1.958597, validation/accuracy=0.488220, validation/loss=2.241187, validation/num_examples=50000
I0203 16:07:56.990977 139774417409792 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.9739087224006653, loss=4.033918380737305
I0203 16:08:43.355910 139774434195200 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7467525601387024, loss=4.930768013000488
I0203 16:09:30.468496 139774417409792 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.1672946214675903, loss=3.172959089279175
I0203 16:10:17.265887 139774434195200 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.059146523475647, loss=3.292841672897339
I0203 16:11:04.151177 139774417409792 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.7514842748641968, loss=5.6119232177734375
I0203 16:11:50.976832 139774434195200 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8535475730895996, loss=3.8842084407806396
I0203 16:12:37.932846 139774417409792 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7539666295051575, loss=5.4882659912109375
I0203 16:13:24.685329 139774434195200 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7662591338157654, loss=5.460300922393799
I0203 16:14:11.369827 139774417409792 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7290415167808533, loss=5.307365894317627
I0203 16:14:14.261644 139936116377408 spec.py:321] Evaluating on the training split.
I0203 16:14:26.339228 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 16:15:05.255773 139936116377408 spec.py:349] Evaluating on the test split.
I0203 16:15:06.867446 139936116377408 submission_runner.py:408] Time since start: 11420.22s, 	Step: 21708, 	{'train/accuracy': 0.5394921898841858, 'train/loss': 1.981076717376709, 'validation/accuracy': 0.49657997488975525, 'validation/loss': 2.200188636779785, 'validation/num_examples': 50000, 'test/accuracy': 0.3945000171661377, 'test/loss': 2.8344380855560303, 'test/num_examples': 10000, 'score': 10123.801826238632, 'total_duration': 11420.220302343369, 'accumulated_submission_time': 10123.801826238632, 'accumulated_eval_time': 1294.5543503761292, 'accumulated_logging_time': 0.7387468814849854}
I0203 16:15:06.890115 139774434195200 logging_writer.py:48] [21708] accumulated_eval_time=1294.554350, accumulated_logging_time=0.738747, accumulated_submission_time=10123.801826, global_step=21708, preemption_count=0, score=10123.801826, test/accuracy=0.394500, test/loss=2.834438, test/num_examples=10000, total_duration=11420.220302, train/accuracy=0.539492, train/loss=1.981077, validation/accuracy=0.496580, validation/loss=2.200189, validation/num_examples=50000
I0203 16:15:46.726949 139774417409792 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.030466079711914, loss=3.188704490661621
I0203 16:16:33.320159 139774434195200 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.1626834869384766, loss=3.234623908996582
I0203 16:17:20.168860 139774417409792 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.170939564704895, loss=3.1551005840301514
I0203 16:18:07.251287 139774434195200 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.034169316291809, loss=3.2772839069366455
I0203 16:18:54.241127 139774417409792 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9659013152122498, loss=4.219419002532959
I0203 16:19:41.430315 139774434195200 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0993293523788452, loss=3.1292998790740967
I0203 16:20:28.354041 139774417409792 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7346587777137756, loss=5.128505706787109
I0203 16:21:15.470227 139774434195200 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.0206918716430664, loss=3.9038639068603516
I0203 16:22:02.992020 139774417409792 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.9243335723876953, loss=3.613844871520996
I0203 16:22:06.930911 139936116377408 spec.py:321] Evaluating on the training split.
I0203 16:22:18.946487 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 16:22:56.585720 139936116377408 spec.py:349] Evaluating on the test split.
I0203 16:22:58.187963 139936116377408 submission_runner.py:408] Time since start: 11891.54s, 	Step: 22610, 	{'train/accuracy': 0.5367578268051147, 'train/loss': 1.97980535030365, 'validation/accuracy': 0.4986799955368042, 'validation/loss': 2.189040422439575, 'validation/num_examples': 50000, 'test/accuracy': 0.380700021982193, 'test/loss': 2.866044759750366, 'test/num_examples': 10000, 'score': 10543.785840034485, 'total_duration': 11891.540809392929, 'accumulated_submission_time': 10543.785840034485, 'accumulated_eval_time': 1345.811369419098, 'accumulated_logging_time': 0.7716727256774902}
I0203 16:22:58.207727 139774434195200 logging_writer.py:48] [22610] accumulated_eval_time=1345.811369, accumulated_logging_time=0.771673, accumulated_submission_time=10543.785840, global_step=22610, preemption_count=0, score=10543.785840, test/accuracy=0.380700, test/loss=2.866045, test/num_examples=10000, total_duration=11891.540809, train/accuracy=0.536758, train/loss=1.979805, validation/accuracy=0.498680, validation/loss=2.189040, validation/num_examples=50000
I0203 16:23:37.229113 139774417409792 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.1924272775650024, loss=3.124070405960083
I0203 16:24:24.017391 139774434195200 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.9803986549377441, loss=3.72514271736145
I0203 16:25:11.083371 139774417409792 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.1759207248687744, loss=3.384227991104126
I0203 16:25:58.090960 139774434195200 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.127788782119751, loss=3.0959746837615967
I0203 16:26:45.518825 139774417409792 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.1279984712600708, loss=3.0529518127441406
I0203 16:27:32.783368 139774434195200 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.1249438524246216, loss=3.1419432163238525
I0203 16:28:19.777607 139774417409792 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.7821271419525146, loss=4.342467308044434
I0203 16:29:06.755035 139774434195200 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0287755727767944, loss=3.0170557498931885
I0203 16:29:53.969814 139774417409792 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.765959620475769, loss=5.288361549377441
I0203 16:29:58.267461 139936116377408 spec.py:321] Evaluating on the training split.
I0203 16:30:10.292179 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 16:30:48.718553 139936116377408 spec.py:349] Evaluating on the test split.
I0203 16:30:50.329367 139936116377408 submission_runner.py:408] Time since start: 12363.68s, 	Step: 23511, 	{'train/accuracy': 0.5564843416213989, 'train/loss': 1.8949991464614868, 'validation/accuracy': 0.5028799772262573, 'validation/loss': 2.1828277111053467, 'validation/num_examples': 50000, 'test/accuracy': 0.39750000834465027, 'test/loss': 2.843322515487671, 'test/num_examples': 10000, 'score': 10963.787488222122, 'total_duration': 12363.682228326797, 'accumulated_submission_time': 10963.787488222122, 'accumulated_eval_time': 1397.873296737671, 'accumulated_logging_time': 0.8026630878448486}
I0203 16:30:50.350321 139774434195200 logging_writer.py:48] [23511] accumulated_eval_time=1397.873297, accumulated_logging_time=0.802663, accumulated_submission_time=10963.787488, global_step=23511, preemption_count=0, score=10963.787488, test/accuracy=0.397500, test/loss=2.843323, test/num_examples=10000, total_duration=12363.682228, train/accuracy=0.556484, train/loss=1.894999, validation/accuracy=0.502880, validation/loss=2.182828, validation/num_examples=50000
I0203 16:31:28.886915 139774417409792 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.178640365600586, loss=3.761268138885498
I0203 16:32:16.115334 139774434195200 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.127278447151184, loss=2.8796088695526123
I0203 16:33:02.993427 139774417409792 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.097042441368103, loss=3.524172782897949
I0203 16:33:49.826958 139774434195200 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.0837764739990234, loss=3.1783528327941895
I0203 16:34:36.730674 139774417409792 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.8846114277839661, loss=4.304863452911377
I0203 16:35:23.742316 139774434195200 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.115709662437439, loss=3.4045658111572266
I0203 16:36:10.566602 139774417409792 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9625012874603271, loss=3.996574878692627
I0203 16:36:57.682544 139774434195200 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.1013107299804688, loss=3.0820271968841553
I0203 16:37:44.996394 139774417409792 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.1831138134002686, loss=4.940706729888916
I0203 16:37:50.446295 139936116377408 spec.py:321] Evaluating on the training split.
I0203 16:38:02.450101 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 16:38:41.276287 139936116377408 spec.py:349] Evaluating on the test split.
I0203 16:38:42.880013 139936116377408 submission_runner.py:408] Time since start: 12836.23s, 	Step: 24413, 	{'train/accuracy': 0.5447851419448853, 'train/loss': 1.9759751558303833, 'validation/accuracy': 0.5110799670219421, 'validation/loss': 2.1550962924957275, 'validation/num_examples': 50000, 'test/accuracy': 0.39980003237724304, 'test/loss': 2.798171281814575, 'test/num_examples': 10000, 'score': 11383.82677412033, 'total_duration': 12836.232869625092, 'accumulated_submission_time': 11383.82677412033, 'accumulated_eval_time': 1450.3069911003113, 'accumulated_logging_time': 0.8335375785827637}
I0203 16:38:42.901046 139774434195200 logging_writer.py:48] [24413] accumulated_eval_time=1450.306991, accumulated_logging_time=0.833538, accumulated_submission_time=11383.826774, global_step=24413, preemption_count=0, score=11383.826774, test/accuracy=0.399800, test/loss=2.798171, test/num_examples=10000, total_duration=12836.232870, train/accuracy=0.544785, train/loss=1.975975, validation/accuracy=0.511080, validation/loss=2.155096, validation/num_examples=50000
I0203 16:39:20.279957 139774417409792 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0138633251190186, loss=3.1273255348205566
I0203 16:40:07.057257 139774434195200 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1867331266403198, loss=3.125042676925659
I0203 16:40:54.106014 139774417409792 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.9618902206420898, loss=4.443325042724609
I0203 16:41:41.288999 139774434195200 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.8236581087112427, loss=5.459691047668457
I0203 16:42:27.974890 139774417409792 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.9518951773643494, loss=5.2105255126953125
I0203 16:43:15.136051 139774434195200 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8102456331253052, loss=5.610000133514404
I0203 16:44:02.152873 139774417409792 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9796247482299805, loss=3.6606831550598145
I0203 16:44:48.941509 139774434195200 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.9014549851417542, loss=4.364324569702148
I0203 16:45:35.808287 139774417409792 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.9313066601753235, loss=5.225533962249756
I0203 16:45:42.945158 139936116377408 spec.py:321] Evaluating on the training split.
I0203 16:45:54.739224 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 16:46:33.839779 139936116377408 spec.py:349] Evaluating on the test split.
I0203 16:46:35.444295 139936116377408 submission_runner.py:408] Time since start: 13308.80s, 	Step: 25317, 	{'train/accuracy': 0.5570703148841858, 'train/loss': 1.8775259256362915, 'validation/accuracy': 0.5131399631500244, 'validation/loss': 2.1032450199127197, 'validation/num_examples': 50000, 'test/accuracy': 0.4092000126838684, 'test/loss': 2.747539758682251, 'test/num_examples': 10000, 'score': 11803.812463283539, 'total_duration': 13308.797145843506, 'accumulated_submission_time': 11803.812463283539, 'accumulated_eval_time': 1502.8061113357544, 'accumulated_logging_time': 0.8648619651794434}
I0203 16:46:35.465026 139774434195200 logging_writer.py:48] [25317] accumulated_eval_time=1502.806111, accumulated_logging_time=0.864862, accumulated_submission_time=11803.812463, global_step=25317, preemption_count=0, score=11803.812463, test/accuracy=0.409200, test/loss=2.747540, test/num_examples=10000, total_duration=13308.797146, train/accuracy=0.557070, train/loss=1.877526, validation/accuracy=0.513140, validation/loss=2.103245, validation/num_examples=50000
I0203 16:47:11.181430 139774417409792 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.121770977973938, loss=3.054837703704834
I0203 16:47:57.818017 139774434195200 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.0353296995162964, loss=3.0156548023223877
I0203 16:48:45.209665 139774417409792 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8603062629699707, loss=4.5630388259887695
I0203 16:49:32.335490 139774434195200 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.9811885356903076, loss=3.2838284969329834
I0203 16:50:19.424115 139774417409792 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1456378698349, loss=3.0690784454345703
I0203 16:51:06.766780 139774434195200 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1963692903518677, loss=3.3093183040618896
I0203 16:51:54.016615 139774417409792 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.397936463356018, loss=2.9416146278381348
I0203 16:52:40.991799 139774434195200 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.92368483543396, loss=5.529243469238281
I0203 16:53:28.077847 139774417409792 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0804803371429443, loss=3.367161989212036
I0203 16:53:35.593515 139936116377408 spec.py:321] Evaluating on the training split.
I0203 16:53:47.211846 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 16:54:26.953434 139936116377408 spec.py:349] Evaluating on the test split.
I0203 16:54:28.560325 139936116377408 submission_runner.py:408] Time since start: 13781.91s, 	Step: 26218, 	{'train/accuracy': 0.5672265291213989, 'train/loss': 1.8946034908294678, 'validation/accuracy': 0.5156599879264832, 'validation/loss': 2.1460132598876953, 'validation/num_examples': 50000, 'test/accuracy': 0.40300002694129944, 'test/loss': 2.811911106109619, 'test/num_examples': 10000, 'score': 12223.884135246277, 'total_duration': 13781.913187265396, 'accumulated_submission_time': 12223.884135246277, 'accumulated_eval_time': 1555.772926568985, 'accumulated_logging_time': 0.895737886428833}
I0203 16:54:28.582960 139774434195200 logging_writer.py:48] [26218] accumulated_eval_time=1555.772927, accumulated_logging_time=0.895738, accumulated_submission_time=12223.884135, global_step=26218, preemption_count=0, score=12223.884135, test/accuracy=0.403000, test/loss=2.811911, test/num_examples=10000, total_duration=13781.913187, train/accuracy=0.567227, train/loss=1.894603, validation/accuracy=0.515660, validation/loss=2.146013, validation/num_examples=50000
I0203 16:55:03.893647 139774417409792 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.1282974481582642, loss=2.954148530960083
I0203 16:55:50.608001 139774434195200 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.9807190895080566, loss=3.8947317600250244
I0203 16:56:38.150985 139774417409792 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.0798126459121704, loss=2.927079677581787
I0203 16:57:25.704330 139774434195200 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.1089403629302979, loss=2.9596621990203857
I0203 16:58:12.794127 139774417409792 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.1045470237731934, loss=3.062709093093872
I0203 16:58:59.958003 139774434195200 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.8055260181427002, loss=5.184329986572266
I0203 16:59:47.364889 139774417409792 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.9060673117637634, loss=4.516704559326172
I0203 17:00:34.752856 139774434195200 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.150820255279541, loss=2.8619582653045654
I0203 17:01:21.724272 139774417409792 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1246166229248047, loss=3.3392016887664795
I0203 17:01:28.612226 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:01:40.845932 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:02:17.952922 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:02:19.583793 139936116377408 submission_runner.py:408] Time since start: 14252.94s, 	Step: 27116, 	{'train/accuracy': 0.5529491901397705, 'train/loss': 1.901785969734192, 'validation/accuracy': 0.5184800028800964, 'validation/loss': 2.0907881259918213, 'validation/num_examples': 50000, 'test/accuracy': 0.4043000340461731, 'test/loss': 2.7777791023254395, 'test/num_examples': 10000, 'score': 12643.856747865677, 'total_duration': 14252.936617612839, 'accumulated_submission_time': 12643.856747865677, 'accumulated_eval_time': 1606.7444801330566, 'accumulated_logging_time': 0.9290673732757568}
I0203 17:02:19.607297 139774434195200 logging_writer.py:48] [27116] accumulated_eval_time=1606.744480, accumulated_logging_time=0.929067, accumulated_submission_time=12643.856748, global_step=27116, preemption_count=0, score=12643.856748, test/accuracy=0.404300, test/loss=2.777779, test/num_examples=10000, total_duration=14252.936618, train/accuracy=0.552949, train/loss=1.901786, validation/accuracy=0.518480, validation/loss=2.090788, validation/num_examples=50000
I0203 17:02:55.756313 139774417409792 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1220349073410034, loss=3.1462645530700684
I0203 17:03:42.649568 139774434195200 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.1277861595153809, loss=2.750408411026001
I0203 17:04:29.912038 139774417409792 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9111349582672119, loss=4.620497226715088
I0203 17:05:17.035501 139774434195200 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0143393278121948, loss=3.163569688796997
I0203 17:06:04.378474 139774417409792 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8955669403076172, loss=5.41094970703125
I0203 17:06:51.389816 139774434195200 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.7987620234489441, loss=4.4875030517578125
I0203 17:07:38.801639 139774417409792 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.860592782497406, loss=5.496652603149414
I0203 17:08:25.749735 139774434195200 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.254900336265564, loss=3.0577306747436523
I0203 17:09:13.147113 139774417409792 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1522425413131714, loss=2.9023759365081787
I0203 17:09:19.827100 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:09:31.749150 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:10:10.297553 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:10:11.904844 139936116377408 submission_runner.py:408] Time since start: 14725.26s, 	Step: 28016, 	{'train/accuracy': 0.5609570145606995, 'train/loss': 1.945162057876587, 'validation/accuracy': 0.5189399719238281, 'validation/loss': 2.1581993103027344, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.7968037128448486, 'test/num_examples': 10000, 'score': 13064.020081281662, 'total_duration': 14725.257707118988, 'accumulated_submission_time': 13064.020081281662, 'accumulated_eval_time': 1658.8222217559814, 'accumulated_logging_time': 0.9629182815551758}
I0203 17:10:11.926898 139774434195200 logging_writer.py:48] [28016] accumulated_eval_time=1658.822222, accumulated_logging_time=0.962918, accumulated_submission_time=13064.020081, global_step=28016, preemption_count=0, score=13064.020081, test/accuracy=0.410400, test/loss=2.796804, test/num_examples=10000, total_duration=14725.257707, train/accuracy=0.560957, train/loss=1.945162, validation/accuracy=0.518940, validation/loss=2.158199, validation/num_examples=50000
I0203 17:10:48.053798 139774417409792 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.994685709476471, loss=3.064619302749634
I0203 17:11:34.478239 139774434195200 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.9976773858070374, loss=3.987708806991577
I0203 17:12:21.605656 139774417409792 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.2510515451431274, loss=3.0680503845214844
I0203 17:13:08.512783 139774434195200 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.042256474494934, loss=2.858689069747925
I0203 17:13:55.331652 139774417409792 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1854592561721802, loss=3.5155692100524902
I0203 17:14:42.520790 139774434195200 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.0108097791671753, loss=3.149993896484375
I0203 17:15:29.824420 139774417409792 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.0643244981765747, loss=4.471057891845703
I0203 17:16:16.805575 139774434195200 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.101119041442871, loss=4.762572288513184
I0203 17:17:03.517866 139774417409792 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.0024149417877197, loss=3.5291872024536133
I0203 17:17:12.059227 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:17:23.979959 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:18:02.409671 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:18:04.015173 139936116377408 submission_runner.py:408] Time since start: 15197.37s, 	Step: 28920, 	{'train/accuracy': 0.5832226276397705, 'train/loss': 1.7956725358963013, 'validation/accuracy': 0.5298199653625488, 'validation/loss': 2.0622482299804688, 'validation/num_examples': 50000, 'test/accuracy': 0.41280001401901245, 'test/loss': 2.7176918983459473, 'test/num_examples': 10000, 'score': 13484.096655845642, 'total_duration': 15197.368015289307, 'accumulated_submission_time': 13484.096655845642, 'accumulated_eval_time': 1710.7781717777252, 'accumulated_logging_time': 0.99446702003479}
I0203 17:18:04.038396 139774434195200 logging_writer.py:48] [28920] accumulated_eval_time=1710.778172, accumulated_logging_time=0.994467, accumulated_submission_time=13484.096656, global_step=28920, preemption_count=0, score=13484.096656, test/accuracy=0.412800, test/loss=2.717692, test/num_examples=10000, total_duration=15197.368015, train/accuracy=0.583223, train/loss=1.795673, validation/accuracy=0.529820, validation/loss=2.062248, validation/num_examples=50000
I0203 17:18:38.523777 139774417409792 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.13820219039917, loss=3.020765781402588
I0203 17:19:25.265424 139774434195200 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.8870617747306824, loss=4.525935173034668
I0203 17:20:12.415288 139774417409792 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.8265730738639832, loss=3.5371928215026855
I0203 17:20:59.062056 139774434195200 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0846590995788574, loss=3.215165615081787
I0203 17:21:46.242408 139774417409792 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.022497534751892, loss=3.080461025238037
I0203 17:22:33.489912 139774434195200 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.0413124561309814, loss=3.060790538787842
I0203 17:23:20.703323 139774417409792 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.1686877012252808, loss=2.9258227348327637
I0203 17:24:08.087906 139774434195200 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.1737825870513916, loss=2.870755434036255
I0203 17:24:55.220484 139774417409792 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.0317429304122925, loss=2.861689329147339
I0203 17:25:04.334691 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:25:16.134603 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:25:56.291425 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:25:57.894181 139936116377408 submission_runner.py:408] Time since start: 15671.25s, 	Step: 29821, 	{'train/accuracy': 0.5748437643051147, 'train/loss': 1.8023862838745117, 'validation/accuracy': 0.538159966468811, 'validation/loss': 1.9932000637054443, 'validation/num_examples': 50000, 'test/accuracy': 0.4171000123023987, 'test/loss': 2.6709110736846924, 'test/num_examples': 10000, 'score': 13904.335749864578, 'total_duration': 15671.247034311295, 'accumulated_submission_time': 13904.335749864578, 'accumulated_eval_time': 1764.337646484375, 'accumulated_logging_time': 1.0288941860198975}
I0203 17:25:57.918746 139774434195200 logging_writer.py:48] [29821] accumulated_eval_time=1764.337646, accumulated_logging_time=1.028894, accumulated_submission_time=13904.335750, global_step=29821, preemption_count=0, score=13904.335750, test/accuracy=0.417100, test/loss=2.670911, test/num_examples=10000, total_duration=15671.247034, train/accuracy=0.574844, train/loss=1.802386, validation/accuracy=0.538160, validation/loss=1.993200, validation/num_examples=50000
I0203 17:26:31.982170 139774417409792 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.7931921482086182, loss=2.931973934173584
I0203 17:27:19.002457 139774434195200 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.1844608783721924, loss=2.846204996109009
I0203 17:28:06.394265 139774417409792 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0082248449325562, loss=3.4214329719543457
I0203 17:28:53.358362 139774434195200 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.8482360243797302, loss=3.901620864868164
I0203 17:29:40.718113 139774417409792 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0831630229949951, loss=2.9320077896118164
I0203 17:30:28.017889 139774434195200 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.0963761806488037, loss=5.184202671051025
I0203 17:31:15.375319 139774417409792 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0819052457809448, loss=3.404047727584839
I0203 17:32:02.782772 139774434195200 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.101185917854309, loss=2.904689311981201
I0203 17:32:49.747250 139774417409792 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.032845377922058, loss=3.684964418411255
I0203 17:32:57.953214 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:33:09.980537 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:33:49.692955 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:33:51.293489 139936116377408 submission_runner.py:408] Time since start: 16144.65s, 	Step: 30719, 	{'train/accuracy': 0.5781054496765137, 'train/loss': 1.7854104042053223, 'validation/accuracy': 0.5322200059890747, 'validation/loss': 2.0117881298065186, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.6758742332458496, 'test/num_examples': 10000, 'score': 14324.31052160263, 'total_duration': 16144.646352529526, 'accumulated_submission_time': 14324.31052160263, 'accumulated_eval_time': 1817.6779038906097, 'accumulated_logging_time': 1.0666768550872803}
I0203 17:33:51.320547 139774434195200 logging_writer.py:48] [30719] accumulated_eval_time=1817.677904, accumulated_logging_time=1.066677, accumulated_submission_time=14324.310522, global_step=30719, preemption_count=0, score=14324.310522, test/accuracy=0.417000, test/loss=2.675874, test/num_examples=10000, total_duration=16144.646353, train/accuracy=0.578105, train/loss=1.785410, validation/accuracy=0.532220, validation/loss=2.011788, validation/num_examples=50000
I0203 17:34:26.268449 139774417409792 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.8245394229888916, loss=4.841975688934326
I0203 17:35:12.460340 139774434195200 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0194729566574097, loss=3.913994073867798
I0203 17:35:59.468957 139774417409792 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.2618615627288818, loss=2.8070333003997803
I0203 17:36:46.468365 139774434195200 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.068633794784546, loss=2.8696701526641846
I0203 17:37:33.565485 139774417409792 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9973700642585754, loss=4.577739715576172
I0203 17:38:20.923599 139774434195200 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0330380201339722, loss=4.363844871520996
I0203 17:39:07.765344 139774417409792 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.1689995527267456, loss=2.811573028564453
I0203 17:39:54.802903 139774434195200 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.9664734601974487, loss=3.508054494857788
I0203 17:40:41.986426 139774417409792 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.042083501815796, loss=2.8448257446289062
I0203 17:40:51.528366 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:41:03.821560 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:41:42.658846 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:41:44.262636 139936116377408 submission_runner.py:408] Time since start: 16617.62s, 	Step: 31622, 	{'train/accuracy': 0.5849804282188416, 'train/loss': 1.757678747177124, 'validation/accuracy': 0.5351200103759766, 'validation/loss': 2.018251895904541, 'validation/num_examples': 50000, 'test/accuracy': 0.42000001668930054, 'test/loss': 2.6826159954071045, 'test/num_examples': 10000, 'score': 14744.462392568588, 'total_duration': 16617.61550116539, 'accumulated_submission_time': 14744.462392568588, 'accumulated_eval_time': 1870.412175655365, 'accumulated_logging_time': 1.103344440460205}
I0203 17:41:44.292685 139774434195200 logging_writer.py:48] [31622] accumulated_eval_time=1870.412176, accumulated_logging_time=1.103344, accumulated_submission_time=14744.462393, global_step=31622, preemption_count=0, score=14744.462393, test/accuracy=0.420000, test/loss=2.682616, test/num_examples=10000, total_duration=16617.615501, train/accuracy=0.584980, train/loss=1.757679, validation/accuracy=0.535120, validation/loss=2.018252, validation/num_examples=50000
I0203 17:42:17.752357 139774417409792 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1343464851379395, loss=3.129180431365967
I0203 17:43:04.311922 139774434195200 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.0556237697601318, loss=3.0482709407806396
I0203 17:43:51.487696 139774417409792 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.2688605785369873, loss=2.8821635246276855
I0203 17:44:38.608798 139774434195200 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.0828218460083008, loss=2.9314801692962646
I0203 17:45:25.571306 139774417409792 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.1850439310073853, loss=5.482755661010742
I0203 17:46:12.748069 139774434195200 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.935039758682251, loss=5.441985130310059
I0203 17:47:00.045245 139774417409792 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0320854187011719, loss=3.174764394760132
I0203 17:47:46.771009 139774434195200 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.1927608251571655, loss=3.0184743404388428
I0203 17:48:34.006398 139774417409792 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.8422388434410095, loss=5.3877482414245605
I0203 17:48:44.358773 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:48:56.091672 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:49:35.858966 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:49:37.463234 139936116377408 submission_runner.py:408] Time since start: 17090.82s, 	Step: 32523, 	{'train/accuracy': 0.5832421779632568, 'train/loss': 1.8074359893798828, 'validation/accuracy': 0.5415599942207336, 'validation/loss': 2.0093460083007812, 'validation/num_examples': 50000, 'test/accuracy': 0.4262000322341919, 'test/loss': 2.6587982177734375, 'test/num_examples': 10000, 'score': 15164.471905231476, 'total_duration': 17090.816098451614, 'accumulated_submission_time': 15164.471905231476, 'accumulated_eval_time': 1923.5166292190552, 'accumulated_logging_time': 1.1434593200683594}
I0203 17:49:37.487583 139774434195200 logging_writer.py:48] [32523] accumulated_eval_time=1923.516629, accumulated_logging_time=1.143459, accumulated_submission_time=15164.471905, global_step=32523, preemption_count=0, score=15164.471905, test/accuracy=0.426200, test/loss=2.658798, test/num_examples=10000, total_duration=17090.816098, train/accuracy=0.583242, train/loss=1.807436, validation/accuracy=0.541560, validation/loss=2.009346, validation/num_examples=50000
I0203 17:50:10.383853 139774417409792 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.1222118139266968, loss=2.857003688812256
I0203 17:50:56.625594 139774434195200 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1572076082229614, loss=2.799968719482422
I0203 17:51:43.812033 139774417409792 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0390921831130981, loss=2.780625581741333
I0203 17:52:30.702577 139774434195200 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.079820990562439, loss=2.8286633491516113
I0203 17:53:17.859786 139774417409792 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.9547324776649475, loss=4.265259265899658
I0203 17:54:04.971606 139774434195200 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.1218316555023193, loss=2.733696937561035
I0203 17:54:51.747213 139774417409792 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.2514725923538208, loss=2.9064459800720215
I0203 17:55:38.898391 139774434195200 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1685442924499512, loss=2.838942050933838
I0203 17:56:25.750255 139774417409792 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8752508759498596, loss=4.412986755371094
I0203 17:56:37.578465 139936116377408 spec.py:321] Evaluating on the training split.
I0203 17:56:49.328179 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 17:57:28.214751 139936116377408 spec.py:349] Evaluating on the test split.
I0203 17:57:29.815628 139936116377408 submission_runner.py:408] Time since start: 17563.17s, 	Step: 33427, 	{'train/accuracy': 0.583300769329071, 'train/loss': 1.8000158071517944, 'validation/accuracy': 0.5366399884223938, 'validation/loss': 2.0200142860412598, 'validation/num_examples': 50000, 'test/accuracy': 0.4164000153541565, 'test/loss': 2.6936354637145996, 'test/num_examples': 10000, 'score': 15584.505164146423, 'total_duration': 17563.168486595154, 'accumulated_submission_time': 15584.505164146423, 'accumulated_eval_time': 1975.7538046836853, 'accumulated_logging_time': 1.178342580795288}
I0203 17:57:29.847861 139774434195200 logging_writer.py:48] [33427] accumulated_eval_time=1975.753805, accumulated_logging_time=1.178343, accumulated_submission_time=15584.505164, global_step=33427, preemption_count=0, score=15584.505164, test/accuracy=0.416400, test/loss=2.693635, test/num_examples=10000, total_duration=17563.168487, train/accuracy=0.583301, train/loss=1.800016, validation/accuracy=0.536640, validation/loss=2.020014, validation/num_examples=50000
I0203 17:58:01.065864 139774417409792 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7840974926948547, loss=4.623844623565674
I0203 17:58:47.559644 139774434195200 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.1370702981948853, loss=2.880239248275757
I0203 17:59:34.806959 139774417409792 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.087825894355774, loss=2.9004948139190674
I0203 18:00:22.031053 139774434195200 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1006457805633545, loss=2.995560646057129
I0203 18:01:08.783814 139774417409792 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.1712863445281982, loss=2.8824751377105713
I0203 18:01:55.945737 139774434195200 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.1631104946136475, loss=2.9256837368011475
I0203 18:02:43.041103 139774417409792 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.2243517637252808, loss=2.8398354053497314
I0203 18:03:30.293970 139774434195200 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.2433724403381348, loss=3.032735824584961
I0203 18:04:17.542939 139774417409792 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0241318941116333, loss=5.323407173156738
I0203 18:04:29.918374 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:04:41.770200 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 18:05:19.694112 139936116377408 spec.py:349] Evaluating on the test split.
I0203 18:05:21.302177 139936116377408 submission_runner.py:408] Time since start: 18034.66s, 	Step: 34328, 	{'train/accuracy': 0.6021679639816284, 'train/loss': 1.6713885068893433, 'validation/accuracy': 0.5493800044059753, 'validation/loss': 1.9222406148910522, 'validation/num_examples': 50000, 'test/accuracy': 0.43300002813339233, 'test/loss': 2.5797040462493896, 'test/num_examples': 10000, 'score': 16004.51957321167, 'total_duration': 18034.655037164688, 'accumulated_submission_time': 16004.51957321167, 'accumulated_eval_time': 2027.1375963687897, 'accumulated_logging_time': 1.2206003665924072}
I0203 18:05:21.330390 139774434195200 logging_writer.py:48] [34328] accumulated_eval_time=2027.137596, accumulated_logging_time=1.220600, accumulated_submission_time=16004.519573, global_step=34328, preemption_count=0, score=16004.519573, test/accuracy=0.433000, test/loss=2.579704, test/num_examples=10000, total_duration=18034.655037, train/accuracy=0.602168, train/loss=1.671389, validation/accuracy=0.549380, validation/loss=1.922241, validation/num_examples=50000
I0203 18:05:51.973815 139774417409792 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.048852562904358, loss=2.923872947692871
I0203 18:06:38.992569 139774434195200 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.230769395828247, loss=2.894820213317871
I0203 18:07:26.188917 139774417409792 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.1361515522003174, loss=2.6936748027801514
I0203 18:08:13.112543 139774434195200 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.1188232898712158, loss=2.7458786964416504
I0203 18:09:00.107618 139774417409792 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8543444275856018, loss=4.197950839996338
I0203 18:09:47.631075 139774434195200 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.9840556979179382, loss=4.200085639953613
I0203 18:10:34.851193 139774417409792 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.9227489233016968, loss=5.305793285369873
I0203 18:11:22.540093 139774434195200 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.9593927264213562, loss=4.949767112731934
I0203 18:12:10.265968 139774417409792 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.047631859779358, loss=2.7906270027160645
I0203 18:12:21.383452 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:12:33.502402 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 18:13:09.960855 139936116377408 spec.py:349] Evaluating on the test split.
I0203 18:13:11.568224 139936116377408 submission_runner.py:408] Time since start: 18504.92s, 	Step: 35225, 	{'train/accuracy': 0.6086132526397705, 'train/loss': 1.6429468393325806, 'validation/accuracy': 0.5566200017929077, 'validation/loss': 1.9106091260910034, 'validation/num_examples': 50000, 'test/accuracy': 0.44130003452301025, 'test/loss': 2.5836009979248047, 'test/num_examples': 10000, 'score': 16424.51560664177, 'total_duration': 18504.921072244644, 'accumulated_submission_time': 16424.51560664177, 'accumulated_eval_time': 2077.322345972061, 'accumulated_logging_time': 1.2598974704742432}
I0203 18:13:11.593614 139774434195200 logging_writer.py:48] [35225] accumulated_eval_time=2077.322346, accumulated_logging_time=1.259897, accumulated_submission_time=16424.515607, global_step=35225, preemption_count=0, score=16424.515607, test/accuracy=0.441300, test/loss=2.583601, test/num_examples=10000, total_duration=18504.921072, train/accuracy=0.608613, train/loss=1.642947, validation/accuracy=0.556620, validation/loss=1.910609, validation/num_examples=50000
I0203 18:13:43.531330 139774417409792 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.0084574222564697, loss=3.2868642807006836
I0203 18:14:30.402127 139774434195200 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.0580374002456665, loss=2.6822948455810547
I0203 18:15:17.489098 139774417409792 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.2267259359359741, loss=2.820410966873169
I0203 18:16:04.502444 139774434195200 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.0300769805908203, loss=4.852878093719482
I0203 18:16:51.791733 139774417409792 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.2005330324172974, loss=2.77402400970459
I0203 18:17:39.024314 139774434195200 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.9458051919937134, loss=5.371456623077393
I0203 18:18:26.284767 139774417409792 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.1453890800476074, loss=2.895552158355713
I0203 18:19:13.265965 139774434195200 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1667143106460571, loss=2.7246696949005127
I0203 18:20:00.660030 139774417409792 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.0185534954071045, loss=2.7431106567382812
I0203 18:20:11.756810 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:20:23.875730 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 18:21:01.314650 139936116377408 spec.py:349] Evaluating on the test split.
I0203 18:21:02.926641 139936116377408 submission_runner.py:408] Time since start: 18976.28s, 	Step: 36125, 	{'train/accuracy': 0.5963281393051147, 'train/loss': 1.712787389755249, 'validation/accuracy': 0.5558199882507324, 'validation/loss': 1.9079389572143555, 'validation/num_examples': 50000, 'test/accuracy': 0.4359000325202942, 'test/loss': 2.588874578475952, 'test/num_examples': 10000, 'score': 16844.621319770813, 'total_duration': 18976.279500722885, 'accumulated_submission_time': 16844.621319770813, 'accumulated_eval_time': 2128.4921691417694, 'accumulated_logging_time': 1.2963852882385254}
I0203 18:21:02.949498 139774434195200 logging_writer.py:48] [36125] accumulated_eval_time=2128.492169, accumulated_logging_time=1.296385, accumulated_submission_time=16844.621320, global_step=36125, preemption_count=0, score=16844.621320, test/accuracy=0.435900, test/loss=2.588875, test/num_examples=10000, total_duration=18976.279501, train/accuracy=0.596328, train/loss=1.712787, validation/accuracy=0.555820, validation/loss=1.907939, validation/num_examples=50000
I0203 18:21:34.901150 139774417409792 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.09608793258667, loss=2.7673912048339844
I0203 18:22:21.625184 139774434195200 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.0738765001296997, loss=2.778563976287842
I0203 18:23:08.888832 139774417409792 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.9877406358718872, loss=3.1373889446258545
I0203 18:23:55.710678 139774434195200 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0217899084091187, loss=3.9904181957244873
I0203 18:24:42.957840 139774417409792 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0973131656646729, loss=2.7587547302246094
I0203 18:25:29.870700 139774434195200 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.1353073120117188, loss=2.741567373275757
I0203 18:26:16.739965 139774417409792 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.238661766052246, loss=2.839585781097412
I0203 18:27:04.078336 139774434195200 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.9000440835952759, loss=4.764019966125488
I0203 18:27:50.701906 139774417409792 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.127333164215088, loss=2.8709468841552734
I0203 18:28:03.090501 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:28:15.055172 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 18:28:52.095077 139936116377408 spec.py:349] Evaluating on the test split.
I0203 18:28:53.706695 139936116377408 submission_runner.py:408] Time since start: 19447.06s, 	Step: 37028, 	{'train/accuracy': 0.6057812571525574, 'train/loss': 1.6403157711029053, 'validation/accuracy': 0.5555599927902222, 'validation/loss': 1.885812759399414, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.56365966796875, 'test/num_examples': 10000, 'score': 17264.70506668091, 'total_duration': 19447.05951809883, 'accumulated_submission_time': 17264.70506668091, 'accumulated_eval_time': 2179.108320236206, 'accumulated_logging_time': 1.329272985458374}
I0203 18:28:53.735605 139774434195200 logging_writer.py:48] [37028] accumulated_eval_time=2179.108320, accumulated_logging_time=1.329273, accumulated_submission_time=17264.705067, global_step=37028, preemption_count=0, score=17264.705067, test/accuracy=0.441000, test/loss=2.563660, test/num_examples=10000, total_duration=19447.059518, train/accuracy=0.605781, train/loss=1.640316, validation/accuracy=0.555560, validation/loss=1.885813, validation/num_examples=50000
I0203 18:29:24.288214 139774417409792 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.3172450065612793, loss=2.8025403022766113
I0203 18:30:10.936591 139774434195200 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.0620604753494263, loss=3.2556838989257812
I0203 18:30:58.095908 139774417409792 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.1526131629943848, loss=2.567214250564575
I0203 18:31:45.187867 139774434195200 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.192604422569275, loss=2.623678684234619
I0203 18:32:32.244293 139774417409792 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.201428771018982, loss=2.9605484008789062
I0203 18:33:19.635297 139774434195200 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1723772287368774, loss=2.743025779724121
I0203 18:34:06.382607 139774417409792 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.2173724174499512, loss=3.1419143676757812
I0203 18:34:53.288907 139774434195200 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.9143555760383606, loss=4.177885055541992
I0203 18:35:40.430987 139774417409792 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.9465073347091675, loss=4.337063789367676
I0203 18:35:53.739618 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:36:06.006574 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 18:36:45.180984 139936116377408 spec.py:349] Evaluating on the test split.
I0203 18:36:46.781133 139936116377408 submission_runner.py:408] Time since start: 19920.13s, 	Step: 37930, 	{'train/accuracy': 0.6322460770606995, 'train/loss': 1.546537160873413, 'validation/accuracy': 0.5542399883270264, 'validation/loss': 1.9083751440048218, 'validation/num_examples': 50000, 'test/accuracy': 0.43630000948905945, 'test/loss': 2.5774965286254883, 'test/num_examples': 10000, 'score': 17684.651047706604, 'total_duration': 19920.133969783783, 'accumulated_submission_time': 17684.651047706604, 'accumulated_eval_time': 2232.1497917175293, 'accumulated_logging_time': 1.3694918155670166}
I0203 18:36:46.807140 139774434195200 logging_writer.py:48] [37930] accumulated_eval_time=2232.149792, accumulated_logging_time=1.369492, accumulated_submission_time=17684.651048, global_step=37930, preemption_count=0, score=17684.651048, test/accuracy=0.436300, test/loss=2.577497, test/num_examples=10000, total_duration=19920.133970, train/accuracy=0.632246, train/loss=1.546537, validation/accuracy=0.554240, validation/loss=1.908375, validation/num_examples=50000
I0203 18:37:16.444890 139774417409792 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.1685024499893188, loss=2.7343692779541016
I0203 18:38:03.310811 139774434195200 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.9386280179023743, loss=5.381082057952881
I0203 18:38:50.396858 139774417409792 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.00991952419281, loss=3.5367238521575928
I0203 18:39:37.302228 139774434195200 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.1535781621932983, loss=2.799278974533081
I0203 18:40:24.683344 139774417409792 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.9413564205169678, loss=3.9000821113586426
I0203 18:41:11.852973 139774434195200 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.236295223236084, loss=2.7104740142822266
I0203 18:41:59.140885 139774417409792 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.19497549533844, loss=2.8174057006835938
I0203 18:42:46.124820 139774434195200 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.8731194138526917, loss=4.041240692138672
I0203 18:43:33.303514 139774417409792 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.1526007652282715, loss=2.730851650238037
I0203 18:43:47.082688 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:43:58.885281 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 18:44:38.317765 139936116377408 spec.py:349] Evaluating on the test split.
I0203 18:44:39.918054 139936116377408 submission_runner.py:408] Time since start: 20393.27s, 	Step: 38831, 	{'train/accuracy': 0.5987695455551147, 'train/loss': 1.698283076286316, 'validation/accuracy': 0.5547999739646912, 'validation/loss': 1.9160023927688599, 'validation/num_examples': 50000, 'test/accuracy': 0.42990002036094666, 'test/loss': 2.6002378463745117, 'test/num_examples': 10000, 'score': 18104.869975566864, 'total_duration': 20393.270917892456, 'accumulated_submission_time': 18104.869975566864, 'accumulated_eval_time': 2284.9851546287537, 'accumulated_logging_time': 1.4052977561950684}
I0203 18:44:39.946343 139774434195200 logging_writer.py:48] [38831] accumulated_eval_time=2284.985155, accumulated_logging_time=1.405298, accumulated_submission_time=18104.869976, global_step=38831, preemption_count=0, score=18104.869976, test/accuracy=0.429900, test/loss=2.600238, test/num_examples=10000, total_duration=20393.270918, train/accuracy=0.598770, train/loss=1.698283, validation/accuracy=0.554800, validation/loss=1.916002, validation/num_examples=50000
I0203 18:45:09.287526 139774417409792 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2709877490997314, loss=2.6602675914764404
I0203 18:45:55.839943 139774434195200 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.042088270187378, loss=3.363497734069824
I0203 18:46:43.439718 139774417409792 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.3801379203796387, loss=2.9316349029541016
I0203 18:47:30.696490 139774434195200 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.0740453004837036, loss=3.2818634510040283
I0203 18:48:18.079701 139774417409792 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.920586884021759, loss=3.8511714935302734
I0203 18:49:05.371196 139774434195200 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9259073138237, loss=3.7547097206115723
I0203 18:49:52.785407 139774417409792 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.1588040590286255, loss=2.8317205905914307
I0203 18:50:40.269296 139774434195200 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.3437564373016357, loss=2.74859356880188
I0203 18:51:27.418385 139774417409792 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.1812598705291748, loss=2.987734317779541
I0203 18:51:39.966818 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:51:51.958359 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 18:52:42.394740 139936116377408 spec.py:349] Evaluating on the test split.
I0203 18:52:44.012370 139936116377408 submission_runner.py:408] Time since start: 20877.37s, 	Step: 39728, 	{'train/accuracy': 0.609570324420929, 'train/loss': 1.6623051166534424, 'validation/accuracy': 0.5617200136184692, 'validation/loss': 1.9099894762039185, 'validation/num_examples': 50000, 'test/accuracy': 0.4417000114917755, 'test/loss': 2.5730700492858887, 'test/num_examples': 10000, 'score': 18524.83588886261, 'total_duration': 20877.36520934105, 'accumulated_submission_time': 18524.83588886261, 'accumulated_eval_time': 2349.030675649643, 'accumulated_logging_time': 1.4428019523620605}
I0203 18:52:44.047179 139774434195200 logging_writer.py:48] [39728] accumulated_eval_time=2349.030676, accumulated_logging_time=1.442802, accumulated_submission_time=18524.835889, global_step=39728, preemption_count=0, score=18524.835889, test/accuracy=0.441700, test/loss=2.573070, test/num_examples=10000, total_duration=20877.365209, train/accuracy=0.609570, train/loss=1.662305, validation/accuracy=0.561720, validation/loss=1.909989, validation/num_examples=50000
I0203 18:53:14.772282 139774417409792 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.222625494003296, loss=2.637303590774536
I0203 18:54:01.316982 139774434195200 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.1806732416152954, loss=2.8146963119506836
I0203 18:54:48.197569 139774417409792 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9022989869117737, loss=4.783831596374512
I0203 18:55:35.739411 139774434195200 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.3211711645126343, loss=2.792802095413208
I0203 18:56:23.537290 139774417409792 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.1962822675704956, loss=2.6789815425872803
I0203 18:57:11.350732 139774434195200 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.090693712234497, loss=2.737417697906494
I0203 18:57:59.605756 139774417409792 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.1244041919708252, loss=2.6885128021240234
I0203 18:58:46.961652 139774434195200 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.3882867097854614, loss=2.7501535415649414
I0203 18:59:34.570796 139774417409792 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.9734297394752502, loss=3.664283275604248
I0203 18:59:44.025635 139936116377408 spec.py:321] Evaluating on the training split.
I0203 18:59:56.132000 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:00:33.954643 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:00:35.567943 139936116377408 submission_runner.py:408] Time since start: 21348.92s, 	Step: 40622, 	{'train/accuracy': 0.6304491758346558, 'train/loss': 1.5252255201339722, 'validation/accuracy': 0.5624600052833557, 'validation/loss': 1.8603116273880005, 'validation/num_examples': 50000, 'test/accuracy': 0.4377000331878662, 'test/loss': 2.548011302947998, 'test/num_examples': 10000, 'score': 18944.7302005291, 'total_duration': 21348.91973233223, 'accumulated_submission_time': 18944.7302005291, 'accumulated_eval_time': 2400.5719459056854, 'accumulated_logging_time': 1.515040397644043}
I0203 19:00:35.591011 139774434195200 logging_writer.py:48] [40622] accumulated_eval_time=2400.571946, accumulated_logging_time=1.515040, accumulated_submission_time=18944.730201, global_step=40622, preemption_count=0, score=18944.730201, test/accuracy=0.437700, test/loss=2.548011, test/num_examples=10000, total_duration=21348.919732, train/accuracy=0.630449, train/loss=1.525226, validation/accuracy=0.562460, validation/loss=1.860312, validation/num_examples=50000
I0203 19:01:09.206939 139774417409792 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.0987027883529663, loss=2.7785089015960693
I0203 19:01:56.030848 139774434195200 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.0407121181488037, loss=2.9756834506988525
I0203 19:02:43.339148 139774417409792 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.877493143081665, loss=5.244214057922363
I0203 19:03:30.463710 139774434195200 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.125604271888733, loss=3.319931745529175
I0203 19:04:17.913450 139774417409792 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.207818865776062, loss=2.861734390258789
I0203 19:05:05.301995 139774434195200 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.188896656036377, loss=2.684197425842285
I0203 19:05:52.734203 139774417409792 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.1889193058013916, loss=3.177332878112793
I0203 19:06:40.315843 139774434195200 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.088054895401001, loss=2.5743885040283203
I0203 19:07:27.760668 139774417409792 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9258795380592346, loss=4.56849479675293
I0203 19:07:36.009282 139936116377408 spec.py:321] Evaluating on the training split.
I0203 19:07:47.894456 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:08:27.494131 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:08:29.112513 139936116377408 submission_runner.py:408] Time since start: 21822.46s, 	Step: 41519, 	{'train/accuracy': 0.6089648008346558, 'train/loss': 1.633178949356079, 'validation/accuracy': 0.568839967250824, 'validation/loss': 1.8405061960220337, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.518946886062622, 'test/num_examples': 10000, 'score': 19365.09273838997, 'total_duration': 21822.46431851387, 'accumulated_submission_time': 19365.09273838997, 'accumulated_eval_time': 2453.674109697342, 'accumulated_logging_time': 1.5481061935424805}
I0203 19:08:29.137810 139774434195200 logging_writer.py:48] [41519] accumulated_eval_time=2453.674110, accumulated_logging_time=1.548106, accumulated_submission_time=19365.092738, global_step=41519, preemption_count=0, score=19365.092738, test/accuracy=0.442200, test/loss=2.518947, test/num_examples=10000, total_duration=21822.464319, train/accuracy=0.608965, train/loss=1.633179, validation/accuracy=0.568840, validation/loss=1.840506, validation/num_examples=50000
I0203 19:09:03.880393 139774417409792 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.937423050403595, loss=4.743252754211426
I0203 19:09:50.310022 139774434195200 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.920098066329956, loss=5.333065509796143
I0203 19:10:37.411413 139774417409792 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.2165452241897583, loss=2.596038818359375
I0203 19:11:24.267870 139774434195200 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9014792442321777, loss=4.834521770477295
I0203 19:12:11.363420 139774417409792 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.087444543838501, loss=2.7802202701568604
I0203 19:12:58.285900 139774434195200 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.9574833512306213, loss=4.402976036071777
I0203 19:13:45.109049 139774417409792 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1773761510849, loss=2.752432346343994
I0203 19:14:32.331236 139774434195200 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.141526222229004, loss=2.630206346511841
I0203 19:15:19.435226 139774417409792 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.1165066957473755, loss=3.3278470039367676
I0203 19:15:29.283998 139936116377408 spec.py:321] Evaluating on the training split.
I0203 19:15:41.221090 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:16:19.830431 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:16:21.439276 139936116377408 submission_runner.py:408] Time since start: 22294.79s, 	Step: 42423, 	{'train/accuracy': 0.6158398389816284, 'train/loss': 1.5993508100509644, 'validation/accuracy': 0.5668599605560303, 'validation/loss': 1.8338241577148438, 'validation/num_examples': 50000, 'test/accuracy': 0.4497000277042389, 'test/loss': 2.5079567432403564, 'test/num_examples': 10000, 'score': 19785.181124925613, 'total_duration': 22294.79103422165, 'accumulated_submission_time': 19785.181124925613, 'accumulated_eval_time': 2505.8282821178436, 'accumulated_logging_time': 1.584458351135254}
I0203 19:16:21.472789 139774434195200 logging_writer.py:48] [42423] accumulated_eval_time=2505.828282, accumulated_logging_time=1.584458, accumulated_submission_time=19785.181125, global_step=42423, preemption_count=0, score=19785.181125, test/accuracy=0.449700, test/loss=2.507957, test/num_examples=10000, total_duration=22294.791034, train/accuracy=0.615840, train/loss=1.599351, validation/accuracy=0.566860, validation/loss=1.833824, validation/num_examples=50000
I0203 19:16:54.187302 139774417409792 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.053774356842041, loss=2.818575620651245
I0203 19:17:41.028111 139774434195200 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.0128583908081055, loss=4.011208534240723
I0203 19:18:27.870341 139774417409792 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0921095609664917, loss=2.688100814819336
I0203 19:19:14.726893 139774434195200 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2526142597198486, loss=2.685433864593506
I0203 19:20:01.636552 139774417409792 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.333577275276184, loss=2.758714437484741
I0203 19:20:48.883581 139774434195200 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.2431086301803589, loss=2.8734660148620605
I0203 19:21:35.903585 139774417409792 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.2112993001937866, loss=2.736060619354248
I0203 19:22:23.235285 139774434195200 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.2229105234146118, loss=2.745968818664551
I0203 19:23:10.317393 139774417409792 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.270216941833496, loss=3.001502752304077
I0203 19:23:21.623226 139936116377408 spec.py:321] Evaluating on the training split.
I0203 19:23:33.481775 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:24:12.015696 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:24:13.619858 139936116377408 submission_runner.py:408] Time since start: 22766.97s, 	Step: 43326, 	{'train/accuracy': 0.6241992115974426, 'train/loss': 1.6059962511062622, 'validation/accuracy': 0.5625, 'validation/loss': 1.9052478075027466, 'validation/num_examples': 50000, 'test/accuracy': 0.4489000141620636, 'test/loss': 2.5377256870269775, 'test/num_examples': 10000, 'score': 20205.273720502853, 'total_duration': 22766.971638917923, 'accumulated_submission_time': 20205.273720502853, 'accumulated_eval_time': 2557.823818206787, 'accumulated_logging_time': 1.6289563179016113}
I0203 19:24:13.643019 139774434195200 logging_writer.py:48] [43326] accumulated_eval_time=2557.823818, accumulated_logging_time=1.628956, accumulated_submission_time=20205.273721, global_step=43326, preemption_count=0, score=20205.273721, test/accuracy=0.448900, test/loss=2.537726, test/num_examples=10000, total_duration=22766.971639, train/accuracy=0.624199, train/loss=1.605996, validation/accuracy=0.562500, validation/loss=1.905248, validation/num_examples=50000
I0203 19:24:45.015488 139774417409792 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.1044734716415405, loss=2.6837635040283203
I0203 19:25:31.764908 139774434195200 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.016287088394165, loss=3.4268414974212646
I0203 19:26:18.710495 139774417409792 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.179084062576294, loss=2.8345324993133545
I0203 19:27:05.649297 139774434195200 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.0042152404785156, loss=3.4479596614837646
I0203 19:27:52.713582 139774417409792 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.9100032448768616, loss=4.223580360412598
I0203 19:28:39.893350 139774434195200 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.0579391717910767, loss=4.727057933807373
I0203 19:29:27.175282 139774417409792 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1131999492645264, loss=2.6529288291931152
I0203 19:30:14.180350 139774434195200 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.1646298170089722, loss=2.6329193115234375
I0203 19:31:01.047220 139774417409792 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.1846413612365723, loss=3.0268402099609375
I0203 19:31:13.980493 139936116377408 spec.py:321] Evaluating on the training split.
I0203 19:31:25.903242 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:32:04.467048 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:32:06.074767 139936116377408 submission_runner.py:408] Time since start: 23239.43s, 	Step: 44229, 	{'train/accuracy': 0.6158398389816284, 'train/loss': 1.6221504211425781, 'validation/accuracy': 0.5743799805641174, 'validation/loss': 1.8318836688995361, 'validation/num_examples': 50000, 'test/accuracy': 0.45190003514289856, 'test/loss': 2.4976508617401123, 'test/num_examples': 10000, 'score': 20625.553758859634, 'total_duration': 23239.426404237747, 'accumulated_submission_time': 20625.553758859634, 'accumulated_eval_time': 2609.916860103607, 'accumulated_logging_time': 1.6631481647491455}
I0203 19:32:06.103323 139774434195200 logging_writer.py:48] [44229] accumulated_eval_time=2609.916860, accumulated_logging_time=1.663148, accumulated_submission_time=20625.553759, global_step=44229, preemption_count=0, score=20625.553759, test/accuracy=0.451900, test/loss=2.497651, test/num_examples=10000, total_duration=23239.426404, train/accuracy=0.615840, train/loss=1.622150, validation/accuracy=0.574380, validation/loss=1.831884, validation/num_examples=50000
I0203 19:32:36.563137 139774417409792 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.144713044166565, loss=2.7372610569000244
I0203 19:33:22.919733 139774434195200 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1200504302978516, loss=2.7925870418548584
I0203 19:34:09.777466 139774417409792 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.3504722118377686, loss=2.743717670440674
I0203 19:34:56.509180 139774434195200 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1596848964691162, loss=3.044729232788086
I0203 19:35:43.275249 139774417409792 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.1441032886505127, loss=2.6635236740112305
I0203 19:36:30.394042 139774434195200 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.0798519849777222, loss=3.222364902496338
I0203 19:37:17.221184 139774417409792 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.0462696552276611, loss=2.9032509326934814
I0203 19:38:04.032011 139774434195200 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.151620626449585, loss=3.1309218406677246
I0203 19:38:51.256070 139774417409792 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.1898307800292969, loss=2.6162381172180176
I0203 19:39:06.080180 139936116377408 spec.py:321] Evaluating on the training split.
I0203 19:39:18.006586 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:39:58.758121 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:40:00.357468 139936116377408 submission_runner.py:408] Time since start: 23713.71s, 	Step: 45133, 	{'train/accuracy': 0.6198632717132568, 'train/loss': 1.6021332740783691, 'validation/accuracy': 0.5716800093650818, 'validation/loss': 1.8257529735565186, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.4946107864379883, 'test/num_examples': 10000, 'score': 21044.993222475052, 'total_duration': 23713.709257364273, 'accumulated_submission_time': 21044.993222475052, 'accumulated_eval_time': 2664.1930780410767, 'accumulated_logging_time': 2.1820499897003174}
I0203 19:40:00.381480 139774434195200 logging_writer.py:48] [45133] accumulated_eval_time=2664.193078, accumulated_logging_time=2.182050, accumulated_submission_time=21044.993222, global_step=45133, preemption_count=0, score=21044.993222, test/accuracy=0.454200, test/loss=2.494611, test/num_examples=10000, total_duration=23713.709257, train/accuracy=0.619863, train/loss=1.602133, validation/accuracy=0.571680, validation/loss=1.825753, validation/num_examples=50000
I0203 19:40:28.793083 139774417409792 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.962863028049469, loss=5.131450176239014
I0203 19:41:15.083981 139774434195200 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.2248404026031494, loss=2.6488146781921387
I0203 19:42:02.299948 139774417409792 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1599708795547485, loss=2.646648406982422
I0203 19:42:49.488713 139774434195200 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0853651762008667, loss=2.6728594303131104
I0203 19:43:36.345161 139774417409792 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9961697459220886, loss=3.4823544025421143
I0203 19:44:23.354505 139774434195200 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1093568801879883, loss=2.7579281330108643
I0203 19:45:10.264477 139774417409792 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1113678216934204, loss=2.7534079551696777
I0203 19:45:57.016257 139774434195200 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.0312598943710327, loss=3.3823602199554443
I0203 19:46:43.823592 139774417409792 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9628745317459106, loss=3.1106061935424805
I0203 19:47:00.756124 139936116377408 spec.py:321] Evaluating on the training split.
I0203 19:47:12.708473 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:47:51.609641 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:47:53.221847 139936116377408 submission_runner.py:408] Time since start: 24186.57s, 	Step: 46038, 	{'train/accuracy': 0.6359961032867432, 'train/loss': 1.5632343292236328, 'validation/accuracy': 0.5751199722290039, 'validation/loss': 1.8546125888824463, 'validation/num_examples': 50000, 'test/accuracy': 0.4570000171661377, 'test/loss': 2.525801658630371, 'test/num_examples': 10000, 'score': 21465.308529138565, 'total_duration': 24186.573442697525, 'accumulated_submission_time': 21465.308529138565, 'accumulated_eval_time': 2716.657520532608, 'accumulated_logging_time': 2.2180991172790527}
I0203 19:47:53.247883 139774434195200 logging_writer.py:48] [46038] accumulated_eval_time=2716.657521, accumulated_logging_time=2.218099, accumulated_submission_time=21465.308529, global_step=46038, preemption_count=0, score=21465.308529, test/accuracy=0.457000, test/loss=2.525802, test/num_examples=10000, total_duration=24186.573443, train/accuracy=0.635996, train/loss=1.563234, validation/accuracy=0.575120, validation/loss=1.854613, validation/num_examples=50000
I0203 19:48:19.378218 139774417409792 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.1049481630325317, loss=3.3417439460754395
I0203 19:49:05.484239 139774434195200 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.0103082656860352, loss=4.089473724365234
I0203 19:49:52.556879 139774417409792 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0994298458099365, loss=2.7442779541015625
I0203 19:50:39.413017 139774434195200 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.9658958911895752, loss=3.974933624267578
I0203 19:51:26.645419 139774417409792 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.043533205986023, loss=3.214421510696411
I0203 19:52:13.830221 139774434195200 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1802548170089722, loss=2.682898998260498
I0203 19:53:00.824803 139774417409792 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.1815845966339111, loss=2.6454572677612305
I0203 19:53:47.903861 139774434195200 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.2748785018920898, loss=2.721778154373169
I0203 19:54:34.938379 139774417409792 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.152195692062378, loss=4.9371137619018555
I0203 19:54:53.263245 139936116377408 spec.py:321] Evaluating on the training split.
I0203 19:55:05.422647 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 19:55:45.929674 139936116377408 spec.py:349] Evaluating on the test split.
I0203 19:55:47.531797 139936116377408 submission_runner.py:408] Time since start: 24660.88s, 	Step: 46941, 	{'train/accuracy': 0.6237695217132568, 'train/loss': 1.5933055877685547, 'validation/accuracy': 0.583840012550354, 'validation/loss': 1.7873661518096924, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.460035562515259, 'test/num_examples': 10000, 'score': 21885.264724254608, 'total_duration': 24660.883398771286, 'accumulated_submission_time': 21885.264724254608, 'accumulated_eval_time': 2770.9248657226562, 'accumulated_logging_time': 2.2558722496032715}
I0203 19:55:47.556120 139774434195200 logging_writer.py:48] [46941] accumulated_eval_time=2770.924866, accumulated_logging_time=2.255872, accumulated_submission_time=21885.264724, global_step=46941, preemption_count=0, score=21885.264724, test/accuracy=0.457900, test/loss=2.460036, test/num_examples=10000, total_duration=24660.883399, train/accuracy=0.623770, train/loss=1.593306, validation/accuracy=0.583840, validation/loss=1.787366, validation/num_examples=50000
I0203 19:56:12.413891 139774417409792 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1844764947891235, loss=2.6556742191314697
I0203 19:56:58.442729 139774434195200 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.2509976625442505, loss=2.604764938354492
I0203 19:57:45.447767 139774417409792 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.9069641828536987, loss=4.906666278839111
I0203 19:58:32.288354 139774434195200 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.182785987854004, loss=2.724238395690918
I0203 19:59:19.429658 139774417409792 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.0561243295669556, loss=3.497746229171753
I0203 20:00:06.526999 139774434195200 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1707262992858887, loss=2.786325216293335
I0203 20:00:53.518748 139774417409792 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.3109585046768188, loss=2.688995361328125
I0203 20:01:40.561328 139774434195200 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2542287111282349, loss=2.5369629859924316
I0203 20:02:27.368782 139774417409792 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.0048092603683472, loss=4.491405487060547
I0203 20:02:47.689749 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:02:59.586531 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:03:39.393089 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:03:41.042145 139936116377408 submission_runner.py:408] Time since start: 25134.39s, 	Step: 47845, 	{'train/accuracy': 0.6243554353713989, 'train/loss': 1.5742669105529785, 'validation/accuracy': 0.5797199606895447, 'validation/loss': 1.8012193441390991, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.4564502239227295, 'test/num_examples': 10000, 'score': 22305.34062218666, 'total_duration': 25134.393942832947, 'accumulated_submission_time': 22305.34062218666, 'accumulated_eval_time': 2824.276191473007, 'accumulated_logging_time': 2.2912559509277344}
I0203 20:03:41.074366 139774434195200 logging_writer.py:48] [47845] accumulated_eval_time=2824.276191, accumulated_logging_time=2.291256, accumulated_submission_time=22305.340622, global_step=47845, preemption_count=0, score=22305.340622, test/accuracy=0.459900, test/loss=2.456450, test/num_examples=10000, total_duration=25134.393943, train/accuracy=0.624355, train/loss=1.574267, validation/accuracy=0.579720, validation/loss=1.801219, validation/num_examples=50000
I0203 20:04:04.283195 139774417409792 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.260061264038086, loss=2.618831157684326
I0203 20:04:49.647185 139774434195200 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9861916303634644, loss=3.8804078102111816
I0203 20:05:36.349345 139774417409792 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.9848935008049011, loss=4.412178039550781
I0203 20:06:23.134219 139774434195200 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.300489902496338, loss=2.717926502227783
I0203 20:07:09.655956 139774417409792 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0810556411743164, loss=2.845043420791626
I0203 20:07:56.422120 139774434195200 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1044548749923706, loss=2.6162219047546387
I0203 20:08:43.240154 139774417409792 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2720063924789429, loss=2.7249505519866943
I0203 20:09:30.317338 139774434195200 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.1709531545639038, loss=2.441166877746582
I0203 20:10:17.396961 139774417409792 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.2001551389694214, loss=2.5919339656829834
I0203 20:10:41.222392 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:10:53.296525 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:11:31.781676 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:11:33.398876 139936116377408 submission_runner.py:408] Time since start: 25606.75s, 	Step: 48753, 	{'train/accuracy': 0.6461523175239563, 'train/loss': 1.4468914270401, 'validation/accuracy': 0.5879200100898743, 'validation/loss': 1.7366979122161865, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.4156606197357178, 'test/num_examples': 10000, 'score': 22725.429410219193, 'total_duration': 25606.750625133514, 'accumulated_submission_time': 22725.429410219193, 'accumulated_eval_time': 2876.451560020447, 'accumulated_logging_time': 2.335695505142212}
I0203 20:11:33.428009 139774434195200 logging_writer.py:48] [48753] accumulated_eval_time=2876.451560, accumulated_logging_time=2.335696, accumulated_submission_time=22725.429410, global_step=48753, preemption_count=0, score=22725.429410, test/accuracy=0.464900, test/loss=2.415661, test/num_examples=10000, total_duration=25606.750625, train/accuracy=0.646152, train/loss=1.446891, validation/accuracy=0.587920, validation/loss=1.736698, validation/num_examples=50000
I0203 20:11:53.326570 139774417409792 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.1508139371871948, loss=2.6021718978881836
I0203 20:12:38.765013 139774434195200 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.1732995510101318, loss=2.476179838180542
I0203 20:13:25.344738 139774417409792 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0755126476287842, loss=4.876688480377197
I0203 20:14:12.316554 139774434195200 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.9512854218482971, loss=5.326140403747559
I0203 20:14:58.896419 139774417409792 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.4544686079025269, loss=2.6451704502105713
I0203 20:15:45.667571 139774434195200 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.1086280345916748, loss=3.064887046813965
I0203 20:16:32.394088 139774417409792 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.1120445728302002, loss=2.6358087062835693
I0203 20:17:19.265415 139774434195200 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0978916883468628, loss=3.112548351287842
I0203 20:18:05.845574 139774417409792 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.1636223793029785, loss=2.590481996536255
I0203 20:18:33.702855 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:18:45.545146 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:19:24.465797 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:19:26.069385 139936116377408 submission_runner.py:408] Time since start: 26079.42s, 	Step: 49661, 	{'train/accuracy': 0.6366406083106995, 'train/loss': 1.5108044147491455, 'validation/accuracy': 0.5891199707984924, 'validation/loss': 1.7323944568634033, 'validation/num_examples': 50000, 'test/accuracy': 0.4748000204563141, 'test/loss': 2.404741048812866, 'test/num_examples': 10000, 'score': 23145.645364046097, 'total_duration': 26079.421197891235, 'accumulated_submission_time': 23145.645364046097, 'accumulated_eval_time': 2928.8170251846313, 'accumulated_logging_time': 2.37554931640625}
I0203 20:19:26.096517 139774434195200 logging_writer.py:48] [49661] accumulated_eval_time=2928.817025, accumulated_logging_time=2.375549, accumulated_submission_time=23145.645364, global_step=49661, preemption_count=0, score=23145.645364, test/accuracy=0.474800, test/loss=2.404741, test/num_examples=10000, total_duration=26079.421198, train/accuracy=0.636641, train/loss=1.510804, validation/accuracy=0.589120, validation/loss=1.732394, validation/num_examples=50000
I0203 20:19:42.684667 139774417409792 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.243177056312561, loss=2.595888614654541
I0203 20:20:28.073758 139774434195200 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1106181144714355, loss=5.246993541717529
I0203 20:21:15.289011 139774417409792 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.9631252288818359, loss=4.583313941955566
I0203 20:22:02.548619 139774434195200 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1401989459991455, loss=2.5517196655273438
I0203 20:22:50.275810 139774417409792 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9648954272270203, loss=4.10552978515625
I0203 20:23:37.319116 139774434195200 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0915470123291016, loss=2.8447155952453613
I0203 20:24:24.792655 139774417409792 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.9978570938110352, loss=4.190096378326416
I0203 20:25:11.986956 139774434195200 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.056553602218628, loss=3.3763744831085205
I0203 20:25:59.160527 139774417409792 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.1100300550460815, loss=2.5897953510284424
I0203 20:26:26.341639 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:26:38.211967 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:27:17.145492 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:27:18.751000 139936116377408 submission_runner.py:408] Time since start: 26552.10s, 	Step: 50559, 	{'train/accuracy': 0.637988269329071, 'train/loss': 1.5000442266464233, 'validation/accuracy': 0.5877199769020081, 'validation/loss': 1.7394182682037354, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.3809423446655273, 'test/num_examples': 10000, 'score': 23565.832825899124, 'total_duration': 26552.102893590927, 'accumulated_submission_time': 23565.832825899124, 'accumulated_eval_time': 2981.225423812866, 'accumulated_logging_time': 2.4138035774230957}
I0203 20:27:18.783838 139774434195200 logging_writer.py:48] [50559] accumulated_eval_time=2981.225424, accumulated_logging_time=2.413804, accumulated_submission_time=23565.832826, global_step=50559, preemption_count=0, score=23565.832826, test/accuracy=0.472600, test/loss=2.380942, test/num_examples=10000, total_duration=26552.102894, train/accuracy=0.637988, train/loss=1.500044, validation/accuracy=0.587720, validation/loss=1.739418, validation/num_examples=50000
I0203 20:27:36.190586 139774417409792 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.9900041222572327, loss=5.174776077270508
I0203 20:28:21.227513 139774434195200 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.0910842418670654, loss=3.354600191116333
I0203 20:29:08.250508 139774417409792 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.1756187677383423, loss=2.4713082313537598
I0203 20:29:54.993003 139774434195200 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.2247836589813232, loss=2.6672980785369873
I0203 20:30:41.963363 139774417409792 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.219565510749817, loss=3.26296329498291
I0203 20:31:28.994186 139774434195200 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.3470458984375, loss=2.666477680206299
I0203 20:32:16.275035 139774417409792 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.3332767486572266, loss=2.589085578918457
I0203 20:33:03.800023 139774434195200 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.2218624353408813, loss=2.603201389312744
I0203 20:33:50.547502 139774417409792 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.1870492696762085, loss=3.056457042694092
I0203 20:34:18.985536 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:34:30.900004 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:35:09.888661 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:35:11.497018 139936116377408 submission_runner.py:408] Time since start: 27024.85s, 	Step: 51462, 	{'train/accuracy': 0.6421484351158142, 'train/loss': 1.4973787069320679, 'validation/accuracy': 0.5862199664115906, 'validation/loss': 1.781178593635559, 'validation/num_examples': 50000, 'test/accuracy': 0.46670001745224, 'test/loss': 2.434500217437744, 'test/num_examples': 10000, 'score': 23985.97656941414, 'total_duration': 27024.848866462708, 'accumulated_submission_time': 23985.97656941414, 'accumulated_eval_time': 3033.735938310623, 'accumulated_logging_time': 2.4571444988250732}
I0203 20:35:11.526985 139774434195200 logging_writer.py:48] [51462] accumulated_eval_time=3033.735938, accumulated_logging_time=2.457144, accumulated_submission_time=23985.976569, global_step=51462, preemption_count=0, score=23985.976569, test/accuracy=0.466700, test/loss=2.434500, test/num_examples=10000, total_duration=27024.848866, train/accuracy=0.642148, train/loss=1.497379, validation/accuracy=0.586220, validation/loss=1.781179, validation/num_examples=50000
I0203 20:35:27.700649 139774417409792 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.2691127061843872, loss=2.7771620750427246
I0203 20:36:12.599921 139774434195200 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.0596370697021484, loss=4.476701259613037
I0203 20:36:58.999351 139774417409792 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.2971017360687256, loss=2.592400312423706
I0203 20:37:46.122282 139774434195200 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.0477862358093262, loss=3.628124713897705
I0203 20:38:33.121202 139774417409792 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.0423256158828735, loss=3.4843454360961914
I0203 20:39:20.095515 139774434195200 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0225780010223389, loss=3.0703084468841553
I0203 20:40:07.367899 139774417409792 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.9439657926559448, loss=3.8329784870147705
I0203 20:40:54.037418 139774434195200 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.1751195192337036, loss=2.764280319213867
I0203 20:41:41.254147 139774417409792 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.1527321338653564, loss=2.969888687133789
I0203 20:42:11.738335 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:42:23.546838 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:43:02.577622 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:43:04.178284 139936116377408 submission_runner.py:408] Time since start: 27497.53s, 	Step: 52366, 	{'train/accuracy': 0.6407226324081421, 'train/loss': 1.4778684377670288, 'validation/accuracy': 0.5956599712371826, 'validation/loss': 1.697656512260437, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.3861453533172607, 'test/num_examples': 10000, 'score': 24406.12998008728, 'total_duration': 27497.530173778534, 'accumulated_submission_time': 24406.12998008728, 'accumulated_eval_time': 3086.1749098300934, 'accumulated_logging_time': 2.498885154724121}
I0203 20:43:04.208542 139774434195200 logging_writer.py:48] [52366] accumulated_eval_time=3086.174910, accumulated_logging_time=2.498885, accumulated_submission_time=24406.129980, global_step=52366, preemption_count=0, score=24406.129980, test/accuracy=0.472200, test/loss=2.386145, test/num_examples=10000, total_duration=27497.530174, train/accuracy=0.640723, train/loss=1.477868, validation/accuracy=0.595660, validation/loss=1.697657, validation/num_examples=50000
I0203 20:43:18.713049 139774417409792 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.097997784614563, loss=2.78161883354187
I0203 20:44:03.641772 139774434195200 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.3079218864440918, loss=2.571218967437744
I0203 20:44:51.058459 139774417409792 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.0362040996551514, loss=3.225414514541626
I0203 20:45:38.237544 139774434195200 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.2398823499679565, loss=2.534421443939209
I0203 20:46:25.450098 139774417409792 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.1767572164535522, loss=2.6491823196411133
I0203 20:47:12.358417 139774434195200 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1097146272659302, loss=3.9961647987365723
I0203 20:47:59.390258 139774417409792 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0898613929748535, loss=4.973807334899902
I0203 20:48:46.484415 139774434195200 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2345820665359497, loss=2.613440752029419
I0203 20:49:33.564865 139774417409792 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2302933931350708, loss=2.726040840148926
I0203 20:50:04.355242 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:50:16.154802 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:50:54.962667 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:50:56.580325 139936116377408 submission_runner.py:408] Time since start: 27969.93s, 	Step: 53267, 	{'train/accuracy': 0.6386523246765137, 'train/loss': 1.511078119277954, 'validation/accuracy': 0.5876399874687195, 'validation/loss': 1.755619764328003, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.407470703125, 'test/num_examples': 10000, 'score': 24826.21729016304, 'total_duration': 27969.93226337433, 'accumulated_submission_time': 24826.21729016304, 'accumulated_eval_time': 3138.3990700244904, 'accumulated_logging_time': 2.5419416427612305}
I0203 20:50:56.605460 139774434195200 logging_writer.py:48] [53267] accumulated_eval_time=3138.399070, accumulated_logging_time=2.541942, accumulated_submission_time=24826.217290, global_step=53267, preemption_count=0, score=24826.217290, test/accuracy=0.470200, test/loss=2.407471, test/num_examples=10000, total_duration=27969.932263, train/accuracy=0.638652, train/loss=1.511078, validation/accuracy=0.587640, validation/loss=1.755620, validation/num_examples=50000
I0203 20:51:10.693530 139774417409792 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9822879433631897, loss=5.125399589538574
I0203 20:51:55.563728 139774434195200 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.1809602975845337, loss=2.7671401500701904
I0203 20:52:42.671956 139774417409792 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0663485527038574, loss=2.9713988304138184
I0203 20:53:29.866337 139774434195200 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.346875548362732, loss=2.5176305770874023
I0203 20:54:16.934072 139774417409792 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.1175761222839355, loss=3.2962775230407715
I0203 20:55:04.194705 139774434195200 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.0287379026412964, loss=3.4544787406921387
I0203 20:55:51.024643 139774417409792 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.9664098620414734, loss=4.309712886810303
I0203 20:56:37.799325 139774434195200 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9611886739730835, loss=4.316652774810791
I0203 20:57:24.880661 139774417409792 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.086593747138977, loss=3.4489293098449707
I0203 20:57:56.818264 139936116377408 spec.py:321] Evaluating on the training split.
I0203 20:58:08.846189 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 20:58:46.875062 139936116377408 spec.py:349] Evaluating on the test split.
I0203 20:58:48.476102 139936116377408 submission_runner.py:408] Time since start: 28441.83s, 	Step: 54170, 	{'train/accuracy': 0.6471874713897705, 'train/loss': 1.4455610513687134, 'validation/accuracy': 0.5920599699020386, 'validation/loss': 1.721648097038269, 'validation/num_examples': 50000, 'test/accuracy': 0.4725000262260437, 'test/loss': 2.3761520385742188, 'test/num_examples': 10000, 'score': 25246.371876716614, 'total_duration': 28441.82798075676, 'accumulated_submission_time': 25246.371876716614, 'accumulated_eval_time': 3190.0559356212616, 'accumulated_logging_time': 2.5783660411834717}
I0203 20:58:48.501098 139774434195200 logging_writer.py:48] [54170] accumulated_eval_time=3190.055936, accumulated_logging_time=2.578366, accumulated_submission_time=25246.371877, global_step=54170, preemption_count=0, score=25246.371877, test/accuracy=0.472500, test/loss=2.376152, test/num_examples=10000, total_duration=28441.827981, train/accuracy=0.647187, train/loss=1.445561, validation/accuracy=0.592060, validation/loss=1.721648, validation/num_examples=50000
I0203 20:59:01.352923 139774417409792 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.3601322174072266, loss=2.6046290397644043
I0203 20:59:45.969728 139774434195200 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0078264474868774, loss=3.786062479019165
I0203 21:00:32.852971 139774417409792 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2828420400619507, loss=2.5715227127075195
I0203 21:01:19.721554 139774434195200 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2887070178985596, loss=2.615797519683838
I0203 21:02:06.874501 139774417409792 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.0364948511123657, loss=3.2696542739868164
I0203 21:02:53.761270 139774434195200 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.2162662744522095, loss=2.61588716506958
I0203 21:03:40.814074 139774417409792 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.0513278245925903, loss=4.365611553192139
I0203 21:04:27.634406 139774434195200 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.186783790588379, loss=2.54709792137146
I0203 21:05:14.732345 139774417409792 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2405643463134766, loss=2.653689384460449
I0203 21:05:48.821831 139936116377408 spec.py:321] Evaluating on the training split.
I0203 21:06:00.651823 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 21:06:39.140079 139936116377408 spec.py:349] Evaluating on the test split.
I0203 21:06:40.746468 139936116377408 submission_runner.py:408] Time since start: 28914.10s, 	Step: 55074, 	{'train/accuracy': 0.6447460651397705, 'train/loss': 1.4728977680206299, 'validation/accuracy': 0.5917400121688843, 'validation/loss': 1.7395529747009277, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.3855762481689453, 'test/num_examples': 10000, 'score': 25666.634063720703, 'total_duration': 28914.098296403885, 'accumulated_submission_time': 25666.634063720703, 'accumulated_eval_time': 3241.9795260429382, 'accumulated_logging_time': 2.6154439449310303}
I0203 21:06:40.773190 139774434195200 logging_writer.py:48] [55074] accumulated_eval_time=3241.979526, accumulated_logging_time=2.615444, accumulated_submission_time=25666.634064, global_step=55074, preemption_count=0, score=25666.634064, test/accuracy=0.471300, test/loss=2.385576, test/num_examples=10000, total_duration=28914.098296, train/accuracy=0.644746, train/loss=1.472898, validation/accuracy=0.591740, validation/loss=1.739553, validation/num_examples=50000
I0203 21:06:51.954113 139774417409792 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.1692289113998413, loss=2.547687292098999
I0203 21:07:36.744933 139774434195200 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.1706998348236084, loss=2.757314443588257
I0203 21:08:23.785686 139774417409792 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.3251265287399292, loss=2.5580759048461914
I0203 21:09:11.246702 139774434195200 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.1561201810836792, loss=2.4439427852630615
I0203 21:09:58.776273 139774417409792 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1417438983917236, loss=2.5640320777893066
I0203 21:10:46.269121 139774434195200 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.3508905172348022, loss=2.643247604370117
I0203 21:11:33.908988 139774417409792 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.282322645187378, loss=2.5603294372558594
I0203 21:12:21.568350 139774434195200 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.309508204460144, loss=2.643716812133789
I0203 21:13:08.679238 139774417409792 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.2949227094650269, loss=2.638535976409912
I0203 21:13:40.973901 139936116377408 spec.py:321] Evaluating on the training split.
I0203 21:13:52.859395 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 21:14:32.237845 139936116377408 spec.py:349] Evaluating on the test split.
I0203 21:14:33.847475 139936116377408 submission_runner.py:408] Time since start: 29387.20s, 	Step: 55970, 	{'train/accuracy': 0.6394140720367432, 'train/loss': 1.5037568807601929, 'validation/accuracy': 0.5911999940872192, 'validation/loss': 1.7273492813110352, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.3871467113494873, 'test/num_examples': 10000, 'score': 26086.776544094086, 'total_duration': 29387.199322223663, 'accumulated_submission_time': 26086.776544094086, 'accumulated_eval_time': 3294.8521118164062, 'accumulated_logging_time': 2.6539435386657715}
I0203 21:14:33.878738 139774434195200 logging_writer.py:48] [55970] accumulated_eval_time=3294.852112, accumulated_logging_time=2.653944, accumulated_submission_time=26086.776544, global_step=55970, preemption_count=0, score=26086.776544, test/accuracy=0.478300, test/loss=2.387147, test/num_examples=10000, total_duration=29387.199322, train/accuracy=0.639414, train/loss=1.503757, validation/accuracy=0.591200, validation/loss=1.727349, validation/num_examples=50000
I0203 21:14:46.722266 139774417409792 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1722289323806763, loss=2.8328816890716553
I0203 21:15:31.802798 139774434195200 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.3048415184020996, loss=3.1418066024780273
I0203 21:16:18.826760 139774417409792 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.1559748649597168, loss=2.506458044052124
I0203 21:17:05.850093 139774434195200 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.2796170711517334, loss=2.5557076930999756
I0203 21:17:52.710735 139774417409792 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2372554540634155, loss=2.7250585556030273
I0203 21:18:39.785422 139774434195200 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2028837203979492, loss=2.673353433609009
I0203 21:19:26.874893 139774417409792 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.07659113407135, loss=5.277469635009766
I0203 21:20:13.838042 139774434195200 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2010003328323364, loss=2.444924831390381
I0203 21:21:00.782392 139774417409792 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.9841873049736023, loss=4.5768303871154785
I0203 21:21:33.921826 139936116377408 spec.py:321] Evaluating on the training split.
I0203 21:21:45.916316 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 21:22:25.197007 139936116377408 spec.py:349] Evaluating on the test split.
I0203 21:22:26.802803 139936116377408 submission_runner.py:408] Time since start: 29860.15s, 	Step: 56872, 	{'train/accuracy': 0.6453320384025574, 'train/loss': 1.4583547115325928, 'validation/accuracy': 0.5988999605178833, 'validation/loss': 1.701833724975586, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.3658065795898438, 'test/num_examples': 10000, 'score': 26506.762736082077, 'total_duration': 29860.154549121857, 'accumulated_submission_time': 26506.762736082077, 'accumulated_eval_time': 3347.73197889328, 'accumulated_logging_time': 2.6959104537963867}
I0203 21:22:26.830489 139774434195200 logging_writer.py:48] [56872] accumulated_eval_time=3347.731979, accumulated_logging_time=2.695910, accumulated_submission_time=26506.762736, global_step=56872, preemption_count=0, score=26506.762736, test/accuracy=0.474500, test/loss=2.365807, test/num_examples=10000, total_duration=29860.154549, train/accuracy=0.645332, train/loss=1.458355, validation/accuracy=0.598900, validation/loss=1.701834, validation/num_examples=50000
I0203 21:22:38.850419 139774417409792 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.9937495589256287, loss=4.627356052398682
I0203 21:23:23.323296 139774434195200 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.9981979131698608, loss=4.576368808746338
I0203 21:24:09.988533 139774417409792 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.9793474078178406, loss=4.672213077545166
I0203 21:24:56.569113 139774434195200 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9294295310974121, loss=4.4538469314575195
I0203 21:25:43.617616 139774417409792 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1871747970581055, loss=3.3905820846557617
I0203 21:26:30.678394 139774434195200 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.2548147439956665, loss=2.4320952892303467
I0203 21:27:17.304545 139774417409792 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.2686017751693726, loss=2.7862322330474854
I0203 21:28:04.246438 139774434195200 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.0294731855392456, loss=3.65139102935791
I0203 21:28:51.044835 139774417409792 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.2867077589035034, loss=2.5821805000305176
I0203 21:29:26.871735 139936116377408 spec.py:321] Evaluating on the training split.
I0203 21:29:38.623446 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 21:30:17.464752 139936116377408 spec.py:349] Evaluating on the test split.
I0203 21:30:19.073379 139936116377408 submission_runner.py:408] Time since start: 30332.43s, 	Step: 57778, 	{'train/accuracy': 0.6798242330551147, 'train/loss': 1.318408727645874, 'validation/accuracy': 0.6038199663162231, 'validation/loss': 1.671668529510498, 'validation/num_examples': 50000, 'test/accuracy': 0.48570001125335693, 'test/loss': 2.3210160732269287, 'test/num_examples': 10000, 'score': 26926.744647026062, 'total_duration': 30332.42511534691, 'accumulated_submission_time': 26926.744647026062, 'accumulated_eval_time': 3399.9325094223022, 'accumulated_logging_time': 2.73496150970459}
I0203 21:30:19.102577 139774434195200 logging_writer.py:48] [57778] accumulated_eval_time=3399.932509, accumulated_logging_time=2.734962, accumulated_submission_time=26926.744647, global_step=57778, preemption_count=0, score=26926.744647, test/accuracy=0.485700, test/loss=2.321016, test/num_examples=10000, total_duration=30332.425115, train/accuracy=0.679824, train/loss=1.318409, validation/accuracy=0.603820, validation/loss=1.671669, validation/num_examples=50000
I0203 21:30:28.633062 139774417409792 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.0197919607162476, loss=3.225160598754883
I0203 21:31:13.039700 139774434195200 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.25095534324646, loss=2.6372196674346924
I0203 21:32:00.003294 139774417409792 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2300485372543335, loss=2.5538713932037354
I0203 21:32:47.400538 139774434195200 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.147148847579956, loss=4.113748073577881
I0203 21:33:34.300927 139774417409792 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9461318254470825, loss=4.299803733825684
I0203 21:34:21.312329 139774434195200 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.1709980964660645, loss=2.531230926513672
I0203 21:35:08.298318 139774417409792 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.3251023292541504, loss=2.57275652885437
I0203 21:35:55.117225 139774434195200 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1934739351272583, loss=3.797300338745117
I0203 21:36:42.479233 139774417409792 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.1878561973571777, loss=2.949727773666382
I0203 21:37:19.377774 139936116377408 spec.py:321] Evaluating on the training split.
I0203 21:37:31.190453 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 21:38:08.440814 139936116377408 spec.py:349] Evaluating on the test split.
I0203 21:38:10.041491 139936116377408 submission_runner.py:408] Time since start: 30803.39s, 	Step: 58680, 	{'train/accuracy': 0.6500585675239563, 'train/loss': 1.4647369384765625, 'validation/accuracy': 0.5982599854469299, 'validation/loss': 1.699963092803955, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.356879711151123, 'test/num_examples': 10000, 'score': 27346.960132598877, 'total_duration': 30803.393351316452, 'accumulated_submission_time': 27346.960132598877, 'accumulated_eval_time': 3450.595221042633, 'accumulated_logging_time': 2.778135299682617}
I0203 21:38:10.067901 139774434195200 logging_writer.py:48] [58680] accumulated_eval_time=3450.595221, accumulated_logging_time=2.778135, accumulated_submission_time=27346.960133, global_step=58680, preemption_count=0, score=27346.960133, test/accuracy=0.478500, test/loss=2.356880, test/num_examples=10000, total_duration=30803.393351, train/accuracy=0.650059, train/loss=1.464737, validation/accuracy=0.598260, validation/loss=1.699963, validation/num_examples=50000
I0203 21:38:18.785266 139774417409792 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2331252098083496, loss=2.5305168628692627
I0203 21:39:03.250958 139774434195200 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.2705354690551758, loss=2.602250576019287
I0203 21:39:50.107312 139774417409792 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.9600449204444885, loss=4.011966705322266
I0203 21:40:36.774067 139774434195200 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.4650951623916626, loss=2.4797604084014893
I0203 21:41:23.977013 139774417409792 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.4566737413406372, loss=3.2931928634643555
I0203 21:42:11.415287 139774434195200 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.268723487854004, loss=2.511970043182373
I0203 21:42:58.231072 139774417409792 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.9056446552276611, loss=5.102538108825684
I0203 21:43:45.346200 139774434195200 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9951512217521667, loss=5.306987285614014
I0203 21:44:32.586234 139774417409792 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.1180230379104614, loss=5.126646995544434
I0203 21:45:10.366254 139936116377408 spec.py:321] Evaluating on the training split.
I0203 21:45:22.341972 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 21:46:01.555815 139936116377408 spec.py:349] Evaluating on the test split.
I0203 21:46:03.172431 139936116377408 submission_runner.py:408] Time since start: 31276.52s, 	Step: 59582, 	{'train/accuracy': 0.6515820026397705, 'train/loss': 1.4490962028503418, 'validation/accuracy': 0.6025399565696716, 'validation/loss': 1.6890755891799927, 'validation/num_examples': 50000, 'test/accuracy': 0.4764000177383423, 'test/loss': 2.356093168258667, 'test/num_examples': 10000, 'score': 27767.20039820671, 'total_duration': 31276.52426123619, 'accumulated_submission_time': 27767.20039820671, 'accumulated_eval_time': 3503.4003579616547, 'accumulated_logging_time': 2.815491199493408}
I0203 21:46:03.202160 139774434195200 logging_writer.py:48] [59582] accumulated_eval_time=3503.400358, accumulated_logging_time=2.815491, accumulated_submission_time=27767.200398, global_step=59582, preemption_count=0, score=27767.200398, test/accuracy=0.476400, test/loss=2.356093, test/num_examples=10000, total_duration=31276.524261, train/accuracy=0.651582, train/loss=1.449096, validation/accuracy=0.602540, validation/loss=1.689076, validation/num_examples=50000
I0203 21:46:11.081243 139774417409792 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.138226866722107, loss=2.454272747039795
I0203 21:46:54.987202 139774434195200 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.9297465085983276, loss=4.894008636474609
I0203 21:47:41.667361 139774417409792 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.1116986274719238, loss=2.9365806579589844
I0203 21:48:28.630705 139774434195200 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2645167112350464, loss=2.4706063270568848
I0203 21:49:15.628680 139774417409792 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0561269521713257, loss=3.4590063095092773
I0203 21:50:02.623641 139774434195200 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2210361957550049, loss=2.579693078994751
I0203 21:50:49.469427 139774417409792 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.294637680053711, loss=2.5338311195373535
I0203 21:51:36.351507 139774434195200 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.0936872959136963, loss=4.04658317565918
I0203 21:52:23.749699 139774417409792 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.3374661207199097, loss=2.542691230773926
I0203 21:53:03.265108 139936116377408 spec.py:321] Evaluating on the training split.
I0203 21:53:15.151898 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 21:53:53.270797 139936116377408 spec.py:349] Evaluating on the test split.
I0203 21:53:54.877005 139936116377408 submission_runner.py:408] Time since start: 31748.23s, 	Step: 60486, 	{'train/accuracy': 0.6818945407867432, 'train/loss': 1.308190941810608, 'validation/accuracy': 0.6083399653434753, 'validation/loss': 1.650006651878357, 'validation/num_examples': 50000, 'test/accuracy': 0.4871000349521637, 'test/loss': 2.3113863468170166, 'test/num_examples': 10000, 'score': 28187.20550107956, 'total_duration': 31748.228764772415, 'accumulated_submission_time': 28187.20550107956, 'accumulated_eval_time': 3555.011140346527, 'accumulated_logging_time': 2.856693983078003}
I0203 21:53:54.905884 139774434195200 logging_writer.py:48] [60486] accumulated_eval_time=3555.011140, accumulated_logging_time=2.856694, accumulated_submission_time=28187.205501, global_step=60486, preemption_count=0, score=28187.205501, test/accuracy=0.487100, test/loss=2.311386, test/num_examples=10000, total_duration=31748.228765, train/accuracy=0.681895, train/loss=1.308191, validation/accuracy=0.608340, validation/loss=1.650007, validation/num_examples=50000
I0203 21:54:01.127882 139774417409792 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.452913761138916, loss=2.574514389038086
I0203 21:54:45.080938 139774434195200 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1530544757843018, loss=2.998269557952881
I0203 21:55:32.066422 139774417409792 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.1543102264404297, loss=2.5701916217803955
I0203 21:56:19.142272 139774434195200 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.2669061422348022, loss=2.686666250228882
I0203 21:57:06.019017 139774417409792 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.1190435886383057, loss=3.6863160133361816
I0203 21:57:52.917760 139774434195200 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.151921033859253, loss=4.478682518005371
I0203 21:58:40.204016 139774417409792 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2157132625579834, loss=2.663597345352173
I0203 21:59:27.202328 139774434195200 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.9552176594734192, loss=4.084900856018066
I0203 22:00:14.318544 139774417409792 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.271278977394104, loss=2.492611885070801
I0203 22:00:55.014406 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:01:07.179793 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:01:45.162402 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:01:46.764353 139936116377408 submission_runner.py:408] Time since start: 32220.12s, 	Step: 61388, 	{'train/accuracy': 0.6479101181030273, 'train/loss': 1.457031011581421, 'validation/accuracy': 0.6025999784469604, 'validation/loss': 1.6835922002792358, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.3440380096435547, 'test/num_examples': 10000, 'score': 28607.257138967514, 'total_duration': 32220.116106033325, 'accumulated_submission_time': 28607.257138967514, 'accumulated_eval_time': 3606.759976387024, 'accumulated_logging_time': 2.8961234092712402}
I0203 22:01:46.791098 139774434195200 logging_writer.py:48] [61388] accumulated_eval_time=3606.759976, accumulated_logging_time=2.896123, accumulated_submission_time=28607.257139, global_step=61388, preemption_count=0, score=28607.257139, test/accuracy=0.480200, test/loss=2.344038, test/num_examples=10000, total_duration=32220.116106, train/accuracy=0.647910, train/loss=1.457031, validation/accuracy=0.602600, validation/loss=1.683592, validation/num_examples=50000
I0203 22:01:52.187980 139774417409792 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.4319097995758057, loss=2.524254560470581
I0203 22:02:36.163331 139774434195200 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.3756330013275146, loss=2.50787353515625
I0203 22:03:22.998559 139774417409792 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.2860431671142578, loss=2.900637626647949
I0203 22:04:09.949752 139774434195200 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.1056907176971436, loss=4.028415679931641
I0203 22:04:56.803189 139774417409792 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1865864992141724, loss=2.576267719268799
I0203 22:05:43.956014 139774434195200 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.203618049621582, loss=2.5038046836853027
I0203 22:06:31.235548 139774417409792 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2258317470550537, loss=2.5854475498199463
I0203 22:07:18.318744 139774434195200 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.2102922201156616, loss=2.7951765060424805
I0203 22:08:05.630488 139774417409792 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.3172603845596313, loss=2.520933151245117
I0203 22:08:47.128392 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:08:59.056379 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:09:35.488109 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:09:37.090540 139936116377408 submission_runner.py:408] Time since start: 32690.44s, 	Step: 62289, 	{'train/accuracy': 0.6520702838897705, 'train/loss': 1.4345024824142456, 'validation/accuracy': 0.6013000011444092, 'validation/loss': 1.6837353706359863, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.3569390773773193, 'test/num_examples': 10000, 'score': 29027.53623533249, 'total_duration': 32690.442022562027, 'accumulated_submission_time': 29027.53623533249, 'accumulated_eval_time': 3656.720737695694, 'accumulated_logging_time': 2.934617042541504}
I0203 22:09:37.119448 139774434195200 logging_writer.py:48] [62289] accumulated_eval_time=3656.720738, accumulated_logging_time=2.934617, accumulated_submission_time=29027.536235, global_step=62289, preemption_count=0, score=29027.536235, test/accuracy=0.480900, test/loss=2.356939, test/num_examples=10000, total_duration=32690.442023, train/accuracy=0.652070, train/loss=1.434502, validation/accuracy=0.601300, validation/loss=1.683735, validation/num_examples=50000
I0203 22:09:42.096470 139774417409792 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0589728355407715, loss=5.095431804656982
I0203 22:10:25.513826 139774434195200 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.0982739925384521, loss=2.604182243347168
I0203 22:11:12.259893 139774417409792 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.3014414310455322, loss=3.244997501373291
I0203 22:11:59.534333 139774434195200 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0624971389770508, loss=2.6683523654937744
I0203 22:12:46.489223 139774417409792 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.0773049592971802, loss=4.144942283630371
I0203 22:13:33.320404 139774434195200 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1168642044067383, loss=4.881500244140625
I0203 22:14:20.417510 139774417409792 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.2052967548370361, loss=2.451730251312256
I0203 22:15:07.472528 139774434195200 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.3345385789871216, loss=2.500612258911133
I0203 22:15:54.397854 139774417409792 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.3498138189315796, loss=2.477795362472534
I0203 22:16:37.526299 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:16:49.314813 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:17:28.320063 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:17:29.924984 139936116377408 submission_runner.py:408] Time since start: 33163.28s, 	Step: 63194, 	{'train/accuracy': 0.6779687404632568, 'train/loss': 1.3029876947402954, 'validation/accuracy': 0.6114599704742432, 'validation/loss': 1.6231791973114014, 'validation/num_examples': 50000, 'test/accuracy': 0.4918000102043152, 'test/loss': 2.2952914237976074, 'test/num_examples': 10000, 'score': 29447.884727954865, 'total_duration': 33163.27663230896, 'accumulated_submission_time': 29447.884727954865, 'accumulated_eval_time': 3709.118235349655, 'accumulated_logging_time': 2.974860906600952}
I0203 22:17:29.954971 139774434195200 logging_writer.py:48] [63194] accumulated_eval_time=3709.118235, accumulated_logging_time=2.974861, accumulated_submission_time=29447.884728, global_step=63194, preemption_count=0, score=29447.884728, test/accuracy=0.491800, test/loss=2.295291, test/num_examples=10000, total_duration=33163.276632, train/accuracy=0.677969, train/loss=1.302988, validation/accuracy=0.611460, validation/loss=1.623179, validation/num_examples=50000
I0203 22:17:32.864347 139774417409792 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0963170528411865, loss=4.39751672744751
I0203 22:18:16.681167 139774434195200 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.1167970895767212, loss=3.005074977874756
I0203 22:19:03.506270 139774417409792 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0578792095184326, loss=5.071379661560059
I0203 22:19:50.652712 139774434195200 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.133619785308838, loss=3.6480839252471924
I0203 22:20:38.361732 139774417409792 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.9680802226066589, loss=4.4546685218811035
I0203 22:21:25.531201 139774434195200 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.2477535009384155, loss=2.426408290863037
I0203 22:22:13.439996 139774417409792 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.9682950377464294, loss=4.448554039001465
I0203 22:23:00.687437 139774434195200 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.167478084564209, loss=4.621560096740723
I0203 22:23:48.641782 139774417409792 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.3147873878479004, loss=2.4964723587036133
I0203 22:24:30.446626 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:24:42.440348 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:25:21.674010 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:25:23.281059 139936116377408 submission_runner.py:408] Time since start: 33636.63s, 	Step: 64090, 	{'train/accuracy': 0.6555468440055847, 'train/loss': 1.3943846225738525, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.6383819580078125, 'validation/num_examples': 50000, 'test/accuracy': 0.48660001158714294, 'test/loss': 2.292811155319214, 'test/num_examples': 10000, 'score': 29868.319049596786, 'total_duration': 33636.6328959465, 'accumulated_submission_time': 29868.319049596786, 'accumulated_eval_time': 3761.9516339302063, 'accumulated_logging_time': 3.0155563354492188}
I0203 22:25:23.313153 139774434195200 logging_writer.py:48] [64090] accumulated_eval_time=3761.951634, accumulated_logging_time=3.015556, accumulated_submission_time=29868.319050, global_step=64090, preemption_count=0, score=29868.319050, test/accuracy=0.486600, test/loss=2.292811, test/num_examples=10000, total_duration=33636.632896, train/accuracy=0.655547, train/loss=1.394385, validation/accuracy=0.605580, validation/loss=1.638382, validation/num_examples=50000
I0203 22:25:27.876932 139774417409792 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.2451306581497192, loss=2.800426721572876
I0203 22:26:11.633679 139774434195200 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1161961555480957, loss=5.045912742614746
I0203 22:26:58.592435 139774417409792 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1235531568527222, loss=2.3963117599487305
I0203 22:27:45.557864 139774434195200 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9831941723823547, loss=4.884792804718018
I0203 22:28:32.779240 139774417409792 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1746944189071655, loss=2.5440189838409424
I0203 22:29:19.625871 139774434195200 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.21615469455719, loss=2.9044220447540283
I0203 22:30:06.714125 139774417409792 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2835698127746582, loss=2.5719499588012695
I0203 22:30:53.589073 139774434195200 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.145024061203003, loss=2.9389166831970215
I0203 22:31:41.016003 139774417409792 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.198623538017273, loss=3.5532259941101074
I0203 22:32:23.487184 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:32:35.264234 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:33:13.667532 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:33:15.275824 139936116377408 submission_runner.py:408] Time since start: 34108.63s, 	Step: 64992, 	{'train/accuracy': 0.6604882478713989, 'train/loss': 1.4079033136367798, 'validation/accuracy': 0.6082199811935425, 'validation/loss': 1.6526474952697754, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.3205599784851074, 'test/num_examples': 10000, 'score': 30288.43496155739, 'total_duration': 34108.627766132355, 'accumulated_submission_time': 30288.43496155739, 'accumulated_eval_time': 3813.7393848896027, 'accumulated_logging_time': 3.059473991394043}
I0203 22:33:15.307998 139774434195200 logging_writer.py:48] [64992] accumulated_eval_time=3813.739385, accumulated_logging_time=3.059474, accumulated_submission_time=30288.434962, global_step=64992, preemption_count=0, score=30288.434962, test/accuracy=0.484400, test/loss=2.320560, test/num_examples=10000, total_duration=34108.627766, train/accuracy=0.660488, train/loss=1.407903, validation/accuracy=0.608220, validation/loss=1.652647, validation/num_examples=50000
I0203 22:33:19.044494 139774417409792 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2096443176269531, loss=4.756346702575684
I0203 22:34:02.554986 139774434195200 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.0329965353012085, loss=3.595731258392334
I0203 22:34:48.955911 139774417409792 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.265312910079956, loss=2.594916582107544
I0203 22:35:36.040818 139774434195200 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1240671873092651, loss=5.136566638946533
I0203 22:36:23.220478 139774417409792 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1770089864730835, loss=2.533421039581299
I0203 22:37:10.318468 139774434195200 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.2446205615997314, loss=2.606321334838867
I0203 22:37:57.273639 139774417409792 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.2049901485443115, loss=2.84854793548584
I0203 22:38:44.255497 139774434195200 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.370477318763733, loss=2.51054048538208
I0203 22:39:31.444891 139774417409792 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.545715570449829, loss=2.5132429599761963
I0203 22:40:15.683173 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:40:27.601589 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:41:08.308834 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:41:09.910377 139936116377408 submission_runner.py:408] Time since start: 34583.26s, 	Step: 65896, 	{'train/accuracy': 0.6743749976158142, 'train/loss': 1.3156598806381226, 'validation/accuracy': 0.6094799637794495, 'validation/loss': 1.6173152923583984, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.2992825508117676, 'test/num_examples': 10000, 'score': 30708.749766349792, 'total_duration': 34583.26203036308, 'accumulated_submission_time': 30708.749766349792, 'accumulated_eval_time': 3867.965404987335, 'accumulated_logging_time': 3.1049630641937256}
I0203 22:41:09.940344 139774434195200 logging_writer.py:48] [65896] accumulated_eval_time=3867.965405, accumulated_logging_time=3.104963, accumulated_submission_time=30708.749766, global_step=65896, preemption_count=0, score=30708.749766, test/accuracy=0.489100, test/loss=2.299283, test/num_examples=10000, total_duration=34583.262030, train/accuracy=0.674375, train/loss=1.315660, validation/accuracy=0.609480, validation/loss=1.617315, validation/num_examples=50000
I0203 22:41:12.095495 139774417409792 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.1234400272369385, loss=3.185976266860962
I0203 22:41:55.481325 139774434195200 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.2639764547348022, loss=2.396257162094116
I0203 22:42:42.236995 139774417409792 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.033827304840088, loss=4.517416477203369
I0203 22:43:29.545026 139774434195200 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.2530460357666016, loss=2.8497233390808105
I0203 22:44:16.788838 139774417409792 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3945810794830322, loss=2.5198519229888916
I0203 22:45:03.647832 139774434195200 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0914177894592285, loss=2.9247021675109863
I0203 22:45:50.325512 139774417409792 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.279353141784668, loss=2.3924570083618164
I0203 22:46:37.011547 139774434195200 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1326173543930054, loss=2.56974196434021
I0203 22:47:24.053337 139774417409792 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.109300136566162, loss=5.037481307983398
I0203 22:48:09.967083 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:48:21.783474 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:49:01.595950 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:49:03.212719 139936116377408 submission_runner.py:408] Time since start: 35056.56s, 	Step: 66799, 	{'train/accuracy': 0.6649413704872131, 'train/loss': 1.3905673027038574, 'validation/accuracy': 0.613599956035614, 'validation/loss': 1.6307467222213745, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.2967209815979004, 'test/num_examples': 10000, 'score': 31128.717413187027, 'total_duration': 35056.56464314461, 'accumulated_submission_time': 31128.717413187027, 'accumulated_eval_time': 3921.2101068496704, 'accumulated_logging_time': 3.146657943725586}
I0203 22:49:03.241577 139774434195200 logging_writer.py:48] [66799] accumulated_eval_time=3921.210107, accumulated_logging_time=3.146658, accumulated_submission_time=31128.717413, global_step=66799, preemption_count=0, score=31128.717413, test/accuracy=0.484100, test/loss=2.296721, test/num_examples=10000, total_duration=35056.564643, train/accuracy=0.664941, train/loss=1.390567, validation/accuracy=0.613600, validation/loss=1.630747, validation/num_examples=50000
I0203 22:49:04.076456 139774417409792 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.1988626718521118, loss=3.0539119243621826
I0203 22:49:47.391386 139774434195200 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.2061535120010376, loss=4.150973320007324
I0203 22:50:34.309643 139774417409792 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.9826974868774414, loss=4.834098815917969
I0203 22:51:21.301145 139774434195200 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.2626214027404785, loss=2.527292251586914
I0203 22:52:08.329914 139774417409792 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.0910180807113647, loss=3.94012713432312
I0203 22:52:55.229142 139774434195200 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.3325576782226562, loss=4.048506736755371
I0203 22:53:42.279219 139774417409792 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.23349130153656, loss=2.9085843563079834
I0203 22:54:29.714645 139774434195200 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.2996251583099365, loss=2.5171148777008057
I0203 22:55:16.902837 139774417409792 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.254689335823059, loss=2.604402780532837
I0203 22:56:03.240287 139936116377408 spec.py:321] Evaluating on the training split.
I0203 22:56:15.195604 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 22:56:55.182805 139936116377408 spec.py:349] Evaluating on the test split.
I0203 22:56:56.790939 139936116377408 submission_runner.py:408] Time since start: 35530.14s, 	Step: 67700, 	{'train/accuracy': 0.666308581829071, 'train/loss': 1.3803210258483887, 'validation/accuracy': 0.6187199950218201, 'validation/loss': 1.6084468364715576, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.2668533325195312, 'test/num_examples': 10000, 'score': 31548.65906047821, 'total_duration': 35530.14267539978, 'accumulated_submission_time': 31548.65906047821, 'accumulated_eval_time': 3974.759640932083, 'accumulated_logging_time': 3.186652421951294}
I0203 22:56:56.827820 139774434195200 logging_writer.py:48] [67700] accumulated_eval_time=3974.759641, accumulated_logging_time=3.186652, accumulated_submission_time=31548.659060, global_step=67700, preemption_count=0, score=31548.659060, test/accuracy=0.495300, test/loss=2.266853, test/num_examples=10000, total_duration=35530.142675, train/accuracy=0.666309, train/loss=1.380321, validation/accuracy=0.618720, validation/loss=1.608447, validation/num_examples=50000
I0203 22:56:57.248896 139774417409792 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.2732834815979004, loss=2.4987804889678955
I0203 22:57:40.309357 139774434195200 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.2998641729354858, loss=2.450993061065674
I0203 22:58:27.097755 139774417409792 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.151089072227478, loss=5.170456886291504
I0203 22:59:14.710533 139774434195200 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1500791311264038, loss=3.211638927459717
I0203 23:00:01.913305 139774417409792 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.1162095069885254, loss=3.159336566925049
I0203 23:00:49.052309 139774434195200 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.14779531955719, loss=2.9906387329101562
I0203 23:01:36.136615 139774417409792 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.1698415279388428, loss=2.802774429321289
I0203 23:02:23.048559 139774434195200 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.107553482055664, loss=3.6053991317749023
I0203 23:03:10.155169 139774417409792 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.3898569345474243, loss=2.484955310821533
I0203 23:03:56.953068 139774434195200 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.437191367149353, loss=2.4579262733459473
I0203 23:03:56.964816 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:04:09.141965 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:04:45.641119 139936116377408 spec.py:349] Evaluating on the test split.
I0203 23:04:47.247579 139936116377408 submission_runner.py:408] Time since start: 36000.60s, 	Step: 68601, 	{'train/accuracy': 0.6759960651397705, 'train/loss': 1.306284785270691, 'validation/accuracy': 0.6125800013542175, 'validation/loss': 1.6162729263305664, 'validation/num_examples': 50000, 'test/accuracy': 0.491100013256073, 'test/loss': 2.2899532318115234, 'test/num_examples': 10000, 'score': 31968.73847270012, 'total_duration': 36000.59927082062, 'accumulated_submission_time': 31968.73847270012, 'accumulated_eval_time': 4025.041212797165, 'accumulated_logging_time': 3.2343225479125977}
I0203 23:04:47.273820 139774417409792 logging_writer.py:48] [68601] accumulated_eval_time=4025.041213, accumulated_logging_time=3.234323, accumulated_submission_time=31968.738473, global_step=68601, preemption_count=0, score=31968.738473, test/accuracy=0.491100, test/loss=2.289953, test/num_examples=10000, total_duration=36000.599271, train/accuracy=0.675996, train/loss=1.306285, validation/accuracy=0.612580, validation/loss=1.616273, validation/num_examples=50000
I0203 23:05:30.164591 139774434195200 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.264542818069458, loss=2.433262825012207
I0203 23:06:17.353581 139774417409792 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.0867490768432617, loss=4.962208271026611
I0203 23:07:04.594932 139774434195200 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.4362596273422241, loss=2.5762624740600586
I0203 23:07:51.394301 139774417409792 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.260562539100647, loss=2.391846179962158
I0203 23:08:38.308913 139774434195200 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.5729224681854248, loss=2.4999852180480957
I0203 23:09:25.617556 139774417409792 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3269476890563965, loss=2.3998987674713135
I0203 23:10:12.627844 139774434195200 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.244372010231018, loss=2.488077402114868
I0203 23:10:59.625424 139774417409792 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.382491111755371, loss=2.431076765060425
I0203 23:11:46.818766 139774434195200 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.2918837070465088, loss=2.5873875617980957
I0203 23:11:47.325590 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:11:59.175367 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:12:40.522121 139936116377408 spec.py:349] Evaluating on the test split.
I0203 23:12:42.122964 139936116377408 submission_runner.py:408] Time since start: 36475.47s, 	Step: 69503, 	{'train/accuracy': 0.6673437356948853, 'train/loss': 1.392255187034607, 'validation/accuracy': 0.6157999634742737, 'validation/loss': 1.6166682243347168, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.2810490131378174, 'test/num_examples': 10000, 'score': 32388.73214316368, 'total_duration': 36475.47456264496, 'accumulated_submission_time': 32388.73214316368, 'accumulated_eval_time': 4079.8373177051544, 'accumulated_logging_time': 3.27199387550354}
I0203 23:12:42.152635 139774417409792 logging_writer.py:48] [69503] accumulated_eval_time=4079.837318, accumulated_logging_time=3.271994, accumulated_submission_time=32388.732143, global_step=69503, preemption_count=0, score=32388.732143, test/accuracy=0.495600, test/loss=2.281049, test/num_examples=10000, total_duration=36475.474563, train/accuracy=0.667344, train/loss=1.392255, validation/accuracy=0.615800, validation/loss=1.616668, validation/num_examples=50000
I0203 23:13:24.561492 139774434195200 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.346256971359253, loss=2.3961448669433594
I0203 23:14:11.741603 139774417409792 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2170147895812988, loss=2.452324867248535
I0203 23:14:58.740315 139774434195200 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.2206186056137085, loss=3.638861656188965
I0203 23:15:46.246050 139774417409792 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.400269627571106, loss=2.593043565750122
I0203 23:16:33.849168 139774434195200 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2816784381866455, loss=2.4324564933776855
I0203 23:17:21.528153 139774417409792 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.2371716499328613, loss=2.5701513290405273
I0203 23:18:09.041555 139774434195200 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.4036532640457153, loss=2.9580042362213135
I0203 23:18:56.431454 139774417409792 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1233549118041992, loss=3.7096471786499023
I0203 23:19:42.308358 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:19:54.134772 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:20:34.650612 139936116377408 spec.py:349] Evaluating on the test split.
I0203 23:20:36.261733 139936116377408 submission_runner.py:408] Time since start: 36949.61s, 	Step: 70398, 	{'train/accuracy': 0.6661718487739563, 'train/loss': 1.4046850204467773, 'validation/accuracy': 0.6162399649620056, 'validation/loss': 1.6451646089553833, 'validation/num_examples': 50000, 'test/accuracy': 0.4962000250816345, 'test/loss': 2.3044371604919434, 'test/num_examples': 10000, 'score': 32808.83074641228, 'total_duration': 36949.61338496208, 'accumulated_submission_time': 32808.83074641228, 'accumulated_eval_time': 4133.789508104324, 'accumulated_logging_time': 3.3121135234832764}
I0203 23:20:36.298466 139774434195200 logging_writer.py:48] [70398] accumulated_eval_time=4133.789508, accumulated_logging_time=3.312114, accumulated_submission_time=32808.830746, global_step=70398, preemption_count=0, score=32808.830746, test/accuracy=0.496200, test/loss=2.304437, test/num_examples=10000, total_duration=36949.613385, train/accuracy=0.666172, train/loss=1.404685, validation/accuracy=0.616240, validation/loss=1.645165, validation/num_examples=50000
I0203 23:20:37.547084 139774417409792 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.2191656827926636, loss=2.6513962745666504
I0203 23:21:20.833598 139774434195200 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1854275465011597, loss=2.460449695587158
I0203 23:22:07.672669 139774417409792 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.42705237865448, loss=2.5163002014160156
I0203 23:22:54.871453 139774434195200 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.2485437393188477, loss=2.5365989208221436
I0203 23:23:41.782758 139774417409792 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2198371887207031, loss=2.3693225383758545
I0203 23:24:28.968209 139774434195200 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.091926097869873, loss=3.9641475677490234
I0203 23:25:16.171514 139774417409792 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.0964703559875488, loss=5.122318744659424
I0203 23:26:03.197635 139774434195200 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2391293048858643, loss=2.4692978858947754
I0203 23:26:50.059332 139774417409792 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3477107286453247, loss=2.6689205169677734
I0203 23:27:36.506792 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:27:48.298324 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:28:27.694133 139936116377408 spec.py:349] Evaluating on the test split.
I0203 23:28:29.301842 139936116377408 submission_runner.py:408] Time since start: 37422.65s, 	Step: 71300, 	{'train/accuracy': 0.671875, 'train/loss': 1.3597897291183472, 'validation/accuracy': 0.6108399629592896, 'validation/loss': 1.6405349969863892, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.279672861099243, 'test/num_examples': 10000, 'score': 33228.976645469666, 'total_duration': 37422.65373325348, 'accumulated_submission_time': 33228.976645469666, 'accumulated_eval_time': 4186.583588838577, 'accumulated_logging_time': 3.3608882427215576}
I0203 23:28:29.333715 139774434195200 logging_writer.py:48] [71300] accumulated_eval_time=4186.583589, accumulated_logging_time=3.360888, accumulated_submission_time=33228.976645, global_step=71300, preemption_count=0, score=33228.976645, test/accuracy=0.491500, test/loss=2.279673, test/num_examples=10000, total_duration=37422.653733, train/accuracy=0.671875, train/loss=1.359790, validation/accuracy=0.610840, validation/loss=1.640535, validation/num_examples=50000
I0203 23:28:29.760068 139774417409792 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3131051063537598, loss=2.6880598068237305
I0203 23:29:12.964624 139774434195200 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0990006923675537, loss=3.6836745738983154
I0203 23:29:59.556889 139774417409792 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1922177076339722, loss=2.8827366828918457
I0203 23:30:46.833570 139774434195200 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.13739013671875, loss=3.6771278381347656
I0203 23:31:33.682946 139774417409792 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.2957847118377686, loss=2.303905487060547
I0203 23:32:20.900198 139774434195200 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.9932348132133484, loss=4.270982265472412
I0203 23:33:08.126493 139774417409792 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.4644311666488647, loss=2.420621871948242
I0203 23:33:54.814235 139774434195200 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.2771954536437988, loss=2.898289203643799
I0203 23:34:42.086562 139774417409792 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.0194398164749146, loss=4.892860412597656
I0203 23:35:29.065127 139774434195200 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.316028118133545, loss=2.495253086090088
I0203 23:35:29.663827 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:35:41.640578 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:36:20.056169 139936116377408 spec.py:349] Evaluating on the test split.
I0203 23:36:21.667233 139936116377408 submission_runner.py:408] Time since start: 37895.02s, 	Step: 72203, 	{'train/accuracy': 0.6790234446525574, 'train/loss': 1.29867684841156, 'validation/accuracy': 0.6268599629402161, 'validation/loss': 1.5448366403579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.2146520614624023, 'test/num_examples': 10000, 'score': 33649.24731469154, 'total_duration': 37895.018824100494, 'accumulated_submission_time': 33649.24731469154, 'accumulated_eval_time': 4238.585715770721, 'accumulated_logging_time': 3.405482292175293}
I0203 23:36:21.698772 139774417409792 logging_writer.py:48] [72203] accumulated_eval_time=4238.585716, accumulated_logging_time=3.405482, accumulated_submission_time=33649.247315, global_step=72203, preemption_count=0, score=33649.247315, test/accuracy=0.504500, test/loss=2.214652, test/num_examples=10000, total_duration=37895.018824, train/accuracy=0.679023, train/loss=1.298677, validation/accuracy=0.626860, validation/loss=1.544837, validation/num_examples=50000
I0203 23:37:03.765414 139774434195200 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.158708930015564, loss=3.2133560180664062
I0203 23:37:50.348595 139774417409792 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.9804037809371948, loss=4.27109956741333
I0203 23:38:37.362869 139774434195200 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2591255903244019, loss=2.448352575302124
I0203 23:39:24.614124 139774417409792 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.1434831619262695, loss=3.451188802719116
I0203 23:40:11.036115 139774434195200 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.188514232635498, loss=2.5999948978424072
I0203 23:40:57.954623 139774417409792 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.9975666403770447, loss=4.545848846435547
I0203 23:41:44.937227 139774434195200 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.0922025442123413, loss=5.071218967437744
I0203 23:42:31.696890 139774417409792 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.1081297397613525, loss=3.9729838371276855
I0203 23:43:18.663636 139774434195200 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2238386869430542, loss=2.3533549308776855
I0203 23:43:22.013105 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:43:33.939797 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:44:13.271487 139936116377408 spec.py:349] Evaluating on the test split.
I0203 23:44:14.878933 139936116377408 submission_runner.py:408] Time since start: 38368.23s, 	Step: 73109, 	{'train/accuracy': 0.6642382740974426, 'train/loss': 1.3958392143249512, 'validation/accuracy': 0.6126999855041504, 'validation/loss': 1.638340711593628, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.272181510925293, 'test/num_examples': 10000, 'score': 34069.50347137451, 'total_duration': 38368.23072266579, 'accumulated_submission_time': 34069.50347137451, 'accumulated_eval_time': 4291.450464725494, 'accumulated_logging_time': 3.4479641914367676}
I0203 23:44:14.911040 139774417409792 logging_writer.py:48] [73109] accumulated_eval_time=4291.450465, accumulated_logging_time=3.447964, accumulated_submission_time=34069.503471, global_step=73109, preemption_count=0, score=34069.503471, test/accuracy=0.496100, test/loss=2.272182, test/num_examples=10000, total_duration=38368.230723, train/accuracy=0.664238, train/loss=1.395839, validation/accuracy=0.612700, validation/loss=1.638341, validation/num_examples=50000
I0203 23:44:54.194849 139774434195200 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.043308138847351, loss=5.051923751831055
I0203 23:45:40.891480 139774417409792 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3853703737258911, loss=2.668931245803833
I0203 23:46:27.771797 139774434195200 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.2250627279281616, loss=2.6241042613983154
I0203 23:47:14.745572 139774417409792 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.274949312210083, loss=2.5590219497680664
I0203 23:48:01.859544 139774434195200 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.2587449550628662, loss=2.5283327102661133
I0203 23:48:48.848358 139774417409792 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.2006562948226929, loss=2.739386796951294
I0203 23:49:36.054789 139774434195200 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.2838554382324219, loss=2.4260478019714355
I0203 23:50:23.549603 139774417409792 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.0271655321121216, loss=4.858547210693359
I0203 23:51:10.698364 139774434195200 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2309963703155518, loss=2.365452766418457
I0203 23:51:15.134111 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:51:27.119934 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:52:06.857328 139936116377408 spec.py:349] Evaluating on the test split.
I0203 23:52:08.469555 139936116377408 submission_runner.py:408] Time since start: 38841.82s, 	Step: 74011, 	{'train/accuracy': 0.6847460865974426, 'train/loss': 1.295527458190918, 'validation/accuracy': 0.6244399547576904, 'validation/loss': 1.5823724269866943, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.2567830085754395, 'test/num_examples': 10000, 'score': 34489.668355703354, 'total_duration': 38841.821252822876, 'accumulated_submission_time': 34489.668355703354, 'accumulated_eval_time': 4344.784727811813, 'accumulated_logging_time': 3.4918909072875977}
I0203 23:52:08.496468 139774417409792 logging_writer.py:48] [74011] accumulated_eval_time=4344.784728, accumulated_logging_time=3.491891, accumulated_submission_time=34489.668356, global_step=74011, preemption_count=0, score=34489.668356, test/accuracy=0.500300, test/loss=2.256783, test/num_examples=10000, total_duration=38841.821253, train/accuracy=0.684746, train/loss=1.295527, validation/accuracy=0.624440, validation/loss=1.582372, validation/num_examples=50000
I0203 23:52:46.931152 139774434195200 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.1810170412063599, loss=5.125278472900391
I0203 23:53:33.451205 139774417409792 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.430576205253601, loss=2.5301623344421387
I0203 23:54:20.339024 139774434195200 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.304581642150879, loss=2.4256110191345215
I0203 23:55:07.450664 139774417409792 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.1293519735336304, loss=3.8974623680114746
I0203 23:55:54.431859 139774434195200 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.2683320045471191, loss=2.4189515113830566
I0203 23:56:41.228995 139774417409792 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2187141180038452, loss=3.107102394104004
I0203 23:57:28.429762 139774434195200 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.310689926147461, loss=5.033442974090576
I0203 23:58:15.703204 139774417409792 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2442858219146729, loss=2.4825828075408936
I0203 23:59:02.591790 139774434195200 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.0987156629562378, loss=4.1762518882751465
I0203 23:59:08.844984 139936116377408 spec.py:321] Evaluating on the training split.
I0203 23:59:20.692157 139936116377408 spec.py:333] Evaluating on the validation split.
I0203 23:59:58.539321 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:00:00.149298 139936116377408 submission_runner.py:408] Time since start: 39313.50s, 	Step: 74915, 	{'train/accuracy': 0.6771484017372131, 'train/loss': 1.3254854679107666, 'validation/accuracy': 0.6175999641418457, 'validation/loss': 1.5920697450637817, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.259209156036377, 'test/num_examples': 10000, 'score': 34909.960586071014, 'total_duration': 39313.50108551979, 'accumulated_submission_time': 34909.960586071014, 'accumulated_eval_time': 4396.087966918945, 'accumulated_logging_time': 3.528670072555542}
I0204 00:00:00.180711 139774417409792 logging_writer.py:48] [74915] accumulated_eval_time=4396.087967, accumulated_logging_time=3.528670, accumulated_submission_time=34909.960586, global_step=74915, preemption_count=0, score=34909.960586, test/accuracy=0.499500, test/loss=2.259209, test/num_examples=10000, total_duration=39313.501086, train/accuracy=0.677148, train/loss=1.325485, validation/accuracy=0.617600, validation/loss=1.592070, validation/num_examples=50000
I0204 00:00:36.763913 139774434195200 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.1898857355117798, loss=4.68166971206665
I0204 00:01:23.780984 139774417409792 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.2309497594833374, loss=2.468991994857788
I0204 00:02:10.594448 139774434195200 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.3336567878723145, loss=2.3476691246032715
I0204 00:02:57.368040 139774417409792 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.5117250680923462, loss=2.3812458515167236
I0204 00:03:44.390796 139774434195200 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.5029594898223877, loss=2.4489684104919434
I0204 00:04:31.431201 139774417409792 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.3832401037216187, loss=2.56982684135437
I0204 00:05:18.415546 139774434195200 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.2725311517715454, loss=3.1428399085998535
I0204 00:06:05.518238 139774417409792 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.0773932933807373, loss=3.7094602584838867
I0204 00:06:52.266295 139774434195200 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.4414422512054443, loss=2.716848850250244
I0204 00:07:00.414777 139936116377408 spec.py:321] Evaluating on the training split.
I0204 00:07:12.452773 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 00:07:49.333668 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:07:50.947645 139936116377408 submission_runner.py:408] Time since start: 39784.30s, 	Step: 75819, 	{'train/accuracy': 0.6713671684265137, 'train/loss': 1.3427538871765137, 'validation/accuracy': 0.6200799942016602, 'validation/loss': 1.5841830968856812, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.25400972366333, 'test/num_examples': 10000, 'score': 35330.13725447655, 'total_duration': 39784.29939389229, 'accumulated_submission_time': 35330.13725447655, 'accumulated_eval_time': 4446.619699478149, 'accumulated_logging_time': 3.5707647800445557}
I0204 00:07:50.982624 139774417409792 logging_writer.py:48] [75819] accumulated_eval_time=4446.619699, accumulated_logging_time=3.570765, accumulated_submission_time=35330.137254, global_step=75819, preemption_count=0, score=35330.137254, test/accuracy=0.498000, test/loss=2.254010, test/num_examples=10000, total_duration=39784.299394, train/accuracy=0.671367, train/loss=1.342754, validation/accuracy=0.620080, validation/loss=1.584183, validation/num_examples=50000
I0204 00:08:25.732277 139774434195200 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.2829481363296509, loss=2.3520145416259766
I0204 00:09:12.439159 139774417409792 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.2910056114196777, loss=2.795943260192871
I0204 00:09:59.595651 139774434195200 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.3101853132247925, loss=2.248408794403076
I0204 00:10:46.657851 139774417409792 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.3302927017211914, loss=2.4354970455169678
I0204 00:11:34.242235 139774434195200 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.3258326053619385, loss=2.482546806335449
I0204 00:12:21.528491 139774417409792 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.039863109588623, loss=4.583538055419922
I0204 00:13:08.349876 139774434195200 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.051889181137085, loss=4.017549991607666
I0204 00:13:55.624654 139774417409792 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.1823933124542236, loss=4.925175189971924
I0204 00:14:42.799368 139774434195200 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.2529133558273315, loss=2.561549663543701
I0204 00:14:51.236994 139936116377408 spec.py:321] Evaluating on the training split.
I0204 00:15:03.421084 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 00:15:41.876482 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:15:43.493578 139936116377408 submission_runner.py:408] Time since start: 40256.85s, 	Step: 76720, 	{'train/accuracy': 0.6835741996765137, 'train/loss': 1.2883057594299316, 'validation/accuracy': 0.6281999945640564, 'validation/loss': 1.5592522621154785, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.233535051345825, 'test/num_examples': 10000, 'score': 35750.3324637413, 'total_duration': 40256.8454182148, 'accumulated_submission_time': 35750.3324637413, 'accumulated_eval_time': 4498.87525844574, 'accumulated_logging_time': 3.6172690391540527}
I0204 00:15:43.521048 139774417409792 logging_writer.py:48] [76720] accumulated_eval_time=4498.875258, accumulated_logging_time=3.617269, accumulated_submission_time=35750.332464, global_step=76720, preemption_count=0, score=35750.332464, test/accuracy=0.504500, test/loss=2.233535, test/num_examples=10000, total_duration=40256.845418, train/accuracy=0.683574, train/loss=1.288306, validation/accuracy=0.628200, validation/loss=1.559252, validation/num_examples=50000
I0204 00:16:17.722121 139774434195200 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.1113405227661133, loss=3.940859794616699
I0204 00:17:04.113486 139774417409792 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.153756856918335, loss=4.587769508361816
I0204 00:17:50.992229 139774434195200 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4402118921279907, loss=2.3578829765319824
I0204 00:18:37.691523 139774417409792 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.3315258026123047, loss=2.289374589920044
I0204 00:19:24.518481 139774434195200 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2611963748931885, loss=2.377258777618408
I0204 00:20:10.960485 139774417409792 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.2800549268722534, loss=2.443673849105835
I0204 00:20:57.888587 139774434195200 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.1883050203323364, loss=2.6566686630249023
I0204 00:21:44.686302 139774417409792 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.3823124170303345, loss=2.332465887069702
I0204 00:22:31.639400 139774434195200 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.1924548149108887, loss=2.838571548461914
I0204 00:22:43.767007 139936116377408 spec.py:321] Evaluating on the training split.
I0204 00:22:55.552136 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 00:23:37.410009 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:23:39.014325 139936116377408 submission_runner.py:408] Time since start: 40732.37s, 	Step: 77628, 	{'train/accuracy': 0.7032226324081421, 'train/loss': 1.1986013650894165, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.526818037033081, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.190584659576416, 'test/num_examples': 10000, 'score': 36170.52016687393, 'total_duration': 40732.36606168747, 'accumulated_submission_time': 36170.52016687393, 'accumulated_eval_time': 4554.121444702148, 'accumulated_logging_time': 3.6560237407684326}
I0204 00:23:39.046909 139774417409792 logging_writer.py:48] [77628] accumulated_eval_time=4554.121445, accumulated_logging_time=3.656024, accumulated_submission_time=36170.520167, global_step=77628, preemption_count=0, score=36170.520167, test/accuracy=0.505800, test/loss=2.190585, test/num_examples=10000, total_duration=40732.366062, train/accuracy=0.703223, train/loss=1.198601, validation/accuracy=0.632680, validation/loss=1.526818, validation/num_examples=50000
I0204 00:24:09.545435 139774434195200 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.349401593208313, loss=2.5597639083862305
I0204 00:24:56.207868 139774417409792 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.0544296503067017, loss=4.889174461364746
I0204 00:25:43.473469 139774434195200 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.2660707235336304, loss=2.942932605743408
I0204 00:26:30.332546 139774417409792 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.373113751411438, loss=2.3621034622192383
I0204 00:27:17.639566 139774434195200 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3403714895248413, loss=2.430860757827759
I0204 00:28:04.627779 139774417409792 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3686244487762451, loss=2.4600942134857178
I0204 00:28:51.616657 139774434195200 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.2151298522949219, loss=4.0410308837890625
I0204 00:29:38.778827 139774417409792 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.50906240940094, loss=2.3508269786834717
I0204 00:30:25.932818 139774434195200 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.334396481513977, loss=2.3946704864501953
I0204 00:30:39.244477 139936116377408 spec.py:321] Evaluating on the training split.
I0204 00:30:51.097712 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 00:31:29.674426 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:31:31.289337 139936116377408 submission_runner.py:408] Time since start: 41204.64s, 	Step: 78530, 	{'train/accuracy': 0.6783398389816284, 'train/loss': 1.3336890935897827, 'validation/accuracy': 0.6248399615287781, 'validation/loss': 1.5802377462387085, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.228025197982788, 'test/num_examples': 10000, 'score': 36590.65875458717, 'total_duration': 41204.64127254486, 'accumulated_submission_time': 36590.65875458717, 'accumulated_eval_time': 4606.165371894836, 'accumulated_logging_time': 3.701697587966919}
I0204 00:31:31.322049 139774417409792 logging_writer.py:48] [78530] accumulated_eval_time=4606.165372, accumulated_logging_time=3.701698, accumulated_submission_time=36590.658755, global_step=78530, preemption_count=0, score=36590.658755, test/accuracy=0.503400, test/loss=2.228025, test/num_examples=10000, total_duration=41204.641273, train/accuracy=0.678340, train/loss=1.333689, validation/accuracy=0.624840, validation/loss=1.580238, validation/num_examples=50000
I0204 00:32:00.806196 139774434195200 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.1241463422775269, loss=3.045424699783325
I0204 00:32:47.612651 139774417409792 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.3819931745529175, loss=2.400691509246826
I0204 00:33:34.755180 139774434195200 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.0293512344360352, loss=4.729495048522949
I0204 00:34:21.982966 139774417409792 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.008932113647461, loss=5.000268936157227
I0204 00:35:08.839712 139774434195200 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.0210442543029785, loss=3.9528257846832275
I0204 00:35:55.782168 139774417409792 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.365864872932434, loss=2.3902854919433594
I0204 00:36:42.815732 139774434195200 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.0491256713867188, loss=5.006983757019043
I0204 00:37:29.994702 139774417409792 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.7094075679779053, loss=2.4738047122955322
I0204 00:38:16.990044 139774434195200 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.4032752513885498, loss=2.3415279388427734
I0204 00:38:31.728640 139936116377408 spec.py:321] Evaluating on the training split.
I0204 00:38:43.576037 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 00:39:21.073935 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:39:22.678158 139936116377408 submission_runner.py:408] Time since start: 41676.03s, 	Step: 79433, 	{'train/accuracy': 0.6779101490974426, 'train/loss': 1.3415353298187256, 'validation/accuracy': 0.6250999569892883, 'validation/loss': 1.593327283859253, 'validation/num_examples': 50000, 'test/accuracy': 0.5078999996185303, 'test/loss': 2.239119291305542, 'test/num_examples': 10000, 'score': 37011.007297992706, 'total_duration': 41676.029999256134, 'accumulated_submission_time': 37011.007297992706, 'accumulated_eval_time': 4657.113859415054, 'accumulated_logging_time': 3.745133638381958}
I0204 00:39:22.705504 139774417409792 logging_writer.py:48] [79433] accumulated_eval_time=4657.113859, accumulated_logging_time=3.745134, accumulated_submission_time=37011.007298, global_step=79433, preemption_count=0, score=37011.007298, test/accuracy=0.507900, test/loss=2.239119, test/num_examples=10000, total_duration=41676.029999, train/accuracy=0.677910, train/loss=1.341535, validation/accuracy=0.625100, validation/loss=1.593327, validation/num_examples=50000
I0204 00:39:50.973005 139774434195200 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.2710267305374146, loss=2.3230905532836914
I0204 00:40:37.537236 139774417409792 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.0325037240982056, loss=4.277499675750732
I0204 00:41:24.746908 139774434195200 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.2825149297714233, loss=4.89707612991333
I0204 00:42:11.954072 139774417409792 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.3825079202651978, loss=2.5018911361694336
I0204 00:42:59.255970 139774434195200 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.3842525482177734, loss=2.425729751586914
I0204 00:43:46.344189 139774417409792 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.5132570266723633, loss=2.2743613719940186
I0204 00:44:33.746665 139774434195200 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.2876313924789429, loss=2.6042444705963135
I0204 00:45:20.966152 139774417409792 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.123104453086853, loss=3.3349971771240234
I0204 00:46:08.468999 139774434195200 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4034637212753296, loss=2.3840994834899902
I0204 00:46:23.156656 139936116377408 spec.py:321] Evaluating on the training split.
I0204 00:46:35.106968 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 00:47:16.202772 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:47:17.807901 139936116377408 submission_runner.py:408] Time since start: 42151.16s, 	Step: 80333, 	{'train/accuracy': 0.712890625, 'train/loss': 1.1509590148925781, 'validation/accuracy': 0.6348599791526794, 'validation/loss': 1.51292884349823, 'validation/num_examples': 50000, 'test/accuracy': 0.5133000016212463, 'test/loss': 2.168611526489258, 'test/num_examples': 10000, 'score': 37431.40174984932, 'total_duration': 42151.15966916084, 'accumulated_submission_time': 37431.40174984932, 'accumulated_eval_time': 4711.764025211334, 'accumulated_logging_time': 3.782468557357788}
I0204 00:47:17.843188 139774417409792 logging_writer.py:48] [80333] accumulated_eval_time=4711.764025, accumulated_logging_time=3.782469, accumulated_submission_time=37431.401750, global_step=80333, preemption_count=0, score=37431.401750, test/accuracy=0.513300, test/loss=2.168612, test/num_examples=10000, total_duration=42151.159669, train/accuracy=0.712891, train/loss=1.150959, validation/accuracy=0.634860, validation/loss=1.512929, validation/num_examples=50000
I0204 00:47:46.231356 139774434195200 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.1779887676239014, loss=4.010756492614746
I0204 00:48:32.648869 139774417409792 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.3177826404571533, loss=4.795717716217041
I0204 00:49:19.773894 139774434195200 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.6599454879760742, loss=2.4592320919036865
I0204 00:50:06.737900 139774417409792 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.371214509010315, loss=2.436302661895752
I0204 00:50:53.477567 139774434195200 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.2215574979782104, loss=3.7676305770874023
I0204 00:51:40.627382 139774417409792 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.4034343957901, loss=2.3056857585906982
I0204 00:52:27.852112 139774434195200 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.3310695886611938, loss=2.8016510009765625
I0204 00:53:14.989247 139774417409792 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.421705722808838, loss=2.147716522216797
I0204 00:54:01.769469 139774434195200 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.2311370372772217, loss=4.815931797027588
I0204 00:54:17.992822 139936116377408 spec.py:321] Evaluating on the training split.
I0204 00:54:29.889116 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 00:55:07.788715 139936116377408 spec.py:349] Evaluating on the test split.
I0204 00:55:09.396512 139936116377408 submission_runner.py:408] Time since start: 42622.75s, 	Step: 81236, 	{'train/accuracy': 0.6807616949081421, 'train/loss': 1.2854599952697754, 'validation/accuracy': 0.6318599581718445, 'validation/loss': 1.5201311111450195, 'validation/num_examples': 50000, 'test/accuracy': 0.49960002303123474, 'test/loss': 2.2001304626464844, 'test/num_examples': 10000, 'score': 37851.49418973923, 'total_duration': 42622.748197078705, 'accumulated_submission_time': 37851.49418973923, 'accumulated_eval_time': 4763.166525602341, 'accumulated_logging_time': 3.8283190727233887}
I0204 00:55:09.429041 139774417409792 logging_writer.py:48] [81236] accumulated_eval_time=4763.166526, accumulated_logging_time=3.828319, accumulated_submission_time=37851.494190, global_step=81236, preemption_count=0, score=37851.494190, test/accuracy=0.499600, test/loss=2.200130, test/num_examples=10000, total_duration=42622.748197, train/accuracy=0.680762, train/loss=1.285460, validation/accuracy=0.631860, validation/loss=1.520131, validation/num_examples=50000
I0204 00:55:36.382194 139774434195200 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.4381581544876099, loss=2.3926212787628174
I0204 00:56:23.008990 139774417409792 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.5863651037216187, loss=2.363372325897217
I0204 00:57:09.835011 139774434195200 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.276829719543457, loss=2.6286110877990723
I0204 00:57:56.855835 139774417409792 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.2386488914489746, loss=4.431943416595459
I0204 00:58:44.001712 139774434195200 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.1803900003433228, loss=4.914904594421387
I0204 00:59:30.744672 139774417409792 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.0379999876022339, loss=4.922086238861084
I0204 01:00:17.693758 139774434195200 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.3752766847610474, loss=2.46114182472229
I0204 01:01:04.526732 139774417409792 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.3200336694717407, loss=4.321279048919678
I0204 01:01:51.539451 139774434195200 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.2955982685089111, loss=2.4391252994537354
I0204 01:02:09.612796 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:02:21.432962 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:03:00.719697 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:03:02.330388 139936116377408 submission_runner.py:408] Time since start: 43095.68s, 	Step: 82140, 	{'train/accuracy': 0.6885156035423279, 'train/loss': 1.2505003213882446, 'validation/accuracy': 0.6342399716377258, 'validation/loss': 1.5106966495513916, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.1858279705047607, 'test/num_examples': 10000, 'score': 38271.61984539032, 'total_duration': 43095.682314157486, 'accumulated_submission_time': 38271.61984539032, 'accumulated_eval_time': 4815.883181333542, 'accumulated_logging_time': 3.870955228805542}
I0204 01:03:02.364452 139774417409792 logging_writer.py:48] [82140] accumulated_eval_time=4815.883181, accumulated_logging_time=3.870955, accumulated_submission_time=38271.619845, global_step=82140, preemption_count=0, score=38271.619845, test/accuracy=0.515400, test/loss=2.185828, test/num_examples=10000, total_duration=43095.682314, train/accuracy=0.688516, train/loss=1.250500, validation/accuracy=0.634240, validation/loss=1.510697, validation/num_examples=50000
I0204 01:03:27.652681 139774434195200 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.2797397375106812, loss=5.016423225402832
I0204 01:04:14.075228 139774417409792 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.2905725240707397, loss=2.4607512950897217
I0204 01:05:01.379321 139774434195200 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3735826015472412, loss=2.2937917709350586
I0204 01:05:48.369951 139774417409792 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.2507508993148804, loss=3.3169243335723877
I0204 01:06:35.861476 139774434195200 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.0404552221298218, loss=4.542638301849365
I0204 01:07:22.710141 139774417409792 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.2926645278930664, loss=2.3002512454986572
I0204 01:08:09.853456 139774434195200 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.600598931312561, loss=2.4847099781036377
I0204 01:08:56.830265 139774417409792 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2638579607009888, loss=2.327117443084717
I0204 01:09:44.240194 139774434195200 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.184080719947815, loss=3.6160178184509277
I0204 01:10:02.583317 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:10:14.538517 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:10:53.345182 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:10:54.958806 139936116377408 submission_runner.py:408] Time since start: 43568.31s, 	Step: 83040, 	{'train/accuracy': 0.7089062333106995, 'train/loss': 1.1777950525283813, 'validation/accuracy': 0.6371200084686279, 'validation/loss': 1.510980486869812, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.158198595046997, 'test/num_examples': 10000, 'score': 38691.78195428848, 'total_duration': 43568.31066441536, 'accumulated_submission_time': 38691.78195428848, 'accumulated_eval_time': 4868.257662296295, 'accumulated_logging_time': 3.9152021408081055}
I0204 01:10:54.988821 139774417409792 logging_writer.py:48] [83040] accumulated_eval_time=4868.257662, accumulated_logging_time=3.915202, accumulated_submission_time=38691.781954, global_step=83040, preemption_count=0, score=38691.781954, test/accuracy=0.518600, test/loss=2.158199, test/num_examples=10000, total_duration=43568.310664, train/accuracy=0.708906, train/loss=1.177795, validation/accuracy=0.637120, validation/loss=1.510980, validation/num_examples=50000
I0204 01:11:20.507705 139774434195200 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.3908755779266357, loss=2.3488266468048096
I0204 01:12:06.965589 139774417409792 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.4284714460372925, loss=2.574674606323242
I0204 01:12:53.860723 139774434195200 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.123400330543518, loss=4.749279499053955
I0204 01:13:41.069001 139774417409792 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.3530482053756714, loss=4.925542831420898
I0204 01:14:28.396164 139774434195200 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.3097801208496094, loss=2.26216721534729
I0204 01:15:15.854768 139774417409792 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.3749632835388184, loss=3.3015592098236084
I0204 01:16:02.616225 139774434195200 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.1278995275497437, loss=4.382236480712891
I0204 01:16:49.339104 139774417409792 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.3599947690963745, loss=2.651942253112793
I0204 01:17:36.921102 139774434195200 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.499125599861145, loss=2.351149797439575
I0204 01:17:55.100684 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:18:07.454858 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:18:44.499761 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:18:46.108621 139936116377408 submission_runner.py:408] Time since start: 44039.46s, 	Step: 83940, 	{'train/accuracy': 0.6898242235183716, 'train/loss': 1.2514363527297974, 'validation/accuracy': 0.6396399736404419, 'validation/loss': 1.4833993911743164, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.157160758972168, 'test/num_examples': 10000, 'score': 39111.83612918854, 'total_duration': 44039.46033358574, 'accumulated_submission_time': 39111.83612918854, 'accumulated_eval_time': 4919.264448881149, 'accumulated_logging_time': 3.956256151199341}
I0204 01:18:46.142274 139774417409792 logging_writer.py:48] [83940] accumulated_eval_time=4919.264449, accumulated_logging_time=3.956256, accumulated_submission_time=39111.836129, global_step=83940, preemption_count=0, score=39111.836129, test/accuracy=0.513400, test/loss=2.157161, test/num_examples=10000, total_duration=44039.460334, train/accuracy=0.689824, train/loss=1.251436, validation/accuracy=0.639640, validation/loss=1.483399, validation/num_examples=50000
I0204 01:19:11.438022 139774434195200 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.3034379482269287, loss=2.796886444091797
I0204 01:19:57.769935 139774417409792 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.4423027038574219, loss=2.3019661903381348
I0204 01:20:44.964346 139774434195200 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.3524552583694458, loss=2.3628740310668945
I0204 01:21:32.146109 139774417409792 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.1745494604110718, loss=3.7020585536956787
I0204 01:22:19.449980 139774434195200 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.5991730690002441, loss=2.3916029930114746
I0204 01:23:06.510172 139774417409792 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.3276221752166748, loss=2.568755865097046
I0204 01:23:53.369045 139774434195200 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.344295859336853, loss=2.6784300804138184
I0204 01:24:40.650667 139774417409792 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.46747624874115, loss=2.3820605278015137
I0204 01:25:27.684008 139774434195200 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.2831499576568604, loss=2.327802896499634
I0204 01:25:46.326216 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:25:58.253201 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:26:37.137887 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:26:38.745120 139936116377408 submission_runner.py:408] Time since start: 44512.10s, 	Step: 84841, 	{'train/accuracy': 0.6862499713897705, 'train/loss': 1.308943748474121, 'validation/accuracy': 0.6337400078773499, 'validation/loss': 1.5579991340637207, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2265491485595703, 'test/num_examples': 10000, 'score': 39531.96011543274, 'total_duration': 44512.09685301781, 'accumulated_submission_time': 39531.96011543274, 'accumulated_eval_time': 4971.682218551636, 'accumulated_logging_time': 4.003744602203369}
I0204 01:26:38.780456 139774417409792 logging_writer.py:48] [84841] accumulated_eval_time=4971.682219, accumulated_logging_time=4.003745, accumulated_submission_time=39531.960115, global_step=84841, preemption_count=0, score=39531.960115, test/accuracy=0.506000, test/loss=2.226549, test/num_examples=10000, total_duration=44512.096853, train/accuracy=0.686250, train/loss=1.308944, validation/accuracy=0.633740, validation/loss=1.557999, validation/num_examples=50000
I0204 01:27:03.689264 139774434195200 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.3209364414215088, loss=4.855725288391113
I0204 01:27:49.808098 139774417409792 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.3693028688430786, loss=2.7077713012695312
I0204 01:28:37.304148 139774434195200 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.349942684173584, loss=2.3944830894470215
I0204 01:29:24.577634 139774417409792 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.4585586786270142, loss=2.3179173469543457
I0204 01:30:11.790837 139774434195200 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.1113059520721436, loss=4.903753757476807
I0204 01:30:58.719702 139774417409792 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.3790550231933594, loss=2.365382671356201
I0204 01:31:46.143801 139774434195200 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.4015395641326904, loss=2.52398943901062
I0204 01:32:33.425112 139774417409792 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.3641413450241089, loss=2.4060347080230713
I0204 01:33:20.689908 139774434195200 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.3255693912506104, loss=2.285635471343994
I0204 01:33:39.161122 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:33:51.059599 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:34:28.517735 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:34:30.124735 139936116377408 submission_runner.py:408] Time since start: 44983.48s, 	Step: 85741, 	{'train/accuracy': 0.7101757526397705, 'train/loss': 1.1636791229248047, 'validation/accuracy': 0.6434400081634521, 'validation/loss': 1.4765359163284302, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.157416820526123, 'test/num_examples': 10000, 'score': 39952.28334736824, 'total_duration': 44983.476644039154, 'accumulated_submission_time': 39952.28334736824, 'accumulated_eval_time': 5022.644863128662, 'accumulated_logging_time': 4.050050735473633}
I0204 01:34:30.156628 139774417409792 logging_writer.py:48] [85741] accumulated_eval_time=5022.644863, accumulated_logging_time=4.050051, accumulated_submission_time=39952.283347, global_step=85741, preemption_count=0, score=39952.283347, test/accuracy=0.515700, test/loss=2.157417, test/num_examples=10000, total_duration=44983.476644, train/accuracy=0.710176, train/loss=1.163679, validation/accuracy=0.643440, validation/loss=1.476536, validation/num_examples=50000
I0204 01:34:55.044555 139774434195200 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.1489357948303223, loss=2.6690053939819336
I0204 01:35:40.950618 139774417409792 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.2941416501998901, loss=2.4174160957336426
I0204 01:36:27.817136 139774434195200 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.442057728767395, loss=2.4580626487731934
I0204 01:37:14.862698 139774417409792 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.3289960622787476, loss=2.683537244796753
I0204 01:38:01.761648 139774434195200 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.3850938081741333, loss=4.828512668609619
I0204 01:38:48.451419 139774417409792 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.72018563747406, loss=4.43572998046875
I0204 01:39:35.626896 139774434195200 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.2080538272857666, loss=4.7799811363220215
I0204 01:40:22.596216 139774417409792 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.3483047485351562, loss=2.2167227268218994
I0204 01:41:09.744250 139774434195200 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.2456048727035522, loss=3.466374158859253
I0204 01:41:30.486603 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:41:42.617811 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:42:21.785851 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:42:23.397952 139936116377408 submission_runner.py:408] Time since start: 45456.75s, 	Step: 86646, 	{'train/accuracy': 0.6906445026397705, 'train/loss': 1.2567678689956665, 'validation/accuracy': 0.6412999629974365, 'validation/loss': 1.5003139972686768, 'validation/num_examples': 50000, 'test/accuracy': 0.5220000147819519, 'test/loss': 2.1490726470947266, 'test/num_examples': 10000, 'score': 40372.555807352066, 'total_duration': 45456.74989652634, 'accumulated_submission_time': 40372.555807352066, 'accumulated_eval_time': 5075.555291175842, 'accumulated_logging_time': 4.092345476150513}
I0204 01:42:23.430772 139774417409792 logging_writer.py:48] [86646] accumulated_eval_time=5075.555291, accumulated_logging_time=4.092345, accumulated_submission_time=40372.555807, global_step=86646, preemption_count=0, score=40372.555807, test/accuracy=0.522000, test/loss=2.149073, test/num_examples=10000, total_duration=45456.749897, train/accuracy=0.690645, train/loss=1.256768, validation/accuracy=0.641300, validation/loss=1.500314, validation/num_examples=50000
I0204 01:42:46.249155 139774434195200 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.3779963254928589, loss=2.2955517768859863
I0204 01:43:32.469187 139774417409792 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.1881840229034424, loss=2.909912109375
I0204 01:44:19.666276 139774434195200 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.2844748497009277, loss=3.2817370891571045
I0204 01:45:06.860150 139774417409792 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.531981348991394, loss=2.5023341178894043
I0204 01:45:53.932868 139774434195200 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.3523205518722534, loss=2.310161590576172
I0204 01:46:42.680834 139774417409792 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.3794641494750977, loss=2.2426514625549316
I0204 01:47:29.700131 139774434195200 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.2448225021362305, loss=2.70432186126709
I0204 01:48:17.045581 139774417409792 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3227248191833496, loss=2.3128507137298584
I0204 01:49:04.257584 139774434195200 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.3993518352508545, loss=2.2842535972595215
I0204 01:49:23.514842 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:49:35.743669 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:50:16.047939 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:50:17.653035 139936116377408 submission_runner.py:408] Time since start: 45931.00s, 	Step: 87543, 	{'train/accuracy': 0.6939257383346558, 'train/loss': 1.2720842361450195, 'validation/accuracy': 0.6372399926185608, 'validation/loss': 1.5326356887817383, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.205008029937744, 'test/num_examples': 10000, 'score': 40792.581500291824, 'total_duration': 45931.0048494339, 'accumulated_submission_time': 40792.581500291824, 'accumulated_eval_time': 5129.692433595657, 'accumulated_logging_time': 4.137173175811768}
I0204 01:50:17.686296 139774417409792 logging_writer.py:48] [87543] accumulated_eval_time=5129.692434, accumulated_logging_time=4.137173, accumulated_submission_time=40792.581500, global_step=87543, preemption_count=0, score=40792.581500, test/accuracy=0.512800, test/loss=2.205008, test/num_examples=10000, total_duration=45931.004849, train/accuracy=0.693926, train/loss=1.272084, validation/accuracy=0.637240, validation/loss=1.532636, validation/num_examples=50000
I0204 01:50:41.716208 139774434195200 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.3490794897079468, loss=3.766950845718384
I0204 01:51:28.303318 139774417409792 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.201281189918518, loss=3.720588445663452
I0204 01:52:15.655883 139774434195200 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.258241891860962, loss=4.081648826599121
I0204 01:53:02.690399 139774417409792 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.334490180015564, loss=2.297142267227173
I0204 01:53:50.059822 139774434195200 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.3624151945114136, loss=4.043229579925537
I0204 01:54:37.022415 139774417409792 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.4208149909973145, loss=2.3818840980529785
I0204 01:55:24.150501 139774434195200 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.2031457424163818, loss=4.542904376983643
I0204 01:56:11.192708 139774417409792 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.4765048027038574, loss=2.3345727920532227
I0204 01:56:57.928054 139774434195200 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.1836744546890259, loss=4.706715106964111
I0204 01:57:17.757030 139936116377408 spec.py:321] Evaluating on the training split.
I0204 01:57:29.521797 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 01:58:08.116967 139936116377408 spec.py:349] Evaluating on the test split.
I0204 01:58:09.726492 139936116377408 submission_runner.py:408] Time since start: 46403.08s, 	Step: 88444, 	{'train/accuracy': 0.7095507383346558, 'train/loss': 1.1825847625732422, 'validation/accuracy': 0.6438800096511841, 'validation/loss': 1.4855278730392456, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.1474361419677734, 'test/num_examples': 10000, 'score': 41212.59487867355, 'total_duration': 46403.07826638222, 'accumulated_submission_time': 41212.59487867355, 'accumulated_eval_time': 5181.660806417465, 'accumulated_logging_time': 4.181311845779419}
I0204 01:58:09.759937 139774417409792 logging_writer.py:48] [88444] accumulated_eval_time=5181.660806, accumulated_logging_time=4.181312, accumulated_submission_time=41212.594879, global_step=88444, preemption_count=0, score=41212.594879, test/accuracy=0.519800, test/loss=2.147436, test/num_examples=10000, total_duration=46403.078266, train/accuracy=0.709551, train/loss=1.182585, validation/accuracy=0.643880, validation/loss=1.485528, validation/num_examples=50000
I0204 01:58:33.381366 139774434195200 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.1682170629501343, loss=3.5027577877044678
I0204 01:59:19.585607 139774417409792 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.281593918800354, loss=2.2848024368286133
I0204 02:00:06.328286 139774434195200 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4197832345962524, loss=2.2894821166992188
I0204 02:00:53.328304 139774417409792 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.555849552154541, loss=2.5309135913848877
I0204 02:01:40.492290 139774434195200 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.1916396617889404, loss=2.7104268074035645
I0204 02:02:27.233208 139774417409792 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.4082810878753662, loss=2.208970546722412
I0204 02:03:13.932569 139774434195200 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.4765106439590454, loss=2.4745044708251953
I0204 02:04:00.863047 139774417409792 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.345605731010437, loss=2.6294782161712646
I0204 02:04:48.285430 139774434195200 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.2763673067092896, loss=2.755539894104004
I0204 02:05:09.850127 139936116377408 spec.py:321] Evaluating on the training split.
I0204 02:05:21.838091 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 02:05:58.944525 139936116377408 spec.py:349] Evaluating on the test split.
I0204 02:06:00.558658 139936116377408 submission_runner.py:408] Time since start: 46873.91s, 	Step: 89347, 	{'train/accuracy': 0.697949230670929, 'train/loss': 1.2049740552902222, 'validation/accuracy': 0.646120011806488, 'validation/loss': 1.454306960105896, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.1224191188812256, 'test/num_examples': 10000, 'score': 41632.62812304497, 'total_duration': 46873.910395145416, 'accumulated_submission_time': 41632.62812304497, 'accumulated_eval_time': 5232.368206501007, 'accumulated_logging_time': 4.225497245788574}
I0204 02:06:00.599274 139774417409792 logging_writer.py:48] [89347] accumulated_eval_time=5232.368207, accumulated_logging_time=4.225497, accumulated_submission_time=41632.628123, global_step=89347, preemption_count=0, score=41632.628123, test/accuracy=0.524300, test/loss=2.122419, test/num_examples=10000, total_duration=46873.910395, train/accuracy=0.697949, train/loss=1.204974, validation/accuracy=0.646120, validation/loss=1.454307, validation/num_examples=50000
I0204 02:06:22.977441 139774434195200 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.2837529182434082, loss=4.320873260498047
I0204 02:07:09.139589 139774417409792 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.0576903820037842, loss=3.572495698928833
I0204 02:07:55.913709 139774434195200 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.3932610750198364, loss=2.3464627265930176
I0204 02:08:42.869298 139774417409792 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.2721590995788574, loss=2.6964850425720215
I0204 02:09:30.174942 139774434195200 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.4610272645950317, loss=4.715676784515381
I0204 02:10:17.145690 139774417409792 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.3366601467132568, loss=2.227764129638672
I0204 02:11:04.613569 139774434195200 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.4475988149642944, loss=2.327948808670044
I0204 02:11:51.732347 139774417409792 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.1429038047790527, loss=4.837274551391602
I0204 02:12:38.898227 139774434195200 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2376716136932373, loss=3.2033185958862305
I0204 02:13:00.682702 139936116377408 spec.py:321] Evaluating on the training split.
I0204 02:13:12.755985 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 02:13:50.417934 139936116377408 spec.py:349] Evaluating on the test split.
I0204 02:13:52.027094 139936116377408 submission_runner.py:408] Time since start: 47345.38s, 	Step: 90248, 	{'train/accuracy': 0.7050976157188416, 'train/loss': 1.1831070184707642, 'validation/accuracy': 0.6489799618721008, 'validation/loss': 1.4485565423965454, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1029837131500244, 'test/num_examples': 10000, 'score': 42052.653123140335, 'total_duration': 47345.378999471664, 'accumulated_submission_time': 42052.653123140335, 'accumulated_eval_time': 5283.711639404297, 'accumulated_logging_time': 4.278661251068115}
I0204 02:13:52.060152 139774417409792 logging_writer.py:48] [90248] accumulated_eval_time=5283.711639, accumulated_logging_time=4.278661, accumulated_submission_time=42052.653123, global_step=90248, preemption_count=0, score=42052.653123, test/accuracy=0.523800, test/loss=2.102984, test/num_examples=10000, total_duration=47345.378999, train/accuracy=0.705098, train/loss=1.183107, validation/accuracy=0.648980, validation/loss=1.448557, validation/num_examples=50000
I0204 02:14:14.035195 139774434195200 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.3128600120544434, loss=3.0811212062835693
I0204 02:14:59.935894 139774417409792 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.5548875331878662, loss=2.3211669921875
I0204 02:15:46.944798 139774434195200 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.5166633129119873, loss=2.3676300048828125
I0204 02:16:33.891540 139774417409792 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.1814011335372925, loss=3.8292152881622314
I0204 02:17:21.000766 139774434195200 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.364900827407837, loss=2.197059154510498
I0204 02:18:08.075047 139774417409792 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.4163590669631958, loss=2.864229679107666
I0204 02:18:55.206835 139774434195200 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.3875406980514526, loss=2.5134544372558594
I0204 02:19:42.247789 139774417409792 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.3481215238571167, loss=3.2695250511169434
I0204 02:20:29.444304 139774434195200 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.5115715265274048, loss=2.3776519298553467
I0204 02:20:52.157464 139936116377408 spec.py:321] Evaluating on the training split.
I0204 02:21:04.620472 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 02:21:42.995296 139936116377408 spec.py:349] Evaluating on the test split.
I0204 02:21:44.609450 139936116377408 submission_runner.py:408] Time since start: 47817.96s, 	Step: 91150, 	{'train/accuracy': 0.7136132717132568, 'train/loss': 1.1587448120117188, 'validation/accuracy': 0.645859956741333, 'validation/loss': 1.4699233770370483, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.1205849647521973, 'test/num_examples': 10000, 'score': 42472.69245290756, 'total_duration': 47817.961052656174, 'accumulated_submission_time': 42472.69245290756, 'accumulated_eval_time': 5336.162372112274, 'accumulated_logging_time': 4.322976589202881}
I0204 02:21:44.642099 139774434195200 logging_writer.py:48] [91150] accumulated_eval_time=5336.162372, accumulated_logging_time=4.322977, accumulated_submission_time=42472.692453, global_step=91150, preemption_count=0, score=42472.692453, test/accuracy=0.523300, test/loss=2.120585, test/num_examples=10000, total_duration=47817.961053, train/accuracy=0.713613, train/loss=1.158745, validation/accuracy=0.645860, validation/loss=1.469923, validation/num_examples=50000
I0204 02:22:05.777167 139774417409792 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.5349359512329102, loss=2.180833339691162
I0204 02:22:51.636226 139774434195200 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.4253623485565186, loss=2.744158983230591
I0204 02:23:39.047577 139774417409792 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.4632426500320435, loss=2.1956841945648193
I0204 02:24:26.190130 139774434195200 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.4551671743392944, loss=4.634682655334473
I0204 02:25:13.309267 139774417409792 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.5036765336990356, loss=2.2833456993103027
I0204 02:26:00.611513 139774434195200 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.5092369318008423, loss=2.2949461936950684
I0204 02:26:48.060476 139774417409792 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.104068398475647, loss=4.592652320861816
I0204 02:27:35.303187 139774434195200 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.2886837720870972, loss=2.5960946083068848
I0204 02:28:22.241818 139774417409792 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.4942001104354858, loss=2.4728519916534424
I0204 02:28:45.019467 139936116377408 spec.py:321] Evaluating on the training split.
I0204 02:28:57.026150 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 02:29:35.817717 139936116377408 spec.py:349] Evaluating on the test split.
I0204 02:29:37.429198 139936116377408 submission_runner.py:408] Time since start: 48290.78s, 	Step: 92050, 	{'train/accuracy': 0.7071288824081421, 'train/loss': 1.196994662284851, 'validation/accuracy': 0.6503399610519409, 'validation/loss': 1.4454437494277954, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.090376138687134, 'test/num_examples': 10000, 'score': 42893.01210975647, 'total_duration': 48290.780958890915, 'accumulated_submission_time': 42893.01210975647, 'accumulated_eval_time': 5388.570991754532, 'accumulated_logging_time': 4.366878986358643}
I0204 02:29:37.463052 139774434195200 logging_writer.py:48] [92050] accumulated_eval_time=5388.570992, accumulated_logging_time=4.366879, accumulated_submission_time=42893.012110, global_step=92050, preemption_count=0, score=42893.012110, test/accuracy=0.526800, test/loss=2.090376, test/num_examples=10000, total_duration=48290.780959, train/accuracy=0.707129, train/loss=1.196995, validation/accuracy=0.650340, validation/loss=1.445444, validation/num_examples=50000
I0204 02:29:58.598829 139774417409792 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.3055528402328491, loss=4.629155158996582
I0204 02:30:44.546025 139774434195200 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.4690011739730835, loss=2.3599908351898193
I0204 02:31:31.553326 139774417409792 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.1756036281585693, loss=4.761080741882324
I0204 02:32:19.232287 139774434195200 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.1779555082321167, loss=4.149581432342529
I0204 02:33:06.619859 139774417409792 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.3153345584869385, loss=3.728034019470215
I0204 02:33:53.933423 139774434195200 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.441131591796875, loss=2.2997941970825195
I0204 02:34:41.425165 139774417409792 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.1025365591049194, loss=4.400768280029297
I0204 02:35:28.641944 139774434195200 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.4556277990341187, loss=2.4014594554901123
I0204 02:36:16.031336 139774417409792 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.2151871919631958, loss=3.580528974533081
I0204 02:36:37.824014 139936116377408 spec.py:321] Evaluating on the training split.
I0204 02:36:49.798161 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 02:37:28.122812 139936116377408 spec.py:349] Evaluating on the test split.
I0204 02:37:29.727917 139936116377408 submission_runner.py:408] Time since start: 48763.08s, 	Step: 92948, 	{'train/accuracy': 0.7015038728713989, 'train/loss': 1.22853684425354, 'validation/accuracy': 0.6442599892616272, 'validation/loss': 1.4875376224517822, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.143444061279297, 'test/num_examples': 10000, 'score': 43313.314470767975, 'total_duration': 48763.079761981964, 'accumulated_submission_time': 43313.314470767975, 'accumulated_eval_time': 5440.473873138428, 'accumulated_logging_time': 4.412621974945068}
I0204 02:37:29.761742 139774434195200 logging_writer.py:48] [92948] accumulated_eval_time=5440.473873, accumulated_logging_time=4.412622, accumulated_submission_time=43313.314471, global_step=92948, preemption_count=0, score=43313.314471, test/accuracy=0.520600, test/loss=2.143444, test/num_examples=10000, total_duration=48763.079762, train/accuracy=0.701504, train/loss=1.228537, validation/accuracy=0.644260, validation/loss=1.487538, validation/num_examples=50000
I0204 02:37:51.732984 139774417409792 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.4402263164520264, loss=3.2503767013549805
I0204 02:38:37.907704 139774434195200 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.4968023300170898, loss=2.2804722785949707
I0204 02:39:25.165075 139774417409792 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.278609275817871, loss=3.1450822353363037
I0204 02:40:12.425712 139774434195200 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.339714527130127, loss=2.3314833641052246
I0204 02:40:59.484989 139774417409792 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.2444802522659302, loss=3.110027551651001
I0204 02:41:46.947036 139774434195200 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.3285248279571533, loss=3.478142738342285
I0204 02:42:34.500584 139774417409792 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.3190056085586548, loss=2.6632986068725586
I0204 02:43:21.931030 139774434195200 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.4026414155960083, loss=2.3871607780456543
I0204 02:44:09.392980 139774417409792 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.381320834159851, loss=2.387044668197632
I0204 02:44:29.777555 139936116377408 spec.py:321] Evaluating on the training split.
I0204 02:44:41.557402 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 02:45:19.708977 139936116377408 spec.py:349] Evaluating on the test split.
I0204 02:45:21.309084 139936116377408 submission_runner.py:408] Time since start: 49234.66s, 	Step: 93845, 	{'train/accuracy': 0.7256445288658142, 'train/loss': 1.0961700677871704, 'validation/accuracy': 0.6585800051689148, 'validation/loss': 1.40444016456604, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.0604445934295654, 'test/num_examples': 10000, 'score': 43733.2725212574, 'total_duration': 49234.66075634956, 'accumulated_submission_time': 43733.2725212574, 'accumulated_eval_time': 5492.004205703735, 'accumulated_logging_time': 4.4586100578308105}
I0204 02:45:21.342526 139774434195200 logging_writer.py:48] [93845] accumulated_eval_time=5492.004206, accumulated_logging_time=4.458610, accumulated_submission_time=43733.272521, global_step=93845, preemption_count=0, score=43733.272521, test/accuracy=0.532300, test/loss=2.060445, test/num_examples=10000, total_duration=49234.660756, train/accuracy=0.725645, train/loss=1.096170, validation/accuracy=0.658580, validation/loss=1.404440, validation/num_examples=50000
I0204 02:45:44.562846 139774417409792 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.2108086347579956, loss=3.8790171146392822
I0204 02:46:30.625037 139774434195200 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.446590542793274, loss=2.224271059036255
I0204 02:47:17.719709 139774417409792 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.1613540649414062, loss=4.730617046356201
I0204 02:48:04.935491 139774434195200 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.2767555713653564, loss=4.435845851898193
I0204 02:48:52.241132 139774417409792 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.2462345361709595, loss=4.510575294494629
I0204 02:49:39.428961 139774434195200 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.3942421674728394, loss=2.5542004108428955
I0204 02:50:26.739876 139774417409792 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.2980027198791504, loss=2.744328498840332
I0204 02:51:13.837873 139774434195200 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.400787591934204, loss=2.2647721767425537
I0204 02:52:01.186974 139774417409792 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.346028447151184, loss=2.6324868202209473
I0204 02:52:21.660962 139936116377408 spec.py:321] Evaluating on the training split.
I0204 02:52:33.547844 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 02:53:11.004354 139936116377408 spec.py:349] Evaluating on the test split.
I0204 02:53:12.607527 139936116377408 submission_runner.py:408] Time since start: 49705.96s, 	Step: 94745, 	{'train/accuracy': 0.7081640362739563, 'train/loss': 1.187019944190979, 'validation/accuracy': 0.6531999707221985, 'validation/loss': 1.43330717086792, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.109593629837036, 'test/num_examples': 10000, 'score': 44153.533059597015, 'total_duration': 49705.9593732357, 'accumulated_submission_time': 44153.533059597015, 'accumulated_eval_time': 5542.949748277664, 'accumulated_logging_time': 4.503470420837402}
I0204 02:53:12.638145 139774434195200 logging_writer.py:48] [94745] accumulated_eval_time=5542.949748, accumulated_logging_time=4.503470, accumulated_submission_time=44153.533060, global_step=94745, preemption_count=0, score=44153.533060, test/accuracy=0.521900, test/loss=2.109594, test/num_examples=10000, total_duration=49705.959373, train/accuracy=0.708164, train/loss=1.187020, validation/accuracy=0.653200, validation/loss=1.433307, validation/num_examples=50000
I0204 02:53:35.873313 139774417409792 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.407366156578064, loss=2.589421033859253
I0204 02:54:22.253394 139774434195200 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.4330607652664185, loss=4.768134117126465
I0204 02:55:09.576839 139774417409792 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.3448599576950073, loss=3.902012348175049
I0204 02:55:56.670273 139774434195200 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.446192741394043, loss=2.261673927307129
I0204 02:56:43.661458 139774417409792 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.4904963970184326, loss=2.3397536277770996
I0204 02:57:30.939868 139774434195200 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.4769996404647827, loss=2.251678943634033
I0204 02:58:18.173886 139774417409792 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.5296577215194702, loss=4.438141345977783
I0204 02:59:05.162092 139774434195200 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.16248619556427, loss=4.788625717163086
I0204 02:59:52.418763 139774417409792 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.3213573694229126, loss=2.396528720855713
I0204 03:00:12.875546 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:00:24.662657 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:01:02.409604 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:01:04.019377 139936116377408 submission_runner.py:408] Time since start: 50177.37s, 	Step: 95645, 	{'train/accuracy': 0.7108983993530273, 'train/loss': 1.1773436069488525, 'validation/accuracy': 0.6543799638748169, 'validation/loss': 1.4492274522781372, 'validation/num_examples': 50000, 'test/accuracy': 0.5276000499725342, 'test/loss': 2.1062769889831543, 'test/num_examples': 10000, 'score': 44573.71209073067, 'total_duration': 50177.37110543251, 'accumulated_submission_time': 44573.71209073067, 'accumulated_eval_time': 5594.092439651489, 'accumulated_logging_time': 4.546373128890991}
I0204 03:01:04.055277 139774434195200 logging_writer.py:48] [95645] accumulated_eval_time=5594.092440, accumulated_logging_time=4.546373, accumulated_submission_time=44573.712091, global_step=95645, preemption_count=0, score=44573.712091, test/accuracy=0.527600, test/loss=2.106277, test/num_examples=10000, total_duration=50177.371105, train/accuracy=0.710898, train/loss=1.177344, validation/accuracy=0.654380, validation/loss=1.449227, validation/num_examples=50000
I0204 03:01:27.283789 139774417409792 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.2704519033432007, loss=3.207871437072754
I0204 03:02:13.141433 139774434195200 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.1131271123886108, loss=4.564023971557617
I0204 03:02:59.991954 139774417409792 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.3955588340759277, loss=2.604905128479004
I0204 03:03:46.874772 139774434195200 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.4430066347122192, loss=2.260571241378784
I0204 03:04:33.654479 139774417409792 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.1272135972976685, loss=4.752751350402832
I0204 03:05:20.487833 139774434195200 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.2294749021530151, loss=2.7127256393432617
I0204 03:06:07.711725 139774417409792 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.4342083930969238, loss=2.2532875537872314
I0204 03:06:54.667262 139774434195200 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.3300740718841553, loss=4.839731693267822
I0204 03:07:41.491216 139774417409792 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.5023436546325684, loss=2.1818642616271973
I0204 03:08:04.246565 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:08:16.221526 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:08:56.124986 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:08:57.734194 139936116377408 submission_runner.py:408] Time since start: 50651.09s, 	Step: 96550, 	{'train/accuracy': 0.7174413800239563, 'train/loss': 1.1969013214111328, 'validation/accuracy': 0.6540799736976624, 'validation/loss': 1.4859381914138794, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.1328086853027344, 'test/num_examples': 10000, 'score': 44993.845116853714, 'total_duration': 50651.08602452278, 'accumulated_submission_time': 44993.845116853714, 'accumulated_eval_time': 5647.579032897949, 'accumulated_logging_time': 4.593611240386963}
I0204 03:08:57.768109 139774434195200 logging_writer.py:48] [96550] accumulated_eval_time=5647.579033, accumulated_logging_time=4.593611, accumulated_submission_time=44993.845117, global_step=96550, preemption_count=0, score=44993.845117, test/accuracy=0.528800, test/loss=2.132809, test/num_examples=10000, total_duration=50651.086025, train/accuracy=0.717441, train/loss=1.196901, validation/accuracy=0.654080, validation/loss=1.485938, validation/num_examples=50000
I0204 03:09:19.077595 139774417409792 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.4530458450317383, loss=2.199751377105713
I0204 03:10:05.024907 139774434195200 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.3950166702270508, loss=2.1603760719299316
I0204 03:10:52.206018 139774417409792 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.3739362955093384, loss=2.250994920730591
I0204 03:11:39.553086 139774434195200 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.4518821239471436, loss=2.327986240386963
I0204 03:12:27.115792 139774417409792 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.4403173923492432, loss=2.516071081161499
I0204 03:13:14.281597 139774434195200 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.3406201601028442, loss=4.691405773162842
I0204 03:14:01.344562 139774417409792 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.2316101789474487, loss=3.0776028633117676
I0204 03:14:48.802307 139774434195200 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.2371549606323242, loss=4.322085380554199
I0204 03:15:36.218542 139774417409792 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.3378307819366455, loss=4.784939765930176
I0204 03:15:57.936514 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:16:10.332332 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:16:47.613371 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:16:49.225094 139936116377408 submission_runner.py:408] Time since start: 51122.58s, 	Step: 97448, 	{'train/accuracy': 0.7165625095367432, 'train/loss': 1.188768744468689, 'validation/accuracy': 0.6569199562072754, 'validation/loss': 1.463334083557129, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.111708402633667, 'test/num_examples': 10000, 'score': 45413.95650100708, 'total_duration': 51122.57685351372, 'accumulated_submission_time': 45413.95650100708, 'accumulated_eval_time': 5698.86651301384, 'accumulated_logging_time': 4.638423681259155}
I0204 03:16:49.262361 139774434195200 logging_writer.py:48] [97448] accumulated_eval_time=5698.866513, accumulated_logging_time=4.638424, accumulated_submission_time=45413.956501, global_step=97448, preemption_count=0, score=45413.956501, test/accuracy=0.534500, test/loss=2.111708, test/num_examples=10000, total_duration=51122.576854, train/accuracy=0.716563, train/loss=1.188769, validation/accuracy=0.656920, validation/loss=1.463334, validation/num_examples=50000
I0204 03:17:11.235745 139774417409792 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.4813472032546997, loss=2.564870595932007
I0204 03:17:57.416926 139774434195200 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.2828928232192993, loss=2.4763619899749756
I0204 03:18:44.739829 139774417409792 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.2784656286239624, loss=2.67256760597229
I0204 03:19:32.123923 139774434195200 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.2415820360183716, loss=3.7377140522003174
I0204 03:20:19.475120 139774417409792 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.6617664098739624, loss=2.220348596572876
I0204 03:21:06.618224 139774434195200 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.4245041608810425, loss=2.2264583110809326
I0204 03:21:54.076165 139774417409792 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.4190436601638794, loss=2.413867235183716
I0204 03:22:41.167435 139774434195200 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.4963046312332153, loss=2.185349464416504
I0204 03:23:28.444752 139774417409792 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.4185422658920288, loss=2.398839235305786
I0204 03:23:49.620112 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:24:01.630051 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:24:38.712133 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:24:40.321796 139936116377408 submission_runner.py:408] Time since start: 51593.67s, 	Step: 98347, 	{'train/accuracy': 0.7182226181030273, 'train/loss': 1.1286863088607788, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.4054856300354004, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.0756752490997314, 'test/num_examples': 10000, 'score': 45834.25590658188, 'total_duration': 51593.67358827591, 'accumulated_submission_time': 45834.25590658188, 'accumulated_eval_time': 5749.567130565643, 'accumulated_logging_time': 4.687403678894043}
I0204 03:24:40.358434 139774434195200 logging_writer.py:48] [98347] accumulated_eval_time=5749.567131, accumulated_logging_time=4.687404, accumulated_submission_time=45834.255907, global_step=98347, preemption_count=0, score=45834.255907, test/accuracy=0.530400, test/loss=2.075675, test/num_examples=10000, total_duration=51593.673588, train/accuracy=0.718223, train/loss=1.128686, validation/accuracy=0.661060, validation/loss=1.405486, validation/num_examples=50000
I0204 03:25:02.744533 139774417409792 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.2290486097335815, loss=4.707677364349365
I0204 03:25:48.368358 139774434195200 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.2791459560394287, loss=4.801126480102539
I0204 03:26:35.782718 139774417409792 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.2315402030944824, loss=4.729506969451904
I0204 03:27:22.987385 139774434195200 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.4332666397094727, loss=2.1262948513031006
I0204 03:28:10.236987 139774417409792 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.5095994472503662, loss=2.1355366706848145
I0204 03:28:57.213174 139774434195200 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.4768050909042358, loss=2.3680624961853027
I0204 03:29:44.484381 139774417409792 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.321299433708191, loss=4.668694496154785
I0204 03:30:31.657064 139774434195200 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.387247920036316, loss=2.2152397632598877
I0204 03:31:18.781592 139774417409792 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.3211911916732788, loss=4.52990198135376
I0204 03:31:40.729034 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:31:52.684209 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:32:31.583243 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:32:33.199737 139936116377408 submission_runner.py:408] Time since start: 52066.55s, 	Step: 99248, 	{'train/accuracy': 0.7224413752555847, 'train/loss': 1.1078177690505981, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.3991767168045044, 'validation/num_examples': 50000, 'test/accuracy': 0.535800039768219, 'test/loss': 2.0520083904266357, 'test/num_examples': 10000, 'score': 46254.56909441948, 'total_duration': 52066.551471710205, 'accumulated_submission_time': 46254.56909441948, 'accumulated_eval_time': 5802.036701917648, 'accumulated_logging_time': 4.73491358757019}
I0204 03:32:33.243822 139774434195200 logging_writer.py:48] [99248] accumulated_eval_time=5802.036702, accumulated_logging_time=4.734914, accumulated_submission_time=46254.569094, global_step=99248, preemption_count=0, score=46254.569094, test/accuracy=0.535800, test/loss=2.052008, test/num_examples=10000, total_duration=52066.551472, train/accuracy=0.722441, train/loss=1.107818, validation/accuracy=0.660320, validation/loss=1.399177, validation/num_examples=50000
I0204 03:32:55.209661 139774417409792 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.6145159006118774, loss=2.2856621742248535
I0204 03:33:41.141956 139774434195200 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.4591017961502075, loss=2.0733962059020996
I0204 03:34:28.356152 139774417409792 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.2294573783874512, loss=4.635016918182373
I0204 03:35:15.374569 139774434195200 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.3351188898086548, loss=2.7201995849609375
I0204 03:36:02.239659 139774417409792 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.187963843345642, loss=4.017704486846924
I0204 03:36:48.886891 139774434195200 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.3908195495605469, loss=3.3028998374938965
I0204 03:37:36.279961 139774417409792 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.2245062589645386, loss=4.189305305480957
I0204 03:38:23.299315 139774434195200 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.4785799980163574, loss=2.055960178375244
I0204 03:39:10.476366 139774417409792 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.2418010234832764, loss=4.721437454223633
I0204 03:39:33.637971 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:39:45.468068 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:40:25.738813 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:40:27.343515 139936116377408 submission_runner.py:408] Time since start: 52540.70s, 	Step: 100151, 	{'train/accuracy': 0.7393554449081421, 'train/loss': 1.0557329654693604, 'validation/accuracy': 0.6615599989891052, 'validation/loss': 1.396098256111145, 'validation/num_examples': 50000, 'test/accuracy': 0.5397000312805176, 'test/loss': 2.0368974208831787, 'test/num_examples': 10000, 'score': 46674.90282559395, 'total_duration': 52540.69532442093, 'accumulated_submission_time': 46674.90282559395, 'accumulated_eval_time': 5855.741189956665, 'accumulated_logging_time': 4.7923808097839355}
I0204 03:40:27.379374 139774434195200 logging_writer.py:48] [100151] accumulated_eval_time=5855.741190, accumulated_logging_time=4.792381, accumulated_submission_time=46674.902826, global_step=100151, preemption_count=0, score=46674.902826, test/accuracy=0.539700, test/loss=2.036897, test/num_examples=10000, total_duration=52540.695324, train/accuracy=0.739355, train/loss=1.055733, validation/accuracy=0.661560, validation/loss=1.396098, validation/num_examples=50000
I0204 03:40:48.106048 139774417409792 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.281928539276123, loss=4.419554710388184
I0204 03:41:33.709545 139774434195200 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.3484371900558472, loss=4.1412811279296875
I0204 03:42:21.308065 139774417409792 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.5662589073181152, loss=2.3300092220306396
I0204 03:43:08.321749 139774434195200 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.5417343378067017, loss=2.182685613632202
I0204 03:43:55.388723 139774417409792 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.354066252708435, loss=3.401512622833252
I0204 03:44:42.096391 139774434195200 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.5428998470306396, loss=2.127164363861084
I0204 03:45:29.267272 139774417409792 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.5266022682189941, loss=2.149447441101074
I0204 03:46:16.208353 139774434195200 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.2690171003341675, loss=4.154403209686279
I0204 03:47:03.507077 139774417409792 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.7537145614624023, loss=2.0019075870513916
I0204 03:47:27.659493 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:47:39.580875 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:48:18.268912 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:48:19.874128 139936116377408 submission_runner.py:408] Time since start: 53013.23s, 	Step: 101053, 	{'train/accuracy': 0.7187304496765137, 'train/loss': 1.1348941326141357, 'validation/accuracy': 0.6573399901390076, 'validation/loss': 1.40870201587677, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.0542080402374268, 'test/num_examples': 10000, 'score': 47095.12478327751, 'total_duration': 53013.22602438927, 'accumulated_submission_time': 47095.12478327751, 'accumulated_eval_time': 5907.954854726791, 'accumulated_logging_time': 4.839926719665527}
I0204 03:48:19.907299 139774434195200 logging_writer.py:48] [101053] accumulated_eval_time=5907.954855, accumulated_logging_time=4.839927, accumulated_submission_time=47095.124783, global_step=101053, preemption_count=0, score=47095.124783, test/accuracy=0.536400, test/loss=2.054208, test/num_examples=10000, total_duration=53013.226024, train/accuracy=0.718730, train/loss=1.134894, validation/accuracy=0.657340, validation/loss=1.408702, validation/num_examples=50000
I0204 03:48:39.822225 139774417409792 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.3857421875, loss=2.9376027584075928
I0204 03:49:25.176422 139774434195200 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.4256675243377686, loss=2.5422332286834717
I0204 03:50:12.216409 139774417409792 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.6340680122375488, loss=2.0532166957855225
I0204 03:50:59.288060 139774434195200 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.39100980758667, loss=4.768256187438965
I0204 03:51:46.478344 139774417409792 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.6365928649902344, loss=2.161907911300659
I0204 03:52:33.193021 139774434195200 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.251744031906128, loss=3.7823097705841064
I0204 03:53:20.291412 139774417409792 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.4300868511199951, loss=2.435041666030884
I0204 03:54:07.633939 139774434195200 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.34300696849823, loss=4.065003395080566
I0204 03:54:54.333852 139774417409792 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.6787755489349365, loss=2.3996658325195312
I0204 03:55:19.929681 139936116377408 spec.py:321] Evaluating on the training split.
I0204 03:55:31.824434 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 03:56:09.017513 139936116377408 spec.py:349] Evaluating on the test split.
I0204 03:56:10.618516 139936116377408 submission_runner.py:408] Time since start: 53483.97s, 	Step: 101956, 	{'train/accuracy': 0.7291796803474426, 'train/loss': 1.1408336162567139, 'validation/accuracy': 0.6621400117874146, 'validation/loss': 1.432441234588623, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.0662755966186523, 'test/num_examples': 10000, 'score': 47515.08821105957, 'total_duration': 53483.97033381462, 'accumulated_submission_time': 47515.08821105957, 'accumulated_eval_time': 5958.642646789551, 'accumulated_logging_time': 4.885095834732056}
I0204 03:56:10.652644 139774434195200 logging_writer.py:48] [101956] accumulated_eval_time=5958.642647, accumulated_logging_time=4.885096, accumulated_submission_time=47515.088211, global_step=101956, preemption_count=0, score=47515.088211, test/accuracy=0.538600, test/loss=2.066276, test/num_examples=10000, total_duration=53483.970334, train/accuracy=0.729180, train/loss=1.140834, validation/accuracy=0.662140, validation/loss=1.432441, validation/num_examples=50000
I0204 03:56:29.318513 139774417409792 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.475348711013794, loss=2.1922640800476074
I0204 03:57:14.621473 139774434195200 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.2333731651306152, loss=3.382462739944458
I0204 03:58:01.480988 139774417409792 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.3349831104278564, loss=3.2405295372009277
I0204 03:58:48.330265 139774434195200 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.3234761953353882, loss=3.0297467708587646
I0204 03:59:35.606429 139774417409792 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.4211658239364624, loss=2.986070156097412
I0204 04:00:22.381695 139774434195200 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.5523518323898315, loss=2.140134811401367
I0204 04:01:09.745607 139774417409792 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.5495264530181885, loss=2.1177096366882324
I0204 04:01:56.806984 139774434195200 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.4904961585998535, loss=2.4800074100494385
I0204 04:02:43.764891 139774417409792 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.3882498741149902, loss=4.784835338592529
I0204 04:03:10.705149 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:03:22.768718 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:04:01.923859 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:04:03.526997 139936116377408 submission_runner.py:408] Time since start: 53956.88s, 	Step: 102858, 	{'train/accuracy': 0.7438867092132568, 'train/loss': 1.0300265550613403, 'validation/accuracy': 0.6672199964523315, 'validation/loss': 1.3864587545394897, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0302770137786865, 'test/num_examples': 10000, 'score': 47935.08392548561, 'total_duration': 53956.87895011902, 'accumulated_submission_time': 47935.08392548561, 'accumulated_eval_time': 6011.463626861572, 'accumulated_logging_time': 4.929841756820679}
I0204 04:04:03.561284 139774434195200 logging_writer.py:48] [102858] accumulated_eval_time=6011.463627, accumulated_logging_time=4.929842, accumulated_submission_time=47935.083925, global_step=102858, preemption_count=0, score=47935.083925, test/accuracy=0.540600, test/loss=2.030277, test/num_examples=10000, total_duration=53956.878950, train/accuracy=0.743887, train/loss=1.030027, validation/accuracy=0.667220, validation/loss=1.386459, validation/num_examples=50000
I0204 04:04:21.392196 139774417409792 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.2459522485733032, loss=4.677428245544434
I0204 04:05:06.568276 139774434195200 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.281655192375183, loss=4.490014553070068
I0204 04:05:53.260378 139774417409792 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.5997130870819092, loss=2.0583980083465576
I0204 04:06:40.121037 139774434195200 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.2672585248947144, loss=3.7092642784118652
I0204 04:07:27.036119 139774417409792 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.598554015159607, loss=2.048222541809082
I0204 04:08:14.089255 139774434195200 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.556392788887024, loss=2.2666361331939697
I0204 04:09:00.919796 139774417409792 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.2854667901992798, loss=3.0949792861938477
I0204 04:09:47.864482 139774434195200 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.5318368673324585, loss=2.0308148860931396
I0204 04:10:34.805269 139774417409792 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.5511671304702759, loss=2.0658347606658936
I0204 04:11:03.605754 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:11:15.631255 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:11:52.840387 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:11:54.445556 139936116377408 submission_runner.py:408] Time since start: 54427.80s, 	Step: 103763, 	{'train/accuracy': 0.723437488079071, 'train/loss': 1.1205108165740967, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.3746145963668823, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.0381054878234863, 'test/num_examples': 10000, 'score': 48355.07038998604, 'total_duration': 54427.79730439186, 'accumulated_submission_time': 48355.07038998604, 'accumulated_eval_time': 6062.30232834816, 'accumulated_logging_time': 4.975308418273926}
I0204 04:11:54.479778 139774434195200 logging_writer.py:48] [103763] accumulated_eval_time=6062.302328, accumulated_logging_time=4.975308, accumulated_submission_time=48355.070390, global_step=103763, preemption_count=0, score=48355.070390, test/accuracy=0.542100, test/loss=2.038105, test/num_examples=10000, total_duration=54427.797304, train/accuracy=0.723437, train/loss=1.120511, validation/accuracy=0.669760, validation/loss=1.374615, validation/num_examples=50000
I0204 04:12:10.220300 139774417409792 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.386046051979065, loss=4.519519805908203
I0204 04:12:54.956851 139774434195200 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.4121767282485962, loss=4.014083385467529
I0204 04:13:41.922209 139774417409792 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.4877398014068604, loss=2.042872667312622
I0204 04:14:29.128370 139774434195200 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.4040443897247314, loss=2.4637510776519775
I0204 04:15:16.484183 139774417409792 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.5551799535751343, loss=4.410178184509277
I0204 04:16:03.394186 139774434195200 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.4666697978973389, loss=2.1545419692993164
I0204 04:16:50.193248 139774417409792 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.536374807357788, loss=1.9291150569915771
I0204 04:17:37.374716 139774434195200 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.527971625328064, loss=2.0064330101013184
I0204 04:18:24.592166 139774417409792 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.3558235168457031, loss=3.138921022415161
I0204 04:18:54.818002 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:19:06.904067 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:19:44.638048 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:19:46.243803 139936116377408 submission_runner.py:408] Time since start: 54899.60s, 	Step: 104666, 	{'train/accuracy': 0.7344530820846558, 'train/loss': 1.0720163583755493, 'validation/accuracy': 0.6725599765777588, 'validation/loss': 1.3530502319335938, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 1.9986283779144287, 'test/num_examples': 10000, 'score': 48775.350959062576, 'total_duration': 54899.595631599426, 'accumulated_submission_time': 48775.350959062576, 'accumulated_eval_time': 6113.727089166641, 'accumulated_logging_time': 5.020384788513184}
I0204 04:19:46.278744 139774434195200 logging_writer.py:48] [104666] accumulated_eval_time=6113.727089, accumulated_logging_time=5.020385, accumulated_submission_time=48775.350959, global_step=104666, preemption_count=0, score=48775.350959, test/accuracy=0.545200, test/loss=1.998628, test/num_examples=10000, total_duration=54899.595632, train/accuracy=0.734453, train/loss=1.072016, validation/accuracy=0.672560, validation/loss=1.353050, validation/num_examples=50000
I0204 04:20:00.795400 139774417409792 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.6700150966644287, loss=2.0325613021850586
I0204 04:20:46.387918 139774434195200 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.4353663921356201, loss=3.2287347316741943
I0204 04:21:33.765181 139774417409792 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.62429940700531, loss=2.2031784057617188
I0204 04:22:21.151674 139774434195200 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.6157629489898682, loss=2.1490089893341064
I0204 04:23:09.058667 139774417409792 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.5420331954956055, loss=2.1514766216278076
I0204 04:23:56.360765 139774434195200 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.309372901916504, loss=4.0601806640625
I0204 04:24:44.028824 139774417409792 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.5834910869598389, loss=2.0430190563201904
I0204 04:25:31.477252 139774434195200 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.2874417304992676, loss=3.070157766342163
I0204 04:26:19.081733 139774417409792 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.7213382720947266, loss=2.163470506668091
I0204 04:26:46.253240 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:26:58.212204 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:27:37.181071 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:27:38.781250 139936116377408 submission_runner.py:408] Time since start: 55372.13s, 	Step: 105559, 	{'train/accuracy': 0.7469140291213989, 'train/loss': 0.9951573610305786, 'validation/accuracy': 0.6689199805259705, 'validation/loss': 1.3489623069763184, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0017428398132324, 'test/num_examples': 10000, 'score': 49195.26766204834, 'total_duration': 55372.13287234306, 'accumulated_submission_time': 49195.26766204834, 'accumulated_eval_time': 6166.253857374191, 'accumulated_logging_time': 5.067118167877197}
I0204 04:27:38.817345 139774434195200 logging_writer.py:48] [105559] accumulated_eval_time=6166.253857, accumulated_logging_time=5.067118, accumulated_submission_time=49195.267662, global_step=105559, preemption_count=0, score=49195.267662, test/accuracy=0.543300, test/loss=2.001743, test/num_examples=10000, total_duration=55372.132872, train/accuracy=0.746914, train/loss=0.995157, validation/accuracy=0.668920, validation/loss=1.348962, validation/num_examples=50000
I0204 04:27:56.228567 139774417409792 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.30876886844635, loss=4.054730415344238
I0204 04:28:41.544839 139774434195200 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.5252020359039307, loss=2.4596400260925293
I0204 04:29:29.130450 139774417409792 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.5753194093704224, loss=2.1128151416778564
I0204 04:30:15.969258 139774434195200 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.306525707244873, loss=3.635406017303467
I0204 04:31:03.301208 139774417409792 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.3532204627990723, loss=3.556553840637207
I0204 04:31:50.394095 139774434195200 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.5058726072311401, loss=2.4610724449157715
I0204 04:32:37.898387 139774417409792 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.5194053649902344, loss=2.0872836112976074
I0204 04:33:25.122745 139774434195200 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.5492873191833496, loss=2.380985975265503
I0204 04:34:12.554450 139774417409792 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.6432684659957886, loss=4.65209436416626
I0204 04:34:39.065025 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:34:51.137190 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:35:29.514645 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:35:31.123740 139936116377408 submission_runner.py:408] Time since start: 55844.48s, 	Step: 106458, 	{'train/accuracy': 0.730273425579071, 'train/loss': 1.1089922189712524, 'validation/accuracy': 0.6736199855804443, 'validation/loss': 1.3686671257019043, 'validation/num_examples': 50000, 'test/accuracy': 0.5472000241279602, 'test/loss': 2.037179470062256, 'test/num_examples': 10000, 'score': 49615.44148349762, 'total_duration': 55844.475420475006, 'accumulated_submission_time': 49615.44148349762, 'accumulated_eval_time': 6218.311373949051, 'accumulated_logging_time': 5.114095211029053}
I0204 04:35:31.158741 139774434195200 logging_writer.py:48] [106458] accumulated_eval_time=6218.311374, accumulated_logging_time=5.114095, accumulated_submission_time=49615.441483, global_step=106458, preemption_count=0, score=49615.441483, test/accuracy=0.547200, test/loss=2.037179, test/num_examples=10000, total_duration=55844.475420, train/accuracy=0.730273, train/loss=1.108992, validation/accuracy=0.673620, validation/loss=1.368667, validation/num_examples=50000
I0204 04:35:48.992283 139774417409792 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.5550001859664917, loss=2.113976001739502
I0204 04:36:33.842433 139774434195200 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.9803099632263184, loss=2.1310667991638184
I0204 04:37:20.740711 139774417409792 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.4962327480316162, loss=2.104722261428833
I0204 04:38:07.798280 139774434195200 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.3345597982406616, loss=2.7676501274108887
I0204 04:38:54.698046 139774417409792 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.5369807481765747, loss=4.586930751800537
I0204 04:39:41.247318 139774434195200 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.5468640327453613, loss=2.312242031097412
I0204 04:40:28.196266 139774417409792 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.4770828485488892, loss=3.6954591274261475
I0204 04:41:15.171946 139774434195200 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.5434412956237793, loss=3.3537588119506836
I0204 04:42:02.154185 139774417409792 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.3688825368881226, loss=4.261565685272217
I0204 04:42:31.395571 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:42:43.186489 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:43:20.517704 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:43:22.125414 139936116377408 submission_runner.py:408] Time since start: 56315.48s, 	Step: 107364, 	{'train/accuracy': 0.731640636920929, 'train/loss': 1.0827327966690063, 'validation/accuracy': 0.6683599948883057, 'validation/loss': 1.369884967803955, 'validation/num_examples': 50000, 'test/accuracy': 0.5441000461578369, 'test/loss': 2.013227939605713, 'test/num_examples': 10000, 'score': 50035.62106966972, 'total_duration': 56315.47707438469, 'accumulated_submission_time': 50035.62106966972, 'accumulated_eval_time': 6269.040015935898, 'accumulated_logging_time': 5.159966707229614}
I0204 04:43:22.158165 139774434195200 logging_writer.py:48] [107364] accumulated_eval_time=6269.040016, accumulated_logging_time=5.159967, accumulated_submission_time=50035.621070, global_step=107364, preemption_count=0, score=50035.621070, test/accuracy=0.544100, test/loss=2.013228, test/num_examples=10000, total_duration=56315.477074, train/accuracy=0.731641, train/loss=1.082733, validation/accuracy=0.668360, validation/loss=1.369885, validation/num_examples=50000
I0204 04:43:37.493951 139774417409792 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.481136679649353, loss=2.434338331222534
I0204 04:44:22.758764 139774434195200 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.5906621217727661, loss=2.0566229820251465
I0204 04:45:10.156332 139774417409792 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.5971477031707764, loss=2.1790051460266113
I0204 04:45:57.589297 139774434195200 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.4848774671554565, loss=2.7839834690093994
I0204 04:46:44.935288 139774417409792 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.5519453287124634, loss=4.809857368469238
I0204 04:47:32.012993 139774434195200 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.2608295679092407, loss=4.334214687347412
I0204 04:48:19.372634 139774417409792 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.599931240081787, loss=2.091027021408081
I0204 04:49:06.679568 139774434195200 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.5488091707229614, loss=3.0153818130493164
I0204 04:49:53.676488 139774417409792 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.4656357765197754, loss=2.171114683151245
I0204 04:50:22.240796 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:50:34.149403 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:51:13.319484 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:51:14.932858 139936116377408 submission_runner.py:408] Time since start: 56788.28s, 	Step: 108262, 	{'train/accuracy': 0.7414648532867432, 'train/loss': 1.0510865449905396, 'validation/accuracy': 0.6689800024032593, 'validation/loss': 1.3708804845809937, 'validation/num_examples': 50000, 'test/accuracy': 0.5437000393867493, 'test/loss': 2.018178701400757, 'test/num_examples': 10000, 'score': 50455.644053697586, 'total_duration': 56788.2848212719, 'accumulated_submission_time': 50455.644053697586, 'accumulated_eval_time': 6321.7311725616455, 'accumulated_logging_time': 5.206001281738281}
I0204 04:51:14.971828 139774434195200 logging_writer.py:48] [108262] accumulated_eval_time=6321.731173, accumulated_logging_time=5.206001, accumulated_submission_time=50455.644054, global_step=108262, preemption_count=0, score=50455.644054, test/accuracy=0.543700, test/loss=2.018179, test/num_examples=10000, total_duration=56788.284821, train/accuracy=0.741465, train/loss=1.051087, validation/accuracy=0.668980, validation/loss=1.370880, validation/num_examples=50000
I0204 04:51:31.144015 139774417409792 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.5755536556243896, loss=2.0470736026763916
I0204 04:52:16.416714 139774434195200 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.4435546398162842, loss=3.1254630088806152
I0204 04:53:03.352872 139774417409792 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.563143014907837, loss=2.375912666320801
I0204 04:53:50.227168 139774434195200 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.7259005308151245, loss=2.1528868675231934
I0204 04:54:37.422391 139774417409792 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.5898551940917969, loss=2.118011951446533
I0204 04:55:24.465662 139774434195200 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.634497046470642, loss=1.9937087297439575
I0204 04:56:11.685812 139774417409792 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.5685553550720215, loss=2.31996750831604
I0204 04:56:58.755159 139774434195200 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.6211847066879272, loss=2.4972989559173584
I0204 04:57:45.955869 139774417409792 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.5816737413406372, loss=2.032700777053833
I0204 04:58:15.385143 139936116377408 spec.py:321] Evaluating on the training split.
I0204 04:58:27.366764 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 04:59:06.871762 139936116377408 spec.py:349] Evaluating on the test split.
I0204 04:59:08.487301 139936116377408 submission_runner.py:408] Time since start: 57261.84s, 	Step: 109164, 	{'train/accuracy': 0.7342773079872131, 'train/loss': 1.0697695016860962, 'validation/accuracy': 0.6750999689102173, 'validation/loss': 1.3431190252304077, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 1.997771978378296, 'test/num_examples': 10000, 'score': 50875.999675273895, 'total_duration': 57261.839129686356, 'accumulated_submission_time': 50875.999675273895, 'accumulated_eval_time': 6374.8322966098785, 'accumulated_logging_time': 5.255663156509399}
I0204 04:59:08.526529 139774434195200 logging_writer.py:48] [109164] accumulated_eval_time=6374.832297, accumulated_logging_time=5.255663, accumulated_submission_time=50875.999675, global_step=109164, preemption_count=0, score=50875.999675, test/accuracy=0.551000, test/loss=1.997772, test/num_examples=10000, total_duration=57261.839130, train/accuracy=0.734277, train/loss=1.069770, validation/accuracy=0.675100, validation/loss=1.343119, validation/num_examples=50000
I0204 04:59:23.862541 139774417409792 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.6248642206192017, loss=1.9638350009918213
I0204 05:00:08.554766 139774434195200 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.5602105855941772, loss=4.20610237121582
I0204 05:00:55.339265 139774417409792 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.6719706058502197, loss=2.1370415687561035
I0204 05:01:42.579409 139774434195200 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.6519817113876343, loss=2.019705295562744
I0204 05:02:29.329304 139774417409792 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.435459017753601, loss=4.6842193603515625
I0204 05:03:16.007047 139774434195200 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.523908019065857, loss=3.1510531902313232
I0204 05:04:02.823221 139774417409792 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.6212751865386963, loss=2.083203077316284
I0204 05:04:49.470105 139774434195200 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.5349459648132324, loss=2.5972862243652344
I0204 05:05:36.534541 139774417409792 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.4678596258163452, loss=2.6668949127197266
I0204 05:06:08.726123 139936116377408 spec.py:321] Evaluating on the training split.
I0204 05:06:20.492447 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 05:06:57.885452 139936116377408 spec.py:349] Evaluating on the test split.
I0204 05:06:59.517757 139936116377408 submission_runner.py:408] Time since start: 57732.87s, 	Step: 110071, 	{'train/accuracy': 0.7425194978713989, 'train/loss': 1.0702048540115356, 'validation/accuracy': 0.6782400012016296, 'validation/loss': 1.3490768671035767, 'validation/num_examples': 50000, 'test/accuracy': 0.550000011920929, 'test/loss': 2.0083322525024414, 'test/num_examples': 10000, 'score': 51296.140135765076, 'total_duration': 57732.86963033676, 'accumulated_submission_time': 51296.140135765076, 'accumulated_eval_time': 6425.622935056686, 'accumulated_logging_time': 5.307147979736328}
I0204 05:06:59.551348 139774434195200 logging_writer.py:48] [110071] accumulated_eval_time=6425.622935, accumulated_logging_time=5.307148, accumulated_submission_time=51296.140136, global_step=110071, preemption_count=0, score=51296.140136, test/accuracy=0.550000, test/loss=2.008332, test/num_examples=10000, total_duration=57732.869630, train/accuracy=0.742519, train/loss=1.070205, validation/accuracy=0.678240, validation/loss=1.349077, validation/num_examples=50000
I0204 05:07:11.985113 139774417409792 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.47654390335083, loss=2.8107082843780518
I0204 05:07:56.369597 139774434195200 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.5202877521514893, loss=2.4273457527160645
I0204 05:08:43.063262 139774417409792 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.5518007278442383, loss=2.1060729026794434
I0204 05:09:29.974260 139774434195200 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.6829886436462402, loss=2.0134124755859375
I0204 05:10:16.860751 139774417409792 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.4267321825027466, loss=4.229796409606934
I0204 05:11:03.688184 139774434195200 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.5237772464752197, loss=2.0319175720214844
I0204 05:11:50.429243 139774417409792 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.5475624799728394, loss=2.0837502479553223
I0204 05:12:37.189625 139774434195200 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.5847305059432983, loss=2.36820125579834
I0204 05:13:24.211195 139774417409792 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.5691325664520264, loss=4.633338928222656
I0204 05:13:59.804862 139936116377408 spec.py:321] Evaluating on the training split.
I0204 05:14:11.816582 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 05:14:49.770785 139936116377408 spec.py:349] Evaluating on the test split.
I0204 05:14:51.378500 139936116377408 submission_runner.py:408] Time since start: 58204.73s, 	Step: 110978, 	{'train/accuracy': 0.7479101419448853, 'train/loss': 1.0002694129943848, 'validation/accuracy': 0.6759999990463257, 'validation/loss': 1.3279269933700562, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 1.9583526849746704, 'test/num_examples': 10000, 'score': 51716.33648109436, 'total_duration': 58204.73041367531, 'accumulated_submission_time': 51716.33648109436, 'accumulated_eval_time': 6477.19561457634, 'accumulated_logging_time': 5.351512670516968}
I0204 05:14:51.413880 139774434195200 logging_writer.py:48] [110978] accumulated_eval_time=6477.195615, accumulated_logging_time=5.351513, accumulated_submission_time=51716.336481, global_step=110978, preemption_count=0, score=51716.336481, test/accuracy=0.557400, test/loss=1.958353, test/num_examples=10000, total_duration=58204.730414, train/accuracy=0.747910, train/loss=1.000269, validation/accuracy=0.676000, validation/loss=1.327927, validation/num_examples=50000
I0204 05:15:00.955716 139774417409792 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.5252952575683594, loss=2.5220961570739746
I0204 05:15:45.139090 139774434195200 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.4652260541915894, loss=3.4351704120635986
I0204 05:16:32.363104 139774417409792 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.342245101928711, loss=3.673008441925049
I0204 05:17:19.500746 139774434195200 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.5434834957122803, loss=2.021833896636963
I0204 05:18:07.029526 139774417409792 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.4328454732894897, loss=3.954116106033325
I0204 05:18:53.627272 139774434195200 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.3531640768051147, loss=3.963886260986328
I0204 05:19:40.837163 139774417409792 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.9867929220199585, loss=2.2146530151367188
I0204 05:20:27.793517 139774434195200 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.6937105655670166, loss=1.9743424654006958
I0204 05:21:14.853527 139774417409792 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.6187448501586914, loss=2.2833714485168457
I0204 05:21:51.848690 139936116377408 spec.py:321] Evaluating on the training split.
I0204 05:22:04.129408 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 05:22:41.614619 139936116377408 spec.py:349] Evaluating on the test split.
I0204 05:22:43.216691 139936116377408 submission_runner.py:408] Time since start: 58676.57s, 	Step: 111880, 	{'train/accuracy': 0.7358593344688416, 'train/loss': 1.0901877880096436, 'validation/accuracy': 0.6753999590873718, 'validation/loss': 1.3613321781158447, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.0046257972717285, 'test/num_examples': 10000, 'score': 52136.712656497955, 'total_duration': 58676.56856536865, 'accumulated_submission_time': 52136.712656497955, 'accumulated_eval_time': 6528.562636137009, 'accumulated_logging_time': 5.398941516876221}
I0204 05:22:43.250744 139774434195200 logging_writer.py:48] [111880] accumulated_eval_time=6528.562636, accumulated_logging_time=5.398942, accumulated_submission_time=52136.712656, global_step=111880, preemption_count=0, score=52136.712656, test/accuracy=0.553500, test/loss=2.004626, test/num_examples=10000, total_duration=58676.568565, train/accuracy=0.735859, train/loss=1.090188, validation/accuracy=0.675400, validation/loss=1.361332, validation/num_examples=50000
I0204 05:22:51.953898 139774417409792 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.4947052001953125, loss=2.6791305541992188
I0204 05:23:36.128462 139774434195200 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.3791230916976929, loss=4.1740312576293945
I0204 05:24:22.785432 139774417409792 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.5482981204986572, loss=3.9274559020996094
I0204 05:25:09.729526 139774434195200 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.7705614566802979, loss=2.0622081756591797
I0204 05:25:56.402049 139774417409792 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.4139761924743652, loss=2.568208932876587
I0204 05:26:43.352826 139774434195200 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.5730557441711426, loss=2.170297622680664
I0204 05:27:30.420069 139774417409792 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.6088640689849854, loss=2.0380477905273438
I0204 05:28:17.495976 139774434195200 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.344600796699524, loss=2.964205503463745
I0204 05:29:04.377145 139774417409792 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.6874593496322632, loss=2.453596591949463
I0204 05:29:43.597860 139936116377408 spec.py:321] Evaluating on the training split.
I0204 05:29:55.512862 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 05:30:33.625968 139936116377408 spec.py:349] Evaluating on the test split.
I0204 05:30:35.236658 139936116377408 submission_runner.py:408] Time since start: 59148.59s, 	Step: 112785, 	{'train/accuracy': 0.7383984327316284, 'train/loss': 1.064749836921692, 'validation/accuracy': 0.6764199733734131, 'validation/loss': 1.340150237083435, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 1.9855674505233765, 'test/num_examples': 10000, 'score': 52557.00045776367, 'total_duration': 59148.58823752403, 'accumulated_submission_time': 52557.00045776367, 'accumulated_eval_time': 6580.2001440525055, 'accumulated_logging_time': 5.444950819015503}
I0204 05:30:35.271080 139774434195200 logging_writer.py:48] [112785] accumulated_eval_time=6580.200144, accumulated_logging_time=5.444951, accumulated_submission_time=52557.000458, global_step=112785, preemption_count=0, score=52557.000458, test/accuracy=0.552900, test/loss=1.985567, test/num_examples=10000, total_duration=59148.588238, train/accuracy=0.738398, train/loss=1.064750, validation/accuracy=0.676420, validation/loss=1.340150, validation/num_examples=50000
I0204 05:30:41.905602 139774417409792 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.7772260904312134, loss=2.089877128601074
I0204 05:31:26.200029 139774434195200 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.5786504745483398, loss=2.204775810241699
I0204 05:32:13.672455 139774417409792 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.8673522472381592, loss=1.950608253479004
I0204 05:33:01.024151 139774434195200 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.6704461574554443, loss=2.000365972518921
I0204 05:33:48.295236 139774417409792 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.6009045839309692, loss=1.996570110321045
I0204 05:34:35.259655 139774434195200 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.8897435665130615, loss=2.1157116889953613
I0204 05:35:22.451472 139774417409792 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.589356780052185, loss=4.200406074523926
I0204 05:36:09.833153 139774434195200 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.4871082305908203, loss=2.736699342727661
I0204 05:36:57.171457 139774417409792 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.4466568231582642, loss=2.20721435546875
I0204 05:37:35.287870 139936116377408 spec.py:321] Evaluating on the training split.
I0204 05:37:47.330450 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 05:38:27.680484 139936116377408 spec.py:349] Evaluating on the test split.
I0204 05:38:29.287863 139936116377408 submission_runner.py:408] Time since start: 59622.64s, 	Step: 113682, 	{'train/accuracy': 0.7493749856948853, 'train/loss': 1.0180168151855469, 'validation/accuracy': 0.6759999990463257, 'validation/loss': 1.3451844453811646, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 1.991071343421936, 'test/num_examples': 10000, 'score': 52976.95989322662, 'total_duration': 59622.63963222504, 'accumulated_submission_time': 52976.95989322662, 'accumulated_eval_time': 6634.199034690857, 'accumulated_logging_time': 5.4898645877838135}
I0204 05:38:29.324429 139774434195200 logging_writer.py:48] [113682] accumulated_eval_time=6634.199035, accumulated_logging_time=5.489865, accumulated_submission_time=52976.959893, global_step=113682, preemption_count=0, score=52976.959893, test/accuracy=0.552800, test/loss=1.991071, test/num_examples=10000, total_duration=59622.639632, train/accuracy=0.749375, train/loss=1.018017, validation/accuracy=0.676000, validation/loss=1.345184, validation/num_examples=50000
I0204 05:38:37.187730 139774417409792 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.6345244646072388, loss=2.130336284637451
I0204 05:39:21.284005 139774434195200 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.6627752780914307, loss=1.9558892250061035
I0204 05:40:08.019813 139774417409792 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.6570755243301392, loss=2.2566514015197754
I0204 05:40:55.099838 139774434195200 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.5102753639221191, loss=3.125235080718994
I0204 05:41:41.967047 139774417409792 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.6623700857162476, loss=1.9560915231704712
I0204 05:42:29.204779 139774434195200 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.5807725191116333, loss=1.9757823944091797
I0204 05:43:16.376085 139774417409792 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.6135481595993042, loss=1.9653794765472412
I0204 05:44:03.372046 139774434195200 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.5071405172348022, loss=2.7276129722595215
I0204 05:44:50.584981 139774417409792 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.756824254989624, loss=2.056668281555176
I0204 05:45:29.689097 139936116377408 spec.py:321] Evaluating on the training split.
I0204 05:45:41.667715 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 05:46:21.441051 139936116377408 spec.py:349] Evaluating on the test split.
I0204 05:46:23.044737 139936116377408 submission_runner.py:408] Time since start: 60096.40s, 	Step: 114585, 	{'train/accuracy': 0.74609375, 'train/loss': 1.0239732265472412, 'validation/accuracy': 0.6865999698638916, 'validation/loss': 1.2994922399520874, 'validation/num_examples': 50000, 'test/accuracy': 0.5619000196456909, 'test/loss': 1.937618613243103, 'test/num_examples': 10000, 'score': 53397.26622343063, 'total_duration': 60096.39663481712, 'accumulated_submission_time': 53397.26622343063, 'accumulated_eval_time': 6687.5537185668945, 'accumulated_logging_time': 5.537170886993408}
I0204 05:46:23.083469 139774434195200 logging_writer.py:48] [114585] accumulated_eval_time=6687.553719, accumulated_logging_time=5.537171, accumulated_submission_time=53397.266223, global_step=114585, preemption_count=0, score=53397.266223, test/accuracy=0.561900, test/loss=1.937619, test/num_examples=10000, total_duration=60096.396635, train/accuracy=0.746094, train/loss=1.023973, validation/accuracy=0.686600, validation/loss=1.299492, validation/num_examples=50000
I0204 05:46:29.718539 139774417409792 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.6170822381973267, loss=2.2942891120910645
I0204 05:47:13.691689 139774434195200 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.7505731582641602, loss=2.151592969894409
I0204 05:48:00.382808 139774417409792 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.5612945556640625, loss=4.652756690979004
I0204 05:48:47.228074 139774434195200 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.6958953142166138, loss=2.4036333560943604
I0204 05:49:33.840941 139774417409792 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.5334854125976562, loss=2.7306301593780518
I0204 05:50:20.943748 139774434195200 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.6409494876861572, loss=1.940821647644043
I0204 05:51:08.082836 139774417409792 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.5040794610977173, loss=3.213705062866211
I0204 05:51:54.844340 139774434195200 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.6374961137771606, loss=2.0898516178131104
I0204 05:52:42.034420 139774417409792 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.607771873474121, loss=2.0014395713806152
I0204 05:53:23.731389 139936116377408 spec.py:321] Evaluating on the training split.
I0204 05:53:35.645012 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 05:54:13.177196 139936116377408 spec.py:349] Evaluating on the test split.
I0204 05:54:14.788553 139936116377408 submission_runner.py:408] Time since start: 60568.14s, 	Step: 115490, 	{'train/accuracy': 0.7510351538658142, 'train/loss': 1.0083297491073608, 'validation/accuracy': 0.687279999256134, 'validation/loss': 1.2991552352905273, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 1.9419201612472534, 'test/num_examples': 10000, 'score': 53817.85648846626, 'total_duration': 60568.140437603, 'accumulated_submission_time': 53817.85648846626, 'accumulated_eval_time': 6738.60989689827, 'accumulated_logging_time': 5.5869903564453125}
I0204 05:54:14.822642 139774434195200 logging_writer.py:48] [115490] accumulated_eval_time=6738.609897, accumulated_logging_time=5.586990, accumulated_submission_time=53817.856488, global_step=115490, preemption_count=0, score=53817.856488, test/accuracy=0.562000, test/loss=1.941920, test/num_examples=10000, total_duration=60568.140438, train/accuracy=0.751035, train/loss=1.008330, validation/accuracy=0.687280, validation/loss=1.299155, validation/num_examples=50000
I0204 05:54:19.384275 139774417409792 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.6000713109970093, loss=2.036160945892334
I0204 05:55:03.221684 139774434195200 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.7094627618789673, loss=2.172661781311035
I0204 05:55:49.844861 139774417409792 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.3990461826324463, loss=3.176743745803833
I0204 05:56:36.974313 139774434195200 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.5958309173583984, loss=1.9919593334197998
I0204 05:57:24.241163 139774417409792 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.5001702308654785, loss=4.525589466094971
I0204 05:58:11.067623 139774434195200 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.7705501317977905, loss=2.0666754245758057
I0204 05:58:58.092333 139774417409792 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.59144926071167, loss=3.674966812133789
I0204 05:59:45.512931 139774434195200 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.636757731437683, loss=2.096384286880493
I0204 06:00:32.705258 139774417409792 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.6227655410766602, loss=2.074113607406616
I0204 06:01:14.874300 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:01:26.878661 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:02:05.276064 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:02:06.880604 139936116377408 submission_runner.py:408] Time since start: 61040.23s, 	Step: 116390, 	{'train/accuracy': 0.7574414014816284, 'train/loss': 0.960770845413208, 'validation/accuracy': 0.6850199699401855, 'validation/loss': 1.277678370475769, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 1.9282153844833374, 'test/num_examples': 10000, 'score': 54237.85001659393, 'total_duration': 61040.232503175735, 'accumulated_submission_time': 54237.85001659393, 'accumulated_eval_time': 6790.61523938179, 'accumulated_logging_time': 5.63244891166687}
I0204 06:02:06.914931 139774434195200 logging_writer.py:48] [116390] accumulated_eval_time=6790.615239, accumulated_logging_time=5.632449, accumulated_submission_time=54237.850017, global_step=116390, preemption_count=0, score=54237.850017, test/accuracy=0.564400, test/loss=1.928215, test/num_examples=10000, total_duration=61040.232503, train/accuracy=0.757441, train/loss=0.960771, validation/accuracy=0.685020, validation/loss=1.277678, validation/num_examples=50000
I0204 06:02:11.478008 139774417409792 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.6632308959960938, loss=2.3058414459228516
I0204 06:02:55.295103 139774434195200 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.3940783739089966, loss=3.267270088195801
I0204 06:03:42.463887 139774417409792 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.6595771312713623, loss=1.9854118824005127
I0204 06:04:29.422978 139774434195200 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.6845585107803345, loss=1.9973844289779663
I0204 06:05:16.498305 139774417409792 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.7764396667480469, loss=2.0311243534088135
I0204 06:06:03.996826 139774434195200 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.8634706735610962, loss=2.015273332595825
I0204 06:06:51.268375 139774417409792 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.7075767517089844, loss=2.103060722351074
I0204 06:07:38.494158 139774434195200 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.8197612762451172, loss=2.124868869781494
I0204 06:08:25.565136 139774417409792 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.5282623767852783, loss=3.168043375015259
I0204 06:09:07.056832 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:09:18.782687 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:09:56.360425 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:09:57.972207 139936116377408 submission_runner.py:408] Time since start: 61511.32s, 	Step: 117290, 	{'train/accuracy': 0.7455663681030273, 'train/loss': 1.0378332138061523, 'validation/accuracy': 0.6839199662208557, 'validation/loss': 1.3181474208831787, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 1.9513601064682007, 'test/num_examples': 10000, 'score': 54657.93375110626, 'total_duration': 61511.32391309738, 'accumulated_submission_time': 54657.93375110626, 'accumulated_eval_time': 6841.529457330704, 'accumulated_logging_time': 5.67902684211731}
I0204 06:09:58.010179 139774434195200 logging_writer.py:48] [117290] accumulated_eval_time=6841.529457, accumulated_logging_time=5.679027, accumulated_submission_time=54657.933751, global_step=117290, preemption_count=0, score=54657.933751, test/accuracy=0.561800, test/loss=1.951360, test/num_examples=10000, total_duration=61511.323913, train/accuracy=0.745566, train/loss=1.037833, validation/accuracy=0.683920, validation/loss=1.318147, validation/num_examples=50000
I0204 06:10:02.575476 139774417409792 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.6337422132492065, loss=1.970383644104004
I0204 06:10:46.479932 139774434195200 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.5929663181304932, loss=4.5859503746032715
I0204 06:11:33.609504 139774417409792 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.5982778072357178, loss=4.212228775024414
I0204 06:12:21.405822 139774434195200 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.478771686553955, loss=4.1596760749816895
I0204 06:13:08.891060 139774417409792 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.5760055780410767, loss=1.8958802223205566
I0204 06:13:56.522287 139774434195200 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.728837251663208, loss=2.0317676067352295
I0204 06:14:43.880754 139774417409792 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.5630533695220947, loss=4.1708221435546875
I0204 06:15:31.115721 139774434195200 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.5566455125808716, loss=4.564907073974609
I0204 06:16:18.867899 139774417409792 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.551327109336853, loss=4.100118160247803
I0204 06:16:58.285079 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:17:10.388821 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:17:48.774028 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:17:50.385093 139936116377408 submission_runner.py:408] Time since start: 61983.74s, 	Step: 118185, 	{'train/accuracy': 0.75244140625, 'train/loss': 1.0247377157211304, 'validation/accuracy': 0.6880599856376648, 'validation/loss': 1.310234546661377, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.9548426866531372, 'test/num_examples': 10000, 'score': 55078.15105628967, 'total_duration': 61983.7370262146, 'accumulated_submission_time': 55078.15105628967, 'accumulated_eval_time': 6893.628536224365, 'accumulated_logging_time': 5.7277820110321045}
I0204 06:17:50.426618 139774434195200 logging_writer.py:48] [118185] accumulated_eval_time=6893.628536, accumulated_logging_time=5.727782, accumulated_submission_time=55078.151056, global_step=118185, preemption_count=0, score=55078.151056, test/accuracy=0.563400, test/loss=1.954843, test/num_examples=10000, total_duration=61983.737026, train/accuracy=0.752441, train/loss=1.024738, validation/accuracy=0.688060, validation/loss=1.310235, validation/num_examples=50000
I0204 06:17:57.054695 139774417409792 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.5177485942840576, loss=3.1269209384918213
I0204 06:18:40.879679 139774434195200 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.574962854385376, loss=2.950705051422119
I0204 06:19:27.689039 139774417409792 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.7295162677764893, loss=2.018585681915283
I0204 06:20:14.781700 139774434195200 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.4297449588775635, loss=3.44999623298645
I0204 06:21:01.902602 139774417409792 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.5853519439697266, loss=2.4784817695617676
I0204 06:21:49.119387 139774434195200 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.7810145616531372, loss=1.9886329174041748
I0204 06:22:36.176510 139774417409792 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.5600184202194214, loss=2.351774215698242
I0204 06:23:23.459568 139774434195200 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.0329606533050537, loss=1.9566713571548462
I0204 06:24:10.553618 139774417409792 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.5478445291519165, loss=2.2919769287109375
I0204 06:24:50.701983 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:25:02.822662 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:25:42.827517 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:25:44.427904 139936116377408 submission_runner.py:408] Time since start: 62457.78s, 	Step: 119087, 	{'train/accuracy': 0.758105456829071, 'train/loss': 0.9552485346794128, 'validation/accuracy': 0.6878600120544434, 'validation/loss': 1.2778600454330444, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 1.9243587255477905, 'test/num_examples': 10000, 'score': 55498.36799407005, 'total_duration': 62457.77957677841, 'accumulated_submission_time': 55498.36799407005, 'accumulated_eval_time': 6947.353275775909, 'accumulated_logging_time': 5.781080007553101}
I0204 06:25:44.466152 139774434195200 logging_writer.py:48] [119087] accumulated_eval_time=6947.353276, accumulated_logging_time=5.781080, accumulated_submission_time=55498.367994, global_step=119087, preemption_count=0, score=55498.367994, test/accuracy=0.560500, test/loss=1.924359, test/num_examples=10000, total_duration=62457.779577, train/accuracy=0.758105, train/loss=0.955249, validation/accuracy=0.687860, validation/loss=1.277860, validation/num_examples=50000
I0204 06:25:50.264940 139774417409792 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.6338622570037842, loss=2.880300521850586
I0204 06:26:33.872842 139774434195200 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.473757028579712, loss=3.6943535804748535
I0204 06:27:20.361741 139774417409792 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.7708185911178589, loss=1.9452322721481323
I0204 06:28:07.378705 139774434195200 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.545196771621704, loss=3.3455519676208496
I0204 06:28:54.256702 139774417409792 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.8893848657608032, loss=2.1897239685058594
I0204 06:29:41.248918 139774434195200 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.5473452806472778, loss=3.314568281173706
I0204 06:30:28.319580 139774417409792 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.7491040229797363, loss=4.209334373474121
I0204 06:31:15.381675 139774434195200 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.726812481880188, loss=2.827045202255249
I0204 06:32:03.010061 139774417409792 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.8262028694152832, loss=1.9415969848632812
I0204 06:32:44.747237 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:32:56.767206 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:33:34.876376 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:33:36.491826 139936116377408 submission_runner.py:408] Time since start: 62929.84s, 	Step: 119991, 	{'train/accuracy': 0.7644335627555847, 'train/loss': 0.937131941318512, 'validation/accuracy': 0.6890999674797058, 'validation/loss': 1.2613743543624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.8964364528656006, 'test/num_examples': 10000, 'score': 55918.59027314186, 'total_duration': 62929.843616724014, 'accumulated_submission_time': 55918.59027314186, 'accumulated_eval_time': 6999.096809387207, 'accumulated_logging_time': 5.830645799636841}
I0204 06:33:36.526721 139774434195200 logging_writer.py:48] [119991] accumulated_eval_time=6999.096809, accumulated_logging_time=5.830646, accumulated_submission_time=55918.590273, global_step=119991, preemption_count=0, score=55918.590273, test/accuracy=0.563900, test/loss=1.896436, test/num_examples=10000, total_duration=62929.843617, train/accuracy=0.764434, train/loss=0.937132, validation/accuracy=0.689100, validation/loss=1.261374, validation/num_examples=50000
I0204 06:33:40.669515 139774417409792 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.7714924812316895, loss=2.2812421321868896
I0204 06:34:24.636606 139774434195200 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.6579644680023193, loss=2.025400161743164
I0204 06:35:11.333673 139774417409792 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.5644904375076294, loss=2.2657883167266846
I0204 06:35:58.603596 139774434195200 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.7126446962356567, loss=2.656459093093872
I0204 06:36:45.374621 139774417409792 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.7956528663635254, loss=2.0340352058410645
I0204 06:37:32.510210 139774434195200 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.937509298324585, loss=2.1018121242523193
I0204 06:38:19.533472 139774417409792 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.069819211959839, loss=2.0094246864318848
I0204 06:39:06.437282 139774434195200 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.7410658597946167, loss=1.9215149879455566
I0204 06:39:53.381644 139774417409792 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.5396358966827393, loss=3.1900060176849365
I0204 06:40:36.968564 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:40:48.708070 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:41:25.859485 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:41:27.465507 139936116377408 submission_runner.py:408] Time since start: 63400.82s, 	Step: 120894, 	{'train/accuracy': 0.7629492282867432, 'train/loss': 0.9582368731498718, 'validation/accuracy': 0.6942200064659119, 'validation/loss': 1.2611232995986938, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.8966219425201416, 'test/num_examples': 10000, 'score': 56338.974491119385, 'total_duration': 63400.81737446785, 'accumulated_submission_time': 56338.974491119385, 'accumulated_eval_time': 7049.592758893967, 'accumulated_logging_time': 5.876109838485718}
I0204 06:41:27.500919 139774434195200 logging_writer.py:48] [120894] accumulated_eval_time=7049.592759, accumulated_logging_time=5.876110, accumulated_submission_time=56338.974491, global_step=120894, preemption_count=0, score=56338.974491, test/accuracy=0.566100, test/loss=1.896622, test/num_examples=10000, total_duration=63400.817374, train/accuracy=0.762949, train/loss=0.958237, validation/accuracy=0.694220, validation/loss=1.261123, validation/num_examples=50000
I0204 06:41:30.403291 139774417409792 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.687357783317566, loss=2.186094045639038
I0204 06:42:14.066263 139774434195200 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.8225934505462646, loss=3.9426140785217285
I0204 06:43:00.873813 139774417409792 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.8080005645751953, loss=1.999354362487793
I0204 06:43:47.723003 139774434195200 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.577286958694458, loss=3.8072683811187744
I0204 06:44:34.798788 139774417409792 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.6202888488769531, loss=2.569023370742798
I0204 06:45:21.861293 139774434195200 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.7265024185180664, loss=3.7459144592285156
I0204 06:46:08.821409 139774417409792 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.7531505823135376, loss=1.950669765472412
I0204 06:46:55.709989 139774434195200 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.7759696245193481, loss=1.9827330112457275
I0204 06:47:42.952954 139774417409792 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.7344526052474976, loss=1.9712224006652832
I0204 06:48:27.639044 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:48:39.584798 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:49:18.848809 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:49:20.460114 139936116377408 submission_runner.py:408] Time since start: 63873.81s, 	Step: 121797, 	{'train/accuracy': 0.7608593702316284, 'train/loss': 0.9513280391693115, 'validation/accuracy': 0.6955199837684631, 'validation/loss': 1.2552928924560547, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.8828102350234985, 'test/num_examples': 10000, 'score': 56759.05333805084, 'total_duration': 63873.81185340881, 'accumulated_submission_time': 56759.05333805084, 'accumulated_eval_time': 7102.412714481354, 'accumulated_logging_time': 5.9235618114471436}
I0204 06:49:20.499284 139774434195200 logging_writer.py:48] [121797] accumulated_eval_time=7102.412714, accumulated_logging_time=5.923562, accumulated_submission_time=56759.053338, global_step=121797, preemption_count=0, score=56759.053338, test/accuracy=0.571300, test/loss=1.882810, test/num_examples=10000, total_duration=63873.811853, train/accuracy=0.760859, train/loss=0.951328, validation/accuracy=0.695520, validation/loss=1.255293, validation/num_examples=50000
I0204 06:49:22.159161 139774417409792 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.547223687171936, loss=2.390015125274658
I0204 06:50:05.471816 139774434195200 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.7720563411712646, loss=1.984065055847168
I0204 06:50:51.971284 139774417409792 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.941543459892273, loss=2.0738062858581543
I0204 06:51:39.067471 139774434195200 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.770537257194519, loss=1.9371761083602905
I0204 06:52:26.148754 139774417409792 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.5618144273757935, loss=2.4565956592559814
I0204 06:53:13.167413 139774434195200 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.6791237592697144, loss=2.4664437770843506
I0204 06:53:59.898087 139774417409792 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.7602386474609375, loss=3.9371562004089355
I0204 06:54:46.558426 139774434195200 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.8104875087738037, loss=2.0238020420074463
I0204 06:55:34.072134 139774417409792 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.7061331272125244, loss=1.9315159320831299
I0204 06:56:20.965160 139774434195200 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.93551504611969, loss=1.943741798400879
I0204 06:56:20.979809 139936116377408 spec.py:321] Evaluating on the training split.
I0204 06:56:32.844965 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 06:57:09.974452 139936116377408 spec.py:349] Evaluating on the test split.
I0204 06:57:11.579166 139936116377408 submission_runner.py:408] Time since start: 64344.93s, 	Step: 122701, 	{'train/accuracy': 0.7781640291213989, 'train/loss': 0.8763917088508606, 'validation/accuracy': 0.6922599673271179, 'validation/loss': 1.2633213996887207, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 1.9046696424484253, 'test/num_examples': 10000, 'score': 57179.4742333889, 'total_duration': 64344.930874586105, 'accumulated_submission_time': 57179.4742333889, 'accumulated_eval_time': 7153.010915279388, 'accumulated_logging_time': 5.974011421203613}
I0204 06:57:11.614988 139774417409792 logging_writer.py:48] [122701] accumulated_eval_time=7153.010915, accumulated_logging_time=5.974011, accumulated_submission_time=57179.474233, global_step=122701, preemption_count=0, score=57179.474233, test/accuracy=0.567000, test/loss=1.904670, test/num_examples=10000, total_duration=64344.930875, train/accuracy=0.778164, train/loss=0.876392, validation/accuracy=0.692260, validation/loss=1.263321, validation/num_examples=50000
I0204 06:57:54.690179 139774434195200 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.6568576097488403, loss=1.9219131469726562
I0204 06:58:41.765294 139774417409792 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.7695873975753784, loss=2.232837677001953
I0204 06:59:28.985224 139774434195200 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.5793349742889404, loss=2.663729190826416
I0204 07:00:16.014780 139774417409792 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.6530108451843262, loss=3.2053232192993164
I0204 07:01:03.281718 139774434195200 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.903672218322754, loss=2.0263421535491943
I0204 07:01:50.308009 139774417409792 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.004396915435791, loss=2.1977574825286865
I0204 07:02:37.337518 139774434195200 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.7628653049468994, loss=1.9221525192260742
I0204 07:03:24.264627 139774417409792 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.0130560398101807, loss=2.127716541290283
I0204 07:04:11.294957 139774434195200 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.639320731163025, loss=4.068779468536377
I0204 07:04:11.854935 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:04:23.685374 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 07:05:02.095917 139936116377408 spec.py:349] Evaluating on the test split.
I0204 07:05:03.707157 139936116377408 submission_runner.py:408] Time since start: 64817.06s, 	Step: 123603, 	{'train/accuracy': 0.7577733993530273, 'train/loss': 0.9707837700843811, 'validation/accuracy': 0.6915799975395203, 'validation/loss': 1.2661999464035034, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 1.9076391458511353, 'test/num_examples': 10000, 'score': 57599.657051086426, 'total_duration': 64817.058979034424, 'accumulated_submission_time': 57599.657051086426, 'accumulated_eval_time': 7204.862086057663, 'accumulated_logging_time': 6.020724296569824}
I0204 07:05:03.744149 139774417409792 logging_writer.py:48] [123603] accumulated_eval_time=7204.862086, accumulated_logging_time=6.020724, accumulated_submission_time=57599.657051, global_step=123603, preemption_count=0, score=57599.657051, test/accuracy=0.567000, test/loss=1.907639, test/num_examples=10000, total_duration=64817.058979, train/accuracy=0.757773, train/loss=0.970784, validation/accuracy=0.691580, validation/loss=1.266200, validation/num_examples=50000
I0204 07:05:45.952465 139774434195200 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.788450002670288, loss=1.9603179693222046
I0204 07:06:32.775199 139774417409792 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.7139053344726562, loss=3.879262924194336
I0204 07:07:19.998038 139774434195200 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.5959569215774536, loss=3.609259605407715
I0204 07:08:07.292778 139774417409792 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.9990599155426025, loss=1.8925883769989014
I0204 07:08:54.747452 139774434195200 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.6596286296844482, loss=2.116476058959961
I0204 07:09:41.893573 139774417409792 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.9564993381500244, loss=2.013387680053711
I0204 07:10:28.980965 139774434195200 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.8149807453155518, loss=2.0167365074157715
I0204 07:11:16.118474 139774417409792 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.6518974304199219, loss=3.040203094482422
I0204 07:12:03.203313 139774434195200 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.580336570739746, loss=3.2065834999084473
I0204 07:12:03.901372 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:12:15.934395 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 07:12:54.033272 139936116377408 spec.py:349] Evaluating on the test split.
I0204 07:12:55.638030 139936116377408 submission_runner.py:408] Time since start: 65288.99s, 	Step: 124503, 	{'train/accuracy': 0.7660741806030273, 'train/loss': 0.9606534838676453, 'validation/accuracy': 0.6967200040817261, 'validation/loss': 1.2727292776107788, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.917148470878601, 'test/num_examples': 10000, 'score': 58019.7577214241, 'total_duration': 65288.98992753029, 'accumulated_submission_time': 58019.7577214241, 'accumulated_eval_time': 7256.597772836685, 'accumulated_logging_time': 6.068363428115845}
I0204 07:12:55.672926 139774417409792 logging_writer.py:48] [124503] accumulated_eval_time=7256.597773, accumulated_logging_time=6.068363, accumulated_submission_time=58019.757721, global_step=124503, preemption_count=0, score=58019.757721, test/accuracy=0.571900, test/loss=1.917148, test/num_examples=10000, total_duration=65288.989928, train/accuracy=0.766074, train/loss=0.960653, validation/accuracy=0.696720, validation/loss=1.272729, validation/num_examples=50000
I0204 07:13:37.847541 139774434195200 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.7110650539398193, loss=2.7920212745666504
I0204 07:14:24.294828 139774417409792 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.5589544773101807, loss=3.3425917625427246
I0204 07:15:11.605202 139774434195200 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.0106565952301025, loss=1.8538267612457275
I0204 07:15:58.286359 139774417409792 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.6838574409484863, loss=1.9075074195861816
I0204 07:16:45.284089 139774434195200 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.8360769748687744, loss=2.119607448577881
I0204 07:17:32.502721 139774417409792 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.0184807777404785, loss=1.8510867357254028
I0204 07:18:19.733181 139774434195200 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.7196269035339355, loss=3.6702542304992676
I0204 07:19:06.835458 139774417409792 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.5372060537338257, loss=2.1075029373168945
I0204 07:19:54.345673 139774434195200 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.8807581663131714, loss=1.856589674949646
I0204 07:19:55.781809 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:20:07.798214 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 07:20:45.001898 139936116377408 spec.py:349] Evaluating on the test split.
I0204 07:20:46.606786 139936116377408 submission_runner.py:408] Time since start: 65759.96s, 	Step: 125405, 	{'train/accuracy': 0.7822851538658142, 'train/loss': 0.8623960018157959, 'validation/accuracy': 0.6998800039291382, 'validation/loss': 1.230411410331726, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 1.8689360618591309, 'test/num_examples': 10000, 'score': 58439.809049129486, 'total_duration': 65759.95871829987, 'accumulated_submission_time': 58439.809049129486, 'accumulated_eval_time': 7307.421809196472, 'accumulated_logging_time': 6.114432096481323}
I0204 07:20:46.644499 139774417409792 logging_writer.py:48] [125405] accumulated_eval_time=7307.421809, accumulated_logging_time=6.114432, accumulated_submission_time=58439.809049, global_step=125405, preemption_count=0, score=58439.809049, test/accuracy=0.574600, test/loss=1.868936, test/num_examples=10000, total_duration=65759.958718, train/accuracy=0.782285, train/loss=0.862396, validation/accuracy=0.699880, validation/loss=1.230411, validation/num_examples=50000
I0204 07:21:28.046721 139774434195200 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.7317909002304077, loss=1.8454875946044922
I0204 07:22:14.882580 139774417409792 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.6510038375854492, loss=4.050689697265625
I0204 07:23:02.074170 139774434195200 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.6326563358306885, loss=3.245666027069092
I0204 07:23:49.271684 139774434195200 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.6945793628692627, loss=2.2750167846679688
I0204 07:24:36.648358 139774417409792 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.762683391571045, loss=4.421877384185791
I0204 07:25:23.917656 139774434195200 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.7521777153015137, loss=1.7784303426742554
I0204 07:26:11.303702 139774417409792 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.6584508419036865, loss=2.3246898651123047
I0204 07:26:58.602476 139774434195200 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.8768752813339233, loss=2.4148218631744385
I0204 07:27:45.763162 139774417409792 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.7670930624008179, loss=4.425049781799316
I0204 07:27:46.913506 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:27:58.766538 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 07:28:36.511383 139936116377408 spec.py:349] Evaluating on the test split.
I0204 07:28:38.118175 139936116377408 submission_runner.py:408] Time since start: 66231.47s, 	Step: 126304, 	{'train/accuracy': 0.764843761920929, 'train/loss': 0.9532604217529297, 'validation/accuracy': 0.6993199586868286, 'validation/loss': 1.2452449798583984, 'validation/num_examples': 50000, 'test/accuracy': 0.5796000361442566, 'test/loss': 1.8750362396240234, 'test/num_examples': 10000, 'score': 58860.01943874359, 'total_duration': 66231.4698665142, 'accumulated_submission_time': 58860.01943874359, 'accumulated_eval_time': 7358.625300168991, 'accumulated_logging_time': 6.164425373077393}
I0204 07:28:38.152902 139774434195200 logging_writer.py:48] [126304] accumulated_eval_time=7358.625300, accumulated_logging_time=6.164425, accumulated_submission_time=58860.019439, global_step=126304, preemption_count=0, score=58860.019439, test/accuracy=0.579600, test/loss=1.875036, test/num_examples=10000, total_duration=66231.469867, train/accuracy=0.764844, train/loss=0.953260, validation/accuracy=0.699320, validation/loss=1.245245, validation/num_examples=50000
I0204 07:29:19.736059 139774417409792 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.5661108493804932, loss=4.3099775314331055
I0204 07:30:06.447010 139774434195200 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.7286427021026611, loss=1.7392539978027344
I0204 07:30:53.618526 139774417409792 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.6503483057022095, loss=4.006409168243408
I0204 07:31:41.202527 139774434195200 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.8099521398544312, loss=2.0073976516723633
I0204 07:32:28.200573 139774417409792 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.7528140544891357, loss=1.9383776187896729
I0204 07:33:15.359119 139774434195200 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.73402738571167, loss=3.1865203380584717
I0204 07:34:02.498399 139774417409792 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.926231026649475, loss=2.007134437561035
I0204 07:34:49.234772 139774434195200 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.8669731616973877, loss=1.7809607982635498
I0204 07:35:36.423936 139774417409792 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.8508262634277344, loss=2.326449394226074
I0204 07:35:38.539848 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:35:50.383219 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 07:36:28.947005 139936116377408 spec.py:349] Evaluating on the test split.
I0204 07:36:30.554483 139936116377408 submission_runner.py:408] Time since start: 66703.91s, 	Step: 127206, 	{'train/accuracy': 0.7702929377555847, 'train/loss': 0.9010846614837646, 'validation/accuracy': 0.7008799910545349, 'validation/loss': 1.2191319465637207, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.8637452125549316, 'test/num_examples': 10000, 'score': 59280.34904909134, 'total_duration': 66703.90618491173, 'accumulated_submission_time': 59280.34904909134, 'accumulated_eval_time': 7410.638778209686, 'accumulated_logging_time': 6.2095441818237305}
I0204 07:36:30.593863 139774434195200 logging_writer.py:48] [127206] accumulated_eval_time=7410.638778, accumulated_logging_time=6.209544, accumulated_submission_time=59280.349049, global_step=127206, preemption_count=0, score=59280.349049, test/accuracy=0.575000, test/loss=1.863745, test/num_examples=10000, total_duration=66703.906185, train/accuracy=0.770293, train/loss=0.901085, validation/accuracy=0.700880, validation/loss=1.219132, validation/num_examples=50000
I0204 07:37:11.548174 139774417409792 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.7237884998321533, loss=2.0387089252471924
I0204 07:37:58.089392 139774434195200 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.7776963710784912, loss=1.7805585861206055
I0204 07:38:44.822164 139774417409792 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.960605263710022, loss=1.8592162132263184
I0204 07:39:31.828372 139774434195200 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.9740850925445557, loss=1.993154764175415
I0204 07:40:18.977882 139774417409792 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.7893768548965454, loss=2.169053077697754
I0204 07:41:05.922854 139774434195200 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.7913857698440552, loss=2.005913257598877
I0204 07:41:53.119024 139774417409792 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.041086435317993, loss=3.6481916904449463
I0204 07:42:40.377205 139774434195200 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.8911521434783936, loss=1.8571746349334717
I0204 07:43:27.417171 139774417409792 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.8277355432510376, loss=2.547696352005005
I0204 07:43:30.872473 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:43:42.748731 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 07:44:21.280937 139936116377408 spec.py:349] Evaluating on the test split.
I0204 07:44:22.894691 139936116377408 submission_runner.py:408] Time since start: 67176.25s, 	Step: 128109, 	{'train/accuracy': 0.7816601395606995, 'train/loss': 0.8531544208526611, 'validation/accuracy': 0.7025600075721741, 'validation/loss': 1.214359164237976, 'validation/num_examples': 50000, 'test/accuracy': 0.579300045967102, 'test/loss': 1.843841552734375, 'test/num_examples': 10000, 'score': 59700.57011055946, 'total_duration': 67176.2463812828, 'accumulated_submission_time': 59700.57011055946, 'accumulated_eval_time': 7462.659845113754, 'accumulated_logging_time': 6.2601258754730225}
I0204 07:44:22.939026 139774434195200 logging_writer.py:48] [128109] accumulated_eval_time=7462.659845, accumulated_logging_time=6.260126, accumulated_submission_time=59700.570111, global_step=128109, preemption_count=0, score=59700.570111, test/accuracy=0.579300, test/loss=1.843842, test/num_examples=10000, total_duration=67176.246381, train/accuracy=0.781660, train/loss=0.853154, validation/accuracy=0.702560, validation/loss=1.214359, validation/num_examples=50000
I0204 07:45:02.582596 139774417409792 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.7304021120071411, loss=1.9065498113632202
I0204 07:45:49.650177 139774434195200 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.9678397178649902, loss=1.9903008937835693
I0204 07:46:36.972021 139774417409792 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.814496397972107, loss=4.326282501220703
I0204 07:47:24.100455 139774434195200 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.7650002241134644, loss=3.4018843173980713
I0204 07:48:11.371592 139774417409792 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.6737478971481323, loss=2.9091007709503174
I0204 07:48:58.510426 139774434195200 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.7770570516586304, loss=3.3422694206237793
I0204 07:49:45.878468 139774417409792 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.9641813039779663, loss=1.845210313796997
I0204 07:50:33.091549 139774434195200 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.769728422164917, loss=2.161586284637451
I0204 07:51:20.370798 139774417409792 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.7803514003753662, loss=3.1994779109954834
I0204 07:51:22.970835 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:51:34.852603 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 07:52:15.088808 139936116377408 spec.py:349] Evaluating on the test split.
I0204 07:52:16.698287 139936116377408 submission_runner.py:408] Time since start: 67650.05s, 	Step: 129007, 	{'train/accuracy': 0.7702538967132568, 'train/loss': 0.9246523976325989, 'validation/accuracy': 0.7058799862861633, 'validation/loss': 1.214882493019104, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.853320598602295, 'test/num_examples': 10000, 'score': 60120.5441262722, 'total_duration': 67650.05022072792, 'accumulated_submission_time': 60120.5441262722, 'accumulated_eval_time': 7516.386365413666, 'accumulated_logging_time': 6.315888404846191}
I0204 07:52:16.737138 139774434195200 logging_writer.py:48] [129007] accumulated_eval_time=7516.386365, accumulated_logging_time=6.315888, accumulated_submission_time=60120.544126, global_step=129007, preemption_count=0, score=60120.544126, test/accuracy=0.580800, test/loss=1.853321, test/num_examples=10000, total_duration=67650.050221, train/accuracy=0.770254, train/loss=0.924652, validation/accuracy=0.705880, validation/loss=1.214882, validation/num_examples=50000
I0204 07:52:57.029050 139774417409792 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.656136393547058, loss=2.8851640224456787
I0204 07:53:43.693856 139774434195200 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.715405821800232, loss=2.6404941082000732
I0204 07:54:30.807756 139774417409792 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.0750749111175537, loss=1.8626039028167725
I0204 07:55:18.077707 139774434195200 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.061546564102173, loss=1.8217060565948486
I0204 07:56:05.031444 139774417409792 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.7339959144592285, loss=2.5058529376983643
I0204 07:56:52.005345 139774434195200 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.1361122131347656, loss=2.0547683238983154
I0204 07:57:39.156924 139774417409792 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.669126033782959, loss=4.473903179168701
I0204 07:58:26.265524 139774434195200 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.910696029663086, loss=1.8033522367477417
I0204 07:59:13.583011 139774417409792 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.8999459743499756, loss=4.4084343910217285
I0204 07:59:16.988397 139936116377408 spec.py:321] Evaluating on the training split.
I0204 07:59:29.013146 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:00:06.648063 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:00:08.253243 139936116377408 submission_runner.py:408] Time since start: 68121.60s, 	Step: 129909, 	{'train/accuracy': 0.7751367092132568, 'train/loss': 0.8928495049476624, 'validation/accuracy': 0.7060399651527405, 'validation/loss': 1.207680106163025, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.8382052183151245, 'test/num_examples': 10000, 'score': 60540.735575675964, 'total_duration': 68121.60476827621, 'accumulated_submission_time': 60540.735575675964, 'accumulated_eval_time': 7567.649868488312, 'accumulated_logging_time': 6.368206024169922}
I0204 08:00:08.291439 139774434195200 logging_writer.py:48] [129909] accumulated_eval_time=7567.649868, accumulated_logging_time=6.368206, accumulated_submission_time=60540.735576, global_step=129909, preemption_count=0, score=60540.735576, test/accuracy=0.580800, test/loss=1.838205, test/num_examples=10000, total_duration=68121.604768, train/accuracy=0.775137, train/loss=0.892850, validation/accuracy=0.706040, validation/loss=1.207680, validation/num_examples=50000
I0204 08:00:48.103864 139774417409792 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.0117440223693848, loss=2.5616183280944824
I0204 08:01:34.779683 139774434195200 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.8151323795318604, loss=3.023444652557373
I0204 08:02:22.342211 139774417409792 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.7589908838272095, loss=3.5955185890197754
I0204 08:03:09.320791 139774434195200 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.8900259733200073, loss=1.8051319122314453
I0204 08:03:56.041514 139774417409792 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.7804665565490723, loss=3.8128762245178223
I0204 08:04:42.805656 139774434195200 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.9714537858963013, loss=1.7563018798828125
I0204 08:05:29.914221 139774417409792 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.8217345476150513, loss=3.7819743156433105
I0204 08:06:16.386256 139774434195200 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.8206140995025635, loss=1.8602721691131592
I0204 08:07:03.538627 139774417409792 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.6409436464309692, loss=3.087153911590576
I0204 08:07:08.311269 139936116377408 spec.py:321] Evaluating on the training split.
I0204 08:07:20.326935 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:07:58.537445 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:08:00.159188 139936116377408 submission_runner.py:408] Time since start: 68593.51s, 	Step: 130812, 	{'train/accuracy': 0.7800390720367432, 'train/loss': 0.8839324712753296, 'validation/accuracy': 0.7036600112915039, 'validation/loss': 1.2200918197631836, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8685678243637085, 'test/num_examples': 10000, 'score': 60960.218658447266, 'total_duration': 68593.51099419594, 'accumulated_submission_time': 60960.218658447266, 'accumulated_eval_time': 7619.496718406677, 'accumulated_logging_time': 6.896216869354248}
I0204 08:08:00.204057 139774434195200 logging_writer.py:48] [130812] accumulated_eval_time=7619.496718, accumulated_logging_time=6.896217, accumulated_submission_time=60960.218658, global_step=130812, preemption_count=0, score=60960.218658, test/accuracy=0.578100, test/loss=1.868568, test/num_examples=10000, total_duration=68593.510994, train/accuracy=0.780039, train/loss=0.883932, validation/accuracy=0.703660, validation/loss=1.220092, validation/num_examples=50000
I0204 08:08:38.413843 139774417409792 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.9524403810501099, loss=3.8536977767944336
I0204 08:09:24.855710 139774434195200 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.79353928565979, loss=1.7771451473236084
I0204 08:10:11.955443 139774417409792 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.9533274173736572, loss=3.3982889652252197
I0204 08:10:58.887394 139774434195200 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.9066082239151, loss=1.9157658815383911
I0204 08:11:46.324124 139774417409792 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.9853309392929077, loss=3.99438738822937
I0204 08:12:33.430293 139774434195200 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.7847262620925903, loss=3.370403289794922
I0204 08:13:20.857086 139774417409792 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.6378498077392578, loss=4.344616889953613
I0204 08:14:07.758310 139774434195200 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.927539348602295, loss=2.1903696060180664
I0204 08:14:54.695065 139774417409792 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.8110053539276123, loss=1.8092032670974731
I0204 08:15:00.417278 139936116377408 spec.py:321] Evaluating on the training split.
I0204 08:15:12.581416 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:15:52.981381 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:15:54.599878 139936116377408 submission_runner.py:408] Time since start: 69067.95s, 	Step: 131714, 	{'train/accuracy': 0.7735546827316284, 'train/loss': 0.887130081653595, 'validation/accuracy': 0.7057799696922302, 'validation/loss': 1.1913654804229736, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.8401563167572021, 'test/num_examples': 10000, 'score': 61380.37313580513, 'total_duration': 69067.9515554905, 'accumulated_submission_time': 61380.37313580513, 'accumulated_eval_time': 7673.678128242493, 'accumulated_logging_time': 6.953572034835815}
I0204 08:15:54.640337 139774434195200 logging_writer.py:48] [131714] accumulated_eval_time=7673.678128, accumulated_logging_time=6.953572, accumulated_submission_time=61380.373136, global_step=131714, preemption_count=0, score=61380.373136, test/accuracy=0.577100, test/loss=1.840156, test/num_examples=10000, total_duration=69067.951555, train/accuracy=0.773555, train/loss=0.887130, validation/accuracy=0.705780, validation/loss=1.191365, validation/num_examples=50000
I0204 08:16:31.866667 139774417409792 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.0909407138824463, loss=1.9456528425216675
I0204 08:17:19.000472 139774434195200 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.8435938358306885, loss=1.8506931066513062
I0204 08:18:06.488014 139774417409792 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.0456626415252686, loss=1.9975836277008057
I0204 08:18:53.580922 139774434195200 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.890138030052185, loss=1.9281561374664307
I0204 08:19:40.655812 139774417409792 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.9941353797912598, loss=4.064794063568115
I0204 08:20:27.668447 139774434195200 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.0313620567321777, loss=1.864753007888794
I0204 08:21:14.992971 139774417409792 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.993263602256775, loss=1.910058856010437
I0204 08:22:02.181998 139774434195200 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.864775538444519, loss=2.070197105407715
I0204 08:22:49.330483 139774417409792 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.7864826917648315, loss=1.7856419086456299
I0204 08:22:54.891319 139936116377408 spec.py:321] Evaluating on the training split.
I0204 08:23:06.998013 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:23:46.944336 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:23:48.551139 139936116377408 submission_runner.py:408] Time since start: 69541.90s, 	Step: 132613, 	{'train/accuracy': 0.7814843654632568, 'train/loss': 0.8491562604904175, 'validation/accuracy': 0.7097600102424622, 'validation/loss': 1.1710385084152222, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.8052948713302612, 'test/num_examples': 10000, 'score': 61800.565898656845, 'total_duration': 69541.9029185772, 'accumulated_submission_time': 61800.565898656845, 'accumulated_eval_time': 7727.336859464645, 'accumulated_logging_time': 7.005480766296387}
I0204 08:23:48.590779 139774434195200 logging_writer.py:48] [132613] accumulated_eval_time=7727.336859, accumulated_logging_time=7.005481, accumulated_submission_time=61800.565899, global_step=132613, preemption_count=0, score=61800.565899, test/accuracy=0.588900, test/loss=1.805295, test/num_examples=10000, total_duration=69541.902919, train/accuracy=0.781484, train/loss=0.849156, validation/accuracy=0.709760, validation/loss=1.171039, validation/num_examples=50000
I0204 08:24:26.288790 139774417409792 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.854967713356018, loss=1.7528009414672852
I0204 08:25:13.461381 139774434195200 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.8262468576431274, loss=3.2055017948150635
I0204 08:26:00.744218 139774417409792 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.9486881494522095, loss=1.9517183303833008
I0204 08:26:47.865055 139774434195200 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.8903290033340454, loss=2.2273690700531006
I0204 08:27:35.160434 139774417409792 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.0210654735565186, loss=1.8085317611694336
I0204 08:28:22.415283 139774434195200 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.079286575317383, loss=1.7615755796432495
I0204 08:29:09.825237 139774417409792 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.835762619972229, loss=3.0542004108428955
I0204 08:29:57.031398 139774434195200 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.8944690227508545, loss=1.9649120569229126
I0204 08:30:44.365309 139774417409792 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.0523784160614014, loss=1.9592266082763672
I0204 08:30:48.814152 139936116377408 spec.py:321] Evaluating on the training split.
I0204 08:31:00.831255 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:31:40.354732 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:31:41.956526 139936116377408 submission_runner.py:408] Time since start: 70015.31s, 	Step: 133511, 	{'train/accuracy': 0.793652355670929, 'train/loss': 0.8182350993156433, 'validation/accuracy': 0.7105199694633484, 'validation/loss': 1.1751962900161743, 'validation/num_examples': 50000, 'test/accuracy': 0.5838000178337097, 'test/loss': 1.8079313039779663, 'test/num_examples': 10000, 'score': 62220.73200464249, 'total_duration': 70015.30837655067, 'accumulated_submission_time': 62220.73200464249, 'accumulated_eval_time': 7780.478232383728, 'accumulated_logging_time': 7.055763244628906}
I0204 08:31:41.997371 139774434195200 logging_writer.py:48] [133511] accumulated_eval_time=7780.478232, accumulated_logging_time=7.055763, accumulated_submission_time=62220.732005, global_step=133511, preemption_count=0, score=62220.732005, test/accuracy=0.583800, test/loss=1.807931, test/num_examples=10000, total_duration=70015.308377, train/accuracy=0.793652, train/loss=0.818235, validation/accuracy=0.710520, validation/loss=1.175196, validation/num_examples=50000
I0204 08:32:20.440569 139774417409792 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.6897621154785156, loss=4.119447231292725
I0204 08:33:07.345429 139774434195200 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.8917945623397827, loss=3.5593762397766113
I0204 08:33:54.521861 139774417409792 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.0060014724731445, loss=1.8903582096099854
I0204 08:34:41.866687 139774434195200 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.827574372291565, loss=2.009249448776245
I0204 08:35:28.939614 139774417409792 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.7380999326705933, loss=2.3939976692199707
I0204 08:36:16.134911 139774434195200 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.7779721021652222, loss=2.428316593170166
I0204 08:37:03.065378 139774417409792 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.8546136617660522, loss=2.7867796421051025
I0204 08:37:50.332957 139774434195200 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.9751724004745483, loss=4.288895606994629
I0204 08:38:37.433888 139774417409792 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.728966236114502, loss=2.221174478530884
I0204 08:38:42.272421 139936116377408 spec.py:321] Evaluating on the training split.
I0204 08:38:54.293890 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:39:30.869136 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:39:32.486505 139936116377408 submission_runner.py:408] Time since start: 70485.84s, 	Step: 134412, 	{'train/accuracy': 0.7824413776397705, 'train/loss': 0.8449131846427917, 'validation/accuracy': 0.7128199934959412, 'validation/loss': 1.1581400632858276, 'validation/num_examples': 50000, 'test/accuracy': 0.591200053691864, 'test/loss': 1.7911345958709717, 'test/num_examples': 10000, 'score': 62640.94975614548, 'total_duration': 70485.8378021717, 'accumulated_submission_time': 62640.94975614548, 'accumulated_eval_time': 7830.6907432079315, 'accumulated_logging_time': 7.107162237167358}
I0204 08:39:32.529348 139774434195200 logging_writer.py:48] [134412] accumulated_eval_time=7830.690743, accumulated_logging_time=7.107162, accumulated_submission_time=62640.949756, global_step=134412, preemption_count=0, score=62640.949756, test/accuracy=0.591200, test/loss=1.791135, test/num_examples=10000, total_duration=70485.837802, train/accuracy=0.782441, train/loss=0.844913, validation/accuracy=0.712820, validation/loss=1.158140, validation/num_examples=50000
I0204 08:40:10.643436 139774417409792 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.8778295516967773, loss=2.0761759281158447
I0204 08:40:57.220496 139774434195200 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.9347496032714844, loss=1.815492033958435
I0204 08:41:44.543651 139774417409792 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.792243242263794, loss=2.5949771404266357
I0204 08:42:31.534056 139774434195200 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.818169355392456, loss=2.929832935333252
I0204 08:43:18.959450 139774417409792 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.0266103744506836, loss=1.9882466793060303
I0204 08:44:06.090176 139774434195200 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.8157650232315063, loss=2.672502279281616
I0204 08:44:53.063975 139774417409792 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.0341567993164062, loss=1.805051326751709
I0204 08:45:40.613834 139774434195200 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.0911660194396973, loss=1.682833194732666
I0204 08:46:27.811960 139774417409792 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.131378412246704, loss=1.9400222301483154
I0204 08:46:32.683269 139936116377408 spec.py:321] Evaluating on the training split.
I0204 08:46:44.538749 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:47:22.217777 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:47:23.824580 139936116377408 submission_runner.py:408] Time since start: 70957.18s, 	Step: 135312, 	{'train/accuracy': 0.782910168170929, 'train/loss': 0.8488849401473999, 'validation/accuracy': 0.7114799618721008, 'validation/loss': 1.1706856489181519, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 1.7887169122695923, 'test/num_examples': 10000, 'score': 63061.04555249214, 'total_duration': 70957.17614507675, 'accumulated_submission_time': 63061.04555249214, 'accumulated_eval_time': 7881.830750465393, 'accumulated_logging_time': 7.161731958389282}
I0204 08:47:23.870007 139774434195200 logging_writer.py:48] [135312] accumulated_eval_time=7881.830750, accumulated_logging_time=7.161732, accumulated_submission_time=63061.045552, global_step=135312, preemption_count=0, score=63061.045552, test/accuracy=0.589900, test/loss=1.788717, test/num_examples=10000, total_duration=70957.176145, train/accuracy=0.782910, train/loss=0.848885, validation/accuracy=0.711480, validation/loss=1.170686, validation/num_examples=50000
I0204 08:48:01.945288 139774417409792 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.9634416103363037, loss=1.7286933660507202
I0204 08:48:48.558275 139774434195200 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.0957086086273193, loss=1.8743691444396973
I0204 08:49:36.066076 139774417409792 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.9809554815292358, loss=1.8705761432647705
I0204 08:50:22.878349 139774434195200 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.8906382322311401, loss=2.139976978302002
I0204 08:51:10.157452 139774417409792 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.827280044555664, loss=1.985122799873352
I0204 08:51:57.385808 139774434195200 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.8841259479522705, loss=2.126862049102783
I0204 08:52:44.348988 139774417409792 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.9151854515075684, loss=2.4551680088043213
I0204 08:53:31.376437 139774434195200 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.8060529232025146, loss=2.4515466690063477
I0204 08:54:18.513640 139774417409792 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.0384280681610107, loss=1.792859435081482
I0204 08:54:23.837064 139936116377408 spec.py:321] Evaluating on the training split.
I0204 08:54:35.714738 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 08:55:14.194827 139936116377408 spec.py:349] Evaluating on the test split.
I0204 08:55:15.797675 139936116377408 submission_runner.py:408] Time since start: 71429.15s, 	Step: 136213, 	{'train/accuracy': 0.7852538824081421, 'train/loss': 0.8555783033370972, 'validation/accuracy': 0.7083799839019775, 'validation/loss': 1.213556170463562, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.8471421003341675, 'test/num_examples': 10000, 'score': 63480.95566868782, 'total_duration': 71429.1494038105, 'accumulated_submission_time': 63480.95566868782, 'accumulated_eval_time': 7933.790218830109, 'accumulated_logging_time': 7.2181336879730225}
I0204 08:55:15.835768 139774434195200 logging_writer.py:48] [136213] accumulated_eval_time=7933.790219, accumulated_logging_time=7.218134, accumulated_submission_time=63480.955669, global_step=136213, preemption_count=0, score=63480.955669, test/accuracy=0.583700, test/loss=1.847142, test/num_examples=10000, total_duration=71429.149404, train/accuracy=0.785254, train/loss=0.855578, validation/accuracy=0.708380, validation/loss=1.213556, validation/num_examples=50000
I0204 08:55:53.241574 139774417409792 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.6585731506347656, loss=3.0399765968322754
I0204 08:56:40.483153 139774434195200 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.0468697547912598, loss=1.753063678741455
I0204 08:57:27.790404 139774417409792 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.1828410625457764, loss=1.8986144065856934
I0204 08:58:14.715324 139774434195200 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.004859685897827, loss=1.7567461729049683
I0204 08:59:01.593820 139774417409792 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.0118584632873535, loss=1.756157398223877
I0204 08:59:48.533768 139774434195200 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.9809461832046509, loss=1.8814994096755981
I0204 09:00:35.669501 139774417409792 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.038909673690796, loss=1.7696185111999512
I0204 09:01:23.000286 139774434195200 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.98770272731781, loss=1.7432794570922852
I0204 09:02:10.375679 139774417409792 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.8955459594726562, loss=3.6232855319976807
I0204 09:02:16.024730 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:02:27.942629 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:03:06.027295 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:03:07.632271 139936116377408 submission_runner.py:408] Time since start: 71900.98s, 	Step: 137114, 	{'train/accuracy': 0.7789257764816284, 'train/loss': 0.8748505711555481, 'validation/accuracy': 0.7115199565887451, 'validation/loss': 1.1856586933135986, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.8299528360366821, 'test/num_examples': 10000, 'score': 63901.086811065674, 'total_duration': 71900.98412513733, 'accumulated_submission_time': 63901.086811065674, 'accumulated_eval_time': 7985.396743774414, 'accumulated_logging_time': 7.266888856887817}
I0204 09:03:07.672262 139774434195200 logging_writer.py:48] [137114] accumulated_eval_time=7985.396744, accumulated_logging_time=7.266889, accumulated_submission_time=63901.086811, global_step=137114, preemption_count=0, score=63901.086811, test/accuracy=0.583700, test/loss=1.829953, test/num_examples=10000, total_duration=71900.984125, train/accuracy=0.778926, train/loss=0.874851, validation/accuracy=0.711520, validation/loss=1.185659, validation/num_examples=50000
I0204 09:03:44.943449 139774417409792 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.057234048843384, loss=1.9465687274932861
I0204 09:04:31.860986 139774434195200 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.044726610183716, loss=1.818143606185913
I0204 09:05:19.155771 139774417409792 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.0029654502868652, loss=1.8878026008605957
I0204 09:06:06.565293 139774434195200 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.8675496578216553, loss=3.844372272491455
I0204 09:06:54.048929 139774417409792 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.0844528675079346, loss=1.912859559059143
I0204 09:07:41.781388 139774434195200 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.9879018068313599, loss=1.7891206741333008
I0204 09:08:29.240787 139774417409792 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.9340850114822388, loss=1.7253937721252441
I0204 09:09:16.828386 139774434195200 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.9577406644821167, loss=1.7296665906906128
I0204 09:10:04.462159 139774417409792 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.1302437782287598, loss=1.8371317386627197
I0204 09:10:07.813224 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:10:19.725337 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:10:57.336250 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:10:58.949040 139936116377408 submission_runner.py:408] Time since start: 72372.30s, 	Step: 138009, 	{'train/accuracy': 0.79212886095047, 'train/loss': 0.8283264636993408, 'validation/accuracy': 0.717960000038147, 'validation/loss': 1.1578574180603027, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.7869914770126343, 'test/num_examples': 10000, 'score': 64321.17126393318, 'total_duration': 72372.30072402954, 'accumulated_submission_time': 64321.17126393318, 'accumulated_eval_time': 8036.531369924545, 'accumulated_logging_time': 7.317673444747925}
I0204 09:10:58.989704 139774434195200 logging_writer.py:48] [138009] accumulated_eval_time=8036.531370, accumulated_logging_time=7.317673, accumulated_submission_time=64321.171264, global_step=138009, preemption_count=0, score=64321.171264, test/accuracy=0.590700, test/loss=1.786991, test/num_examples=10000, total_duration=72372.300724, train/accuracy=0.792129, train/loss=0.828326, validation/accuracy=0.717960, validation/loss=1.157857, validation/num_examples=50000
I0204 09:11:38.545987 139774417409792 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.9285943508148193, loss=3.0311596393585205
I0204 09:12:25.640857 139774434195200 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.176633358001709, loss=1.8155272006988525
I0204 09:13:12.934779 139774417409792 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.156499147415161, loss=1.661749005317688
I0204 09:14:00.134952 139774434195200 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.8991161584854126, loss=3.9646849632263184
I0204 09:14:47.350878 139774417409792 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.9609386920928955, loss=1.7919641733169556
I0204 09:15:34.555956 139774434195200 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.10569429397583, loss=1.7830883264541626
I0204 09:16:21.777637 139774417409792 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.044009208679199, loss=1.6569768190383911
I0204 09:17:08.984005 139774434195200 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.055246591567993, loss=1.725891351699829
I0204 09:17:56.365544 139774417409792 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.162262201309204, loss=1.8322023153305054
I0204 09:17:59.299806 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:18:11.413756 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:18:49.296147 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:18:50.902287 139936116377408 submission_runner.py:408] Time since start: 72844.25s, 	Step: 138908, 	{'train/accuracy': 0.7974609136581421, 'train/loss': 0.8029530048370361, 'validation/accuracy': 0.7170799970626831, 'validation/loss': 1.1547406911849976, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.7757651805877686, 'test/num_examples': 10000, 'score': 64741.42377257347, 'total_duration': 72844.25393557549, 'accumulated_submission_time': 64741.42377257347, 'accumulated_eval_time': 8088.132665634155, 'accumulated_logging_time': 7.369465112686157}
I0204 09:18:50.944029 139774434195200 logging_writer.py:48] [138908] accumulated_eval_time=8088.132666, accumulated_logging_time=7.369465, accumulated_submission_time=64741.423773, global_step=138908, preemption_count=0, score=64741.423773, test/accuracy=0.595800, test/loss=1.775765, test/num_examples=10000, total_duration=72844.253936, train/accuracy=0.797461, train/loss=0.802953, validation/accuracy=0.717080, validation/loss=1.154741, validation/num_examples=50000
I0204 09:19:30.897573 139774417409792 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.0620150566101074, loss=3.342454195022583
I0204 09:20:17.727708 139774434195200 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.8953849077224731, loss=1.767820119857788
I0204 09:21:04.830157 139774417409792 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.1699347496032715, loss=1.7381260395050049
I0204 09:21:51.990444 139774434195200 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.22365403175354, loss=2.123267650604248
I0204 09:22:39.064252 139774417409792 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.9195680618286133, loss=3.4373936653137207
I0204 09:23:26.520960 139774434195200 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.853428840637207, loss=3.885899066925049
I0204 09:24:13.795968 139774417409792 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.190199613571167, loss=1.7784391641616821
I0204 09:25:00.920362 139774434195200 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.9729996919631958, loss=1.6501245498657227
I0204 09:25:48.251986 139774417409792 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.9896172285079956, loss=3.243102550506592
I0204 09:25:51.122556 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:26:03.377217 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:26:41.983825 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:26:43.607359 139936116377408 submission_runner.py:408] Time since start: 73316.96s, 	Step: 139808, 	{'train/accuracy': 0.7875585556030273, 'train/loss': 0.8376666903495789, 'validation/accuracy': 0.7172799706459045, 'validation/loss': 1.1552940607070923, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.7900747060775757, 'test/num_examples': 10000, 'score': 65161.54459476471, 'total_duration': 73316.9589650631, 'accumulated_submission_time': 65161.54459476471, 'accumulated_eval_time': 8140.6161987781525, 'accumulated_logging_time': 7.4225077629089355}
I0204 09:26:43.644392 139774434195200 logging_writer.py:48] [139808] accumulated_eval_time=8140.616199, accumulated_logging_time=7.422508, accumulated_submission_time=65161.544595, global_step=139808, preemption_count=0, score=65161.544595, test/accuracy=0.594500, test/loss=1.790075, test/num_examples=10000, total_duration=73316.958965, train/accuracy=0.787559, train/loss=0.837667, validation/accuracy=0.717280, validation/loss=1.155294, validation/num_examples=50000
I0204 09:27:23.478109 139774417409792 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.138770818710327, loss=2.3139050006866455
I0204 09:28:10.098248 139774434195200 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.8130717277526855, loss=3.0586445331573486
I0204 09:28:57.197474 139774417409792 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.9716649055480957, loss=1.5235235691070557
I0204 09:29:44.103428 139774434195200 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.0423052310943604, loss=1.7997254133224487
I0204 09:30:31.252090 139774417409792 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.9486433267593384, loss=1.8817695379257202
I0204 09:31:18.386707 139774434195200 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.1474740505218506, loss=3.6731441020965576
I0204 09:32:05.366275 139774417409792 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.965178370475769, loss=2.7472167015075684
I0204 09:32:52.327133 139774434195200 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.172468662261963, loss=1.7974247932434082
I0204 09:33:39.632484 139774417409792 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.1265199184417725, loss=1.8170835971832275
I0204 09:33:44.063943 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:33:56.029258 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:34:35.103304 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:34:36.710899 139936116377408 submission_runner.py:408] Time since start: 73790.06s, 	Step: 140711, 	{'train/accuracy': 0.7987695336341858, 'train/loss': 0.7999251484870911, 'validation/accuracy': 0.7211399674415588, 'validation/loss': 1.1418266296386719, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.7677903175354004, 'test/num_examples': 10000, 'score': 65581.90678691864, 'total_duration': 73790.06270742416, 'accumulated_submission_time': 65581.90678691864, 'accumulated_eval_time': 8193.26209139824, 'accumulated_logging_time': 7.47014856338501}
I0204 09:34:36.750277 139774434195200 logging_writer.py:48] [140711] accumulated_eval_time=8193.262091, accumulated_logging_time=7.470149, accumulated_submission_time=65581.906787, global_step=140711, preemption_count=0, score=65581.906787, test/accuracy=0.602800, test/loss=1.767790, test/num_examples=10000, total_duration=73790.062707, train/accuracy=0.798770, train/loss=0.799925, validation/accuracy=0.721140, validation/loss=1.141827, validation/num_examples=50000
I0204 09:35:15.172448 139774417409792 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.9778817892074585, loss=1.77472984790802
I0204 09:36:01.747861 139774434195200 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.239870548248291, loss=1.7063745260238647
I0204 09:36:48.748510 139774417409792 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.9760680198669434, loss=3.9257986545562744
I0204 09:37:35.735644 139774434195200 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.8804192543029785, loss=2.2419402599334717
I0204 09:38:22.836360 139774417409792 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.436401605606079, loss=1.7605574131011963
I0204 09:39:09.969868 139774434195200 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.2371370792388916, loss=1.6814515590667725
I0204 09:39:57.099939 139774417409792 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.0814478397369385, loss=1.8645259141921997
I0204 09:40:44.236463 139774434195200 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.9107859134674072, loss=3.8164596557617188
I0204 09:41:31.337858 139774417409792 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.4648096561431885, loss=1.7298007011413574
I0204 09:41:37.135881 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:41:49.086682 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:42:27.348736 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:42:28.957342 139936116377408 submission_runner.py:408] Time since start: 74262.31s, 	Step: 141614, 	{'train/accuracy': 0.8019140362739563, 'train/loss': 0.7914997935295105, 'validation/accuracy': 0.7195999622344971, 'validation/loss': 1.156431794166565, 'validation/num_examples': 50000, 'test/accuracy': 0.595300018787384, 'test/loss': 1.7889913320541382, 'test/num_examples': 10000, 'score': 66002.23467111588, 'total_duration': 74262.30921554565, 'accumulated_submission_time': 66002.23467111588, 'accumulated_eval_time': 8245.082573652267, 'accumulated_logging_time': 7.5203306674957275}
I0204 09:42:28.994645 139774434195200 logging_writer.py:48] [141614] accumulated_eval_time=8245.082574, accumulated_logging_time=7.520331, accumulated_submission_time=66002.234671, global_step=141614, preemption_count=0, score=66002.234671, test/accuracy=0.595300, test/loss=1.788991, test/num_examples=10000, total_duration=74262.309216, train/accuracy=0.801914, train/loss=0.791500, validation/accuracy=0.719600, validation/loss=1.156432, validation/num_examples=50000
I0204 09:43:06.379665 139774417409792 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.1063919067382812, loss=1.846611499786377
I0204 09:43:53.141212 139774434195200 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.9377998113632202, loss=2.7319531440734863
I0204 09:44:40.777893 139774417409792 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.1542856693267822, loss=1.7175838947296143
I0204 09:45:27.906642 139774434195200 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.0304200649261475, loss=3.4236910343170166
I0204 09:46:15.060693 139774417409792 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.8322417736053467, loss=3.3523643016815186
I0204 09:47:02.257582 139774434195200 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.0300981998443604, loss=1.7676384449005127
I0204 09:47:49.192159 139774417409792 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.1846768856048584, loss=1.758993148803711
I0204 09:48:36.337620 139774434195200 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.0208663940429688, loss=3.9780869483947754
I0204 09:49:23.469728 139774417409792 logging_writer.py:48] [142500] global_step=142500, grad_norm=1.8305294513702393, loss=3.1354990005493164
I0204 09:49:29.188061 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:49:41.088891 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:50:18.435895 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:50:20.040394 139936116377408 submission_runner.py:408] Time since start: 74733.39s, 	Step: 142514, 	{'train/accuracy': 0.8023241758346558, 'train/loss': 0.7893538475036621, 'validation/accuracy': 0.7219600081443787, 'validation/loss': 1.13852059841156, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.7685530185699463, 'test/num_examples': 10000, 'score': 66422.37011957169, 'total_duration': 74733.39208197594, 'accumulated_submission_time': 66422.37011957169, 'accumulated_eval_time': 8295.933718919754, 'accumulated_logging_time': 7.569155216217041}
I0204 09:50:20.094357 139774434195200 logging_writer.py:48] [142514] accumulated_eval_time=8295.933719, accumulated_logging_time=7.569155, accumulated_submission_time=66422.370120, global_step=142514, preemption_count=0, score=66422.370120, test/accuracy=0.600700, test/loss=1.768553, test/num_examples=10000, total_duration=74733.392082, train/accuracy=0.802324, train/loss=0.789354, validation/accuracy=0.721960, validation/loss=1.138521, validation/num_examples=50000
I0204 09:50:57.020449 139774417409792 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.017314910888672, loss=1.7378315925598145
I0204 09:51:44.333219 139774434195200 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.0463263988494873, loss=1.695467233657837
I0204 09:52:31.381863 139774417409792 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.246415376663208, loss=3.2013158798217773
I0204 09:53:18.575062 139774434195200 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.2718799114227295, loss=1.8434245586395264
I0204 09:54:05.530721 139774417409792 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.0200655460357666, loss=3.8374972343444824
I0204 09:54:52.589821 139774434195200 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.1048357486724854, loss=2.035846710205078
I0204 09:55:39.867182 139774417409792 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.1480066776275635, loss=3.6556410789489746
I0204 09:56:27.213143 139774434195200 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.8326846361160278, loss=2.054835796356201
I0204 09:57:14.270485 139774417409792 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.120206117630005, loss=1.806038737297058
I0204 09:57:20.086854 139936116377408 spec.py:321] Evaluating on the training split.
I0204 09:57:32.073138 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 09:58:09.936875 139936116377408 spec.py:349] Evaluating on the test split.
I0204 09:58:11.562912 139936116377408 submission_runner.py:408] Time since start: 75204.91s, 	Step: 143414, 	{'train/accuracy': 0.7995898127555847, 'train/loss': 0.8224471211433411, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.164766550064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5990000367164612, 'test/loss': 1.7969648838043213, 'test/num_examples': 10000, 'score': 66842.30435395241, 'total_duration': 75204.91482305527, 'accumulated_submission_time': 66842.30435395241, 'accumulated_eval_time': 8347.408848524094, 'accumulated_logging_time': 7.634051322937012}
I0204 09:58:11.604411 139774434195200 logging_writer.py:48] [143414] accumulated_eval_time=8347.408849, accumulated_logging_time=7.634051, accumulated_submission_time=66842.304354, global_step=143414, preemption_count=0, score=66842.304354, test/accuracy=0.599000, test/loss=1.796965, test/num_examples=10000, total_duration=75204.914823, train/accuracy=0.799590, train/loss=0.822447, validation/accuracy=0.721800, validation/loss=1.164767, validation/num_examples=50000
I0204 09:58:48.667501 139774417409792 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.3991081714630127, loss=1.7150646448135376
I0204 09:59:35.991325 139774434195200 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.8533347845077515, loss=2.566009759902954
I0204 10:00:23.388339 139774417409792 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.195296049118042, loss=2.0408854484558105
I0204 10:01:10.424633 139774434195200 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.091296434402466, loss=1.8511199951171875
I0204 10:01:57.796254 139774417409792 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.3804972171783447, loss=1.6206209659576416
I0204 10:02:45.204142 139774434195200 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.3953185081481934, loss=1.8698294162750244
I0204 10:03:32.488661 139774417409792 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.0136048793792725, loss=3.5022530555725098
I0204 10:04:19.976578 139774434195200 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.1194071769714355, loss=2.4257473945617676
I0204 10:05:07.574540 139774417409792 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.0638341903686523, loss=2.145740509033203
I0204 10:05:11.842353 139936116377408 spec.py:321] Evaluating on the training split.
I0204 10:05:23.677856 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 10:06:03.547176 139936116377408 spec.py:349] Evaluating on the test split.
I0204 10:06:05.159680 139936116377408 submission_runner.py:408] Time since start: 75678.51s, 	Step: 144311, 	{'train/accuracy': 0.8031640648841858, 'train/loss': 0.8152992725372314, 'validation/accuracy': 0.7218199968338013, 'validation/loss': 1.1692537069320679, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.7974989414215088, 'test/num_examples': 10000, 'score': 67262.48490691185, 'total_duration': 75678.5110874176, 'accumulated_submission_time': 67262.48490691185, 'accumulated_eval_time': 8400.72471165657, 'accumulated_logging_time': 7.686464548110962}
I0204 10:06:05.200264 139774434195200 logging_writer.py:48] [144311] accumulated_eval_time=8400.724712, accumulated_logging_time=7.686465, accumulated_submission_time=67262.484907, global_step=144311, preemption_count=0, score=67262.484907, test/accuracy=0.595800, test/loss=1.797499, test/num_examples=10000, total_duration=75678.511087, train/accuracy=0.803164, train/loss=0.815299, validation/accuracy=0.721820, validation/loss=1.169254, validation/num_examples=50000
I0204 10:06:43.772949 139774417409792 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.503835916519165, loss=1.602895736694336
I0204 10:07:30.517415 139774434195200 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.049909830093384, loss=1.8021957874298096
I0204 10:08:17.336612 139774417409792 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.2751474380493164, loss=3.1415414810180664
I0204 10:09:04.137476 139774434195200 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.2281665802001953, loss=2.4532058238983154
I0204 10:09:50.847601 139774417409792 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.3717498779296875, loss=2.1508948802948
I0204 10:10:38.013978 139774434195200 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.1305601596832275, loss=2.8553643226623535
I0204 10:11:25.196251 139774417409792 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.1243340969085693, loss=2.3082189559936523
I0204 10:12:12.399545 139774434195200 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.068673610687256, loss=2.0232303142547607
I0204 10:12:59.428870 139774417409792 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.1825027465820312, loss=1.870636224746704
I0204 10:13:05.290535 139936116377408 spec.py:321] Evaluating on the training split.
I0204 10:13:17.235748 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 10:13:55.091367 139936116377408 spec.py:349] Evaluating on the test split.
I0204 10:13:56.694234 139936116377408 submission_runner.py:408] Time since start: 76150.05s, 	Step: 145214, 	{'train/accuracy': 0.8151953220367432, 'train/loss': 0.7412189841270447, 'validation/accuracy': 0.7258599996566772, 'validation/loss': 1.138275146484375, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7650368213653564, 'test/num_examples': 10000, 'score': 67682.51352453232, 'total_duration': 76150.04582834244, 'accumulated_submission_time': 67682.51352453232, 'accumulated_eval_time': 8452.127160549164, 'accumulated_logging_time': 7.742059707641602}
I0204 10:13:56.734708 139774434195200 logging_writer.py:48] [145214] accumulated_eval_time=8452.127161, accumulated_logging_time=7.742060, accumulated_submission_time=67682.513525, global_step=145214, preemption_count=0, score=67682.513525, test/accuracy=0.603800, test/loss=1.765037, test/num_examples=10000, total_duration=76150.045828, train/accuracy=0.815195, train/loss=0.741219, validation/accuracy=0.725860, validation/loss=1.138275, validation/num_examples=50000
I0204 10:14:33.808993 139774417409792 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.220411539077759, loss=1.9952034950256348
I0204 10:15:20.765460 139774434195200 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.3187336921691895, loss=3.596229076385498
I0204 10:16:08.009915 139774417409792 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.178656578063965, loss=2.194856882095337
I0204 10:16:54.985546 139774434195200 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.1267879009246826, loss=4.140056610107422
I0204 10:17:42.099966 139774417409792 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.15852427482605, loss=1.7835239171981812
I0204 10:18:29.305448 139774434195200 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.154623508453369, loss=1.6082473993301392
I0204 10:19:16.357941 139774417409792 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.0638411045074463, loss=1.7423532009124756
I0204 10:20:03.517852 139774434195200 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.2791595458984375, loss=1.732735514640808
I0204 10:20:50.662358 139774417409792 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.302555799484253, loss=2.9460182189941406
I0204 10:20:56.891573 139936116377408 spec.py:321] Evaluating on the training split.
I0204 10:21:09.117290 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 10:21:46.900677 139936116377408 spec.py:349] Evaluating on the test split.
I0204 10:21:48.504554 139936116377408 submission_runner.py:408] Time since start: 76621.86s, 	Step: 146115, 	{'train/accuracy': 0.8047655820846558, 'train/loss': 0.7920248508453369, 'validation/accuracy': 0.726099967956543, 'validation/loss': 1.126613736152649, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.7594192028045654, 'test/num_examples': 10000, 'score': 68102.61263489723, 'total_duration': 76621.85620999336, 'accumulated_submission_time': 68102.61263489723, 'accumulated_eval_time': 8503.738919973373, 'accumulated_logging_time': 7.794044256210327}
I0204 10:21:48.543270 139774434195200 logging_writer.py:48] [146115] accumulated_eval_time=8503.738920, accumulated_logging_time=7.794044, accumulated_submission_time=68102.612635, global_step=146115, preemption_count=0, score=68102.612635, test/accuracy=0.603700, test/loss=1.759419, test/num_examples=10000, total_duration=76621.856210, train/accuracy=0.804766, train/loss=0.792025, validation/accuracy=0.726100, validation/loss=1.126614, validation/num_examples=50000
I0204 10:22:25.180490 139774417409792 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.223320245742798, loss=4.048830509185791
I0204 10:23:12.093697 139774434195200 logging_writer.py:48] [146300] global_step=146300, grad_norm=2.1760330200195312, loss=1.5487889051437378
I0204 10:23:59.443078 139774417409792 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.163112163543701, loss=4.1324567794799805
I0204 10:24:46.353385 139774434195200 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.2221317291259766, loss=1.6382845640182495
I0204 10:25:33.545616 139774417409792 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.303574323654175, loss=3.9558093547821045
I0204 10:26:20.747314 139774434195200 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.5101540088653564, loss=1.729741096496582
I0204 10:27:07.935774 139774417409792 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.251945734024048, loss=1.6143662929534912
I0204 10:27:55.305844 139774434195200 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.326870918273926, loss=1.5853439569473267
I0204 10:28:42.454539 139774417409792 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.18375563621521, loss=1.9121763706207275
I0204 10:28:48.773303 139936116377408 spec.py:321] Evaluating on the training split.
I0204 10:29:00.670639 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 10:29:39.032545 139936116377408 spec.py:349] Evaluating on the test split.
I0204 10:29:40.634712 139936116377408 submission_runner.py:408] Time since start: 77093.99s, 	Step: 147015, 	{'train/accuracy': 0.8040234446525574, 'train/loss': 0.7940880656242371, 'validation/accuracy': 0.7256399989128113, 'validation/loss': 1.1346133947372437, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.7695281505584717, 'test/num_examples': 10000, 'score': 68522.7866191864, 'total_duration': 77093.98651838303, 'accumulated_submission_time': 68522.7866191864, 'accumulated_eval_time': 8555.59926533699, 'accumulated_logging_time': 7.842405796051025}
I0204 10:29:40.673637 139774434195200 logging_writer.py:48] [147015] accumulated_eval_time=8555.599265, accumulated_logging_time=7.842406, accumulated_submission_time=68522.786619, global_step=147015, preemption_count=0, score=68522.786619, test/accuracy=0.601100, test/loss=1.769528, test/num_examples=10000, total_duration=77093.986518, train/accuracy=0.804023, train/loss=0.794088, validation/accuracy=0.725640, validation/loss=1.134613, validation/num_examples=50000
I0204 10:30:17.282471 139774417409792 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.602447986602783, loss=1.6367496252059937
I0204 10:31:04.157386 139774434195200 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.2560834884643555, loss=2.4212729930877686
I0204 10:31:51.269517 139774417409792 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.3112733364105225, loss=1.6102380752563477
I0204 10:32:38.720804 139774434195200 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.1176018714904785, loss=2.719632863998413
I0204 10:33:25.803665 139774417409792 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.389702796936035, loss=3.681877374649048
I0204 10:34:13.303348 139774434195200 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.0234627723693848, loss=2.7133498191833496
I0204 10:35:00.397703 139774417409792 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.2746315002441406, loss=2.1896352767944336
I0204 10:35:47.414233 139774434195200 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.3326854705810547, loss=1.6988084316253662
I0204 10:36:34.820958 139774417409792 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.2202343940734863, loss=1.587162971496582
I0204 10:36:40.684960 139936116377408 spec.py:321] Evaluating on the training split.
I0204 10:36:52.553899 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 10:37:32.264130 139936116377408 spec.py:349] Evaluating on the test split.
I0204 10:37:33.875427 139936116377408 submission_runner.py:408] Time since start: 77567.23s, 	Step: 147914, 	{'train/accuracy': 0.8194531202316284, 'train/loss': 0.7242646813392639, 'validation/accuracy': 0.7303400039672852, 'validation/loss': 1.1180957555770874, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.7449946403503418, 'test/num_examples': 10000, 'score': 68942.73813343048, 'total_duration': 77567.22704386711, 'accumulated_submission_time': 68942.73813343048, 'accumulated_eval_time': 8608.788478851318, 'accumulated_logging_time': 7.8940629959106445}
I0204 10:37:33.918364 139774434195200 logging_writer.py:48] [147914] accumulated_eval_time=8608.788479, accumulated_logging_time=7.894063, accumulated_submission_time=68942.738133, global_step=147914, preemption_count=0, score=68942.738133, test/accuracy=0.605500, test/loss=1.744995, test/num_examples=10000, total_duration=77567.227044, train/accuracy=0.819453, train/loss=0.724265, validation/accuracy=0.730340, validation/loss=1.118096, validation/num_examples=50000
I0204 10:38:11.348588 139774417409792 logging_writer.py:48] [148000] global_step=148000, grad_norm=1.9788755178451538, loss=2.034014940261841
I0204 10:38:58.488867 139774434195200 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.1611568927764893, loss=3.6651740074157715
I0204 10:39:45.939328 139774417409792 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.2218973636627197, loss=1.679978370666504
I0204 10:40:33.226409 139774434195200 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.378662347793579, loss=1.7056432962417603
I0204 10:41:21.020248 139774417409792 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.1973836421966553, loss=1.7156296968460083
I0204 10:42:08.302336 139774434195200 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.1996774673461914, loss=3.885382652282715
I0204 10:42:55.874349 139774417409792 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.2407736778259277, loss=1.7924795150756836
I0204 10:43:43.459264 139774434195200 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.2487504482269287, loss=2.100046157836914
I0204 10:44:30.800981 139774417409792 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.5287129878997803, loss=1.7615495920181274
I0204 10:44:34.284878 139936116377408 spec.py:321] Evaluating on the training split.
I0204 10:44:46.301270 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 10:45:24.590534 139936116377408 spec.py:349] Evaluating on the test split.
I0204 10:45:26.199726 139936116377408 submission_runner.py:408] Time since start: 78039.55s, 	Step: 148809, 	{'train/accuracy': 0.8100390434265137, 'train/loss': 0.7443209886550903, 'validation/accuracy': 0.7328599691390991, 'validation/loss': 1.0836445093154907, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.7161839008331299, 'test/num_examples': 10000, 'score': 69363.04661417007, 'total_duration': 78039.55141115189, 'accumulated_submission_time': 69363.04661417007, 'accumulated_eval_time': 8660.702143192291, 'accumulated_logging_time': 7.949257135391235}
I0204 10:45:26.247587 139774434195200 logging_writer.py:48] [148809] accumulated_eval_time=8660.702143, accumulated_logging_time=7.949257, accumulated_submission_time=69363.046614, global_step=148809, preemption_count=0, score=69363.046614, test/accuracy=0.607600, test/loss=1.716184, test/num_examples=10000, total_duration=78039.551411, train/accuracy=0.810039, train/loss=0.744321, validation/accuracy=0.732860, validation/loss=1.083645, validation/num_examples=50000
I0204 10:46:05.804194 139774417409792 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.2066516876220703, loss=2.2145321369171143
I0204 10:46:52.626554 139774434195200 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.1082308292388916, loss=3.972348690032959
I0204 10:47:39.838233 139774417409792 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.474127769470215, loss=1.691868782043457
I0204 10:48:26.873836 139774434195200 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.035355806350708, loss=2.0114927291870117
I0204 10:49:14.264751 139774417409792 logging_writer.py:48] [149300] global_step=149300, grad_norm=1.9515788555145264, loss=2.615870952606201
I0204 10:50:01.187458 139774434195200 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.5021753311157227, loss=2.305551528930664
I0204 10:50:48.299498 139774417409792 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.2154762744903564, loss=3.697730302810669
I0204 10:51:35.475265 139774434195200 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.1746842861175537, loss=3.2816710472106934
I0204 10:52:22.454521 139774417409792 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.6620051860809326, loss=1.8389439582824707
I0204 10:52:26.375807 139936116377408 spec.py:321] Evaluating on the training split.
I0204 10:52:38.468421 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 10:53:18.862171 139936116377408 spec.py:349] Evaluating on the test split.
I0204 10:53:20.471364 139936116377408 submission_runner.py:408] Time since start: 78513.82s, 	Step: 149710, 	{'train/accuracy': 0.8106640577316284, 'train/loss': 0.7712754011154175, 'validation/accuracy': 0.7304799556732178, 'validation/loss': 1.1216198205947876, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.7387804985046387, 'test/num_examples': 10000, 'score': 69783.11700677872, 'total_duration': 78513.82325577736, 'accumulated_submission_time': 69783.11700677872, 'accumulated_eval_time': 8714.796729803085, 'accumulated_logging_time': 8.007672309875488}
I0204 10:53:20.514709 139774434195200 logging_writer.py:48] [149710] accumulated_eval_time=8714.796730, accumulated_logging_time=8.007672, accumulated_submission_time=69783.117007, global_step=149710, preemption_count=0, score=69783.117007, test/accuracy=0.610500, test/loss=1.738780, test/num_examples=10000, total_duration=78513.823256, train/accuracy=0.810664, train/loss=0.771275, validation/accuracy=0.730480, validation/loss=1.121620, validation/num_examples=50000
I0204 10:53:59.381107 139774417409792 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.6925270557403564, loss=1.5857195854187012
I0204 10:54:46.416201 139774434195200 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.4006106853485107, loss=1.7665702104568481
I0204 10:55:33.537925 139774417409792 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.8671257495880127, loss=1.7751400470733643
I0204 10:56:20.480083 139774434195200 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.3101813793182373, loss=1.636333703994751
I0204 10:57:07.966712 139774417409792 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.5146031379699707, loss=3.62680983543396
I0204 10:57:54.982426 139774434195200 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.30059552192688, loss=4.064558982849121
I0204 10:58:42.446211 139774417409792 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.2806944847106934, loss=1.7963366508483887
I0204 10:59:29.958699 139774434195200 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.2934608459472656, loss=1.6992913484573364
I0204 11:00:17.167650 139774417409792 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.4826395511627197, loss=1.8432176113128662
I0204 11:00:20.704767 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:00:32.542027 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:01:12.216952 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:01:13.826145 139936116377408 submission_runner.py:408] Time since start: 78987.18s, 	Step: 150609, 	{'train/accuracy': 0.8217577934265137, 'train/loss': 0.6940465569496155, 'validation/accuracy': 0.7341199517250061, 'validation/loss': 1.0832279920578003, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7156203985214233, 'test/num_examples': 10000, 'score': 70203.25068831444, 'total_duration': 78987.17783665657, 'accumulated_submission_time': 70203.25068831444, 'accumulated_eval_time': 8767.916935682297, 'accumulated_logging_time': 8.061208963394165}
I0204 11:01:13.867288 139774434195200 logging_writer.py:48] [150609] accumulated_eval_time=8767.916936, accumulated_logging_time=8.061209, accumulated_submission_time=70203.250688, global_step=150609, preemption_count=0, score=70203.250688, test/accuracy=0.612100, test/loss=1.715620, test/num_examples=10000, total_duration=78987.177837, train/accuracy=0.821758, train/loss=0.694047, validation/accuracy=0.734120, validation/loss=1.083228, validation/num_examples=50000
I0204 11:01:53.389511 139774417409792 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.473926544189453, loss=3.6412339210510254
I0204 11:02:40.399828 139774434195200 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.3423068523406982, loss=2.0779151916503906
I0204 11:03:27.354476 139774417409792 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.3938522338867188, loss=3.5677828788757324
I0204 11:04:14.494322 139774434195200 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.294856309890747, loss=1.6825329065322876
I0204 11:05:01.352005 139774417409792 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.4538776874542236, loss=1.6050605773925781
I0204 11:05:48.794171 139774434195200 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.290135383605957, loss=1.7492984533309937
I0204 11:06:35.894862 139774417409792 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.29895281791687, loss=1.918362021446228
I0204 11:07:23.269922 139774434195200 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.365382432937622, loss=1.556981086730957
I0204 11:08:10.397689 139774417409792 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.529000997543335, loss=2.0702078342437744
I0204 11:08:13.901673 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:08:25.934429 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:09:05.342185 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:09:06.963017 139936116377408 submission_runner.py:408] Time since start: 79460.31s, 	Step: 151509, 	{'train/accuracy': 0.8132226467132568, 'train/loss': 0.7564131021499634, 'validation/accuracy': 0.7326799631118774, 'validation/loss': 1.1018426418304443, 'validation/num_examples': 50000, 'test/accuracy': 0.6114000082015991, 'test/loss': 1.7240487337112427, 'test/num_examples': 10000, 'score': 70623.22665309906, 'total_duration': 79460.3146841526, 'accumulated_submission_time': 70623.22665309906, 'accumulated_eval_time': 8820.977076530457, 'accumulated_logging_time': 8.114773273468018}
I0204 11:09:07.013159 139774434195200 logging_writer.py:48] [151509] accumulated_eval_time=8820.977077, accumulated_logging_time=8.114773, accumulated_submission_time=70623.226653, global_step=151509, preemption_count=0, score=70623.226653, test/accuracy=0.611400, test/loss=1.724049, test/num_examples=10000, total_duration=79460.314684, train/accuracy=0.813223, train/loss=0.756413, validation/accuracy=0.732680, validation/loss=1.101843, validation/num_examples=50000
I0204 11:09:46.338352 139774417409792 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.6142618656158447, loss=2.397186756134033
I0204 11:10:33.336480 139774434195200 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.2699646949768066, loss=2.280032157897949
I0204 11:11:20.607873 139774417409792 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.4854605197906494, loss=1.8713319301605225
I0204 11:12:07.882502 139774434195200 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.078697443008423, loss=3.301762342453003
I0204 11:12:54.842477 139774417409792 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.497497797012329, loss=1.5923826694488525
I0204 11:13:41.976688 139774434195200 logging_writer.py:48] [152100] global_step=152100, grad_norm=1.988600254058838, loss=2.656920909881592
I0204 11:14:28.943177 139774417409792 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.2063615322113037, loss=3.493015766143799
I0204 11:15:15.826544 139774434195200 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.50827693939209, loss=1.7546499967575073
I0204 11:16:02.966954 139774417409792 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.4622507095336914, loss=1.5496395826339722
I0204 11:16:07.362108 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:16:19.439891 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:16:55.926476 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:16:57.543474 139936116377408 submission_runner.py:408] Time since start: 79930.90s, 	Step: 152411, 	{'train/accuracy': 0.8139452934265137, 'train/loss': 0.7501934170722961, 'validation/accuracy': 0.7333599925041199, 'validation/loss': 1.1036591529846191, 'validation/num_examples': 50000, 'test/accuracy': 0.6057000160217285, 'test/loss': 1.7319797277450562, 'test/num_examples': 10000, 'score': 71043.51707410812, 'total_duration': 79930.89526510239, 'accumulated_submission_time': 71043.51707410812, 'accumulated_eval_time': 8871.157362699509, 'accumulated_logging_time': 8.176369428634644}
I0204 11:16:57.588115 139774434195200 logging_writer.py:48] [152411] accumulated_eval_time=8871.157363, accumulated_logging_time=8.176369, accumulated_submission_time=71043.517074, global_step=152411, preemption_count=0, score=71043.517074, test/accuracy=0.605700, test/loss=1.731980, test/num_examples=10000, total_duration=79930.895265, train/accuracy=0.813945, train/loss=0.750193, validation/accuracy=0.733360, validation/loss=1.103659, validation/num_examples=50000
I0204 11:17:36.032447 139774417409792 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.3372838497161865, loss=1.5379161834716797
I0204 11:18:23.024640 139774434195200 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.509357213973999, loss=1.6505141258239746
I0204 11:19:09.859920 139774417409792 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.338409900665283, loss=3.8265700340270996
I0204 11:19:56.860472 139774434195200 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.0517687797546387, loss=2.4361255168914795
I0204 11:20:43.938364 139774417409792 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.563025951385498, loss=1.7112892866134644
I0204 11:21:31.233705 139774434195200 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.6332767009735107, loss=2.4262914657592773
I0204 11:22:18.672668 139774417409792 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.685081958770752, loss=1.593252182006836
I0204 11:23:05.842126 139774434195200 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.4066624641418457, loss=1.5867509841918945
I0204 11:23:52.809494 139774417409792 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.3715550899505615, loss=1.815130352973938
I0204 11:23:57.586634 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:24:09.629038 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:24:48.333644 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:24:49.943440 139936116377408 submission_runner.py:408] Time since start: 80403.30s, 	Step: 153312, 	{'train/accuracy': 0.826464831829071, 'train/loss': 0.6860719919204712, 'validation/accuracy': 0.7383999824523926, 'validation/loss': 1.0645508766174316, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.6989619731903076, 'test/num_examples': 10000, 'score': 71463.45511341095, 'total_duration': 80403.29509854317, 'accumulated_submission_time': 71463.45511341095, 'accumulated_eval_time': 8923.512953042984, 'accumulated_logging_time': 8.234338998794556}
I0204 11:24:49.985131 139774434195200 logging_writer.py:48] [153312] accumulated_eval_time=8923.512953, accumulated_logging_time=8.234339, accumulated_submission_time=71463.455113, global_step=153312, preemption_count=0, score=71463.455113, test/accuracy=0.613500, test/loss=1.698962, test/num_examples=10000, total_duration=80403.295099, train/accuracy=0.826465, train/loss=0.686072, validation/accuracy=0.738400, validation/loss=1.064551, validation/num_examples=50000
I0204 11:25:27.994355 139774417409792 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.4066717624664307, loss=1.7408447265625
I0204 11:26:14.731193 139774434195200 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.8332064151763916, loss=1.702094316482544
I0204 11:27:01.506355 139774417409792 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.859492540359497, loss=1.5662153959274292
I0204 11:27:48.802151 139774434195200 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.3379077911376953, loss=3.811277389526367
I0204 11:28:36.311992 139774417409792 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.5097503662109375, loss=3.5712764263153076
I0204 11:29:23.983900 139774434195200 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.3628652095794678, loss=1.9686551094055176
I0204 11:30:11.385448 139774417409792 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.209533929824829, loss=1.6604599952697754
I0204 11:30:58.663666 139774434195200 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.4253883361816406, loss=1.5974446535110474
I0204 11:31:46.113317 139774417409792 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.1068453788757324, loss=1.7579524517059326
I0204 11:31:50.018069 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:32:01.853617 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:32:39.932279 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:32:41.547057 139936116377408 submission_runner.py:408] Time since start: 80874.90s, 	Step: 154210, 	{'train/accuracy': 0.8174414038658142, 'train/loss': 0.7186430096626282, 'validation/accuracy': 0.7394199967384338, 'validation/loss': 1.062025547027588, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.6909252405166626, 'test/num_examples': 10000, 'score': 71883.42968702316, 'total_duration': 80874.89892697334, 'accumulated_submission_time': 71883.42968702316, 'accumulated_eval_time': 8975.040938138962, 'accumulated_logging_time': 8.287301778793335}
I0204 11:32:41.589663 139774434195200 logging_writer.py:48] [154210] accumulated_eval_time=8975.040938, accumulated_logging_time=8.287302, accumulated_submission_time=71883.429687, global_step=154210, preemption_count=0, score=71883.429687, test/accuracy=0.616500, test/loss=1.690925, test/num_examples=10000, total_duration=80874.898927, train/accuracy=0.817441, train/loss=0.718643, validation/accuracy=0.739420, validation/loss=1.062026, validation/num_examples=50000
I0204 11:33:20.428391 139774417409792 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.376265525817871, loss=1.6510770320892334
I0204 11:34:07.167849 139774434195200 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.2924301624298096, loss=2.4692952632904053
I0204 11:34:54.820692 139774417409792 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.250157594680786, loss=2.9973907470703125
I0204 11:35:41.856708 139774434195200 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.4645636081695557, loss=3.2946717739105225
I0204 11:36:28.798116 139774417409792 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.51448130607605, loss=1.4995824098587036
I0204 11:37:15.936397 139774434195200 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.217451333999634, loss=2.043060064315796
I0204 11:38:02.997715 139774417409792 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.2787959575653076, loss=1.791599154472351
I0204 11:38:50.325133 139774434195200 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.7733495235443115, loss=1.5858242511749268
I0204 11:39:37.531475 139774417409792 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.3047642707824707, loss=1.5843778848648071
I0204 11:39:41.749159 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:39:53.538730 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:40:32.019919 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:40:33.624043 139936116377408 submission_runner.py:408] Time since start: 81346.98s, 	Step: 155111, 	{'train/accuracy': 0.8227929472923279, 'train/loss': 0.6901466250419617, 'validation/accuracy': 0.7375199794769287, 'validation/loss': 1.0573351383209229, 'validation/num_examples': 50000, 'test/accuracy': 0.6168000102043152, 'test/loss': 1.6916823387145996, 'test/num_examples': 10000, 'score': 72303.53037238121, 'total_duration': 81346.9751830101, 'accumulated_submission_time': 72303.53037238121, 'accumulated_eval_time': 9026.914111852646, 'accumulated_logging_time': 8.34177279472351}
I0204 11:40:33.666378 139774434195200 logging_writer.py:48] [155111] accumulated_eval_time=9026.914112, accumulated_logging_time=8.341773, accumulated_submission_time=72303.530372, global_step=155111, preemption_count=0, score=72303.530372, test/accuracy=0.616800, test/loss=1.691682, test/num_examples=10000, total_duration=81346.975183, train/accuracy=0.822793, train/loss=0.690147, validation/accuracy=0.737520, validation/loss=1.057335, validation/num_examples=50000
I0204 11:41:12.247920 139774417409792 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.2891080379486084, loss=1.8006267547607422
I0204 11:41:58.998418 139774434195200 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.3560853004455566, loss=3.9573118686676025
I0204 11:42:46.230668 139774417409792 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.3277580738067627, loss=1.55850088596344
I0204 11:43:33.447458 139774434195200 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.3504536151885986, loss=2.2050583362579346
I0204 11:44:20.477479 139774417409792 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.6420280933380127, loss=4.0765557289123535
I0204 11:45:07.287693 139774434195200 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.684401750564575, loss=2.6982498168945312
I0204 11:45:54.080180 139774417409792 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.261514902114868, loss=3.4561192989349365
I0204 11:46:41.408562 139774434195200 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.39599347114563, loss=2.1811556816101074
I0204 11:47:28.668064 139774417409792 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.5357425212860107, loss=2.0472748279571533
I0204 11:47:33.968965 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:47:45.866003 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:48:25.859507 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:48:27.460046 139936116377408 submission_runner.py:408] Time since start: 81820.81s, 	Step: 156013, 	{'train/accuracy': 0.8246484398841858, 'train/loss': 0.6990688443183899, 'validation/accuracy': 0.7361199855804443, 'validation/loss': 1.0750834941864014, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.709358811378479, 'test/num_examples': 10000, 'score': 72723.77445316315, 'total_duration': 81820.81177711487, 'accumulated_submission_time': 72723.77445316315, 'accumulated_eval_time': 9080.40405368805, 'accumulated_logging_time': 8.39595341682434}
I0204 11:48:27.504480 139774434195200 logging_writer.py:48] [156013] accumulated_eval_time=9080.404054, accumulated_logging_time=8.395953, accumulated_submission_time=72723.774453, global_step=156013, preemption_count=0, score=72723.774453, test/accuracy=0.609700, test/loss=1.709359, test/num_examples=10000, total_duration=81820.811777, train/accuracy=0.824648, train/loss=0.699069, validation/accuracy=0.736120, validation/loss=1.075083, validation/num_examples=50000
I0204 11:49:04.937314 139774417409792 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.398552179336548, loss=1.619382381439209
I0204 11:49:51.438719 139774434195200 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.479748249053955, loss=1.4391015768051147
I0204 11:50:38.805775 139774417409792 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.653383731842041, loss=1.4987471103668213
I0204 11:51:25.945366 139774434195200 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.580717086791992, loss=1.665095329284668
I0204 11:52:12.967853 139774417409792 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.6002726554870605, loss=2.9910244941711426
I0204 11:52:59.931705 139774434195200 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.2960352897644043, loss=2.1970763206481934
I0204 11:53:47.248151 139774417409792 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.4177112579345703, loss=1.931153655052185
I0204 11:54:34.297158 139774434195200 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.5815176963806152, loss=3.9563472270965576
I0204 11:55:21.286287 139774417409792 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.2417218685150146, loss=1.5071312189102173
I0204 11:55:27.621962 139936116377408 spec.py:321] Evaluating on the training split.
I0204 11:55:39.676132 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 11:56:17.577218 139936116377408 spec.py:349] Evaluating on the test split.
I0204 11:56:19.182161 139936116377408 submission_runner.py:408] Time since start: 82292.53s, 	Step: 156915, 	{'train/accuracy': 0.8210351467132568, 'train/loss': 0.7016533613204956, 'validation/accuracy': 0.74263995885849, 'validation/loss': 1.0572545528411865, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.6846977472305298, 'test/num_examples': 10000, 'score': 73143.83418130875, 'total_duration': 82292.53384494781, 'accumulated_submission_time': 73143.83418130875, 'accumulated_eval_time': 9131.963080406189, 'accumulated_logging_time': 8.450865745544434}
I0204 11:56:19.232817 139774434195200 logging_writer.py:48] [156915] accumulated_eval_time=9131.963080, accumulated_logging_time=8.450866, accumulated_submission_time=73143.834181, global_step=156915, preemption_count=0, score=73143.834181, test/accuracy=0.618900, test/loss=1.684698, test/num_examples=10000, total_duration=82292.533845, train/accuracy=0.821035, train/loss=0.701653, validation/accuracy=0.742640, validation/loss=1.057255, validation/num_examples=50000
I0204 11:56:55.916553 139774417409792 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.5143790245056152, loss=2.103048324584961
I0204 11:57:42.893625 139774434195200 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.870116949081421, loss=1.6861498355865479
I0204 11:58:30.113335 139774417409792 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.4129340648651123, loss=2.50655460357666
I0204 11:59:17.167083 139774434195200 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.542268991470337, loss=1.6075397729873657
I0204 12:00:04.220319 139774417409792 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.337169647216797, loss=2.5546677112579346
I0204 12:00:51.355427 139774434195200 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.4257078170776367, loss=1.628204107284546
I0204 12:01:38.484627 139774417409792 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.560711145401001, loss=3.5044515132904053
I0204 12:02:25.959482 139774434195200 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.9283037185668945, loss=1.5728514194488525
I0204 12:03:13.371194 139774417409792 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.930354356765747, loss=1.6205883026123047
I0204 12:03:19.431543 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:03:31.194570 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:04:08.853050 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:04:10.464962 139936116377408 submission_runner.py:408] Time since start: 82763.82s, 	Step: 157815, 	{'train/accuracy': 0.8214843273162842, 'train/loss': 0.6942675709724426, 'validation/accuracy': 0.7416399717330933, 'validation/loss': 1.0510855913162231, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.6868818998336792, 'test/num_examples': 10000, 'score': 73563.9743270874, 'total_duration': 82763.81664323807, 'accumulated_submission_time': 73563.9743270874, 'accumulated_eval_time': 9182.99531197548, 'accumulated_logging_time': 8.513504981994629}
I0204 12:04:10.510020 139774434195200 logging_writer.py:48] [157815] accumulated_eval_time=9182.995312, accumulated_logging_time=8.513505, accumulated_submission_time=73563.974327, global_step=157815, preemption_count=0, score=73563.974327, test/accuracy=0.615600, test/loss=1.686882, test/num_examples=10000, total_duration=82763.816643, train/accuracy=0.821484, train/loss=0.694268, validation/accuracy=0.741640, validation/loss=1.051086, validation/num_examples=50000
I0204 12:04:47.319824 139774417409792 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.3962340354919434, loss=1.4288358688354492
I0204 12:05:34.067941 139774434195200 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.5355920791625977, loss=1.507357120513916
I0204 12:06:21.413912 139774417409792 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.509031295776367, loss=2.915855884552002
I0204 12:07:08.582299 139774434195200 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.692793369293213, loss=1.755553960800171
I0204 12:07:55.437207 139774417409792 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.63012433052063, loss=1.5443247556686401
I0204 12:08:43.026629 139774434195200 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.7129499912261963, loss=1.559350609779358
I0204 12:09:30.440259 139774417409792 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.5867364406585693, loss=3.945016622543335
I0204 12:10:17.863824 139774434195200 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.4690797328948975, loss=1.856210708618164
I0204 12:11:05.021650 139774417409792 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.564950942993164, loss=3.584787607192993
I0204 12:11:10.947572 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:11:22.871447 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:12:01.338477 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:12:02.942073 139936116377408 submission_runner.py:408] Time since start: 83236.29s, 	Step: 158714, 	{'train/accuracy': 0.8269335627555847, 'train/loss': 0.649540901184082, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.031722068786621, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.66560959815979, 'test/num_examples': 10000, 'score': 73984.35371112823, 'total_duration': 83236.29381513596, 'accumulated_submission_time': 73984.35371112823, 'accumulated_eval_time': 9234.988682985306, 'accumulated_logging_time': 8.569389820098877}
I0204 12:12:02.984135 139774434195200 logging_writer.py:48] [158714] accumulated_eval_time=9234.988683, accumulated_logging_time=8.569390, accumulated_submission_time=73984.353711, global_step=158714, preemption_count=0, score=73984.353711, test/accuracy=0.620300, test/loss=1.665610, test/num_examples=10000, total_duration=83236.293815, train/accuracy=0.826934, train/loss=0.649541, validation/accuracy=0.741580, validation/loss=1.031722, validation/num_examples=50000
I0204 12:12:39.954863 139774417409792 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.4422073364257812, loss=2.1569976806640625
I0204 12:13:27.157964 139774434195200 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.4477384090423584, loss=1.4439630508422852
I0204 12:14:14.176776 139774417409792 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.428178310394287, loss=2.828190565109253
I0204 12:15:01.061642 139774434195200 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.4018609523773193, loss=3.1974501609802246
I0204 12:15:48.357086 139774417409792 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.306241750717163, loss=1.6951398849487305
I0204 12:16:35.289998 139774434195200 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.8885157108306885, loss=1.637686848640442
I0204 12:17:22.365028 139774417409792 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.456148147583008, loss=2.0243568420410156
I0204 12:18:10.047994 139774434195200 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.562739610671997, loss=1.4410845041275024
I0204 12:18:57.137598 139774417409792 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.839071273803711, loss=1.4937220811843872
I0204 12:19:03.123171 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:19:14.971767 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:19:53.057304 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:19:54.670894 139936116377408 submission_runner.py:408] Time since start: 83708.02s, 	Step: 159614, 	{'train/accuracy': 0.8246288895606995, 'train/loss': 0.6832771897315979, 'validation/accuracy': 0.7423799633979797, 'validation/loss': 1.0461320877075195, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.683318853378296, 'test/num_examples': 10000, 'score': 74404.43561291695, 'total_duration': 83708.0224416256, 'accumulated_submission_time': 74404.43561291695, 'accumulated_eval_time': 9286.535109996796, 'accumulated_logging_time': 8.62211298942566}
I0204 12:19:54.721179 139774434195200 logging_writer.py:48] [159614] accumulated_eval_time=9286.535110, accumulated_logging_time=8.622113, accumulated_submission_time=74404.435613, global_step=159614, preemption_count=0, score=74404.435613, test/accuracy=0.620400, test/loss=1.683319, test/num_examples=10000, total_duration=83708.022442, train/accuracy=0.824629, train/loss=0.683277, validation/accuracy=0.742380, validation/loss=1.046132, validation/num_examples=50000
I0204 12:20:32.043932 139774417409792 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.631199598312378, loss=1.5236858129501343
I0204 12:21:19.057566 139774434195200 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.349733829498291, loss=1.6245100498199463
I0204 12:22:06.858359 139774417409792 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.4034459590911865, loss=3.0466623306274414
I0204 12:22:54.144768 139774434195200 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.563772678375244, loss=2.4782018661499023
I0204 12:23:41.565978 139774417409792 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.530174732208252, loss=1.4844088554382324
I0204 12:24:29.340369 139774434195200 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.7597885131835938, loss=2.121809959411621
I0204 12:25:16.685291 139774417409792 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.7116305828094482, loss=3.262834072113037
I0204 12:26:04.257903 139774434195200 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.4379916191101074, loss=1.511946201324463
I0204 12:26:51.402288 139774417409792 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.394404172897339, loss=2.3402798175811768
I0204 12:26:54.908540 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:27:06.905086 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:27:44.631141 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:27:46.241328 139936116377408 submission_runner.py:408] Time since start: 84179.59s, 	Step: 160509, 	{'train/accuracy': 0.8296093344688416, 'train/loss': 0.6793026328086853, 'validation/accuracy': 0.744159996509552, 'validation/loss': 1.0463520288467407, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.6868876218795776, 'test/num_examples': 10000, 'score': 74824.56441307068, 'total_duration': 84179.59306025505, 'accumulated_submission_time': 74824.56441307068, 'accumulated_eval_time': 9337.866757631302, 'accumulated_logging_time': 8.684773206710815}
I0204 12:27:46.285918 139774434195200 logging_writer.py:48] [160509] accumulated_eval_time=9337.866758, accumulated_logging_time=8.684773, accumulated_submission_time=74824.564413, global_step=160509, preemption_count=0, score=74824.564413, test/accuracy=0.616700, test/loss=1.686888, test/num_examples=10000, total_duration=84179.593060, train/accuracy=0.829609, train/loss=0.679303, validation/accuracy=0.744160, validation/loss=1.046352, validation/num_examples=50000
I0204 12:28:25.676537 139774417409792 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.647189140319824, loss=2.534144878387451
I0204 12:29:12.316237 139774434195200 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.5100860595703125, loss=3.083378791809082
I0204 12:29:59.489407 139774417409792 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.5131828784942627, loss=3.454293966293335
I0204 12:30:46.389348 139774434195200 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.922421932220459, loss=1.5566048622131348
I0204 12:31:33.723375 139774417409792 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.2582387924194336, loss=2.541433334350586
I0204 12:32:20.830987 139774434195200 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.996558427810669, loss=1.6316572427749634
I0204 12:33:08.033329 139774417409792 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.4483048915863037, loss=1.4763147830963135
I0204 12:33:55.219041 139774434195200 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.590583324432373, loss=1.6353517770767212
I0204 12:34:42.378663 139774417409792 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.56864333152771, loss=3.2748923301696777
I0204 12:34:46.678827 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:34:58.830589 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:35:38.530038 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:35:40.138876 139936116377408 submission_runner.py:408] Time since start: 84653.49s, 	Step: 161411, 	{'train/accuracy': 0.8265038728713989, 'train/loss': 0.695087730884552, 'validation/accuracy': 0.7425000071525574, 'validation/loss': 1.076271653175354, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.708626389503479, 'test/num_examples': 10000, 'score': 75244.89977121353, 'total_duration': 84653.49061632156, 'accumulated_submission_time': 75244.89977121353, 'accumulated_eval_time': 9391.32567691803, 'accumulated_logging_time': 8.740829706192017}
I0204 12:35:40.180107 139774434195200 logging_writer.py:48] [161411] accumulated_eval_time=9391.325677, accumulated_logging_time=8.740830, accumulated_submission_time=75244.899771, global_step=161411, preemption_count=0, score=75244.899771, test/accuracy=0.614200, test/loss=1.708626, test/num_examples=10000, total_duration=84653.490616, train/accuracy=0.826504, train/loss=0.695088, validation/accuracy=0.742500, validation/loss=1.076272, validation/num_examples=50000
I0204 12:36:18.604430 139774417409792 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.45619535446167, loss=1.3952295780181885
I0204 12:37:05.512319 139774434195200 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.4309022426605225, loss=1.5400598049163818
I0204 12:37:52.640882 139774417409792 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.496307373046875, loss=1.9514473676681519
I0204 12:38:39.780925 139774434195200 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.4384028911590576, loss=3.322143316268921
I0204 12:39:26.962950 139774417409792 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.5808591842651367, loss=1.5705451965332031
I0204 12:40:13.993538 139774434195200 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.7669990062713623, loss=2.4840431213378906
I0204 12:41:00.682949 139774417409792 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.7773990631103516, loss=1.490855097770691
I0204 12:41:48.025069 139774434195200 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.7408671379089355, loss=1.5815999507904053
I0204 12:42:35.235497 139774417409792 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.616922616958618, loss=3.7556796073913574
I0204 12:42:40.431550 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:42:52.298385 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:43:29.867559 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:43:31.470089 139936116377408 submission_runner.py:408] Time since start: 85124.82s, 	Step: 162313, 	{'train/accuracy': 0.8310937285423279, 'train/loss': 0.6638551950454712, 'validation/accuracy': 0.7471999526023865, 'validation/loss': 1.0294922590255737, 'validation/num_examples': 50000, 'test/accuracy': 0.6253000497817993, 'test/loss': 1.6685092449188232, 'test/num_examples': 10000, 'score': 75665.09418177605, 'total_duration': 85124.82177829742, 'accumulated_submission_time': 75665.09418177605, 'accumulated_eval_time': 9442.36303639412, 'accumulated_logging_time': 8.792722463607788}
I0204 12:43:31.511780 139774434195200 logging_writer.py:48] [162313] accumulated_eval_time=9442.363036, accumulated_logging_time=8.792722, accumulated_submission_time=75665.094182, global_step=162313, preemption_count=0, score=75665.094182, test/accuracy=0.625300, test/loss=1.668509, test/num_examples=10000, total_duration=85124.821778, train/accuracy=0.831094, train/loss=0.663855, validation/accuracy=0.747200, validation/loss=1.029492, validation/num_examples=50000
I0204 12:44:09.224467 139774417409792 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.8832900524139404, loss=1.5514131784439087
I0204 12:44:55.913937 139774434195200 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.7763233184814453, loss=1.4523903131484985
I0204 12:45:43.380417 139774417409792 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.7175981998443604, loss=1.8016808032989502
I0204 12:46:30.774488 139774434195200 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.4147043228149414, loss=2.102294683456421
I0204 12:47:17.651860 139774417409792 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.613694190979004, loss=4.082071781158447
I0204 12:48:04.809470 139774434195200 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.6171250343322754, loss=1.4801275730133057
I0204 12:48:51.896199 139774417409792 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.421431064605713, loss=1.561572790145874
I0204 12:49:39.103570 139774434195200 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.465435028076172, loss=1.4848265647888184
I0204 12:50:26.295016 139774417409792 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.5836238861083984, loss=1.8781237602233887
I0204 12:50:31.958541 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:50:43.824260 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:51:21.862175 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:51:23.473837 139936116377408 submission_runner.py:408] Time since start: 85596.83s, 	Step: 163214, 	{'train/accuracy': 0.8335351347923279, 'train/loss': 0.658676266670227, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.0346159934997559, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.664926290512085, 'test/num_examples': 10000, 'score': 76085.4808113575, 'total_duration': 85596.82547187805, 'accumulated_submission_time': 76085.4808113575, 'accumulated_eval_time': 9493.877099275589, 'accumulated_logging_time': 8.844767093658447}
I0204 12:51:23.519592 139774434195200 logging_writer.py:48] [163214] accumulated_eval_time=9493.877099, accumulated_logging_time=8.844767, accumulated_submission_time=76085.480811, global_step=163214, preemption_count=0, score=76085.480811, test/accuracy=0.626100, test/loss=1.664926, test/num_examples=10000, total_duration=85596.825472, train/accuracy=0.833535, train/loss=0.658676, validation/accuracy=0.747060, validation/loss=1.034616, validation/num_examples=50000
I0204 12:52:00.663339 139774417409792 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.9142210483551025, loss=2.673464298248291
I0204 12:52:47.592843 139774434195200 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.350576400756836, loss=1.9241948127746582
I0204 12:53:34.541773 139774417409792 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.3410098552703857, loss=1.3663289546966553
I0204 12:54:21.854978 139774434195200 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.692702054977417, loss=3.9005281925201416
I0204 12:55:09.146443 139774417409792 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.483682155609131, loss=1.470320224761963
I0204 12:55:56.271495 139774434195200 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.197225570678711, loss=1.4724115133285522
I0204 12:56:43.809671 139774417409792 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.798485040664673, loss=1.5445753335952759
I0204 12:57:30.932821 139774434195200 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.7194888591766357, loss=1.5249322652816772
I0204 12:58:17.671054 139774417409792 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.724337577819824, loss=2.8584165573120117
I0204 12:58:23.563203 139936116377408 spec.py:321] Evaluating on the training split.
I0204 12:58:35.489395 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 12:59:13.067873 139936116377408 spec.py:349] Evaluating on the test split.
I0204 12:59:14.670582 139936116377408 submission_runner.py:408] Time since start: 86068.02s, 	Step: 164114, 	{'train/accuracy': 0.8374804258346558, 'train/loss': 0.6364269256591797, 'validation/accuracy': 0.747439980506897, 'validation/loss': 1.0335808992385864, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.6698811054229736, 'test/num_examples': 10000, 'score': 76505.46322965622, 'total_duration': 86068.02240657806, 'accumulated_submission_time': 76505.46322965622, 'accumulated_eval_time': 9544.983451128006, 'accumulated_logging_time': 8.900535106658936}
I0204 12:59:14.713084 139774434195200 logging_writer.py:48] [164114] accumulated_eval_time=9544.983451, accumulated_logging_time=8.900535, accumulated_submission_time=76505.463230, global_step=164114, preemption_count=0, score=76505.463230, test/accuracy=0.623400, test/loss=1.669881, test/num_examples=10000, total_duration=86068.022407, train/accuracy=0.837480, train/loss=0.636427, validation/accuracy=0.747440, validation/loss=1.033581, validation/num_examples=50000
I0204 12:59:51.828668 139774417409792 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.9229183197021484, loss=1.6875461339950562
I0204 13:00:38.312642 139774434195200 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.690328598022461, loss=1.4931256771087646
I0204 13:01:25.581754 139774417409792 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.3078863620758057, loss=2.9756264686584473
I0204 13:02:12.591094 139774434195200 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.8175387382507324, loss=1.5262253284454346
I0204 13:02:59.471689 139774417409792 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.635021686553955, loss=1.5058739185333252
I0204 13:03:46.470727 139774434195200 logging_writer.py:48] [164700] global_step=164700, grad_norm=3.2430946826934814, loss=1.5741634368896484
I0204 13:04:33.514344 139774417409792 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.7246158123016357, loss=1.4825513362884521
I0204 13:05:20.748802 139774434195200 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.5897815227508545, loss=1.7180583477020264
I0204 13:06:07.815108 139774417409792 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.802865743637085, loss=1.5438765287399292
I0204 13:06:14.960921 139936116377408 spec.py:321] Evaluating on the training split.
I0204 13:06:26.962939 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 13:07:05.270315 139936116377408 spec.py:349] Evaluating on the test split.
I0204 13:07:06.873316 139936116377408 submission_runner.py:408] Time since start: 86540.23s, 	Step: 165017, 	{'train/accuracy': 0.838183581829071, 'train/loss': 0.639176070690155, 'validation/accuracy': 0.7498399615287781, 'validation/loss': 1.0234603881835938, 'validation/num_examples': 50000, 'test/accuracy': 0.6254000067710876, 'test/loss': 1.6488336324691772, 'test/num_examples': 10000, 'score': 76925.65456914902, 'total_duration': 86540.2251765728, 'accumulated_submission_time': 76925.65456914902, 'accumulated_eval_time': 9596.894824266434, 'accumulated_logging_time': 8.953065156936646}
I0204 13:07:06.918146 139774434195200 logging_writer.py:48] [165017] accumulated_eval_time=9596.894824, accumulated_logging_time=8.953065, accumulated_submission_time=76925.654569, global_step=165017, preemption_count=0, score=76925.654569, test/accuracy=0.625400, test/loss=1.648834, test/num_examples=10000, total_duration=86540.225177, train/accuracy=0.838184, train/loss=0.639176, validation/accuracy=0.749840, validation/loss=1.023460, validation/num_examples=50000
I0204 13:07:42.653895 139774417409792 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.8723130226135254, loss=1.5378317832946777
I0204 13:08:29.592104 139774434195200 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.7926619052886963, loss=1.58046555519104
I0204 13:09:16.840175 139774417409792 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.648106098175049, loss=3.966325044631958
I0204 13:10:03.704975 139774434195200 logging_writer.py:48] [165400] global_step=165400, grad_norm=3.2266857624053955, loss=1.515718698501587
I0204 13:10:50.895629 139774417409792 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.6895883083343506, loss=1.9980460405349731
I0204 13:11:38.473219 139774434195200 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.5182628631591797, loss=1.5556219816207886
I0204 13:12:25.858059 139774417409792 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.7817726135253906, loss=1.4384241104125977
I0204 13:13:13.225256 139774434195200 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.656313180923462, loss=3.7063474655151367
I0204 13:14:00.077200 139774417409792 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.641787528991699, loss=4.040780544281006
I0204 13:14:06.979553 139936116377408 spec.py:321] Evaluating on the training split.
I0204 13:14:19.013487 139936116377408 spec.py:333] Evaluating on the validation split.
I0204 13:14:58.024082 139936116377408 spec.py:349] Evaluating on the test split.
I0204 13:14:59.636718 139936116377408 submission_runner.py:408] Time since start: 87012.99s, 	Step: 165916, 	{'train/accuracy': 0.8354687094688416, 'train/loss': 0.6519188284873962, 'validation/accuracy': 0.7498599886894226, 'validation/loss': 1.0220907926559448, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.657045841217041, 'test/num_examples': 10000, 'score': 77345.65645003319, 'total_duration': 87012.98839354515, 'accumulated_submission_time': 77345.65645003319, 'accumulated_eval_time': 9649.550794363022, 'accumulated_logging_time': 9.01101303100586}
I0204 13:14:59.680426 139774434195200 logging_writer.py:48] [165916] accumulated_eval_time=9649.550794, accumulated_logging_time=9.011013, accumulated_submission_time=77345.656450, global_step=165916, preemption_count=0, score=77345.656450, test/accuracy=0.627200, test/loss=1.657046, test/num_examples=10000, total_duration=87012.988394, train/accuracy=0.835469, train/loss=0.651919, validation/accuracy=0.749860, validation/loss=1.022091, validation/num_examples=50000
I0204 13:15:35.642487 139774417409792 logging_writer.py:48] [166000] global_step=166000, grad_norm=3.022615909576416, loss=1.479039192199707
I0204 13:16:22.355656 139774434195200 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.830413818359375, loss=1.6995148658752441
I0204 13:17:09.537114 139774417409792 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.666724920272827, loss=1.4840986728668213
I0204 13:17:54.482641 139774434195200 logging_writer.py:48] [166298] global_step=166298, preemption_count=0, score=77520.369380
I0204 13:17:55.675611 139936116377408 checkpoints.py:490] Saving checkpoint at step: 166298
I0204 13:17:57.119529 139936116377408 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_5/checkpoint_166298
I0204 13:17:57.146155 139936116377408 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_2/imagenet_vit_jax/trial_5/checkpoint_166298.
I0204 13:17:58.228593 139936116377408 submission_runner.py:583] Tuning trial 5/5
I0204 13:17:58.228887 139936116377408 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0204 13:17:58.243086 139936116377408 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009374999790452421, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 40.29682993888855, 'total_duration': 93.49904775619507, 'accumulated_submission_time': 40.29682993888855, 'accumulated_eval_time': 53.20211386680603, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (850, {'train/accuracy': 0.034003905951976776, 'train/loss': 5.898688793182373, 'validation/accuracy': 0.029259998351335526, 'validation/loss': 5.964993476867676, 'validation/num_examples': 50000, 'test/accuracy': 0.02240000106394291, 'test/loss': 6.085418224334717, 'test/num_examples': 10000, 'score': 460.35428380966187, 'total_duration': 564.3717305660248, 'accumulated_submission_time': 460.35428380966187, 'accumulated_eval_time': 103.95526385307312, 'accumulated_logging_time': 0.018001556396484375, 'global_step': 850, 'preemption_count': 0}), (1755, {'train/accuracy': 0.06978515535593033, 'train/loss': 5.347999572753906, 'validation/accuracy': 0.06480000168085098, 'validation/loss': 5.401942253112793, 'validation/num_examples': 50000, 'test/accuracy': 0.05040000379085541, 'test/loss': 5.601574420928955, 'test/num_examples': 10000, 'score': 880.7335257530212, 'total_duration': 1034.3701009750366, 'accumulated_submission_time': 880.7335257530212, 'accumulated_eval_time': 153.4896755218506, 'accumulated_logging_time': 0.055764198303222656, 'global_step': 1755, 'preemption_count': 0}), (2667, {'train/accuracy': 0.11636718362569809, 'train/loss': 4.784284591674805, 'validation/accuracy': 0.1093599945306778, 'validation/loss': 4.85033655166626, 'validation/num_examples': 50000, 'test/accuracy': 0.0861000046133995, 'test/loss': 5.135005950927734, 'test/num_examples': 10000, 'score': 1301.019949913025, 'total_duration': 1506.262699842453, 'accumulated_submission_time': 1301.019949913025, 'accumulated_eval_time': 205.0133285522461, 'accumulated_logging_time': 0.09068870544433594, 'global_step': 2667, 'preemption_count': 0}), (3575, {'train/accuracy': 0.17994140088558197, 'train/loss': 4.237734794616699, 'validation/accuracy': 0.16110000014305115, 'validation/loss': 4.358086585998535, 'validation/num_examples': 50000, 'test/accuracy': 0.12190000712871552, 'test/loss': 4.7613372802734375, 'test/num_examples': 10000, 'score': 1721.0184772014618, 'total_duration': 1976.8429546356201, 'accumulated_submission_time': 1721.0184772014618, 'accumulated_eval_time': 255.52188277244568, 'accumulated_logging_time': 0.11698031425476074, 'global_step': 3575, 'preemption_count': 0}), (4481, {'train/accuracy': 0.21689452230930328, 'train/loss': 3.9525179862976074, 'validation/accuracy': 0.2021399885416031, 'validation/loss': 4.050927639007568, 'validation/num_examples': 50000, 'test/accuracy': 0.1535000056028366, 'test/loss': 4.470614910125732, 'test/num_examples': 10000, 'score': 2141.226948738098, 'total_duration': 2449.213127851486, 'accumulated_submission_time': 2141.226948738098, 'accumulated_eval_time': 307.60889863967896, 'accumulated_logging_time': 0.14408445358276367, 'global_step': 4481, 'preemption_count': 0}), (5390, {'train/accuracy': 0.26624998450279236, 'train/loss': 3.6010043621063232, 'validation/accuracy': 0.24743999540805817, 'validation/loss': 3.722931385040283, 'validation/num_examples': 50000, 'test/accuracy': 0.1933000087738037, 'test/loss': 4.191379547119141, 'test/num_examples': 10000, 'score': 2561.221724510193, 'total_duration': 2921.4478216171265, 'accumulated_submission_time': 2561.221724510193, 'accumulated_eval_time': 359.7699909210205, 'accumulated_logging_time': 0.17619538307189941, 'global_step': 5390, 'preemption_count': 0}), (6302, {'train/accuracy': 0.3132421672344208, 'train/loss': 3.299769163131714, 'validation/accuracy': 0.2857999801635742, 'validation/loss': 3.4638993740081787, 'validation/num_examples': 50000, 'test/accuracy': 0.21870000660419464, 'test/loss': 3.9623067378997803, 'test/num_examples': 10000, 'score': 2981.236423969269, 'total_duration': 3395.177416563034, 'accumulated_submission_time': 2981.236423969269, 'accumulated_eval_time': 413.40653800964355, 'accumulated_logging_time': 0.20651817321777344, 'global_step': 6302, 'preemption_count': 0}), (7214, {'train/accuracy': 0.3377148509025574, 'train/loss': 3.1365878582000732, 'validation/accuracy': 0.3125399947166443, 'validation/loss': 3.2730464935302734, 'validation/num_examples': 50000, 'test/accuracy': 0.2492000162601471, 'test/loss': 3.8009603023529053, 'test/num_examples': 10000, 'score': 3401.2242062091827, 'total_duration': 3866.0432856082916, 'accumulated_submission_time': 3401.2242062091827, 'accumulated_eval_time': 464.2115104198456, 'accumulated_logging_time': 0.23270773887634277, 'global_step': 7214, 'preemption_count': 0}), (8125, {'train/accuracy': 0.37572264671325684, 'train/loss': 2.8953447341918945, 'validation/accuracy': 0.3463599979877472, 'validation/loss': 3.064788341522217, 'validation/num_examples': 50000, 'test/accuracy': 0.26820001006126404, 'test/loss': 3.6436169147491455, 'test/num_examples': 10000, 'score': 3821.3774077892303, 'total_duration': 4336.0475380420685, 'accumulated_submission_time': 3821.3774077892303, 'accumulated_eval_time': 513.9831821918488, 'accumulated_logging_time': 0.26525330543518066, 'global_step': 8125, 'preemption_count': 0}), (9031, {'train/accuracy': 0.3907421827316284, 'train/loss': 2.777474880218506, 'validation/accuracy': 0.35627999901771545, 'validation/loss': 2.988455295562744, 'validation/num_examples': 50000, 'test/accuracy': 0.2786000072956085, 'test/loss': 3.5704877376556396, 'test/num_examples': 10000, 'score': 4241.45069026947, 'total_duration': 4807.605703353882, 'accumulated_submission_time': 4241.45069026947, 'accumulated_eval_time': 565.3924582004547, 'accumulated_logging_time': 0.2942073345184326, 'global_step': 9031, 'preemption_count': 0}), (9936, {'train/accuracy': 0.3995312452316284, 'train/loss': 2.739250659942627, 'validation/accuracy': 0.373879998922348, 'validation/loss': 2.8733174800872803, 'validation/num_examples': 50000, 'test/accuracy': 0.2880000174045563, 'test/loss': 3.4860610961914062, 'test/num_examples': 10000, 'score': 4661.742277622223, 'total_duration': 5279.853937387466, 'accumulated_submission_time': 4661.742277622223, 'accumulated_eval_time': 617.273538351059, 'accumulated_logging_time': 0.3234410285949707, 'global_step': 9936, 'preemption_count': 0}), (10843, {'train/accuracy': 0.4029882848262787, 'train/loss': 2.765798807144165, 'validation/accuracy': 0.3730599880218506, 'validation/loss': 2.929877519607544, 'validation/num_examples': 50000, 'test/accuracy': 0.29330000281333923, 'test/loss': 3.4927845001220703, 'test/num_examples': 10000, 'score': 5081.717084169388, 'total_duration': 5753.537490606308, 'accumulated_submission_time': 5081.717084169388, 'accumulated_eval_time': 670.9042701721191, 'accumulated_logging_time': 0.35435938835144043, 'global_step': 10843, 'preemption_count': 0}), (11750, {'train/accuracy': 0.4351171851158142, 'train/loss': 2.5774996280670166, 'validation/accuracy': 0.39941999316215515, 'validation/loss': 2.767735004425049, 'validation/num_examples': 50000, 'test/accuracy': 0.31130000948905945, 'test/loss': 3.375816583633423, 'test/num_examples': 10000, 'score': 5501.863046884537, 'total_duration': 6225.0086896419525, 'accumulated_submission_time': 5501.863046884537, 'accumulated_eval_time': 722.1547908782959, 'accumulated_logging_time': 0.3821694850921631, 'global_step': 11750, 'preemption_count': 0}), (12656, {'train/accuracy': 0.4435742199420929, 'train/loss': 2.502842903137207, 'validation/accuracy': 0.4075399935245514, 'validation/loss': 2.688591718673706, 'validation/num_examples': 50000, 'test/accuracy': 0.3148000240325928, 'test/loss': 3.308043956756592, 'test/num_examples': 10000, 'score': 5921.816805839539, 'total_duration': 6697.850959300995, 'accumulated_submission_time': 5921.816805839539, 'accumulated_eval_time': 774.9626307487488, 'accumulated_logging_time': 0.41584062576293945, 'global_step': 12656, 'preemption_count': 0}), (13561, {'train/accuracy': 0.4646874964237213, 'train/loss': 2.3886172771453857, 'validation/accuracy': 0.4336400032043457, 'validation/loss': 2.552778720855713, 'validation/num_examples': 50000, 'test/accuracy': 0.33830001950263977, 'test/loss': 3.179518222808838, 'test/num_examples': 10000, 'score': 6342.030628442764, 'total_duration': 7169.624086141586, 'accumulated_submission_time': 6342.030628442764, 'accumulated_eval_time': 826.442524433136, 'accumulated_logging_time': 0.4475893974304199, 'global_step': 13561, 'preemption_count': 0}), (14465, {'train/accuracy': 0.48277342319488525, 'train/loss': 2.269944429397583, 'validation/accuracy': 0.4453199803829193, 'validation/loss': 2.4657223224639893, 'validation/num_examples': 50000, 'test/accuracy': 0.34390002489089966, 'test/loss': 3.1078310012817383, 'test/num_examples': 10000, 'score': 6762.030198812485, 'total_duration': 7642.619294166565, 'accumulated_submission_time': 6762.030198812485, 'accumulated_eval_time': 879.3584513664246, 'accumulated_logging_time': 0.47969818115234375, 'global_step': 14465, 'preemption_count': 0}), (15371, {'train/accuracy': 0.5088281035423279, 'train/loss': 2.144819974899292, 'validation/accuracy': 0.4482799768447876, 'validation/loss': 2.458869457244873, 'validation/num_examples': 50000, 'test/accuracy': 0.344400018453598, 'test/loss': 3.1176199913024902, 'test/num_examples': 10000, 'score': 7182.221163272858, 'total_duration': 8113.95642209053, 'accumulated_submission_time': 7182.221163272858, 'accumulated_eval_time': 930.4304752349854, 'accumulated_logging_time': 0.5069384574890137, 'global_step': 15371, 'preemption_count': 0}), (16277, {'train/accuracy': 0.4963085949420929, 'train/loss': 2.2050812244415283, 'validation/accuracy': 0.45799997448921204, 'validation/loss': 2.4059407711029053, 'validation/num_examples': 50000, 'test/accuracy': 0.35430002212524414, 'test/loss': 3.049616813659668, 'test/num_examples': 10000, 'score': 7602.466048240662, 'total_duration': 8585.593609571457, 'accumulated_submission_time': 7602.466048240662, 'accumulated_eval_time': 981.7441575527191, 'accumulated_logging_time': 0.5383155345916748, 'global_step': 16277, 'preemption_count': 0}), (17183, {'train/accuracy': 0.49980467557907104, 'train/loss': 2.1979124546051025, 'validation/accuracy': 0.46143999695777893, 'validation/loss': 2.4079489707946777, 'validation/num_examples': 50000, 'test/accuracy': 0.36330002546310425, 'test/loss': 3.038583993911743, 'test/num_examples': 10000, 'score': 8022.563579797745, 'total_duration': 9057.795295476913, 'accumulated_submission_time': 8022.563579797745, 'accumulated_eval_time': 1033.7704393863678, 'accumulated_logging_time': 0.5695366859436035, 'global_step': 17183, 'preemption_count': 0}), (18085, {'train/accuracy': 0.5384179353713989, 'train/loss': 1.9806084632873535, 'validation/accuracy': 0.47623997926712036, 'validation/loss': 2.3108084201812744, 'validation/num_examples': 50000, 'test/accuracy': 0.3718000054359436, 'test/loss': 2.9495246410369873, 'test/num_examples': 10000, 'score': 8442.695451498032, 'total_duration': 9530.55030632019, 'accumulated_submission_time': 8442.695451498032, 'accumulated_eval_time': 1086.3128879070282, 'accumulated_logging_time': 0.6030721664428711, 'global_step': 18085, 'preemption_count': 0}), (18992, {'train/accuracy': 0.5228710770606995, 'train/loss': 2.079188823699951, 'validation/accuracy': 0.487419992685318, 'validation/loss': 2.272768020629883, 'validation/num_examples': 50000, 'test/accuracy': 0.380700021982193, 'test/loss': 2.9152309894561768, 'test/num_examples': 10000, 'score': 8862.955168247223, 'total_duration': 10002.043234586716, 'accumulated_submission_time': 8862.955168247223, 'accumulated_eval_time': 1137.469367980957, 'accumulated_logging_time': 0.6330897808074951, 'global_step': 18992, 'preemption_count': 0}), (19898, {'train/accuracy': 0.5240820050239563, 'train/loss': 2.092133045196533, 'validation/accuracy': 0.48131999373435974, 'validation/loss': 2.309021234512329, 'validation/num_examples': 50000, 'test/accuracy': 0.37890002131462097, 'test/loss': 2.931926727294922, 'test/num_examples': 10000, 'score': 9283.24651813507, 'total_duration': 10475.134246349335, 'accumulated_submission_time': 9283.24651813507, 'accumulated_eval_time': 1190.1830968856812, 'accumulated_logging_time': 0.6724350452423096, 'global_step': 19898, 'preemption_count': 0}), (20801, {'train/accuracy': 0.5454296469688416, 'train/loss': 1.9585974216461182, 'validation/accuracy': 0.4882199764251709, 'validation/loss': 2.2411868572235107, 'validation/num_examples': 50000, 'test/accuracy': 0.3815000057220459, 'test/loss': 2.893972873687744, 'test/num_examples': 10000, 'score': 9703.740409374237, 'total_duration': 10947.46983218193, 'accumulated_submission_time': 9703.740409374237, 'accumulated_eval_time': 1241.9485597610474, 'accumulated_logging_time': 0.7021317481994629, 'global_step': 20801, 'preemption_count': 0}), (21708, {'train/accuracy': 0.5394921898841858, 'train/loss': 1.981076717376709, 'validation/accuracy': 0.49657997488975525, 'validation/loss': 2.200188636779785, 'validation/num_examples': 50000, 'test/accuracy': 0.3945000171661377, 'test/loss': 2.8344380855560303, 'test/num_examples': 10000, 'score': 10123.801826238632, 'total_duration': 11420.220302343369, 'accumulated_submission_time': 10123.801826238632, 'accumulated_eval_time': 1294.5543503761292, 'accumulated_logging_time': 0.7387468814849854, 'global_step': 21708, 'preemption_count': 0}), (22610, {'train/accuracy': 0.5367578268051147, 'train/loss': 1.97980535030365, 'validation/accuracy': 0.4986799955368042, 'validation/loss': 2.189040422439575, 'validation/num_examples': 50000, 'test/accuracy': 0.380700021982193, 'test/loss': 2.866044759750366, 'test/num_examples': 10000, 'score': 10543.785840034485, 'total_duration': 11891.540809392929, 'accumulated_submission_time': 10543.785840034485, 'accumulated_eval_time': 1345.811369419098, 'accumulated_logging_time': 0.7716727256774902, 'global_step': 22610, 'preemption_count': 0}), (23511, {'train/accuracy': 0.5564843416213989, 'train/loss': 1.8949991464614868, 'validation/accuracy': 0.5028799772262573, 'validation/loss': 2.1828277111053467, 'validation/num_examples': 50000, 'test/accuracy': 0.39750000834465027, 'test/loss': 2.843322515487671, 'test/num_examples': 10000, 'score': 10963.787488222122, 'total_duration': 12363.682228326797, 'accumulated_submission_time': 10963.787488222122, 'accumulated_eval_time': 1397.873296737671, 'accumulated_logging_time': 0.8026630878448486, 'global_step': 23511, 'preemption_count': 0}), (24413, {'train/accuracy': 0.5447851419448853, 'train/loss': 1.9759751558303833, 'validation/accuracy': 0.5110799670219421, 'validation/loss': 2.1550962924957275, 'validation/num_examples': 50000, 'test/accuracy': 0.39980003237724304, 'test/loss': 2.798171281814575, 'test/num_examples': 10000, 'score': 11383.82677412033, 'total_duration': 12836.232869625092, 'accumulated_submission_time': 11383.82677412033, 'accumulated_eval_time': 1450.3069911003113, 'accumulated_logging_time': 0.8335375785827637, 'global_step': 24413, 'preemption_count': 0}), (25317, {'train/accuracy': 0.5570703148841858, 'train/loss': 1.8775259256362915, 'validation/accuracy': 0.5131399631500244, 'validation/loss': 2.1032450199127197, 'validation/num_examples': 50000, 'test/accuracy': 0.4092000126838684, 'test/loss': 2.747539758682251, 'test/num_examples': 10000, 'score': 11803.812463283539, 'total_duration': 13308.797145843506, 'accumulated_submission_time': 11803.812463283539, 'accumulated_eval_time': 1502.8061113357544, 'accumulated_logging_time': 0.8648619651794434, 'global_step': 25317, 'preemption_count': 0}), (26218, {'train/accuracy': 0.5672265291213989, 'train/loss': 1.8946034908294678, 'validation/accuracy': 0.5156599879264832, 'validation/loss': 2.1460132598876953, 'validation/num_examples': 50000, 'test/accuracy': 0.40300002694129944, 'test/loss': 2.811911106109619, 'test/num_examples': 10000, 'score': 12223.884135246277, 'total_duration': 13781.913187265396, 'accumulated_submission_time': 12223.884135246277, 'accumulated_eval_time': 1555.772926568985, 'accumulated_logging_time': 0.895737886428833, 'global_step': 26218, 'preemption_count': 0}), (27116, {'train/accuracy': 0.5529491901397705, 'train/loss': 1.901785969734192, 'validation/accuracy': 0.5184800028800964, 'validation/loss': 2.0907881259918213, 'validation/num_examples': 50000, 'test/accuracy': 0.4043000340461731, 'test/loss': 2.7777791023254395, 'test/num_examples': 10000, 'score': 12643.856747865677, 'total_duration': 14252.936617612839, 'accumulated_submission_time': 12643.856747865677, 'accumulated_eval_time': 1606.7444801330566, 'accumulated_logging_time': 0.9290673732757568, 'global_step': 27116, 'preemption_count': 0}), (28016, {'train/accuracy': 0.5609570145606995, 'train/loss': 1.945162057876587, 'validation/accuracy': 0.5189399719238281, 'validation/loss': 2.1581993103027344, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.7968037128448486, 'test/num_examples': 10000, 'score': 13064.020081281662, 'total_duration': 14725.257707118988, 'accumulated_submission_time': 13064.020081281662, 'accumulated_eval_time': 1658.8222217559814, 'accumulated_logging_time': 0.9629182815551758, 'global_step': 28016, 'preemption_count': 0}), (28920, {'train/accuracy': 0.5832226276397705, 'train/loss': 1.7956725358963013, 'validation/accuracy': 0.5298199653625488, 'validation/loss': 2.0622482299804688, 'validation/num_examples': 50000, 'test/accuracy': 0.41280001401901245, 'test/loss': 2.7176918983459473, 'test/num_examples': 10000, 'score': 13484.096655845642, 'total_duration': 15197.368015289307, 'accumulated_submission_time': 13484.096655845642, 'accumulated_eval_time': 1710.7781717777252, 'accumulated_logging_time': 0.99446702003479, 'global_step': 28920, 'preemption_count': 0}), (29821, {'train/accuracy': 0.5748437643051147, 'train/loss': 1.8023862838745117, 'validation/accuracy': 0.538159966468811, 'validation/loss': 1.9932000637054443, 'validation/num_examples': 50000, 'test/accuracy': 0.4171000123023987, 'test/loss': 2.6709110736846924, 'test/num_examples': 10000, 'score': 13904.335749864578, 'total_duration': 15671.247034311295, 'accumulated_submission_time': 13904.335749864578, 'accumulated_eval_time': 1764.337646484375, 'accumulated_logging_time': 1.0288941860198975, 'global_step': 29821, 'preemption_count': 0}), (30719, {'train/accuracy': 0.5781054496765137, 'train/loss': 1.7854104042053223, 'validation/accuracy': 0.5322200059890747, 'validation/loss': 2.0117881298065186, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.6758742332458496, 'test/num_examples': 10000, 'score': 14324.31052160263, 'total_duration': 16144.646352529526, 'accumulated_submission_time': 14324.31052160263, 'accumulated_eval_time': 1817.6779038906097, 'accumulated_logging_time': 1.0666768550872803, 'global_step': 30719, 'preemption_count': 0}), (31622, {'train/accuracy': 0.5849804282188416, 'train/loss': 1.757678747177124, 'validation/accuracy': 0.5351200103759766, 'validation/loss': 2.018251895904541, 'validation/num_examples': 50000, 'test/accuracy': 0.42000001668930054, 'test/loss': 2.6826159954071045, 'test/num_examples': 10000, 'score': 14744.462392568588, 'total_duration': 16617.61550116539, 'accumulated_submission_time': 14744.462392568588, 'accumulated_eval_time': 1870.412175655365, 'accumulated_logging_time': 1.103344440460205, 'global_step': 31622, 'preemption_count': 0}), (32523, {'train/accuracy': 0.5832421779632568, 'train/loss': 1.8074359893798828, 'validation/accuracy': 0.5415599942207336, 'validation/loss': 2.0093460083007812, 'validation/num_examples': 50000, 'test/accuracy': 0.4262000322341919, 'test/loss': 2.6587982177734375, 'test/num_examples': 10000, 'score': 15164.471905231476, 'total_duration': 17090.816098451614, 'accumulated_submission_time': 15164.471905231476, 'accumulated_eval_time': 1923.5166292190552, 'accumulated_logging_time': 1.1434593200683594, 'global_step': 32523, 'preemption_count': 0}), (33427, {'train/accuracy': 0.583300769329071, 'train/loss': 1.8000158071517944, 'validation/accuracy': 0.5366399884223938, 'validation/loss': 2.0200142860412598, 'validation/num_examples': 50000, 'test/accuracy': 0.4164000153541565, 'test/loss': 2.6936354637145996, 'test/num_examples': 10000, 'score': 15584.505164146423, 'total_duration': 17563.168486595154, 'accumulated_submission_time': 15584.505164146423, 'accumulated_eval_time': 1975.7538046836853, 'accumulated_logging_time': 1.178342580795288, 'global_step': 33427, 'preemption_count': 0}), (34328, {'train/accuracy': 0.6021679639816284, 'train/loss': 1.6713885068893433, 'validation/accuracy': 0.5493800044059753, 'validation/loss': 1.9222406148910522, 'validation/num_examples': 50000, 'test/accuracy': 0.43300002813339233, 'test/loss': 2.5797040462493896, 'test/num_examples': 10000, 'score': 16004.51957321167, 'total_duration': 18034.655037164688, 'accumulated_submission_time': 16004.51957321167, 'accumulated_eval_time': 2027.1375963687897, 'accumulated_logging_time': 1.2206003665924072, 'global_step': 34328, 'preemption_count': 0}), (35225, {'train/accuracy': 0.6086132526397705, 'train/loss': 1.6429468393325806, 'validation/accuracy': 0.5566200017929077, 'validation/loss': 1.9106091260910034, 'validation/num_examples': 50000, 'test/accuracy': 0.44130003452301025, 'test/loss': 2.5836009979248047, 'test/num_examples': 10000, 'score': 16424.51560664177, 'total_duration': 18504.921072244644, 'accumulated_submission_time': 16424.51560664177, 'accumulated_eval_time': 2077.322345972061, 'accumulated_logging_time': 1.2598974704742432, 'global_step': 35225, 'preemption_count': 0}), (36125, {'train/accuracy': 0.5963281393051147, 'train/loss': 1.712787389755249, 'validation/accuracy': 0.5558199882507324, 'validation/loss': 1.9079389572143555, 'validation/num_examples': 50000, 'test/accuracy': 0.4359000325202942, 'test/loss': 2.588874578475952, 'test/num_examples': 10000, 'score': 16844.621319770813, 'total_duration': 18976.279500722885, 'accumulated_submission_time': 16844.621319770813, 'accumulated_eval_time': 2128.4921691417694, 'accumulated_logging_time': 1.2963852882385254, 'global_step': 36125, 'preemption_count': 0}), (37028, {'train/accuracy': 0.6057812571525574, 'train/loss': 1.6403157711029053, 'validation/accuracy': 0.5555599927902222, 'validation/loss': 1.885812759399414, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.56365966796875, 'test/num_examples': 10000, 'score': 17264.70506668091, 'total_duration': 19447.05951809883, 'accumulated_submission_time': 17264.70506668091, 'accumulated_eval_time': 2179.108320236206, 'accumulated_logging_time': 1.329272985458374, 'global_step': 37028, 'preemption_count': 0}), (37930, {'train/accuracy': 0.6322460770606995, 'train/loss': 1.546537160873413, 'validation/accuracy': 0.5542399883270264, 'validation/loss': 1.9083751440048218, 'validation/num_examples': 50000, 'test/accuracy': 0.43630000948905945, 'test/loss': 2.5774965286254883, 'test/num_examples': 10000, 'score': 17684.651047706604, 'total_duration': 19920.133969783783, 'accumulated_submission_time': 17684.651047706604, 'accumulated_eval_time': 2232.1497917175293, 'accumulated_logging_time': 1.3694918155670166, 'global_step': 37930, 'preemption_count': 0}), (38831, {'train/accuracy': 0.5987695455551147, 'train/loss': 1.698283076286316, 'validation/accuracy': 0.5547999739646912, 'validation/loss': 1.9160023927688599, 'validation/num_examples': 50000, 'test/accuracy': 0.42990002036094666, 'test/loss': 2.6002378463745117, 'test/num_examples': 10000, 'score': 18104.869975566864, 'total_duration': 20393.270917892456, 'accumulated_submission_time': 18104.869975566864, 'accumulated_eval_time': 2284.9851546287537, 'accumulated_logging_time': 1.4052977561950684, 'global_step': 38831, 'preemption_count': 0}), (39728, {'train/accuracy': 0.609570324420929, 'train/loss': 1.6623051166534424, 'validation/accuracy': 0.5617200136184692, 'validation/loss': 1.9099894762039185, 'validation/num_examples': 50000, 'test/accuracy': 0.4417000114917755, 'test/loss': 2.5730700492858887, 'test/num_examples': 10000, 'score': 18524.83588886261, 'total_duration': 20877.36520934105, 'accumulated_submission_time': 18524.83588886261, 'accumulated_eval_time': 2349.030675649643, 'accumulated_logging_time': 1.4428019523620605, 'global_step': 39728, 'preemption_count': 0}), (40622, {'train/accuracy': 0.6304491758346558, 'train/loss': 1.5252255201339722, 'validation/accuracy': 0.5624600052833557, 'validation/loss': 1.8603116273880005, 'validation/num_examples': 50000, 'test/accuracy': 0.4377000331878662, 'test/loss': 2.548011302947998, 'test/num_examples': 10000, 'score': 18944.7302005291, 'total_duration': 21348.91973233223, 'accumulated_submission_time': 18944.7302005291, 'accumulated_eval_time': 2400.5719459056854, 'accumulated_logging_time': 1.515040397644043, 'global_step': 40622, 'preemption_count': 0}), (41519, {'train/accuracy': 0.6089648008346558, 'train/loss': 1.633178949356079, 'validation/accuracy': 0.568839967250824, 'validation/loss': 1.8405061960220337, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.518946886062622, 'test/num_examples': 10000, 'score': 19365.09273838997, 'total_duration': 21822.46431851387, 'accumulated_submission_time': 19365.09273838997, 'accumulated_eval_time': 2453.674109697342, 'accumulated_logging_time': 1.5481061935424805, 'global_step': 41519, 'preemption_count': 0}), (42423, {'train/accuracy': 0.6158398389816284, 'train/loss': 1.5993508100509644, 'validation/accuracy': 0.5668599605560303, 'validation/loss': 1.8338241577148438, 'validation/num_examples': 50000, 'test/accuracy': 0.4497000277042389, 'test/loss': 2.5079567432403564, 'test/num_examples': 10000, 'score': 19785.181124925613, 'total_duration': 22294.79103422165, 'accumulated_submission_time': 19785.181124925613, 'accumulated_eval_time': 2505.8282821178436, 'accumulated_logging_time': 1.584458351135254, 'global_step': 42423, 'preemption_count': 0}), (43326, {'train/accuracy': 0.6241992115974426, 'train/loss': 1.6059962511062622, 'validation/accuracy': 0.5625, 'validation/loss': 1.9052478075027466, 'validation/num_examples': 50000, 'test/accuracy': 0.4489000141620636, 'test/loss': 2.5377256870269775, 'test/num_examples': 10000, 'score': 20205.273720502853, 'total_duration': 22766.971638917923, 'accumulated_submission_time': 20205.273720502853, 'accumulated_eval_time': 2557.823818206787, 'accumulated_logging_time': 1.6289563179016113, 'global_step': 43326, 'preemption_count': 0}), (44229, {'train/accuracy': 0.6158398389816284, 'train/loss': 1.6221504211425781, 'validation/accuracy': 0.5743799805641174, 'validation/loss': 1.8318836688995361, 'validation/num_examples': 50000, 'test/accuracy': 0.45190003514289856, 'test/loss': 2.4976508617401123, 'test/num_examples': 10000, 'score': 20625.553758859634, 'total_duration': 23239.426404237747, 'accumulated_submission_time': 20625.553758859634, 'accumulated_eval_time': 2609.916860103607, 'accumulated_logging_time': 1.6631481647491455, 'global_step': 44229, 'preemption_count': 0}), (45133, {'train/accuracy': 0.6198632717132568, 'train/loss': 1.6021332740783691, 'validation/accuracy': 0.5716800093650818, 'validation/loss': 1.8257529735565186, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.4946107864379883, 'test/num_examples': 10000, 'score': 21044.993222475052, 'total_duration': 23713.709257364273, 'accumulated_submission_time': 21044.993222475052, 'accumulated_eval_time': 2664.1930780410767, 'accumulated_logging_time': 2.1820499897003174, 'global_step': 45133, 'preemption_count': 0}), (46038, {'train/accuracy': 0.6359961032867432, 'train/loss': 1.5632343292236328, 'validation/accuracy': 0.5751199722290039, 'validation/loss': 1.8546125888824463, 'validation/num_examples': 50000, 'test/accuracy': 0.4570000171661377, 'test/loss': 2.525801658630371, 'test/num_examples': 10000, 'score': 21465.308529138565, 'total_duration': 24186.573442697525, 'accumulated_submission_time': 21465.308529138565, 'accumulated_eval_time': 2716.657520532608, 'accumulated_logging_time': 2.2180991172790527, 'global_step': 46038, 'preemption_count': 0}), (46941, {'train/accuracy': 0.6237695217132568, 'train/loss': 1.5933055877685547, 'validation/accuracy': 0.583840012550354, 'validation/loss': 1.7873661518096924, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.460035562515259, 'test/num_examples': 10000, 'score': 21885.264724254608, 'total_duration': 24660.883398771286, 'accumulated_submission_time': 21885.264724254608, 'accumulated_eval_time': 2770.9248657226562, 'accumulated_logging_time': 2.2558722496032715, 'global_step': 46941, 'preemption_count': 0}), (47845, {'train/accuracy': 0.6243554353713989, 'train/loss': 1.5742669105529785, 'validation/accuracy': 0.5797199606895447, 'validation/loss': 1.8012193441390991, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.4564502239227295, 'test/num_examples': 10000, 'score': 22305.34062218666, 'total_duration': 25134.393942832947, 'accumulated_submission_time': 22305.34062218666, 'accumulated_eval_time': 2824.276191473007, 'accumulated_logging_time': 2.2912559509277344, 'global_step': 47845, 'preemption_count': 0}), (48753, {'train/accuracy': 0.6461523175239563, 'train/loss': 1.4468914270401, 'validation/accuracy': 0.5879200100898743, 'validation/loss': 1.7366979122161865, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.4156606197357178, 'test/num_examples': 10000, 'score': 22725.429410219193, 'total_duration': 25606.750625133514, 'accumulated_submission_time': 22725.429410219193, 'accumulated_eval_time': 2876.451560020447, 'accumulated_logging_time': 2.335695505142212, 'global_step': 48753, 'preemption_count': 0}), (49661, {'train/accuracy': 0.6366406083106995, 'train/loss': 1.5108044147491455, 'validation/accuracy': 0.5891199707984924, 'validation/loss': 1.7323944568634033, 'validation/num_examples': 50000, 'test/accuracy': 0.4748000204563141, 'test/loss': 2.404741048812866, 'test/num_examples': 10000, 'score': 23145.645364046097, 'total_duration': 26079.421197891235, 'accumulated_submission_time': 23145.645364046097, 'accumulated_eval_time': 2928.8170251846313, 'accumulated_logging_time': 2.37554931640625, 'global_step': 49661, 'preemption_count': 0}), (50559, {'train/accuracy': 0.637988269329071, 'train/loss': 1.5000442266464233, 'validation/accuracy': 0.5877199769020081, 'validation/loss': 1.7394182682037354, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.3809423446655273, 'test/num_examples': 10000, 'score': 23565.832825899124, 'total_duration': 26552.102893590927, 'accumulated_submission_time': 23565.832825899124, 'accumulated_eval_time': 2981.225423812866, 'accumulated_logging_time': 2.4138035774230957, 'global_step': 50559, 'preemption_count': 0}), (51462, {'train/accuracy': 0.6421484351158142, 'train/loss': 1.4973787069320679, 'validation/accuracy': 0.5862199664115906, 'validation/loss': 1.781178593635559, 'validation/num_examples': 50000, 'test/accuracy': 0.46670001745224, 'test/loss': 2.434500217437744, 'test/num_examples': 10000, 'score': 23985.97656941414, 'total_duration': 27024.848866462708, 'accumulated_submission_time': 23985.97656941414, 'accumulated_eval_time': 3033.735938310623, 'accumulated_logging_time': 2.4571444988250732, 'global_step': 51462, 'preemption_count': 0}), (52366, {'train/accuracy': 0.6407226324081421, 'train/loss': 1.4778684377670288, 'validation/accuracy': 0.5956599712371826, 'validation/loss': 1.697656512260437, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.3861453533172607, 'test/num_examples': 10000, 'score': 24406.12998008728, 'total_duration': 27497.530173778534, 'accumulated_submission_time': 24406.12998008728, 'accumulated_eval_time': 3086.1749098300934, 'accumulated_logging_time': 2.498885154724121, 'global_step': 52366, 'preemption_count': 0}), (53267, {'train/accuracy': 0.6386523246765137, 'train/loss': 1.511078119277954, 'validation/accuracy': 0.5876399874687195, 'validation/loss': 1.755619764328003, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.407470703125, 'test/num_examples': 10000, 'score': 24826.21729016304, 'total_duration': 27969.93226337433, 'accumulated_submission_time': 24826.21729016304, 'accumulated_eval_time': 3138.3990700244904, 'accumulated_logging_time': 2.5419416427612305, 'global_step': 53267, 'preemption_count': 0}), (54170, {'train/accuracy': 0.6471874713897705, 'train/loss': 1.4455610513687134, 'validation/accuracy': 0.5920599699020386, 'validation/loss': 1.721648097038269, 'validation/num_examples': 50000, 'test/accuracy': 0.4725000262260437, 'test/loss': 2.3761520385742188, 'test/num_examples': 10000, 'score': 25246.371876716614, 'total_duration': 28441.82798075676, 'accumulated_submission_time': 25246.371876716614, 'accumulated_eval_time': 3190.0559356212616, 'accumulated_logging_time': 2.5783660411834717, 'global_step': 54170, 'preemption_count': 0}), (55074, {'train/accuracy': 0.6447460651397705, 'train/loss': 1.4728977680206299, 'validation/accuracy': 0.5917400121688843, 'validation/loss': 1.7395529747009277, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.3855762481689453, 'test/num_examples': 10000, 'score': 25666.634063720703, 'total_duration': 28914.098296403885, 'accumulated_submission_time': 25666.634063720703, 'accumulated_eval_time': 3241.9795260429382, 'accumulated_logging_time': 2.6154439449310303, 'global_step': 55074, 'preemption_count': 0}), (55970, {'train/accuracy': 0.6394140720367432, 'train/loss': 1.5037568807601929, 'validation/accuracy': 0.5911999940872192, 'validation/loss': 1.7273492813110352, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.3871467113494873, 'test/num_examples': 10000, 'score': 26086.776544094086, 'total_duration': 29387.199322223663, 'accumulated_submission_time': 26086.776544094086, 'accumulated_eval_time': 3294.8521118164062, 'accumulated_logging_time': 2.6539435386657715, 'global_step': 55970, 'preemption_count': 0}), (56872, {'train/accuracy': 0.6453320384025574, 'train/loss': 1.4583547115325928, 'validation/accuracy': 0.5988999605178833, 'validation/loss': 1.701833724975586, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.3658065795898438, 'test/num_examples': 10000, 'score': 26506.762736082077, 'total_duration': 29860.154549121857, 'accumulated_submission_time': 26506.762736082077, 'accumulated_eval_time': 3347.73197889328, 'accumulated_logging_time': 2.6959104537963867, 'global_step': 56872, 'preemption_count': 0}), (57778, {'train/accuracy': 0.6798242330551147, 'train/loss': 1.318408727645874, 'validation/accuracy': 0.6038199663162231, 'validation/loss': 1.671668529510498, 'validation/num_examples': 50000, 'test/accuracy': 0.48570001125335693, 'test/loss': 2.3210160732269287, 'test/num_examples': 10000, 'score': 26926.744647026062, 'total_duration': 30332.42511534691, 'accumulated_submission_time': 26926.744647026062, 'accumulated_eval_time': 3399.9325094223022, 'accumulated_logging_time': 2.73496150970459, 'global_step': 57778, 'preemption_count': 0}), (58680, {'train/accuracy': 0.6500585675239563, 'train/loss': 1.4647369384765625, 'validation/accuracy': 0.5982599854469299, 'validation/loss': 1.699963092803955, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.356879711151123, 'test/num_examples': 10000, 'score': 27346.960132598877, 'total_duration': 30803.393351316452, 'accumulated_submission_time': 27346.960132598877, 'accumulated_eval_time': 3450.595221042633, 'accumulated_logging_time': 2.778135299682617, 'global_step': 58680, 'preemption_count': 0}), (59582, {'train/accuracy': 0.6515820026397705, 'train/loss': 1.4490962028503418, 'validation/accuracy': 0.6025399565696716, 'validation/loss': 1.6890755891799927, 'validation/num_examples': 50000, 'test/accuracy': 0.4764000177383423, 'test/loss': 2.356093168258667, 'test/num_examples': 10000, 'score': 27767.20039820671, 'total_duration': 31276.52426123619, 'accumulated_submission_time': 27767.20039820671, 'accumulated_eval_time': 3503.4003579616547, 'accumulated_logging_time': 2.815491199493408, 'global_step': 59582, 'preemption_count': 0}), (60486, {'train/accuracy': 0.6818945407867432, 'train/loss': 1.308190941810608, 'validation/accuracy': 0.6083399653434753, 'validation/loss': 1.650006651878357, 'validation/num_examples': 50000, 'test/accuracy': 0.4871000349521637, 'test/loss': 2.3113863468170166, 'test/num_examples': 10000, 'score': 28187.20550107956, 'total_duration': 31748.228764772415, 'accumulated_submission_time': 28187.20550107956, 'accumulated_eval_time': 3555.011140346527, 'accumulated_logging_time': 2.856693983078003, 'global_step': 60486, 'preemption_count': 0}), (61388, {'train/accuracy': 0.6479101181030273, 'train/loss': 1.457031011581421, 'validation/accuracy': 0.6025999784469604, 'validation/loss': 1.6835922002792358, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.3440380096435547, 'test/num_examples': 10000, 'score': 28607.257138967514, 'total_duration': 32220.116106033325, 'accumulated_submission_time': 28607.257138967514, 'accumulated_eval_time': 3606.759976387024, 'accumulated_logging_time': 2.8961234092712402, 'global_step': 61388, 'preemption_count': 0}), (62289, {'train/accuracy': 0.6520702838897705, 'train/loss': 1.4345024824142456, 'validation/accuracy': 0.6013000011444092, 'validation/loss': 1.6837353706359863, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.3569390773773193, 'test/num_examples': 10000, 'score': 29027.53623533249, 'total_duration': 32690.442022562027, 'accumulated_submission_time': 29027.53623533249, 'accumulated_eval_time': 3656.720737695694, 'accumulated_logging_time': 2.934617042541504, 'global_step': 62289, 'preemption_count': 0}), (63194, {'train/accuracy': 0.6779687404632568, 'train/loss': 1.3029876947402954, 'validation/accuracy': 0.6114599704742432, 'validation/loss': 1.6231791973114014, 'validation/num_examples': 50000, 'test/accuracy': 0.4918000102043152, 'test/loss': 2.2952914237976074, 'test/num_examples': 10000, 'score': 29447.884727954865, 'total_duration': 33163.27663230896, 'accumulated_submission_time': 29447.884727954865, 'accumulated_eval_time': 3709.118235349655, 'accumulated_logging_time': 2.974860906600952, 'global_step': 63194, 'preemption_count': 0}), (64090, {'train/accuracy': 0.6555468440055847, 'train/loss': 1.3943846225738525, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.6383819580078125, 'validation/num_examples': 50000, 'test/accuracy': 0.48660001158714294, 'test/loss': 2.292811155319214, 'test/num_examples': 10000, 'score': 29868.319049596786, 'total_duration': 33636.6328959465, 'accumulated_submission_time': 29868.319049596786, 'accumulated_eval_time': 3761.9516339302063, 'accumulated_logging_time': 3.0155563354492188, 'global_step': 64090, 'preemption_count': 0}), (64992, {'train/accuracy': 0.6604882478713989, 'train/loss': 1.4079033136367798, 'validation/accuracy': 0.6082199811935425, 'validation/loss': 1.6526474952697754, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.3205599784851074, 'test/num_examples': 10000, 'score': 30288.43496155739, 'total_duration': 34108.627766132355, 'accumulated_submission_time': 30288.43496155739, 'accumulated_eval_time': 3813.7393848896027, 'accumulated_logging_time': 3.059473991394043, 'global_step': 64992, 'preemption_count': 0}), (65896, {'train/accuracy': 0.6743749976158142, 'train/loss': 1.3156598806381226, 'validation/accuracy': 0.6094799637794495, 'validation/loss': 1.6173152923583984, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.2992825508117676, 'test/num_examples': 10000, 'score': 30708.749766349792, 'total_duration': 34583.26203036308, 'accumulated_submission_time': 30708.749766349792, 'accumulated_eval_time': 3867.965404987335, 'accumulated_logging_time': 3.1049630641937256, 'global_step': 65896, 'preemption_count': 0}), (66799, {'train/accuracy': 0.6649413704872131, 'train/loss': 1.3905673027038574, 'validation/accuracy': 0.613599956035614, 'validation/loss': 1.6307467222213745, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.2967209815979004, 'test/num_examples': 10000, 'score': 31128.717413187027, 'total_duration': 35056.56464314461, 'accumulated_submission_time': 31128.717413187027, 'accumulated_eval_time': 3921.2101068496704, 'accumulated_logging_time': 3.146657943725586, 'global_step': 66799, 'preemption_count': 0}), (67700, {'train/accuracy': 0.666308581829071, 'train/loss': 1.3803210258483887, 'validation/accuracy': 0.6187199950218201, 'validation/loss': 1.6084468364715576, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.2668533325195312, 'test/num_examples': 10000, 'score': 31548.65906047821, 'total_duration': 35530.14267539978, 'accumulated_submission_time': 31548.65906047821, 'accumulated_eval_time': 3974.759640932083, 'accumulated_logging_time': 3.186652421951294, 'global_step': 67700, 'preemption_count': 0}), (68601, {'train/accuracy': 0.6759960651397705, 'train/loss': 1.306284785270691, 'validation/accuracy': 0.6125800013542175, 'validation/loss': 1.6162729263305664, 'validation/num_examples': 50000, 'test/accuracy': 0.491100013256073, 'test/loss': 2.2899532318115234, 'test/num_examples': 10000, 'score': 31968.73847270012, 'total_duration': 36000.59927082062, 'accumulated_submission_time': 31968.73847270012, 'accumulated_eval_time': 4025.041212797165, 'accumulated_logging_time': 3.2343225479125977, 'global_step': 68601, 'preemption_count': 0}), (69503, {'train/accuracy': 0.6673437356948853, 'train/loss': 1.392255187034607, 'validation/accuracy': 0.6157999634742737, 'validation/loss': 1.6166682243347168, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.2810490131378174, 'test/num_examples': 10000, 'score': 32388.73214316368, 'total_duration': 36475.47456264496, 'accumulated_submission_time': 32388.73214316368, 'accumulated_eval_time': 4079.8373177051544, 'accumulated_logging_time': 3.27199387550354, 'global_step': 69503, 'preemption_count': 0}), (70398, {'train/accuracy': 0.6661718487739563, 'train/loss': 1.4046850204467773, 'validation/accuracy': 0.6162399649620056, 'validation/loss': 1.6451646089553833, 'validation/num_examples': 50000, 'test/accuracy': 0.4962000250816345, 'test/loss': 2.3044371604919434, 'test/num_examples': 10000, 'score': 32808.83074641228, 'total_duration': 36949.61338496208, 'accumulated_submission_time': 32808.83074641228, 'accumulated_eval_time': 4133.789508104324, 'accumulated_logging_time': 3.3121135234832764, 'global_step': 70398, 'preemption_count': 0}), (71300, {'train/accuracy': 0.671875, 'train/loss': 1.3597897291183472, 'validation/accuracy': 0.6108399629592896, 'validation/loss': 1.6405349969863892, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.279672861099243, 'test/num_examples': 10000, 'score': 33228.976645469666, 'total_duration': 37422.65373325348, 'accumulated_submission_time': 33228.976645469666, 'accumulated_eval_time': 4186.583588838577, 'accumulated_logging_time': 3.3608882427215576, 'global_step': 71300, 'preemption_count': 0}), (72203, {'train/accuracy': 0.6790234446525574, 'train/loss': 1.29867684841156, 'validation/accuracy': 0.6268599629402161, 'validation/loss': 1.5448366403579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.2146520614624023, 'test/num_examples': 10000, 'score': 33649.24731469154, 'total_duration': 37895.018824100494, 'accumulated_submission_time': 33649.24731469154, 'accumulated_eval_time': 4238.585715770721, 'accumulated_logging_time': 3.405482292175293, 'global_step': 72203, 'preemption_count': 0}), (73109, {'train/accuracy': 0.6642382740974426, 'train/loss': 1.3958392143249512, 'validation/accuracy': 0.6126999855041504, 'validation/loss': 1.638340711593628, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.272181510925293, 'test/num_examples': 10000, 'score': 34069.50347137451, 'total_duration': 38368.23072266579, 'accumulated_submission_time': 34069.50347137451, 'accumulated_eval_time': 4291.450464725494, 'accumulated_logging_time': 3.4479641914367676, 'global_step': 73109, 'preemption_count': 0}), (74011, {'train/accuracy': 0.6847460865974426, 'train/loss': 1.295527458190918, 'validation/accuracy': 0.6244399547576904, 'validation/loss': 1.5823724269866943, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.2567830085754395, 'test/num_examples': 10000, 'score': 34489.668355703354, 'total_duration': 38841.821252822876, 'accumulated_submission_time': 34489.668355703354, 'accumulated_eval_time': 4344.784727811813, 'accumulated_logging_time': 3.4918909072875977, 'global_step': 74011, 'preemption_count': 0}), (74915, {'train/accuracy': 0.6771484017372131, 'train/loss': 1.3254854679107666, 'validation/accuracy': 0.6175999641418457, 'validation/loss': 1.5920697450637817, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.259209156036377, 'test/num_examples': 10000, 'score': 34909.960586071014, 'total_duration': 39313.50108551979, 'accumulated_submission_time': 34909.960586071014, 'accumulated_eval_time': 4396.087966918945, 'accumulated_logging_time': 3.528670072555542, 'global_step': 74915, 'preemption_count': 0}), (75819, {'train/accuracy': 0.6713671684265137, 'train/loss': 1.3427538871765137, 'validation/accuracy': 0.6200799942016602, 'validation/loss': 1.5841830968856812, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.25400972366333, 'test/num_examples': 10000, 'score': 35330.13725447655, 'total_duration': 39784.29939389229, 'accumulated_submission_time': 35330.13725447655, 'accumulated_eval_time': 4446.619699478149, 'accumulated_logging_time': 3.5707647800445557, 'global_step': 75819, 'preemption_count': 0}), (76720, {'train/accuracy': 0.6835741996765137, 'train/loss': 1.2883057594299316, 'validation/accuracy': 0.6281999945640564, 'validation/loss': 1.5592522621154785, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.233535051345825, 'test/num_examples': 10000, 'score': 35750.3324637413, 'total_duration': 40256.8454182148, 'accumulated_submission_time': 35750.3324637413, 'accumulated_eval_time': 4498.87525844574, 'accumulated_logging_time': 3.6172690391540527, 'global_step': 76720, 'preemption_count': 0}), (77628, {'train/accuracy': 0.7032226324081421, 'train/loss': 1.1986013650894165, 'validation/accuracy': 0.6326799988746643, 'validation/loss': 1.526818037033081, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.190584659576416, 'test/num_examples': 10000, 'score': 36170.52016687393, 'total_duration': 40732.36606168747, 'accumulated_submission_time': 36170.52016687393, 'accumulated_eval_time': 4554.121444702148, 'accumulated_logging_time': 3.6560237407684326, 'global_step': 77628, 'preemption_count': 0}), (78530, {'train/accuracy': 0.6783398389816284, 'train/loss': 1.3336890935897827, 'validation/accuracy': 0.6248399615287781, 'validation/loss': 1.5802377462387085, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.228025197982788, 'test/num_examples': 10000, 'score': 36590.65875458717, 'total_duration': 41204.64127254486, 'accumulated_submission_time': 36590.65875458717, 'accumulated_eval_time': 4606.165371894836, 'accumulated_logging_time': 3.701697587966919, 'global_step': 78530, 'preemption_count': 0}), (79433, {'train/accuracy': 0.6779101490974426, 'train/loss': 1.3415353298187256, 'validation/accuracy': 0.6250999569892883, 'validation/loss': 1.593327283859253, 'validation/num_examples': 50000, 'test/accuracy': 0.5078999996185303, 'test/loss': 2.239119291305542, 'test/num_examples': 10000, 'score': 37011.007297992706, 'total_duration': 41676.029999256134, 'accumulated_submission_time': 37011.007297992706, 'accumulated_eval_time': 4657.113859415054, 'accumulated_logging_time': 3.745133638381958, 'global_step': 79433, 'preemption_count': 0}), (80333, {'train/accuracy': 0.712890625, 'train/loss': 1.1509590148925781, 'validation/accuracy': 0.6348599791526794, 'validation/loss': 1.51292884349823, 'validation/num_examples': 50000, 'test/accuracy': 0.5133000016212463, 'test/loss': 2.168611526489258, 'test/num_examples': 10000, 'score': 37431.40174984932, 'total_duration': 42151.15966916084, 'accumulated_submission_time': 37431.40174984932, 'accumulated_eval_time': 4711.764025211334, 'accumulated_logging_time': 3.782468557357788, 'global_step': 80333, 'preemption_count': 0}), (81236, {'train/accuracy': 0.6807616949081421, 'train/loss': 1.2854599952697754, 'validation/accuracy': 0.6318599581718445, 'validation/loss': 1.5201311111450195, 'validation/num_examples': 50000, 'test/accuracy': 0.49960002303123474, 'test/loss': 2.2001304626464844, 'test/num_examples': 10000, 'score': 37851.49418973923, 'total_duration': 42622.748197078705, 'accumulated_submission_time': 37851.49418973923, 'accumulated_eval_time': 4763.166525602341, 'accumulated_logging_time': 3.8283190727233887, 'global_step': 81236, 'preemption_count': 0}), (82140, {'train/accuracy': 0.6885156035423279, 'train/loss': 1.2505003213882446, 'validation/accuracy': 0.6342399716377258, 'validation/loss': 1.5106966495513916, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.1858279705047607, 'test/num_examples': 10000, 'score': 38271.61984539032, 'total_duration': 43095.682314157486, 'accumulated_submission_time': 38271.61984539032, 'accumulated_eval_time': 4815.883181333542, 'accumulated_logging_time': 3.870955228805542, 'global_step': 82140, 'preemption_count': 0}), (83040, {'train/accuracy': 0.7089062333106995, 'train/loss': 1.1777950525283813, 'validation/accuracy': 0.6371200084686279, 'validation/loss': 1.510980486869812, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.158198595046997, 'test/num_examples': 10000, 'score': 38691.78195428848, 'total_duration': 43568.31066441536, 'accumulated_submission_time': 38691.78195428848, 'accumulated_eval_time': 4868.257662296295, 'accumulated_logging_time': 3.9152021408081055, 'global_step': 83040, 'preemption_count': 0}), (83940, {'train/accuracy': 0.6898242235183716, 'train/loss': 1.2514363527297974, 'validation/accuracy': 0.6396399736404419, 'validation/loss': 1.4833993911743164, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.157160758972168, 'test/num_examples': 10000, 'score': 39111.83612918854, 'total_duration': 44039.46033358574, 'accumulated_submission_time': 39111.83612918854, 'accumulated_eval_time': 4919.264448881149, 'accumulated_logging_time': 3.956256151199341, 'global_step': 83940, 'preemption_count': 0}), (84841, {'train/accuracy': 0.6862499713897705, 'train/loss': 1.308943748474121, 'validation/accuracy': 0.6337400078773499, 'validation/loss': 1.5579991340637207, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2265491485595703, 'test/num_examples': 10000, 'score': 39531.96011543274, 'total_duration': 44512.09685301781, 'accumulated_submission_time': 39531.96011543274, 'accumulated_eval_time': 4971.682218551636, 'accumulated_logging_time': 4.003744602203369, 'global_step': 84841, 'preemption_count': 0}), (85741, {'train/accuracy': 0.7101757526397705, 'train/loss': 1.1636791229248047, 'validation/accuracy': 0.6434400081634521, 'validation/loss': 1.4765359163284302, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.157416820526123, 'test/num_examples': 10000, 'score': 39952.28334736824, 'total_duration': 44983.476644039154, 'accumulated_submission_time': 39952.28334736824, 'accumulated_eval_time': 5022.644863128662, 'accumulated_logging_time': 4.050050735473633, 'global_step': 85741, 'preemption_count': 0}), (86646, {'train/accuracy': 0.6906445026397705, 'train/loss': 1.2567678689956665, 'validation/accuracy': 0.6412999629974365, 'validation/loss': 1.5003139972686768, 'validation/num_examples': 50000, 'test/accuracy': 0.5220000147819519, 'test/loss': 2.1490726470947266, 'test/num_examples': 10000, 'score': 40372.555807352066, 'total_duration': 45456.74989652634, 'accumulated_submission_time': 40372.555807352066, 'accumulated_eval_time': 5075.555291175842, 'accumulated_logging_time': 4.092345476150513, 'global_step': 86646, 'preemption_count': 0}), (87543, {'train/accuracy': 0.6939257383346558, 'train/loss': 1.2720842361450195, 'validation/accuracy': 0.6372399926185608, 'validation/loss': 1.5326356887817383, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.205008029937744, 'test/num_examples': 10000, 'score': 40792.581500291824, 'total_duration': 45931.0048494339, 'accumulated_submission_time': 40792.581500291824, 'accumulated_eval_time': 5129.692433595657, 'accumulated_logging_time': 4.137173175811768, 'global_step': 87543, 'preemption_count': 0}), (88444, {'train/accuracy': 0.7095507383346558, 'train/loss': 1.1825847625732422, 'validation/accuracy': 0.6438800096511841, 'validation/loss': 1.4855278730392456, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.1474361419677734, 'test/num_examples': 10000, 'score': 41212.59487867355, 'total_duration': 46403.07826638222, 'accumulated_submission_time': 41212.59487867355, 'accumulated_eval_time': 5181.660806417465, 'accumulated_logging_time': 4.181311845779419, 'global_step': 88444, 'preemption_count': 0}), (89347, {'train/accuracy': 0.697949230670929, 'train/loss': 1.2049740552902222, 'validation/accuracy': 0.646120011806488, 'validation/loss': 1.454306960105896, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.1224191188812256, 'test/num_examples': 10000, 'score': 41632.62812304497, 'total_duration': 46873.910395145416, 'accumulated_submission_time': 41632.62812304497, 'accumulated_eval_time': 5232.368206501007, 'accumulated_logging_time': 4.225497245788574, 'global_step': 89347, 'preemption_count': 0}), (90248, {'train/accuracy': 0.7050976157188416, 'train/loss': 1.1831070184707642, 'validation/accuracy': 0.6489799618721008, 'validation/loss': 1.4485565423965454, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1029837131500244, 'test/num_examples': 10000, 'score': 42052.653123140335, 'total_duration': 47345.378999471664, 'accumulated_submission_time': 42052.653123140335, 'accumulated_eval_time': 5283.711639404297, 'accumulated_logging_time': 4.278661251068115, 'global_step': 90248, 'preemption_count': 0}), (91150, {'train/accuracy': 0.7136132717132568, 'train/loss': 1.1587448120117188, 'validation/accuracy': 0.645859956741333, 'validation/loss': 1.4699233770370483, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.1205849647521973, 'test/num_examples': 10000, 'score': 42472.69245290756, 'total_duration': 47817.961052656174, 'accumulated_submission_time': 42472.69245290756, 'accumulated_eval_time': 5336.162372112274, 'accumulated_logging_time': 4.322976589202881, 'global_step': 91150, 'preemption_count': 0}), (92050, {'train/accuracy': 0.7071288824081421, 'train/loss': 1.196994662284851, 'validation/accuracy': 0.6503399610519409, 'validation/loss': 1.4454437494277954, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.090376138687134, 'test/num_examples': 10000, 'score': 42893.01210975647, 'total_duration': 48290.780958890915, 'accumulated_submission_time': 42893.01210975647, 'accumulated_eval_time': 5388.570991754532, 'accumulated_logging_time': 4.366878986358643, 'global_step': 92050, 'preemption_count': 0}), (92948, {'train/accuracy': 0.7015038728713989, 'train/loss': 1.22853684425354, 'validation/accuracy': 0.6442599892616272, 'validation/loss': 1.4875376224517822, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.143444061279297, 'test/num_examples': 10000, 'score': 43313.314470767975, 'total_duration': 48763.079761981964, 'accumulated_submission_time': 43313.314470767975, 'accumulated_eval_time': 5440.473873138428, 'accumulated_logging_time': 4.412621974945068, 'global_step': 92948, 'preemption_count': 0}), (93845, {'train/accuracy': 0.7256445288658142, 'train/loss': 1.0961700677871704, 'validation/accuracy': 0.6585800051689148, 'validation/loss': 1.40444016456604, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.0604445934295654, 'test/num_examples': 10000, 'score': 43733.2725212574, 'total_duration': 49234.66075634956, 'accumulated_submission_time': 43733.2725212574, 'accumulated_eval_time': 5492.004205703735, 'accumulated_logging_time': 4.4586100578308105, 'global_step': 93845, 'preemption_count': 0}), (94745, {'train/accuracy': 0.7081640362739563, 'train/loss': 1.187019944190979, 'validation/accuracy': 0.6531999707221985, 'validation/loss': 1.43330717086792, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.109593629837036, 'test/num_examples': 10000, 'score': 44153.533059597015, 'total_duration': 49705.9593732357, 'accumulated_submission_time': 44153.533059597015, 'accumulated_eval_time': 5542.949748277664, 'accumulated_logging_time': 4.503470420837402, 'global_step': 94745, 'preemption_count': 0}), (95645, {'train/accuracy': 0.7108983993530273, 'train/loss': 1.1773436069488525, 'validation/accuracy': 0.6543799638748169, 'validation/loss': 1.4492274522781372, 'validation/num_examples': 50000, 'test/accuracy': 0.5276000499725342, 'test/loss': 2.1062769889831543, 'test/num_examples': 10000, 'score': 44573.71209073067, 'total_duration': 50177.37110543251, 'accumulated_submission_time': 44573.71209073067, 'accumulated_eval_time': 5594.092439651489, 'accumulated_logging_time': 4.546373128890991, 'global_step': 95645, 'preemption_count': 0}), (96550, {'train/accuracy': 0.7174413800239563, 'train/loss': 1.1969013214111328, 'validation/accuracy': 0.6540799736976624, 'validation/loss': 1.4859381914138794, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.1328086853027344, 'test/num_examples': 10000, 'score': 44993.845116853714, 'total_duration': 50651.08602452278, 'accumulated_submission_time': 44993.845116853714, 'accumulated_eval_time': 5647.579032897949, 'accumulated_logging_time': 4.593611240386963, 'global_step': 96550, 'preemption_count': 0}), (97448, {'train/accuracy': 0.7165625095367432, 'train/loss': 1.188768744468689, 'validation/accuracy': 0.6569199562072754, 'validation/loss': 1.463334083557129, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.111708402633667, 'test/num_examples': 10000, 'score': 45413.95650100708, 'total_duration': 51122.57685351372, 'accumulated_submission_time': 45413.95650100708, 'accumulated_eval_time': 5698.86651301384, 'accumulated_logging_time': 4.638423681259155, 'global_step': 97448, 'preemption_count': 0}), (98347, {'train/accuracy': 0.7182226181030273, 'train/loss': 1.1286863088607788, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.4054856300354004, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.0756752490997314, 'test/num_examples': 10000, 'score': 45834.25590658188, 'total_duration': 51593.67358827591, 'accumulated_submission_time': 45834.25590658188, 'accumulated_eval_time': 5749.567130565643, 'accumulated_logging_time': 4.687403678894043, 'global_step': 98347, 'preemption_count': 0}), (99248, {'train/accuracy': 0.7224413752555847, 'train/loss': 1.1078177690505981, 'validation/accuracy': 0.660319983959198, 'validation/loss': 1.3991767168045044, 'validation/num_examples': 50000, 'test/accuracy': 0.535800039768219, 'test/loss': 2.0520083904266357, 'test/num_examples': 10000, 'score': 46254.56909441948, 'total_duration': 52066.551471710205, 'accumulated_submission_time': 46254.56909441948, 'accumulated_eval_time': 5802.036701917648, 'accumulated_logging_time': 4.73491358757019, 'global_step': 99248, 'preemption_count': 0}), (100151, {'train/accuracy': 0.7393554449081421, 'train/loss': 1.0557329654693604, 'validation/accuracy': 0.6615599989891052, 'validation/loss': 1.396098256111145, 'validation/num_examples': 50000, 'test/accuracy': 0.5397000312805176, 'test/loss': 2.0368974208831787, 'test/num_examples': 10000, 'score': 46674.90282559395, 'total_duration': 52540.69532442093, 'accumulated_submission_time': 46674.90282559395, 'accumulated_eval_time': 5855.741189956665, 'accumulated_logging_time': 4.7923808097839355, 'global_step': 100151, 'preemption_count': 0}), (101053, {'train/accuracy': 0.7187304496765137, 'train/loss': 1.1348941326141357, 'validation/accuracy': 0.6573399901390076, 'validation/loss': 1.40870201587677, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.0542080402374268, 'test/num_examples': 10000, 'score': 47095.12478327751, 'total_duration': 53013.22602438927, 'accumulated_submission_time': 47095.12478327751, 'accumulated_eval_time': 5907.954854726791, 'accumulated_logging_time': 4.839926719665527, 'global_step': 101053, 'preemption_count': 0}), (101956, {'train/accuracy': 0.7291796803474426, 'train/loss': 1.1408336162567139, 'validation/accuracy': 0.6621400117874146, 'validation/loss': 1.432441234588623, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.0662755966186523, 'test/num_examples': 10000, 'score': 47515.08821105957, 'total_duration': 53483.97033381462, 'accumulated_submission_time': 47515.08821105957, 'accumulated_eval_time': 5958.642646789551, 'accumulated_logging_time': 4.885095834732056, 'global_step': 101956, 'preemption_count': 0}), (102858, {'train/accuracy': 0.7438867092132568, 'train/loss': 1.0300265550613403, 'validation/accuracy': 0.6672199964523315, 'validation/loss': 1.3864587545394897, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0302770137786865, 'test/num_examples': 10000, 'score': 47935.08392548561, 'total_duration': 53956.87895011902, 'accumulated_submission_time': 47935.08392548561, 'accumulated_eval_time': 6011.463626861572, 'accumulated_logging_time': 4.929841756820679, 'global_step': 102858, 'preemption_count': 0}), (103763, {'train/accuracy': 0.723437488079071, 'train/loss': 1.1205108165740967, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.3746145963668823, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.0381054878234863, 'test/num_examples': 10000, 'score': 48355.07038998604, 'total_duration': 54427.79730439186, 'accumulated_submission_time': 48355.07038998604, 'accumulated_eval_time': 6062.30232834816, 'accumulated_logging_time': 4.975308418273926, 'global_step': 103763, 'preemption_count': 0}), (104666, {'train/accuracy': 0.7344530820846558, 'train/loss': 1.0720163583755493, 'validation/accuracy': 0.6725599765777588, 'validation/loss': 1.3530502319335938, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 1.9986283779144287, 'test/num_examples': 10000, 'score': 48775.350959062576, 'total_duration': 54899.595631599426, 'accumulated_submission_time': 48775.350959062576, 'accumulated_eval_time': 6113.727089166641, 'accumulated_logging_time': 5.020384788513184, 'global_step': 104666, 'preemption_count': 0}), (105559, {'train/accuracy': 0.7469140291213989, 'train/loss': 0.9951573610305786, 'validation/accuracy': 0.6689199805259705, 'validation/loss': 1.3489623069763184, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0017428398132324, 'test/num_examples': 10000, 'score': 49195.26766204834, 'total_duration': 55372.13287234306, 'accumulated_submission_time': 49195.26766204834, 'accumulated_eval_time': 6166.253857374191, 'accumulated_logging_time': 5.067118167877197, 'global_step': 105559, 'preemption_count': 0}), (106458, {'train/accuracy': 0.730273425579071, 'train/loss': 1.1089922189712524, 'validation/accuracy': 0.6736199855804443, 'validation/loss': 1.3686671257019043, 'validation/num_examples': 50000, 'test/accuracy': 0.5472000241279602, 'test/loss': 2.037179470062256, 'test/num_examples': 10000, 'score': 49615.44148349762, 'total_duration': 55844.475420475006, 'accumulated_submission_time': 49615.44148349762, 'accumulated_eval_time': 6218.311373949051, 'accumulated_logging_time': 5.114095211029053, 'global_step': 106458, 'preemption_count': 0}), (107364, {'train/accuracy': 0.731640636920929, 'train/loss': 1.0827327966690063, 'validation/accuracy': 0.6683599948883057, 'validation/loss': 1.369884967803955, 'validation/num_examples': 50000, 'test/accuracy': 0.5441000461578369, 'test/loss': 2.013227939605713, 'test/num_examples': 10000, 'score': 50035.62106966972, 'total_duration': 56315.47707438469, 'accumulated_submission_time': 50035.62106966972, 'accumulated_eval_time': 6269.040015935898, 'accumulated_logging_time': 5.159966707229614, 'global_step': 107364, 'preemption_count': 0}), (108262, {'train/accuracy': 0.7414648532867432, 'train/loss': 1.0510865449905396, 'validation/accuracy': 0.6689800024032593, 'validation/loss': 1.3708804845809937, 'validation/num_examples': 50000, 'test/accuracy': 0.5437000393867493, 'test/loss': 2.018178701400757, 'test/num_examples': 10000, 'score': 50455.644053697586, 'total_duration': 56788.2848212719, 'accumulated_submission_time': 50455.644053697586, 'accumulated_eval_time': 6321.7311725616455, 'accumulated_logging_time': 5.206001281738281, 'global_step': 108262, 'preemption_count': 0}), (109164, {'train/accuracy': 0.7342773079872131, 'train/loss': 1.0697695016860962, 'validation/accuracy': 0.6750999689102173, 'validation/loss': 1.3431190252304077, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 1.997771978378296, 'test/num_examples': 10000, 'score': 50875.999675273895, 'total_duration': 57261.839129686356, 'accumulated_submission_time': 50875.999675273895, 'accumulated_eval_time': 6374.8322966098785, 'accumulated_logging_time': 5.255663156509399, 'global_step': 109164, 'preemption_count': 0}), (110071, {'train/accuracy': 0.7425194978713989, 'train/loss': 1.0702048540115356, 'validation/accuracy': 0.6782400012016296, 'validation/loss': 1.3490768671035767, 'validation/num_examples': 50000, 'test/accuracy': 0.550000011920929, 'test/loss': 2.0083322525024414, 'test/num_examples': 10000, 'score': 51296.140135765076, 'total_duration': 57732.86963033676, 'accumulated_submission_time': 51296.140135765076, 'accumulated_eval_time': 6425.622935056686, 'accumulated_logging_time': 5.307147979736328, 'global_step': 110071, 'preemption_count': 0}), (110978, {'train/accuracy': 0.7479101419448853, 'train/loss': 1.0002694129943848, 'validation/accuracy': 0.6759999990463257, 'validation/loss': 1.3279269933700562, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 1.9583526849746704, 'test/num_examples': 10000, 'score': 51716.33648109436, 'total_duration': 58204.73041367531, 'accumulated_submission_time': 51716.33648109436, 'accumulated_eval_time': 6477.19561457634, 'accumulated_logging_time': 5.351512670516968, 'global_step': 110978, 'preemption_count': 0}), (111880, {'train/accuracy': 0.7358593344688416, 'train/loss': 1.0901877880096436, 'validation/accuracy': 0.6753999590873718, 'validation/loss': 1.3613321781158447, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.0046257972717285, 'test/num_examples': 10000, 'score': 52136.712656497955, 'total_duration': 58676.56856536865, 'accumulated_submission_time': 52136.712656497955, 'accumulated_eval_time': 6528.562636137009, 'accumulated_logging_time': 5.398941516876221, 'global_step': 111880, 'preemption_count': 0}), (112785, {'train/accuracy': 0.7383984327316284, 'train/loss': 1.064749836921692, 'validation/accuracy': 0.6764199733734131, 'validation/loss': 1.340150237083435, 'validation/num_examples': 50000, 'test/accuracy': 0.5529000163078308, 'test/loss': 1.9855674505233765, 'test/num_examples': 10000, 'score': 52557.00045776367, 'total_duration': 59148.58823752403, 'accumulated_submission_time': 52557.00045776367, 'accumulated_eval_time': 6580.2001440525055, 'accumulated_logging_time': 5.444950819015503, 'global_step': 112785, 'preemption_count': 0}), (113682, {'train/accuracy': 0.7493749856948853, 'train/loss': 1.0180168151855469, 'validation/accuracy': 0.6759999990463257, 'validation/loss': 1.3451844453811646, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 1.991071343421936, 'test/num_examples': 10000, 'score': 52976.95989322662, 'total_duration': 59622.63963222504, 'accumulated_submission_time': 52976.95989322662, 'accumulated_eval_time': 6634.199034690857, 'accumulated_logging_time': 5.4898645877838135, 'global_step': 113682, 'preemption_count': 0}), (114585, {'train/accuracy': 0.74609375, 'train/loss': 1.0239732265472412, 'validation/accuracy': 0.6865999698638916, 'validation/loss': 1.2994922399520874, 'validation/num_examples': 50000, 'test/accuracy': 0.5619000196456909, 'test/loss': 1.937618613243103, 'test/num_examples': 10000, 'score': 53397.26622343063, 'total_duration': 60096.39663481712, 'accumulated_submission_time': 53397.26622343063, 'accumulated_eval_time': 6687.5537185668945, 'accumulated_logging_time': 5.537170886993408, 'global_step': 114585, 'preemption_count': 0}), (115490, {'train/accuracy': 0.7510351538658142, 'train/loss': 1.0083297491073608, 'validation/accuracy': 0.687279999256134, 'validation/loss': 1.2991552352905273, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 1.9419201612472534, 'test/num_examples': 10000, 'score': 53817.85648846626, 'total_duration': 60568.140437603, 'accumulated_submission_time': 53817.85648846626, 'accumulated_eval_time': 6738.60989689827, 'accumulated_logging_time': 5.5869903564453125, 'global_step': 115490, 'preemption_count': 0}), (116390, {'train/accuracy': 0.7574414014816284, 'train/loss': 0.960770845413208, 'validation/accuracy': 0.6850199699401855, 'validation/loss': 1.277678370475769, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 1.9282153844833374, 'test/num_examples': 10000, 'score': 54237.85001659393, 'total_duration': 61040.232503175735, 'accumulated_submission_time': 54237.85001659393, 'accumulated_eval_time': 6790.61523938179, 'accumulated_logging_time': 5.63244891166687, 'global_step': 116390, 'preemption_count': 0}), (117290, {'train/accuracy': 0.7455663681030273, 'train/loss': 1.0378332138061523, 'validation/accuracy': 0.6839199662208557, 'validation/loss': 1.3181474208831787, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 1.9513601064682007, 'test/num_examples': 10000, 'score': 54657.93375110626, 'total_duration': 61511.32391309738, 'accumulated_submission_time': 54657.93375110626, 'accumulated_eval_time': 6841.529457330704, 'accumulated_logging_time': 5.67902684211731, 'global_step': 117290, 'preemption_count': 0}), (118185, {'train/accuracy': 0.75244140625, 'train/loss': 1.0247377157211304, 'validation/accuracy': 0.6880599856376648, 'validation/loss': 1.310234546661377, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.9548426866531372, 'test/num_examples': 10000, 'score': 55078.15105628967, 'total_duration': 61983.7370262146, 'accumulated_submission_time': 55078.15105628967, 'accumulated_eval_time': 6893.628536224365, 'accumulated_logging_time': 5.7277820110321045, 'global_step': 118185, 'preemption_count': 0}), (119087, {'train/accuracy': 0.758105456829071, 'train/loss': 0.9552485346794128, 'validation/accuracy': 0.6878600120544434, 'validation/loss': 1.2778600454330444, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 1.9243587255477905, 'test/num_examples': 10000, 'score': 55498.36799407005, 'total_duration': 62457.77957677841, 'accumulated_submission_time': 55498.36799407005, 'accumulated_eval_time': 6947.353275775909, 'accumulated_logging_time': 5.781080007553101, 'global_step': 119087, 'preemption_count': 0}), (119991, {'train/accuracy': 0.7644335627555847, 'train/loss': 0.937131941318512, 'validation/accuracy': 0.6890999674797058, 'validation/loss': 1.2613743543624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.8964364528656006, 'test/num_examples': 10000, 'score': 55918.59027314186, 'total_duration': 62929.843616724014, 'accumulated_submission_time': 55918.59027314186, 'accumulated_eval_time': 6999.096809387207, 'accumulated_logging_time': 5.830645799636841, 'global_step': 119991, 'preemption_count': 0}), (120894, {'train/accuracy': 0.7629492282867432, 'train/loss': 0.9582368731498718, 'validation/accuracy': 0.6942200064659119, 'validation/loss': 1.2611232995986938, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.8966219425201416, 'test/num_examples': 10000, 'score': 56338.974491119385, 'total_duration': 63400.81737446785, 'accumulated_submission_time': 56338.974491119385, 'accumulated_eval_time': 7049.592758893967, 'accumulated_logging_time': 5.876109838485718, 'global_step': 120894, 'preemption_count': 0}), (121797, {'train/accuracy': 0.7608593702316284, 'train/loss': 0.9513280391693115, 'validation/accuracy': 0.6955199837684631, 'validation/loss': 1.2552928924560547, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.8828102350234985, 'test/num_examples': 10000, 'score': 56759.05333805084, 'total_duration': 63873.81185340881, 'accumulated_submission_time': 56759.05333805084, 'accumulated_eval_time': 7102.412714481354, 'accumulated_logging_time': 5.9235618114471436, 'global_step': 121797, 'preemption_count': 0}), (122701, {'train/accuracy': 0.7781640291213989, 'train/loss': 0.8763917088508606, 'validation/accuracy': 0.6922599673271179, 'validation/loss': 1.2633213996887207, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 1.9046696424484253, 'test/num_examples': 10000, 'score': 57179.4742333889, 'total_duration': 64344.930874586105, 'accumulated_submission_time': 57179.4742333889, 'accumulated_eval_time': 7153.010915279388, 'accumulated_logging_time': 5.974011421203613, 'global_step': 122701, 'preemption_count': 0}), (123603, {'train/accuracy': 0.7577733993530273, 'train/loss': 0.9707837700843811, 'validation/accuracy': 0.6915799975395203, 'validation/loss': 1.2661999464035034, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 1.9076391458511353, 'test/num_examples': 10000, 'score': 57599.657051086426, 'total_duration': 64817.058979034424, 'accumulated_submission_time': 57599.657051086426, 'accumulated_eval_time': 7204.862086057663, 'accumulated_logging_time': 6.020724296569824, 'global_step': 123603, 'preemption_count': 0}), (124503, {'train/accuracy': 0.7660741806030273, 'train/loss': 0.9606534838676453, 'validation/accuracy': 0.6967200040817261, 'validation/loss': 1.2727292776107788, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.917148470878601, 'test/num_examples': 10000, 'score': 58019.7577214241, 'total_duration': 65288.98992753029, 'accumulated_submission_time': 58019.7577214241, 'accumulated_eval_time': 7256.597772836685, 'accumulated_logging_time': 6.068363428115845, 'global_step': 124503, 'preemption_count': 0}), (125405, {'train/accuracy': 0.7822851538658142, 'train/loss': 0.8623960018157959, 'validation/accuracy': 0.6998800039291382, 'validation/loss': 1.230411410331726, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 1.8689360618591309, 'test/num_examples': 10000, 'score': 58439.809049129486, 'total_duration': 65759.95871829987, 'accumulated_submission_time': 58439.809049129486, 'accumulated_eval_time': 7307.421809196472, 'accumulated_logging_time': 6.114432096481323, 'global_step': 125405, 'preemption_count': 0}), (126304, {'train/accuracy': 0.764843761920929, 'train/loss': 0.9532604217529297, 'validation/accuracy': 0.6993199586868286, 'validation/loss': 1.2452449798583984, 'validation/num_examples': 50000, 'test/accuracy': 0.5796000361442566, 'test/loss': 1.8750362396240234, 'test/num_examples': 10000, 'score': 58860.01943874359, 'total_duration': 66231.4698665142, 'accumulated_submission_time': 58860.01943874359, 'accumulated_eval_time': 7358.625300168991, 'accumulated_logging_time': 6.164425373077393, 'global_step': 126304, 'preemption_count': 0}), (127206, {'train/accuracy': 0.7702929377555847, 'train/loss': 0.9010846614837646, 'validation/accuracy': 0.7008799910545349, 'validation/loss': 1.2191319465637207, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.8637452125549316, 'test/num_examples': 10000, 'score': 59280.34904909134, 'total_duration': 66703.90618491173, 'accumulated_submission_time': 59280.34904909134, 'accumulated_eval_time': 7410.638778209686, 'accumulated_logging_time': 6.2095441818237305, 'global_step': 127206, 'preemption_count': 0}), (128109, {'train/accuracy': 0.7816601395606995, 'train/loss': 0.8531544208526611, 'validation/accuracy': 0.7025600075721741, 'validation/loss': 1.214359164237976, 'validation/num_examples': 50000, 'test/accuracy': 0.579300045967102, 'test/loss': 1.843841552734375, 'test/num_examples': 10000, 'score': 59700.57011055946, 'total_duration': 67176.2463812828, 'accumulated_submission_time': 59700.57011055946, 'accumulated_eval_time': 7462.659845113754, 'accumulated_logging_time': 6.2601258754730225, 'global_step': 128109, 'preemption_count': 0}), (129007, {'train/accuracy': 0.7702538967132568, 'train/loss': 0.9246523976325989, 'validation/accuracy': 0.7058799862861633, 'validation/loss': 1.214882493019104, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.853320598602295, 'test/num_examples': 10000, 'score': 60120.5441262722, 'total_duration': 67650.05022072792, 'accumulated_submission_time': 60120.5441262722, 'accumulated_eval_time': 7516.386365413666, 'accumulated_logging_time': 6.315888404846191, 'global_step': 129007, 'preemption_count': 0}), (129909, {'train/accuracy': 0.7751367092132568, 'train/loss': 0.8928495049476624, 'validation/accuracy': 0.7060399651527405, 'validation/loss': 1.207680106163025, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.8382052183151245, 'test/num_examples': 10000, 'score': 60540.735575675964, 'total_duration': 68121.60476827621, 'accumulated_submission_time': 60540.735575675964, 'accumulated_eval_time': 7567.649868488312, 'accumulated_logging_time': 6.368206024169922, 'global_step': 129909, 'preemption_count': 0}), (130812, {'train/accuracy': 0.7800390720367432, 'train/loss': 0.8839324712753296, 'validation/accuracy': 0.7036600112915039, 'validation/loss': 1.2200918197631836, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8685678243637085, 'test/num_examples': 10000, 'score': 60960.218658447266, 'total_duration': 68593.51099419594, 'accumulated_submission_time': 60960.218658447266, 'accumulated_eval_time': 7619.496718406677, 'accumulated_logging_time': 6.896216869354248, 'global_step': 130812, 'preemption_count': 0}), (131714, {'train/accuracy': 0.7735546827316284, 'train/loss': 0.887130081653595, 'validation/accuracy': 0.7057799696922302, 'validation/loss': 1.1913654804229736, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.8401563167572021, 'test/num_examples': 10000, 'score': 61380.37313580513, 'total_duration': 69067.9515554905, 'accumulated_submission_time': 61380.37313580513, 'accumulated_eval_time': 7673.678128242493, 'accumulated_logging_time': 6.953572034835815, 'global_step': 131714, 'preemption_count': 0}), (132613, {'train/accuracy': 0.7814843654632568, 'train/loss': 0.8491562604904175, 'validation/accuracy': 0.7097600102424622, 'validation/loss': 1.1710385084152222, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.8052948713302612, 'test/num_examples': 10000, 'score': 61800.565898656845, 'total_duration': 69541.9029185772, 'accumulated_submission_time': 61800.565898656845, 'accumulated_eval_time': 7727.336859464645, 'accumulated_logging_time': 7.005480766296387, 'global_step': 132613, 'preemption_count': 0}), (133511, {'train/accuracy': 0.793652355670929, 'train/loss': 0.8182350993156433, 'validation/accuracy': 0.7105199694633484, 'validation/loss': 1.1751962900161743, 'validation/num_examples': 50000, 'test/accuracy': 0.5838000178337097, 'test/loss': 1.8079313039779663, 'test/num_examples': 10000, 'score': 62220.73200464249, 'total_duration': 70015.30837655067, 'accumulated_submission_time': 62220.73200464249, 'accumulated_eval_time': 7780.478232383728, 'accumulated_logging_time': 7.055763244628906, 'global_step': 133511, 'preemption_count': 0}), (134412, {'train/accuracy': 0.7824413776397705, 'train/loss': 0.8449131846427917, 'validation/accuracy': 0.7128199934959412, 'validation/loss': 1.1581400632858276, 'validation/num_examples': 50000, 'test/accuracy': 0.591200053691864, 'test/loss': 1.7911345958709717, 'test/num_examples': 10000, 'score': 62640.94975614548, 'total_duration': 70485.8378021717, 'accumulated_submission_time': 62640.94975614548, 'accumulated_eval_time': 7830.6907432079315, 'accumulated_logging_time': 7.107162237167358, 'global_step': 134412, 'preemption_count': 0}), (135312, {'train/accuracy': 0.782910168170929, 'train/loss': 0.8488849401473999, 'validation/accuracy': 0.7114799618721008, 'validation/loss': 1.1706856489181519, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 1.7887169122695923, 'test/num_examples': 10000, 'score': 63061.04555249214, 'total_duration': 70957.17614507675, 'accumulated_submission_time': 63061.04555249214, 'accumulated_eval_time': 7881.830750465393, 'accumulated_logging_time': 7.161731958389282, 'global_step': 135312, 'preemption_count': 0}), (136213, {'train/accuracy': 0.7852538824081421, 'train/loss': 0.8555783033370972, 'validation/accuracy': 0.7083799839019775, 'validation/loss': 1.213556170463562, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.8471421003341675, 'test/num_examples': 10000, 'score': 63480.95566868782, 'total_duration': 71429.1494038105, 'accumulated_submission_time': 63480.95566868782, 'accumulated_eval_time': 7933.790218830109, 'accumulated_logging_time': 7.2181336879730225, 'global_step': 136213, 'preemption_count': 0}), (137114, {'train/accuracy': 0.7789257764816284, 'train/loss': 0.8748505711555481, 'validation/accuracy': 0.7115199565887451, 'validation/loss': 1.1856586933135986, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.8299528360366821, 'test/num_examples': 10000, 'score': 63901.086811065674, 'total_duration': 71900.98412513733, 'accumulated_submission_time': 63901.086811065674, 'accumulated_eval_time': 7985.396743774414, 'accumulated_logging_time': 7.266888856887817, 'global_step': 137114, 'preemption_count': 0}), (138009, {'train/accuracy': 0.79212886095047, 'train/loss': 0.8283264636993408, 'validation/accuracy': 0.717960000038147, 'validation/loss': 1.1578574180603027, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.7869914770126343, 'test/num_examples': 10000, 'score': 64321.17126393318, 'total_duration': 72372.30072402954, 'accumulated_submission_time': 64321.17126393318, 'accumulated_eval_time': 8036.531369924545, 'accumulated_logging_time': 7.317673444747925, 'global_step': 138009, 'preemption_count': 0}), (138908, {'train/accuracy': 0.7974609136581421, 'train/loss': 0.8029530048370361, 'validation/accuracy': 0.7170799970626831, 'validation/loss': 1.1547406911849976, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.7757651805877686, 'test/num_examples': 10000, 'score': 64741.42377257347, 'total_duration': 72844.25393557549, 'accumulated_submission_time': 64741.42377257347, 'accumulated_eval_time': 8088.132665634155, 'accumulated_logging_time': 7.369465112686157, 'global_step': 138908, 'preemption_count': 0}), (139808, {'train/accuracy': 0.7875585556030273, 'train/loss': 0.8376666903495789, 'validation/accuracy': 0.7172799706459045, 'validation/loss': 1.1552940607070923, 'validation/num_examples': 50000, 'test/accuracy': 0.5945000052452087, 'test/loss': 1.7900747060775757, 'test/num_examples': 10000, 'score': 65161.54459476471, 'total_duration': 73316.9589650631, 'accumulated_submission_time': 65161.54459476471, 'accumulated_eval_time': 8140.6161987781525, 'accumulated_logging_time': 7.4225077629089355, 'global_step': 139808, 'preemption_count': 0}), (140711, {'train/accuracy': 0.7987695336341858, 'train/loss': 0.7999251484870911, 'validation/accuracy': 0.7211399674415588, 'validation/loss': 1.1418266296386719, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.7677903175354004, 'test/num_examples': 10000, 'score': 65581.90678691864, 'total_duration': 73790.06270742416, 'accumulated_submission_time': 65581.90678691864, 'accumulated_eval_time': 8193.26209139824, 'accumulated_logging_time': 7.47014856338501, 'global_step': 140711, 'preemption_count': 0}), (141614, {'train/accuracy': 0.8019140362739563, 'train/loss': 0.7914997935295105, 'validation/accuracy': 0.7195999622344971, 'validation/loss': 1.156431794166565, 'validation/num_examples': 50000, 'test/accuracy': 0.595300018787384, 'test/loss': 1.7889913320541382, 'test/num_examples': 10000, 'score': 66002.23467111588, 'total_duration': 74262.30921554565, 'accumulated_submission_time': 66002.23467111588, 'accumulated_eval_time': 8245.082573652267, 'accumulated_logging_time': 7.5203306674957275, 'global_step': 141614, 'preemption_count': 0}), (142514, {'train/accuracy': 0.8023241758346558, 'train/loss': 0.7893538475036621, 'validation/accuracy': 0.7219600081443787, 'validation/loss': 1.13852059841156, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.7685530185699463, 'test/num_examples': 10000, 'score': 66422.37011957169, 'total_duration': 74733.39208197594, 'accumulated_submission_time': 66422.37011957169, 'accumulated_eval_time': 8295.933718919754, 'accumulated_logging_time': 7.569155216217041, 'global_step': 142514, 'preemption_count': 0}), (143414, {'train/accuracy': 0.7995898127555847, 'train/loss': 0.8224471211433411, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.164766550064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5990000367164612, 'test/loss': 1.7969648838043213, 'test/num_examples': 10000, 'score': 66842.30435395241, 'total_duration': 75204.91482305527, 'accumulated_submission_time': 66842.30435395241, 'accumulated_eval_time': 8347.408848524094, 'accumulated_logging_time': 7.634051322937012, 'global_step': 143414, 'preemption_count': 0}), (144311, {'train/accuracy': 0.8031640648841858, 'train/loss': 0.8152992725372314, 'validation/accuracy': 0.7218199968338013, 'validation/loss': 1.1692537069320679, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.7974989414215088, 'test/num_examples': 10000, 'score': 67262.48490691185, 'total_duration': 75678.5110874176, 'accumulated_submission_time': 67262.48490691185, 'accumulated_eval_time': 8400.72471165657, 'accumulated_logging_time': 7.686464548110962, 'global_step': 144311, 'preemption_count': 0}), (145214, {'train/accuracy': 0.8151953220367432, 'train/loss': 0.7412189841270447, 'validation/accuracy': 0.7258599996566772, 'validation/loss': 1.138275146484375, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7650368213653564, 'test/num_examples': 10000, 'score': 67682.51352453232, 'total_duration': 76150.04582834244, 'accumulated_submission_time': 67682.51352453232, 'accumulated_eval_time': 8452.127160549164, 'accumulated_logging_time': 7.742059707641602, 'global_step': 145214, 'preemption_count': 0}), (146115, {'train/accuracy': 0.8047655820846558, 'train/loss': 0.7920248508453369, 'validation/accuracy': 0.726099967956543, 'validation/loss': 1.126613736152649, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.7594192028045654, 'test/num_examples': 10000, 'score': 68102.61263489723, 'total_duration': 76621.85620999336, 'accumulated_submission_time': 68102.61263489723, 'accumulated_eval_time': 8503.738919973373, 'accumulated_logging_time': 7.794044256210327, 'global_step': 146115, 'preemption_count': 0}), (147015, {'train/accuracy': 0.8040234446525574, 'train/loss': 0.7940880656242371, 'validation/accuracy': 0.7256399989128113, 'validation/loss': 1.1346133947372437, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.7695281505584717, 'test/num_examples': 10000, 'score': 68522.7866191864, 'total_duration': 77093.98651838303, 'accumulated_submission_time': 68522.7866191864, 'accumulated_eval_time': 8555.59926533699, 'accumulated_logging_time': 7.842405796051025, 'global_step': 147015, 'preemption_count': 0}), (147914, {'train/accuracy': 0.8194531202316284, 'train/loss': 0.7242646813392639, 'validation/accuracy': 0.7303400039672852, 'validation/loss': 1.1180957555770874, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.7449946403503418, 'test/num_examples': 10000, 'score': 68942.73813343048, 'total_duration': 77567.22704386711, 'accumulated_submission_time': 68942.73813343048, 'accumulated_eval_time': 8608.788478851318, 'accumulated_logging_time': 7.8940629959106445, 'global_step': 147914, 'preemption_count': 0}), (148809, {'train/accuracy': 0.8100390434265137, 'train/loss': 0.7443209886550903, 'validation/accuracy': 0.7328599691390991, 'validation/loss': 1.0836445093154907, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.7161839008331299, 'test/num_examples': 10000, 'score': 69363.04661417007, 'total_duration': 78039.55141115189, 'accumulated_submission_time': 69363.04661417007, 'accumulated_eval_time': 8660.702143192291, 'accumulated_logging_time': 7.949257135391235, 'global_step': 148809, 'preemption_count': 0}), (149710, {'train/accuracy': 0.8106640577316284, 'train/loss': 0.7712754011154175, 'validation/accuracy': 0.7304799556732178, 'validation/loss': 1.1216198205947876, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.7387804985046387, 'test/num_examples': 10000, 'score': 69783.11700677872, 'total_duration': 78513.82325577736, 'accumulated_submission_time': 69783.11700677872, 'accumulated_eval_time': 8714.796729803085, 'accumulated_logging_time': 8.007672309875488, 'global_step': 149710, 'preemption_count': 0}), (150609, {'train/accuracy': 0.8217577934265137, 'train/loss': 0.6940465569496155, 'validation/accuracy': 0.7341199517250061, 'validation/loss': 1.0832279920578003, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.7156203985214233, 'test/num_examples': 10000, 'score': 70203.25068831444, 'total_duration': 78987.17783665657, 'accumulated_submission_time': 70203.25068831444, 'accumulated_eval_time': 8767.916935682297, 'accumulated_logging_time': 8.061208963394165, 'global_step': 150609, 'preemption_count': 0}), (151509, {'train/accuracy': 0.8132226467132568, 'train/loss': 0.7564131021499634, 'validation/accuracy': 0.7326799631118774, 'validation/loss': 1.1018426418304443, 'validation/num_examples': 50000, 'test/accuracy': 0.6114000082015991, 'test/loss': 1.7240487337112427, 'test/num_examples': 10000, 'score': 70623.22665309906, 'total_duration': 79460.3146841526, 'accumulated_submission_time': 70623.22665309906, 'accumulated_eval_time': 8820.977076530457, 'accumulated_logging_time': 8.114773273468018, 'global_step': 151509, 'preemption_count': 0}), (152411, {'train/accuracy': 0.8139452934265137, 'train/loss': 0.7501934170722961, 'validation/accuracy': 0.7333599925041199, 'validation/loss': 1.1036591529846191, 'validation/num_examples': 50000, 'test/accuracy': 0.6057000160217285, 'test/loss': 1.7319797277450562, 'test/num_examples': 10000, 'score': 71043.51707410812, 'total_duration': 79930.89526510239, 'accumulated_submission_time': 71043.51707410812, 'accumulated_eval_time': 8871.157362699509, 'accumulated_logging_time': 8.176369428634644, 'global_step': 152411, 'preemption_count': 0}), (153312, {'train/accuracy': 0.826464831829071, 'train/loss': 0.6860719919204712, 'validation/accuracy': 0.7383999824523926, 'validation/loss': 1.0645508766174316, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.6989619731903076, 'test/num_examples': 10000, 'score': 71463.45511341095, 'total_duration': 80403.29509854317, 'accumulated_submission_time': 71463.45511341095, 'accumulated_eval_time': 8923.512953042984, 'accumulated_logging_time': 8.234338998794556, 'global_step': 153312, 'preemption_count': 0}), (154210, {'train/accuracy': 0.8174414038658142, 'train/loss': 0.7186430096626282, 'validation/accuracy': 0.7394199967384338, 'validation/loss': 1.062025547027588, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.6909252405166626, 'test/num_examples': 10000, 'score': 71883.42968702316, 'total_duration': 80874.89892697334, 'accumulated_submission_time': 71883.42968702316, 'accumulated_eval_time': 8975.040938138962, 'accumulated_logging_time': 8.287301778793335, 'global_step': 154210, 'preemption_count': 0}), (155111, {'train/accuracy': 0.8227929472923279, 'train/loss': 0.6901466250419617, 'validation/accuracy': 0.7375199794769287, 'validation/loss': 1.0573351383209229, 'validation/num_examples': 50000, 'test/accuracy': 0.6168000102043152, 'test/loss': 1.6916823387145996, 'test/num_examples': 10000, 'score': 72303.53037238121, 'total_duration': 81346.9751830101, 'accumulated_submission_time': 72303.53037238121, 'accumulated_eval_time': 9026.914111852646, 'accumulated_logging_time': 8.34177279472351, 'global_step': 155111, 'preemption_count': 0}), (156013, {'train/accuracy': 0.8246484398841858, 'train/loss': 0.6990688443183899, 'validation/accuracy': 0.7361199855804443, 'validation/loss': 1.0750834941864014, 'validation/num_examples': 50000, 'test/accuracy': 0.6097000241279602, 'test/loss': 1.709358811378479, 'test/num_examples': 10000, 'score': 72723.77445316315, 'total_duration': 81820.81177711487, 'accumulated_submission_time': 72723.77445316315, 'accumulated_eval_time': 9080.40405368805, 'accumulated_logging_time': 8.39595341682434, 'global_step': 156013, 'preemption_count': 0}), (156915, {'train/accuracy': 0.8210351467132568, 'train/loss': 0.7016533613204956, 'validation/accuracy': 0.74263995885849, 'validation/loss': 1.0572545528411865, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.6846977472305298, 'test/num_examples': 10000, 'score': 73143.83418130875, 'total_duration': 82292.53384494781, 'accumulated_submission_time': 73143.83418130875, 'accumulated_eval_time': 9131.963080406189, 'accumulated_logging_time': 8.450865745544434, 'global_step': 156915, 'preemption_count': 0}), (157815, {'train/accuracy': 0.8214843273162842, 'train/loss': 0.6942675709724426, 'validation/accuracy': 0.7416399717330933, 'validation/loss': 1.0510855913162231, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.6868818998336792, 'test/num_examples': 10000, 'score': 73563.9743270874, 'total_duration': 82763.81664323807, 'accumulated_submission_time': 73563.9743270874, 'accumulated_eval_time': 9182.99531197548, 'accumulated_logging_time': 8.513504981994629, 'global_step': 157815, 'preemption_count': 0}), (158714, {'train/accuracy': 0.8269335627555847, 'train/loss': 0.649540901184082, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.031722068786621, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.66560959815979, 'test/num_examples': 10000, 'score': 73984.35371112823, 'total_duration': 83236.29381513596, 'accumulated_submission_time': 73984.35371112823, 'accumulated_eval_time': 9234.988682985306, 'accumulated_logging_time': 8.569389820098877, 'global_step': 158714, 'preemption_count': 0}), (159614, {'train/accuracy': 0.8246288895606995, 'train/loss': 0.6832771897315979, 'validation/accuracy': 0.7423799633979797, 'validation/loss': 1.0461320877075195, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.683318853378296, 'test/num_examples': 10000, 'score': 74404.43561291695, 'total_duration': 83708.0224416256, 'accumulated_submission_time': 74404.43561291695, 'accumulated_eval_time': 9286.535109996796, 'accumulated_logging_time': 8.62211298942566, 'global_step': 159614, 'preemption_count': 0}), (160509, {'train/accuracy': 0.8296093344688416, 'train/loss': 0.6793026328086853, 'validation/accuracy': 0.744159996509552, 'validation/loss': 1.0463520288467407, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.6868876218795776, 'test/num_examples': 10000, 'score': 74824.56441307068, 'total_duration': 84179.59306025505, 'accumulated_submission_time': 74824.56441307068, 'accumulated_eval_time': 9337.866757631302, 'accumulated_logging_time': 8.684773206710815, 'global_step': 160509, 'preemption_count': 0}), (161411, {'train/accuracy': 0.8265038728713989, 'train/loss': 0.695087730884552, 'validation/accuracy': 0.7425000071525574, 'validation/loss': 1.076271653175354, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.708626389503479, 'test/num_examples': 10000, 'score': 75244.89977121353, 'total_duration': 84653.49061632156, 'accumulated_submission_time': 75244.89977121353, 'accumulated_eval_time': 9391.32567691803, 'accumulated_logging_time': 8.740829706192017, 'global_step': 161411, 'preemption_count': 0}), (162313, {'train/accuracy': 0.8310937285423279, 'train/loss': 0.6638551950454712, 'validation/accuracy': 0.7471999526023865, 'validation/loss': 1.0294922590255737, 'validation/num_examples': 50000, 'test/accuracy': 0.6253000497817993, 'test/loss': 1.6685092449188232, 'test/num_examples': 10000, 'score': 75665.09418177605, 'total_duration': 85124.82177829742, 'accumulated_submission_time': 75665.09418177605, 'accumulated_eval_time': 9442.36303639412, 'accumulated_logging_time': 8.792722463607788, 'global_step': 162313, 'preemption_count': 0}), (163214, {'train/accuracy': 0.8335351347923279, 'train/loss': 0.658676266670227, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.0346159934997559, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.664926290512085, 'test/num_examples': 10000, 'score': 76085.4808113575, 'total_duration': 85596.82547187805, 'accumulated_submission_time': 76085.4808113575, 'accumulated_eval_time': 9493.877099275589, 'accumulated_logging_time': 8.844767093658447, 'global_step': 163214, 'preemption_count': 0}), (164114, {'train/accuracy': 0.8374804258346558, 'train/loss': 0.6364269256591797, 'validation/accuracy': 0.747439980506897, 'validation/loss': 1.0335808992385864, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.6698811054229736, 'test/num_examples': 10000, 'score': 76505.46322965622, 'total_duration': 86068.02240657806, 'accumulated_submission_time': 76505.46322965622, 'accumulated_eval_time': 9544.983451128006, 'accumulated_logging_time': 8.900535106658936, 'global_step': 164114, 'preemption_count': 0}), (165017, {'train/accuracy': 0.838183581829071, 'train/loss': 0.639176070690155, 'validation/accuracy': 0.7498399615287781, 'validation/loss': 1.0234603881835938, 'validation/num_examples': 50000, 'test/accuracy': 0.6254000067710876, 'test/loss': 1.6488336324691772, 'test/num_examples': 10000, 'score': 76925.65456914902, 'total_duration': 86540.2251765728, 'accumulated_submission_time': 76925.65456914902, 'accumulated_eval_time': 9596.894824266434, 'accumulated_logging_time': 8.953065156936646, 'global_step': 165017, 'preemption_count': 0}), (165916, {'train/accuracy': 0.8354687094688416, 'train/loss': 0.6519188284873962, 'validation/accuracy': 0.7498599886894226, 'validation/loss': 1.0220907926559448, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.657045841217041, 'test/num_examples': 10000, 'score': 77345.65645003319, 'total_duration': 87012.98839354515, 'accumulated_submission_time': 77345.65645003319, 'accumulated_eval_time': 9649.550794363022, 'accumulated_logging_time': 9.01101303100586, 'global_step': 165916, 'preemption_count': 0})], 'global_step': 166298}
I0204 13:17:58.244394 139936116377408 submission_runner.py:586] Timing: 77520.36937975883
I0204 13:17:58.244555 139936116377408 submission_runner.py:588] Total number of evals: 185
I0204 13:17:58.244631 139936116377408 submission_runner.py:589] ====================
I0204 13:17:58.247669 139936116377408 submission_runner.py:673] Final imagenet_vit score: 77520.01207590103
