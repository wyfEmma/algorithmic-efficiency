python3 submission_runner.py --framework=jax --workload=librispeech_conformer --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_4 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=4151861576 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_conformer_jax_02-14-2024-02-58-52.log
I0214 02:59:13.227483 140599226058560 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax.
I0214 02:59:14.318552 140599226058560 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0214 02:59:14.319296 140599226058560 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0214 02:59:14.319450 140599226058560 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0214 02:59:14.320567 140599226058560 submission_runner.py:542] Using RNG seed 4151861576
I0214 02:59:19.756400 140599226058560 submission_runner.py:551] --- Tuning run 1/5 ---
I0214 02:59:19.756606 140599226058560 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_1.
I0214 02:59:19.756786 140599226058560 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_1/hparams.json.
I0214 02:59:19.941886 140599226058560 submission_runner.py:206] Initializing dataset.
I0214 02:59:19.942126 140599226058560 submission_runner.py:213] Initializing model.
I0214 02:59:24.821128 140599226058560 submission_runner.py:255] Initializing optimizer.
I0214 02:59:26.069061 140599226058560 submission_runner.py:262] Initializing metrics bundle.
I0214 02:59:26.069261 140599226058560 submission_runner.py:280] Initializing checkpoint and logger.
I0214 02:59:26.070638 140599226058560 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_1 with prefix checkpoint_
I0214 02:59:26.070800 140599226058560 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_1/meta_data_0.json.
I0214 02:59:26.071014 140599226058560 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 02:59:26.071081 140599226058560 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 02:59:26.485684 140599226058560 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 02:59:26.862598 140599226058560 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_1/flags_0.json.
I0214 02:59:26.877158 140599226058560 submission_runner.py:314] Starting training loop.
I0214 02:59:27.182476 140599226058560 input_pipeline.py:20] Loading split = train-clean-100
I0214 02:59:27.221549 140599226058560 input_pipeline.py:20] Loading split = train-clean-360
I0214 02:59:27.641621 140599226058560 input_pipeline.py:20] Loading split = train-other-500
2024-02-14 03:00:37.726633: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2024-02-14 03:00:40.506108: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 03:00:42.452142 140426329712384 logging_writer.py:48] [0] global_step=0, grad_norm=33.21240997314453, loss=32.96635437011719
I0214 03:00:42.488899 140599226058560 spec.py:321] Evaluating on the training split.
I0214 03:00:42.658587 140599226058560 input_pipeline.py:20] Loading split = train-clean-100
I0214 03:00:42.693959 140599226058560 input_pipeline.py:20] Loading split = train-clean-360
I0214 03:00:43.093156 140599226058560 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0214 03:02:02.190746 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 03:02:02.308028 140599226058560 input_pipeline.py:20] Loading split = dev-clean
I0214 03:02:02.313761 140599226058560 input_pipeline.py:20] Loading split = dev-other
I0214 03:03:04.047934 140599226058560 spec.py:349] Evaluating on the test split.
I0214 03:03:04.166757 140599226058560 input_pipeline.py:20] Loading split = test-clean
I0214 03:03:40.718317 140599226058560 submission_runner.py:408] Time since start: 253.84s, 	Step: 1, 	{'train/ctc_loss': Array(32.12085, dtype=float32), 'train/wer': 1.3685577743997668, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 75.61166572570801, 'total_duration': 253.83884978294373, 'accumulated_submission_time': 75.61166572570801, 'accumulated_eval_time': 178.22711658477783, 'accumulated_logging_time': 0}
I0214 03:03:40.744585 140420491241216 logging_writer.py:48] [1] accumulated_eval_time=178.227117, accumulated_logging_time=0, accumulated_submission_time=75.611666, global_step=1, preemption_count=0, score=75.611666, test/ctc_loss=31.23744010925293, test/num_examples=2472, test/wer=1.102350, total_duration=253.838850, train/ctc_loss=32.120849609375, train/wer=1.368558, validation/ctc_loss=31.08704948425293, validation/num_examples=5348, validation/wer=1.058565
I0214 03:05:18.723377 140427648493312 logging_writer.py:48] [100] global_step=100, grad_norm=9.983563423156738, loss=12.461341857910156
I0214 03:06:34.476038 140427656886016 logging_writer.py:48] [200] global_step=200, grad_norm=7.734189033508301, loss=8.108963966369629
I0214 03:07:50.273565 140427648493312 logging_writer.py:48] [300] global_step=300, grad_norm=1.9566586017608643, loss=5.884060859680176
I0214 03:09:06.013319 140427656886016 logging_writer.py:48] [400] global_step=400, grad_norm=0.29759401082992554, loss=5.817328929901123
I0214 03:10:21.749443 140427648493312 logging_writer.py:48] [500] global_step=500, grad_norm=0.25465887784957886, loss=5.805249214172363
I0214 03:11:37.552198 140427656886016 logging_writer.py:48] [600] global_step=600, grad_norm=0.516717255115509, loss=5.832650661468506
I0214 03:12:53.282351 140427648493312 logging_writer.py:48] [700] global_step=700, grad_norm=0.25856465101242065, loss=5.816504955291748
I0214 03:14:09.376760 140427656886016 logging_writer.py:48] [800] global_step=800, grad_norm=0.36578133702278137, loss=5.794175624847412
I0214 03:15:30.320910 140427648493312 logging_writer.py:48] [900] global_step=900, grad_norm=0.31033700704574585, loss=5.786650657653809
I0214 03:16:51.797009 140427656886016 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3963676989078522, loss=5.784908771514893
I0214 03:18:12.106072 140428404565760 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.3842294216156006, loss=5.79263162612915
I0214 03:19:27.838932 140428396173056 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.24342109262943268, loss=5.788358688354492
I0214 03:20:43.718921 140428404565760 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.3235010802745819, loss=5.773707389831543
I0214 03:21:59.518504 140428396173056 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.922287106513977, loss=5.783596992492676
I0214 03:23:15.393607 140428404565760 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2964918315410614, loss=5.790072917938232
I0214 03:24:31.121469 140428396173056 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.42886877059936523, loss=5.7762556076049805
I0214 03:25:46.964454 140428404565760 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5882627367973328, loss=5.740624904632568
I0214 03:27:06.714439 140428396173056 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9408767819404602, loss=5.725736618041992
I0214 03:27:41.145995 140599226058560 spec.py:321] Evaluating on the training split.
I0214 03:28:18.813696 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 03:29:04.016794 140599226058560 spec.py:349] Evaluating on the test split.
I0214 03:29:26.792435 140599226058560 submission_runner.py:408] Time since start: 1799.91s, 	Step: 1844, 	{'train/ctc_loss': Array(6.1664042, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.2940016, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.2753453, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1515.929588317871, 'total_duration': 1799.9081389904022, 'accumulated_submission_time': 1515.929588317871, 'accumulated_eval_time': 283.8664665222168, 'accumulated_logging_time': 0.040500640869140625}
I0214 03:29:26.828019 140428404565760 logging_writer.py:48] [1844] accumulated_eval_time=283.866467, accumulated_logging_time=0.040501, accumulated_submission_time=1515.929588, global_step=1844, preemption_count=0, score=1515.929588, test/ctc_loss=6.275345325469971, test/num_examples=2472, test/wer=0.899580, total_duration=1799.908139, train/ctc_loss=6.1664042472839355, train/wer=0.944636, validation/ctc_loss=6.294001579284668, validation/num_examples=5348, validation/wer=0.896618
I0214 03:30:09.951452 140428396173056 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4200288653373718, loss=5.5648908615112305
I0214 03:31:25.680243 140428404565760 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.4083840548992157, loss=5.499200344085693
I0214 03:32:45.344466 140427749205760 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1358263492584229, loss=5.445907115936279
I0214 03:34:01.000633 140427740813056 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7055021524429321, loss=5.19973087310791
I0214 03:35:16.754860 140427749205760 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8540747761726379, loss=4.6469035148620605
I0214 03:36:32.702824 140427740813056 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8894240856170654, loss=4.130167007446289
I0214 03:37:48.445195 140427749205760 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.0272407531738281, loss=3.777484893798828
I0214 03:39:04.188009 140427740813056 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9011462330818176, loss=3.621957540512085
I0214 03:40:21.554055 140427749205760 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.2010910511016846, loss=3.4155712127685547
I0214 03:41:43.970853 140427740813056 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.0838958024978638, loss=3.271115303039551
I0214 03:43:06.459635 140427749205760 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.473423957824707, loss=3.1744847297668457
I0214 03:44:29.029865 140427740813056 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4715248346328735, loss=3.137115240097046
I0214 03:45:52.935639 140428404565760 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8342764973640442, loss=2.987835168838501
I0214 03:47:08.295560 140428396173056 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.3438547849655151, loss=2.9279298782348633
I0214 03:48:23.742149 140428404565760 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.6347707509994507, loss=2.8715996742248535
I0214 03:49:39.375568 140428396173056 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9641250967979431, loss=2.786856174468994
I0214 03:50:54.968841 140428404565760 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0264078378677368, loss=2.7086474895477295
I0214 03:52:10.463386 140428396173056 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8189948797225952, loss=2.7102041244506836
I0214 03:53:27.956159 140428404565760 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.720758318901062, loss=2.652895450592041
I0214 03:53:27.964857 140599226058560 spec.py:321] Evaluating on the training split.
I0214 03:54:21.402344 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 03:55:11.993412 140599226058560 spec.py:349] Evaluating on the test split.
I0214 03:55:37.355163 140599226058560 submission_runner.py:408] Time since start: 3370.47s, 	Step: 3701, 	{'train/ctc_loss': Array(2.3516572, dtype=float32), 'train/wer': 0.5535412726740004, 'validation/ctc_loss': Array(2.7404253, dtype=float32), 'validation/wer': 0.5721250856850459, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.4001753, dtype=float32), 'test/wer': 0.5354335506672354, 'test/num_examples': 2472, 'score': 2956.9796600341797, 'total_duration': 3370.4715399742126, 'accumulated_submission_time': 2956.9796600341797, 'accumulated_eval_time': 413.2503607273102, 'accumulated_logging_time': 0.09172964096069336}
I0214 03:55:37.392544 140428404565760 logging_writer.py:48] [3701] accumulated_eval_time=413.250361, accumulated_logging_time=0.091730, accumulated_submission_time=2956.979660, global_step=3701, preemption_count=0, score=2956.979660, test/ctc_loss=2.4001753330230713, test/num_examples=2472, test/wer=0.535434, total_duration=3370.471540, train/ctc_loss=2.3516571521759033, train/wer=0.553541, validation/ctc_loss=2.7404253482818604, validation/num_examples=5348, validation/wer=0.572125
I0214 03:56:52.724945 140428396173056 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8361737728118896, loss=2.5447239875793457
I0214 03:58:08.370304 140428404565760 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7557063102722168, loss=2.458972930908203
I0214 03:59:24.080456 140428396173056 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.88457190990448, loss=2.4472737312316895
I0214 04:00:39.726779 140428404565760 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8999870419502258, loss=2.485592842102051
I0214 04:01:58.807957 140427749205760 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7802202105522156, loss=2.3884687423706055
I0214 04:03:14.361117 140427740813056 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.013960361480713, loss=2.317294120788574
I0214 04:04:29.889037 140427749205760 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8832537531852722, loss=2.273336887359619
I0214 04:05:45.527436 140427740813056 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.3582931756973267, loss=2.264312744140625
I0214 04:07:00.860291 140427749205760 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9533033967018127, loss=2.189257860183716
I0214 04:08:19.472971 140427740813056 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.0467692613601685, loss=2.1984364986419678
I0214 04:09:41.969884 140427749205760 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9151458144187927, loss=2.1454081535339355
I0214 04:11:04.836557 140427740813056 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8842661380767822, loss=2.175931215286255
I0214 04:12:27.604102 140427749205760 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.0184905529022217, loss=2.093482494354248
I0214 04:13:51.195444 140427740813056 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7641158103942871, loss=2.0912015438079834
I0214 04:15:13.584576 140428404565760 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8831274509429932, loss=2.0199742317199707
I0214 04:16:29.052719 140428396173056 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7891043424606323, loss=2.024872064590454
I0214 04:17:44.528933 140428404565760 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.0291661024093628, loss=2.003185272216797
I0214 04:19:00.060404 140428396173056 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7799565196037292, loss=1.9532470703125
I0214 04:19:37.801722 140599226058560 spec.py:321] Evaluating on the training split.
I0214 04:20:32.518610 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 04:21:23.542532 140599226058560 spec.py:349] Evaluating on the test split.
I0214 04:21:49.261065 140599226058560 submission_runner.py:408] Time since start: 4942.38s, 	Step: 5551, 	{'train/ctc_loss': Array(0.7146427, dtype=float32), 'train/wer': 0.2437602458476314, 'validation/ctc_loss': Array(1.0680373, dtype=float32), 'validation/wer': 0.31081224596194135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.78583676, dtype=float32), 'test/wer': 0.25501188227408444, 'test/num_examples': 2472, 'score': 4397.298481225967, 'total_duration': 4942.37772154808, 'accumulated_submission_time': 4397.298481225967, 'accumulated_eval_time': 544.7035682201385, 'accumulated_logging_time': 0.14594745635986328}
I0214 04:21:49.295353 140427820885760 logging_writer.py:48] [5551] accumulated_eval_time=544.703568, accumulated_logging_time=0.145947, accumulated_submission_time=4397.298481, global_step=5551, preemption_count=0, score=4397.298481, test/ctc_loss=0.7858367562294006, test/num_examples=2472, test/wer=0.255012, total_duration=4942.377722, train/ctc_loss=0.7146427035331726, train/wer=0.243760, validation/ctc_loss=1.0680372714996338, validation/num_examples=5348, validation/wer=0.310812
I0214 04:22:26.797391 140427812493056 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9299502968788147, loss=1.950082778930664
I0214 04:23:42.058664 140427820885760 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6646159291267395, loss=1.9642781019210815
I0214 04:24:57.378108 140427812493056 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6984373927116394, loss=1.9510616064071655
I0214 04:26:12.725804 140427820885760 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8121982216835022, loss=1.965237021446228
I0214 04:27:31.626446 140427812493056 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8305241465568542, loss=1.9033912420272827
I0214 04:28:55.352970 140427820885760 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.720172107219696, loss=1.896979808807373
I0214 04:30:19.842698 140427820885760 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7017037868499756, loss=1.8482484817504883
I0214 04:31:35.140606 140427812493056 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8703514933586121, loss=1.8341068029403687
I0214 04:32:50.526017 140427820885760 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.6815410852432251, loss=1.8296561241149902
I0214 04:34:05.932144 140427812493056 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7608620524406433, loss=1.8802597522735596
I0214 04:35:21.567223 140427820885760 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7309241890907288, loss=1.7879441976547241
I0214 04:36:36.950481 140427812493056 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8631555438041687, loss=1.8960543870925903
I0214 04:37:59.779998 140427820885760 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8242068886756897, loss=1.773305058479309
I0214 04:39:24.362274 140427812493056 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7485937476158142, loss=1.8199830055236816
I0214 04:40:47.638191 140427820885760 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7458122372627258, loss=1.8089202642440796
I0214 04:42:10.425623 140427812493056 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6582963466644287, loss=1.834114909172058
I0214 04:43:33.661186 140427820885760 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7459347248077393, loss=1.8010923862457275
I0214 04:44:53.072477 140427820885760 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6691693663597107, loss=1.745121955871582
I0214 04:45:49.993278 140599226058560 spec.py:321] Evaluating on the training split.
I0214 04:46:44.881799 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 04:47:35.674509 140599226058560 spec.py:349] Evaluating on the test split.
I0214 04:48:01.509665 140599226058560 submission_runner.py:408] Time since start: 6514.63s, 	Step: 7377, 	{'train/ctc_loss': Array(0.5079412, dtype=float32), 'train/wer': 0.17687377603703044, 'validation/ctc_loss': Array(0.821356, dtype=float32), 'validation/wer': 0.24684051478610117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5604038, dtype=float32), 'test/wer': 0.18680559787134646, 'test/num_examples': 2472, 'score': 5837.907940626144, 'total_duration': 6514.627334594727, 'accumulated_submission_time': 5837.907940626144, 'accumulated_eval_time': 676.2148461341858, 'accumulated_logging_time': 0.19725966453552246}
I0214 04:48:01.545499 140427820885760 logging_writer.py:48] [7377] accumulated_eval_time=676.214846, accumulated_logging_time=0.197260, accumulated_submission_time=5837.907941, global_step=7377, preemption_count=0, score=5837.907941, test/ctc_loss=0.5604038238525391, test/num_examples=2472, test/wer=0.186806, total_duration=6514.627335, train/ctc_loss=0.5079411864280701, train/wer=0.176874, validation/ctc_loss=0.8213559985160828, validation/num_examples=5348, validation/wer=0.246841
I0214 04:48:19.610199 140427812493056 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.6956133842468262, loss=1.712463140487671
I0214 04:49:34.746498 140427820885760 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8203646540641785, loss=1.8332254886627197
I0214 04:50:50.032237 140427812493056 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8061602115631104, loss=1.7414793968200684
I0214 04:52:05.625170 140427820885760 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6952457427978516, loss=1.7902657985687256
I0214 04:53:20.971452 140427812493056 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8580008149147034, loss=1.7359397411346436
I0214 04:54:37.327400 140427820885760 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6678094267845154, loss=1.7229782342910767
I0214 04:55:59.797038 140427812493056 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6264748573303223, loss=1.7487690448760986
I0214 04:57:22.810077 140427820885760 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6789434552192688, loss=1.685041069984436
I0214 04:58:46.574721 140427812493056 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7328283190727234, loss=1.724578619003296
I0214 05:00:08.417675 140427278165760 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6793127655982971, loss=1.705411434173584
I0214 05:01:23.634406 140427269773056 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7050513029098511, loss=1.6118148565292358
I0214 05:02:39.051685 140427278165760 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7078624367713928, loss=1.681689739227295
I0214 05:03:54.418959 140427269773056 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7337898015975952, loss=1.684072732925415
I0214 05:05:09.791014 140427278165760 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.7302640080451965, loss=1.707736611366272
I0214 05:06:30.361618 140427269773056 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6759493947029114, loss=1.663685917854309
I0214 05:07:53.522127 140427278165760 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6922748684883118, loss=1.5920509099960327
I0214 05:09:16.345408 140427269773056 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8077222108840942, loss=1.5863842964172363
I0214 05:10:40.691954 140427278165760 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6442899703979492, loss=1.6644490957260132
I0214 05:12:01.687232 140599226058560 spec.py:321] Evaluating on the training split.
I0214 05:12:55.961415 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 05:13:47.497868 140599226058560 spec.py:349] Evaluating on the test split.
I0214 05:14:13.576360 140599226058560 submission_runner.py:408] Time since start: 8086.69s, 	Step: 9199, 	{'train/ctc_loss': Array(0.43121392, dtype=float32), 'train/wer': 0.1545967513970732, 'validation/ctc_loss': Array(0.7380511, dtype=float32), 'validation/wer': 0.22283904727883602, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48268652, dtype=float32), 'test/wer': 0.16399569394511812, 'test/num_examples': 2472, 'score': 7277.963893651962, 'total_duration': 8086.693351507187, 'accumulated_submission_time': 7277.963893651962, 'accumulated_eval_time': 808.0982096195221, 'accumulated_logging_time': 0.24878954887390137}
I0214 05:14:13.611297 140427493205760 logging_writer.py:48] [9199] accumulated_eval_time=808.098210, accumulated_logging_time=0.248790, accumulated_submission_time=7277.963894, global_step=9199, preemption_count=0, score=7277.963894, test/ctc_loss=0.48268651962280273, test/num_examples=2472, test/wer=0.163996, total_duration=8086.693352, train/ctc_loss=0.431213915348053, train/wer=0.154597, validation/ctc_loss=0.7380511164665222, validation/num_examples=5348, validation/wer=0.222839
I0214 05:14:15.222774 140427484813056 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8295561671257019, loss=1.7076337337493896
I0214 05:15:33.919414 140427820885760 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.8451241850852966, loss=1.6367101669311523
I0214 05:16:49.187539 140427812493056 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.818891704082489, loss=1.6348353624343872
I0214 05:18:04.421671 140427820885760 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5944734811782837, loss=1.627393364906311
I0214 05:19:19.771029 140427812493056 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7626677751541138, loss=1.5779589414596558
I0214 05:20:34.942746 140427820885760 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6684412360191345, loss=1.6348350048065186
I0214 05:21:54.094261 140427812493056 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.7396520972251892, loss=1.6581230163574219
I0214 05:23:17.241371 140427820885760 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.6855018734931946, loss=1.5996308326721191
I0214 05:24:41.032073 140427812493056 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6798449754714966, loss=1.6368025541305542
I0214 05:26:04.317069 140427820885760 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.6879978775978088, loss=1.6687780618667603
I0214 05:27:27.701381 140427812493056 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6695102453231812, loss=1.5618878602981567
I0214 05:28:53.841135 140427165525760 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6221006512641907, loss=1.5982608795166016
I0214 05:30:08.992463 140427157133056 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.6974484324455261, loss=1.566389560699463
I0214 05:31:24.367303 140427165525760 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7093265056610107, loss=1.6075361967086792
I0214 05:32:39.730261 140427157133056 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.6036011576652527, loss=1.6465988159179688
I0214 05:33:55.119658 140427165525760 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5906873941421509, loss=1.519909143447876
I0214 05:35:12.082211 140427157133056 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.6714099049568176, loss=1.5481185913085938
I0214 05:36:35.781936 140427165525760 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.6871069669723511, loss=1.5724611282348633
I0214 05:37:59.123384 140427157133056 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7572152614593506, loss=1.582224726676941
I0214 05:38:14.149012 140599226058560 spec.py:321] Evaluating on the training split.
I0214 05:39:09.000959 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 05:40:00.910757 140599226058560 spec.py:349] Evaluating on the test split.
I0214 05:40:27.075229 140599226058560 submission_runner.py:408] Time since start: 9660.19s, 	Step: 11020, 	{'train/ctc_loss': Array(0.41369617, dtype=float32), 'train/wer': 0.14623021482947543, 'validation/ctc_loss': Array(0.6845674, dtype=float32), 'validation/wer': 0.20788398968883054, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4419891, dtype=float32), 'test/wer': 0.15097597140129587, 'test/num_examples': 2472, 'score': 8718.414444446564, 'total_duration': 9660.191576719284, 'accumulated_submission_time': 8718.414444446564, 'accumulated_eval_time': 941.0179760456085, 'accumulated_logging_time': 0.3003580570220947}
I0214 05:40:27.113079 140427390805760 logging_writer.py:48] [11020] accumulated_eval_time=941.017976, accumulated_logging_time=0.300358, accumulated_submission_time=8718.414444, global_step=11020, preemption_count=0, score=8718.414444, test/ctc_loss=0.44198909401893616, test/num_examples=2472, test/wer=0.150976, total_duration=9660.191577, train/ctc_loss=0.41369616985321045, train/wer=0.146230, validation/ctc_loss=0.684567391872406, validation/num_examples=5348, validation/wer=0.207884
I0214 05:41:27.834619 140427382413056 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.6422040462493896, loss=1.6013823747634888
I0214 05:42:43.005438 140427390805760 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6371093392372131, loss=1.5651813745498657
I0214 05:43:58.342026 140427382413056 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.660458505153656, loss=1.5361692905426025
I0214 05:45:17.258225 140427390805760 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7282193303108215, loss=1.5529603958129883
I0214 05:46:32.634806 140427382413056 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5889009833335876, loss=1.5798535346984863
I0214 05:47:47.898841 140427390805760 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5939101576805115, loss=1.5661535263061523
I0214 05:49:03.153604 140427382413056 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7498772740364075, loss=1.5393685102462769
I0214 05:50:19.336219 140427390805760 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6446552276611328, loss=1.562238097190857
I0214 05:51:43.586487 140427382413056 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.621677815914154, loss=1.5373588800430298
I0214 05:53:07.524390 140427390805760 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.710122287273407, loss=1.5210325717926025
I0214 05:54:30.910130 140427382413056 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.7013983130455017, loss=1.5445712804794312
I0214 05:55:55.038283 140427390805760 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6694418787956238, loss=1.5935884714126587
I0214 05:57:19.440101 140427382413056 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7422582507133484, loss=1.5080920457839966
I0214 05:58:43.092156 140427390805760 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.670685887336731, loss=1.5015606880187988
I0214 05:59:58.324954 140427382413056 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6323076486587524, loss=1.4774478673934937
I0214 06:01:13.810895 140427390805760 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6259533166885376, loss=1.5592904090881348
I0214 06:02:29.127322 140427382413056 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6390592455863953, loss=1.5376062393188477
I0214 06:03:44.541436 140427390805760 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7827937602996826, loss=1.505467414855957
I0214 06:04:28.301108 140599226058560 spec.py:321] Evaluating on the training split.
I0214 06:05:23.865353 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 06:06:15.766171 140599226058560 spec.py:349] Evaluating on the test split.
I0214 06:06:42.589711 140599226058560 submission_runner.py:408] Time since start: 11235.71s, 	Step: 12857, 	{'train/ctc_loss': Array(0.35117, dtype=float32), 'train/wer': 0.12689043456065344, 'validation/ctc_loss': Array(0.6366001, dtype=float32), 'validation/wer': 0.19441574867007155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.402794, dtype=float32), 'test/wer': 0.1361485182702659, 'test/num_examples': 2472, 'score': 10159.513425588608, 'total_duration': 11235.706638336182, 'accumulated_submission_time': 10159.513425588608, 'accumulated_eval_time': 1075.3007299900055, 'accumulated_logging_time': 0.35523295402526855}
I0214 06:06:42.626477 140428404565760 logging_writer.py:48] [12857] accumulated_eval_time=1075.300730, accumulated_logging_time=0.355233, accumulated_submission_time=10159.513426, global_step=12857, preemption_count=0, score=10159.513426, test/ctc_loss=0.4027940034866333, test/num_examples=2472, test/wer=0.136149, total_duration=11235.706638, train/ctc_loss=0.35117000341415405, train/wer=0.126890, validation/ctc_loss=0.6366000771522522, validation/num_examples=5348, validation/wer=0.194416
I0214 06:07:15.699860 140428396173056 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6438838839530945, loss=1.512147068977356
I0214 06:08:31.235706 140428404565760 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6357643604278564, loss=1.4404951333999634
I0214 06:09:46.615516 140428396173056 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5355638861656189, loss=1.4979257583618164
I0214 06:11:02.044342 140428404565760 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5850211977958679, loss=1.5138847827911377
I0214 06:12:23.130469 140428396173056 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6526304483413696, loss=1.4944075345993042
I0214 06:13:50.360353 140428404565760 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.594876766204834, loss=1.4523978233337402
I0214 06:15:05.610823 140428396173056 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5754111409187317, loss=1.4740707874298096
I0214 06:16:20.963369 140428404565760 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.570508599281311, loss=1.5028959512710571
I0214 06:17:36.318690 140428396173056 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6734678149223328, loss=1.471725583076477
I0214 06:18:51.714404 140428404565760 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.7320103049278259, loss=1.433316946029663
I0214 06:20:12.855802 140428396173056 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6773921847343445, loss=1.437709927558899
I0214 06:21:38.023732 140428404565760 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7606739401817322, loss=1.538360834121704
I0214 06:23:02.817697 140428396173056 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.7166123986244202, loss=1.4191933870315552
I0214 06:24:27.633340 140428404565760 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.6110212206840515, loss=1.5002799034118652
I0214 06:25:51.823045 140428396173056 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7094728350639343, loss=1.4870097637176514
I0214 06:27:16.146745 140428404565760 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5934903025627136, loss=1.474098801612854
I0214 06:28:36.242135 140427646805760 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7113026976585388, loss=1.4621658325195312
I0214 06:29:51.530134 140427638413056 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5932393074035645, loss=1.488612413406372
I0214 06:30:43.050502 140599226058560 spec.py:321] Evaluating on the training split.
I0214 06:31:38.366338 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 06:32:30.589406 140599226058560 spec.py:349] Evaluating on the test split.
I0214 06:32:56.789038 140599226058560 submission_runner.py:408] Time since start: 12809.91s, 	Step: 14670, 	{'train/ctc_loss': Array(0.301788, dtype=float32), 'train/wer': 0.11338828362671936, 'validation/ctc_loss': Array(0.6053062, dtype=float32), 'validation/wer': 0.1848866060998098, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38013542, dtype=float32), 'test/wer': 0.12979099384559137, 'test/num_examples': 2472, 'score': 11599.84973692894, 'total_duration': 12809.905858516693, 'accumulated_submission_time': 11599.84973692894, 'accumulated_eval_time': 1209.0333228111267, 'accumulated_logging_time': 0.40857839584350586}
I0214 06:32:56.826560 140428076885760 logging_writer.py:48] [14670] accumulated_eval_time=1209.033323, accumulated_logging_time=0.408578, accumulated_submission_time=11599.849737, global_step=14670, preemption_count=0, score=11599.849737, test/ctc_loss=0.3801354169845581, test/num_examples=2472, test/wer=0.129791, total_duration=12809.905859, train/ctc_loss=0.30178800225257874, train/wer=0.113388, validation/ctc_loss=0.6053062081336975, validation/num_examples=5348, validation/wer=0.184887
I0214 06:33:20.151298 140428068493056 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.632525622844696, loss=1.5651873350143433
I0214 06:34:35.283476 140428076885760 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.5483956336975098, loss=1.447760820388794
I0214 06:35:50.537674 140428068493056 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.5529601573944092, loss=1.492180585861206
I0214 06:37:05.951699 140428076885760 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6049938201904297, loss=1.4435253143310547
I0214 06:38:26.885374 140428068493056 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7302179932594299, loss=1.453908920288086
I0214 06:39:51.250208 140428076885760 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.7837010025978088, loss=1.4712810516357422
I0214 06:41:16.206723 140428068493056 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8188775181770325, loss=1.515104055404663
I0214 06:42:40.907263 140428076885760 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.7658781409263611, loss=1.4438281059265137
I0214 06:44:04.212474 140427749205760 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.551400363445282, loss=1.4538570642471313
I0214 06:45:19.469060 140427740813056 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.6075353026390076, loss=1.4034498929977417
I0214 06:46:34.866509 140427749205760 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5607479810714722, loss=1.4672905206680298
I0214 06:47:50.260160 140427740813056 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6641682982444763, loss=1.4414242506027222
I0214 06:49:06.074085 140427749205760 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.557073712348938, loss=1.4714767932891846
I0214 06:50:30.562732 140427740813056 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.720576822757721, loss=1.3964935541152954
I0214 06:51:55.198523 140427749205760 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.912545919418335, loss=1.4647804498672485
I0214 06:53:20.830628 140427740813056 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6671244502067566, loss=1.450330138206482
I0214 06:54:45.376802 140427749205760 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6570681929588318, loss=1.4211921691894531
I0214 06:56:10.055224 140427740813056 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.6403898000717163, loss=1.4748539924621582
I0214 06:56:56.822624 140599226058560 spec.py:321] Evaluating on the training split.
I0214 06:57:51.906096 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 06:58:43.943992 140599226058560 spec.py:349] Evaluating on the test split.
I0214 06:59:10.373386 140599226058560 submission_runner.py:408] Time since start: 14383.49s, 	Step: 16457, 	{'train/ctc_loss': Array(0.27452624, dtype=float32), 'train/wer': 0.10105921721134868, 'validation/ctc_loss': Array(0.5743748, dtype=float32), 'validation/wer': 0.1744981994072043, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36317125, dtype=float32), 'test/wer': 0.12386001259317937, 'test/num_examples': 2472, 'score': 13039.76072025299, 'total_duration': 14383.490197658539, 'accumulated_submission_time': 13039.76072025299, 'accumulated_eval_time': 1342.5781121253967, 'accumulated_logging_time': 0.46301817893981934}
I0214 06:59:10.408170 140427457365760 logging_writer.py:48] [16457] accumulated_eval_time=1342.578112, accumulated_logging_time=0.463018, accumulated_submission_time=13039.760720, global_step=16457, preemption_count=0, score=13039.760720, test/ctc_loss=0.363171249628067, test/num_examples=2472, test/wer=0.123860, total_duration=14383.490198, train/ctc_loss=0.2745262384414673, train/wer=0.101059, validation/ctc_loss=0.574374794960022, validation/num_examples=5348, validation/wer=0.174498
I0214 06:59:47.189980 140427129685760 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6326727867126465, loss=1.4486238956451416
I0214 07:01:02.338335 140427121293056 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6876586079597473, loss=1.4263575077056885
I0214 07:02:17.647664 140427129685760 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.6223418712615967, loss=1.4167513847351074
I0214 07:03:32.943715 140427121293056 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.5797486901283264, loss=1.4565861225128174
I0214 07:04:50.285818 140427129685760 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.614277184009552, loss=1.4550880193710327
I0214 07:06:13.595078 140427121293056 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5806686878204346, loss=1.442112922668457
I0214 07:07:36.928394 140427129685760 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.6247732639312744, loss=1.4651570320129395
I0214 07:09:02.496704 140427121293056 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.8508631587028503, loss=1.4161229133605957
I0214 07:10:27.331727 140427129685760 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.6947524547576904, loss=1.436086893081665
I0214 07:11:51.667562 140427121293056 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7108651399612427, loss=1.4771631956100464
I0214 07:13:16.183269 140427129685760 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.606582760810852, loss=1.3951889276504517
I0214 07:14:35.460827 140427129685760 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7081082463264465, loss=1.4430336952209473
I0214 07:15:50.686918 140427121293056 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7256587147712708, loss=1.3916188478469849
I0214 07:17:05.902913 140427129685760 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.5978139042854309, loss=1.4188222885131836
I0214 07:18:21.100765 140427121293056 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6427026391029358, loss=1.3858736753463745
I0214 07:19:39.966379 140427129685760 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6379757523536682, loss=1.436378002166748
I0214 07:21:04.616340 140427121293056 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0619139671325684, loss=1.4013630151748657
I0214 07:22:29.193122 140427129685760 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.6591748595237732, loss=1.4360861778259277
I0214 07:23:10.801407 140599226058560 spec.py:321] Evaluating on the training split.
I0214 07:24:05.596623 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 07:24:57.403424 140599226058560 spec.py:349] Evaluating on the test split.
I0214 07:25:23.542372 140599226058560 submission_runner.py:408] Time since start: 15956.66s, 	Step: 18251, 	{'train/ctc_loss': Array(0.2666409, dtype=float32), 'train/wer': 0.10048963087302311, 'validation/ctc_loss': Array(0.5565517, dtype=float32), 'validation/wer': 0.16945847050986224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34133852, dtype=float32), 'test/wer': 0.11551195336461316, 'test/num_examples': 2472, 'score': 14480.069234132767, 'total_duration': 15956.655859947205, 'accumulated_submission_time': 14480.069234132767, 'accumulated_eval_time': 1475.30983710289, 'accumulated_logging_time': 0.512258768081665}
I0214 07:25:23.577847 140427165525760 logging_writer.py:48] [18251] accumulated_eval_time=1475.309837, accumulated_logging_time=0.512259, accumulated_submission_time=14480.069234, global_step=18251, preemption_count=0, score=14480.069234, test/ctc_loss=0.34133851528167725, test/num_examples=2472, test/wer=0.115512, total_duration=15956.655860, train/ctc_loss=0.26664090156555176, train/wer=0.100490, validation/ctc_loss=0.5565516948699951, validation/num_examples=5348, validation/wer=0.169458
I0214 07:26:01.490226 140427157133056 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6562219858169556, loss=1.406158447265625
I0214 07:27:16.814055 140427165525760 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6367197036743164, loss=1.3642520904541016
I0214 07:28:32.223905 140427157133056 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6101210117340088, loss=1.3918335437774658
I0214 07:29:51.111394 140427165525760 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5879678130149841, loss=1.3250277042388916
I0214 07:31:06.450714 140427157133056 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.8201500177383423, loss=1.4441663026809692
I0214 07:32:21.840234 140427165525760 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7468787431716919, loss=1.4184170961380005
I0214 07:33:37.234640 140427157133056 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.6619234681129456, loss=1.4025211334228516
I0214 07:34:56.214730 140427165525760 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6455687284469604, loss=1.3498713970184326
I0214 07:36:20.081166 140427157133056 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.8041406869888306, loss=1.4558515548706055
I0214 07:37:45.033824 140427165525760 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5792575478553772, loss=1.3684982061386108
I0214 07:39:10.279077 140427157133056 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.6585829854011536, loss=1.3753392696380615
I0214 07:40:35.082626 140427165525760 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6882642507553101, loss=1.4317067861557007
I0214 07:41:59.936255 140427157133056 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.592779815196991, loss=1.3891972303390503
I0214 07:43:25.773441 140427165525760 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.691565215587616, loss=1.400789499282837
I0214 07:44:40.925694 140427157133056 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6495363116264343, loss=1.4284775257110596
I0214 07:45:56.147150 140427165525760 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.5730544328689575, loss=1.4248095750808716
I0214 07:47:11.509916 140427157133056 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6195109486579895, loss=1.4127410650253296
I0214 07:48:26.793755 140427165525760 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7596438527107239, loss=1.4152641296386719
I0214 07:49:24.060867 140599226058560 spec.py:321] Evaluating on the training split.
I0214 07:50:18.978484 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 07:51:10.730421 140599226058560 spec.py:349] Evaluating on the test split.
I0214 07:51:37.147533 140599226058560 submission_runner.py:408] Time since start: 17530.26s, 	Step: 20072, 	{'train/ctc_loss': Array(0.27632526, dtype=float32), 'train/wer': 0.09994447235516539, 'validation/ctc_loss': Array(0.53343123, dtype=float32), 'validation/wer': 0.1639939368778783, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3296891, dtype=float32), 'test/wer': 0.11335892592366908, 'test/num_examples': 2472, 'score': 15920.46349310875, 'total_duration': 17530.263286590576, 'accumulated_submission_time': 15920.46349310875, 'accumulated_eval_time': 1608.389491558075, 'accumulated_logging_time': 0.5656495094299316}
I0214 07:51:37.182771 140428404565760 logging_writer.py:48] [20072] accumulated_eval_time=1608.389492, accumulated_logging_time=0.565650, accumulated_submission_time=15920.463493, global_step=20072, preemption_count=0, score=15920.463493, test/ctc_loss=0.329689085483551, test/num_examples=2472, test/wer=0.113359, total_duration=17530.263287, train/ctc_loss=0.2763252556324005, train/wer=0.099944, validation/ctc_loss=0.5334312319755554, validation/num_examples=5348, validation/wer=0.163994
I0214 07:51:59.024454 140428396173056 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.664605975151062, loss=1.3915294408798218
I0214 07:53:14.206336 140428404565760 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7009855508804321, loss=1.3953317403793335
I0214 07:54:29.560459 140428396173056 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.6099349856376648, loss=1.323925256729126
I0214 07:55:46.141252 140428404565760 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7171242237091064, loss=1.427873134613037
I0214 07:57:10.851425 140428396173056 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7807871699333191, loss=1.3933072090148926
I0214 07:58:39.091459 140428404565760 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6033561825752258, loss=1.3406040668487549
I0214 07:59:54.354092 140428396173056 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6319208145141602, loss=1.3116005659103394
I0214 08:01:09.666477 140428404565760 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5528213977813721, loss=1.403934121131897
I0214 08:02:25.072743 140428396173056 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6639984250068665, loss=1.3912312984466553
I0214 08:03:40.352222 140428404565760 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6082504987716675, loss=1.3852171897888184
I0214 08:05:01.713686 140428396173056 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7216615080833435, loss=1.377465844154358
I0214 08:06:27.307959 140428404565760 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6404696106910706, loss=1.4024875164031982
I0214 08:07:52.570297 140428396173056 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.6771078705787659, loss=1.3648769855499268
I0214 08:09:17.761449 140428404565760 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.652168869972229, loss=1.4008080959320068
I0214 08:10:42.864927 140428396173056 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5753554701805115, loss=1.3872008323669434
I0214 08:12:08.434028 140428404565760 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6972288489341736, loss=1.361946940422058
I0214 08:13:29.875436 140428404565760 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7088284492492676, loss=1.3779174089431763
I0214 08:14:45.106123 140428396173056 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7367233037948608, loss=1.3962080478668213
I0214 08:15:37.418499 140599226058560 spec.py:321] Evaluating on the training split.
I0214 08:16:32.660782 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 08:17:23.966655 140599226058560 spec.py:349] Evaluating on the test split.
I0214 08:17:50.199862 140599226058560 submission_runner.py:408] Time since start: 19103.32s, 	Step: 21871, 	{'train/ctc_loss': Array(0.2690903, dtype=float32), 'train/wer': 0.09882079826944891, 'validation/ctc_loss': Array(0.52979755, dtype=float32), 'validation/wer': 0.16038309663342248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3216067, dtype=float32), 'test/wer': 0.10862632786951841, 'test/num_examples': 2472, 'score': 17360.60667347908, 'total_duration': 19103.31728196144, 'accumulated_submission_time': 17360.60667347908, 'accumulated_eval_time': 1741.1654961109161, 'accumulated_logging_time': 0.6244997978210449}
I0214 08:17:50.236356 140428112725760 logging_writer.py:48] [21871] accumulated_eval_time=1741.165496, accumulated_logging_time=0.624500, accumulated_submission_time=17360.606673, global_step=21871, preemption_count=0, score=17360.606673, test/ctc_loss=0.32160669565200806, test/num_examples=2472, test/wer=0.108626, total_duration=19103.317282, train/ctc_loss=0.26909029483795166, train/wer=0.098821, validation/ctc_loss=0.5297975540161133, validation/num_examples=5348, validation/wer=0.160383
I0214 08:18:12.842089 140428104333056 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7116495370864868, loss=1.3208379745483398
I0214 08:19:28.179484 140428112725760 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.575579822063446, loss=1.39628005027771
I0214 08:20:43.593916 140428104333056 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7252333164215088, loss=1.3483322858810425
I0214 08:21:59.101432 140428112725760 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.6629493832588196, loss=1.3224941492080688
I0214 08:23:20.765673 140428104333056 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.5888683795928955, loss=1.3724744319915771
I0214 08:24:45.277811 140428112725760 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.5913330316543579, loss=1.32518470287323
I0214 08:26:10.585776 140428104333056 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6633410453796387, loss=1.344495177268982
I0214 08:27:35.599200 140428112725760 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5706438422203064, loss=1.335722804069519
I0214 08:28:59.919919 140428112725760 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7102369666099548, loss=1.325505256652832
I0214 08:30:15.042977 140428104333056 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.7280310988426208, loss=1.3785154819488525
I0214 08:31:30.399742 140428112725760 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8174612522125244, loss=1.3318573236465454
I0214 08:32:45.669911 140428104333056 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6268784999847412, loss=1.3666020631790161
I0214 08:34:02.817025 140428112725760 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7031548023223877, loss=1.3302326202392578
I0214 08:35:27.107926 140428104333056 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6563431620597839, loss=1.3490411043167114
I0214 08:36:52.928191 140428112725760 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6846848130226135, loss=1.3691308498382568
I0214 08:38:18.679245 140428104333056 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5970543026924133, loss=1.3490458726882935
I0214 08:39:44.244625 140428112725760 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7971791625022888, loss=1.3279777765274048
I0214 08:41:09.200628 140428104333056 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.6835988163948059, loss=1.367861032485962
I0214 08:41:50.372573 140599226058560 spec.py:321] Evaluating on the training split.
I0214 08:42:47.177356 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 08:43:39.023340 140599226058560 spec.py:349] Evaluating on the test split.
I0214 08:44:05.201892 140599226058560 submission_runner.py:408] Time since start: 20678.32s, 	Step: 23650, 	{'train/ctc_loss': Array(0.2543325, dtype=float32), 'train/wer': 0.09427924683609425, 'validation/ctc_loss': Array(0.51538944, dtype=float32), 'validation/wer': 0.15831700087857342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3166944, dtype=float32), 'test/wer': 0.10704202465825767, 'test/num_examples': 2472, 'score': 18800.65876030922, 'total_duration': 20678.319122314453, 'accumulated_submission_time': 18800.65876030922, 'accumulated_eval_time': 1875.9892790317535, 'accumulated_logging_time': 0.6764576435089111}
I0214 08:44:05.240256 140428112725760 logging_writer.py:48] [23650] accumulated_eval_time=1875.989279, accumulated_logging_time=0.676458, accumulated_submission_time=18800.658760, global_step=23650, preemption_count=0, score=18800.658760, test/ctc_loss=0.3166944086551666, test/num_examples=2472, test/wer=0.107042, total_duration=20678.319122, train/ctc_loss=0.2543325126171112, train/wer=0.094279, validation/ctc_loss=0.5153894424438477, validation/num_examples=5348, validation/wer=0.158317
I0214 08:44:47.350030 140427457365760 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6952201128005981, loss=1.3520786762237549
I0214 08:46:02.394176 140427448973056 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5884546041488647, loss=1.315664291381836
I0214 08:47:17.553349 140427457365760 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.763775646686554, loss=1.3431727886199951
I0214 08:48:32.824839 140427448973056 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.609613835811615, loss=1.344938039779663
I0214 08:49:50.591995 140427457365760 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.6338472366333008, loss=1.2865211963653564
I0214 08:51:14.309084 140427448973056 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6302127838134766, loss=1.3284980058670044
I0214 08:52:39.628897 140427457365760 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7292704582214355, loss=1.3353968858718872
I0214 08:54:04.389554 140427448973056 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.6017923355102539, loss=1.3596596717834473
I0214 08:55:29.663796 140427457365760 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6510388255119324, loss=1.323438286781311
I0214 08:56:54.985544 140427448973056 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6784645915031433, loss=1.3703466653823853
I0214 08:58:21.121119 140427457365760 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6767396926879883, loss=1.3140532970428467
I0214 08:59:41.742064 140427457365760 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7963370084762573, loss=1.3647464513778687
I0214 09:00:57.108040 140427448973056 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6734172105789185, loss=1.3254626989364624
I0214 09:02:12.451675 140427457365760 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8014134168624878, loss=1.3467527627944946
I0214 09:03:27.782795 140427448973056 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5853745937347412, loss=1.336993932723999
I0214 09:04:47.653577 140427457365760 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6244779229164124, loss=1.3454233407974243
I0214 09:06:12.807865 140427448973056 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6609678268432617, loss=1.2800108194351196
I0214 09:07:38.721200 140427457365760 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6887763142585754, loss=1.3754873275756836
I0214 09:08:06.441036 140599226058560 spec.py:321] Evaluating on the training split.
I0214 09:09:01.269244 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 09:09:52.397726 140599226058560 spec.py:349] Evaluating on the test split.
I0214 09:10:18.326523 140599226058560 submission_runner.py:408] Time since start: 22251.44s, 	Step: 25434, 	{'train/ctc_loss': Array(0.23121329, dtype=float32), 'train/wer': 0.08628225159410498, 'validation/ctc_loss': Array(0.4990429, dtype=float32), 'validation/wer': 0.15016847369589773, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30282408, dtype=float32), 'test/wer': 0.1014157171003189, 'test/num_examples': 2472, 'score': 20241.77404141426, 'total_duration': 22251.441853284836, 'accumulated_submission_time': 20241.77404141426, 'accumulated_eval_time': 2007.8673095703125, 'accumulated_logging_time': 0.731112003326416}
I0214 09:10:18.369780 140427165525760 logging_writer.py:48] [25434] accumulated_eval_time=2007.867310, accumulated_logging_time=0.731112, accumulated_submission_time=20241.774041, global_step=25434, preemption_count=0, score=20241.774041, test/ctc_loss=0.30282407999038696, test/num_examples=2472, test/wer=0.101416, total_duration=22251.441853, train/ctc_loss=0.2312132865190506, train/wer=0.086282, validation/ctc_loss=0.49904289841651917, validation/num_examples=5348, validation/wer=0.150168
I0214 09:11:08.786326 140427157133056 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6385405659675598, loss=1.2922370433807373
I0214 09:12:24.103979 140427165525760 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.819612443447113, loss=1.2767387628555298
I0214 09:13:39.560304 140427157133056 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6893061995506287, loss=1.2770756483078003
I0214 09:14:58.831362 140427165525760 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.7023862600326538, loss=1.2783739566802979
I0214 09:16:14.090501 140427157133056 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.7135636806488037, loss=1.334510326385498
I0214 09:17:29.378604 140427165525760 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5464558005332947, loss=1.306542992591858
I0214 09:18:44.697808 140427157133056 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.8373332619667053, loss=1.3261865377426147
I0214 09:20:05.133485 140427165525760 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7823761701583862, loss=1.3302303552627563
I0214 09:21:30.129041 140427157133056 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7720749974250793, loss=1.3228180408477783
I0214 09:22:55.783273 140427165525760 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6351698040962219, loss=1.2967541217803955
I0214 09:24:21.458931 140427157133056 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5505366921424866, loss=1.287001371383667
I0214 09:25:46.314962 140427165525760 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7112911343574524, loss=1.345146656036377
I0214 09:27:11.019384 140427157133056 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6774753928184509, loss=1.3433042764663696
I0214 09:28:37.311372 140427165525760 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6215985417366028, loss=1.3320112228393555
I0214 09:29:52.640228 140427157133056 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.593279242515564, loss=1.2640845775604248
I0214 09:31:07.792289 140427165525760 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7824733853340149, loss=1.3258755207061768
I0214 09:32:22.982397 140427157133056 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6222212910652161, loss=1.355494737625122
I0214 09:33:38.120565 140427165525760 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7179363369941711, loss=1.3260819911956787
I0214 09:34:18.632173 140599226058560 spec.py:321] Evaluating on the training split.
I0214 09:35:14.087400 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 09:36:05.675684 140599226058560 spec.py:349] Evaluating on the test split.
I0214 09:36:31.918941 140599226058560 submission_runner.py:408] Time since start: 23825.04s, 	Step: 27251, 	{'train/ctc_loss': Array(0.21819447, dtype=float32), 'train/wer': 0.08222436683272907, 'validation/ctc_loss': Array(0.4925936, dtype=float32), 'validation/wer': 0.14788032092066772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29341123, dtype=float32), 'test/wer': 0.09824711067779741, 'test/num_examples': 2472, 'score': 21681.949670553207, 'total_duration': 23825.036254882812, 'accumulated_submission_time': 21681.949670553207, 'accumulated_eval_time': 2141.1486024856567, 'accumulated_logging_time': 0.7904298305511475}
I0214 09:36:31.954894 140428112725760 logging_writer.py:48] [27251] accumulated_eval_time=2141.148602, accumulated_logging_time=0.790430, accumulated_submission_time=21681.949671, global_step=27251, preemption_count=0, score=21681.949671, test/ctc_loss=0.2934112250804901, test/num_examples=2472, test/wer=0.098247, total_duration=23825.036255, train/ctc_loss=0.21819446980953217, train/wer=0.082224, validation/ctc_loss=0.49259358644485474, validation/num_examples=5348, validation/wer=0.147880
I0214 09:37:09.539064 140428104333056 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6869005560874939, loss=1.3340351581573486
I0214 09:38:24.741396 140428112725760 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.6697473526000977, loss=1.305856466293335
I0214 09:39:40.132873 140428104333056 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5915540456771851, loss=1.3155593872070312
I0214 09:40:57.803370 140428112725760 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.7031822204589844, loss=1.3715810775756836
I0214 09:42:22.238116 140428104333056 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.691814661026001, loss=1.2870700359344482
I0214 09:43:46.911875 140428112725760 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6790222525596619, loss=1.3012715578079224
I0214 09:45:06.848108 140428112725760 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6993900537490845, loss=1.262006163597107
I0214 09:46:22.220310 140428104333056 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5378313064575195, loss=1.251869797706604
I0214 09:47:37.641458 140428112725760 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7396672964096069, loss=1.310225248336792
I0214 09:48:52.871191 140428104333056 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6329662799835205, loss=1.3151097297668457
I0214 09:50:12.387299 140428112725760 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.6277439594268799, loss=1.2986456155776978
I0214 09:51:36.460004 140428104333056 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.6093674898147583, loss=1.3020955324172974
I0214 09:53:01.221297 140428112725760 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7191896438598633, loss=1.3324171304702759
I0214 09:54:26.632610 140428104333056 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.6863877177238464, loss=1.2906262874603271
I0214 09:55:51.284190 140428112725760 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7913007140159607, loss=1.3251290321350098
I0214 09:57:17.152724 140428104333056 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.769301176071167, loss=1.287235975265503
I0214 09:58:39.985873 140428112725760 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.6832044720649719, loss=1.3450247049331665
I0214 09:59:55.522306 140428104333056 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6922307014465332, loss=1.2711005210876465
I0214 10:00:32.150496 140599226058560 spec.py:321] Evaluating on the training split.
I0214 10:01:28.528849 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 10:02:20.481245 140599226058560 spec.py:349] Evaluating on the test split.
I0214 10:02:46.633115 140599226058560 submission_runner.py:408] Time since start: 25399.75s, 	Step: 29050, 	{'train/ctc_loss': Array(0.21523209, dtype=float32), 'train/wer': 0.08024363052438863, 'validation/ctc_loss': Array(0.48609242, dtype=float32), 'validation/wer': 0.146576942757562, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28730962, dtype=float32), 'test/wer': 0.09810492962037658, 'test/num_examples': 2472, 'score': 23122.05716252327, 'total_duration': 25399.749225378036, 'accumulated_submission_time': 23122.05716252327, 'accumulated_eval_time': 2275.6245658397675, 'accumulated_logging_time': 0.8445065021514893}
I0214 10:02:46.667520 140427605845760 logging_writer.py:48] [29050] accumulated_eval_time=2275.624566, accumulated_logging_time=0.844507, accumulated_submission_time=23122.057163, global_step=29050, preemption_count=0, score=23122.057163, test/ctc_loss=0.2873096168041229, test/num_examples=2472, test/wer=0.098105, total_duration=25399.749225, train/ctc_loss=0.21523208916187286, train/wer=0.080244, validation/ctc_loss=0.4860924184322357, validation/num_examples=5348, validation/wer=0.146577
I0214 10:03:24.955209 140427597453056 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.6057115793228149, loss=1.319761037826538
I0214 10:04:40.329716 140427605845760 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.6336257457733154, loss=1.2463932037353516
I0214 10:05:55.713381 140427597453056 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6563653945922852, loss=1.2977089881896973
I0214 10:07:11.073906 140427605845760 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7723190784454346, loss=1.2562077045440674
I0214 10:08:32.939153 140427597453056 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9861371517181396, loss=1.2873029708862305
I0214 10:09:58.406701 140427605845760 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.9322668313980103, loss=1.3284521102905273
I0214 10:11:23.653129 140427597453056 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.652131974697113, loss=1.321427822113037
I0214 10:12:48.679667 140427605845760 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.7955818176269531, loss=1.2905141115188599
I0214 10:14:14.579546 140427605845760 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6235092878341675, loss=1.3168792724609375
I0214 10:15:30.126903 140427597453056 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6852842569351196, loss=1.292208194732666
I0214 10:16:45.469776 140427605845760 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.7793555855751038, loss=1.2823994159698486
I0214 10:18:00.851805 140427597453056 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6586087942123413, loss=1.3130344152450562
I0214 10:19:16.991420 140427605845760 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7148057222366333, loss=1.2886717319488525
I0214 10:20:41.900629 140427597453056 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.9046022891998291, loss=1.2702492475509644
I0214 10:22:07.235397 140427605845760 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.726218581199646, loss=1.3341916799545288
I0214 10:23:32.056125 140427597453056 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6815282106399536, loss=1.273290991783142
I0214 10:24:56.949415 140427605845760 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6913266777992249, loss=1.3242055177688599
I0214 10:26:22.623268 140427597453056 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.6698169708251953, loss=1.2891809940338135
I0214 10:26:47.276402 140599226058560 spec.py:321] Evaluating on the training split.
I0214 10:27:42.067248 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 10:28:33.662869 140599226058560 spec.py:349] Evaluating on the test split.
I0214 10:28:59.791418 140599226058560 submission_runner.py:408] Time since start: 26972.91s, 	Step: 30830, 	{'train/ctc_loss': Array(0.22447392, dtype=float32), 'train/wer': 0.08388043855087537, 'validation/ctc_loss': Array(0.46955344, dtype=float32), 'validation/wer': 0.1418944360234415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27968472, dtype=float32), 'test/wer': 0.09595190217943249, 'test/num_examples': 2472, 'score': 24562.581042051315, 'total_duration': 26972.9074883461, 'accumulated_submission_time': 24562.581042051315, 'accumulated_eval_time': 2408.1328547000885, 'accumulated_logging_time': 0.8940913677215576}
I0214 10:28:59.828263 140427308881664 logging_writer.py:48] [30830] accumulated_eval_time=2408.132855, accumulated_logging_time=0.894091, accumulated_submission_time=24562.581042, global_step=30830, preemption_count=0, score=24562.581042, test/ctc_loss=0.27968472242355347, test/num_examples=2472, test/wer=0.095952, total_duration=26972.907488, train/ctc_loss=0.22447392344474792, train/wer=0.083880, validation/ctc_loss=0.4695534408092499, validation/num_examples=5348, validation/wer=0.141894
I0214 10:29:56.934334 140427308881664 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.613900899887085, loss=1.2635619640350342
I0214 10:31:12.141982 140427300488960 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7868967652320862, loss=1.2744567394256592
I0214 10:32:27.698557 140427308881664 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6869485378265381, loss=1.3045068979263306
I0214 10:33:43.155694 140427300488960 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6111445426940918, loss=1.2079575061798096
I0214 10:34:59.209694 140427308881664 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6853625178337097, loss=1.2739554643630981
I0214 10:36:23.909093 140427300488960 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.776125967502594, loss=1.295508861541748
I0214 10:37:48.519563 140427308881664 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.68015456199646, loss=1.290001630783081
I0214 10:39:12.991713 140427300488960 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.7413328289985657, loss=1.2760825157165527
I0214 10:40:37.898535 140427308881664 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.7129881978034973, loss=1.285818099975586
I0214 10:42:02.619609 140427300488960 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.721864640712738, loss=1.2700215578079224
I0214 10:43:27.624058 140427308881664 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7474544644355774, loss=1.313272476196289
I0214 10:44:48.932382 140427308881664 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6065154075622559, loss=1.2109427452087402
I0214 10:46:04.227408 140427300488960 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.7150156497955322, loss=1.2962702512741089
I0214 10:47:19.668744 140427308881664 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.7457571625709534, loss=1.2470505237579346
I0214 10:48:34.943365 140427300488960 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.6595566868782043, loss=1.2531245946884155
I0214 10:49:54.498018 140427308881664 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.7570739984512329, loss=1.2735646963119507
I0214 10:51:19.134052 140427300488960 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6410949230194092, loss=1.237960934638977
I0214 10:52:43.897806 140427308881664 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6585342884063721, loss=1.2612723112106323
I0214 10:53:00.228214 140599226058560 spec.py:321] Evaluating on the training split.
I0214 10:53:54.616215 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 10:54:45.457079 140599226058560 spec.py:349] Evaluating on the test split.
I0214 10:55:11.780936 140599226058560 submission_runner.py:408] Time since start: 28544.90s, 	Step: 32621, 	{'train/ctc_loss': Array(0.21071382, dtype=float32), 'train/wer': 0.0778517517729342, 'validation/ctc_loss': Array(0.4662035, dtype=float32), 'validation/wer': 0.14140204871737935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27777946, dtype=float32), 'test/wer': 0.09463164950338188, 'test/num_examples': 2472, 'score': 26002.89443707466, 'total_duration': 28544.897045135498, 'accumulated_submission_time': 26002.89443707466, 'accumulated_eval_time': 2539.6788890361786, 'accumulated_logging_time': 0.9472103118896484}
I0214 10:55:11.822106 140427308881664 logging_writer.py:48] [32621] accumulated_eval_time=2539.678889, accumulated_logging_time=0.947210, accumulated_submission_time=26002.894437, global_step=32621, preemption_count=0, score=26002.894437, test/ctc_loss=0.2777794599533081, test/num_examples=2472, test/wer=0.094632, total_duration=28544.897045, train/ctc_loss=0.21071381866931915, train/wer=0.077852, validation/ctc_loss=0.466203510761261, validation/num_examples=5348, validation/wer=0.141402
I0214 10:56:11.800640 140427300488960 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.7678197622299194, loss=1.2606197595596313
I0214 10:57:26.977452 140427308881664 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.684907078742981, loss=1.337472677230835
I0214 10:58:42.196268 140427300488960 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.6966297030448914, loss=1.2744901180267334
I0214 11:00:04.174778 140427308881664 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.7286338210105896, loss=1.2215547561645508
I0214 11:01:19.463913 140427300488960 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.8120850920677185, loss=1.2478704452514648
I0214 11:02:34.973695 140427308881664 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.7405829429626465, loss=1.2382491827011108
I0214 11:03:50.282430 140427300488960 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7695452570915222, loss=1.3248918056488037
I0214 11:05:06.081290 140427308881664 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6378895044326782, loss=1.273929238319397
I0214 11:06:30.817153 140427300488960 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7131219506263733, loss=1.2495554685592651
I0214 11:07:55.789304 140427308881664 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6771078109741211, loss=1.2438076734542847
I0214 11:09:20.540454 140427300488960 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.7238767147064209, loss=1.2263379096984863
I0214 11:10:45.344627 140427308881664 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6321170330047607, loss=1.3363957405090332
I0214 11:12:11.032484 140427300488960 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.718971848487854, loss=1.2303990125656128
I0214 11:13:38.388280 140427308881664 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6693177223205566, loss=1.2067073583602905
I0214 11:14:53.735446 140427300488960 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.649332582950592, loss=1.2982383966445923
I0214 11:16:09.222454 140427308881664 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5922785401344299, loss=1.2464067935943604
I0214 11:17:24.946316 140427300488960 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6975009441375732, loss=1.199175477027893
I0214 11:18:40.355847 140427308881664 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.6243920922279358, loss=1.2445728778839111
I0214 11:19:12.404355 140599226058560 spec.py:321] Evaluating on the training split.
I0214 11:20:08.409433 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 11:21:00.138304 140599226058560 spec.py:349] Evaluating on the test split.
I0214 11:21:26.019653 140599226058560 submission_runner.py:408] Time since start: 30119.14s, 	Step: 34442, 	{'train/ctc_loss': Array(0.21640244, dtype=float32), 'train/wer': 0.07870282376752333, 'validation/ctc_loss': Array(0.45866513, dtype=float32), 'validation/wer': 0.13789740965658398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26703775, dtype=float32), 'test/wer': 0.09119899254565028, 'test/num_examples': 2472, 'score': 27443.38892006874, 'total_duration': 30119.136954545975, 'accumulated_submission_time': 27443.38892006874, 'accumulated_eval_time': 2673.288696050644, 'accumulated_logging_time': 1.005300521850586}
I0214 11:21:26.060976 140427308881664 logging_writer.py:48] [34442] accumulated_eval_time=2673.288696, accumulated_logging_time=1.005301, accumulated_submission_time=27443.388920, global_step=34442, preemption_count=0, score=27443.388920, test/ctc_loss=0.2670377492904663, test/num_examples=2472, test/wer=0.091199, total_duration=30119.136955, train/ctc_loss=0.21640244126319885, train/wer=0.078703, validation/ctc_loss=0.458665132522583, validation/num_examples=5348, validation/wer=0.137897
I0214 11:22:10.324987 140427300488960 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.683292806148529, loss=1.2086782455444336
I0214 11:23:25.634689 140427308881664 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.7775732278823853, loss=1.288612961769104
I0214 11:24:40.957491 140427300488960 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7365723848342896, loss=1.2297756671905518
I0214 11:26:00.920943 140427308881664 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.7425150275230408, loss=1.2816202640533447
I0214 11:27:27.253096 140427300488960 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6353211998939514, loss=1.2411365509033203
I0214 11:28:52.516176 140427308881664 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6385357975959778, loss=1.2807103395462036
I0214 11:30:12.904290 140427308881664 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.674781084060669, loss=1.2295756340026855
I0214 11:31:28.112022 140427300488960 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6646418571472168, loss=1.2272706031799316
I0214 11:32:43.582452 140427308881664 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.1601166725158691, loss=1.2954485416412354
I0214 11:33:59.260093 140427300488960 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.6988666653633118, loss=1.2338764667510986
I0214 11:35:17.202790 140427308881664 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8206623196601868, loss=1.2474095821380615
I0214 11:36:41.762367 140427300488960 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7071912884712219, loss=1.2487246990203857
I0214 11:38:07.757602 140427308881664 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6390767693519592, loss=1.2278738021850586
I0214 11:39:33.559152 140427300488960 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.7459086179733276, loss=1.2469552755355835
I0214 11:40:58.156328 140427308881664 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.8158639073371887, loss=1.315506100654602
I0214 11:42:23.611648 140427300488960 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.9018086791038513, loss=1.2237485647201538
I0214 11:43:47.535165 140427308881664 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.9167570471763611, loss=1.2317419052124023
I0214 11:45:02.759589 140427300488960 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.8100051283836365, loss=1.231914758682251
I0214 11:45:26.556085 140599226058560 spec.py:321] Evaluating on the training split.
I0214 11:46:21.009039 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 11:47:12.479346 140599226058560 spec.py:349] Evaluating on the test split.
I0214 11:47:38.607257 140599226058560 submission_runner.py:408] Time since start: 31691.72s, 	Step: 36233, 	{'train/ctc_loss': Array(0.21171698, dtype=float32), 'train/wer': 0.07614267293285165, 'validation/ctc_loss': Array(0.4551876, dtype=float32), 'validation/wer': 0.13615957210577637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2658482, dtype=float32), 'test/wer': 0.08847724087502284, 'test/num_examples': 2472, 'score': 28883.798013210297, 'total_duration': 31691.72387599945, 'accumulated_submission_time': 28883.798013210297, 'accumulated_eval_time': 2805.333705663681, 'accumulated_logging_time': 1.062840461730957}
I0214 11:47:38.645838 140427308881664 logging_writer.py:48] [36233] accumulated_eval_time=2805.333706, accumulated_logging_time=1.062840, accumulated_submission_time=28883.798013, global_step=36233, preemption_count=0, score=28883.798013, test/ctc_loss=0.26584818959236145, test/num_examples=2472, test/wer=0.088477, total_duration=31691.723876, train/ctc_loss=0.21171697974205017, train/wer=0.076143, validation/ctc_loss=0.45518758893013, validation/num_examples=5348, validation/wer=0.136160
I0214 11:48:29.570938 140427300488960 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.678237795829773, loss=1.236974835395813
I0214 11:49:44.982524 140427308881664 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.6547641158103943, loss=1.2237695455551147
I0214 11:51:00.354981 140427300488960 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.5931549072265625, loss=1.1966570615768433
I0214 11:52:15.699928 140427308881664 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6470267176628113, loss=1.216579556465149
I0214 11:53:38.796211 140427300488960 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.71925950050354, loss=1.1925923824310303
I0214 11:55:04.629180 140427308881664 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6881162524223328, loss=1.2722257375717163
I0214 11:56:30.223973 140427300488960 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7038107514381409, loss=1.253706932067871
I0214 11:57:56.061199 140427308881664 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6176852583885193, loss=1.245626449584961
I0214 11:59:23.594453 140427308881664 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.9249444007873535, loss=1.2133570909500122
I0214 12:00:38.756955 140427300488960 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.7246304750442505, loss=1.2198842763900757
I0214 12:01:54.205647 140427308881664 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.7089352011680603, loss=1.212175965309143
I0214 12:03:09.700229 140427300488960 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.6739758253097534, loss=1.211336612701416
I0214 12:04:25.601757 140427308881664 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6193462014198303, loss=1.2096296548843384
I0214 12:05:50.490791 140427300488960 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7923529744148254, loss=1.2744396924972534
I0214 12:07:15.337361 140427308881664 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.7006910443305969, loss=1.2727330923080444
I0214 12:08:40.148032 140427300488960 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6898662447929382, loss=1.2256649732589722
I0214 12:10:05.025708 140427308881664 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6711483597755432, loss=1.1860699653625488
I0214 12:11:29.417578 140427300488960 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7499450445175171, loss=1.2205448150634766
I0214 12:11:39.016971 140599226058560 spec.py:321] Evaluating on the training split.
I0214 12:12:35.159937 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 12:13:27.114093 140599226058560 spec.py:349] Evaluating on the test split.
I0214 12:13:53.268849 140599226058560 submission_runner.py:408] Time since start: 33266.39s, 	Step: 38013, 	{'train/ctc_loss': Array(0.16391788, dtype=float32), 'train/wer': 0.0609964843229503, 'validation/ctc_loss': Array(0.43490544, dtype=float32), 'validation/wer': 0.1313708641879954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25705752, dtype=float32), 'test/wer': 0.08595860500071091, 'test/num_examples': 2472, 'score': 30324.081511735916, 'total_duration': 33266.3851544857, 'accumulated_submission_time': 30324.081511735916, 'accumulated_eval_time': 2939.579090833664, 'accumulated_logging_time': 1.119497299194336}
I0214 12:13:53.305910 140427308881664 logging_writer.py:48] [38013] accumulated_eval_time=2939.579091, accumulated_logging_time=1.119497, accumulated_submission_time=30324.081512, global_step=38013, preemption_count=0, score=30324.081512, test/ctc_loss=0.2570575177669525, test/num_examples=2472, test/wer=0.085959, total_duration=33266.385154, train/ctc_loss=0.1639178842306137, train/wer=0.060996, validation/ctc_loss=0.43490543961524963, validation/num_examples=5348, validation/wer=0.131371
I0214 12:14:59.242611 140427300488960 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.8049916625022888, loss=1.2555062770843506
I0214 12:16:18.049150 140427308881664 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.7157188653945923, loss=1.2021663188934326
I0214 12:17:33.274946 140427300488960 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7303697466850281, loss=1.2119684219360352
I0214 12:18:48.467834 140427308881664 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6476815342903137, loss=1.2111377716064453
I0214 12:20:06.167305 140427300488960 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7577542066574097, loss=1.1790105104446411
I0214 12:21:29.930284 140427308881664 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.668318510055542, loss=1.2446755170822144
I0214 12:22:53.847771 140427300488960 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.8155390024185181, loss=1.2192943096160889
I0214 12:24:19.334241 140427308881664 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6625407338142395, loss=1.2290420532226562
I0214 12:25:44.155461 140427300488960 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7240635752677917, loss=1.1666101217269897
I0214 12:27:09.533728 140427308881664 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7132943272590637, loss=1.1797850131988525
I0214 12:28:35.170221 140427300488960 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6669454574584961, loss=1.1873847246170044
I0214 12:29:58.022854 140427308881664 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.7423563003540039, loss=1.2565494775772095
I0214 12:31:13.315582 140427300488960 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.5975993871688843, loss=1.180793046951294
I0214 12:32:28.735213 140427308881664 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9812882542610168, loss=1.1759345531463623
I0214 12:33:44.139922 140427300488960 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7892781496047974, loss=1.2190824747085571
I0214 12:35:03.250612 140427308881664 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.8216843605041504, loss=1.2136898040771484
I0214 12:36:28.675578 140427300488960 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7649732828140259, loss=1.2109811305999756
I0214 12:37:53.646663 140427308881664 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7682651877403259, loss=1.2595977783203125
I0214 12:37:53.655482 140599226058560 spec.py:321] Evaluating on the training split.
I0214 12:38:48.834703 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 12:39:40.944577 140599226058560 spec.py:349] Evaluating on the test split.
I0214 12:40:06.979115 140599226058560 submission_runner.py:408] Time since start: 34840.10s, 	Step: 39801, 	{'train/ctc_loss': Array(0.1884352, dtype=float32), 'train/wer': 0.06887394141623197, 'validation/ctc_loss': Array(0.43504775, dtype=float32), 'validation/wer': 0.13085916757581317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25120482, dtype=float32), 'test/wer': 0.08437430178945017, 'test/num_examples': 2472, 'score': 31764.344473838806, 'total_duration': 34840.09559392929, 'accumulated_submission_time': 31764.344473838806, 'accumulated_eval_time': 3072.8963837623596, 'accumulated_logging_time': 1.1725895404815674}
I0214 12:40:07.018842 140427308881664 logging_writer.py:48] [39801] accumulated_eval_time=3072.896384, accumulated_logging_time=1.172590, accumulated_submission_time=31764.344474, global_step=39801, preemption_count=0, score=31764.344474, test/ctc_loss=0.25120481848716736, test/num_examples=2472, test/wer=0.084374, total_duration=34840.095594, train/ctc_loss=0.18843519687652588, train/wer=0.068874, validation/ctc_loss=0.4350477457046509, validation/num_examples=5348, validation/wer=0.130859
I0214 12:41:22.073560 140427300488960 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.718623161315918, loss=1.2246956825256348
I0214 12:42:37.409585 140427308881664 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.817857563495636, loss=1.1958674192428589
I0214 12:43:52.760421 140427300488960 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.832036554813385, loss=1.2204506397247314
I0214 12:45:15.865548 140427308881664 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7167887091636658, loss=1.2100821733474731
I0214 12:46:30.974802 140427300488960 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7450098991394043, loss=1.162227988243103
I0214 12:47:46.346027 140427308881664 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7387627959251404, loss=1.2524943351745605
I0214 12:49:01.709228 140427300488960 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.822575032711029, loss=1.198881983757019
I0214 12:50:18.400055 140427308881664 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7774986624717712, loss=1.204582929611206
I0214 12:51:42.436238 140427300488960 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.765798807144165, loss=1.1434518098831177
I0214 12:53:08.164122 140427308881664 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.7849563360214233, loss=1.1995587348937988
I0214 12:54:32.352052 140427300488960 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8225311636924744, loss=1.2042969465255737
I0214 12:55:58.045829 140427308881664 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7052820324897766, loss=1.2263067960739136
I0214 12:57:23.584261 140427300488960 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.763861894607544, loss=1.1975593566894531
I0214 12:58:52.937271 140427308881664 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6368991732597351, loss=1.1140422821044922
I0214 13:00:08.242411 140427300488960 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.814914882183075, loss=1.172712802886963
I0214 13:01:23.643399 140427308881664 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7183777689933777, loss=1.2197906970977783
I0214 13:02:38.935022 140427300488960 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.766761064529419, loss=1.233700156211853
I0214 13:03:54.549337 140427308881664 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6698959469795227, loss=1.209174394607544
I0214 13:04:07.469883 140599226058560 spec.py:321] Evaluating on the training split.
I0214 13:05:02.634220 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 13:05:55.073452 140599226058560 spec.py:349] Evaluating on the test split.
I0214 13:06:21.636977 140599226058560 submission_runner.py:408] Time since start: 36414.75s, 	Step: 41617, 	{'train/ctc_loss': Array(0.23098333, dtype=float32), 'train/wer': 0.08507672363760624, 'validation/ctc_loss': Array(0.4273046, dtype=float32), 'validation/wer': 0.12748003900479837, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24482445, dtype=float32), 'test/wer': 0.08297280279487336, 'test/num_examples': 2472, 'score': 33204.71107053757, 'total_duration': 36414.75404071808, 'accumulated_submission_time': 33204.71107053757, 'accumulated_eval_time': 3207.057755947113, 'accumulated_logging_time': 1.2265305519104004}
I0214 13:06:21.673472 140427308881664 logging_writer.py:48] [41617] accumulated_eval_time=3207.057756, accumulated_logging_time=1.226531, accumulated_submission_time=33204.711071, global_step=41617, preemption_count=0, score=33204.711071, test/ctc_loss=0.24482445418834686, test/num_examples=2472, test/wer=0.082973, total_duration=36414.754041, train/ctc_loss=0.23098333179950714, train/wer=0.085077, validation/ctc_loss=0.42730459570884705, validation/num_examples=5348, validation/wer=0.127480
I0214 13:07:24.725843 140427300488960 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.7340641021728516, loss=1.182910442352295
I0214 13:08:40.239632 140427308881664 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7054515480995178, loss=1.1943784952163696
I0214 13:09:55.491193 140427300488960 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6777623295783997, loss=1.2064322233200073
I0214 13:11:18.583269 140427308881664 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6938195824623108, loss=1.1711941957473755
I0214 13:12:43.544935 140427300488960 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7416399717330933, loss=1.1440480947494507
I0214 13:14:09.222632 140427308881664 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.9156590700149536, loss=1.1934270858764648
I0214 13:15:30.833266 140427308881664 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.8925626873970032, loss=1.1230589151382446
I0214 13:16:46.157706 140427300488960 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.6918187737464905, loss=1.1977534294128418
I0214 13:18:01.531824 140427308881664 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0195647478103638, loss=1.1778327226638794
I0214 13:19:16.853766 140427300488960 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7295946478843689, loss=1.1939823627471924
I0214 13:20:37.870408 140427308881664 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.9793472290039062, loss=1.1974762678146362
I0214 13:22:04.266204 140427300488960 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6560221314430237, loss=1.1705083847045898
I0214 13:23:30.232311 140427308881664 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.9053370952606201, loss=1.1860207319259644
I0214 13:24:55.755067 140427300488960 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.8871334195137024, loss=1.192943811416626
I0214 13:26:21.708065 140427308881664 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6855350136756897, loss=1.2029587030410767
I0214 13:27:46.687879 140427300488960 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.8173210620880127, loss=1.2174618244171143
I0214 13:29:10.943050 140427308881664 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7977023124694824, loss=1.1581581830978394
I0214 13:30:21.843835 140599226058560 spec.py:321] Evaluating on the training split.
I0214 13:31:15.572691 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 13:32:07.429662 140599226058560 spec.py:349] Evaluating on the test split.
I0214 13:32:33.790223 140599226058560 submission_runner.py:408] Time since start: 37986.91s, 	Step: 43396, 	{'train/ctc_loss': Array(0.22959857, dtype=float32), 'train/wer': 0.08449124832766172, 'validation/ctc_loss': Array(0.42108253, dtype=float32), 'validation/wer': 0.1264373364743138, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23914756, dtype=float32), 'test/wer': 0.08161192695955964, 'test/num_examples': 2472, 'score': 34644.79412436485, 'total_duration': 37986.90700483322, 'accumulated_submission_time': 34644.79412436485, 'accumulated_eval_time': 3338.9981787204742, 'accumulated_logging_time': 1.2806153297424316}
I0214 13:32:33.829191 140428189525760 logging_writer.py:48] [43396] accumulated_eval_time=3338.998179, accumulated_logging_time=1.280615, accumulated_submission_time=34644.794124, global_step=43396, preemption_count=0, score=34644.794124, test/ctc_loss=0.23914755880832672, test/num_examples=2472, test/wer=0.081612, total_duration=37986.907005, train/ctc_loss=0.22959856688976288, train/wer=0.084491, validation/ctc_loss=0.4210825264453888, validation/num_examples=5348, validation/wer=0.126437
I0214 13:32:37.689128 140428181133056 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.7481188178062439, loss=1.179802417755127
I0214 13:33:52.778892 140428189525760 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.6832120418548584, loss=1.256969690322876
I0214 13:35:08.046862 140428181133056 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.9461597204208374, loss=1.1579363346099854
I0214 13:36:23.278496 140428189525760 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.8107446432113647, loss=1.1579172611236572
I0214 13:37:38.630867 140428181133056 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.9153753519058228, loss=1.172775149345398
I0214 13:39:01.175522 140428189525760 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7475324869155884, loss=1.2092676162719727
I0214 13:40:26.304117 140428181133056 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6929468512535095, loss=1.2184991836547852
I0214 13:41:52.753369 140428189525760 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.8426903486251831, loss=1.187065839767456
I0214 13:43:18.108108 140428181133056 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.8592705130577087, loss=1.1644498109817505
I0214 13:44:46.092324 140428189525760 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.8293978571891785, loss=1.1817525625228882
I0214 13:46:01.257583 140428181133056 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7863331437110901, loss=1.16200590133667
I0214 13:47:16.525827 140428189525760 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7169643640518188, loss=1.13943612575531
I0214 13:48:31.907263 140428181133056 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.8333213329315186, loss=1.1625361442565918
I0214 13:49:49.551842 140428189525760 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.737301766872406, loss=1.1727608442306519
I0214 13:51:13.451232 140428181133056 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.7061742544174194, loss=1.1379098892211914
I0214 13:52:40.033237 140428189525760 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.8358945846557617, loss=1.0848047733306885
I0214 13:54:06.225566 140428181133056 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.7456566691398621, loss=1.1629395484924316
I0214 13:55:31.865721 140428189525760 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7144433259963989, loss=1.1594929695129395
I0214 13:56:34.158176 140599226058560 spec.py:321] Evaluating on the training split.
I0214 13:57:26.775802 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 13:58:19.059695 140599226058560 spec.py:349] Evaluating on the test split.
I0214 13:58:45.402815 140599226058560 submission_runner.py:408] Time since start: 39558.52s, 	Step: 45174, 	{'train/ctc_loss': Array(0.26250243, dtype=float32), 'train/wer': 0.09666512626425117, 'validation/ctc_loss': Array(0.40786082, dtype=float32), 'validation/wer': 0.12261409386253705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23407225, dtype=float32), 'test/wer': 0.07791521946661792, 'test/num_examples': 2472, 'score': 36085.03482174873, 'total_duration': 39558.517823934555, 'accumulated_submission_time': 36085.03482174873, 'accumulated_eval_time': 3470.2350482940674, 'accumulated_logging_time': 1.3378307819366455}
I0214 13:58:45.445470 140428189525760 logging_writer.py:48] [45174] accumulated_eval_time=3470.235048, accumulated_logging_time=1.337831, accumulated_submission_time=36085.034822, global_step=45174, preemption_count=0, score=36085.034822, test/ctc_loss=0.2340722531080246, test/num_examples=2472, test/wer=0.077915, total_duration=39558.517824, train/ctc_loss=0.26250243186950684, train/wer=0.096665, validation/ctc_loss=0.40786081552505493, validation/num_examples=5348, validation/wer=0.122614
I0214 13:59:05.830341 140428181133056 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.7375280857086182, loss=1.234851360321045
I0214 14:00:21.062994 140428189525760 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7722794413566589, loss=1.1690593957901
I0214 14:01:40.062528 140428189525760 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.8281146287918091, loss=1.1754831075668335
I0214 14:02:55.496124 140428181133056 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9270835518836975, loss=1.137794852256775
I0214 14:04:10.888178 140428189525760 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7469533681869507, loss=1.1762853860855103
I0214 14:05:26.178872 140428181133056 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.0548393726348877, loss=1.1466305255889893
I0214 14:06:48.540046 140428189525760 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.8292621970176697, loss=1.1641664505004883
I0214 14:08:13.679863 140428181133056 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7124660611152649, loss=1.1799565553665161
I0214 14:09:38.647456 140428189525760 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.769370973110199, loss=1.1808451414108276
I0214 14:11:03.646576 140428181133056 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6707924604415894, loss=1.1353905200958252
I0214 14:12:28.185516 140428189525760 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7990732789039612, loss=1.1202905178070068
I0214 14:13:53.713608 140428181133056 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6846289038658142, loss=1.1296981573104858
I0214 14:15:17.179607 140428189525760 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7445690035820007, loss=1.1209371089935303
I0214 14:16:32.697402 140428181133056 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7989400625228882, loss=1.1454334259033203
I0214 14:17:48.156937 140428189525760 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.8123376965522766, loss=1.1621148586273193
I0214 14:19:03.640606 140428181133056 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7908999919891357, loss=1.1364977359771729
I0214 14:20:20.111090 140428189525760 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8362053632736206, loss=1.187111496925354
I0214 14:21:45.391729 140428181133056 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7687201499938965, loss=1.1240090131759644
I0214 14:22:45.732556 140599226058560 spec.py:321] Evaluating on the training split.
I0214 14:23:38.465388 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 14:24:31.205980 140599226058560 spec.py:349] Evaluating on the test split.
I0214 14:24:57.998564 140599226058560 submission_runner.py:408] Time since start: 41131.11s, 	Step: 46972, 	{'train/ctc_loss': Array(0.22550887, dtype=float32), 'train/wer': 0.08165488168902967, 'validation/ctc_loss': Array(0.39984173, dtype=float32), 'validation/wer': 0.11989148169960512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22878855, dtype=float32), 'test/wer': 0.07667621310909349, 'test/num_examples': 2472, 'score': 37525.23612308502, 'total_duration': 41131.114505529404, 'accumulated_submission_time': 37525.23612308502, 'accumulated_eval_time': 3602.4942483901978, 'accumulated_logging_time': 1.3967421054840088}
I0214 14:24:58.039637 140427974485760 logging_writer.py:48] [46972] accumulated_eval_time=3602.494248, accumulated_logging_time=1.396742, accumulated_submission_time=37525.236123, global_step=46972, preemption_count=0, score=37525.236123, test/ctc_loss=0.2287885546684265, test/num_examples=2472, test/wer=0.076676, total_duration=41131.114506, train/ctc_loss=0.22550886869430542, train/wer=0.081655, validation/ctc_loss=0.3998417258262634, validation/num_examples=5348, validation/wer=0.119891
I0214 14:25:19.894072 140427966093056 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7664331793785095, loss=1.1751374006271362
I0214 14:26:35.512170 140427974485760 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.8151763081550598, loss=1.1160815954208374
I0214 14:27:50.942198 140427966093056 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.8134780526161194, loss=1.1416141986846924
I0214 14:29:09.381910 140427974485760 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.8535442352294922, loss=1.1473631858825684
I0214 14:30:35.577464 140427319125760 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7361581921577454, loss=1.1400504112243652
I0214 14:31:50.741199 140427310733056 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8448109030723572, loss=1.1694481372833252
I0214 14:33:05.864373 140427319125760 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.9938912391662598, loss=1.0694599151611328
I0214 14:34:21.056236 140427310733056 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7341240048408508, loss=1.0828019380569458
I0214 14:35:38.755717 140427319125760 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.7080078721046448, loss=1.0867568254470825
I0214 14:37:03.337216 140427310733056 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.7537037134170532, loss=1.1528418064117432
I0214 14:38:29.274283 140427319125760 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6881463527679443, loss=1.1324143409729004
I0214 14:39:54.658569 140427310733056 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.8081284761428833, loss=1.1367712020874023
I0214 14:41:19.975592 140427319125760 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6500958800315857, loss=1.1198394298553467
I0214 14:42:45.335743 140427310733056 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.790043830871582, loss=1.1188812255859375
I0214 14:44:11.121374 140427319125760 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.7782212495803833, loss=1.1169055700302124
I0214 14:45:30.376236 140427974485760 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.7201659679412842, loss=1.094710111618042
I0214 14:46:45.717370 140427966093056 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.7410624027252197, loss=1.0702005624771118
I0214 14:48:01.029132 140427974485760 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.7693265676498413, loss=1.1487703323364258
I0214 14:48:58.666572 140599226058560 spec.py:321] Evaluating on the training split.
I0214 14:49:52.261906 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 14:50:44.201729 140599226058560 spec.py:349] Evaluating on the test split.
I0214 14:51:10.285951 140599226058560 submission_runner.py:408] Time since start: 42703.40s, 	Step: 48778, 	{'train/ctc_loss': Array(0.20397122, dtype=float32), 'train/wer': 0.07678175765201029, 'validation/ctc_loss': Array(0.3939008, dtype=float32), 'validation/wer': 0.11731368933257383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22130717, dtype=float32), 'test/wer': 0.07452318566814942, 'test/num_examples': 2472, 'score': 38965.769352674484, 'total_duration': 42703.40314888954, 'accumulated_submission_time': 38965.769352674484, 'accumulated_eval_time': 3734.108068704605, 'accumulated_logging_time': 1.460125207901001}
I0214 14:51:10.332455 140428404565760 logging_writer.py:48] [48778] accumulated_eval_time=3734.108069, accumulated_logging_time=1.460125, accumulated_submission_time=38965.769353, global_step=48778, preemption_count=0, score=38965.769353, test/ctc_loss=0.221307173371315, test/num_examples=2472, test/wer=0.074523, total_duration=42703.403149, train/ctc_loss=0.20397122204303741, train/wer=0.076782, validation/ctc_loss=0.3939008116722107, validation/num_examples=5348, validation/wer=0.117314
I0214 14:51:27.631263 140428396173056 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.7633156180381775, loss=1.0923758745193481
I0214 14:52:42.766051 140428404565760 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8316283822059631, loss=1.158057689666748
I0214 14:53:58.098154 140428396173056 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9062044024467468, loss=1.1314446926116943
I0214 14:55:15.021631 140428404565760 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.8473803400993347, loss=1.080100655555725
I0214 14:56:39.220162 140428396173056 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.8186935186386108, loss=1.0881872177124023
I0214 14:58:05.685995 140428404565760 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.8080287575721741, loss=1.0842528343200684
I0214 14:59:31.678702 140428396173056 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.8035470843315125, loss=1.1197538375854492
I0214 15:00:54.647947 140427749205760 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.8129239678382874, loss=1.1415679454803467
I0214 15:02:09.885738 140427740813056 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6838379502296448, loss=1.068090796470642
I0214 15:03:25.322725 140427749205760 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.8181434869766235, loss=1.064887285232544
I0214 15:04:40.701957 140427740813056 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.8264349699020386, loss=1.1371866464614868
I0214 15:06:00.457998 140427749205760 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.816615104675293, loss=1.1118245124816895
I0214 15:07:25.537556 140427740813056 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.7094358801841736, loss=1.1215964555740356
I0214 15:08:50.345542 140427749205760 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.80292147397995, loss=1.0905652046203613
I0214 15:10:16.193274 140427740813056 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.7426601052284241, loss=1.0885624885559082
I0214 15:11:41.976876 140427749205760 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7152473330497742, loss=1.0774790048599243
I0214 15:13:07.526998 140427740813056 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.8867585062980652, loss=1.1491953134536743
I0214 15:14:33.147658 140428404565760 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7659638524055481, loss=1.0800232887268066
I0214 15:15:10.350482 140599226058560 spec.py:321] Evaluating on the training split.
I0214 15:16:04.925524 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 15:16:56.682667 140599226058560 spec.py:349] Evaluating on the test split.
I0214 15:17:23.122808 140599226058560 submission_runner.py:408] Time since start: 44276.24s, 	Step: 50551, 	{'train/ctc_loss': Array(0.1726337, dtype=float32), 'train/wer': 0.06562877247169627, 'validation/ctc_loss': Array(0.38651025, dtype=float32), 'validation/wer': 0.11306564198615522, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21482898, dtype=float32), 'test/wer': 0.07208579611236365, 'test/num_examples': 2472, 'score': 40405.70114803314, 'total_duration': 44276.238609075546, 'accumulated_submission_time': 40405.70114803314, 'accumulated_eval_time': 3866.8734180927277, 'accumulated_logging_time': 1.5228583812713623}
I0214 15:17:23.162321 140428404565760 logging_writer.py:48] [50551] accumulated_eval_time=3866.873418, accumulated_logging_time=1.522858, accumulated_submission_time=40405.701148, global_step=50551, preemption_count=0, score=40405.701148, test/ctc_loss=0.2148289829492569, test/num_examples=2472, test/wer=0.072086, total_duration=44276.238609, train/ctc_loss=0.17263369262218475, train/wer=0.065629, validation/ctc_loss=0.3865102529525757, validation/num_examples=5348, validation/wer=0.113066
I0214 15:18:00.727289 140428396173056 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.8264944553375244, loss=1.1258996725082397
I0214 15:19:15.843851 140428404565760 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8510281443595886, loss=1.0433955192565918
I0214 15:20:31.242182 140428396173056 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7470364570617676, loss=1.0793718099594116
I0214 15:21:46.481568 140428404565760 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.7099571228027344, loss=1.0642993450164795
I0214 15:23:01.818272 140428396173056 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.8283069729804993, loss=1.0423328876495361
I0214 15:24:26.577608 140428404565760 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.8487482070922852, loss=1.0895469188690186
I0214 15:25:53.291915 140428396173056 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8053677082061768, loss=1.1056076288223267
I0214 15:27:18.061974 140428404565760 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.8006541132926941, loss=1.1278783082962036
I0214 15:28:43.648695 140428396173056 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.9368798136711121, loss=1.0764250755310059
I0214 15:30:12.562379 140427749205760 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7749474048614502, loss=1.0429365634918213
I0214 15:31:27.604937 140427740813056 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.9743531346321106, loss=1.1051526069641113
I0214 15:32:42.780228 140427749205760 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.040358066558838, loss=1.0874327421188354
I0214 15:33:58.077472 140427740813056 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.7699826955795288, loss=1.0703686475753784
I0214 15:35:13.183117 140427749205760 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.8957332968711853, loss=1.0920029878616333
I0214 15:36:37.137993 140427740813056 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8254231810569763, loss=1.0920078754425049
I0214 15:38:02.514381 140427749205760 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8984359502792358, loss=1.0847797393798828
I0214 15:39:28.329094 140427740813056 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.7659096717834473, loss=1.1096413135528564
I0214 15:40:54.434785 140427749205760 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.898708164691925, loss=1.1253926753997803
I0214 15:41:23.481884 140599226058560 spec.py:321] Evaluating on the training split.
I0214 15:42:18.179237 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 15:43:09.985410 140599226058560 spec.py:349] Evaluating on the test split.
I0214 15:43:36.375766 140599226058560 submission_runner.py:408] Time since start: 45849.49s, 	Step: 52335, 	{'train/ctc_loss': Array(0.19079773, dtype=float32), 'train/wer': 0.07166792485621337, 'validation/ctc_loss': Array(0.37963584, dtype=float32), 'validation/wer': 0.11314287921063557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21250953, dtype=float32), 'test/wer': 0.07273576666057319, 'test/num_examples': 2472, 'score': 41845.93539810181, 'total_duration': 45849.49245977402, 'accumulated_submission_time': 41845.93539810181, 'accumulated_eval_time': 3999.7612252235413, 'accumulated_logging_time': 1.5781021118164062}
I0214 15:43:36.420378 140427534165760 logging_writer.py:48] [52335] accumulated_eval_time=3999.761225, accumulated_logging_time=1.578102, accumulated_submission_time=41845.935398, global_step=52335, preemption_count=0, score=41845.935398, test/ctc_loss=0.21250952780246735, test/num_examples=2472, test/wer=0.072736, total_duration=45849.492460, train/ctc_loss=0.19079773128032684, train/wer=0.071668, validation/ctc_loss=0.37963584065437317, validation/num_examples=5348, validation/wer=0.113143
I0214 15:44:26.419274 140427525773056 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8278615474700928, loss=1.0798399448394775
I0214 15:45:41.962921 140427534165760 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7841151356697083, loss=1.1091253757476807
I0214 15:47:00.979645 140427534165760 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.8988581299781799, loss=1.081383466720581
I0214 15:48:16.143958 140427525773056 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.7444037199020386, loss=1.0226434469223022
I0214 15:49:31.381341 140427534165760 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.9621497392654419, loss=1.0975673198699951
I0214 15:50:46.600426 140427525773056 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.8749396800994873, loss=1.1122102737426758
I0214 15:52:05.746362 140427534165760 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.8524460196495056, loss=1.0534485578536987
I0214 15:53:30.382535 140427525773056 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9562298655509949, loss=1.0709786415100098
I0214 15:54:55.580336 140427534165760 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.7881999611854553, loss=1.0661368370056152
I0214 15:56:20.440682 140427525773056 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.7670037150382996, loss=1.0578114986419678
I0214 15:57:45.725020 140427534165760 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.7966710329055786, loss=1.042489767074585
I0214 15:59:10.327208 140427525773056 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8361386656761169, loss=1.0664469003677368
I0214 16:00:34.285297 140427534165760 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.0270737409591675, loss=1.0168566703796387
I0214 16:01:49.545932 140427525773056 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.0191205739974976, loss=1.0642105340957642
I0214 16:03:05.087116 140427534165760 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9275403618812561, loss=1.052817463874817
I0214 16:04:20.541968 140427525773056 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8722435832023621, loss=1.0297365188598633
I0214 16:05:35.923812 140427534165760 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9416964054107666, loss=1.0597258806228638
I0214 16:06:58.558984 140427525773056 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.8154733777046204, loss=1.1125091314315796
I0214 16:07:36.432056 140599226058560 spec.py:321] Evaluating on the training split.
I0214 16:08:31.627364 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 16:09:22.581470 140599226058560 spec.py:349] Evaluating on the test split.
I0214 16:09:48.368286 140599226058560 submission_runner.py:408] Time since start: 47421.48s, 	Step: 54147, 	{'train/ctc_loss': Array(0.16157928, dtype=float32), 'train/wer': 0.06215641642329497, 'validation/ctc_loss': Array(0.36532888, dtype=float32), 'validation/wer': 0.1073983606399104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20436308, dtype=float32), 'test/wer': 0.0686734507342636, 'test/num_examples': 2472, 'score': 43285.859830617905, 'total_duration': 47421.48486161232, 'accumulated_submission_time': 43285.859830617905, 'accumulated_eval_time': 4131.691259860992, 'accumulated_logging_time': 1.6392731666564941}
I0214 16:09:48.410294 140427242325760 logging_writer.py:48] [54147] accumulated_eval_time=4131.691260, accumulated_logging_time=1.639273, accumulated_submission_time=43285.859831, global_step=54147, preemption_count=0, score=43285.859831, test/ctc_loss=0.20436307787895203, test/num_examples=2472, test/wer=0.068673, total_duration=47421.484862, train/ctc_loss=0.16157928109169006, train/wer=0.062156, validation/ctc_loss=0.3653288781642914, validation/num_examples=5348, validation/wer=0.107398
I0214 16:10:28.939392 140427233933056 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.8487352132797241, loss=1.0570900440216064
I0214 16:11:44.281242 140427242325760 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.9285390973091125, loss=1.069727897644043
I0214 16:12:59.528011 140427233933056 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8149089217185974, loss=1.0938323736190796
I0214 16:14:18.191507 140427242325760 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.8093273043632507, loss=1.0489630699157715
I0214 16:15:45.357936 140428404565760 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.8554107546806335, loss=1.0712077617645264
I0214 16:17:00.567016 140428396173056 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.9407603740692139, loss=1.0940881967544556
I0214 16:18:15.817201 140428404565760 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.7461683750152588, loss=1.056080937385559
I0214 16:19:31.112904 140428396173056 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.8032636642456055, loss=1.0428179502487183
I0214 16:20:46.460568 140428404565760 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.8615629076957703, loss=1.076210856437683
I0214 16:22:06.849003 140428396173056 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0492485761642456, loss=1.011987566947937
I0214 16:23:31.692396 140428404565760 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.8966822624206543, loss=1.1098074913024902
I0214 16:24:56.843271 140428396173056 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.7949202060699463, loss=1.0540169477462769
I0214 16:26:21.557170 140428404565760 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.003171682357788, loss=1.1039867401123047
I0214 16:27:47.139268 140428396173056 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7699608206748962, loss=1.0233736038208008
I0214 16:29:12.940670 140428404565760 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.8234735131263733, loss=1.011419415473938
I0214 16:30:33.007092 140428404565760 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.9329308271408081, loss=1.0611923933029175
I0214 16:31:48.353028 140428396173056 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.8459838628768921, loss=1.021841049194336
I0214 16:33:03.648363 140428404565760 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.9189454913139343, loss=1.0245261192321777
I0214 16:33:48.407998 140599226058560 spec.py:321] Evaluating on the training split.
I0214 16:34:42.694315 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 16:35:33.769268 140599226058560 spec.py:349] Evaluating on the test split.
I0214 16:35:59.734765 140599226058560 submission_runner.py:408] Time since start: 48992.85s, 	Step: 55961, 	{'train/ctc_loss': Array(0.15674654, dtype=float32), 'train/wer': 0.05982924527278447, 'validation/ctc_loss': Array(0.35715374, dtype=float32), 'validation/wer': 0.10511986251774043, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19516303, dtype=float32), 'test/wer': 0.0657892064265838, 'test/num_examples': 2472, 'score': 44725.770195007324, 'total_duration': 48992.8511402607, 'accumulated_submission_time': 44725.770195007324, 'accumulated_eval_time': 4263.011621952057, 'accumulated_logging_time': 1.696920394897461}
I0214 16:35:59.776425 140428404565760 logging_writer.py:48] [55961] accumulated_eval_time=4263.011622, accumulated_logging_time=1.696920, accumulated_submission_time=44725.770195, global_step=55961, preemption_count=0, score=44725.770195, test/ctc_loss=0.19516302645206451, test/num_examples=2472, test/wer=0.065789, total_duration=48992.851140, train/ctc_loss=0.1567465364933014, train/wer=0.059829, validation/ctc_loss=0.3571537435054779, validation/num_examples=5348, validation/wer=0.105120
I0214 16:36:29.799579 140428396173056 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0251431465148926, loss=1.0195857286453247
I0214 16:37:44.877738 140428404565760 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.9071407914161682, loss=1.1381862163543701
I0214 16:39:00.131842 140428396173056 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9814143776893616, loss=1.0367547273635864
I0214 16:40:17.788476 140428404565760 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.8925827741622925, loss=1.076434850692749
I0214 16:41:42.186793 140428396173056 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.9351935386657715, loss=1.0609813928604126
I0214 16:43:08.389403 140428404565760 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.8804368376731873, loss=1.086971402168274
I0214 16:44:33.832695 140428396173056 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.8884172439575195, loss=1.0477651357650757
I0214 16:45:57.066509 140428404565760 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.9426347613334656, loss=1.0791133642196655
I0214 16:47:12.166459 140428396173056 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.8155186772346497, loss=0.9867994785308838
I0214 16:48:27.494038 140428404565760 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.9903955459594727, loss=1.0642832517623901
I0214 16:49:42.800255 140428396173056 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.8582053780555725, loss=1.0936009883880615
I0214 16:51:00.269746 140428404565760 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0169874429702759, loss=1.0909881591796875
I0214 16:52:24.307079 140428396173056 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.8881051540374756, loss=1.0880697965621948
I0214 16:53:48.950648 140428404565760 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.0805338621139526, loss=1.0180408954620361
I0214 16:55:13.071752 140428396173056 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.8627995848655701, loss=0.9567117094993591
I0214 16:56:38.469012 140428404565760 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.905022382736206, loss=1.0258959531784058
I0214 16:58:03.179012 140428396173056 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.8803285360336304, loss=1.0452826023101807
I0214 16:59:30.687636 140428076885760 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.9207276105880737, loss=0.9973475933074951
I0214 17:00:00.415781 140599226058560 spec.py:321] Evaluating on the training split.
I0214 17:00:54.294605 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 17:01:45.441115 140599226058560 spec.py:349] Evaluating on the test split.
I0214 17:02:11.958934 140599226058560 submission_runner.py:408] Time since start: 50565.08s, 	Step: 57741, 	{'train/ctc_loss': Array(0.14496757, dtype=float32), 'train/wer': 0.05509454796411318, 'validation/ctc_loss': Array(0.3530484, dtype=float32), 'validation/wer': 0.10409646929337595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19236514, dtype=float32), 'test/wer': 0.06357524424674507, 'test/num_examples': 2472, 'score': 46166.32611012459, 'total_duration': 50565.07560944557, 'accumulated_submission_time': 46166.32611012459, 'accumulated_eval_time': 4394.548684120178, 'accumulated_logging_time': 1.7538185119628906}
I0214 17:02:12.002998 140427605845760 logging_writer.py:48] [57741] accumulated_eval_time=4394.548684, accumulated_logging_time=1.753819, accumulated_submission_time=46166.326110, global_step=57741, preemption_count=0, score=46166.326110, test/ctc_loss=0.1923651397228241, test/num_examples=2472, test/wer=0.063575, total_duration=50565.075609, train/ctc_loss=0.14496757090091705, train/wer=0.055095, validation/ctc_loss=0.3530484139919281, validation/num_examples=5348, validation/wer=0.104096
I0214 17:02:57.052301 140427597453056 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.8823634386062622, loss=1.0151411294937134
I0214 17:04:12.314119 140427605845760 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.028959035873413, loss=1.0298911333084106
I0214 17:05:27.547337 140427597453056 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1677591800689697, loss=0.9935299158096313
I0214 17:06:42.792112 140427605845760 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.1445978879928589, loss=1.019437551498413
I0214 17:07:57.930116 140427597453056 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.8724710941314697, loss=1.0232815742492676
I0214 17:09:20.303168 140427605845760 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.8905097842216492, loss=1.041103482246399
I0214 17:10:45.612314 140427597453056 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.8917261958122253, loss=0.9680069088935852
I0214 17:12:10.676893 140427605845760 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.7989872097969055, loss=1.0050827264785767
I0214 17:13:35.974880 140427597453056 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.9273192286491394, loss=1.0738874673843384
I0214 17:15:02.049549 140427605845760 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8203491568565369, loss=1.0438164472579956
I0214 17:16:22.085847 140427605845760 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9416906833648682, loss=0.9993247985839844
I0214 17:17:37.457969 140427597453056 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.003088355064392, loss=1.0163369178771973
I0214 17:18:52.974889 140427605845760 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9334137439727783, loss=1.019394874572754
I0214 17:20:08.243485 140427597453056 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0804497003555298, loss=1.0298113822937012
I0214 17:21:28.066354 140427605845760 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.8792920708656311, loss=0.9788263440132141
I0214 17:22:52.749214 140427597453056 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.9078601002693176, loss=0.9931952357292175
I0214 17:24:18.655227 140427605845760 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9621279239654541, loss=1.0077857971191406
I0214 17:25:44.003794 140427597453056 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9544560313224792, loss=0.9211779236793518
I0214 17:26:12.379092 140599226058560 spec.py:321] Evaluating on the training split.
I0214 17:27:05.629441 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 17:27:57.316137 140599226058560 spec.py:349] Evaluating on the test split.
I0214 17:28:23.163609 140599226058560 submission_runner.py:408] Time since start: 52136.28s, 	Step: 59535, 	{'train/ctc_loss': Array(0.1456565, dtype=float32), 'train/wer': 0.05550976958208853, 'validation/ctc_loss': Array(0.3391099, dtype=float32), 'validation/wer': 0.10038908251831971, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1857698, dtype=float32), 'test/wer': 0.061056608372433124, 'test/num_examples': 2472, 'score': 47606.610372543335, 'total_duration': 52136.28013706207, 'accumulated_submission_time': 47606.610372543335, 'accumulated_eval_time': 4525.326953172684, 'accumulated_logging_time': 1.8184218406677246}
I0214 17:28:23.210208 140427308881664 logging_writer.py:48] [59535] accumulated_eval_time=4525.326953, accumulated_logging_time=1.818422, accumulated_submission_time=47606.610373, global_step=59535, preemption_count=0, score=47606.610373, test/ctc_loss=0.18576979637145996, test/num_examples=2472, test/wer=0.061057, total_duration=52136.280137, train/ctc_loss=0.1456564962863922, train/wer=0.055510, validation/ctc_loss=0.3391098976135254, validation/num_examples=5348, validation/wer=0.100389
I0214 17:29:12.822549 140427300488960 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9129520058631897, loss=0.9803317189216614
I0214 17:30:28.114982 140427308881664 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.8467993140220642, loss=1.0204964876174927
I0214 17:31:46.971368 140427308881664 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.8499305248260498, loss=0.9705929160118103
I0214 17:33:02.674397 140427300488960 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.036543846130371, loss=0.9810052514076233
I0214 17:34:18.151674 140427308881664 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0713096857070923, loss=0.9754390120506287
I0214 17:35:33.717141 140427300488960 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9741208553314209, loss=1.0034111738204956
I0214 17:36:53.103435 140427308881664 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.0591521263122559, loss=0.9689691662788391
I0214 17:38:17.586458 140427300488960 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.84941565990448, loss=0.9990295767784119
I0214 17:39:42.943167 140427308881664 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0414904356002808, loss=0.9755014181137085
I0214 17:41:09.014707 140427300488960 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9648362398147583, loss=1.0065275430679321
I0214 17:42:34.362515 140427308881664 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9560725092887878, loss=1.0088413953781128
I0214 17:43:59.247851 140427300488960 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9372726678848267, loss=0.9935268759727478
I0214 17:45:24.997204 140427308881664 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.8407300710678101, loss=0.9559388160705566
I0214 17:46:40.487234 140427300488960 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.9844163060188293, loss=0.9395888447761536
I0214 17:47:55.871106 140427308881664 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0068178176879883, loss=0.9938381314277649
I0214 17:49:11.273085 140427300488960 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.9480136036872864, loss=0.974572479724884
I0214 17:50:26.572788 140427308881664 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.9132102727890015, loss=0.9976822137832642
I0214 17:51:49.815648 140427300488960 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.9197553992271423, loss=0.9767846465110779
I0214 17:52:23.732413 140599226058560 spec.py:321] Evaluating on the training split.
I0214 17:53:18.818837 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 17:54:10.873125 140599226058560 spec.py:349] Evaluating on the test split.
I0214 17:54:37.135683 140599226058560 submission_runner.py:408] Time since start: 53710.25s, 	Step: 61341, 	{'train/ctc_loss': Array(0.1419333, dtype=float32), 'train/wer': 0.053888628715189665, 'validation/ctc_loss': Array(0.33404234, dtype=float32), 'validation/wer': 0.09824574953899032, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1807087, dtype=float32), 'test/wer': 0.059594174638961675, 'test/num_examples': 2472, 'score': 49047.04236245155, 'total_duration': 53710.25221323967, 'accumulated_submission_time': 49047.04236245155, 'accumulated_eval_time': 4658.723962068558, 'accumulated_logging_time': 1.8841652870178223}
I0214 17:54:37.178751 140427308881664 logging_writer.py:48] [61341] accumulated_eval_time=4658.723962, accumulated_logging_time=1.884165, accumulated_submission_time=49047.042362, global_step=61341, preemption_count=0, score=49047.042362, test/ctc_loss=0.18070870637893677, test/num_examples=2472, test/wer=0.059594, total_duration=53710.252213, train/ctc_loss=0.14193330705165863, train/wer=0.053889, validation/ctc_loss=0.33404234051704407, validation/num_examples=5348, validation/wer=0.098246
I0214 17:55:22.230941 140427300488960 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.337870478630066, loss=0.9670860767364502
I0214 17:56:37.549359 140427308881664 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.0785056352615356, loss=0.9798936247825623
I0214 17:57:52.780466 140427300488960 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.0022282600402832, loss=0.9521483182907104
I0214 17:59:09.876444 140427308881664 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.8664573431015015, loss=0.9357247352600098
I0214 18:00:37.921615 140427308881664 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.3229997158050537, loss=0.9835384488105774
I0214 18:01:53.187541 140427300488960 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0413168668746948, loss=0.9979813098907471
I0214 18:03:08.827658 140427308881664 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2199370861053467, loss=0.9803175330162048
I0214 18:04:24.365427 140427300488960 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.9159051775932312, loss=0.9497696161270142
I0214 18:05:39.813695 140427308881664 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.9753090143203735, loss=0.9176092743873596
I0214 18:07:00.311057 140427300488960 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0690799951553345, loss=0.9886191487312317
I0214 18:08:25.160030 140427308881664 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.2293221950531006, loss=0.9648634195327759
I0214 18:09:50.100189 140427300488960 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.937239408493042, loss=0.9996473789215088
I0214 18:11:15.752002 140427308881664 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0516343116760254, loss=0.9741834998130798
I0214 18:12:40.534959 140427300488960 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.050506591796875, loss=1.0108340978622437
I0214 18:14:05.522115 140427308881664 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.8915008306503296, loss=0.9413467645645142
I0214 18:15:26.740742 140427308881664 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1224191188812256, loss=0.9565504789352417
I0214 18:16:42.198132 140427300488960 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0000430345535278, loss=0.961683988571167
I0214 18:17:57.441061 140427308881664 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.0293927192687988, loss=0.966492235660553
I0214 18:18:37.681977 140599226058560 spec.py:321] Evaluating on the training split.
I0214 18:19:31.346077 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 18:20:23.227047 140599226058560 spec.py:349] Evaluating on the test split.
I0214 18:20:49.206364 140599226058560 submission_runner.py:408] Time since start: 55282.32s, 	Step: 63155, 	{'train/ctc_loss': Array(0.11702894, dtype=float32), 'train/wer': 0.04568229201694801, 'validation/ctc_loss': Array(0.32601267, dtype=float32), 'validation/wer': 0.09507902333529644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17809816, dtype=float32), 'test/wer': 0.058152052485121765, 'test/num_examples': 2472, 'score': 50487.46106958389, 'total_duration': 55282.32294297218, 'accumulated_submission_time': 50487.46106958389, 'accumulated_eval_time': 4790.242144107819, 'accumulated_logging_time': 1.94150710105896}
I0214 18:20:49.247120 140428404565760 logging_writer.py:48] [63155] accumulated_eval_time=4790.242144, accumulated_logging_time=1.941507, accumulated_submission_time=50487.461070, global_step=63155, preemption_count=0, score=50487.461070, test/ctc_loss=0.1780981570482254, test/num_examples=2472, test/wer=0.058152, total_duration=55282.322943, train/ctc_loss=0.11702893674373627, train/wer=0.045682, validation/ctc_loss=0.32601267099380493, validation/num_examples=5348, validation/wer=0.095079
I0214 18:21:23.829128 140428396173056 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.012284755706787, loss=0.9347846508026123
I0214 18:22:39.061287 140428404565760 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0592199563980103, loss=0.9634792804718018
I0214 18:23:54.327598 140428396173056 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.1341441869735718, loss=1.0009963512420654
I0214 18:25:11.240515 140428404565760 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.0025291442871094, loss=0.9311140179634094
I0214 18:26:35.360859 140428396173056 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.0164964199066162, loss=0.9358828663825989
I0214 18:28:00.445898 140428404565760 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.616896629333496, loss=0.9656655788421631
I0214 18:29:25.515024 140428396173056 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.5913037061691284, loss=0.9676634669303894
I0214 18:30:49.835232 140427534165760 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0781906843185425, loss=0.9399785399436951
I0214 18:32:05.056752 140427525773056 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.2980555295944214, loss=0.9314037561416626
I0214 18:33:20.636124 140427534165760 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.015279769897461, loss=0.9106612801551819
I0214 18:34:35.963824 140427525773056 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0779247283935547, loss=0.9188863039016724
I0214 18:35:52.059034 140427534165760 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1484780311584473, loss=0.9106014966964722
I0214 18:37:16.399701 140427525773056 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.0523031949996948, loss=0.9216064810752869
I0214 18:38:42.058821 140427534165760 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.0455830097198486, loss=0.9681267738342285
I0214 18:40:07.739634 140427525773056 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.0897250175476074, loss=0.994149386882782
I0214 18:41:32.481411 140427534165760 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.9608085751533508, loss=0.9644007086753845
I0214 18:42:57.604313 140427525773056 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.045102596282959, loss=0.9525152444839478
I0214 18:44:25.293049 140427534165760 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2878341674804688, loss=0.915148138999939
I0214 18:44:49.747454 140599226058560 spec.py:321] Evaluating on the training split.
I0214 18:45:43.219125 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 18:46:34.492761 140599226058560 spec.py:349] Evaluating on the test split.
I0214 18:47:00.711308 140599226058560 submission_runner.py:408] Time since start: 56853.83s, 	Step: 64934, 	{'train/ctc_loss': Array(0.11297734, dtype=float32), 'train/wer': 0.04389918305873762, 'validation/ctc_loss': Array(0.3239654, dtype=float32), 'validation/wer': 0.09357289745792985, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16898389, dtype=float32), 'test/wer': 0.055409989234862796, 'test/num_examples': 2472, 'score': 51927.87755322456, 'total_duration': 56853.82898592949, 'accumulated_submission_time': 51927.87755322456, 'accumulated_eval_time': 4921.200897216797, 'accumulated_logging_time': 1.9962365627288818}
I0214 18:47:00.758680 140427897685760 logging_writer.py:48] [64934] accumulated_eval_time=4921.200897, accumulated_logging_time=1.996237, accumulated_submission_time=51927.877553, global_step=64934, preemption_count=0, score=51927.877553, test/ctc_loss=0.16898389160633087, test/num_examples=2472, test/wer=0.055410, total_duration=56853.828986, train/ctc_loss=0.11297734081745148, train/wer=0.043899, validation/ctc_loss=0.3239654004573822, validation/num_examples=5348, validation/wer=0.093573
I0214 18:47:51.100245 140427889293056 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.9246649742126465, loss=0.8935961127281189
I0214 18:49:06.342337 140427897685760 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.9964449405670166, loss=0.9105684161186218
I0214 18:50:21.824770 140427889293056 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1485919952392578, loss=0.9494590163230896
I0214 18:51:37.249185 140427897685760 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.0672012567520142, loss=0.9449838399887085
I0214 18:52:52.533856 140427889293056 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1557786464691162, loss=0.961382269859314
I0214 18:54:11.541772 140427897685760 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.1730594635009766, loss=0.9048700928688049
I0214 18:55:37.153555 140427889293056 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.3592761754989624, loss=0.9388465285301208
I0214 18:57:02.473578 140427897685760 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.9901805520057678, loss=0.9559710025787354
I0214 18:58:28.741312 140427889293056 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.0248312950134277, loss=0.9706854820251465
I0214 18:59:53.778373 140427897685760 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.167402744293213, loss=0.9548362493515015
I0214 19:01:14.087953 140427897685760 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.129510521888733, loss=0.8683918714523315
I0214 19:02:29.364313 140427889293056 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.0878785848617554, loss=0.9786239266395569
I0214 19:03:44.819472 140427897685760 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.1505482196807861, loss=0.949904203414917
I0214 19:05:00.164479 140427889293056 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.103128433227539, loss=0.9028372168540955
I0214 19:06:16.987129 140427897685760 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.4291685819625854, loss=0.8859699964523315
I0214 19:07:41.541237 140427889293056 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2682323455810547, loss=0.8785719275474548
I0214 19:09:06.841112 140427897685760 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.9620182514190674, loss=0.9125284552574158
I0214 19:10:31.403128 140427889293056 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.0831935405731201, loss=0.8603346347808838
I0214 19:11:01.074883 140599226058560 spec.py:321] Evaluating on the training split.
I0214 19:11:54.922136 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 19:12:46.711557 140599226058560 spec.py:349] Evaluating on the test split.
I0214 19:13:12.805458 140599226058560 submission_runner.py:408] Time since start: 58425.92s, 	Step: 66736, 	{'train/ctc_loss': Array(0.10090725, dtype=float32), 'train/wer': 0.03881734589117529, 'validation/ctc_loss': Array(0.31437576, dtype=float32), 'validation/wer': 0.09140060051942034, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16851163, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 53368.10543203354, 'total_duration': 58425.920551776886, 'accumulated_submission_time': 53368.10543203354, 'accumulated_eval_time': 5052.923782587051, 'accumulated_logging_time': 2.061052083969116}
I0214 19:13:12.851404 140427605845760 logging_writer.py:48] [66736] accumulated_eval_time=5052.923783, accumulated_logging_time=2.061052, accumulated_submission_time=53368.105432, global_step=66736, preemption_count=0, score=53368.105432, test/ctc_loss=0.16851162910461426, test/num_examples=2472, test/wer=0.055816, total_duration=58425.920552, train/ctc_loss=0.10090725123882294, train/wer=0.038817, validation/ctc_loss=0.31437575817108154, validation/num_examples=5348, validation/wer=0.091401
I0214 19:14:01.706889 140427597453056 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.9809439182281494, loss=0.9146418571472168
I0214 19:15:16.971215 140427605845760 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.1551332473754883, loss=0.901714026927948
I0214 19:16:35.886798 140427278165760 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.9711654186248779, loss=0.887238085269928
I0214 19:17:51.275272 140427269773056 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.2076058387756348, loss=0.9450691938400269
I0214 19:19:06.711170 140427278165760 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.0389117002487183, loss=0.9500024318695068
I0214 19:20:22.503746 140427269773056 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.200424075126648, loss=0.8983491659164429
I0214 19:21:40.742856 140427278165760 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3413903713226318, loss=0.8484535217285156
I0214 19:23:05.343001 140427269773056 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3006092309951782, loss=0.9490264058113098
I0214 19:24:30.117602 140427278165760 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.219064712524414, loss=0.902323842048645
I0214 19:25:55.418367 140427269773056 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.0593147277832031, loss=0.9071314334869385
I0214 19:27:21.602771 140427278165760 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.0417639017105103, loss=0.9188884496688843
I0214 19:28:48.304225 140427269773056 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2912660837173462, loss=0.9260692000389099
I0214 19:30:14.640424 140427605845760 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1038422584533691, loss=0.8669881820678711
I0214 19:31:29.816691 140427597453056 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.9520313143730164, loss=0.9217618107795715
I0214 19:32:44.945942 140427605845760 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.2826323509216309, loss=0.8696329593658447
I0214 19:34:00.271991 140427597453056 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.2639020681381226, loss=0.8678953051567078
I0214 19:35:15.772377 140427605845760 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2752920389175415, loss=0.9020915627479553
I0214 19:36:38.080476 140427597453056 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.166878342628479, loss=0.8823908567428589
I0214 19:37:13.192182 140599226058560 spec.py:321] Evaluating on the training split.
I0214 19:38:06.204063 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 19:38:57.420147 140599226058560 spec.py:349] Evaluating on the test split.
I0214 19:39:23.131953 140599226058560 submission_runner.py:408] Time since start: 59996.25s, 	Step: 68543, 	{'train/ctc_loss': Array(0.09813993, dtype=float32), 'train/wer': 0.03740100320547493, 'validation/ctc_loss': Array(0.3097387, dtype=float32), 'validation/wer': 0.08974000019309306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16452241, dtype=float32), 'test/wer': 0.054231917616233014, 'test/num_examples': 2472, 'score': 54808.36094260216, 'total_duration': 59996.24846315384, 'accumulated_submission_time': 54808.36094260216, 'accumulated_eval_time': 5182.85727763176, 'accumulated_logging_time': 2.1215648651123047}
I0214 19:39:23.178263 140427605845760 logging_writer.py:48] [68543] accumulated_eval_time=5182.857278, accumulated_logging_time=2.121565, accumulated_submission_time=54808.360943, global_step=68543, preemption_count=0, score=54808.360943, test/ctc_loss=0.16452240943908691, test/num_examples=2472, test/wer=0.054232, total_duration=59996.248463, train/ctc_loss=0.0981399267911911, train/wer=0.037401, validation/ctc_loss=0.3097386956214905, validation/num_examples=5348, validation/wer=0.089740
I0214 19:40:06.622338 140427597453056 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.2370333671569824, loss=0.8923282623291016
I0214 19:41:21.893589 140427605845760 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.8566534519195557, loss=0.8782921433448792
I0214 19:42:37.227025 140427597453056 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.067082166671753, loss=0.9134939312934875
I0214 19:43:54.387086 140427605845760 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.0224356651306152, loss=0.895564615726471
I0214 19:45:19.438072 140427597453056 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1595826148986816, loss=0.9126392006874084
I0214 19:46:38.708568 140427278165760 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3206560611724854, loss=0.8964236378669739
I0214 19:47:53.953859 140427269773056 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.2264443635940552, loss=0.8532629013061523
I0214 19:49:09.249645 140427278165760 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.1787391901016235, loss=0.8939322233200073
I0214 19:50:24.752574 140427269773056 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.0707083940505981, loss=0.9005836844444275
I0214 19:51:46.812593 140427278165760 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.4279581308364868, loss=0.8810173273086548
I0214 19:53:11.704184 140427269773056 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.0599809885025024, loss=0.8485562205314636
I0214 19:54:37.280669 140427278165760 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.362148642539978, loss=0.9021188616752625
I0214 19:56:02.882126 140427269773056 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1648125648498535, loss=0.919694721698761
I0214 19:57:28.473167 140427278165760 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.0588622093200684, loss=0.89780592918396
I0214 19:58:53.248538 140427269773056 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1688401699066162, loss=0.9109309911727905
I0214 20:00:15.660128 140427605845760 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1958870887756348, loss=0.8916730284690857
I0214 20:01:31.054158 140427597453056 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.09028160572052, loss=0.874866247177124
I0214 20:02:46.451849 140427605845760 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.0989238023757935, loss=0.9181095361709595
I0214 20:03:23.829781 140599226058560 spec.py:321] Evaluating on the training split.
I0214 20:04:17.619278 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 20:05:09.621559 140599226058560 spec.py:349] Evaluating on the test split.
I0214 20:05:35.461349 140599226058560 submission_runner.py:408] Time since start: 61568.58s, 	Step: 70351, 	{'train/ctc_loss': Array(0.08928266, dtype=float32), 'train/wer': 0.03431273898168646, 'validation/ctc_loss': Array(0.30275312, dtype=float32), 'validation/wer': 0.08661189260163936, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1618215, dtype=float32), 'test/wer': 0.05248512176791989, 'test/num_examples': 2472, 'score': 56248.92697405815, 'total_duration': 61568.578528642654, 'accumulated_submission_time': 56248.92697405815, 'accumulated_eval_time': 5314.483243465424, 'accumulated_logging_time': 2.182187557220459}
I0214 20:05:35.509445 140427605845760 logging_writer.py:48] [70351] accumulated_eval_time=5314.483243, accumulated_logging_time=2.182188, accumulated_submission_time=56248.926974, global_step=70351, preemption_count=0, score=56248.926974, test/ctc_loss=0.16182149946689606, test/num_examples=2472, test/wer=0.052485, total_duration=61568.578529, train/ctc_loss=0.08928266167640686, train/wer=0.034313, validation/ctc_loss=0.30275312066078186, validation/num_examples=5348, validation/wer=0.086612
I0214 20:06:13.057202 140427597453056 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.293332815170288, loss=0.9207161068916321
I0214 20:07:28.516307 140427605845760 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1131820678710938, loss=0.8708071708679199
I0214 20:08:43.742555 140427597453056 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.1209323406219482, loss=0.8441953659057617
I0214 20:09:59.425527 140427605845760 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0881234407424927, loss=0.8400307297706604
I0214 20:11:24.048028 140427597453056 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3747278451919556, loss=0.8425957560539246
I0214 20:12:50.001044 140427605845760 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.0678162574768066, loss=0.8659272789955139
I0214 20:14:14.243735 140427597453056 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.398360252380371, loss=0.8848963975906372
I0214 20:15:40.116547 140427605845760 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.1364655494689941, loss=0.8858464956283569
I0214 20:16:55.401571 140427597453056 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.9850198030471802, loss=0.8291258811950684
I0214 20:18:10.760080 140427605845760 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.297078013420105, loss=0.8715832233428955
I0214 20:19:26.125420 140427597453056 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0609201192855835, loss=0.8570777177810669
I0214 20:20:41.511958 140427605845760 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.349371314048767, loss=0.8533320426940918
I0214 20:22:03.765634 140427597453056 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.900278925895691, loss=0.9259356260299683
I0214 20:23:29.836160 140427605845760 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3541688919067383, loss=0.877260684967041
I0214 20:24:55.212642 140427597453056 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.1917511224746704, loss=0.8853033185005188
I0214 20:26:20.218041 140427605845760 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.304847002029419, loss=0.9432052969932556
I0214 20:27:45.761773 140427597453056 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.1369760036468506, loss=0.8740374445915222
I0214 20:29:14.065461 140427605845760 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3097507953643799, loss=0.8579078912734985
I0214 20:29:35.559429 140599226058560 spec.py:321] Evaluating on the training split.
I0214 20:30:29.769554 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 20:31:21.427147 140599226058560 spec.py:349] Evaluating on the test split.
I0214 20:31:47.271258 140599226058560 submission_runner.py:408] Time since start: 63140.39s, 	Step: 72130, 	{'train/ctc_loss': Array(0.08532207, dtype=float32), 'train/wer': 0.03239874699755377, 'validation/ctc_loss': Array(0.29723024, dtype=float32), 'validation/wer': 0.08538575166301399, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15875757, dtype=float32), 'test/wer': 0.05124611541039547, 'test/num_examples': 2472, 'score': 57688.89278316498, 'total_duration': 63140.387882232666, 'accumulated_submission_time': 57688.89278316498, 'accumulated_eval_time': 5446.1889128685, 'accumulated_logging_time': 2.245168685913086}
I0214 20:31:47.319787 140427605845760 logging_writer.py:48] [72130] accumulated_eval_time=5446.188913, accumulated_logging_time=2.245169, accumulated_submission_time=57688.892783, global_step=72130, preemption_count=0, score=57688.892783, test/ctc_loss=0.15875756740570068, test/num_examples=2472, test/wer=0.051246, total_duration=63140.387882, train/ctc_loss=0.0853220671415329, train/wer=0.032399, validation/ctc_loss=0.29723024368286133, validation/num_examples=5348, validation/wer=0.085386
I0214 20:32:40.538368 140427597453056 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.1524137258529663, loss=0.8298739790916443
I0214 20:33:55.708776 140427605845760 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3058929443359375, loss=0.8800991773605347
I0214 20:35:10.908852 140427597453056 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.0887118577957153, loss=0.9103268384933472
I0214 20:36:26.177125 140427605845760 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2815418243408203, loss=0.869644820690155
I0214 20:37:41.625188 140427597453056 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.5888808965682983, loss=0.8680384755134583
I0214 20:38:58.384900 140427605845760 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.786537766456604, loss=0.8520990014076233
I0214 20:40:22.258870 140427597453056 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.1144871711730957, loss=0.8432070016860962
I0214 20:41:47.668108 140427605845760 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.9854174256324768, loss=0.8660060167312622
I0214 20:43:13.046925 140427597453056 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.424859881401062, loss=0.9055294394493103
I0214 20:44:38.392745 140427605845760 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.618032455444336, loss=0.8507716655731201
I0214 20:45:59.829233 140427605845760 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.249851107597351, loss=0.8145712018013
I0214 20:47:15.373002 140427597453056 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.0887351036071777, loss=0.9098061919212341
I0214 20:48:30.765061 140427605845760 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.0270339250564575, loss=0.8371008634567261
I0214 20:49:46.284174 140427597453056 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.1463614702224731, loss=0.7854282855987549
I0214 20:51:05.715247 140427605845760 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.34250009059906, loss=0.8747684359550476
I0214 20:52:30.724141 140427597453056 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0507348775863647, loss=0.8162659406661987
I0214 20:53:55.280302 140427605845760 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.2230782508850098, loss=0.8798583745956421
I0214 20:55:20.763027 140427597453056 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.284912347793579, loss=0.8204238414764404
I0214 20:55:47.392184 140599226058560 spec.py:321] Evaluating on the training split.
I0214 20:56:40.780031 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 20:57:31.654377 140599226058560 spec.py:349] Evaluating on the test split.
I0214 20:57:57.718078 140599226058560 submission_runner.py:408] Time since start: 64710.84s, 	Step: 73933, 	{'train/ctc_loss': Array(0.07010609, dtype=float32), 'train/wer': 0.02700242546912191, 'validation/ctc_loss': Array(0.29594654, dtype=float32), 'validation/wer': 0.08368653272444655, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15754385, dtype=float32), 'test/wer': 0.05116486909186928, 'test/num_examples': 2472, 'score': 59128.87972784042, 'total_duration': 64710.83512163162, 'accumulated_submission_time': 59128.87972784042, 'accumulated_eval_time': 5576.509063482285, 'accumulated_logging_time': 2.3098487854003906}
I0214 20:57:57.761798 140427308881664 logging_writer.py:48] [73933] accumulated_eval_time=5576.509063, accumulated_logging_time=2.309849, accumulated_submission_time=59128.879728, global_step=73933, preemption_count=0, score=59128.879728, test/ctc_loss=0.1575438529253006, test/num_examples=2472, test/wer=0.051165, total_duration=64710.835122, train/ctc_loss=0.07010608911514282, train/wer=0.027002, validation/ctc_loss=0.29594653844833374, validation/num_examples=5348, validation/wer=0.083687
I0214 20:58:48.921907 140427300488960 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.1610496044158936, loss=0.8786777257919312
I0214 21:00:04.401726 140427308881664 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.2510168552398682, loss=0.8945134878158569
I0214 21:01:23.415071 140427308881664 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.1672747135162354, loss=0.8449276089668274
I0214 21:02:38.710294 140427300488960 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.3827011585235596, loss=0.8910396099090576
I0214 21:03:54.093488 140427308881664 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.2476773262023926, loss=0.8861689567565918
I0214 21:05:09.493768 140427300488960 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.396785020828247, loss=0.8099342584609985
I0214 21:06:27.494932 140427308881664 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2228665351867676, loss=0.8466839790344238
I0214 21:07:53.638648 140427300488960 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.215226173400879, loss=0.8997658491134644
I0214 21:09:20.189574 140427308881664 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1267621517181396, loss=0.842126190662384
I0214 21:10:45.733026 140427300488960 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1420115232467651, loss=0.846513569355011
I0214 21:12:10.803269 140427308881664 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.061942458152771, loss=0.8293541073799133
I0214 21:13:35.757369 140427300488960 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.2456756830215454, loss=0.8748423457145691
I0214 21:15:03.474887 140427308881664 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1552224159240723, loss=0.9075517058372498
I0214 21:16:18.834114 140427300488960 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.2331173419952393, loss=0.9079381823539734
I0214 21:17:34.138876 140427308881664 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.0296392440795898, loss=0.8087959289550781
I0214 21:18:49.563168 140427300488960 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1612974405288696, loss=0.8402429819107056
I0214 21:20:04.961271 140427308881664 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.404388189315796, loss=0.867861807346344
I0214 21:21:26.849452 140427300488960 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.2411091327667236, loss=0.8788847327232361
I0214 21:21:58.587321 140599226058560 spec.py:321] Evaluating on the training split.
I0214 21:22:51.878187 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 21:23:43.085373 140599226058560 spec.py:349] Evaluating on the test split.
I0214 21:24:09.040547 140599226058560 submission_runner.py:408] Time since start: 66282.16s, 	Step: 75739, 	{'train/ctc_loss': Array(0.06693444, dtype=float32), 'train/wer': 0.025029311311617473, 'validation/ctc_loss': Array(0.2931396, dtype=float32), 'validation/wer': 0.08405340954072815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15539056, dtype=float32), 'test/wer': 0.04945869640281925, 'test/num_examples': 2472, 'score': 60569.61856889725, 'total_duration': 66282.15744042397, 'accumulated_submission_time': 60569.61856889725, 'accumulated_eval_time': 5706.956416845322, 'accumulated_logging_time': 2.3700406551361084}
I0214 21:24:09.084429 140427682645760 logging_writer.py:48] [75739] accumulated_eval_time=5706.956417, accumulated_logging_time=2.370041, accumulated_submission_time=60569.618569, global_step=75739, preemption_count=0, score=60569.618569, test/ctc_loss=0.15539056062698364, test/num_examples=2472, test/wer=0.049459, total_duration=66282.157440, train/ctc_loss=0.06693444401025772, train/wer=0.025029, validation/ctc_loss=0.29313960671424866, validation/num_examples=5348, validation/wer=0.084053
I0214 21:24:56.012148 140427674253056 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.15171480178833, loss=0.8525676727294922
I0214 21:26:11.537985 140427682645760 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1022316217422485, loss=0.8733091950416565
I0214 21:27:27.061798 140427674253056 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.953770637512207, loss=0.8270698189735413
I0214 21:28:46.273650 140427682645760 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1694806814193726, loss=0.9139054417610168
I0214 21:30:10.564039 140427674253056 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.1093173027038574, loss=0.867432713508606
I0214 21:31:30.909764 140427354965760 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.1363078355789185, loss=0.8568288683891296
I0214 21:32:27.660200 140427346573056 logging_writer.py:48] [76377] global_step=76377, preemption_count=0, score=61068.133901
I0214 21:32:28.587625 140599226058560 checkpoints.py:490] Saving checkpoint at step: 76377
I0214 21:32:30.066023 140599226058560 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_1/checkpoint_76377
I0214 21:32:30.105816 140599226058560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_1/checkpoint_76377.
I0214 21:32:33.080984 140599226058560 submission_runner.py:583] Tuning trial 1/5
I0214 21:32:33.081261 140599226058560 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0214 21:32:33.107615 140599226058560 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.12085, dtype=float32), 'train/wer': 1.3685577743997668, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 75.61166572570801, 'total_duration': 253.83884978294373, 'accumulated_submission_time': 75.61166572570801, 'accumulated_eval_time': 178.22711658477783, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1844, {'train/ctc_loss': Array(6.1664042, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.2940016, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.2753453, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1515.929588317871, 'total_duration': 1799.9081389904022, 'accumulated_submission_time': 1515.929588317871, 'accumulated_eval_time': 283.8664665222168, 'accumulated_logging_time': 0.040500640869140625, 'global_step': 1844, 'preemption_count': 0}), (3701, {'train/ctc_loss': Array(2.3516572, dtype=float32), 'train/wer': 0.5535412726740004, 'validation/ctc_loss': Array(2.7404253, dtype=float32), 'validation/wer': 0.5721250856850459, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.4001753, dtype=float32), 'test/wer': 0.5354335506672354, 'test/num_examples': 2472, 'score': 2956.9796600341797, 'total_duration': 3370.4715399742126, 'accumulated_submission_time': 2956.9796600341797, 'accumulated_eval_time': 413.2503607273102, 'accumulated_logging_time': 0.09172964096069336, 'global_step': 3701, 'preemption_count': 0}), (5551, {'train/ctc_loss': Array(0.7146427, dtype=float32), 'train/wer': 0.2437602458476314, 'validation/ctc_loss': Array(1.0680373, dtype=float32), 'validation/wer': 0.31081224596194135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.78583676, dtype=float32), 'test/wer': 0.25501188227408444, 'test/num_examples': 2472, 'score': 4397.298481225967, 'total_duration': 4942.37772154808, 'accumulated_submission_time': 4397.298481225967, 'accumulated_eval_time': 544.7035682201385, 'accumulated_logging_time': 0.14594745635986328, 'global_step': 5551, 'preemption_count': 0}), (7377, {'train/ctc_loss': Array(0.5079412, dtype=float32), 'train/wer': 0.17687377603703044, 'validation/ctc_loss': Array(0.821356, dtype=float32), 'validation/wer': 0.24684051478610117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5604038, dtype=float32), 'test/wer': 0.18680559787134646, 'test/num_examples': 2472, 'score': 5837.907940626144, 'total_duration': 6514.627334594727, 'accumulated_submission_time': 5837.907940626144, 'accumulated_eval_time': 676.2148461341858, 'accumulated_logging_time': 0.19725966453552246, 'global_step': 7377, 'preemption_count': 0}), (9199, {'train/ctc_loss': Array(0.43121392, dtype=float32), 'train/wer': 0.1545967513970732, 'validation/ctc_loss': Array(0.7380511, dtype=float32), 'validation/wer': 0.22283904727883602, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48268652, dtype=float32), 'test/wer': 0.16399569394511812, 'test/num_examples': 2472, 'score': 7277.963893651962, 'total_duration': 8086.693351507187, 'accumulated_submission_time': 7277.963893651962, 'accumulated_eval_time': 808.0982096195221, 'accumulated_logging_time': 0.24878954887390137, 'global_step': 9199, 'preemption_count': 0}), (11020, {'train/ctc_loss': Array(0.41369617, dtype=float32), 'train/wer': 0.14623021482947543, 'validation/ctc_loss': Array(0.6845674, dtype=float32), 'validation/wer': 0.20788398968883054, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4419891, dtype=float32), 'test/wer': 0.15097597140129587, 'test/num_examples': 2472, 'score': 8718.414444446564, 'total_duration': 9660.191576719284, 'accumulated_submission_time': 8718.414444446564, 'accumulated_eval_time': 941.0179760456085, 'accumulated_logging_time': 0.3003580570220947, 'global_step': 11020, 'preemption_count': 0}), (12857, {'train/ctc_loss': Array(0.35117, dtype=float32), 'train/wer': 0.12689043456065344, 'validation/ctc_loss': Array(0.6366001, dtype=float32), 'validation/wer': 0.19441574867007155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.402794, dtype=float32), 'test/wer': 0.1361485182702659, 'test/num_examples': 2472, 'score': 10159.513425588608, 'total_duration': 11235.706638336182, 'accumulated_submission_time': 10159.513425588608, 'accumulated_eval_time': 1075.3007299900055, 'accumulated_logging_time': 0.35523295402526855, 'global_step': 12857, 'preemption_count': 0}), (14670, {'train/ctc_loss': Array(0.301788, dtype=float32), 'train/wer': 0.11338828362671936, 'validation/ctc_loss': Array(0.6053062, dtype=float32), 'validation/wer': 0.1848866060998098, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38013542, dtype=float32), 'test/wer': 0.12979099384559137, 'test/num_examples': 2472, 'score': 11599.84973692894, 'total_duration': 12809.905858516693, 'accumulated_submission_time': 11599.84973692894, 'accumulated_eval_time': 1209.0333228111267, 'accumulated_logging_time': 0.40857839584350586, 'global_step': 14670, 'preemption_count': 0}), (16457, {'train/ctc_loss': Array(0.27452624, dtype=float32), 'train/wer': 0.10105921721134868, 'validation/ctc_loss': Array(0.5743748, dtype=float32), 'validation/wer': 0.1744981994072043, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36317125, dtype=float32), 'test/wer': 0.12386001259317937, 'test/num_examples': 2472, 'score': 13039.76072025299, 'total_duration': 14383.490197658539, 'accumulated_submission_time': 13039.76072025299, 'accumulated_eval_time': 1342.5781121253967, 'accumulated_logging_time': 0.46301817893981934, 'global_step': 16457, 'preemption_count': 0}), (18251, {'train/ctc_loss': Array(0.2666409, dtype=float32), 'train/wer': 0.10048963087302311, 'validation/ctc_loss': Array(0.5565517, dtype=float32), 'validation/wer': 0.16945847050986224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34133852, dtype=float32), 'test/wer': 0.11551195336461316, 'test/num_examples': 2472, 'score': 14480.069234132767, 'total_duration': 15956.655859947205, 'accumulated_submission_time': 14480.069234132767, 'accumulated_eval_time': 1475.30983710289, 'accumulated_logging_time': 0.512258768081665, 'global_step': 18251, 'preemption_count': 0}), (20072, {'train/ctc_loss': Array(0.27632526, dtype=float32), 'train/wer': 0.09994447235516539, 'validation/ctc_loss': Array(0.53343123, dtype=float32), 'validation/wer': 0.1639939368778783, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3296891, dtype=float32), 'test/wer': 0.11335892592366908, 'test/num_examples': 2472, 'score': 15920.46349310875, 'total_duration': 17530.263286590576, 'accumulated_submission_time': 15920.46349310875, 'accumulated_eval_time': 1608.389491558075, 'accumulated_logging_time': 0.5656495094299316, 'global_step': 20072, 'preemption_count': 0}), (21871, {'train/ctc_loss': Array(0.2690903, dtype=float32), 'train/wer': 0.09882079826944891, 'validation/ctc_loss': Array(0.52979755, dtype=float32), 'validation/wer': 0.16038309663342248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3216067, dtype=float32), 'test/wer': 0.10862632786951841, 'test/num_examples': 2472, 'score': 17360.60667347908, 'total_duration': 19103.31728196144, 'accumulated_submission_time': 17360.60667347908, 'accumulated_eval_time': 1741.1654961109161, 'accumulated_logging_time': 0.6244997978210449, 'global_step': 21871, 'preemption_count': 0}), (23650, {'train/ctc_loss': Array(0.2543325, dtype=float32), 'train/wer': 0.09427924683609425, 'validation/ctc_loss': Array(0.51538944, dtype=float32), 'validation/wer': 0.15831700087857342, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3166944, dtype=float32), 'test/wer': 0.10704202465825767, 'test/num_examples': 2472, 'score': 18800.65876030922, 'total_duration': 20678.319122314453, 'accumulated_submission_time': 18800.65876030922, 'accumulated_eval_time': 1875.9892790317535, 'accumulated_logging_time': 0.6764576435089111, 'global_step': 23650, 'preemption_count': 0}), (25434, {'train/ctc_loss': Array(0.23121329, dtype=float32), 'train/wer': 0.08628225159410498, 'validation/ctc_loss': Array(0.4990429, dtype=float32), 'validation/wer': 0.15016847369589773, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30282408, dtype=float32), 'test/wer': 0.1014157171003189, 'test/num_examples': 2472, 'score': 20241.77404141426, 'total_duration': 22251.441853284836, 'accumulated_submission_time': 20241.77404141426, 'accumulated_eval_time': 2007.8673095703125, 'accumulated_logging_time': 0.731112003326416, 'global_step': 25434, 'preemption_count': 0}), (27251, {'train/ctc_loss': Array(0.21819447, dtype=float32), 'train/wer': 0.08222436683272907, 'validation/ctc_loss': Array(0.4925936, dtype=float32), 'validation/wer': 0.14788032092066772, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29341123, dtype=float32), 'test/wer': 0.09824711067779741, 'test/num_examples': 2472, 'score': 21681.949670553207, 'total_duration': 23825.036254882812, 'accumulated_submission_time': 21681.949670553207, 'accumulated_eval_time': 2141.1486024856567, 'accumulated_logging_time': 0.7904298305511475, 'global_step': 27251, 'preemption_count': 0}), (29050, {'train/ctc_loss': Array(0.21523209, dtype=float32), 'train/wer': 0.08024363052438863, 'validation/ctc_loss': Array(0.48609242, dtype=float32), 'validation/wer': 0.146576942757562, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28730962, dtype=float32), 'test/wer': 0.09810492962037658, 'test/num_examples': 2472, 'score': 23122.05716252327, 'total_duration': 25399.749225378036, 'accumulated_submission_time': 23122.05716252327, 'accumulated_eval_time': 2275.6245658397675, 'accumulated_logging_time': 0.8445065021514893, 'global_step': 29050, 'preemption_count': 0}), (30830, {'train/ctc_loss': Array(0.22447392, dtype=float32), 'train/wer': 0.08388043855087537, 'validation/ctc_loss': Array(0.46955344, dtype=float32), 'validation/wer': 0.1418944360234415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27968472, dtype=float32), 'test/wer': 0.09595190217943249, 'test/num_examples': 2472, 'score': 24562.581042051315, 'total_duration': 26972.9074883461, 'accumulated_submission_time': 24562.581042051315, 'accumulated_eval_time': 2408.1328547000885, 'accumulated_logging_time': 0.8940913677215576, 'global_step': 30830, 'preemption_count': 0}), (32621, {'train/ctc_loss': Array(0.21071382, dtype=float32), 'train/wer': 0.0778517517729342, 'validation/ctc_loss': Array(0.4662035, dtype=float32), 'validation/wer': 0.14140204871737935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27777946, dtype=float32), 'test/wer': 0.09463164950338188, 'test/num_examples': 2472, 'score': 26002.89443707466, 'total_duration': 28544.897045135498, 'accumulated_submission_time': 26002.89443707466, 'accumulated_eval_time': 2539.6788890361786, 'accumulated_logging_time': 0.9472103118896484, 'global_step': 32621, 'preemption_count': 0}), (34442, {'train/ctc_loss': Array(0.21640244, dtype=float32), 'train/wer': 0.07870282376752333, 'validation/ctc_loss': Array(0.45866513, dtype=float32), 'validation/wer': 0.13789740965658398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26703775, dtype=float32), 'test/wer': 0.09119899254565028, 'test/num_examples': 2472, 'score': 27443.38892006874, 'total_duration': 30119.136954545975, 'accumulated_submission_time': 27443.38892006874, 'accumulated_eval_time': 2673.288696050644, 'accumulated_logging_time': 1.005300521850586, 'global_step': 34442, 'preemption_count': 0}), (36233, {'train/ctc_loss': Array(0.21171698, dtype=float32), 'train/wer': 0.07614267293285165, 'validation/ctc_loss': Array(0.4551876, dtype=float32), 'validation/wer': 0.13615957210577637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2658482, dtype=float32), 'test/wer': 0.08847724087502284, 'test/num_examples': 2472, 'score': 28883.798013210297, 'total_duration': 31691.72387599945, 'accumulated_submission_time': 28883.798013210297, 'accumulated_eval_time': 2805.333705663681, 'accumulated_logging_time': 1.062840461730957, 'global_step': 36233, 'preemption_count': 0}), (38013, {'train/ctc_loss': Array(0.16391788, dtype=float32), 'train/wer': 0.0609964843229503, 'validation/ctc_loss': Array(0.43490544, dtype=float32), 'validation/wer': 0.1313708641879954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25705752, dtype=float32), 'test/wer': 0.08595860500071091, 'test/num_examples': 2472, 'score': 30324.081511735916, 'total_duration': 33266.3851544857, 'accumulated_submission_time': 30324.081511735916, 'accumulated_eval_time': 2939.579090833664, 'accumulated_logging_time': 1.119497299194336, 'global_step': 38013, 'preemption_count': 0}), (39801, {'train/ctc_loss': Array(0.1884352, dtype=float32), 'train/wer': 0.06887394141623197, 'validation/ctc_loss': Array(0.43504775, dtype=float32), 'validation/wer': 0.13085916757581317, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25120482, dtype=float32), 'test/wer': 0.08437430178945017, 'test/num_examples': 2472, 'score': 31764.344473838806, 'total_duration': 34840.09559392929, 'accumulated_submission_time': 31764.344473838806, 'accumulated_eval_time': 3072.8963837623596, 'accumulated_logging_time': 1.1725895404815674, 'global_step': 39801, 'preemption_count': 0}), (41617, {'train/ctc_loss': Array(0.23098333, dtype=float32), 'train/wer': 0.08507672363760624, 'validation/ctc_loss': Array(0.4273046, dtype=float32), 'validation/wer': 0.12748003900479837, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24482445, dtype=float32), 'test/wer': 0.08297280279487336, 'test/num_examples': 2472, 'score': 33204.71107053757, 'total_duration': 36414.75404071808, 'accumulated_submission_time': 33204.71107053757, 'accumulated_eval_time': 3207.057755947113, 'accumulated_logging_time': 1.2265305519104004, 'global_step': 41617, 'preemption_count': 0}), (43396, {'train/ctc_loss': Array(0.22959857, dtype=float32), 'train/wer': 0.08449124832766172, 'validation/ctc_loss': Array(0.42108253, dtype=float32), 'validation/wer': 0.1264373364743138, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23914756, dtype=float32), 'test/wer': 0.08161192695955964, 'test/num_examples': 2472, 'score': 34644.79412436485, 'total_duration': 37986.90700483322, 'accumulated_submission_time': 34644.79412436485, 'accumulated_eval_time': 3338.9981787204742, 'accumulated_logging_time': 1.2806153297424316, 'global_step': 43396, 'preemption_count': 0}), (45174, {'train/ctc_loss': Array(0.26250243, dtype=float32), 'train/wer': 0.09666512626425117, 'validation/ctc_loss': Array(0.40786082, dtype=float32), 'validation/wer': 0.12261409386253705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23407225, dtype=float32), 'test/wer': 0.07791521946661792, 'test/num_examples': 2472, 'score': 36085.03482174873, 'total_duration': 39558.517823934555, 'accumulated_submission_time': 36085.03482174873, 'accumulated_eval_time': 3470.2350482940674, 'accumulated_logging_time': 1.3378307819366455, 'global_step': 45174, 'preemption_count': 0}), (46972, {'train/ctc_loss': Array(0.22550887, dtype=float32), 'train/wer': 0.08165488168902967, 'validation/ctc_loss': Array(0.39984173, dtype=float32), 'validation/wer': 0.11989148169960512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22878855, dtype=float32), 'test/wer': 0.07667621310909349, 'test/num_examples': 2472, 'score': 37525.23612308502, 'total_duration': 41131.114505529404, 'accumulated_submission_time': 37525.23612308502, 'accumulated_eval_time': 3602.4942483901978, 'accumulated_logging_time': 1.3967421054840088, 'global_step': 46972, 'preemption_count': 0}), (48778, {'train/ctc_loss': Array(0.20397122, dtype=float32), 'train/wer': 0.07678175765201029, 'validation/ctc_loss': Array(0.3939008, dtype=float32), 'validation/wer': 0.11731368933257383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22130717, dtype=float32), 'test/wer': 0.07452318566814942, 'test/num_examples': 2472, 'score': 38965.769352674484, 'total_duration': 42703.40314888954, 'accumulated_submission_time': 38965.769352674484, 'accumulated_eval_time': 3734.108068704605, 'accumulated_logging_time': 1.460125207901001, 'global_step': 48778, 'preemption_count': 0}), (50551, {'train/ctc_loss': Array(0.1726337, dtype=float32), 'train/wer': 0.06562877247169627, 'validation/ctc_loss': Array(0.38651025, dtype=float32), 'validation/wer': 0.11306564198615522, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21482898, dtype=float32), 'test/wer': 0.07208579611236365, 'test/num_examples': 2472, 'score': 40405.70114803314, 'total_duration': 44276.238609075546, 'accumulated_submission_time': 40405.70114803314, 'accumulated_eval_time': 3866.8734180927277, 'accumulated_logging_time': 1.5228583812713623, 'global_step': 50551, 'preemption_count': 0}), (52335, {'train/ctc_loss': Array(0.19079773, dtype=float32), 'train/wer': 0.07166792485621337, 'validation/ctc_loss': Array(0.37963584, dtype=float32), 'validation/wer': 0.11314287921063557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21250953, dtype=float32), 'test/wer': 0.07273576666057319, 'test/num_examples': 2472, 'score': 41845.93539810181, 'total_duration': 45849.49245977402, 'accumulated_submission_time': 41845.93539810181, 'accumulated_eval_time': 3999.7612252235413, 'accumulated_logging_time': 1.5781021118164062, 'global_step': 52335, 'preemption_count': 0}), (54147, {'train/ctc_loss': Array(0.16157928, dtype=float32), 'train/wer': 0.06215641642329497, 'validation/ctc_loss': Array(0.36532888, dtype=float32), 'validation/wer': 0.1073983606399104, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20436308, dtype=float32), 'test/wer': 0.0686734507342636, 'test/num_examples': 2472, 'score': 43285.859830617905, 'total_duration': 47421.48486161232, 'accumulated_submission_time': 43285.859830617905, 'accumulated_eval_time': 4131.691259860992, 'accumulated_logging_time': 1.6392731666564941, 'global_step': 54147, 'preemption_count': 0}), (55961, {'train/ctc_loss': Array(0.15674654, dtype=float32), 'train/wer': 0.05982924527278447, 'validation/ctc_loss': Array(0.35715374, dtype=float32), 'validation/wer': 0.10511986251774043, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19516303, dtype=float32), 'test/wer': 0.0657892064265838, 'test/num_examples': 2472, 'score': 44725.770195007324, 'total_duration': 48992.8511402607, 'accumulated_submission_time': 44725.770195007324, 'accumulated_eval_time': 4263.011621952057, 'accumulated_logging_time': 1.696920394897461, 'global_step': 55961, 'preemption_count': 0}), (57741, {'train/ctc_loss': Array(0.14496757, dtype=float32), 'train/wer': 0.05509454796411318, 'validation/ctc_loss': Array(0.3530484, dtype=float32), 'validation/wer': 0.10409646929337595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19236514, dtype=float32), 'test/wer': 0.06357524424674507, 'test/num_examples': 2472, 'score': 46166.32611012459, 'total_duration': 50565.07560944557, 'accumulated_submission_time': 46166.32611012459, 'accumulated_eval_time': 4394.548684120178, 'accumulated_logging_time': 1.7538185119628906, 'global_step': 57741, 'preemption_count': 0}), (59535, {'train/ctc_loss': Array(0.1456565, dtype=float32), 'train/wer': 0.05550976958208853, 'validation/ctc_loss': Array(0.3391099, dtype=float32), 'validation/wer': 0.10038908251831971, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1857698, dtype=float32), 'test/wer': 0.061056608372433124, 'test/num_examples': 2472, 'score': 47606.610372543335, 'total_duration': 52136.28013706207, 'accumulated_submission_time': 47606.610372543335, 'accumulated_eval_time': 4525.326953172684, 'accumulated_logging_time': 1.8184218406677246, 'global_step': 59535, 'preemption_count': 0}), (61341, {'train/ctc_loss': Array(0.1419333, dtype=float32), 'train/wer': 0.053888628715189665, 'validation/ctc_loss': Array(0.33404234, dtype=float32), 'validation/wer': 0.09824574953899032, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1807087, dtype=float32), 'test/wer': 0.059594174638961675, 'test/num_examples': 2472, 'score': 49047.04236245155, 'total_duration': 53710.25221323967, 'accumulated_submission_time': 49047.04236245155, 'accumulated_eval_time': 4658.723962068558, 'accumulated_logging_time': 1.8841652870178223, 'global_step': 61341, 'preemption_count': 0}), (63155, {'train/ctc_loss': Array(0.11702894, dtype=float32), 'train/wer': 0.04568229201694801, 'validation/ctc_loss': Array(0.32601267, dtype=float32), 'validation/wer': 0.09507902333529644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17809816, dtype=float32), 'test/wer': 0.058152052485121765, 'test/num_examples': 2472, 'score': 50487.46106958389, 'total_duration': 55282.32294297218, 'accumulated_submission_time': 50487.46106958389, 'accumulated_eval_time': 4790.242144107819, 'accumulated_logging_time': 1.94150710105896, 'global_step': 63155, 'preemption_count': 0}), (64934, {'train/ctc_loss': Array(0.11297734, dtype=float32), 'train/wer': 0.04389918305873762, 'validation/ctc_loss': Array(0.3239654, dtype=float32), 'validation/wer': 0.09357289745792985, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16898389, dtype=float32), 'test/wer': 0.055409989234862796, 'test/num_examples': 2472, 'score': 51927.87755322456, 'total_duration': 56853.82898592949, 'accumulated_submission_time': 51927.87755322456, 'accumulated_eval_time': 4921.200897216797, 'accumulated_logging_time': 1.9962365627288818, 'global_step': 64934, 'preemption_count': 0}), (66736, {'train/ctc_loss': Array(0.10090725, dtype=float32), 'train/wer': 0.03881734589117529, 'validation/ctc_loss': Array(0.31437576, dtype=float32), 'validation/wer': 0.09140060051942034, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16851163, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 53368.10543203354, 'total_duration': 58425.920551776886, 'accumulated_submission_time': 53368.10543203354, 'accumulated_eval_time': 5052.923782587051, 'accumulated_logging_time': 2.061052083969116, 'global_step': 66736, 'preemption_count': 0}), (68543, {'train/ctc_loss': Array(0.09813993, dtype=float32), 'train/wer': 0.03740100320547493, 'validation/ctc_loss': Array(0.3097387, dtype=float32), 'validation/wer': 0.08974000019309306, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16452241, dtype=float32), 'test/wer': 0.054231917616233014, 'test/num_examples': 2472, 'score': 54808.36094260216, 'total_duration': 59996.24846315384, 'accumulated_submission_time': 54808.36094260216, 'accumulated_eval_time': 5182.85727763176, 'accumulated_logging_time': 2.1215648651123047, 'global_step': 68543, 'preemption_count': 0}), (70351, {'train/ctc_loss': Array(0.08928266, dtype=float32), 'train/wer': 0.03431273898168646, 'validation/ctc_loss': Array(0.30275312, dtype=float32), 'validation/wer': 0.08661189260163936, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1618215, dtype=float32), 'test/wer': 0.05248512176791989, 'test/num_examples': 2472, 'score': 56248.92697405815, 'total_duration': 61568.578528642654, 'accumulated_submission_time': 56248.92697405815, 'accumulated_eval_time': 5314.483243465424, 'accumulated_logging_time': 2.182187557220459, 'global_step': 70351, 'preemption_count': 0}), (72130, {'train/ctc_loss': Array(0.08532207, dtype=float32), 'train/wer': 0.03239874699755377, 'validation/ctc_loss': Array(0.29723024, dtype=float32), 'validation/wer': 0.08538575166301399, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15875757, dtype=float32), 'test/wer': 0.05124611541039547, 'test/num_examples': 2472, 'score': 57688.89278316498, 'total_duration': 63140.387882232666, 'accumulated_submission_time': 57688.89278316498, 'accumulated_eval_time': 5446.1889128685, 'accumulated_logging_time': 2.245168685913086, 'global_step': 72130, 'preemption_count': 0}), (73933, {'train/ctc_loss': Array(0.07010609, dtype=float32), 'train/wer': 0.02700242546912191, 'validation/ctc_loss': Array(0.29594654, dtype=float32), 'validation/wer': 0.08368653272444655, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15754385, dtype=float32), 'test/wer': 0.05116486909186928, 'test/num_examples': 2472, 'score': 59128.87972784042, 'total_duration': 64710.83512163162, 'accumulated_submission_time': 59128.87972784042, 'accumulated_eval_time': 5576.509063482285, 'accumulated_logging_time': 2.3098487854003906, 'global_step': 73933, 'preemption_count': 0}), (75739, {'train/ctc_loss': Array(0.06693444, dtype=float32), 'train/wer': 0.025029311311617473, 'validation/ctc_loss': Array(0.2931396, dtype=float32), 'validation/wer': 0.08405340954072815, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15539056, dtype=float32), 'test/wer': 0.04945869640281925, 'test/num_examples': 2472, 'score': 60569.61856889725, 'total_duration': 66282.15744042397, 'accumulated_submission_time': 60569.61856889725, 'accumulated_eval_time': 5706.956416845322, 'accumulated_logging_time': 2.3700406551361084, 'global_step': 75739, 'preemption_count': 0})], 'global_step': 76377}
I0214 21:32:33.107874 140599226058560 submission_runner.py:586] Timing: 61068.13390135765
I0214 21:32:33.107933 140599226058560 submission_runner.py:588] Total number of evals: 43
I0214 21:32:33.107981 140599226058560 submission_runner.py:589] ====================
I0214 21:32:33.108031 140599226058560 submission_runner.py:542] Using RNG seed 4151861576
I0214 21:32:33.111375 140599226058560 submission_runner.py:551] --- Tuning run 2/5 ---
I0214 21:32:33.111509 140599226058560 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_2.
I0214 21:32:33.113129 140599226058560 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_2/hparams.json.
I0214 21:32:33.115237 140599226058560 submission_runner.py:206] Initializing dataset.
I0214 21:32:33.115374 140599226058560 submission_runner.py:213] Initializing model.
I0214 21:32:36.718557 140599226058560 submission_runner.py:255] Initializing optimizer.
I0214 21:32:37.146886 140599226058560 submission_runner.py:262] Initializing metrics bundle.
I0214 21:32:37.147091 140599226058560 submission_runner.py:280] Initializing checkpoint and logger.
I0214 21:32:37.244484 140599226058560 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_2 with prefix checkpoint_
I0214 21:32:37.244622 140599226058560 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_2/meta_data_0.json.
I0214 21:32:37.244895 140599226058560 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0214 21:32:37.244974 140599226058560 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0214 21:32:38.048849 140599226058560 logger_utils.py:220] Unable to record git information. Continuing without it.
I0214 21:32:38.775749 140599226058560 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_2/flags_0.json.
I0214 21:32:38.899667 140599226058560 submission_runner.py:314] Starting training loop.
I0214 21:32:38.902920 140599226058560 input_pipeline.py:20] Loading split = train-clean-100
I0214 21:32:39.275071 140599226058560 input_pipeline.py:20] Loading split = train-clean-360
I0214 21:32:39.412666 140599226058560 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0214 21:33:14.829020 140427187111680 logging_writer.py:48] [0] global_step=0, grad_norm=40.786842346191406, loss=32.30488586425781
I0214 21:33:14.844838 140599226058560 spec.py:321] Evaluating on the training split.
I0214 21:34:13.122493 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 21:35:03.992721 140599226058560 spec.py:349] Evaluating on the test split.
I0214 21:35:30.487627 140599226058560 submission_runner.py:408] Time since start: 171.59s, 	Step: 1, 	{'train/ctc_loss': Array(32.57337, dtype=float32), 'train/wer': 1.3618151438861104, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 35.945107221603394, 'total_duration': 171.58516669273376, 'accumulated_submission_time': 35.945107221603394, 'accumulated_eval_time': 135.64001059532166, 'accumulated_logging_time': 0}
I0214 21:35:30.505625 140532797925120 logging_writer.py:48] [1] accumulated_eval_time=135.640011, accumulated_logging_time=0, accumulated_submission_time=35.945107, global_step=1, preemption_count=0, score=35.945107, test/ctc_loss=31.23744010925293, test/num_examples=2472, test/wer=1.102350, total_duration=171.585167, train/ctc_loss=32.57337188720703, train/wer=1.361815, validation/ctc_loss=31.08704948425293, validation/num_examples=5348, validation/wer=1.058565
I0214 21:37:12.342727 140428045584128 logging_writer.py:48] [100] global_step=100, grad_norm=4.311082363128662, loss=10.498649597167969
I0214 21:38:28.381114 140428053976832 logging_writer.py:48] [200] global_step=200, grad_norm=0.9840106964111328, loss=6.046526908874512
I0214 21:39:44.415679 140428045584128 logging_writer.py:48] [300] global_step=300, grad_norm=0.7924591898918152, loss=5.859475612640381
I0214 21:41:00.515576 140428053976832 logging_writer.py:48] [400] global_step=400, grad_norm=0.612436056137085, loss=5.8249335289001465
I0214 21:42:16.688241 140428045584128 logging_writer.py:48] [500] global_step=500, grad_norm=2.1186506748199463, loss=5.809329986572266
I0214 21:43:35.675125 140428053976832 logging_writer.py:48] [600] global_step=600, grad_norm=0.5174270272254944, loss=5.825859069824219
I0214 21:45:01.039108 140428045584128 logging_writer.py:48] [700] global_step=700, grad_norm=2.842317581176758, loss=5.800404071807861
I0214 21:46:26.864618 140428053976832 logging_writer.py:48] [800] global_step=800, grad_norm=0.778113842010498, loss=5.803668975830078
I0214 21:47:53.156133 140428045584128 logging_writer.py:48] [900] global_step=900, grad_norm=2.1722612380981445, loss=5.812424182891846
I0214 21:49:18.023111 140428053976832 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.42358577251434326, loss=5.80610466003418
I0214 21:50:40.445034 140532797925120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5556226968765259, loss=5.764389991760254
I0214 21:51:56.402055 140532789532416 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3796497881412506, loss=5.811295032501221
I0214 21:53:12.336737 140532797925120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7388076186180115, loss=5.79283332824707
I0214 21:54:28.086912 140532789532416 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6025985479354858, loss=5.6580491065979
I0214 21:55:48.749066 140532797925120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.41349247097969055, loss=5.529139995574951
I0214 21:57:14.428236 140532789532416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7574542164802551, loss=5.530147075653076
I0214 21:58:40.401176 140532797925120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5048182010650635, loss=5.5258097648620605
I0214 21:59:31.014850 140599226058560 spec.py:321] Evaluating on the training split.
I0214 22:00:09.852234 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 22:00:56.542674 140599226058560 spec.py:349] Evaluating on the test split.
I0214 22:01:20.387482 140599226058560 submission_runner.py:408] Time since start: 1721.48s, 	Step: 1760, 	{'train/ctc_loss': Array(7.147141, dtype=float32), 'train/wer': 0.938839590443686, 'validation/ctc_loss': Array(7.041991, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(7.0403576, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1476.374610900879, 'total_duration': 1721.4816236495972, 'accumulated_submission_time': 1476.374610900879, 'accumulated_eval_time': 245.00649857521057, 'accumulated_logging_time': 0.028804540634155273}
I0214 22:01:20.425337 140532797925120 logging_writer.py:48] [1760] accumulated_eval_time=245.006499, accumulated_logging_time=0.028805, accumulated_submission_time=1476.374611, global_step=1760, preemption_count=0, score=1476.374611, test/ctc_loss=7.04035758972168, test/num_examples=2472, test/wer=0.899580, total_duration=1721.481624, train/ctc_loss=7.147140979766846, train/wer=0.938840, validation/ctc_loss=7.041991233825684, validation/num_examples=5348, validation/wer=0.896618
I0214 22:01:51.465560 140532789532416 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.8511450290679932, loss=5.521854400634766
I0214 22:03:06.589967 140532797925120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.21175344288349152, loss=5.7959489822387695
I0214 22:04:21.652392 140532789532416 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.2483954280614853, loss=5.792898178100586
I0214 22:05:40.595800 140532797925120 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.2769366204738617, loss=5.773965835571289
I0214 22:06:55.607791 140532789532416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.2351676970720291, loss=5.783642768859863
I0214 22:08:10.709100 140532797925120 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.4074525833129883, loss=5.792595386505127
I0214 22:09:25.885781 140532789532416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3246469497680664, loss=5.780691146850586
I0214 22:10:43.883156 140532797925120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.29806461930274963, loss=5.793095588684082
I0214 22:12:08.106020 140532789532416 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.4039900004863739, loss=5.7848992347717285
I0214 22:13:33.377378 140532797925120 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6775420904159546, loss=5.7771172523498535
I0214 22:14:58.118033 140532789532416 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.28869345784187317, loss=5.811248302459717
I0214 22:16:22.706156 140532797925120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.703963041305542, loss=5.786163330078125
I0214 22:17:47.113680 140532789532416 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.908367931842804, loss=5.807667255401611
I0214 22:19:14.790528 140532797925120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9250736832618713, loss=5.774843692779541
I0214 22:20:29.868263 140532789532416 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.645941972732544, loss=5.7670745849609375
I0214 22:21:45.092896 140532797925120 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5272079706192017, loss=5.797604084014893
I0214 22:23:00.416953 140532789532416 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.4906644821166992, loss=5.60251522064209
I0214 22:24:15.786092 140532797925120 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.9987263679504395, loss=5.812966823577881
I0214 22:25:20.708746 140599226058560 spec.py:321] Evaluating on the training split.
I0214 22:25:59.577081 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 22:26:45.432022 140599226058560 spec.py:349] Evaluating on the test split.
I0214 22:27:09.100232 140599226058560 submission_runner.py:408] Time since start: 3270.19s, 	Step: 3580, 	{'train/ctc_loss': Array(5.8638206, dtype=float32), 'train/wer': 0.9417131519458765, 'validation/ctc_loss': Array(5.820514, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.784349, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2916.5684781074524, 'total_duration': 3270.1946194171906, 'accumulated_submission_time': 2916.5684781074524, 'accumulated_eval_time': 353.3921241760254, 'accumulated_logging_time': 0.08565068244934082}
I0214 22:27:09.136700 140532797925120 logging_writer.py:48] [3580] accumulated_eval_time=353.392124, accumulated_logging_time=0.085651, accumulated_submission_time=2916.568478, global_step=3580, preemption_count=0, score=2916.568478, test/ctc_loss=5.784348964691162, test/num_examples=2472, test/wer=0.899580, total_duration=3270.194619, train/ctc_loss=5.863820552825928, train/wer=0.941713, validation/ctc_loss=5.82051420211792, validation/num_examples=5348, validation/wer=0.896618
I0214 22:27:24.923090 140532789532416 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.294102668762207, loss=5.5767083168029785
I0214 22:28:39.971180 140532797925120 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.1472688913345337, loss=5.5120391845703125
I0214 22:29:55.296066 140532789532416 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5707582235336304, loss=5.534480094909668
I0214 22:31:11.292129 140532797925120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7536299824714661, loss=5.525723934173584
I0214 22:32:35.384387 140532789532416 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.5002001523971558, loss=5.538712024688721
I0214 22:34:00.421354 140532797925120 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.4564697742462158, loss=5.509579181671143
I0214 22:35:21.152310 140532797925120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.49383699893951416, loss=5.516818523406982
I0214 22:36:36.445995 140532789532416 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.3551064729690552, loss=5.526008129119873
I0214 22:37:51.705353 140532797925120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.483561635017395, loss=5.519063472747803
I0214 22:39:07.088329 140532789532416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.37526798248291016, loss=5.523414134979248
I0214 22:40:29.521530 140532797925120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.34308674931526184, loss=5.526596546173096
I0214 22:41:53.917285 140532789532416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7196221351623535, loss=5.5180230140686035
I0214 22:43:18.853465 140532797925120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7655572295188904, loss=5.523727893829346
I0214 22:44:44.407801 140532789532416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6393851637840271, loss=5.494861602783203
I0214 22:46:10.937177 140532797925120 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8109748959541321, loss=5.5008368492126465
I0214 22:47:36.388245 140532789532416 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.4886025190353394, loss=5.510054588317871
I0214 22:48:59.577326 140532797925120 logging_writer.py:48] [5200] global_step=5200, grad_norm=9.531803131103516, loss=5.538419723510742
I0214 22:50:14.782385 140532789532416 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.30111703276634216, loss=5.51389741897583
I0214 22:51:09.215449 140599226058560 spec.py:321] Evaluating on the training split.
I0214 22:51:48.324442 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 22:52:35.177619 140599226058560 spec.py:349] Evaluating on the test split.
I0214 22:52:58.848170 140599226058560 submission_runner.py:408] Time since start: 4819.94s, 	Step: 5374, 	{'train/ctc_loss': Array(5.8075776, dtype=float32), 'train/wer': 0.940334510929565, 'validation/ctc_loss': Array(5.716846, dtype=float32), 'validation/wer': 0.8965503924616469, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7255216, dtype=float32), 'test/wer': 0.8994983039831007, 'test/num_examples': 2472, 'score': 4356.562227487564, 'total_duration': 4819.940863370895, 'accumulated_submission_time': 4356.562227487564, 'accumulated_eval_time': 463.0172669887543, 'accumulated_logging_time': 0.13686490058898926}
I0214 22:52:58.880611 140532797925120 logging_writer.py:48] [5374] accumulated_eval_time=463.017267, accumulated_logging_time=0.136865, accumulated_submission_time=4356.562227, global_step=5374, preemption_count=0, score=4356.562227, test/ctc_loss=5.725521564483643, test/num_examples=2472, test/wer=0.899498, total_duration=4819.940863, train/ctc_loss=5.807577610015869, train/wer=0.940335, validation/ctc_loss=5.716845989227295, validation/num_examples=5348, validation/wer=0.896550
I0214 22:53:19.114226 140532789532416 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8504049181938171, loss=5.501849174499512
I0214 22:54:34.094810 140532797925120 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5702204704284668, loss=5.496132850646973
I0214 22:55:49.189144 140532789532416 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.840825080871582, loss=5.498365879058838
I0214 22:57:04.295479 140532797925120 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9273332357406616, loss=5.496942520141602
I0214 22:58:24.643769 140532789532416 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.4970316886901855, loss=5.564277648925781
I0214 22:59:49.092916 140532797925120 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.38682734966278076, loss=5.48266077041626
I0214 23:01:13.925720 140532789532416 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9348911046981812, loss=5.518443584442139
I0214 23:02:38.553044 140532797925120 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.35270655155181885, loss=5.472891807556152
I0214 23:04:05.593645 140532797925120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.27464523911476135, loss=5.476277828216553
I0214 23:05:20.620545 140532789532416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.41664716601371765, loss=5.49120569229126
I0214 23:06:35.749353 140532797925120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4059271812438965, loss=5.451427459716797
I0214 23:07:50.828414 140532789532416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.350177526473999, loss=5.484899520874023
I0214 23:09:07.892078 140532797925120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7976030707359314, loss=5.495402812957764
I0214 23:10:31.877056 140532789532416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5674749612808228, loss=5.470335483551025
I0214 23:11:57.899988 140532797925120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.3159433901309967, loss=5.490644931793213
I0214 23:13:22.318554 140532789532416 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9918735027313232, loss=5.486289024353027
I0214 23:14:47.313768 140532797925120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5411698818206787, loss=5.462322235107422
I0214 23:16:12.750251 140532789532416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.592294454574585, loss=5.4569573402404785
I0214 23:16:59.853664 140599226058560 spec.py:321] Evaluating on the training split.
I0214 23:17:38.301397 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 23:18:24.710487 140599226058560 spec.py:349] Evaluating on the test split.
I0214 23:18:48.139547 140599226058560 submission_runner.py:408] Time since start: 6369.23s, 	Step: 7157, 	{'train/ctc_loss': Array(5.684661, dtype=float32), 'train/wer': 0.9380313186211507, 'validation/ctc_loss': Array(5.6359296, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.6091366, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5797.4471888542175, 'total_duration': 6369.234001159668, 'accumulated_submission_time': 5797.4471888542175, 'accumulated_eval_time': 571.2973206043243, 'accumulated_logging_time': 0.18694400787353516}
I0214 23:18:48.180612 140532797925120 logging_writer.py:48] [7157] accumulated_eval_time=571.297321, accumulated_logging_time=0.186944, accumulated_submission_time=5797.447189, global_step=7157, preemption_count=0, score=5797.447189, test/ctc_loss=5.609136581420898, test/num_examples=2472, test/wer=0.899580, total_duration=6369.234001, train/ctc_loss=5.684660911560059, train/wer=0.938031, validation/ctc_loss=5.635929584503174, validation/num_examples=5348, validation/wer=0.896618
I0214 23:19:21.058716 140532789532416 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.6401659846305847, loss=5.469575881958008
I0214 23:20:39.951583 140532797925120 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5691813826560974, loss=5.457304954528809
I0214 23:21:55.182375 140532789532416 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8107566833496094, loss=5.477365016937256
I0214 23:23:10.268684 140532797925120 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.5821881294250488, loss=5.465222358703613
I0214 23:24:25.446117 140532789532416 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.6196199059486389, loss=5.458224773406982
I0214 23:25:46.939889 140532797925120 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.020836591720581, loss=5.466437816619873
I0214 23:27:11.633036 140532789532416 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6164023280143738, loss=5.434881210327148
I0214 23:28:35.993529 140532797925120 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.8003418445587158, loss=5.45574951171875
I0214 23:30:00.703029 140532789532416 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.361409306526184, loss=5.479847431182861
I0214 23:31:25.883896 140532797925120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7599719166755676, loss=5.488924980163574
I0214 23:32:50.336902 140532789532416 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6045185327529907, loss=5.447652339935303
I0214 23:34:12.630799 140532797925120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5179182887077332, loss=5.446383476257324
I0214 23:35:27.676788 140532789532416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.39556756615638733, loss=5.424192428588867
I0214 23:36:42.728862 140532797925120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9371677041053772, loss=5.425042629241943
I0214 23:37:57.787864 140532789532416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5586368441581726, loss=5.431656837463379
I0214 23:39:12.917411 140532797925120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.4019792675971985, loss=5.430675983428955
I0214 23:40:35.234193 140532789532416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9935648441314697, loss=5.424017906188965
I0214 23:42:01.131440 140532797925120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.4189206659793854, loss=5.410806655883789
I0214 23:42:48.485434 140599226058560 spec.py:321] Evaluating on the training split.
I0214 23:43:27.025123 140599226058560 spec.py:333] Evaluating on the validation split.
I0214 23:44:13.309668 140599226058560 spec.py:349] Evaluating on the test split.
I0214 23:44:37.102830 140599226058560 submission_runner.py:408] Time since start: 7918.20s, 	Step: 8957, 	{'train/ctc_loss': Array(5.55107, dtype=float32), 'train/wer': 0.9408213025698041, 'validation/ctc_loss': Array(5.475975, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4412518, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7237.664181232452, 'total_duration': 7918.197555303574, 'accumulated_submission_time': 7237.664181232452, 'accumulated_eval_time': 679.9091982841492, 'accumulated_logging_time': 0.2448720932006836}
I0214 23:44:37.138641 140532797925120 logging_writer.py:48] [8957] accumulated_eval_time=679.909198, accumulated_logging_time=0.244872, accumulated_submission_time=7237.664181, global_step=8957, preemption_count=0, score=7237.664181, test/ctc_loss=5.441251754760742, test/num_examples=2472, test/wer=0.899580, total_duration=7918.197555, train/ctc_loss=5.551070213317871, train/wer=0.940821, validation/ctc_loss=5.475975036621094, validation/num_examples=5348, validation/wer=0.896618
I0214 23:45:10.183831 140532789532416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5247256755828857, loss=5.407474040985107
I0214 23:46:25.245496 140532797925120 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6045505404472351, loss=5.420510768890381
I0214 23:47:40.252237 140532789532416 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9389544725418091, loss=5.39493465423584
I0214 23:48:59.746789 140532797925120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.7159284949302673, loss=5.391617774963379
I0214 23:50:14.704072 140532789532416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5770336985588074, loss=5.373418807983398
I0214 23:51:29.826939 140532797925120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.41257375478744507, loss=5.411031246185303
I0214 23:52:44.897615 140532789532416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7001106142997742, loss=5.387077331542969
I0214 23:53:59.897137 140532797925120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3951041102409363, loss=5.383955955505371
I0214 23:55:15.234224 140532789532416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8747938871383667, loss=5.376669883728027
I0214 23:56:39.388045 140532797925120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.37294793128967285, loss=5.3629536628723145
I0214 23:58:05.384531 140532789532416 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.1997865438461304, loss=5.349490642547607
I0214 23:59:30.385660 140532797925120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.7553691864013672, loss=5.278092384338379
I0215 00:00:54.597651 140532789532416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7007465362548828, loss=5.133475303649902
I0215 00:02:22.520210 140532797925120 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.353474736213684, loss=4.927495002746582
I0215 00:03:37.671442 140532789532416 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.6800984144210815, loss=4.614495277404785
I0215 00:04:52.692275 140532797925120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7644578814506531, loss=4.399341106414795
I0215 00:06:07.877834 140532789532416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8297514915466309, loss=4.468192100524902
I0215 00:07:22.889590 140532797925120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.935431957244873, loss=4.208700180053711
I0215 00:08:37.181674 140599226058560 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0215 00:09:51.120497 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 00:10:41.069298 140599226058560 spec.py:349] Evaluating on the test split.
I0215 00:11:06.363939 140599226058560 submission_runner.py:408] Time since start: 9507.46s, 	Step: 10800, 	{'train/ctc_loss': Array(4.311075, dtype=float32), 'train/wer': 0.812946352661112, 'validation/ctc_loss': Array(4.4674177, dtype=float32), 'validation/wer': 0.795195844637323, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.28696, dtype=float32), 'test/wer': 0.7847175674852234, 'test/num_examples': 2472, 'score': 8677.619053125381, 'total_duration': 9507.45812034607, 'accumulated_submission_time': 8677.619053125381, 'accumulated_eval_time': 829.0853753089905, 'accumulated_logging_time': 0.2981421947479248}
I0215 00:11:06.395609 140532797925120 logging_writer.py:48] [10800] accumulated_eval_time=829.085375, accumulated_logging_time=0.298142, accumulated_submission_time=8677.619053, global_step=10800, preemption_count=0, score=8677.619053, test/ctc_loss=4.286960124969482, test/num_examples=2472, test/wer=0.784718, total_duration=9507.458120, train/ctc_loss=4.311075210571289, train/wer=0.812946, validation/ctc_loss=4.4674177169799805, validation/num_examples=5348, validation/wer=0.795196
I0215 00:11:07.275280 140532789532416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.768747091293335, loss=4.132221698760986
I0215 00:12:22.176282 140532797925120 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7613910436630249, loss=4.006479740142822
I0215 00:13:37.280989 140532789532416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7643001675605774, loss=3.949570655822754
I0215 00:14:52.428855 140532797925120 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7371251583099365, loss=3.9503774642944336
I0215 00:16:10.715081 140532789532416 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.0501136779785156, loss=3.87534761428833
I0215 00:17:34.669385 140532797925120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9345540404319763, loss=3.8657710552215576
I0215 00:18:56.187764 140532797925120 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.2338193655014038, loss=3.7659835815429688
I0215 00:20:11.383800 140532789532416 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.1008256673812866, loss=3.7106385231018066
I0215 00:21:26.599510 140532797925120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8421493172645569, loss=3.729954957962036
I0215 00:22:41.927335 140532789532416 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.237186074256897, loss=3.632516860961914
I0215 00:23:59.472235 140532797925120 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.418771982192993, loss=3.593324661254883
I0215 00:25:23.503713 140532789532416 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0327973365783691, loss=3.5152251720428467
I0215 00:26:48.439235 140532797925120 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4041717052459717, loss=3.5536105632781982
I0215 00:28:13.317614 140532789532416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.8443013429641724, loss=3.433367967605591
I0215 00:29:37.884535 140532797925120 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0598441362380981, loss=3.3634257316589355
I0215 00:31:02.756586 140532789532416 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.130448341369629, loss=3.3593451976776123
I0215 00:32:26.780771 140532797925120 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.0136127471923828, loss=3.3305559158325195
I0215 00:33:41.827332 140532789532416 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.4148914813995361, loss=3.286653518676758
I0215 00:34:56.892133 140532797925120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9236208200454712, loss=3.30047607421875
I0215 00:35:06.372988 140599226058560 spec.py:321] Evaluating on the training split.
I0215 00:36:01.639627 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 00:36:53.657568 140599226058560 spec.py:349] Evaluating on the test split.
I0215 00:37:19.648466 140599226058560 submission_runner.py:408] Time since start: 11080.74s, 	Step: 12614, 	{'train/ctc_loss': Array(2.5407991, dtype=float32), 'train/wer': 0.6001672788101385, 'validation/ctc_loss': Array(2.867275, dtype=float32), 'validation/wer': 0.6317522229838671, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5912423, dtype=float32), 'test/wer': 0.5990697296528751, 'test/num_examples': 2472, 'score': 10117.509083271027, 'total_duration': 11080.74349808693, 'accumulated_submission_time': 10117.509083271027, 'accumulated_eval_time': 962.355612039566, 'accumulated_logging_time': 0.34575724601745605}
I0215 00:37:19.681749 140532797925120 logging_writer.py:48] [12614] accumulated_eval_time=962.355612, accumulated_logging_time=0.345757, accumulated_submission_time=10117.509083, global_step=12614, preemption_count=0, score=10117.509083, test/ctc_loss=2.5912423133850098, test/num_examples=2472, test/wer=0.599070, total_duration=11080.743498, train/ctc_loss=2.540799140930176, train/wer=0.600167, validation/ctc_loss=2.8672749996185303, validation/num_examples=5348, validation/wer=0.631752
I0215 00:38:25.006565 140532789532416 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.0358552932739258, loss=3.2086312770843506
I0215 00:39:40.360481 140532797925120 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.1377466917037964, loss=3.210068702697754
I0215 00:40:55.707166 140532789532416 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.041242241859436, loss=3.1584339141845703
I0215 00:42:14.652878 140532797925120 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9628803730010986, loss=3.0282771587371826
I0215 00:43:38.814473 140532789532416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.7593884468078613, loss=3.0661556720733643
I0215 00:45:03.763263 140532797925120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9525596499443054, loss=3.0005993843078613
I0215 00:46:28.555810 140532789532416 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4232062101364136, loss=2.9263556003570557
I0215 00:47:56.675678 140532797925120 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.1970282793045044, loss=2.9484469890594482
I0215 00:49:11.878338 140532789532416 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.1244438886642456, loss=2.99153208732605
I0215 00:50:27.082616 140532797925120 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0458842515945435, loss=2.952993154525757
I0215 00:51:42.232790 140532789532416 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.133660078048706, loss=2.8779714107513428
I0215 00:52:59.695040 140532797925120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.842628538608551, loss=2.8236424922943115
I0215 00:54:25.128313 140532789532416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9536529183387756, loss=2.8055362701416016
I0215 00:55:49.170962 140532797925120 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.5885040760040283, loss=2.837369918823242
I0215 00:57:14.369832 140532789532416 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0706663131713867, loss=2.795625686645508
I0215 00:58:39.692661 140532797925120 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.03074312210083, loss=2.781137704849243
I0215 01:00:06.062347 140532789532416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8706629276275635, loss=2.7391960620880127
I0215 01:01:19.982048 140599226058560 spec.py:321] Evaluating on the training split.
I0215 01:02:15.109574 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 01:03:07.108919 140599226058560 spec.py:349] Evaluating on the test split.
I0215 01:03:33.504515 140599226058560 submission_runner.py:408] Time since start: 12654.60s, 	Step: 14388, 	{'train/ctc_loss': Array(1.5154848, dtype=float32), 'train/wer': 0.45609027722719947, 'validation/ctc_loss': Array(1.8800801, dtype=float32), 'validation/wer': 0.4955057590005503, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.5566329, dtype=float32), 'test/wer': 0.4498202425202608, 'test/num_examples': 2472, 'score': 11557.722923755646, 'total_duration': 12654.597905158997, 'accumulated_submission_time': 11557.722923755646, 'accumulated_eval_time': 1095.8711938858032, 'accumulated_logging_time': 0.395554780960083}
I0215 01:03:33.539445 140532797925120 logging_writer.py:48] [14388] accumulated_eval_time=1095.871194, accumulated_logging_time=0.395555, accumulated_submission_time=11557.722924, global_step=14388, preemption_count=0, score=11557.722924, test/ctc_loss=1.5566328763961792, test/num_examples=2472, test/wer=0.449820, total_duration=12654.597905, train/ctc_loss=1.5154848098754883, train/wer=0.456090, validation/ctc_loss=1.8800801038742065, validation/num_examples=5348, validation/wer=0.495506
I0215 01:03:43.303691 140532789532416 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7246303558349609, loss=2.74411678314209
I0215 01:05:02.406012 140532797925120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9690197110176086, loss=2.69433856010437
I0215 01:06:17.779031 140532789532416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9040948748588562, loss=2.658998727798462
I0215 01:07:32.891635 140532797925120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7923905849456787, loss=2.7082359790802
I0215 01:08:49.753547 140532789532416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9899680018424988, loss=2.625603437423706
I0215 01:10:13.551361 140532797925120 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.3962868452072144, loss=2.6326491832733154
I0215 01:11:39.027298 140532789532416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9468417167663574, loss=2.6596310138702393
I0215 01:13:04.144298 140532797925120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8878626823425293, loss=2.6225249767303467
I0215 01:14:30.506644 140532789532416 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0407086610794067, loss=2.6254284381866455
I0215 01:15:55.949482 140532797925120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.84673672914505, loss=2.5923006534576416
I0215 01:17:21.490818 140532789532416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.8080322742462158, loss=2.480548143386841
I0215 01:18:45.105383 140532797925120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7812358140945435, loss=2.520057201385498
I0215 01:20:00.141237 140532789532416 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.4264369010925293, loss=2.4687914848327637
I0215 01:21:15.457745 140532797925120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9481299519538879, loss=2.51208758354187
I0215 01:22:30.517239 140532789532416 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.1230013370513916, loss=2.4368786811828613
I0215 01:23:49.699403 140532797925120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8651819229125977, loss=2.4517247676849365
I0215 01:25:14.974785 140532789532416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.8364753723144531, loss=2.354245662689209
I0215 01:26:40.224293 140532797925120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.9082248210906982, loss=2.477295398712158
I0215 01:27:34.220352 140599226058560 spec.py:321] Evaluating on the training split.
I0215 01:28:29.738098 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 01:29:21.443236 140599226058560 spec.py:349] Evaluating on the test split.
I0215 01:29:47.675965 140599226058560 submission_runner.py:408] Time since start: 14228.77s, 	Step: 16164, 	{'train/ctc_loss': Array(1.1518191, dtype=float32), 'train/wer': 0.3756104432757325, 'validation/ctc_loss': Array(1.5427254, dtype=float32), 'validation/wer': 0.42724736186605133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2202077, dtype=float32), 'test/wer': 0.37438303576869175, 'test/num_examples': 2472, 'score': 12998.31579875946, 'total_duration': 14228.770209550858, 'accumulated_submission_time': 12998.31579875946, 'accumulated_eval_time': 1229.3207650184631, 'accumulated_logging_time': 0.4482710361480713}
I0215 01:29:47.715079 140532797925120 logging_writer.py:48] [16164] accumulated_eval_time=1229.320765, accumulated_logging_time=0.448271, accumulated_submission_time=12998.315799, global_step=16164, preemption_count=0, score=12998.315799, test/ctc_loss=1.220207691192627, test/num_examples=2472, test/wer=0.374383, total_duration=14228.770210, train/ctc_loss=1.151819109916687, train/wer=0.375610, validation/ctc_loss=1.5427254438400269, validation/num_examples=5348, validation/wer=0.427247
I0215 01:30:15.398821 140532789532416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8765091300010681, loss=2.3703830242156982
I0215 01:31:30.468342 140532797925120 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.0905447006225586, loss=2.3626441955566406
I0215 01:32:45.497308 140532789532416 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.1967976093292236, loss=2.3921775817871094
I0215 01:34:05.899476 140532797925120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9035312533378601, loss=2.393886089324951
I0215 01:35:21.032365 140532789532416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.7414656281471252, loss=2.2719638347625732
I0215 01:36:36.298541 140532797925120 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0563092231750488, loss=2.2835352420806885
I0215 01:37:51.804952 140532789532416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.7917095422744751, loss=2.2646126747131348
I0215 01:39:08.782512 140532797925120 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.1536011695861816, loss=2.284829616546631
I0215 01:40:32.511921 140532789532416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7161067128181458, loss=2.227504253387451
I0215 01:41:58.553095 140532797925120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.7628895044326782, loss=2.278874397277832
I0215 01:43:24.157642 140532789532416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6687316298484802, loss=2.2279067039489746
I0215 01:44:49.355748 140532797925120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.8235756754875183, loss=2.208010673522949
I0215 01:46:14.663150 140532789532416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.7430895566940308, loss=2.218355894088745
I0215 01:47:39.978168 140532797925120 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0217409133911133, loss=2.190864324569702
I0215 01:48:59.769158 140532797925120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.8393860459327698, loss=2.150153398513794
I0215 01:50:15.074575 140532789532416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7950564622879028, loss=2.164332151412964
I0215 01:51:30.369446 140532797925120 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.9143400192260742, loss=2.2050578594207764
I0215 01:52:45.933447 140532789532416 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.203031301498413, loss=2.1127817630767822
I0215 01:53:48.139989 140599226058560 spec.py:321] Evaluating on the training split.
I0215 01:54:42.474485 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 01:55:33.505351 140599226058560 spec.py:349] Evaluating on the test split.
I0215 01:55:59.907873 140599226058560 submission_runner.py:408] Time since start: 15801.00s, 	Step: 17977, 	{'train/ctc_loss': Array(0.89231455, dtype=float32), 'train/wer': 0.2999344248311306, 'validation/ctc_loss': Array(1.2439867, dtype=float32), 'validation/wer': 0.3619143246087452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.92897916, dtype=float32), 'test/wer': 0.2999410964190685, 'test/num_examples': 2472, 'score': 14438.654992818832, 'total_duration': 15801.002537488937, 'accumulated_submission_time': 14438.654992818832, 'accumulated_eval_time': 1361.0830354690552, 'accumulated_logging_time': 0.5029153823852539}
I0215 01:55:59.947473 140532797925120 logging_writer.py:48] [17977] accumulated_eval_time=1361.083035, accumulated_logging_time=0.502915, accumulated_submission_time=14438.654993, global_step=17977, preemption_count=0, score=14438.654993, test/ctc_loss=0.9289791584014893, test/num_examples=2472, test/wer=0.299941, total_duration=15801.002537, train/ctc_loss=0.8923145532608032, train/wer=0.299934, validation/ctc_loss=1.24398672580719, validation/num_examples=5348, validation/wer=0.361914
I0215 01:56:18.041946 140532789532416 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7969751954078674, loss=2.1053242683410645
I0215 01:57:33.173647 140532797925120 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9015945792198181, loss=2.0776329040527344
I0215 01:58:48.593079 140532789532416 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.8579802513122559, loss=2.197178840637207
I0215 02:00:06.008670 140532797925120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.7462399005889893, loss=2.0459306240081787
I0215 02:01:31.699289 140532789532416 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.9669405221939087, loss=2.037875175476074
I0215 02:02:57.930583 140532797925120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8024440407752991, loss=2.068572521209717
I0215 02:04:20.940725 140532797925120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8948532938957214, loss=2.079558849334717
I0215 02:05:36.007841 140532789532416 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.0276401042938232, loss=2.073118209838867
I0215 02:06:51.194453 140532797925120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7089601755142212, loss=2.031782865524292
I0215 02:08:06.431232 140532789532416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7935292720794678, loss=2.0123817920684814
I0215 02:09:26.475524 140532797925120 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.0376362800598145, loss=2.004070997238159
I0215 02:10:51.056011 140532789532416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9660373330116272, loss=2.025001049041748
I0215 02:12:16.683174 140532797925120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8186861276626587, loss=1.98216712474823
I0215 02:13:41.694613 140532789532416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7142565846443176, loss=2.096302032470703
I0215 02:15:07.563571 140532797925120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7414759397506714, loss=1.9887081384658813
I0215 02:16:33.260514 140532789532416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7136231064796448, loss=2.024228572845459
I0215 02:17:59.080798 140532797925120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7120744585990906, loss=1.9988811016082764
I0215 02:19:14.416648 140532789532416 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.009820580482483, loss=2.0033352375030518
I0215 02:20:00.051670 140599226058560 spec.py:321] Evaluating on the training split.
I0215 02:20:54.573545 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 02:21:46.268734 140599226058560 spec.py:349] Evaluating on the test split.
I0215 02:22:12.592801 140599226058560 submission_runner.py:408] Time since start: 17373.69s, 	Step: 19762, 	{'train/ctc_loss': Array(0.69894594, dtype=float32), 'train/wer': 0.24397619327470696, 'validation/ctc_loss': Array(1.0527449, dtype=float32), 'validation/wer': 0.3146741071859583, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.75496924, dtype=float32), 'test/wer': 0.2517214063737737, 'test/num_examples': 2472, 'score': 15878.671095132828, 'total_duration': 17373.685792684555, 'accumulated_submission_time': 15878.671095132828, 'accumulated_eval_time': 1493.616887807846, 'accumulated_logging_time': 0.5617284774780273}
I0215 02:22:12.636558 140532797925120 logging_writer.py:48] [19762] accumulated_eval_time=1493.616888, accumulated_logging_time=0.561728, accumulated_submission_time=15878.671095, global_step=19762, preemption_count=0, score=15878.671095, test/ctc_loss=0.7549692392349243, test/num_examples=2472, test/wer=0.251721, total_duration=17373.685793, train/ctc_loss=0.698945939540863, train/wer=0.243976, validation/ctc_loss=1.0527448654174805, validation/num_examples=5348, validation/wer=0.314674
I0215 02:22:41.984180 140532789532416 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.0285491943359375, loss=1.9603267908096313
I0215 02:23:57.422808 140532797925120 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.7561634182929993, loss=1.9378786087036133
I0215 02:25:12.982456 140532789532416 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.8698228001594543, loss=1.9306399822235107
I0215 02:26:28.706516 140532797925120 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.3813713788986206, loss=1.8631348609924316
I0215 02:27:49.768139 140532789532416 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7364758253097534, loss=1.9530984163284302
I0215 02:29:15.030515 140532797925120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7882989048957825, loss=1.8933097124099731
I0215 02:30:40.332336 140532789532416 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.674990177154541, loss=1.9015697240829468
I0215 02:32:05.533326 140532797925120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7549072504043579, loss=1.958470344543457
I0215 02:33:35.707246 140532797925120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6856162548065186, loss=1.936050534248352
I0215 02:34:50.897900 140532789532416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.7584766149520874, loss=1.8723198175430298
I0215 02:36:06.238967 140532797925120 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0984206199645996, loss=1.9380244016647339
I0215 02:37:21.542359 140532789532416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8306389451026917, loss=1.9438755512237549
I0215 02:38:40.169147 140532797925120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7084007263183594, loss=1.9094475507736206
I0215 02:40:04.789877 140532789532416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6958216428756714, loss=1.8813364505767822
I0215 02:41:30.648452 140532797925120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7881478071212769, loss=1.8092453479766846
I0215 02:42:56.793517 140532789532416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.8179405927658081, loss=1.8751293420791626
I0215 02:44:22.035270 140532797925120 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.029029369354248, loss=1.883887767791748
I0215 02:45:47.130802 140532789532416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8358454704284668, loss=1.8699936866760254
I0215 02:46:12.813486 140599226058560 spec.py:321] Evaluating on the training split.
I0215 02:47:07.517902 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 02:47:59.073179 140599226058560 spec.py:349] Evaluating on the test split.
I0215 02:48:25.406716 140599226058560 submission_runner.py:408] Time since start: 18946.50s, 	Step: 21532, 	{'train/ctc_loss': Array(0.62691694, dtype=float32), 'train/wer': 0.22013692162417375, 'validation/ctc_loss': Array(0.92000806, dtype=float32), 'validation/wer': 0.2802745783330276, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6438148, dtype=float32), 'test/wer': 0.21741514837608922, 'test/num_examples': 2472, 'score': 17318.76188802719, 'total_duration': 18946.500923633575, 'accumulated_submission_time': 17318.76188802719, 'accumulated_eval_time': 1626.2040507793427, 'accumulated_logging_time': 0.6224701404571533}
I0215 02:48:25.447649 140532797925120 logging_writer.py:48] [21532] accumulated_eval_time=1626.204051, accumulated_logging_time=0.622470, accumulated_submission_time=17318.761888, global_step=21532, preemption_count=0, score=17318.761888, test/ctc_loss=0.6438148021697998, test/num_examples=2472, test/wer=0.217415, total_duration=18946.500924, train/ctc_loss=0.6269169449806213, train/wer=0.220137, validation/ctc_loss=0.9200080633163452, validation/num_examples=5348, validation/wer=0.280275
I0215 02:49:17.235628 140532789532416 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7353638410568237, loss=1.8286902904510498
I0215 02:50:36.478976 140532797925120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8120149970054626, loss=1.8002965450286865
I0215 02:51:51.773653 140532789532416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.9170945882797241, loss=1.8188786506652832
I0215 02:53:07.106895 140532797925120 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9825019240379333, loss=1.7864766120910645
I0215 02:54:22.393459 140532789532416 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.1061513423919678, loss=1.8539294004440308
I0215 02:55:43.892375 140532797925120 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1242133378982544, loss=1.8350952863693237
I0215 02:57:08.912753 140532789532416 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.8251491785049438, loss=1.8217294216156006
I0215 02:58:34.762848 140532797925120 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.3698188066482544, loss=1.8303263187408447
I0215 03:00:00.705612 140532789532416 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7796649932861328, loss=1.7926287651062012
I0215 03:01:26.264295 140532797925120 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6979548335075378, loss=1.771710753440857
I0215 03:02:52.042067 140532789532416 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.8123698830604553, loss=1.8491990566253662
I0215 03:04:16.920952 140532797925120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.785963237285614, loss=1.7937716245651245
I0215 03:05:32.106213 140532789532416 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8473710417747498, loss=1.7782375812530518
I0215 03:06:47.365025 140532797925120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.7261949777603149, loss=1.743965983390808
I0215 03:08:02.568848 140532789532416 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.2247798442840576, loss=1.807509183883667
I0215 03:09:21.761223 140532797925120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.7105258107185364, loss=1.7657127380371094
I0215 03:10:45.857908 140532789532416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.9902371168136597, loss=1.7802996635437012
I0215 03:12:10.405157 140532797925120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.64554762840271, loss=1.7757829427719116
I0215 03:12:25.740217 140599226058560 spec.py:321] Evaluating on the training split.
I0215 03:13:21.196043 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 03:14:13.357969 140599226058560 spec.py:349] Evaluating on the test split.
I0215 03:14:39.666228 140599226058560 submission_runner.py:408] Time since start: 20520.76s, 	Step: 23320, 	{'train/ctc_loss': Array(0.5428806, dtype=float32), 'train/wer': 0.1951972010178117, 'validation/ctc_loss': Array(0.85429704, dtype=float32), 'validation/wer': 0.2618341909883468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58487004, dtype=float32), 'test/wer': 0.19905348038916987, 'test/num_examples': 2472, 'score': 18758.965651988983, 'total_duration': 20520.760549545288, 'accumulated_submission_time': 18758.965651988983, 'accumulated_eval_time': 1760.1241040229797, 'accumulated_logging_time': 0.6804590225219727}
I0215 03:14:39.705099 140532797925120 logging_writer.py:48] [23320] accumulated_eval_time=1760.124104, accumulated_logging_time=0.680459, accumulated_submission_time=18758.965652, global_step=23320, preemption_count=0, score=18758.965652, test/ctc_loss=0.5848700404167175, test/num_examples=2472, test/wer=0.199053, total_duration=20520.760550, train/ctc_loss=0.5428805947303772, train/wer=0.195197, validation/ctc_loss=0.8542970418930054, validation/num_examples=5348, validation/wer=0.261834
I0215 03:15:40.434493 140532789532416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.9872751235961914, loss=1.7926563024520874
I0215 03:16:55.884605 140532797925120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.7579617500305176, loss=1.7689930200576782
I0215 03:18:11.194256 140532789532416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7228446006774902, loss=1.7775788307189941
I0215 03:19:36.711747 140532797925120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7954680919647217, loss=1.7517316341400146
I0215 03:20:51.922262 140532789532416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8620503544807434, loss=1.7716779708862305
I0215 03:22:07.226428 140532797925120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8632302284240723, loss=1.7292083501815796
I0215 03:23:22.597491 140532789532416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7383638620376587, loss=1.7275031805038452
I0215 03:24:38.914301 140532797925120 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.002665638923645, loss=1.7584964036941528
I0215 03:26:03.199881 140532789532416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7184640169143677, loss=1.770877480506897
I0215 03:27:28.973780 140532797925120 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.3149830102920532, loss=1.7168481349945068
I0215 03:28:55.214415 140532789532416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7511683702468872, loss=1.7862602472305298
I0215 03:30:21.598802 140532797925120 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.287850260734558, loss=1.7218666076660156
I0215 03:31:46.828230 140532789532416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.7689019441604614, loss=1.7008477449417114
I0215 03:33:13.226155 140532797925120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6555450558662415, loss=1.7457351684570312
I0215 03:34:34.320219 140532797925120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.7477826476097107, loss=1.7712879180908203
I0215 03:35:49.609638 140532789532416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6885045766830444, loss=1.7123620510101318
I0215 03:37:04.863147 140532797925120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7985767126083374, loss=1.7673344612121582
I0215 03:38:20.171799 140532789532416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.6677718758583069, loss=1.6591930389404297
I0215 03:38:40.191621 140599226058560 spec.py:321] Evaluating on the training split.
I0215 03:39:35.613739 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 03:40:27.631382 140599226058560 spec.py:349] Evaluating on the test split.
I0215 03:40:53.910511 140599226058560 submission_runner.py:408] Time since start: 22095.01s, 	Step: 25128, 	{'train/ctc_loss': Array(0.44918406, dtype=float32), 'train/wer': 0.16700946971674374, 'validation/ctc_loss': Array(0.79768723, dtype=float32), 'validation/wer': 0.24599090531681744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.534569, dtype=float32), 'test/wer': 0.18357605670993032, 'test/num_examples': 2472, 'score': 20199.365215063095, 'total_duration': 22095.005754709244, 'accumulated_submission_time': 20199.365215063095, 'accumulated_eval_time': 1893.8379728794098, 'accumulated_logging_time': 0.7349987030029297}
I0215 03:40:53.947249 140532797925120 logging_writer.py:48] [25128] accumulated_eval_time=1893.837973, accumulated_logging_time=0.734999, accumulated_submission_time=20199.365215, global_step=25128, preemption_count=0, score=20199.365215, test/ctc_loss=0.5345690250396729, test/num_examples=2472, test/wer=0.183576, total_duration=22095.005755, train/ctc_loss=0.4491840600967407, train/wer=0.167009, validation/ctc_loss=0.7976872324943542, validation/num_examples=5348, validation/wer=0.245991
I0215 03:41:48.854015 140532789532416 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2819150686264038, loss=1.671790361404419
I0215 03:43:04.153061 140532797925120 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.8019315004348755, loss=1.7216445207595825
I0215 03:44:19.513989 140532789532416 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.6404460668563843, loss=1.6599981784820557
I0215 03:45:41.399917 140532797925120 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.0831127166748047, loss=1.66011381149292
I0215 03:47:06.935079 140532789532416 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.9373339414596558, loss=1.7138394117355347
I0215 03:48:32.724161 140532797925120 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.8787853121757507, loss=1.7195953130722046
I0215 03:49:57.049769 140532797925120 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.0869470834732056, loss=1.6640547513961792
I0215 03:51:12.280322 140532789532416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.8734548091888428, loss=1.6943939924240112
I0215 03:52:27.686537 140532797925120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7136815190315247, loss=1.7186033725738525
I0215 03:53:42.969966 140532789532416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.8040163516998291, loss=1.682174801826477
I0215 03:55:03.887685 140532797925120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7531477212905884, loss=1.6625099182128906
I0215 03:56:29.171668 140532789532416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7697897553443909, loss=1.6641815900802612
I0215 03:57:54.681468 140532797925120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6903946399688721, loss=1.6522297859191895
I0215 03:59:19.478385 140532789532416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6744906306266785, loss=1.7237547636032104
I0215 04:00:45.332770 140532797925120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.6994063258171082, loss=1.65127694606781
I0215 04:02:10.031282 140532789532416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8507516980171204, loss=1.6899522542953491
I0215 04:03:37.106184 140532797925120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.8732890486717224, loss=1.7596156597137451
I0215 04:04:52.711934 140532789532416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6821997761726379, loss=1.7136611938476562
I0215 04:04:53.938167 140599226058560 spec.py:321] Evaluating on the training split.
I0215 04:05:47.898739 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 04:06:39.209793 140599226058560 spec.py:349] Evaluating on the test split.
I0215 04:07:05.279818 140599226058560 submission_runner.py:408] Time since start: 23666.37s, 	Step: 26903, 	{'train/ctc_loss': Array(0.4337378, dtype=float32), 'train/wer': 0.15935238745757646, 'validation/ctc_loss': Array(0.77539897, dtype=float32), 'validation/wer': 0.23939677727680855, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51442176, dtype=float32), 'test/wer': 0.17604046066662604, 'test/num_examples': 2472, 'score': 21639.267746448517, 'total_duration': 23666.374056100845, 'accumulated_submission_time': 21639.267746448517, 'accumulated_eval_time': 2025.1735785007477, 'accumulated_logging_time': 0.7892842292785645}
I0215 04:07:05.319183 140532797925120 logging_writer.py:48] [26903] accumulated_eval_time=2025.173579, accumulated_logging_time=0.789284, accumulated_submission_time=21639.267746, global_step=26903, preemption_count=0, score=21639.267746, test/ctc_loss=0.5144217610359192, test/num_examples=2472, test/wer=0.176040, total_duration=23666.374056, train/ctc_loss=0.4337378144264221, train/wer=0.159352, validation/ctc_loss=0.7753989696502686, validation/num_examples=5348, validation/wer=0.239397
I0215 04:08:18.979432 140532789532416 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.0881909132003784, loss=1.7363661527633667
I0215 04:09:34.298734 140532797925120 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6542847156524658, loss=1.6152454614639282
I0215 04:10:49.651783 140532789532416 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.3033422231674194, loss=1.6036893129348755
I0215 04:12:04.997852 140532797925120 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.6789920926094055, loss=1.60219407081604
I0215 04:13:29.213395 140532789532416 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7773434519767761, loss=1.6134506464004517
I0215 04:14:55.171966 140532797925120 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1692277193069458, loss=1.5865100622177124
I0215 04:16:21.555828 140532789532416 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8985551595687866, loss=1.7136203050613403
I0215 04:17:47.764150 140532797925120 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.766142725944519, loss=1.6296346187591553
I0215 04:19:13.671216 140532789532416 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6744486689567566, loss=1.647491693496704
I0215 04:20:33.299389 140532797925120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.7021414637565613, loss=1.6020889282226562
I0215 04:21:48.915619 140532789532416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6227276921272278, loss=1.5763100385665894
I0215 04:23:04.287023 140532797925120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.8521440625190735, loss=1.6539666652679443
I0215 04:24:19.858121 140532789532416 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.21845543384552, loss=1.6174204349517822
I0215 04:25:40.394017 140532797925120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5755251049995422, loss=1.6189486980438232
I0215 04:27:05.058665 140532789532416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.691850483417511, loss=1.6073089838027954
I0215 04:28:29.321885 140532797925120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.7835230827331543, loss=1.6345866918563843
I0215 04:29:53.402186 140532789532416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.7075692415237427, loss=1.6119942665100098
I0215 04:31:05.402226 140599226058560 spec.py:321] Evaluating on the training split.
I0215 04:32:00.039402 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 04:32:51.407217 140599226058560 spec.py:349] Evaluating on the test split.
I0215 04:33:17.485230 140599226058560 submission_runner.py:408] Time since start: 25238.58s, 	Step: 28686, 	{'train/ctc_loss': Array(0.3969769, dtype=float32), 'train/wer': 0.1507755277897458, 'validation/ctc_loss': Array(0.7187384, dtype=float32), 'validation/wer': 0.22330247062571806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4679157, dtype=float32), 'test/wer': 0.1655596855767473, 'test/num_examples': 2472, 'score': 23079.26572728157, 'total_duration': 25238.579748630524, 'accumulated_submission_time': 23079.26572728157, 'accumulated_eval_time': 2157.2508704662323, 'accumulated_logging_time': 0.8437254428863525}
I0215 04:33:17.525875 140532797925120 logging_writer.py:48] [28686] accumulated_eval_time=2157.250870, accumulated_logging_time=0.843725, accumulated_submission_time=23079.265727, global_step=28686, preemption_count=0, score=23079.265727, test/ctc_loss=0.46791571378707886, test/num_examples=2472, test/wer=0.165560, total_duration=25238.579749, train/ctc_loss=0.39697688817977905, train/wer=0.150776, validation/ctc_loss=0.7187383770942688, validation/num_examples=5348, validation/wer=0.223302
I0215 04:33:28.858083 140532789532416 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.7160229682922363, loss=1.578995943069458
I0215 04:34:43.952377 140532797925120 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.6512488126754761, loss=1.5799415111541748
I0215 04:36:02.806071 140532797925120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.6235401034355164, loss=1.6087931394577026
I0215 04:37:17.866077 140532789532416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.843710720539093, loss=1.564958095550537
I0215 04:38:33.286659 140532797925120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.6966091394424438, loss=1.6031779050827026
I0215 04:39:48.437582 140532789532416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9828237891197205, loss=1.5892466306686401
I0215 04:41:09.522414 140532797925120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6479974389076233, loss=1.614715337753296
I0215 04:42:34.735276 140532789532416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.9311987161636353, loss=1.5675421953201294
I0215 04:44:00.277167 140532797925120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7234616875648499, loss=1.622060775756836
I0215 04:45:25.030268 140532789532416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.8596420884132385, loss=1.5662635564804077
I0215 04:46:50.477678 140532797925120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.6592874526977539, loss=1.6139044761657715
I0215 04:48:16.728780 140532789532416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.638090968132019, loss=1.5616997480392456
I0215 04:49:43.326241 140532797925120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.9060631990432739, loss=1.522830843925476
I0215 04:50:58.750835 140532789532416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7319000959396362, loss=1.5458928346633911
I0215 04:52:14.175737 140532797925120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.819582462310791, loss=1.5042797327041626
I0215 04:53:29.931211 140532789532416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.6812377572059631, loss=1.5278321504592896
I0215 04:54:45.932220 140532797925120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.824916422367096, loss=1.5518814325332642
I0215 04:56:11.420258 140532789532416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.7589571475982666, loss=1.5179792642593384
I0215 04:57:17.686716 140599226058560 spec.py:321] Evaluating on the training split.
I0215 04:58:13.770146 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 04:59:06.081239 140599226058560 spec.py:349] Evaluating on the test split.
I0215 04:59:32.372756 140599226058560 submission_runner.py:408] Time since start: 26813.47s, 	Step: 30480, 	{'train/ctc_loss': Array(0.37746027, dtype=float32), 'train/wer': 0.13832676898003532, 'validation/ctc_loss': Array(0.68599296, dtype=float32), 'validation/wer': 0.21370574548403604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44529587, dtype=float32), 'test/wer': 0.15583043893323584, 'test/num_examples': 2472, 'score': 24519.34055161476, 'total_duration': 26813.466148376465, 'accumulated_submission_time': 24519.34055161476, 'accumulated_eval_time': 2291.930029153824, 'accumulated_logging_time': 0.8999524116516113}
I0215 04:59:32.412095 140532797925120 logging_writer.py:48] [30480] accumulated_eval_time=2291.930029, accumulated_logging_time=0.899952, accumulated_submission_time=24519.340552, global_step=30480, preemption_count=0, score=24519.340552, test/ctc_loss=0.44529587030410767, test/num_examples=2472, test/wer=0.155830, total_duration=26813.466148, train/ctc_loss=0.3774602711200714, train/wer=0.138327, validation/ctc_loss=0.685992956161499, validation/num_examples=5348, validation/wer=0.213706
I0215 04:59:48.283944 140532789532416 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6924724578857422, loss=1.554481863975525
I0215 05:01:03.557685 140532797925120 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.9921384453773499, loss=1.5287998914718628
I0215 05:02:18.925298 140532789532416 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6929848194122314, loss=1.5754011869430542
I0215 05:03:34.764378 140532797925120 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.9142481088638306, loss=1.485097050666809
I0215 05:05:03.110449 140532797925120 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0178707838058472, loss=1.5461910963058472
I0215 05:06:18.306383 140532789532416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7744040489196777, loss=1.555256724357605
I0215 05:07:33.622719 140532797925120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6505495309829712, loss=1.493114948272705
I0215 05:08:49.002106 140532789532416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.7542855739593506, loss=1.5083565711975098
I0215 05:10:06.388196 140532797925120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6940625905990601, loss=1.5669292211532593
I0215 05:11:30.331415 140532789532416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.745743453502655, loss=1.575520634651184
I0215 05:12:56.341442 140532797925120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7482578754425049, loss=1.4744998216629028
I0215 05:14:22.136220 140532789532416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6924164295196533, loss=1.560222864151001
I0215 05:15:47.026710 140532797925120 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.036532998085022, loss=1.5712707042694092
I0215 05:17:12.120161 140532789532416 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.4245418310165405, loss=1.5002882480621338
I0215 05:18:38.484506 140532797925120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7645342350006104, loss=1.5663551092147827
I0215 05:19:59.756410 140532797925120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.9479002356529236, loss=1.5163379907608032
I0215 05:21:15.297532 140532789532416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.7993085980415344, loss=1.5183186531066895
I0215 05:22:30.800586 140532797925120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.8347914218902588, loss=1.5280486345291138
I0215 05:23:32.994996 140599226058560 spec.py:321] Evaluating on the training split.
I0215 05:24:28.385422 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 05:25:20.045779 140599226058560 spec.py:349] Evaluating on the test split.
I0215 05:25:46.589082 140599226058560 submission_runner.py:408] Time since start: 28387.68s, 	Step: 32284, 	{'train/ctc_loss': Array(0.36455256, dtype=float32), 'train/wer': 0.13647401651287033, 'validation/ctc_loss': Array(0.659797, dtype=float32), 'validation/wer': 0.20718885466850748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42393988, dtype=float32), 'test/wer': 0.14782767655840595, 'test/num_examples': 2472, 'score': 25959.834961652756, 'total_duration': 28387.683755159378, 'accumulated_submission_time': 25959.834961652756, 'accumulated_eval_time': 2425.518532037735, 'accumulated_logging_time': 0.9573686122894287}
I0215 05:25:46.630270 140532797925120 logging_writer.py:48] [32284] accumulated_eval_time=2425.518532, accumulated_logging_time=0.957369, accumulated_submission_time=25959.834962, global_step=32284, preemption_count=0, score=25959.834962, test/ctc_loss=0.42393988370895386, test/num_examples=2472, test/wer=0.147828, total_duration=28387.683755, train/ctc_loss=0.3645525574684143, train/wer=0.136474, validation/ctc_loss=0.6597970128059387, validation/num_examples=5348, validation/wer=0.207189
I0215 05:25:59.473645 140532789532416 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0039000511169434, loss=1.5150212049484253
I0215 05:27:14.792578 140532797925120 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.1239371299743652, loss=1.4585009813308716
I0215 05:28:30.038957 140532789532416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.731728732585907, loss=1.4929327964782715
I0215 05:29:48.310748 140532797925120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.6767047643661499, loss=1.5125141143798828
I0215 05:31:13.237615 140532789532416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.8276709318161011, loss=1.5210667848587036
I0215 05:32:38.646380 140532797925120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.7170565128326416, loss=1.5480135679244995
I0215 05:34:04.810011 140532789532416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.7417725324630737, loss=1.4541229009628296
I0215 05:35:29.364907 140532797925120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.79121333360672, loss=1.5103150606155396
I0215 05:36:44.445727 140532789532416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6926165819168091, loss=1.4866222143173218
I0215 05:37:59.609304 140532797925120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.9464952349662781, loss=1.5241446495056152
I0215 05:39:14.822388 140532789532416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.7191436290740967, loss=1.4976050853729248
I0215 05:40:34.804363 140532797925120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6726619601249695, loss=1.5151797533035278
I0215 05:42:00.540844 140532789532416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6787437796592712, loss=1.4637038707733154
I0215 05:43:25.712356 140532797925120 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.592233419418335, loss=1.491428256034851
I0215 05:44:51.366428 140532789532416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.8510033488273621, loss=1.5512845516204834
I0215 05:46:16.610267 140532797925120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.9072775840759277, loss=1.5238735675811768
I0215 05:47:42.423959 140532789532416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6878389120101929, loss=1.5153244733810425
I0215 05:49:10.716908 140532797925120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7799429297447205, loss=1.4879040718078613
I0215 05:49:47.170904 140599226058560 spec.py:321] Evaluating on the training split.
I0215 05:50:42.871701 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 05:51:34.519248 140599226058560 spec.py:349] Evaluating on the test split.
I0215 05:52:01.035873 140599226058560 submission_runner.py:408] Time since start: 29962.13s, 	Step: 34050, 	{'train/ctc_loss': Array(0.34491673, dtype=float32), 'train/wer': 0.13033875247637422, 'validation/ctc_loss': Array(0.6365954, dtype=float32), 'validation/wer': 0.1974473097309248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39865455, dtype=float32), 'test/wer': 0.13779375622042125, 'test/num_examples': 2472, 'score': 27400.28605747223, 'total_duration': 29962.1297519207, 'accumulated_submission_time': 27400.28605747223, 'accumulated_eval_time': 2559.377106666565, 'accumulated_logging_time': 1.0186927318572998}
I0215 05:52:01.099601 140532797925120 logging_writer.py:48] [34050] accumulated_eval_time=2559.377107, accumulated_logging_time=1.018693, accumulated_submission_time=27400.286057, global_step=34050, preemption_count=0, score=27400.286057, test/ctc_loss=0.3986545503139496, test/num_examples=2472, test/wer=0.137794, total_duration=29962.129752, train/ctc_loss=0.3449167311191559, train/wer=0.130339, validation/ctc_loss=0.6365954279899597, validation/num_examples=5348, validation/wer=0.197447
I0215 05:52:39.595018 140532789532416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.8705483675003052, loss=1.4972994327545166
I0215 05:53:54.984179 140532797925120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.8562583327293396, loss=1.4745005369186401
I0215 05:55:10.423182 140532789532416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.7626873850822449, loss=1.4673770666122437
I0215 05:56:25.924465 140532797925120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7257872223854065, loss=1.5337331295013428
I0215 05:57:41.404248 140532789532416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.8416155576705933, loss=1.5064315795898438
I0215 05:59:04.509759 140532797925120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.9289132356643677, loss=1.504872441291809
I0215 06:00:31.514294 140532789532416 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.0044413805007935, loss=1.5204875469207764
I0215 06:01:57.976118 140532797925120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.9522823691368103, loss=1.5240620374679565
I0215 06:03:24.357802 140532789532416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6614921689033508, loss=1.4380645751953125
I0215 06:04:50.689390 140532797925120 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.8088667392730713, loss=1.479498267173767
I0215 06:06:11.455702 140532797925120 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1669502258300781, loss=1.498782753944397
I0215 06:07:26.714710 140532789532416 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.0783963203430176, loss=1.471472144126892
I0215 06:08:42.011409 140532797925120 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.8498378992080688, loss=1.497213363647461
I0215 06:09:57.303472 140532789532416 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.2418088912963867, loss=1.4817514419555664
I0215 06:11:19.451252 140532797925120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7886705994606018, loss=1.4733691215515137
I0215 06:12:44.694887 140532789532416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.8233548402786255, loss=1.4399776458740234
I0215 06:14:09.982448 140532797925120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.7404618263244629, loss=1.5185900926589966
I0215 06:15:37.310728 140532789532416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.9675507545471191, loss=1.4844335317611694
I0215 06:16:01.284542 140599226058560 spec.py:321] Evaluating on the training split.
I0215 06:16:56.456980 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 06:17:48.269032 140599226058560 spec.py:349] Evaluating on the test split.
I0215 06:18:14.764593 140599226058560 submission_runner.py:408] Time since start: 31535.86s, 	Step: 35829, 	{'train/ctc_loss': Array(0.33196563, dtype=float32), 'train/wer': 0.12498755573719812, 'validation/ctc_loss': Array(0.6245508, dtype=float32), 'validation/wer': 0.1948598627108335, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39754122, dtype=float32), 'test/wer': 0.13694066987589626, 'test/num_examples': 2472, 'score': 28840.38019633293, 'total_duration': 31535.858542203903, 'accumulated_submission_time': 28840.38019633293, 'accumulated_eval_time': 2692.8508801460266, 'accumulated_logging_time': 1.1042413711547852}
I0215 06:18:14.802247 140532797925120 logging_writer.py:48] [35829] accumulated_eval_time=2692.850880, accumulated_logging_time=1.104241, accumulated_submission_time=28840.380196, global_step=35829, preemption_count=0, score=28840.380196, test/ctc_loss=0.39754122495651245, test/num_examples=2472, test/wer=0.136941, total_duration=31535.858542, train/ctc_loss=0.3319656252861023, train/wer=0.124988, validation/ctc_loss=0.6245508193969727, validation/num_examples=5348, validation/wer=0.194860
I0215 06:19:08.778600 140532789532416 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.6661248803138733, loss=1.509765625
I0215 06:20:24.281051 140532797925120 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7190019488334656, loss=1.5146257877349854
I0215 06:21:43.168898 140532797925120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.696267306804657, loss=1.4869542121887207
I0215 06:22:58.342471 140532789532416 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.034437894821167, loss=1.4935804605484009
I0215 06:24:13.592101 140532797925120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.8311111330986023, loss=1.4566656351089478
I0215 06:25:28.898240 140532789532416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7587085366249084, loss=1.433209776878357
I0215 06:26:48.866540 140532797925120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.7235996723175049, loss=1.4347407817840576
I0215 06:28:14.012929 140532789532416 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0230696201324463, loss=1.4570889472961426
I0215 06:29:40.020399 140532797925120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.9639713764190674, loss=1.496932029724121
I0215 06:31:05.154151 140532789532416 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.4703491926193237, loss=1.4480600357055664
I0215 06:32:30.476184 140532797925120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8195961117744446, loss=1.4857444763183594
I0215 06:33:55.537361 140532789532416 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7824787497520447, loss=1.441987156867981
I0215 06:35:22.001199 140532797925120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.6979012489318848, loss=1.4076862335205078
I0215 06:36:37.323588 140532789532416 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1687148809432983, loss=1.4686670303344727
I0215 06:37:52.707373 140532797925120 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.0295521020889282, loss=1.4421651363372803
I0215 06:39:08.144293 140532789532416 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.1517854928970337, loss=1.4067238569259644
I0215 06:40:23.706119 140532797925120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6544996500015259, loss=1.4366371631622314
I0215 06:41:49.277587 140532789532416 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1801273822784424, loss=1.465063452720642
I0215 06:42:15.315433 140599226058560 spec.py:321] Evaluating on the training split.
I0215 06:43:11.477744 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 06:44:03.687880 140599226058560 spec.py:349] Evaluating on the test split.
I0215 06:44:29.606100 140599226058560 submission_runner.py:408] Time since start: 33110.70s, 	Step: 37632, 	{'train/ctc_loss': Array(0.2906541, dtype=float32), 'train/wer': 0.11264768994732985, 'validation/ctc_loss': Array(0.6097289, dtype=float32), 'validation/wer': 0.19014839201753286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37677324, dtype=float32), 'test/wer': 0.13182215180874618, 'test/num_examples': 2472, 'score': 30280.80466890335, 'total_duration': 33110.70022511482, 'accumulated_submission_time': 30280.80466890335, 'accumulated_eval_time': 2827.135392189026, 'accumulated_logging_time': 1.1598222255706787}
I0215 06:44:29.643334 140532797925120 logging_writer.py:48] [37632] accumulated_eval_time=2827.135392, accumulated_logging_time=1.159822, accumulated_submission_time=30280.804669, global_step=37632, preemption_count=0, score=30280.804669, test/ctc_loss=0.37677323818206787, test/num_examples=2472, test/wer=0.131822, total_duration=33110.700225, train/ctc_loss=0.29065409302711487, train/wer=0.112648, validation/ctc_loss=0.6097288727760315, validation/num_examples=5348, validation/wer=0.190148
I0215 06:45:21.508025 140532789532416 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.2580746412277222, loss=1.4622434377670288
I0215 06:46:36.789398 140532797925120 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6594366431236267, loss=1.4237275123596191
I0215 06:47:52.022680 140532789532416 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.7038636803627014, loss=1.4357028007507324
I0215 06:49:11.492067 140532797925120 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8266633152961731, loss=1.4781678915023804
I0215 06:50:35.921602 140532789532416 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6854661107063293, loss=1.4059903621673584
I0215 06:51:55.704376 140532797925120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.8676422238349915, loss=1.4451392889022827
I0215 06:53:10.847997 140532789532416 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.0972907543182373, loss=1.44230318069458
I0215 06:54:26.058360 140532797925120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.7040988802909851, loss=1.396611213684082
I0215 06:55:41.770119 140532789532416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8472416400909424, loss=1.4450199604034424
I0215 06:57:05.180730 140532797925120 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.6718953847885132, loss=1.4314323663711548
I0215 06:58:30.675133 140532789532416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.8896985650062561, loss=1.4283736944198608
I0215 06:59:55.550066 140532797925120 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7893878817558289, loss=1.387772798538208
I0215 07:01:21.058398 140532789532416 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.8283110857009888, loss=1.4395698308944702
I0215 07:02:47.225699 140532797925120 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.0343811511993408, loss=1.3921705484390259
I0215 07:04:12.551436 140532789532416 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.6779270172119141, loss=1.428750991821289
I0215 07:05:35.164631 140532797925120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.8143084645271301, loss=1.397019863128662
I0215 07:06:50.398829 140532789532416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.6657112240791321, loss=1.3958492279052734
I0215 07:08:05.664083 140532797925120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9001463055610657, loss=1.4300569295883179
I0215 07:08:30.224927 140599226058560 spec.py:321] Evaluating on the training split.
I0215 07:09:26.264064 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 07:10:17.407166 140599226058560 spec.py:349] Evaluating on the test split.
I0215 07:10:43.441792 140599226058560 submission_runner.py:408] Time since start: 34684.54s, 	Step: 39434, 	{'train/ctc_loss': Array(0.2863048, dtype=float32), 'train/wer': 0.1086859870300435, 'validation/ctc_loss': Array(0.5883182, dtype=float32), 'validation/wer': 0.18471282234472905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37109178, dtype=float32), 'test/wer': 0.12936445067332886, 'test/num_examples': 2472, 'score': 31721.29601264, 'total_duration': 34684.53674340248, 'accumulated_submission_time': 31721.29601264, 'accumulated_eval_time': 2960.3469581604004, 'accumulated_logging_time': 1.2174606323242188}
I0215 07:10:43.483932 140532797925120 logging_writer.py:48] [39434] accumulated_eval_time=2960.346958, accumulated_logging_time=1.217461, accumulated_submission_time=31721.296013, global_step=39434, preemption_count=0, score=31721.296013, test/ctc_loss=0.3710917830467224, test/num_examples=2472, test/wer=0.129364, total_duration=34684.536743, train/ctc_loss=0.2863048017024994, train/wer=0.108686, validation/ctc_loss=0.5883182287216187, validation/num_examples=5348, validation/wer=0.184713
I0215 07:11:33.845805 140532789532416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.6543179154396057, loss=1.4190292358398438
I0215 07:12:49.240756 140532797925120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7772069573402405, loss=1.381333351135254
I0215 07:14:04.693751 140532789532416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7738064527511597, loss=1.4250891208648682
I0215 07:15:24.286151 140532797925120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7004114389419556, loss=1.365326166152954
I0215 07:16:49.776913 140532789532416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7245723009109497, loss=1.3987629413604736
I0215 07:18:15.955438 140532797925120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7205118536949158, loss=1.3736414909362793
I0215 07:19:42.310034 140532789532416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.8131319880485535, loss=1.3694956302642822
I0215 07:21:08.049071 140532797925120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.9955670833587646, loss=1.4644992351531982
I0215 07:22:23.570223 140532789532416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7628311514854431, loss=1.4052859544754028
I0215 07:23:38.901664 140532797925120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6668630242347717, loss=1.4390217065811157
I0215 07:24:54.226067 140532789532416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7921979427337646, loss=1.4185452461242676
I0215 07:26:11.169814 140532797925120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.8347135782241821, loss=1.3943426609039307
I0215 07:27:36.103299 140532789532416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.8948864340782166, loss=1.3896887302398682
I0215 07:29:01.530389 140532797925120 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.033982276916504, loss=1.4138792753219604
I0215 07:30:28.154035 140532789532416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.8090632557868958, loss=1.41034996509552
I0215 07:31:53.988720 140532797925120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.8795311450958252, loss=1.3746278285980225
I0215 07:33:21.227914 140532789532416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.9468743801116943, loss=1.3982080221176147
I0215 07:34:44.025418 140599226058560 spec.py:321] Evaluating on the training split.
I0215 07:35:37.529384 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 07:36:29.074629 140599226058560 spec.py:349] Evaluating on the test split.
I0215 07:36:55.280884 140599226058560 submission_runner.py:408] Time since start: 36256.37s, 	Step: 41198, 	{'train/ctc_loss': Array(0.28539166, dtype=float32), 'train/wer': 0.10729820385198009, 'validation/ctc_loss': Array(0.5655498, dtype=float32), 'validation/wer': 0.176255346264132, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.351271, dtype=float32), 'test/wer': 0.12142262303739362, 'test/num_examples': 2472, 'score': 33161.7505197525, 'total_duration': 36256.37497663498, 'accumulated_submission_time': 33161.7505197525, 'accumulated_eval_time': 3091.5962493419647, 'accumulated_logging_time': 1.2778022289276123}
I0215 07:36:55.320899 140532797925120 logging_writer.py:48] [41198] accumulated_eval_time=3091.596249, accumulated_logging_time=1.277802, accumulated_submission_time=33161.750520, global_step=41198, preemption_count=0, score=33161.750520, test/ctc_loss=0.35127100348472595, test/num_examples=2472, test/wer=0.121423, total_duration=36256.374977, train/ctc_loss=0.2853916585445404, train/wer=0.107298, validation/ctc_loss=0.5655497908592224, validation/num_examples=5348, validation/wer=0.176255
I0215 07:37:01.561232 140532797925120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6809577345848083, loss=1.3795981407165527
I0215 07:38:16.626953 140532789532416 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.2070271968841553, loss=1.3431223630905151
I0215 07:39:32.136357 140532797925120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7888744473457336, loss=1.3827553987503052
I0215 07:40:47.396072 140532789532416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9262401461601257, loss=1.3570011854171753
I0215 07:42:08.350783 140532797925120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.8271729946136475, loss=1.4176156520843506
I0215 07:43:33.075880 140532789532416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8115319609642029, loss=1.3718712329864502
I0215 07:44:58.258430 140532797925120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.6922122836112976, loss=1.3789029121398926
I0215 07:46:23.880096 140532789532416 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.2611424922943115, loss=1.3876169919967651
I0215 07:47:49.306758 140532797925120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.7449941635131836, loss=1.3859323263168335
I0215 07:49:16.101897 140532789532416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7939860224723816, loss=1.3512828350067139
I0215 07:50:42.389218 140532797925120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7691311836242676, loss=1.3746553659439087
I0215 07:52:04.442155 140532797925120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7277236580848694, loss=1.3539739847183228
I0215 07:53:19.797718 140532789532416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7416378259658813, loss=1.394822597503662
I0215 07:54:35.353057 140532797925120 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0994961261749268, loss=1.3528199195861816
I0215 07:55:50.628369 140532789532416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.8065952658653259, loss=1.400769591331482
I0215 07:57:12.626885 140532797925120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.6744721531867981, loss=1.3523839712142944
I0215 07:58:38.518198 140532789532416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.74355548620224, loss=1.3482266664505005
I0215 08:00:04.375873 140532797925120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.9079371690750122, loss=1.33039128780365
I0215 08:00:55.780274 140599226058560 spec.py:321] Evaluating on the training split.
I0215 08:01:50.026472 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 08:02:41.920744 140599226058560 spec.py:349] Evaluating on the test split.
I0215 08:03:08.517208 140599226058560 submission_runner.py:408] Time since start: 37829.61s, 	Step: 42961, 	{'train/ctc_loss': Array(0.2625836, dtype=float32), 'train/wer': 0.09887727364344041, 'validation/ctc_loss': Array(0.5573182, dtype=float32), 'validation/wer': 0.1747299110806453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3410652, dtype=float32), 'test/wer': 0.11941177665387037, 'test/num_examples': 2472, 'score': 34602.12437748909, 'total_duration': 37829.61024093628, 'accumulated_submission_time': 34602.12437748909, 'accumulated_eval_time': 3224.325934410095, 'accumulated_logging_time': 1.335150957107544}
I0215 08:03:08.556776 140532797925120 logging_writer.py:48] [42961] accumulated_eval_time=3224.325934, accumulated_logging_time=1.335151, accumulated_submission_time=34602.124377, global_step=42961, preemption_count=0, score=34602.124377, test/ctc_loss=0.3410651981830597, test/num_examples=2472, test/wer=0.119412, total_duration=37829.610241, train/ctc_loss=0.2625836133956909, train/wer=0.098877, validation/ctc_loss=0.5573182106018066, validation/num_examples=5348, validation/wer=0.174730
I0215 08:03:38.639670 140532789532416 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.3721235990524292, loss=1.426035761833191
I0215 08:04:53.969724 140532797925120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.9280807375907898, loss=1.4202255010604858
I0215 08:06:09.349126 140532789532416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.8041072487831116, loss=1.4102048873901367
I0215 08:07:28.293628 140532797925120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.689241886138916, loss=1.342022180557251
I0215 08:08:43.547712 140532789532416 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.5720746517181396, loss=1.3584851026535034
I0215 08:09:58.859515 140532797925120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7081876993179321, loss=1.3451319932937622
I0215 08:11:14.403199 140532789532416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.7571212649345398, loss=1.4036017656326294
I0215 08:12:32.733985 140532797925120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7270894646644592, loss=1.3517805337905884
I0215 08:13:56.950161 140532789532416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.7450125813484192, loss=1.358388066291809
I0215 08:15:22.579431 140532797925120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7225409746170044, loss=1.40915846824646
I0215 08:16:47.415711 140532789532416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.8509080410003662, loss=1.4488883018493652
I0215 08:18:12.813198 140532797925120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.756869375705719, loss=1.39029860496521
I0215 08:19:39.056746 140532789532416 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.078748345375061, loss=1.3358579874038696
I0215 08:21:06.624188 140532797925120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.757308840751648, loss=1.3435144424438477
I0215 08:22:21.931055 140532789532416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7544225454330444, loss=1.3483973741531372
I0215 08:23:37.247317 140532797925120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7399559020996094, loss=1.2779566049575806
I0215 08:24:52.535052 140532789532416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.9068672060966492, loss=1.3274192810058594
I0215 08:26:10.037026 140532797925120 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.320885419845581, loss=1.3344782590866089
I0215 08:27:08.512697 140599226058560 spec.py:321] Evaluating on the training split.
I0215 08:28:04.821348 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 08:28:57.204159 140599226058560 spec.py:349] Evaluating on the test split.
I0215 08:29:23.692898 140599226058560 submission_runner.py:408] Time since start: 39404.79s, 	Step: 44771, 	{'train/ctc_loss': Array(0.25002244, dtype=float32), 'train/wer': 0.09749153585718683, 'validation/ctc_loss': Array(0.55120695, dtype=float32), 'validation/wer': 0.17160180348919163, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33385333, dtype=float32), 'test/wer': 0.11618223549245425, 'test/num_examples': 2472, 'score': 36041.995379924774, 'total_duration': 39404.787296772, 'accumulated_submission_time': 36041.995379924774, 'accumulated_eval_time': 3359.500274658203, 'accumulated_logging_time': 1.389387607574463}
I0215 08:29:23.738403 140532797925120 logging_writer.py:48] [44771] accumulated_eval_time=3359.500275, accumulated_logging_time=1.389388, accumulated_submission_time=36041.995380, global_step=44771, preemption_count=0, score=36041.995380, test/ctc_loss=0.3338533341884613, test/num_examples=2472, test/wer=0.116182, total_duration=39404.787297, train/ctc_loss=0.25002244114875793, train/wer=0.097492, validation/ctc_loss=0.5512069463729858, validation/num_examples=5348, validation/wer=0.171602
I0215 08:29:46.373144 140532789532416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.8497704267501831, loss=1.35597562789917
I0215 08:31:01.772413 140532797925120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.9384944438934326, loss=1.3403578996658325
I0215 08:32:17.052303 140532789532416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.8391726016998291, loss=1.2947803735733032
I0215 08:33:34.098683 140532797925120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.8577536940574646, loss=1.3452149629592896
I0215 08:35:00.460987 140532789532416 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.0016124248504639, loss=1.4009641408920288
I0215 08:36:26.997687 140532797925120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.9087297320365906, loss=1.3622270822525024
I0215 08:37:47.614366 140532797925120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.767611026763916, loss=1.3067494630813599
I0215 08:39:02.740892 140532789532416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8034512400627136, loss=1.3602299690246582
I0215 08:40:17.936473 140532797925120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.7060657739639282, loss=1.322792649269104
I0215 08:41:34.461964 140532789532416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.8190683722496033, loss=1.300098180770874
I0215 08:42:58.542659 140532797925120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.7514862418174744, loss=1.3061479330062866
I0215 08:44:24.790497 140532789532416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.8399595022201538, loss=1.332216501235962
I0215 08:45:51.416748 140532797925120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.8686167001724243, loss=1.3745954036712646
I0215 08:47:17.654705 140532789532416 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7484480738639832, loss=1.3191853761672974
I0215 08:48:43.286261 140532797925120 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7366443276405334, loss=1.3640248775482178
I0215 08:50:09.532312 140532789532416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.8545638918876648, loss=1.299238920211792
I0215 08:51:33.015492 140532797925120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7781836986541748, loss=1.2738591432571411
I0215 08:52:48.231372 140532789532416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8500353693962097, loss=1.31320059299469
I0215 08:53:24.035917 140599226058560 spec.py:321] Evaluating on the training split.
I0215 08:54:19.993889 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 08:55:12.194948 140599226058560 spec.py:349] Evaluating on the test split.
I0215 08:55:38.953100 140599226058560 submission_runner.py:408] Time since start: 40980.05s, 	Step: 46549, 	{'train/ctc_loss': Array(0.27212, dtype=float32), 'train/wer': 0.09631768722677814, 'validation/ctc_loss': Array(0.52371204, dtype=float32), 'validation/wer': 0.16276779593925292, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32607883, dtype=float32), 'test/wer': 0.11234334694209169, 'test/num_examples': 2472, 'score': 37482.20679521561, 'total_duration': 40980.0468621254, 'accumulated_submission_time': 37482.20679521561, 'accumulated_eval_time': 3494.410943031311, 'accumulated_logging_time': 1.4525690078735352}
I0215 08:55:38.995415 140532797925120 logging_writer.py:48] [46549] accumulated_eval_time=3494.410943, accumulated_logging_time=1.452569, accumulated_submission_time=37482.206795, global_step=46549, preemption_count=0, score=37482.206795, test/ctc_loss=0.3260788321495056, test/num_examples=2472, test/wer=0.112343, total_duration=40980.046862, train/ctc_loss=0.27211999893188477, train/wer=0.096318, validation/ctc_loss=0.5237120389938354, validation/num_examples=5348, validation/wer=0.162768
I0215 08:56:18.083380 140532789532416 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.9755398631095886, loss=1.3337608575820923
I0215 08:57:33.381628 140532797925120 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7952338457107544, loss=1.3257842063903809
I0215 08:58:48.835082 140532789532416 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8232191801071167, loss=1.3597434759140015
I0215 09:00:04.131441 140532797925120 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7950896620750427, loss=1.2909753322601318
I0215 09:01:29.402828 140532789532416 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8459107279777527, loss=1.303326964378357
I0215 09:02:54.829862 140532797925120 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.3565165996551514, loss=1.303161859512329
I0215 09:04:20.642215 140532789532416 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.2543755769729614, loss=1.323555588722229
I0215 09:05:47.451616 140532797925120 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.8597055077552795, loss=1.3571557998657227
I0215 09:07:14.977823 140532797925120 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1894128322601318, loss=1.3042550086975098
I0215 09:08:30.201581 140532789532416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.848112940788269, loss=1.3218551874160767
I0215 09:09:45.566485 140532797925120 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7972807884216309, loss=1.3047778606414795
I0215 09:11:01.016758 140532789532416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.8329577445983887, loss=1.2170175313949585
I0215 09:12:19.758484 140532797925120 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.0277458429336548, loss=1.2844940423965454
I0215 09:13:44.746616 140532789532416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.8484998345375061, loss=1.3204898834228516
I0215 09:15:11.366991 140532797925120 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9081509113311768, loss=1.2933002710342407
I0215 09:16:38.419920 140532789532416 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.3116995096206665, loss=1.303566575050354
I0215 09:18:04.895942 140532797925120 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9480332136154175, loss=1.3117690086364746
I0215 09:19:30.727277 140532789532416 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.8769223093986511, loss=1.3453458547592163
I0215 09:19:39.225263 140599226058560 spec.py:321] Evaluating on the training split.
I0215 09:20:35.788577 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 09:21:28.103415 140599226058560 spec.py:349] Evaluating on the test split.
I0215 09:21:54.405608 140599226058560 submission_runner.py:408] Time since start: 42555.50s, 	Step: 48311, 	{'train/ctc_loss': Array(0.19800447, dtype=float32), 'train/wer': 0.07766064359989334, 'validation/ctc_loss': Array(0.51308125, dtype=float32), 'validation/wer': 0.16020931287834173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30938765, dtype=float32), 'test/wer': 0.10740763309162553, 'test/num_examples': 2472, 'score': 38922.34973311424, 'total_duration': 42555.49884772301, 'accumulated_submission_time': 38922.34973311424, 'accumulated_eval_time': 3629.584242582321, 'accumulated_logging_time': 1.512967586517334}
I0215 09:21:54.448723 140532797925120 logging_writer.py:48] [48311] accumulated_eval_time=3629.584243, accumulated_logging_time=1.512968, accumulated_submission_time=38922.349733, global_step=48311, preemption_count=0, score=38922.349733, test/ctc_loss=0.3093876540660858, test/num_examples=2472, test/wer=0.107408, total_duration=42555.498848, train/ctc_loss=0.19800446927547455, train/wer=0.077661, validation/ctc_loss=0.5130812525749207, validation/num_examples=5348, validation/wer=0.160209
I0215 09:23:02.115403 140532789532416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8301225900650024, loss=1.2915821075439453
I0215 09:24:21.183508 140532797925120 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0177342891693115, loss=1.2451225519180298
I0215 09:25:36.510751 140532789532416 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.2026060819625854, loss=1.2875220775604248
I0215 09:26:51.867206 140532797925120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.7080909013748169, loss=1.3523658514022827
I0215 09:28:11.971987 140532789532416 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.0266189575195312, loss=1.302735447883606
I0215 09:29:36.348722 140532797925120 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0519680976867676, loss=1.2850524187088013
I0215 09:31:01.516038 140532789532416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9333653450012207, loss=1.3228026628494263
I0215 09:32:28.617131 140532797925120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.9205074310302734, loss=1.2649646997451782
I0215 09:33:54.402646 140532789532416 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.4845589399337769, loss=1.332628607749939
I0215 09:35:20.096214 140532797925120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.8306814432144165, loss=1.281350016593933
I0215 09:36:45.879219 140532789532416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.7140302062034607, loss=1.3247984647750854
I0215 09:38:08.385594 140532797925120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.9228072762489319, loss=1.3067830801010132
I0215 09:39:23.625701 140532789532416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.915641725063324, loss=1.3008147478103638
I0215 09:40:38.938597 140532797925120 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0260539054870605, loss=1.2829771041870117
I0215 09:41:54.305401 140532789532416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.8590714931488037, loss=1.3259421586990356
I0215 09:43:16.361716 140532797925120 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.0996052026748657, loss=1.2788180112838745
I0215 09:44:42.128355 140532789532416 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.0330215692520142, loss=1.3027706146240234
I0215 09:45:54.538303 140599226058560 spec.py:321] Evaluating on the training split.
I0215 09:46:50.717191 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 09:47:43.001770 140599226058560 spec.py:349] Evaluating on the test split.
I0215 09:48:09.553298 140599226058560 submission_runner.py:408] Time since start: 44130.65s, 	Step: 50086, 	{'train/ctc_loss': Array(0.21282442, dtype=float32), 'train/wer': 0.0822324569887133, 'validation/ctc_loss': Array(0.50448716, dtype=float32), 'validation/wer': 0.157303262307269, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30191362, dtype=float32), 'test/wer': 0.10570146040257551, 'test/num_examples': 2472, 'score': 40362.35197472572, 'total_duration': 44130.646955013275, 'accumulated_submission_time': 40362.35197472572, 'accumulated_eval_time': 3764.5926241874695, 'accumulated_logging_time': 1.574493646621704}
I0215 09:48:09.594968 140532797925120 logging_writer.py:48] [50086] accumulated_eval_time=3764.592624, accumulated_logging_time=1.574494, accumulated_submission_time=40362.351975, global_step=50086, preemption_count=0, score=40362.351975, test/ctc_loss=0.30191361904144287, test/num_examples=2472, test/wer=0.105701, total_duration=44130.646955, train/ctc_loss=0.21282441914081573, train/wer=0.082232, validation/ctc_loss=0.504487156867981, validation/num_examples=5348, validation/wer=0.157303
I0215 09:48:20.970548 140532789532416 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9428963661193848, loss=1.2710951566696167
I0215 09:49:36.034861 140532797925120 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.8413155674934387, loss=1.3037617206573486
I0215 09:50:51.483813 140532789532416 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.8025202751159668, loss=1.3163633346557617
I0215 09:52:06.695323 140532797925120 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2044498920440674, loss=1.3253010511398315
I0215 09:53:33.068608 140532797925120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7839073538780212, loss=1.2264655828475952
I0215 09:54:48.324135 140532789532416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.8285094499588013, loss=1.2883652448654175
I0215 09:56:03.569234 140532797925120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8837489485740662, loss=1.2546859979629517
I0215 09:57:18.925697 140532789532416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.8029794692993164, loss=1.3011895418167114
I0215 09:58:40.974008 140532797925120 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.130533218383789, loss=1.285228967666626
I0215 10:00:06.444573 140532789532416 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7164273858070374, loss=1.2283825874328613
I0215 10:01:32.253073 140532797925120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.861448347568512, loss=1.2776398658752441
I0215 10:02:58.480301 140532789532416 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8670879006385803, loss=1.3181660175323486
I0215 10:04:24.815789 140532797925120 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.011211633682251, loss=1.296762228012085
I0215 10:05:50.447726 140532789532416 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.2233917713165283, loss=1.3323462009429932
I0215 10:07:19.612398 140532797925120 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.167551040649414, loss=1.25955331325531
I0215 10:08:34.647391 140532789532416 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.1354478597640991, loss=1.2520520687103271
I0215 10:09:49.785247 140532797925120 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.0170772075653076, loss=1.286556601524353
I0215 10:11:04.877487 140532789532416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.8743835091590881, loss=1.2147653102874756
I0215 10:12:09.694786 140599226058560 spec.py:321] Evaluating on the training split.
I0215 10:13:04.179789 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 10:13:56.594111 140599226058560 spec.py:349] Evaluating on the test split.
I0215 10:14:23.202200 140599226058560 submission_runner.py:408] Time since start: 45704.30s, 	Step: 51885, 	{'train/ctc_loss': Array(0.27286735, dtype=float32), 'train/wer': 0.1037494229120408, 'validation/ctc_loss': Array(0.5019356, dtype=float32), 'validation/wer': 0.1571680971644284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29826474, dtype=float32), 'test/wer': 0.10381248349684155, 'test/num_examples': 2472, 'score': 41802.366864681244, 'total_duration': 45704.29691815376, 'accumulated_submission_time': 41802.366864681244, 'accumulated_eval_time': 3898.0944876670837, 'accumulated_logging_time': 1.630784034729004}
I0215 10:14:23.243444 140532797925120 logging_writer.py:48] [51885] accumulated_eval_time=3898.094488, accumulated_logging_time=1.630784, accumulated_submission_time=41802.366865, global_step=51885, preemption_count=0, score=41802.366865, test/ctc_loss=0.298264741897583, test/num_examples=2472, test/wer=0.103812, total_duration=45704.296918, train/ctc_loss=0.272867351770401, train/wer=0.103749, validation/ctc_loss=0.501935601234436, validation/num_examples=5348, validation/wer=0.157168
I0215 10:14:35.360557 140532789532416 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.5047224760055542, loss=1.2339686155319214
I0215 10:15:50.453290 140532797925120 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8957534432411194, loss=1.296947717666626
I0215 10:17:05.718486 140532789532416 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.9569562673568726, loss=1.276292324066162
I0215 10:18:22.655510 140532797925120 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.421035051345825, loss=1.279591679573059
I0215 10:19:47.710137 140532789532416 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.854063868522644, loss=1.3085566759109497
I0215 10:21:14.046480 140532797925120 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8537201285362244, loss=1.2562450170516968
I0215 10:22:39.369462 140532789532416 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.8340550661087036, loss=1.3069041967391968
I0215 10:24:00.999153 140532797925120 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.0485793352127075, loss=1.2724599838256836
I0215 10:25:16.182363 140532789532416 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.1799184083938599, loss=1.2973343133926392
I0215 10:26:31.467380 140532797925120 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.2550159692764282, loss=1.276309847831726
I0215 10:27:46.800411 140532789532416 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.6659303903579712, loss=1.2772350311279297
I0215 10:29:08.011931 140532797925120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.7999327778816223, loss=1.29245126247406
I0215 10:30:33.534819 140532789532416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9074487686157227, loss=1.284803032875061
I0215 10:31:58.310547 140532797925120 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.5299829244613647, loss=1.2511595487594604
I0215 10:33:24.099131 140532789532416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8297080993652344, loss=1.2608239650726318
I0215 10:34:50.088279 140532797925120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.7810176014900208, loss=1.2989685535430908
I0215 10:36:16.342078 140532789532416 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0687185525894165, loss=1.283767580986023
I0215 10:37:40.914298 140532797925120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.9226443767547607, loss=1.2445807456970215
I0215 10:38:23.746527 140599226058560 spec.py:321] Evaluating on the training split.
I0215 10:39:17.530067 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 10:40:09.477908 140599226058560 spec.py:349] Evaluating on the test split.
I0215 10:40:35.851766 140599226058560 submission_runner.py:408] Time since start: 47276.95s, 	Step: 53658, 	{'train/ctc_loss': Array(0.28197432, dtype=float32), 'train/wer': 0.1062609862536667, 'validation/ctc_loss': Array(0.4842883, dtype=float32), 'validation/wer': 0.1505739691244195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28853884, dtype=float32), 'test/wer': 0.10021733390205756, 'test/num_examples': 2472, 'score': 43242.77938055992, 'total_duration': 47276.94588851929, 'accumulated_submission_time': 43242.77938055992, 'accumulated_eval_time': 4030.1935641765594, 'accumulated_logging_time': 1.6926136016845703}
I0215 10:40:35.892122 140532797925120 logging_writer.py:48] [53658] accumulated_eval_time=4030.193564, accumulated_logging_time=1.692614, accumulated_submission_time=43242.779381, global_step=53658, preemption_count=0, score=43242.779381, test/ctc_loss=0.2885388433933258, test/num_examples=2472, test/wer=0.100217, total_duration=47276.945889, train/ctc_loss=0.28197431564331055, train/wer=0.106261, validation/ctc_loss=0.4842883050441742, validation/num_examples=5348, validation/wer=0.150574
I0215 10:41:08.130926 140532789532416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.7705038189888, loss=1.2325445413589478
I0215 10:42:23.194912 140532797925120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.8555026650428772, loss=1.2529261112213135
I0215 10:43:38.478770 140532789532416 logging_writer.py:48] [53900] global_step=53900, grad_norm=2.0082101821899414, loss=1.2189593315124512
I0215 10:44:53.801597 140532797925120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9020887613296509, loss=1.2770705223083496
I0215 10:46:10.845315 140532789532416 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.0696983337402344, loss=1.2872081995010376
I0215 10:47:36.756220 140532797925120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.8816107511520386, loss=1.2351179122924805
I0215 10:49:02.981649 140532789532416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.8958950042724609, loss=1.210898995399475
I0215 10:50:29.180233 140532797925120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.8001454472541809, loss=1.2202918529510498
I0215 10:51:54.691329 140532789532416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.82203209400177, loss=1.2254910469055176
I0215 10:53:22.637305 140532797925120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.7842956185340881, loss=1.2368850708007812
I0215 10:54:37.847866 140532789532416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8571841716766357, loss=1.1977837085723877
I0215 10:55:53.481169 140532797925120 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.1841261386871338, loss=1.2208133935928345
I0215 10:57:08.903869 140532789532416 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.089460015296936, loss=1.2424798011779785
I0215 10:58:24.944610 140532797925120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.8505579233169556, loss=1.214072823524475
I0215 10:59:49.853471 140532789532416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.973692774772644, loss=1.2500743865966797
I0215 11:01:15.949723 140532797925120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.9205712080001831, loss=1.2444733381271362
I0215 11:02:42.120575 140532789532416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.839806079864502, loss=1.2500948905944824
I0215 11:04:07.865321 140532797925120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8635072112083435, loss=1.2548792362213135
I0215 11:04:36.142146 140599226058560 spec.py:321] Evaluating on the training split.
I0215 11:05:29.789263 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 11:06:21.830337 140599226058560 spec.py:349] Evaluating on the test split.
I0215 11:06:48.372469 140599226058560 submission_runner.py:408] Time since start: 48849.47s, 	Step: 55435, 	{'train/ctc_loss': Array(0.3120024, dtype=float32), 'train/wer': 0.11836945424169297, 'validation/ctc_loss': Array(0.47713417, dtype=float32), 'validation/wer': 0.1493864467980343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28182715, dtype=float32), 'test/wer': 0.0968456116832206, 'test/num_examples': 2472, 'score': 44682.94412112236, 'total_duration': 48849.46684360504, 'accumulated_submission_time': 44682.94412112236, 'accumulated_eval_time': 4162.418050527573, 'accumulated_logging_time': 1.748811960220337}
I0215 11:06:48.417059 140532797925120 logging_writer.py:48] [55435] accumulated_eval_time=4162.418051, accumulated_logging_time=1.748812, accumulated_submission_time=44682.944121, global_step=55435, preemption_count=0, score=44682.944121, test/ctc_loss=0.2818271517753601, test/num_examples=2472, test/wer=0.096846, total_duration=48849.466844, train/ctc_loss=0.31200239062309265, train/wer=0.118369, validation/ctc_loss=0.47713416814804077, validation/num_examples=5348, validation/wer=0.149386
I0215 11:07:38.428400 140532789532416 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.001408338546753, loss=1.2419370412826538
I0215 11:08:53.786713 140532797925120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.8606536984443665, loss=1.2302441596984863
I0215 11:10:12.800005 140532797925120 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.503245234489441, loss=1.213315725326538
I0215 11:11:28.075397 140532789532416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.9176413416862488, loss=1.2087048292160034
I0215 11:12:43.539978 140532797925120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.8091775178909302, loss=1.2316771745681763
I0215 11:14:00.649792 140532789532416 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0606311559677124, loss=1.2474867105484009
I0215 11:15:24.306799 140532797925120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.7679771184921265, loss=1.2554742097854614
I0215 11:16:49.039959 140532789532416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.8466702699661255, loss=1.2149444818496704
I0215 11:18:15.395282 140532797925120 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.4689162969589233, loss=1.2686500549316406
I0215 11:19:41.276548 140532789532416 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.257933259010315, loss=1.242512583732605
I0215 11:21:07.931839 140532797925120 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1578971147537231, loss=1.2372263669967651
I0215 11:22:33.901299 140532789532416 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.0138168334960938, loss=1.1904851198196411
I0215 11:23:58.113129 140532797925120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.9554792642593384, loss=1.3029519319534302
I0215 11:25:13.239356 140532789532416 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.1298003196716309, loss=1.1855601072311401
I0215 11:26:28.563244 140532797925120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7595230937004089, loss=1.2113041877746582
I0215 11:27:44.078666 140532789532416 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.2987918853759766, loss=1.2109777927398682
I0215 11:29:03.733491 140532797925120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.918428361415863, loss=1.2019401788711548
I0215 11:30:29.515639 140532789532416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9919424653053284, loss=1.1693942546844482
I0215 11:30:48.847094 140599226058560 spec.py:321] Evaluating on the training split.
I0215 11:31:41.342194 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 11:32:34.045073 140599226058560 spec.py:349] Evaluating on the test split.
I0215 11:33:00.373973 140599226058560 submission_runner.py:408] Time since start: 50421.47s, 	Step: 57224, 	{'train/ctc_loss': Array(0.27040535, dtype=float32), 'train/wer': 0.10101537739171264, 'validation/ctc_loss': Array(0.46657196, dtype=float32), 'validation/wer': 0.1453218378597565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2736266, dtype=float32), 'test/wer': 0.09241768732354315, 'test/num_examples': 2472, 'score': 46123.28657770157, 'total_duration': 50421.46864414215, 'accumulated_submission_time': 46123.28657770157, 'accumulated_eval_time': 4293.939308643341, 'accumulated_logging_time': 1.8109087944030762}
I0215 11:33:00.414373 140532797925120 logging_writer.py:48] [57224] accumulated_eval_time=4293.939309, accumulated_logging_time=1.810909, accumulated_submission_time=46123.286578, global_step=57224, preemption_count=0, score=46123.286578, test/ctc_loss=0.2736265957355499, test/num_examples=2472, test/wer=0.092418, total_duration=50421.468644, train/ctc_loss=0.2704053521156311, train/wer=0.101015, validation/ctc_loss=0.46657195687294006, validation/num_examples=5348, validation/wer=0.145322
I0215 11:33:58.123275 140532789532416 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.6872453689575195, loss=1.2022573947906494
I0215 11:35:13.245911 140532797925120 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.846865177154541, loss=1.2526588439941406
I0215 11:36:28.449294 140532789532416 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.4435172080993652, loss=1.2102457284927368
I0215 11:37:51.566151 140532797925120 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.6325914859771729, loss=1.206757664680481
I0215 11:39:18.696177 140532797925120 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.8891199827194214, loss=1.2399801015853882
I0215 11:40:33.930006 140532789532416 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.028031826019287, loss=1.211334228515625
I0215 11:41:49.258841 140532797925120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.9352497458457947, loss=1.2293944358825684
I0215 11:43:04.683444 140532789532416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9965137243270874, loss=1.1704031229019165
I0215 11:44:25.460107 140532797925120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.8132985234260559, loss=1.23270845413208
I0215 11:45:49.869203 140532789532416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.8287211656570435, loss=1.1877368688583374
I0215 11:47:16.386604 140532797925120 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.1485658884048462, loss=1.2170343399047852
I0215 11:48:42.654333 140532789532416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.8001561760902405, loss=1.2173371315002441
I0215 11:50:09.145618 140532797925120 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.3074417114257812, loss=1.2609282732009888
I0215 11:51:36.280435 140532789532416 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.536170244216919, loss=1.1632901430130005
I0215 11:53:02.589722 140532797925120 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8310810327529907, loss=1.2120167016983032
I0215 11:54:22.184363 140532797925120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.944207489490509, loss=1.179416537284851
I0215 11:55:37.365183 140532789532416 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.229003667831421, loss=1.1688148975372314
I0215 11:56:52.593031 140532797925120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9688811898231506, loss=1.1901092529296875
I0215 11:57:00.603569 140599226058560 spec.py:321] Evaluating on the training split.
I0215 11:57:54.332098 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 11:58:46.579267 140599226058560 spec.py:349] Evaluating on the test split.
I0215 11:59:13.226081 140599226058560 submission_runner.py:408] Time since start: 51994.32s, 	Step: 59012, 	{'train/ctc_loss': Array(0.24490872, dtype=float32), 'train/wer': 0.09386993633364989, 'validation/ctc_loss': Array(0.45609862, dtype=float32), 'validation/wer': 0.14097724398273748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26979008, dtype=float32), 'test/wer': 0.09203176731054374, 'test/num_examples': 2472, 'score': 47563.390914440155, 'total_duration': 51994.320743083954, 'accumulated_submission_time': 47563.390914440155, 'accumulated_eval_time': 4426.556207895279, 'accumulated_logging_time': 1.867297887802124}
I0215 11:59:13.267106 140532797925120 logging_writer.py:48] [59012] accumulated_eval_time=4426.556208, accumulated_logging_time=1.867298, accumulated_submission_time=47563.390914, global_step=59012, preemption_count=0, score=47563.390914, test/ctc_loss=0.26979008316993713, test/num_examples=2472, test/wer=0.092032, total_duration=51994.320743, train/ctc_loss=0.24490872025489807, train/wer=0.093870, validation/ctc_loss=0.45609861612319946, validation/num_examples=5348, validation/wer=0.140977
I0215 12:00:20.125625 140532789532416 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.8849199414253235, loss=1.1667487621307373
I0215 12:01:35.685260 140532797925120 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.7112500667572021, loss=1.199305534362793
I0215 12:02:51.039462 140532789532416 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.476372241973877, loss=1.182030439376831
I0215 12:04:15.893075 140532797925120 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.2346603870391846, loss=1.212225079536438
I0215 12:05:42.632732 140532789532416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9797050952911377, loss=1.1707977056503296
I0215 12:07:09.610823 140532797925120 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.4421520233154297, loss=1.2301028966903687
I0215 12:08:36.330653 140532789532416 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.3742775917053223, loss=1.1690800189971924
I0215 12:09:59.520696 140532797925120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9134832620620728, loss=1.1683956384658813
I0215 12:11:14.767387 140532789532416 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.0278995037078857, loss=1.1637144088745117
I0215 12:12:30.037828 140532797925120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.7784200310707092, loss=1.148486852645874
I0215 12:13:45.315387 140532789532416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.8200381994247437, loss=1.1711395978927612
I0215 12:15:07.683186 140532797925120 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.005108118057251, loss=1.1675083637237549
I0215 12:16:35.144216 140532789532416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.8653439879417419, loss=1.1868436336517334
I0215 12:18:02.357984 140532797925120 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.1689441204071045, loss=1.1471912860870361
I0215 12:19:29.515208 140532789532416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9755079746246338, loss=1.2390244007110596
I0215 12:20:55.595798 140532797925120 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.0178799629211426, loss=1.238881230354309
I0215 12:22:21.743740 140532789532416 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.8855350017547607, loss=1.1614421606063843
I0215 12:23:13.648965 140599226058560 spec.py:321] Evaluating on the training split.
I0215 12:24:07.994977 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 12:24:59.870443 140599226058560 spec.py:349] Evaluating on the test split.
I0215 12:25:26.262695 140599226058560 submission_runner.py:408] Time since start: 53567.36s, 	Step: 60762, 	{'train/ctc_loss': Array(0.20578378, dtype=float32), 'train/wer': 0.07972726732020136, 'validation/ctc_loss': Array(0.45228767, dtype=float32), 'validation/wer': 0.13957731928903136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26356447, dtype=float32), 'test/wer': 0.09038652936038835, 'test/num_examples': 2472, 'score': 49003.68826889992, 'total_duration': 53567.35671496391, 'accumulated_submission_time': 49003.68826889992, 'accumulated_eval_time': 4559.163670539856, 'accumulated_logging_time': 1.9240319728851318}
I0215 12:25:26.311067 140532797925120 logging_writer.py:48] [60762] accumulated_eval_time=4559.163671, accumulated_logging_time=1.924032, accumulated_submission_time=49003.688269, global_step=60762, preemption_count=0, score=49003.688269, test/ctc_loss=0.26356446743011475, test/num_examples=2472, test/wer=0.090387, total_duration=53567.356715, train/ctc_loss=0.20578378438949585, train/wer=0.079727, validation/ctc_loss=0.4522876739501953, validation/num_examples=5348, validation/wer=0.139577
I0215 12:25:59.521948 140532797925120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.8631258010864258, loss=1.1120388507843018
I0215 12:27:14.755372 140532789532416 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0905500650405884, loss=1.1395891904830933
I0215 12:28:29.944676 140532797925120 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.265770435333252, loss=1.2088181972503662
I0215 12:29:45.298555 140532789532416 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.3566226959228516, loss=1.1320124864578247
I0215 12:31:06.847358 140532797925120 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.1167736053466797, loss=1.187200903892517
I0215 12:32:32.797455 140532789532416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.8687683939933777, loss=1.1763312816619873
I0215 12:33:59.784111 140532797925120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.846424400806427, loss=1.1592968702316284
I0215 12:35:25.080096 140532789532416 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.1134403944015503, loss=1.1899971961975098
I0215 12:36:51.032040 140532797925120 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1717568635940552, loss=1.1559443473815918
I0215 12:38:16.978626 140532789532416 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.2491755485534668, loss=1.202549695968628
I0215 12:39:46.101403 140532797925120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.9953973889350891, loss=1.167521595954895
I0215 12:41:01.273846 140532789532416 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.2912890911102295, loss=1.1432982683181763
I0215 12:42:16.444877 140532797925120 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.8811149001121521, loss=1.18867027759552
I0215 12:43:31.619222 140532789532416 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.5258750915527344, loss=1.166103482246399
I0215 12:44:49.616277 140532797925120 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.1038298606872559, loss=1.1655603647232056
I0215 12:46:14.247798 140532789532416 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.2389891147613525, loss=1.1649792194366455
I0215 12:47:39.833407 140532797925120 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.9641042947769165, loss=1.195453405380249
I0215 12:49:06.388418 140532789532416 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.8563623428344727, loss=1.1793893575668335
I0215 12:49:26.659978 140599226058560 spec.py:321] Evaluating on the training split.
I0215 12:50:21.184796 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 12:51:13.557865 140599226058560 spec.py:349] Evaluating on the test split.
I0215 12:51:40.529840 140599226058560 submission_runner.py:408] Time since start: 55141.62s, 	Step: 62525, 	{'train/ctc_loss': Array(0.23597239, dtype=float32), 'train/wer': 0.09127745904321459, 'validation/ctc_loss': Array(0.44328588, dtype=float32), 'validation/wer': 0.13635266516697722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2582945, dtype=float32), 'test/wer': 0.08983811671033656, 'test/num_examples': 2472, 'score': 50443.94960784912, 'total_duration': 55141.62321925163, 'accumulated_submission_time': 50443.94960784912, 'accumulated_eval_time': 4693.026634216309, 'accumulated_logging_time': 1.9902966022491455}
I0215 12:51:40.576202 140532797925120 logging_writer.py:48] [62525] accumulated_eval_time=4693.026634, accumulated_logging_time=1.990297, accumulated_submission_time=50443.949608, global_step=62525, preemption_count=0, score=50443.949608, test/ctc_loss=0.2582944929599762, test/num_examples=2472, test/wer=0.089838, total_duration=55141.623219, train/ctc_loss=0.23597238957881927, train/wer=0.091277, validation/ctc_loss=0.44328588247299194, validation/num_examples=5348, validation/wer=0.136353
I0215 12:52:37.621170 140532789532416 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.5768500566482544, loss=1.1399431228637695
I0215 12:53:52.820964 140532797925120 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.9896386861801147, loss=1.2007887363433838
I0215 12:55:08.166287 140532789532416 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.9154870510101318, loss=1.2188057899475098
I0215 12:56:27.190665 140532797925120 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.068153738975525, loss=1.1824891567230225
I0215 12:57:42.454825 140532789532416 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.3723909854888916, loss=1.1173540353775024
I0215 12:58:57.800120 140532797925120 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.2842309474945068, loss=1.1887354850769043
I0215 13:00:13.096140 140532789532416 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.4500168561935425, loss=1.178492784500122
I0215 13:01:35.790580 140532797925120 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.1657962799072266, loss=1.1565091609954834
I0215 13:03:00.741032 140532789532416 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0020676851272583, loss=1.2054533958435059
I0215 13:04:27.047383 140532797925120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.9842706322669983, loss=1.1805775165557861
I0215 13:05:52.666070 140532789532416 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.829178810119629, loss=1.1430046558380127
I0215 13:07:18.676456 140532797925120 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.2234842777252197, loss=1.1333922147750854
I0215 13:08:44.224233 140532789532416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.9780483245849609, loss=1.125160813331604
I0215 13:10:10.183015 140532797925120 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.1809495687484741, loss=1.1459057331085205
I0215 13:11:25.419474 140532789532416 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.0366414785385132, loss=1.093484878540039
I0215 13:12:40.770097 140532797925120 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.9674357771873474, loss=1.1585344076156616
I0215 13:13:56.218598 140532789532416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.9090862274169922, loss=1.1471236944198608
I0215 13:15:14.450732 140532797925120 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.4006034135818481, loss=1.131519079208374
I0215 13:15:40.976667 140599226058560 spec.py:321] Evaluating on the training split.
I0215 13:16:36.536302 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 13:17:28.919299 140599226058560 spec.py:349] Evaluating on the test split.
I0215 13:17:54.994190 140599226058560 submission_runner.py:408] Time since start: 56716.09s, 	Step: 64333, 	{'train/ctc_loss': Array(0.20355521, dtype=float32), 'train/wer': 0.07782055395144905, 'validation/ctc_loss': Array(0.44492757, dtype=float32), 'validation/wer': 0.13546443708545333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25623506, dtype=float32), 'test/wer': 0.0880710092823919, 'test/num_examples': 2472, 'score': 51884.26449036598, 'total_duration': 56716.089381694794, 'accumulated_submission_time': 51884.26449036598, 'accumulated_eval_time': 4827.03906917572, 'accumulated_logging_time': 2.0525200366973877}
I0215 13:17:55.035809 140532797925120 logging_writer.py:48] [64333] accumulated_eval_time=4827.039069, accumulated_logging_time=2.052520, accumulated_submission_time=51884.264490, global_step=64333, preemption_count=0, score=51884.264490, test/ctc_loss=0.2562350630760193, test/num_examples=2472, test/wer=0.088071, total_duration=56716.089382, train/ctc_loss=0.20355521142482758, train/wer=0.077821, validation/ctc_loss=0.4449275732040405, validation/num_examples=5348, validation/wer=0.135464
I0215 13:18:46.164427 140532789532416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9894663691520691, loss=1.130262017250061
I0215 13:20:01.433898 140532797925120 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.7205497026443481, loss=1.1608247756958008
I0215 13:21:16.696368 140532789532416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.9701131582260132, loss=1.2045260667800903
I0215 13:22:38.683521 140532797925120 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.3043971061706543, loss=1.1477185487747192
I0215 13:24:05.159837 140532789532416 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.3620033264160156, loss=1.1673851013183594
I0215 13:25:33.682456 140532797925120 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.913381040096283, loss=1.1740564107894897
I0215 13:26:48.908529 140532789532416 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.0056440830230713, loss=1.1056190729141235
I0215 13:28:04.153871 140532797925120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8639127612113953, loss=1.1328153610229492
I0215 13:29:19.422917 140532789532416 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1501312255859375, loss=1.173232078552246
I0215 13:30:36.147694 140532797925120 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1596457958221436, loss=1.1614567041397095
I0215 13:32:00.646575 140532789532416 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.0236680507659912, loss=1.1057900190353394
I0215 13:33:26.015027 140532797925120 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.9962020516395569, loss=1.1377383470535278
I0215 13:34:51.777074 140532789532416 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.3003028631210327, loss=1.169386863708496
I0215 13:36:17.346278 140532797925120 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.9583986401557922, loss=1.1718111038208008
I0215 13:37:42.872075 140532789532416 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.298972249031067, loss=1.1738942861557007
I0215 13:39:08.900710 140532797925120 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.5175021886825562, loss=1.151409387588501
I0215 13:40:29.493056 140532797925120 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0598710775375366, loss=1.1110950708389282
I0215 13:41:44.787908 140532789532416 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.9464571475982666, loss=1.1581453084945679
I0215 13:41:55.052024 140599226058560 spec.py:321] Evaluating on the training split.
I0215 13:42:49.641450 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 13:43:41.735161 140599226058560 spec.py:349] Evaluating on the test split.
I0215 13:44:08.291403 140599226058560 submission_runner.py:408] Time since start: 58289.38s, 	Step: 66115, 	{'train/ctc_loss': Array(0.20436361, dtype=float32), 'train/wer': 0.08001046030803428, 'validation/ctc_loss': Array(0.43434986, dtype=float32), 'validation/wer': 0.13363970765710534, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2523467, dtype=float32), 'test/wer': 0.08577580078402697, 'test/num_examples': 2472, 'score': 53324.194618463516, 'total_duration': 58289.38478684425, 'accumulated_submission_time': 53324.194618463516, 'accumulated_eval_time': 4960.271555185318, 'accumulated_logging_time': 2.1096343994140625}
I0215 13:44:08.339390 140532797925120 logging_writer.py:48] [66115] accumulated_eval_time=4960.271555, accumulated_logging_time=2.109634, accumulated_submission_time=53324.194618, global_step=66115, preemption_count=0, score=53324.194618, test/ctc_loss=0.2523466944694519, test/num_examples=2472, test/wer=0.085776, total_duration=58289.384787, train/ctc_loss=0.204363614320755, train/wer=0.080010, validation/ctc_loss=0.4343498647212982, validation/num_examples=5348, validation/wer=0.133640
I0215 13:45:12.925308 140532789532416 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.987384021282196, loss=1.190172791481018
I0215 13:46:28.236838 140532797925120 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.9511629939079285, loss=1.1372653245925903
I0215 13:47:43.602213 140532789532416 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.979912519454956, loss=1.1335997581481934
I0215 13:48:58.978965 140532797925120 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1536444425582886, loss=1.148669719696045
I0215 13:50:24.109434 140532789532416 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.172272801399231, loss=1.165162205696106
I0215 13:51:48.465191 140532797925120 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.9105350971221924, loss=1.1596243381500244
I0215 13:53:13.569081 140532789532416 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.2456717491149902, loss=1.152219295501709
I0215 13:54:38.514675 140532797925120 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.9347246885299683, loss=1.0984549522399902
I0215 13:56:03.028098 140532797925120 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.8162840604782104, loss=1.1131041049957275
I0215 13:57:18.333891 140532789532416 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.132503867149353, loss=1.1558332443237305
I0215 13:58:33.624585 140532797925120 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.3483790159225464, loss=1.1881650686264038
I0215 13:59:48.988367 140532789532416 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.4729366302490234, loss=1.1428170204162598
I0215 14:01:08.633561 140532797925120 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3502545356750488, loss=1.0725457668304443
I0215 14:02:34.971083 140532789532416 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.530246615409851, loss=1.1658117771148682
I0215 14:04:00.795716 140532797925120 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.405122995376587, loss=1.1282709836959839
I0215 14:05:26.886727 140532789532416 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.354520559310913, loss=1.138892650604248
I0215 14:06:53.269974 140532797925120 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.8756317496299744, loss=1.1043025255203247
I0215 14:08:08.766854 140599226058560 spec.py:321] Evaluating on the training split.
I0215 14:09:03.537671 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 14:09:55.837415 140599226058560 spec.py:349] Evaluating on the test split.
I0215 14:10:22.384912 140599226058560 submission_runner.py:408] Time since start: 59863.48s, 	Step: 67890, 	{'train/ctc_loss': Array(0.19853008, dtype=float32), 'train/wer': 0.0757643860245349, 'validation/ctc_loss': Array(0.426306, dtype=float32), 'validation/wer': 0.1307722756982728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24856624, dtype=float32), 'test/wer': 0.08457741758576565, 'test/num_examples': 2472, 'score': 54764.53757071495, 'total_duration': 59863.47870230675, 'accumulated_submission_time': 54764.53757071495, 'accumulated_eval_time': 5093.883127212524, 'accumulated_logging_time': 2.1731672286987305}
I0215 14:10:22.431963 140532797925120 logging_writer.py:48] [67890] accumulated_eval_time=5093.883127, accumulated_logging_time=2.173167, accumulated_submission_time=54764.537571, global_step=67890, preemption_count=0, score=54764.537571, test/ctc_loss=0.24856624007225037, test/num_examples=2472, test/wer=0.084577, total_duration=59863.478702, train/ctc_loss=0.19853007793426514, train/wer=0.075764, validation/ctc_loss=0.42630600929260254, validation/num_examples=5348, validation/wer=0.130772
I0215 14:10:30.769087 140532789532416 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.403048038482666, loss=1.1644363403320312
I0215 14:11:49.705377 140532797925120 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.8489723801612854, loss=1.1222968101501465
I0215 14:13:04.894033 140532789532416 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.3050472736358643, loss=1.0979079008102417
I0215 14:14:20.435775 140532797925120 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.496390461921692, loss=1.1364742517471313
I0215 14:15:35.838797 140532789532416 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.010527491569519, loss=1.0982046127319336
I0215 14:16:54.729858 140532797925120 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.9317613244056702, loss=1.095550775527954
I0215 14:18:18.616777 140532789532416 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.8950526714324951, loss=1.1145628690719604
I0215 14:19:43.310726 140532797925120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.9753050208091736, loss=1.1144973039627075
I0215 14:21:09.062451 140532789532416 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1324104070663452, loss=1.1276410818099976
I0215 14:22:35.149704 140532797925120 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.3736873865127563, loss=1.143683910369873
I0215 14:24:01.206721 140532789532416 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.0875144004821777, loss=1.131706953048706
I0215 14:25:26.641799 140532797925120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.9050478935241699, loss=1.1676894426345825
I0215 14:26:46.822211 140532797925120 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.8574377298355103, loss=1.0836741924285889
I0215 14:28:02.104568 140532789532416 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3694710731506348, loss=1.0390452146530151
I0215 14:29:17.628683 140532797925120 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.124672532081604, loss=1.0942506790161133
I0215 14:30:32.840461 140532789532416 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.751006841659546, loss=1.0998948812484741
I0215 14:31:56.411659 140532797925120 logging_writer.py:48] [69500] global_step=69500, grad_norm=4.556885242462158, loss=1.0985506772994995
I0215 14:33:21.456388 140532789532416 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.3284350633621216, loss=1.1342922449111938
I0215 14:34:22.917190 140599226058560 spec.py:321] Evaluating on the training split.
I0215 14:35:16.333580 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 14:36:08.004748 140599226058560 spec.py:349] Evaluating on the test split.
I0215 14:36:33.915530 140599226058560 submission_runner.py:408] Time since start: 61435.01s, 	Step: 69674, 	{'train/ctc_loss': Array(0.20562021, dtype=float32), 'train/wer': 0.07878492061213356, 'validation/ctc_loss': Array(0.42373237, dtype=float32), 'validation/wer': 0.12933373239232648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24726346, dtype=float32), 'test/wer': 0.08378526598013528, 'test/num_examples': 2472, 'score': 56204.93294286728, 'total_duration': 61435.010118722916, 'accumulated_submission_time': 56204.93294286728, 'accumulated_eval_time': 5224.875775098801, 'accumulated_logging_time': 2.240447998046875}
I0215 14:36:33.959450 140532797925120 logging_writer.py:48] [69674] accumulated_eval_time=5224.875775, accumulated_logging_time=2.240448, accumulated_submission_time=56204.932943, global_step=69674, preemption_count=0, score=56204.932943, test/ctc_loss=0.24726346135139465, test/num_examples=2472, test/wer=0.083785, total_duration=61435.010119, train/ctc_loss=0.20562021434307098, train/wer=0.078785, validation/ctc_loss=0.42373237013816833, validation/num_examples=5348, validation/wer=0.129334
I0215 14:36:54.295467 140532789532416 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.9098941683769226, loss=1.1152945756912231
I0215 14:38:09.435806 140532797925120 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.817127227783203, loss=1.1592320203781128
I0215 14:39:24.692829 140532789532416 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3025298118591309, loss=1.1340031623840332
I0215 14:40:41.613831 140532797925120 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.28574538230896, loss=1.1395076513290405
I0215 14:42:04.729651 140532797925120 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.5508015155792236, loss=1.080490231513977
I0215 14:43:20.300423 140532789532416 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.1663811206817627, loss=1.117941975593567
I0215 14:44:35.768881 140532797925120 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.9269188642501831, loss=1.076839566230774
I0215 14:45:51.514657 140532789532416 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.03117036819458, loss=1.1586846113204956
I0215 14:47:09.900229 140532797925120 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.0528547763824463, loss=1.115760326385498
I0215 14:48:34.671270 140532789532416 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.004404902458191, loss=1.1156022548675537
I0215 14:49:58.998793 140532797925120 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.9787735342979431, loss=1.0671688318252563
I0215 14:51:23.703866 140532789532416 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.6710566282272339, loss=1.0669926404953003
I0215 14:52:49.800911 140532797925120 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.2173205614089966, loss=1.0960733890533447
I0215 14:54:15.115374 140532789532416 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.0680557489395142, loss=1.1034196615219116
I0215 14:55:40.519767 140532797925120 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.1077123880386353, loss=1.1081265211105347
I0215 14:56:55.816193 140532789532416 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.6986559629440308, loss=1.0443195104599
I0215 14:58:11.081556 140532797925120 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.2216724157333374, loss=1.1045254468917847
I0215 14:59:26.343870 140532789532416 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.401989221572876, loss=1.1019301414489746
I0215 15:00:33.960678 140599226058560 spec.py:321] Evaluating on the training split.
I0215 15:01:28.163038 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 15:02:20.054656 140599226058560 spec.py:349] Evaluating on the test split.
I0215 15:02:46.704504 140599226058560 submission_runner.py:408] Time since start: 63007.80s, 	Step: 71491, 	{'train/ctc_loss': Array(0.2064576, dtype=float32), 'train/wer': 0.0766548233632427, 'validation/ctc_loss': Array(0.41951063, dtype=float32), 'validation/wer': 0.1278082972088398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24452537, dtype=float32), 'test/wer': 0.08337903438750431, 'test/num_examples': 2472, 'score': 57644.84786987305, 'total_duration': 63007.79894852638, 'accumulated_submission_time': 57644.84786987305, 'accumulated_eval_time': 5357.613756656647, 'accumulated_logging_time': 2.299800157546997}
I0215 15:02:46.752516 140532797925120 logging_writer.py:48] [71491] accumulated_eval_time=5357.613757, accumulated_logging_time=2.299800, accumulated_submission_time=57644.847870, global_step=71491, preemption_count=0, score=57644.847870, test/ctc_loss=0.24452537298202515, test/num_examples=2472, test/wer=0.083379, total_duration=63007.798949, train/ctc_loss=0.20645759999752045, train/wer=0.076655, validation/ctc_loss=0.4195106327533722, validation/num_examples=5348, validation/wer=0.127808
I0215 15:02:54.344157 140532789532416 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1849888563156128, loss=1.1125000715255737
I0215 15:04:09.507321 140532797925120 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.3431203365325928, loss=1.1579822301864624
I0215 15:05:24.874206 140532789532416 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3492279052734375, loss=1.096915364265442
I0215 15:06:41.514167 140532797925120 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.0619525909423828, loss=1.1459091901779175
I0215 15:08:07.260615 140532789532416 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.4310250282287598, loss=1.1145884990692139
I0215 15:09:32.696897 140532797925120 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.0786042213439941, loss=1.058214783668518
I0215 15:11:01.793941 140532797925120 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.2478697299957275, loss=1.119803786277771
I0215 15:12:17.104950 140532789532416 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.0907405614852905, loss=1.1156142950057983
I0215 15:13:32.485955 140532797925120 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.1163063049316406, loss=1.0832726955413818
I0215 15:14:47.850816 140532789532416 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.3596088886260986, loss=1.1395227909088135
I0215 15:16:03.695131 140532797925120 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.1420994997024536, loss=1.105108618736267
I0215 15:17:27.641620 140532789532416 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.0763205289840698, loss=1.0862048864364624
I0215 15:18:53.577651 140532797925120 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3787739276885986, loss=1.1499741077423096
I0215 15:20:18.692131 140532789532416 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.0461357831954956, loss=1.0940821170806885
I0215 15:21:43.757894 140532797925120 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.415764570236206, loss=1.079319953918457
I0215 15:23:09.574610 140532789532416 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.9405068159103394, loss=1.1072866916656494
I0215 15:24:34.887841 140532797925120 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.9804847240447998, loss=1.0550326108932495
I0215 15:25:56.536330 140532797925120 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.1606993675231934, loss=1.11868417263031
I0215 15:26:47.303480 140599226058560 spec.py:321] Evaluating on the training split.
I0215 15:27:40.703985 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 15:28:32.334065 140599226058560 spec.py:349] Evaluating on the test split.
I0215 15:28:58.290076 140599226058560 submission_runner.py:408] Time since start: 64579.38s, 	Step: 73269, 	{'train/ctc_loss': Array(0.18731399, dtype=float32), 'train/wer': 0.07255515228553748, 'validation/ctc_loss': Array(0.41804275, dtype=float32), 'validation/wer': 0.12786622512720006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24227335, dtype=float32), 'test/wer': 0.08333841122824122, 'test/num_examples': 2472, 'score': 59085.31165289879, 'total_duration': 64579.38373923302, 'accumulated_submission_time': 59085.31165289879, 'accumulated_eval_time': 5488.593738079071, 'accumulated_logging_time': 2.365021228790283}
I0215 15:28:58.341628 140532797925120 logging_writer.py:48] [73269] accumulated_eval_time=5488.593738, accumulated_logging_time=2.365021, accumulated_submission_time=59085.311653, global_step=73269, preemption_count=0, score=59085.311653, test/ctc_loss=0.24227334558963776, test/num_examples=2472, test/wer=0.083338, total_duration=64579.383739, train/ctc_loss=0.1873139888048172, train/wer=0.072555, validation/ctc_loss=0.41804274916648865, validation/num_examples=5348, validation/wer=0.127866
I0215 15:29:22.398497 140532789532416 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.9484296441078186, loss=1.1226459741592407
I0215 15:30:37.633828 140532797925120 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4284132719039917, loss=1.0967916250228882
I0215 15:31:52.985477 140532789532416 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.8308713436126709, loss=1.0586754083633423
I0215 15:33:08.338793 140532797925120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9322523474693298, loss=1.083335280418396
I0215 15:34:27.259932 140532789532416 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0621689558029175, loss=1.0926278829574585
I0215 15:35:52.270405 140532797925120 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.9115428328514099, loss=1.1114063262939453
I0215 15:37:18.078265 140532789532416 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.8852986097335815, loss=1.0565649271011353
I0215 15:38:44.779718 140532797925120 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.904447078704834, loss=1.0960307121276855
I0215 15:40:10.593562 140532789532416 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.1378352642059326, loss=1.081470012664795
I0215 15:41:35.259295 140532797925120 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.2435393333435059, loss=1.1140285730361938
I0215 15:42:50.252773 140532789532416 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.16718590259552, loss=1.0859872102737427
I0215 15:44:05.446815 140532797925120 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.2467418909072876, loss=1.0439244508743286
I0215 15:45:20.736995 140532789532416 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3928221464157104, loss=1.0591267347335815
I0215 15:46:38.379921 140532797925120 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.9315246939659119, loss=1.1051162481307983
I0215 15:48:02.298265 140532789532416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.9394460916519165, loss=1.10894775390625
I0215 15:49:27.624248 140532797925120 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.8601850271224976, loss=1.1203421354293823
I0215 15:50:52.497585 140532789532416 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.064233660697937, loss=1.0968530178070068
I0215 15:52:17.730640 140532797925120 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.240196704864502, loss=1.0773881673812866
I0215 15:52:58.551521 140599226058560 spec.py:321] Evaluating on the training split.
I0215 15:53:51.444325 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 15:54:43.778975 140599226058560 spec.py:349] Evaluating on the test split.
I0215 15:55:10.249165 140599226058560 submission_runner.py:408] Time since start: 66151.34s, 	Step: 75049, 	{'train/ctc_loss': Array(0.19145286, dtype=float32), 'train/wer': 0.07430065901654453, 'validation/ctc_loss': Array(0.41606402, dtype=float32), 'validation/wer': 0.12715178080075693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2408368, dtype=float32), 'test/wer': 0.08199784697255906, 'test/num_examples': 2472, 'score': 60525.435987234116, 'total_duration': 66151.34201812744, 'accumulated_submission_time': 60525.435987234116, 'accumulated_eval_time': 5620.283970355988, 'accumulated_logging_time': 2.4329538345336914}
I0215 15:55:10.295538 140532797925120 logging_writer.py:48] [75049] accumulated_eval_time=5620.283970, accumulated_logging_time=2.432954, accumulated_submission_time=60525.435987, global_step=75049, preemption_count=0, score=60525.435987, test/ctc_loss=0.24083679914474487, test/num_examples=2472, test/wer=0.081998, total_duration=66151.342018, train/ctc_loss=0.19145286083221436, train/wer=0.074301, validation/ctc_loss=0.4160640239715576, validation/num_examples=5348, validation/wer=0.127152
I0215 15:55:49.298773 140532789532416 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.211718201637268, loss=1.0950876474380493
I0215 15:57:08.322342 140532797925120 logging_writer.py:48] [75200] global_step=75200, grad_norm=4.896625995635986, loss=1.1133378744125366
I0215 15:58:23.453553 140532789532416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.9422386288642883, loss=1.1006441116333008
I0215 15:59:38.599014 140532797925120 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.2184842824935913, loss=1.0692312717437744
I0215 16:00:53.892977 140532789532416 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.224627137184143, loss=1.136785626411438
I0215 16:02:11.311599 140532797925120 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.099898338317871, loss=1.113804578781128
I0215 16:03:35.645473 140532789532416 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.221821665763855, loss=1.0795917510986328
I0215 16:04:13.476568 140532797925120 logging_writer.py:48] [75746] global_step=75746, preemption_count=0, score=61068.553511
I0215 16:04:14.389478 140599226058560 checkpoints.py:490] Saving checkpoint at step: 75746
I0215 16:04:15.926815 140599226058560 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_2/checkpoint_75746
I0215 16:04:15.960086 140599226058560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_2/checkpoint_75746.
I0215 16:04:19.594191 140599226058560 submission_runner.py:583] Tuning trial 2/5
I0215 16:04:19.594453 140599226058560 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0215 16:04:19.615903 140599226058560 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.57337, dtype=float32), 'train/wer': 1.3618151438861104, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 35.945107221603394, 'total_duration': 171.58516669273376, 'accumulated_submission_time': 35.945107221603394, 'accumulated_eval_time': 135.64001059532166, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1760, {'train/ctc_loss': Array(7.147141, dtype=float32), 'train/wer': 0.938839590443686, 'validation/ctc_loss': Array(7.041991, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(7.0403576, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1476.374610900879, 'total_duration': 1721.4816236495972, 'accumulated_submission_time': 1476.374610900879, 'accumulated_eval_time': 245.00649857521057, 'accumulated_logging_time': 0.028804540634155273, 'global_step': 1760, 'preemption_count': 0}), (3580, {'train/ctc_loss': Array(5.8638206, dtype=float32), 'train/wer': 0.9417131519458765, 'validation/ctc_loss': Array(5.820514, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.784349, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2916.5684781074524, 'total_duration': 3270.1946194171906, 'accumulated_submission_time': 2916.5684781074524, 'accumulated_eval_time': 353.3921241760254, 'accumulated_logging_time': 0.08565068244934082, 'global_step': 3580, 'preemption_count': 0}), (5374, {'train/ctc_loss': Array(5.8075776, dtype=float32), 'train/wer': 0.940334510929565, 'validation/ctc_loss': Array(5.716846, dtype=float32), 'validation/wer': 0.8965503924616469, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7255216, dtype=float32), 'test/wer': 0.8994983039831007, 'test/num_examples': 2472, 'score': 4356.562227487564, 'total_duration': 4819.940863370895, 'accumulated_submission_time': 4356.562227487564, 'accumulated_eval_time': 463.0172669887543, 'accumulated_logging_time': 0.13686490058898926, 'global_step': 5374, 'preemption_count': 0}), (7157, {'train/ctc_loss': Array(5.684661, dtype=float32), 'train/wer': 0.9380313186211507, 'validation/ctc_loss': Array(5.6359296, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.6091366, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5797.4471888542175, 'total_duration': 6369.234001159668, 'accumulated_submission_time': 5797.4471888542175, 'accumulated_eval_time': 571.2973206043243, 'accumulated_logging_time': 0.18694400787353516, 'global_step': 7157, 'preemption_count': 0}), (8957, {'train/ctc_loss': Array(5.55107, dtype=float32), 'train/wer': 0.9408213025698041, 'validation/ctc_loss': Array(5.475975, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4412518, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7237.664181232452, 'total_duration': 7918.197555303574, 'accumulated_submission_time': 7237.664181232452, 'accumulated_eval_time': 679.9091982841492, 'accumulated_logging_time': 0.2448720932006836, 'global_step': 8957, 'preemption_count': 0}), (10800, {'train/ctc_loss': Array(4.311075, dtype=float32), 'train/wer': 0.812946352661112, 'validation/ctc_loss': Array(4.4674177, dtype=float32), 'validation/wer': 0.795195844637323, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.28696, dtype=float32), 'test/wer': 0.7847175674852234, 'test/num_examples': 2472, 'score': 8677.619053125381, 'total_duration': 9507.45812034607, 'accumulated_submission_time': 8677.619053125381, 'accumulated_eval_time': 829.0853753089905, 'accumulated_logging_time': 0.2981421947479248, 'global_step': 10800, 'preemption_count': 0}), (12614, {'train/ctc_loss': Array(2.5407991, dtype=float32), 'train/wer': 0.6001672788101385, 'validation/ctc_loss': Array(2.867275, dtype=float32), 'validation/wer': 0.6317522229838671, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.5912423, dtype=float32), 'test/wer': 0.5990697296528751, 'test/num_examples': 2472, 'score': 10117.509083271027, 'total_duration': 11080.74349808693, 'accumulated_submission_time': 10117.509083271027, 'accumulated_eval_time': 962.355612039566, 'accumulated_logging_time': 0.34575724601745605, 'global_step': 12614, 'preemption_count': 0}), (14388, {'train/ctc_loss': Array(1.5154848, dtype=float32), 'train/wer': 0.45609027722719947, 'validation/ctc_loss': Array(1.8800801, dtype=float32), 'validation/wer': 0.4955057590005503, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.5566329, dtype=float32), 'test/wer': 0.4498202425202608, 'test/num_examples': 2472, 'score': 11557.722923755646, 'total_duration': 12654.597905158997, 'accumulated_submission_time': 11557.722923755646, 'accumulated_eval_time': 1095.8711938858032, 'accumulated_logging_time': 0.395554780960083, 'global_step': 14388, 'preemption_count': 0}), (16164, {'train/ctc_loss': Array(1.1518191, dtype=float32), 'train/wer': 0.3756104432757325, 'validation/ctc_loss': Array(1.5427254, dtype=float32), 'validation/wer': 0.42724736186605133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2202077, dtype=float32), 'test/wer': 0.37438303576869175, 'test/num_examples': 2472, 'score': 12998.31579875946, 'total_duration': 14228.770209550858, 'accumulated_submission_time': 12998.31579875946, 'accumulated_eval_time': 1229.3207650184631, 'accumulated_logging_time': 0.4482710361480713, 'global_step': 16164, 'preemption_count': 0}), (17977, {'train/ctc_loss': Array(0.89231455, dtype=float32), 'train/wer': 0.2999344248311306, 'validation/ctc_loss': Array(1.2439867, dtype=float32), 'validation/wer': 0.3619143246087452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.92897916, dtype=float32), 'test/wer': 0.2999410964190685, 'test/num_examples': 2472, 'score': 14438.654992818832, 'total_duration': 15801.002537488937, 'accumulated_submission_time': 14438.654992818832, 'accumulated_eval_time': 1361.0830354690552, 'accumulated_logging_time': 0.5029153823852539, 'global_step': 17977, 'preemption_count': 0}), (19762, {'train/ctc_loss': Array(0.69894594, dtype=float32), 'train/wer': 0.24397619327470696, 'validation/ctc_loss': Array(1.0527449, dtype=float32), 'validation/wer': 0.3146741071859583, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.75496924, dtype=float32), 'test/wer': 0.2517214063737737, 'test/num_examples': 2472, 'score': 15878.671095132828, 'total_duration': 17373.685792684555, 'accumulated_submission_time': 15878.671095132828, 'accumulated_eval_time': 1493.616887807846, 'accumulated_logging_time': 0.5617284774780273, 'global_step': 19762, 'preemption_count': 0}), (21532, {'train/ctc_loss': Array(0.62691694, dtype=float32), 'train/wer': 0.22013692162417375, 'validation/ctc_loss': Array(0.92000806, dtype=float32), 'validation/wer': 0.2802745783330276, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6438148, dtype=float32), 'test/wer': 0.21741514837608922, 'test/num_examples': 2472, 'score': 17318.76188802719, 'total_duration': 18946.500923633575, 'accumulated_submission_time': 17318.76188802719, 'accumulated_eval_time': 1626.2040507793427, 'accumulated_logging_time': 0.6224701404571533, 'global_step': 21532, 'preemption_count': 0}), (23320, {'train/ctc_loss': Array(0.5428806, dtype=float32), 'train/wer': 0.1951972010178117, 'validation/ctc_loss': Array(0.85429704, dtype=float32), 'validation/wer': 0.2618341909883468, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58487004, dtype=float32), 'test/wer': 0.19905348038916987, 'test/num_examples': 2472, 'score': 18758.965651988983, 'total_duration': 20520.760549545288, 'accumulated_submission_time': 18758.965651988983, 'accumulated_eval_time': 1760.1241040229797, 'accumulated_logging_time': 0.6804590225219727, 'global_step': 23320, 'preemption_count': 0}), (25128, {'train/ctc_loss': Array(0.44918406, dtype=float32), 'train/wer': 0.16700946971674374, 'validation/ctc_loss': Array(0.79768723, dtype=float32), 'validation/wer': 0.24599090531681744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.534569, dtype=float32), 'test/wer': 0.18357605670993032, 'test/num_examples': 2472, 'score': 20199.365215063095, 'total_duration': 22095.005754709244, 'accumulated_submission_time': 20199.365215063095, 'accumulated_eval_time': 1893.8379728794098, 'accumulated_logging_time': 0.7349987030029297, 'global_step': 25128, 'preemption_count': 0}), (26903, {'train/ctc_loss': Array(0.4337378, dtype=float32), 'train/wer': 0.15935238745757646, 'validation/ctc_loss': Array(0.77539897, dtype=float32), 'validation/wer': 0.23939677727680855, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51442176, dtype=float32), 'test/wer': 0.17604046066662604, 'test/num_examples': 2472, 'score': 21639.267746448517, 'total_duration': 23666.374056100845, 'accumulated_submission_time': 21639.267746448517, 'accumulated_eval_time': 2025.1735785007477, 'accumulated_logging_time': 0.7892842292785645, 'global_step': 26903, 'preemption_count': 0}), (28686, {'train/ctc_loss': Array(0.3969769, dtype=float32), 'train/wer': 0.1507755277897458, 'validation/ctc_loss': Array(0.7187384, dtype=float32), 'validation/wer': 0.22330247062571806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4679157, dtype=float32), 'test/wer': 0.1655596855767473, 'test/num_examples': 2472, 'score': 23079.26572728157, 'total_duration': 25238.579748630524, 'accumulated_submission_time': 23079.26572728157, 'accumulated_eval_time': 2157.2508704662323, 'accumulated_logging_time': 0.8437254428863525, 'global_step': 28686, 'preemption_count': 0}), (30480, {'train/ctc_loss': Array(0.37746027, dtype=float32), 'train/wer': 0.13832676898003532, 'validation/ctc_loss': Array(0.68599296, dtype=float32), 'validation/wer': 0.21370574548403604, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44529587, dtype=float32), 'test/wer': 0.15583043893323584, 'test/num_examples': 2472, 'score': 24519.34055161476, 'total_duration': 26813.466148376465, 'accumulated_submission_time': 24519.34055161476, 'accumulated_eval_time': 2291.930029153824, 'accumulated_logging_time': 0.8999524116516113, 'global_step': 30480, 'preemption_count': 0}), (32284, {'train/ctc_loss': Array(0.36455256, dtype=float32), 'train/wer': 0.13647401651287033, 'validation/ctc_loss': Array(0.659797, dtype=float32), 'validation/wer': 0.20718885466850748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42393988, dtype=float32), 'test/wer': 0.14782767655840595, 'test/num_examples': 2472, 'score': 25959.834961652756, 'total_duration': 28387.683755159378, 'accumulated_submission_time': 25959.834961652756, 'accumulated_eval_time': 2425.518532037735, 'accumulated_logging_time': 0.9573686122894287, 'global_step': 32284, 'preemption_count': 0}), (34050, {'train/ctc_loss': Array(0.34491673, dtype=float32), 'train/wer': 0.13033875247637422, 'validation/ctc_loss': Array(0.6365954, dtype=float32), 'validation/wer': 0.1974473097309248, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39865455, dtype=float32), 'test/wer': 0.13779375622042125, 'test/num_examples': 2472, 'score': 27400.28605747223, 'total_duration': 29962.1297519207, 'accumulated_submission_time': 27400.28605747223, 'accumulated_eval_time': 2559.377106666565, 'accumulated_logging_time': 1.0186927318572998, 'global_step': 34050, 'preemption_count': 0}), (35829, {'train/ctc_loss': Array(0.33196563, dtype=float32), 'train/wer': 0.12498755573719812, 'validation/ctc_loss': Array(0.6245508, dtype=float32), 'validation/wer': 0.1948598627108335, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39754122, dtype=float32), 'test/wer': 0.13694066987589626, 'test/num_examples': 2472, 'score': 28840.38019633293, 'total_duration': 31535.858542203903, 'accumulated_submission_time': 28840.38019633293, 'accumulated_eval_time': 2692.8508801460266, 'accumulated_logging_time': 1.1042413711547852, 'global_step': 35829, 'preemption_count': 0}), (37632, {'train/ctc_loss': Array(0.2906541, dtype=float32), 'train/wer': 0.11264768994732985, 'validation/ctc_loss': Array(0.6097289, dtype=float32), 'validation/wer': 0.19014839201753286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37677324, dtype=float32), 'test/wer': 0.13182215180874618, 'test/num_examples': 2472, 'score': 30280.80466890335, 'total_duration': 33110.70022511482, 'accumulated_submission_time': 30280.80466890335, 'accumulated_eval_time': 2827.135392189026, 'accumulated_logging_time': 1.1598222255706787, 'global_step': 37632, 'preemption_count': 0}), (39434, {'train/ctc_loss': Array(0.2863048, dtype=float32), 'train/wer': 0.1086859870300435, 'validation/ctc_loss': Array(0.5883182, dtype=float32), 'validation/wer': 0.18471282234472905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37109178, dtype=float32), 'test/wer': 0.12936445067332886, 'test/num_examples': 2472, 'score': 31721.29601264, 'total_duration': 34684.53674340248, 'accumulated_submission_time': 31721.29601264, 'accumulated_eval_time': 2960.3469581604004, 'accumulated_logging_time': 1.2174606323242188, 'global_step': 39434, 'preemption_count': 0}), (41198, {'train/ctc_loss': Array(0.28539166, dtype=float32), 'train/wer': 0.10729820385198009, 'validation/ctc_loss': Array(0.5655498, dtype=float32), 'validation/wer': 0.176255346264132, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.351271, dtype=float32), 'test/wer': 0.12142262303739362, 'test/num_examples': 2472, 'score': 33161.7505197525, 'total_duration': 36256.37497663498, 'accumulated_submission_time': 33161.7505197525, 'accumulated_eval_time': 3091.5962493419647, 'accumulated_logging_time': 1.2778022289276123, 'global_step': 41198, 'preemption_count': 0}), (42961, {'train/ctc_loss': Array(0.2625836, dtype=float32), 'train/wer': 0.09887727364344041, 'validation/ctc_loss': Array(0.5573182, dtype=float32), 'validation/wer': 0.1747299110806453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3410652, dtype=float32), 'test/wer': 0.11941177665387037, 'test/num_examples': 2472, 'score': 34602.12437748909, 'total_duration': 37829.61024093628, 'accumulated_submission_time': 34602.12437748909, 'accumulated_eval_time': 3224.325934410095, 'accumulated_logging_time': 1.335150957107544, 'global_step': 42961, 'preemption_count': 0}), (44771, {'train/ctc_loss': Array(0.25002244, dtype=float32), 'train/wer': 0.09749153585718683, 'validation/ctc_loss': Array(0.55120695, dtype=float32), 'validation/wer': 0.17160180348919163, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33385333, dtype=float32), 'test/wer': 0.11618223549245425, 'test/num_examples': 2472, 'score': 36041.995379924774, 'total_duration': 39404.787296772, 'accumulated_submission_time': 36041.995379924774, 'accumulated_eval_time': 3359.500274658203, 'accumulated_logging_time': 1.389387607574463, 'global_step': 44771, 'preemption_count': 0}), (46549, {'train/ctc_loss': Array(0.27212, dtype=float32), 'train/wer': 0.09631768722677814, 'validation/ctc_loss': Array(0.52371204, dtype=float32), 'validation/wer': 0.16276779593925292, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32607883, dtype=float32), 'test/wer': 0.11234334694209169, 'test/num_examples': 2472, 'score': 37482.20679521561, 'total_duration': 40980.0468621254, 'accumulated_submission_time': 37482.20679521561, 'accumulated_eval_time': 3494.410943031311, 'accumulated_logging_time': 1.4525690078735352, 'global_step': 46549, 'preemption_count': 0}), (48311, {'train/ctc_loss': Array(0.19800447, dtype=float32), 'train/wer': 0.07766064359989334, 'validation/ctc_loss': Array(0.51308125, dtype=float32), 'validation/wer': 0.16020931287834173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30938765, dtype=float32), 'test/wer': 0.10740763309162553, 'test/num_examples': 2472, 'score': 38922.34973311424, 'total_duration': 42555.49884772301, 'accumulated_submission_time': 38922.34973311424, 'accumulated_eval_time': 3629.584242582321, 'accumulated_logging_time': 1.512967586517334, 'global_step': 48311, 'preemption_count': 0}), (50086, {'train/ctc_loss': Array(0.21282442, dtype=float32), 'train/wer': 0.0822324569887133, 'validation/ctc_loss': Array(0.50448716, dtype=float32), 'validation/wer': 0.157303262307269, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30191362, dtype=float32), 'test/wer': 0.10570146040257551, 'test/num_examples': 2472, 'score': 40362.35197472572, 'total_duration': 44130.646955013275, 'accumulated_submission_time': 40362.35197472572, 'accumulated_eval_time': 3764.5926241874695, 'accumulated_logging_time': 1.574493646621704, 'global_step': 50086, 'preemption_count': 0}), (51885, {'train/ctc_loss': Array(0.27286735, dtype=float32), 'train/wer': 0.1037494229120408, 'validation/ctc_loss': Array(0.5019356, dtype=float32), 'validation/wer': 0.1571680971644284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29826474, dtype=float32), 'test/wer': 0.10381248349684155, 'test/num_examples': 2472, 'score': 41802.366864681244, 'total_duration': 45704.29691815376, 'accumulated_submission_time': 41802.366864681244, 'accumulated_eval_time': 3898.0944876670837, 'accumulated_logging_time': 1.630784034729004, 'global_step': 51885, 'preemption_count': 0}), (53658, {'train/ctc_loss': Array(0.28197432, dtype=float32), 'train/wer': 0.1062609862536667, 'validation/ctc_loss': Array(0.4842883, dtype=float32), 'validation/wer': 0.1505739691244195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28853884, dtype=float32), 'test/wer': 0.10021733390205756, 'test/num_examples': 2472, 'score': 43242.77938055992, 'total_duration': 47276.94588851929, 'accumulated_submission_time': 43242.77938055992, 'accumulated_eval_time': 4030.1935641765594, 'accumulated_logging_time': 1.6926136016845703, 'global_step': 53658, 'preemption_count': 0}), (55435, {'train/ctc_loss': Array(0.3120024, dtype=float32), 'train/wer': 0.11836945424169297, 'validation/ctc_loss': Array(0.47713417, dtype=float32), 'validation/wer': 0.1493864467980343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28182715, dtype=float32), 'test/wer': 0.0968456116832206, 'test/num_examples': 2472, 'score': 44682.94412112236, 'total_duration': 48849.46684360504, 'accumulated_submission_time': 44682.94412112236, 'accumulated_eval_time': 4162.418050527573, 'accumulated_logging_time': 1.748811960220337, 'global_step': 55435, 'preemption_count': 0}), (57224, {'train/ctc_loss': Array(0.27040535, dtype=float32), 'train/wer': 0.10101537739171264, 'validation/ctc_loss': Array(0.46657196, dtype=float32), 'validation/wer': 0.1453218378597565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2736266, dtype=float32), 'test/wer': 0.09241768732354315, 'test/num_examples': 2472, 'score': 46123.28657770157, 'total_duration': 50421.46864414215, 'accumulated_submission_time': 46123.28657770157, 'accumulated_eval_time': 4293.939308643341, 'accumulated_logging_time': 1.8109087944030762, 'global_step': 57224, 'preemption_count': 0}), (59012, {'train/ctc_loss': Array(0.24490872, dtype=float32), 'train/wer': 0.09386993633364989, 'validation/ctc_loss': Array(0.45609862, dtype=float32), 'validation/wer': 0.14097724398273748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26979008, dtype=float32), 'test/wer': 0.09203176731054374, 'test/num_examples': 2472, 'score': 47563.390914440155, 'total_duration': 51994.320743083954, 'accumulated_submission_time': 47563.390914440155, 'accumulated_eval_time': 4426.556207895279, 'accumulated_logging_time': 1.867297887802124, 'global_step': 59012, 'preemption_count': 0}), (60762, {'train/ctc_loss': Array(0.20578378, dtype=float32), 'train/wer': 0.07972726732020136, 'validation/ctc_loss': Array(0.45228767, dtype=float32), 'validation/wer': 0.13957731928903136, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26356447, dtype=float32), 'test/wer': 0.09038652936038835, 'test/num_examples': 2472, 'score': 49003.68826889992, 'total_duration': 53567.35671496391, 'accumulated_submission_time': 49003.68826889992, 'accumulated_eval_time': 4559.163670539856, 'accumulated_logging_time': 1.9240319728851318, 'global_step': 60762, 'preemption_count': 0}), (62525, {'train/ctc_loss': Array(0.23597239, dtype=float32), 'train/wer': 0.09127745904321459, 'validation/ctc_loss': Array(0.44328588, dtype=float32), 'validation/wer': 0.13635266516697722, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2582945, dtype=float32), 'test/wer': 0.08983811671033656, 'test/num_examples': 2472, 'score': 50443.94960784912, 'total_duration': 55141.62321925163, 'accumulated_submission_time': 50443.94960784912, 'accumulated_eval_time': 4693.026634216309, 'accumulated_logging_time': 1.9902966022491455, 'global_step': 62525, 'preemption_count': 0}), (64333, {'train/ctc_loss': Array(0.20355521, dtype=float32), 'train/wer': 0.07782055395144905, 'validation/ctc_loss': Array(0.44492757, dtype=float32), 'validation/wer': 0.13546443708545333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25623506, dtype=float32), 'test/wer': 0.0880710092823919, 'test/num_examples': 2472, 'score': 51884.26449036598, 'total_duration': 56716.089381694794, 'accumulated_submission_time': 51884.26449036598, 'accumulated_eval_time': 4827.03906917572, 'accumulated_logging_time': 2.0525200366973877, 'global_step': 64333, 'preemption_count': 0}), (66115, {'train/ctc_loss': Array(0.20436361, dtype=float32), 'train/wer': 0.08001046030803428, 'validation/ctc_loss': Array(0.43434986, dtype=float32), 'validation/wer': 0.13363970765710534, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2523467, dtype=float32), 'test/wer': 0.08577580078402697, 'test/num_examples': 2472, 'score': 53324.194618463516, 'total_duration': 58289.38478684425, 'accumulated_submission_time': 53324.194618463516, 'accumulated_eval_time': 4960.271555185318, 'accumulated_logging_time': 2.1096343994140625, 'global_step': 66115, 'preemption_count': 0}), (67890, {'train/ctc_loss': Array(0.19853008, dtype=float32), 'train/wer': 0.0757643860245349, 'validation/ctc_loss': Array(0.426306, dtype=float32), 'validation/wer': 0.1307722756982728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24856624, dtype=float32), 'test/wer': 0.08457741758576565, 'test/num_examples': 2472, 'score': 54764.53757071495, 'total_duration': 59863.47870230675, 'accumulated_submission_time': 54764.53757071495, 'accumulated_eval_time': 5093.883127212524, 'accumulated_logging_time': 2.1731672286987305, 'global_step': 67890, 'preemption_count': 0}), (69674, {'train/ctc_loss': Array(0.20562021, dtype=float32), 'train/wer': 0.07878492061213356, 'validation/ctc_loss': Array(0.42373237, dtype=float32), 'validation/wer': 0.12933373239232648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24726346, dtype=float32), 'test/wer': 0.08378526598013528, 'test/num_examples': 2472, 'score': 56204.93294286728, 'total_duration': 61435.010118722916, 'accumulated_submission_time': 56204.93294286728, 'accumulated_eval_time': 5224.875775098801, 'accumulated_logging_time': 2.240447998046875, 'global_step': 69674, 'preemption_count': 0}), (71491, {'train/ctc_loss': Array(0.2064576, dtype=float32), 'train/wer': 0.0766548233632427, 'validation/ctc_loss': Array(0.41951063, dtype=float32), 'validation/wer': 0.1278082972088398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24452537, dtype=float32), 'test/wer': 0.08337903438750431, 'test/num_examples': 2472, 'score': 57644.84786987305, 'total_duration': 63007.79894852638, 'accumulated_submission_time': 57644.84786987305, 'accumulated_eval_time': 5357.613756656647, 'accumulated_logging_time': 2.299800157546997, 'global_step': 71491, 'preemption_count': 0}), (73269, {'train/ctc_loss': Array(0.18731399, dtype=float32), 'train/wer': 0.07255515228553748, 'validation/ctc_loss': Array(0.41804275, dtype=float32), 'validation/wer': 0.12786622512720006, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24227335, dtype=float32), 'test/wer': 0.08333841122824122, 'test/num_examples': 2472, 'score': 59085.31165289879, 'total_duration': 64579.38373923302, 'accumulated_submission_time': 59085.31165289879, 'accumulated_eval_time': 5488.593738079071, 'accumulated_logging_time': 2.365021228790283, 'global_step': 73269, 'preemption_count': 0}), (75049, {'train/ctc_loss': Array(0.19145286, dtype=float32), 'train/wer': 0.07430065901654453, 'validation/ctc_loss': Array(0.41606402, dtype=float32), 'validation/wer': 0.12715178080075693, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2408368, dtype=float32), 'test/wer': 0.08199784697255906, 'test/num_examples': 2472, 'score': 60525.435987234116, 'total_duration': 66151.34201812744, 'accumulated_submission_time': 60525.435987234116, 'accumulated_eval_time': 5620.283970355988, 'accumulated_logging_time': 2.4329538345336914, 'global_step': 75049, 'preemption_count': 0})], 'global_step': 75746}
I0215 16:04:19.616151 140599226058560 submission_runner.py:586] Timing: 61068.55351090431
I0215 16:04:19.616213 140599226058560 submission_runner.py:588] Total number of evals: 43
I0215 16:04:19.616265 140599226058560 submission_runner.py:589] ====================
I0215 16:04:19.616317 140599226058560 submission_runner.py:542] Using RNG seed 4151861576
I0215 16:04:19.619502 140599226058560 submission_runner.py:551] --- Tuning run 3/5 ---
I0215 16:04:19.619640 140599226058560 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_3.
I0215 16:04:19.621443 140599226058560 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_3/hparams.json.
I0215 16:04:19.623649 140599226058560 submission_runner.py:206] Initializing dataset.
I0215 16:04:19.623780 140599226058560 submission_runner.py:213] Initializing model.
I0215 16:04:23.613495 140599226058560 submission_runner.py:255] Initializing optimizer.
I0215 16:04:24.050429 140599226058560 submission_runner.py:262] Initializing metrics bundle.
I0215 16:04:24.050639 140599226058560 submission_runner.py:280] Initializing checkpoint and logger.
I0215 16:04:24.055709 140599226058560 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_3 with prefix checkpoint_
I0215 16:04:24.055850 140599226058560 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_3/meta_data_0.json.
I0215 16:04:24.056086 140599226058560 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 16:04:24.056163 140599226058560 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 16:04:24.893151 140599226058560 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 16:04:25.686752 140599226058560 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_3/flags_0.json.
I0215 16:04:25.707926 140599226058560 submission_runner.py:314] Starting training loop.
I0215 16:04:25.711288 140599226058560 input_pipeline.py:20] Loading split = train-clean-100
I0215 16:04:25.759601 140599226058560 input_pipeline.py:20] Loading split = train-clean-360
I0215 16:04:25.897327 140599226058560 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0215 16:05:00.015021 140427209434880 logging_writer.py:48] [0] global_step=0, grad_norm=38.067623138427734, loss=32.596923828125
I0215 16:05:00.038682 140599226058560 spec.py:321] Evaluating on the training split.
I0215 16:05:58.840210 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 16:06:49.343956 140599226058560 spec.py:349] Evaluating on the test split.
I0215 16:07:15.357405 140599226058560 submission_runner.py:408] Time since start: 169.65s, 	Step: 1, 	{'train/ctc_loss': Array(32.871246, dtype=float32), 'train/wer': 1.3708653155889337, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 34.33066487312317, 'total_duration': 169.64612746238708, 'accumulated_submission_time': 34.33066487312317, 'accumulated_eval_time': 135.31539034843445, 'accumulated_logging_time': 0}
I0215 16:07:15.373764 140532797925120 logging_writer.py:48] [1] accumulated_eval_time=135.315390, accumulated_logging_time=0, accumulated_submission_time=34.330665, global_step=1, preemption_count=0, score=34.330665, test/ctc_loss=31.23744010925293, test/num_examples=2472, test/wer=1.102350, total_duration=169.646127, train/ctc_loss=32.871246337890625, train/wer=1.370865, validation/ctc_loss=31.08704948425293, validation/num_examples=5348, validation/wer=1.058565
I0215 16:08:55.858807 140427323664128 logging_writer.py:48] [100] global_step=100, grad_norm=13.501869201660156, loss=12.081585884094238
I0215 16:10:11.645279 140427332056832 logging_writer.py:48] [200] global_step=200, grad_norm=10.620638847351074, loss=8.440752029418945
I0215 16:11:27.472867 140427323664128 logging_writer.py:48] [300] global_step=300, grad_norm=1.7551785707473755, loss=5.917462348937988
I0215 16:12:43.432158 140427332056832 logging_writer.py:48] [400] global_step=400, grad_norm=0.2783355414867401, loss=5.8390607833862305
I0215 16:13:59.280840 140427323664128 logging_writer.py:48] [500] global_step=500, grad_norm=0.37716808915138245, loss=5.843367576599121
I0215 16:15:20.814080 140427332056832 logging_writer.py:48] [600] global_step=600, grad_norm=0.44930049777030945, loss=5.816015243530273
I0215 16:16:47.613275 140427323664128 logging_writer.py:48] [700] global_step=700, grad_norm=0.4449516236782074, loss=5.815016746520996
I0215 16:18:14.069205 140427332056832 logging_writer.py:48] [800] global_step=800, grad_norm=0.23562686145305634, loss=5.796150207519531
I0215 16:19:40.485464 140427323664128 logging_writer.py:48] [900] global_step=900, grad_norm=0.4641354978084564, loss=5.798855304718018
I0215 16:21:07.360662 140427332056832 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3949391841888428, loss=5.775940418243408
I0215 16:22:29.534498 140532797925120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.23527026176452637, loss=5.787579536437988
I0215 16:23:45.366716 140532789532416 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6307424306869507, loss=5.791107177734375
I0215 16:25:01.221477 140532797925120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8951825499534607, loss=5.795159339904785
I0215 16:26:17.345411 140532789532416 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.36926519870758057, loss=5.780788421630859
I0215 16:27:40.974410 140532797925120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2680862545967102, loss=5.765839576721191
I0215 16:29:07.454662 140532789532416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.8086360096931458, loss=5.800333023071289
I0215 16:30:34.222770 140532797925120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7391113638877869, loss=5.773538112640381
I0215 16:31:15.948844 140599226058560 spec.py:321] Evaluating on the training split.
I0215 16:31:55.115586 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 16:32:41.595573 140599226058560 spec.py:349] Evaluating on the test split.
I0215 16:33:05.631084 140599226058560 submission_runner.py:408] Time since start: 1719.92s, 	Step: 1750, 	{'train/ctc_loss': Array(6.003965, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.1192107, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0945807, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1474.825115442276, 'total_duration': 1719.9163784980774, 'accumulated_submission_time': 1474.825115442276, 'accumulated_eval_time': 244.99092769622803, 'accumulated_logging_time': 0.028472900390625}
I0215 16:33:05.670716 140532797925120 logging_writer.py:48] [1750] accumulated_eval_time=244.990928, accumulated_logging_time=0.028473, accumulated_submission_time=1474.825115, global_step=1750, preemption_count=0, score=1474.825115, test/ctc_loss=6.09458065032959, test/num_examples=2472, test/wer=0.899580, total_duration=1719.916378, train/ctc_loss=6.003964900970459, train/wer=0.939190, validation/ctc_loss=6.119210720062256, validation/num_examples=5348, validation/wer=0.896618
I0215 16:33:44.348849 140532789532416 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.3290808200836182, loss=5.762103080749512
I0215 16:35:00.371677 140532797925120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.41134119033813477, loss=5.601807117462158
I0215 16:36:16.425650 140532789532416 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.4535269737243652, loss=5.532371997833252
I0215 16:37:37.172597 140532797925120 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2075151205062866, loss=5.426021575927734
I0215 16:38:52.975320 140532789532416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7388757467269897, loss=5.165703773498535
I0215 16:40:08.841862 140532797925120 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.2570123672485352, loss=4.538601398468018
I0215 16:41:24.681081 140532789532416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8994136452674866, loss=4.0177388191223145
I0215 16:42:45.087050 140532797925120 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.9838961362838745, loss=3.7496650218963623
I0215 16:44:11.057544 140532789532416 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.0504306554794312, loss=3.5693886280059814
I0215 16:45:36.553214 140532797925120 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.316805362701416, loss=3.3104746341705322
I0215 16:47:02.238082 140532789532416 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.4955388307571411, loss=3.2840359210968018
I0215 16:48:27.438379 140532797925120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9881799817085266, loss=3.114577054977417
I0215 16:49:53.251793 140532789532416 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0053937435150146, loss=3.0817716121673584
I0215 16:51:21.647916 140532797925120 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1306427717208862, loss=2.9097673892974854
I0215 16:52:37.239625 140532789532416 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.1879925727844238, loss=2.8740944862365723
I0215 16:53:52.805283 140532797925120 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.4372923374176025, loss=2.90868878364563
I0215 16:55:08.298200 140532789532416 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.089656949043274, loss=2.765045166015625
I0215 16:56:25.628325 140532797925120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9943317174911499, loss=2.744288682937622
I0215 16:57:05.815194 140599226058560 spec.py:321] Evaluating on the training split.
I0215 16:57:51.916361 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 16:58:41.430974 140599226058560 spec.py:349] Evaluating on the test split.
I0215 16:59:06.772819 140599226058560 submission_runner.py:408] Time since start: 3281.06s, 	Step: 3549, 	{'train/ctc_loss': Array(3.3109722, dtype=float32), 'train/wer': 0.7425314465408805, 'validation/ctc_loss': Array(3.2170153, dtype=float32), 'validation/wer': 0.7051565501993686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8966093, dtype=float32), 'test/wer': 0.6532000893709504, 'test/num_examples': 2472, 'score': 2914.8816883563995, 'total_duration': 3281.0594849586487, 'accumulated_submission_time': 2914.8816883563995, 'accumulated_eval_time': 365.9432199001312, 'accumulated_logging_time': 0.08463764190673828}
I0215 16:59:06.805868 140532797925120 logging_writer.py:48] [3549] accumulated_eval_time=365.943220, accumulated_logging_time=0.084638, accumulated_submission_time=2914.881688, global_step=3549, preemption_count=0, score=2914.881688, test/ctc_loss=2.896609306335449, test/num_examples=2472, test/wer=0.653200, total_duration=3281.059485, train/ctc_loss=3.310972213745117, train/wer=0.742531, validation/ctc_loss=3.217015266418457, validation/num_examples=5348, validation/wer=0.705157
I0215 16:59:46.011791 140532789532416 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.077833890914917, loss=2.6886842250823975
I0215 17:01:01.443736 140532797925120 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.4357258081436157, loss=2.7275006771087646
I0215 17:02:17.076861 140532789532416 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0021120309829712, loss=2.6376054286956787
I0215 17:03:37.920239 140532797925120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.944682240486145, loss=2.5778679847717285
I0215 17:05:03.001625 140532789532416 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.2682769298553467, loss=2.471238851547241
I0215 17:06:28.777519 140532797925120 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.9745675325393677, loss=2.5315346717834473
I0215 17:07:50.046574 140532797925120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.905173659324646, loss=2.3955066204071045
I0215 17:09:05.613020 140532789532416 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.0497807264328003, loss=2.3176300525665283
I0215 17:10:21.283866 140532797925120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7404426336288452, loss=2.310153007507324
I0215 17:11:36.829571 140532789532416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8263541460037231, loss=2.2375547885894775
I0215 17:12:59.900405 140532797925120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.7987271547317505, loss=2.2377102375030518
I0215 17:14:25.207377 140532789532416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7611460089683533, loss=2.1870615482330322
I0215 17:15:50.860518 140532797925120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.7649149298667908, loss=2.174091339111328
I0215 17:17:16.861664 140532789532416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7878329157829285, loss=2.185242176055908
I0215 17:18:42.188884 140532797925120 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.2414566278457642, loss=2.1858274936676025
I0215 17:20:08.072747 140532789532416 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.019729495048523, loss=2.1294310092926025
I0215 17:21:32.378727 140532797925120 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0027682781219482, loss=2.0483627319335938
I0215 17:22:47.868818 140532789532416 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7498112320899963, loss=1.9800480604171753
I0215 17:23:07.149841 140599226058560 spec.py:321] Evaluating on the training split.
I0215 17:24:01.045175 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 17:24:53.203049 140599226058560 spec.py:349] Evaluating on the test split.
I0215 17:25:19.177598 140599226058560 submission_runner.py:408] Time since start: 4853.46s, 	Step: 5327, 	{'train/ctc_loss': Array(1.1042546, dtype=float32), 'train/wer': 0.34265022443973503, 'validation/ctc_loss': Array(1.1603082, dtype=float32), 'validation/wer': 0.33234212228583565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.85711485, dtype=float32), 'test/wer': 0.2735563544776877, 'test/num_examples': 2472, 'score': 4355.136976242065, 'total_duration': 4853.463213682175, 'accumulated_submission_time': 4355.136976242065, 'accumulated_eval_time': 497.9646122455597, 'accumulated_logging_time': 0.136824369430542}
I0215 17:25:19.211640 140532797925120 logging_writer.py:48] [5327] accumulated_eval_time=497.964612, accumulated_logging_time=0.136824, accumulated_submission_time=4355.136976, global_step=5327, preemption_count=0, score=4355.136976, test/ctc_loss=0.857114851474762, test/num_examples=2472, test/wer=0.273556, total_duration=4853.463214, train/ctc_loss=1.1042546033859253, train/wer=0.342650, validation/ctc_loss=1.1603082418441772, validation/num_examples=5348, validation/wer=0.332342
I0215 17:26:14.747967 140532789532416 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7469812035560608, loss=1.9782665967941284
I0215 17:27:30.014312 140532797925120 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8233426213264465, loss=1.9319052696228027
I0215 17:28:45.324124 140532789532416 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7867581844329834, loss=1.9201867580413818
I0215 17:30:00.717270 140532797925120 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8277223706245422, loss=1.998779058456421
I0215 17:31:25.204901 140532789532416 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7994690537452698, loss=1.9456948041915894
I0215 17:32:50.877377 140532797925120 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7118740081787109, loss=1.9679855108261108
I0215 17:34:16.725767 140532789532416 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.704378068447113, loss=1.9520066976547241
I0215 17:35:42.018929 140532797925120 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7964513301849365, loss=1.9400115013122559
I0215 17:37:08.978044 140532797925120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.7047010660171509, loss=1.8728151321411133
I0215 17:38:24.308001 140532789532416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7460404634475708, loss=1.8291994333267212
I0215 17:39:39.549350 140532797925120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8053494095802307, loss=1.8402531147003174
I0215 17:40:54.806309 140532789532416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7998384833335876, loss=1.863391637802124
I0215 17:42:10.381122 140532797925120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.7639485001564026, loss=1.811743974685669
I0215 17:43:36.013271 140532789532416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7212369441986084, loss=1.8853007555007935
I0215 17:45:02.452780 140532797925120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.6319283246994019, loss=1.779598593711853
I0215 17:46:28.222171 140532789532416 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.752124547958374, loss=1.7970927953720093
I0215 17:47:54.270900 140532797925120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6972737908363342, loss=1.8311676979064941
I0215 17:49:20.750059 140532789532416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.7220609784126282, loss=1.8341090679168701
I0215 17:49:20.758995 140599226058560 spec.py:321] Evaluating on the training split.
I0215 17:50:14.284117 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 17:51:06.245639 140599226058560 spec.py:349] Evaluating on the test split.
I0215 17:51:32.568096 140599226058560 submission_runner.py:408] Time since start: 6426.85s, 	Step: 7101, 	{'train/ctc_loss': Array(0.71781844, dtype=float32), 'train/wer': 0.23816366362119198, 'validation/ctc_loss': Array(0.856381, dtype=float32), 'validation/wer': 0.25759579829498824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58395475, dtype=float32), 'test/wer': 0.19566144659070137, 'test/num_examples': 2472, 'score': 5796.596266269684, 'total_duration': 6426.853343486786, 'accumulated_submission_time': 5796.596266269684, 'accumulated_eval_time': 629.766964673996, 'accumulated_logging_time': 0.1887216567993164}
I0215 17:51:32.604046 140532797925120 logging_writer.py:48] [7101] accumulated_eval_time=629.766965, accumulated_logging_time=0.188722, accumulated_submission_time=5796.596266, global_step=7101, preemption_count=0, score=5796.596266, test/ctc_loss=0.5839547514915466, test/num_examples=2472, test/wer=0.195661, total_duration=6426.853343, train/ctc_loss=0.7178184390068054, train/wer=0.238164, validation/ctc_loss=0.8563809990882874, validation/num_examples=5348, validation/wer=0.257596
I0215 17:52:47.808883 140532789532416 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.8042266368865967, loss=1.7944891452789307
I0215 17:54:08.017097 140532797925120 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8489634394645691, loss=1.7319748401641846
I0215 17:55:23.264730 140532789532416 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.7175194025039673, loss=1.7838687896728516
I0215 17:56:38.655045 140532797925120 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6907898187637329, loss=1.7964552640914917
I0215 17:57:55.733493 140532789532416 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7355417013168335, loss=1.7463819980621338
I0215 17:59:20.307802 140532797925120 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6524435877799988, loss=1.7277543544769287
I0215 18:00:44.860654 140532789532416 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.7964155077934265, loss=1.7453856468200684
I0215 18:02:10.061780 140532797925120 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7555558085441589, loss=1.7302645444869995
I0215 18:03:35.119397 140532789532416 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7887920141220093, loss=1.7952958345413208
I0215 18:05:00.767520 140532797925120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.7624750137329102, loss=1.7191511392593384
I0215 18:06:25.919383 140532789532416 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6938626766204834, loss=1.7229920625686646
I0215 18:07:48.670900 140532797925120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7116948366165161, loss=1.735664963722229
I0215 18:09:04.350565 140532789532416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8083807826042175, loss=1.6977461576461792
I0215 18:10:19.654217 140532797925120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6059253811836243, loss=1.6710553169250488
I0215 18:11:35.114790 140532789532416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7094631195068359, loss=1.6895973682403564
I0215 18:12:54.752799 140532797925120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6270229816436768, loss=1.69691002368927
I0215 18:14:20.987703 140532789532416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.6084744334220886, loss=1.6847325563430786
I0215 18:15:32.800243 140599226058560 spec.py:321] Evaluating on the training split.
I0215 18:16:26.289495 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 18:17:18.644143 140599226058560 spec.py:349] Evaluating on the test split.
I0215 18:17:45.138306 140599226058560 submission_runner.py:408] Time since start: 7999.42s, 	Step: 8886, 	{'train/ctc_loss': Array(0.6037436, dtype=float32), 'train/wer': 0.20082768397105394, 'validation/ctc_loss': Array(0.7638324, dtype=float32), 'validation/wer': 0.23031174874730875, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5030347, dtype=float32), 'test/wer': 0.16748928564174437, 'test/num_examples': 2472, 'score': 7236.706146240234, 'total_duration': 7999.423669338226, 'accumulated_submission_time': 7236.706146240234, 'accumulated_eval_time': 762.0983710289001, 'accumulated_logging_time': 0.2411792278289795}
I0215 18:17:45.175234 140532797925120 logging_writer.py:48] [8886] accumulated_eval_time=762.098371, accumulated_logging_time=0.241179, accumulated_submission_time=7236.706146, global_step=8886, preemption_count=0, score=7236.706146, test/ctc_loss=0.5030347108840942, test/num_examples=2472, test/wer=0.167489, total_duration=7999.423669, train/ctc_loss=0.6037436127662659, train/wer=0.200828, validation/ctc_loss=0.7638323903083801, validation/num_examples=5348, validation/wer=0.230312
I0215 18:17:56.487575 140532789532416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6160654425621033, loss=1.6940258741378784
I0215 18:19:11.592117 140532797925120 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7061586380004883, loss=1.680504560470581
I0215 18:20:26.932863 140532789532416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7998855113983154, loss=1.664057731628418
I0215 18:21:43.864196 140532797925120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7339854836463928, loss=1.7566481828689575
I0215 18:23:09.920274 140532797925120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6893565654754639, loss=1.6630195379257202
I0215 18:24:25.031558 140532789532416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.6685876846313477, loss=1.6848835945129395
I0215 18:25:40.396430 140532797925120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7868973016738892, loss=1.6589690446853638
I0215 18:26:55.818095 140532789532416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.6296210289001465, loss=1.598508358001709
I0215 18:28:14.759289 140532797925120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.7351860404014587, loss=1.6602303981781006
I0215 18:29:40.718896 140532789532416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.6656329035758972, loss=1.6597472429275513
I0215 18:31:06.504786 140532797925120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.7190307378768921, loss=1.6462239027023315
I0215 18:32:32.284003 140532789532416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8285794854164124, loss=1.672930121421814
I0215 18:33:58.607232 140532797925120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.631586492061615, loss=1.6412369012832642
I0215 18:35:24.863118 140532789532416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6990032196044922, loss=1.6145347356796265
I0215 18:36:54.151971 140532797925120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6560351252555847, loss=1.5937527418136597
I0215 18:38:09.399439 140532789532416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7357862591743469, loss=1.6628748178482056
I0215 18:39:24.746602 140532797925120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6748980283737183, loss=1.6405366659164429
I0215 18:40:40.097954 140532789532416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.7346164584159851, loss=1.6242202520370483
I0215 18:41:45.513268 140599226058560 spec.py:321] Evaluating on the training split.
I0215 18:42:39.645333 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 18:43:32.864078 140599226058560 spec.py:349] Evaluating on the test split.
I0215 18:43:59.338315 140599226058560 submission_runner.py:408] Time since start: 9573.62s, 	Step: 10688, 	{'train/ctc_loss': Array(0.59205496, dtype=float32), 'train/wer': 0.20361274677653848, 'validation/ctc_loss': Array(0.702261, dtype=float32), 'validation/wer': 0.2120741091168889, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45312193, dtype=float32), 'test/wer': 0.15556638839802572, 'test/num_examples': 2472, 'score': 8676.956310272217, 'total_duration': 9573.624148845673, 'accumulated_submission_time': 8676.956310272217, 'accumulated_eval_time': 895.9172258377075, 'accumulated_logging_time': 0.2942352294921875}
I0215 18:43:59.377669 140532797925120 logging_writer.py:48] [10688] accumulated_eval_time=895.917226, accumulated_logging_time=0.294235, accumulated_submission_time=8676.956310, global_step=10688, preemption_count=0, score=8676.956310, test/ctc_loss=0.45312193036079407, test/num_examples=2472, test/wer=0.155566, total_duration=9573.624149, train/ctc_loss=0.5920549631118774, train/wer=0.203613, validation/ctc_loss=0.7022609710693359, validation/num_examples=5348, validation/wer=0.212074
I0215 18:44:09.259169 140532789532416 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.5601897835731506, loss=1.6149070262908936
I0215 18:45:24.624077 140532797925120 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.603941023349762, loss=1.5457288026809692
I0215 18:46:40.254183 140532789532416 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8899335265159607, loss=1.6160513162612915
I0215 18:47:55.715832 140532797925120 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7376868724822998, loss=1.5589404106140137
I0215 18:49:19.913749 140532789532416 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.891735315322876, loss=1.6285550594329834
I0215 18:50:46.242183 140532797925120 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6135944724082947, loss=1.561397910118103
I0215 18:52:12.278256 140532789532416 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.6216000914573669, loss=1.5962644815444946
I0215 18:53:34.381001 140532797925120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6538002490997314, loss=1.5419188737869263
I0215 18:54:49.672176 140532789532416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6276084780693054, loss=1.5783872604370117
I0215 18:56:04.896394 140532797925120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5932217240333557, loss=1.5411847829818726
I0215 18:57:20.274951 140532789532416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.788081169128418, loss=1.549035906791687
I0215 18:58:43.432852 140532797925120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.7914727926254272, loss=1.6353648900985718
I0215 19:00:09.705721 140532789532416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.6169198751449585, loss=1.5470792055130005
I0215 19:01:35.828769 140532797925120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5578944683074951, loss=1.571232795715332
I0215 19:03:00.532139 140532789532416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5617573261260986, loss=1.506037950515747
I0215 19:04:26.044523 140532797925120 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6774933934211731, loss=1.5879864692687988
I0215 19:05:52.176450 140532789532416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5601001381874084, loss=1.4628534317016602
I0215 19:07:17.294196 140532797925120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.7449190020561218, loss=1.5831109285354614
I0215 19:07:59.797651 140599226058560 spec.py:321] Evaluating on the training split.
I0215 19:08:53.167052 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 19:09:45.101962 140599226058560 spec.py:349] Evaluating on the test split.
I0215 19:10:11.389236 140599226058560 submission_runner.py:408] Time since start: 11145.68s, 	Step: 12458, 	{'train/ctc_loss': Array(0.49941278, dtype=float32), 'train/wer': 0.17151479117353408, 'validation/ctc_loss': Array(0.64830595, dtype=float32), 'validation/wer': 0.19569981752705717, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4115406, dtype=float32), 'test/wer': 0.1393374362724189, 'test/num_examples': 2472, 'score': 10117.289225816727, 'total_duration': 11145.675840854645, 'accumulated_submission_time': 10117.289225816727, 'accumulated_eval_time': 1027.5034348964691, 'accumulated_logging_time': 0.3512380123138428}
I0215 19:10:11.428596 140532797925120 logging_writer.py:48] [12458] accumulated_eval_time=1027.503435, accumulated_logging_time=0.351238, accumulated_submission_time=10117.289226, global_step=12458, preemption_count=0, score=10117.289226, test/ctc_loss=0.41154059767723083, test/num_examples=2472, test/wer=0.139337, total_duration=11145.675841, train/ctc_loss=0.49941277503967285, train/wer=0.171515, validation/ctc_loss=0.6483059525489807, validation/num_examples=5348, validation/wer=0.195700
I0215 19:10:43.734445 140532789532416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5998048186302185, loss=1.5038516521453857
I0215 19:11:58.988782 140532797925120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6651785373687744, loss=1.5512340068817139
I0215 19:13:14.196997 140532789532416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6292232275009155, loss=1.5463312864303589
I0215 19:14:29.561879 140532797925120 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.535845160484314, loss=1.5177620649337769
I0215 19:15:44.965472 140532789532416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7401024699211121, loss=1.5293041467666626
I0215 19:17:08.092391 140532797925120 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6752579808235168, loss=1.4810723066329956
I0215 19:18:33.308094 140532789532416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.6829417943954468, loss=1.609346866607666
I0215 19:19:58.024862 140532797925120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5881965756416321, loss=1.5542864799499512
I0215 19:21:23.799119 140532789532416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.6495851874351501, loss=1.5639832019805908
I0215 19:22:51.842508 140532797925120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.6103933453559875, loss=1.4947651624679565
I0215 19:24:06.944551 140532789532416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6284677386283875, loss=1.4898154735565186
I0215 19:25:22.206004 140532797925120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.7789697647094727, loss=1.5424212217330933
I0215 19:26:37.427380 140532789532416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.603583812713623, loss=1.4954180717468262
I0215 19:27:52.805523 140532797925120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5971930623054504, loss=1.5076690912246704
I0215 19:29:15.899171 140532789532416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6177733540534973, loss=1.5066930055618286
I0215 19:30:40.999617 140532797925120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5902298092842102, loss=1.5356950759887695
I0215 19:32:06.653727 140532789532416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5833434462547302, loss=1.5173187255859375
I0215 19:33:31.463593 140532797925120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.7579343318939209, loss=1.5090183019638062
I0215 19:34:11.457324 140599226058560 spec.py:321] Evaluating on the training split.
I0215 19:35:05.565395 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 19:35:57.960872 140599226058560 spec.py:349] Evaluating on the test split.
I0215 19:36:24.926059 140599226058560 submission_runner.py:408] Time since start: 12719.21s, 	Step: 14248, 	{'train/ctc_loss': Array(0.49112177, dtype=float32), 'train/wer': 0.16632042911473102, 'validation/ctc_loss': Array(0.6159968, dtype=float32), 'validation/wer': 0.18688511928323856, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38463014, dtype=float32), 'test/wer': 0.13159872443279913, 'test/num_examples': 2472, 'score': 11557.23063492775, 'total_duration': 12719.211951255798, 'accumulated_submission_time': 11557.23063492775, 'accumulated_eval_time': 1160.9660465717316, 'accumulated_logging_time': 0.408066987991333}
I0215 19:36:24.959993 140532797925120 logging_writer.py:48] [14248] accumulated_eval_time=1160.966047, accumulated_logging_time=0.408067, accumulated_submission_time=11557.230635, global_step=14248, preemption_count=0, score=11557.230635, test/ctc_loss=0.38463014364242554, test/num_examples=2472, test/wer=0.131599, total_duration=12719.211951, train/ctc_loss=0.491121768951416, train/wer=0.166320, validation/ctc_loss=0.615996778011322, validation/num_examples=5348, validation/wer=0.186885
I0215 19:37:04.862609 140532789532416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6300610899925232, loss=1.5219568014144897
I0215 19:38:20.321636 140532797925120 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6057978272438049, loss=1.564403772354126
I0215 19:39:39.365772 140532797925120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6505774855613708, loss=1.4832855463027954
I0215 19:40:54.653461 140532789532416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.61432945728302, loss=1.5018972158432007
I0215 19:42:09.949424 140532797925120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.6192694306373596, loss=1.5311691761016846
I0215 19:43:26.215060 140532789532416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6525828838348389, loss=1.4623793363571167
I0215 19:44:49.214860 140532797925120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7295563817024231, loss=1.4952003955841064
I0215 19:46:10.203218 140532789532416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6518147587776184, loss=1.5768275260925293
I0215 19:47:32.253660 140532797925120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.7036993503570557, loss=1.4980772733688354
I0215 19:48:53.201049 140532789532416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6435202956199646, loss=1.5450940132141113
I0215 19:50:13.972332 140532797925120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9817212224006653, loss=1.5440927743911743
I0215 19:51:35.202908 140532789532416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5999351143836975, loss=1.4427450895309448
I0215 19:52:56.695580 140532797925120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.7493273019790649, loss=1.4212913513183594
I0215 19:54:12.011154 140532789532416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7311820983886719, loss=1.4474114179611206
I0215 19:55:27.197332 140532797925120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.6831851005554199, loss=1.4816774129867554
I0215 19:56:42.447684 140532789532416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.6974111795425415, loss=1.4640979766845703
I0215 19:57:57.723041 140532797925120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7060333490371704, loss=1.5197229385375977
I0215 19:59:13.021279 140532789532416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7241064310073853, loss=1.437978982925415
I0215 20:00:25.665201 140599226058560 spec.py:321] Evaluating on the training split.
I0215 20:01:18.470900 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 20:02:07.762516 140599226058560 spec.py:349] Evaluating on the test split.
I0215 20:02:33.133148 140599226058560 submission_runner.py:408] Time since start: 14287.42s, 	Step: 16095, 	{'train/ctc_loss': Array(0.41128248, dtype=float32), 'train/wer': 0.1451225741954266, 'validation/ctc_loss': Array(0.5923722, dtype=float32), 'validation/wer': 0.17998204234530832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36829486, dtype=float32), 'test/wer': 0.1253021347470193, 'test/num_examples': 2472, 'score': 12997.853597402573, 'total_duration': 14287.417057275772, 'accumulated_submission_time': 12997.853597402573, 'accumulated_eval_time': 1288.4258918762207, 'accumulated_logging_time': 0.45952868461608887}
I0215 20:02:33.172314 140532797925120 logging_writer.py:48] [16095] accumulated_eval_time=1288.425892, accumulated_logging_time=0.459529, accumulated_submission_time=12997.853597, global_step=16095, preemption_count=0, score=12997.853597, test/ctc_loss=0.3682948648929596, test/num_examples=2472, test/wer=0.125302, total_duration=14287.417057, train/ctc_loss=0.411282479763031, train/wer=0.145123, validation/ctc_loss=0.5923721790313721, validation/num_examples=5348, validation/wer=0.179982
I0215 20:02:37.769634 140532789532416 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.5917782187461853, loss=1.480277419090271
I0215 20:03:52.826738 140532797925120 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.7425186634063721, loss=1.4720121622085571
I0215 20:05:08.091526 140532789532416 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.8395602703094482, loss=1.438899040222168
I0215 20:06:23.240531 140532797925120 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5773184299468994, loss=1.4581378698349
I0215 20:07:42.322509 140532797925120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5909290909767151, loss=1.4834741353988647
I0215 20:08:57.495456 140532789532416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.6626471877098083, loss=1.4379724264144897
I0215 20:10:12.814279 140532797925120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5675663948059082, loss=1.4331834316253662
I0215 20:11:28.166525 140532789532416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.6089175343513489, loss=1.417056679725647
I0215 20:12:43.478426 140532797925120 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5891411304473877, loss=1.4733866453170776
I0215 20:13:58.758714 140532789532416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6452457308769226, loss=1.4084606170654297
I0215 20:15:16.232384 140532797925120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5954275727272034, loss=1.4491201639175415
I0215 20:16:37.434045 140532789532416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6036795377731323, loss=1.441717267036438
I0215 20:17:58.990003 140532797925120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.665991485118866, loss=1.4301304817199707
I0215 20:19:22.660647 140532789532416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6372461915016174, loss=1.528923749923706
I0215 20:20:44.561384 140532797925120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5676309466362, loss=1.3698362112045288
I0215 20:22:03.776942 140532797925120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.5660290122032166, loss=1.4069247245788574
I0215 20:23:19.056611 140532789532416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7605959177017212, loss=1.4832109212875366
I0215 20:24:34.566129 140532797925120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.6248984336853027, loss=1.4273170232772827
I0215 20:25:49.895105 140532789532416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7052804231643677, loss=1.4457225799560547
I0215 20:26:33.204714 140599226058560 spec.py:321] Evaluating on the training split.
I0215 20:27:24.056636 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 20:28:12.845917 140599226058560 spec.py:349] Evaluating on the test split.
I0215 20:28:38.304783 140599226058560 submission_runner.py:408] Time since start: 15852.59s, 	Step: 17959, 	{'train/ctc_loss': Array(0.41662517, dtype=float32), 'train/wer': 0.14680189517323067, 'validation/ctc_loss': Array(0.57269543, dtype=float32), 'validation/wer': 0.17245141295847533, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35398287, dtype=float32), 'test/wer': 0.12046797879471087, 'test/num_examples': 2472, 'score': 14437.80846619606, 'total_duration': 15852.590964317322, 'accumulated_submission_time': 14437.80846619606, 'accumulated_eval_time': 1413.5200998783112, 'accumulated_logging_time': 0.5142014026641846}
I0215 20:28:38.336847 140532797925120 logging_writer.py:48] [17959] accumulated_eval_time=1413.520100, accumulated_logging_time=0.514201, accumulated_submission_time=14437.808466, global_step=17959, preemption_count=0, score=14437.808466, test/ctc_loss=0.3539828658103943, test/num_examples=2472, test/wer=0.120468, total_duration=15852.590964, train/ctc_loss=0.41662517189979553, train/wer=0.146802, validation/ctc_loss=0.5726954340934753, validation/num_examples=5348, validation/wer=0.172451
I0215 20:29:09.917128 140532789532416 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6463114023208618, loss=1.4398959875106812
I0215 20:30:25.224398 140532797925120 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6631871461868286, loss=1.4549752473831177
I0215 20:31:40.525360 140532789532416 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.5771496295928955, loss=1.3811663389205933
I0215 20:32:55.985957 140532797925120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6207644939422607, loss=1.4056880474090576
I0215 20:34:11.429774 140532789532416 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.695175051689148, loss=1.4756124019622803
I0215 20:35:33.218329 140532797925120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6316853165626526, loss=1.3971534967422485
I0215 20:36:54.219707 140532797925120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.6810768842697144, loss=1.424801230430603
I0215 20:38:09.530656 140532789532416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6781052350997925, loss=1.3677301406860352
I0215 20:39:24.952078 140532797925120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5966258645057678, loss=1.4245226383209229
I0215 20:40:40.238657 140532789532416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.5773407816886902, loss=1.4263449907302856
I0215 20:41:55.485423 140532797925120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6377074718475342, loss=1.386352777481079
I0215 20:43:10.826992 140532789532416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.6734054684638977, loss=1.4635319709777832
I0215 20:44:28.270858 140532797925120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5888233780860901, loss=1.3650997877120972
I0215 20:45:49.851674 140532789532416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7109591364860535, loss=1.470383882522583
I0215 20:47:11.065421 140532797925120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6021464467048645, loss=1.3993196487426758
I0215 20:48:31.441725 140532789532416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.742251992225647, loss=1.4308501482009888
I0215 20:49:55.224579 140532797925120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.6243024468421936, loss=1.3877233266830444
I0215 20:51:10.450130 140532789532416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.754838228225708, loss=1.4643434286117554
I0215 20:52:25.742614 140532797925120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.6498148441314697, loss=1.370042324066162
I0215 20:52:38.979128 140599226058560 spec.py:321] Evaluating on the training split.
I0215 20:53:29.917987 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 20:54:19.124698 140599226058560 spec.py:349] Evaluating on the test split.
I0215 20:54:44.244005 140599226058560 submission_runner.py:408] Time since start: 17418.53s, 	Step: 19819, 	{'train/ctc_loss': Array(0.3939056, dtype=float32), 'train/wer': 0.13807692737617508, 'validation/ctc_loss': Array(0.55853176, dtype=float32), 'validation/wer': 0.16761443177539415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33988407, dtype=float32), 'test/wer': 0.11502447545345601, 'test/num_examples': 2472, 'score': 15878.370507478714, 'total_duration': 17418.529887914658, 'accumulated_submission_time': 15878.370507478714, 'accumulated_eval_time': 1538.7788639068604, 'accumulated_logging_time': 0.561873197555542}
I0215 20:54:44.274828 140532797925120 logging_writer.py:48] [19819] accumulated_eval_time=1538.778864, accumulated_logging_time=0.561873, accumulated_submission_time=15878.370507, global_step=19819, preemption_count=0, score=15878.370507, test/ctc_loss=0.33988407254219055, test/num_examples=2472, test/wer=0.115024, total_duration=17418.529888, train/ctc_loss=0.3939056098461151, train/wer=0.138077, validation/ctc_loss=0.5585317611694336, validation/num_examples=5348, validation/wer=0.167614
I0215 20:55:46.123618 140532789532416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.6501307487487793, loss=1.3540011644363403
I0215 20:57:01.446289 140532797925120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6593189835548401, loss=1.3614931106567383
I0215 20:58:16.782693 140532789532416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6427375674247742, loss=1.3593569993972778
I0215 20:59:32.072841 140532797925120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6332947611808777, loss=1.4109910726547241
I0215 21:00:47.463056 140532789532416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.7669359445571899, loss=1.3404152393341064
I0215 21:02:05.638890 140532797925120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.6265496015548706, loss=1.3872945308685303
I0215 21:03:27.386555 140532789532416 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7036574482917786, loss=1.5026862621307373
I0215 21:04:52.919914 140532797925120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.7215650677680969, loss=1.365983486175537
I0215 21:06:07.992014 140532789532416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.6003838181495667, loss=1.3655319213867188
I0215 21:07:23.149611 140532797925120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.7527967691421509, loss=1.3474451303482056
I0215 21:08:38.343611 140532789532416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.663885235786438, loss=1.3729993104934692
I0215 21:09:53.800379 140532797925120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6060348749160767, loss=1.3217424154281616
I0215 21:11:09.047348 140532789532416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6494060158729553, loss=1.3334839344024658
I0215 21:12:24.302043 140532797925120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.6525047421455383, loss=1.40234375
I0215 21:13:39.664304 140532789532416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.621540367603302, loss=1.378158450126648
I0215 21:14:59.132322 140532797925120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.6250436305999756, loss=1.3853111267089844
I0215 21:16:21.219488 140532789532416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6949840188026428, loss=1.3690903186798096
I0215 21:17:42.585335 140532797925120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7149876952171326, loss=1.3878309726715088
I0215 21:18:44.970499 140599226058560 spec.py:321] Evaluating on the training split.
I0215 21:19:47.784978 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 21:20:37.540151 140599226058560 spec.py:349] Evaluating on the test split.
I0215 21:21:03.202703 140599226058560 submission_runner.py:408] Time since start: 18997.49s, 	Step: 21678, 	{'train/ctc_loss': Array(0.26414925, dtype=float32), 'train/wer': 0.0981504777709376, 'validation/ctc_loss': Array(0.5420662, dtype=float32), 'validation/wer': 0.16272917732701275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32638323, dtype=float32), 'test/wer': 0.10870757418804461, 'test/num_examples': 2472, 'score': 17318.98425388336, 'total_duration': 18997.489067554474, 'accumulated_submission_time': 17318.98425388336, 'accumulated_eval_time': 1677.0056114196777, 'accumulated_logging_time': 0.6107773780822754}
I0215 21:21:03.235605 140532797925120 logging_writer.py:48] [21678] accumulated_eval_time=1677.005611, accumulated_logging_time=0.610777, accumulated_submission_time=17318.984254, global_step=21678, preemption_count=0, score=17318.984254, test/ctc_loss=0.32638323307037354, test/num_examples=2472, test/wer=0.108708, total_duration=18997.489068, train/ctc_loss=0.2641492486000061, train/wer=0.098150, validation/ctc_loss=0.542066216468811, validation/num_examples=5348, validation/wer=0.162729
I0215 21:21:20.555489 140532789532416 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.7573685646057129, loss=1.4302668571472168
I0215 21:22:35.636750 140532797925120 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5676988959312439, loss=1.3762857913970947
I0215 21:23:50.861299 140532789532416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5535851120948792, loss=1.3146110773086548
I0215 21:25:06.108166 140532797925120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5986887812614441, loss=1.4108521938323975
I0215 21:26:21.342862 140532789532416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7670823335647583, loss=1.3989953994750977
I0215 21:27:36.856326 140532797925120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7473048567771912, loss=1.422526478767395
I0215 21:28:52.148359 140532789532416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.7014009952545166, loss=1.3720917701721191
I0215 21:30:07.388431 140532797925120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6204647421836853, loss=1.3870688676834106
I0215 21:31:27.500288 140532789532416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7297077178955078, loss=1.3724029064178467
I0215 21:32:48.956502 140532797925120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.6222844123840332, loss=1.4052103757858276
I0215 21:34:11.013295 140532797925120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.728813648223877, loss=1.3410271406173706
I0215 21:35:26.404277 140532789532416 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.587946355342865, loss=1.3063873052597046
I0215 21:36:41.718813 140532797925120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.6612011790275574, loss=1.3432435989379883
I0215 21:37:57.142906 140532789532416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6334787011146545, loss=1.4063087701797485
I0215 21:39:12.518363 140532797925120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.6569708585739136, loss=1.3503637313842773
I0215 21:40:27.836116 140532789532416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.6642778515815735, loss=1.3823142051696777
I0215 21:41:43.423388 140532797925120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6662744283676147, loss=1.3829728364944458
I0215 21:43:04.115992 140532789532416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.739830732345581, loss=1.4016098976135254
I0215 21:44:26.243168 140532797925120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5752273201942444, loss=1.3444362878799438
I0215 21:45:03.837770 140599226058560 spec.py:321] Evaluating on the training split.
I0215 21:45:58.250120 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 21:46:49.022427 140599226058560 spec.py:349] Evaluating on the test split.
I0215 21:47:14.248934 140599226058560 submission_runner.py:408] Time since start: 20568.53s, 	Step: 23548, 	{'train/ctc_loss': Array(0.24399593, dtype=float32), 'train/wer': 0.08901873262866412, 'validation/ctc_loss': Array(0.5178802, dtype=float32), 'validation/wer': 0.15804667059289224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3171775, dtype=float32), 'test/wer': 0.10893100156399163, 'test/num_examples': 2472, 'score': 18759.508040905, 'total_duration': 20568.534235477448, 'accumulated_submission_time': 18759.508040905, 'accumulated_eval_time': 1807.410040140152, 'accumulated_logging_time': 0.6587982177734375}
I0215 21:47:14.288744 140532797925120 logging_writer.py:48] [23548] accumulated_eval_time=1807.410040, accumulated_logging_time=0.658798, accumulated_submission_time=18759.508041, global_step=23548, preemption_count=0, score=18759.508041, test/ctc_loss=0.31717750430107117, test/num_examples=2472, test/wer=0.108931, total_duration=20568.534235, train/ctc_loss=0.24399593472480774, train/wer=0.089019, validation/ctc_loss=0.5178802013397217, validation/num_examples=5348, validation/wer=0.158047
I0215 21:47:54.040957 140532789532416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7192886471748352, loss=1.4014899730682373
I0215 21:49:13.110520 140532797925120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.6737293004989624, loss=1.3497955799102783
I0215 21:50:28.155476 140532789532416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.773979127407074, loss=1.3310935497283936
I0215 21:51:43.328719 140532797925120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.587552011013031, loss=1.3525149822235107
I0215 21:52:58.528280 140532789532416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7897197008132935, loss=1.355212926864624
I0215 21:54:13.768129 140532797925120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5680616497993469, loss=1.3212071657180786
I0215 21:55:29.096427 140532789532416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.7572311162948608, loss=1.3586844205856323
I0215 21:56:47.448149 140532797925120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6193611025810242, loss=1.3972141742706299
I0215 21:58:08.655816 140532789532416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7122102379798889, loss=1.3574061393737793
I0215 21:59:30.478899 140532797925120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6179635524749756, loss=1.3187769651412964
I0215 22:00:52.409222 140532789532416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6264740824699402, loss=1.4315727949142456
I0215 22:02:15.108435 140532797925120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.6311317086219788, loss=1.3374687433242798
I0215 22:03:34.736559 140532797925120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6540306210517883, loss=1.3378801345825195
I0215 22:04:50.015473 140532789532416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.7457894682884216, loss=1.352669358253479
I0215 22:06:05.211065 140532797925120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6529779434204102, loss=1.3817496299743652
I0215 22:07:20.323773 140532789532416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8265878558158875, loss=1.360960841178894
I0215 22:08:35.492063 140532797925120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6403840780258179, loss=1.3000489473342896
I0215 22:09:53.454059 140532789532416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6694468855857849, loss=1.373892068862915
I0215 22:11:14.917547 140532797925120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.8518837690353394, loss=1.3677598237991333
I0215 22:11:14.926037 140599226058560 spec.py:321] Evaluating on the training split.
I0215 22:12:08.863236 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 22:12:59.189749 140599226058560 spec.py:349] Evaluating on the test split.
I0215 22:13:24.302373 140599226058560 submission_runner.py:408] Time since start: 22138.59s, 	Step: 25401, 	{'train/ctc_loss': Array(0.2405506, dtype=float32), 'train/wer': 0.08822214860736464, 'validation/ctc_loss': Array(0.51276976, dtype=float32), 'validation/wer': 0.15382758720565376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31287903, dtype=float32), 'test/wer': 0.10529522880994455, 'test/num_examples': 2472, 'score': 20200.06555223465, 'total_duration': 22138.588984251022, 'accumulated_submission_time': 20200.06555223465, 'accumulated_eval_time': 1936.7809422016144, 'accumulated_logging_time': 0.7138986587524414}
I0215 22:13:24.341592 140532797925120 logging_writer.py:48] [25401] accumulated_eval_time=1936.780942, accumulated_logging_time=0.713899, accumulated_submission_time=20200.065552, global_step=25401, preemption_count=0, score=20200.065552, test/ctc_loss=0.3128790259361267, test/num_examples=2472, test/wer=0.105295, total_duration=22138.588984, train/ctc_loss=0.24055060744285583, train/wer=0.088222, validation/ctc_loss=0.5127697587013245, validation/num_examples=5348, validation/wer=0.153828
I0215 22:14:39.305089 140532789532416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6909930109977722, loss=1.3170185089111328
I0215 22:15:54.775500 140532797925120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6677065491676331, loss=1.4077099561691284
I0215 22:17:10.007203 140532789532416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.6393433809280396, loss=1.2994701862335205
I0215 22:18:29.049396 140532797925120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.6608566641807556, loss=1.300863265991211
I0215 22:19:44.179051 140532789532416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.6657844185829163, loss=1.3412929773330688
I0215 22:20:59.334327 140532797925120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.695732057094574, loss=1.3074074983596802
I0215 22:22:14.550100 140532789532416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6774054765701294, loss=1.4000239372253418
I0215 22:23:29.730149 140532797925120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.7391319274902344, loss=1.4108102321624756
I0215 22:24:45.318166 140532789532416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.6751266717910767, loss=1.2962098121643066
I0215 22:26:06.966042 140532797925120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6027122139930725, loss=1.3920907974243164
I0215 22:27:30.178355 140532789532416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6228839159011841, loss=1.3471860885620117
I0215 22:28:51.350623 140532797925120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.7020664811134338, loss=1.2812752723693848
I0215 22:30:13.235194 140532789532416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6817833185195923, loss=1.37995445728302
I0215 22:31:36.803448 140532797925120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.7321377992630005, loss=1.3342088460922241
I0215 22:32:52.032226 140532789532416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6674951910972595, loss=1.3340681791305542
I0215 22:34:07.278810 140532797925120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6697603464126587, loss=1.3477962017059326
I0215 22:35:22.616745 140532789532416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.7203673720359802, loss=1.3077985048294067
I0215 22:36:37.890278 140532797925120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5858623385429382, loss=1.3010445833206177
I0215 22:37:24.916373 140599226058560 spec.py:321] Evaluating on the training split.
I0215 22:38:18.889374 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 22:39:08.543613 140599226058560 spec.py:349] Evaluating on the test split.
I0215 22:39:34.488930 140599226058560 submission_runner.py:408] Time since start: 23708.78s, 	Step: 27264, 	{'train/ctc_loss': Array(0.22414844, dtype=float32), 'train/wer': 0.08301188654325906, 'validation/ctc_loss': Array(0.50314236, dtype=float32), 'validation/wer': 0.1510277378182415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29457772, dtype=float32), 'test/wer': 0.10123291288363497, 'test/num_examples': 2472, 'score': 21640.561172246933, 'total_duration': 23708.775426387787, 'accumulated_submission_time': 21640.561172246933, 'accumulated_eval_time': 2066.3479647636414, 'accumulated_logging_time': 0.7682766914367676}
I0215 22:39:34.523670 140532797925120 logging_writer.py:48] [27264] accumulated_eval_time=2066.347965, accumulated_logging_time=0.768277, accumulated_submission_time=21640.561172, global_step=27264, preemption_count=0, score=21640.561172, test/ctc_loss=0.2945777177810669, test/num_examples=2472, test/wer=0.101233, total_duration=23708.775426, train/ctc_loss=0.2241484373807907, train/wer=0.083012, validation/ctc_loss=0.5031423568725586, validation/num_examples=5348, validation/wer=0.151028
I0215 22:40:02.268936 140532789532416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.7554945349693298, loss=1.3383537530899048
I0215 22:41:17.447580 140532797925120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7232828140258789, loss=1.2960401773452759
I0215 22:42:32.647113 140532789532416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.6375970840454102, loss=1.2870725393295288
I0215 22:43:47.867613 140532797925120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6963925957679749, loss=1.3746204376220703
I0215 22:45:03.088420 140532789532416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.6196202635765076, loss=1.2947545051574707
I0215 22:46:23.544235 140532797925120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6358590722084045, loss=1.3471531867980957
I0215 22:47:43.128118 140532797925120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5904725790023804, loss=1.241214394569397
I0215 22:48:58.278560 140532789532416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.752606987953186, loss=1.2942399978637695
I0215 22:50:13.564674 140532797925120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.7430915832519531, loss=1.3501609563827515
I0215 22:51:28.751804 140532789532416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6213535070419312, loss=1.3228744268417358
I0215 22:52:44.103708 140532797925120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.665179431438446, loss=1.3419355154037476
I0215 22:54:01.254328 140532789532416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.7429977059364319, loss=1.3310377597808838
I0215 22:55:22.796224 140532797925120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6155882477760315, loss=1.354894757270813
I0215 22:56:43.876728 140532789532416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.751693069934845, loss=1.3356140851974487
I0215 22:58:06.614854 140532797925120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.6338992714881897, loss=1.3339194059371948
I0215 22:59:29.280866 140532789532416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.624579906463623, loss=1.2872151136398315
I0215 23:00:50.315806 140532797925120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.6428316831588745, loss=1.337471604347229
I0215 23:02:05.941968 140532789532416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9243532419204712, loss=1.2434172630310059
I0215 23:03:21.304164 140532797925120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.802596390247345, loss=1.2738653421401978
I0215 23:03:34.557570 140599226058560 spec.py:321] Evaluating on the training split.
I0215 23:04:27.294451 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 23:05:17.187170 140599226058560 spec.py:349] Evaluating on the test split.
I0215 23:05:42.509490 140599226058560 submission_runner.py:408] Time since start: 25276.80s, 	Step: 29119, 	{'train/ctc_loss': Array(0.22378603, dtype=float32), 'train/wer': 0.08479563335536555, 'validation/ctc_loss': Array(0.4944983, dtype=float32), 'validation/wer': 0.14928024561437384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29227978, dtype=float32), 'test/wer': 0.09968923283163732, 'test/num_examples': 2472, 'score': 23080.513560533524, 'total_duration': 25276.796125650406, 'accumulated_submission_time': 23080.513560533524, 'accumulated_eval_time': 2194.294489145279, 'accumulated_logging_time': 0.8199701309204102}
I0215 23:05:42.543729 140532797925120 logging_writer.py:48] [29119] accumulated_eval_time=2194.294489, accumulated_logging_time=0.819970, accumulated_submission_time=23080.513561, global_step=29119, preemption_count=0, score=23080.513561, test/ctc_loss=0.29227977991104126, test/num_examples=2472, test/wer=0.099689, total_duration=25276.796126, train/ctc_loss=0.22378602623939514, train/wer=0.084796, validation/ctc_loss=0.4944983124732971, validation/num_examples=5348, validation/wer=0.149280
I0215 23:06:44.136202 140532789532416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.628596842288971, loss=1.2589356899261475
I0215 23:07:59.263482 140532797925120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.704261064529419, loss=1.3259050846099854
I0215 23:09:14.530934 140532789532416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.6931261420249939, loss=1.2996019124984741
I0215 23:10:29.752225 140532797925120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6764509081840515, loss=1.2730392217636108
I0215 23:11:44.986274 140532789532416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6231526732444763, loss=1.2891710996627808
I0215 23:13:06.573150 140532797925120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.687025785446167, loss=1.2863750457763672
I0215 23:14:29.549736 140532789532416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5930538177490234, loss=1.274925708770752
I0215 23:15:53.157846 140532797925120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.6359178423881531, loss=1.2591252326965332
I0215 23:17:08.263187 140532789532416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6385017037391663, loss=1.2768536806106567
I0215 23:18:23.470988 140532797925120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6368931531906128, loss=1.29474675655365
I0215 23:19:38.925620 140532789532416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.8007249236106873, loss=1.2916845083236694
I0215 23:20:54.037841 140532797925120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.7472195625305176, loss=1.2891720533370972
I0215 23:22:09.262268 140532789532416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.599843442440033, loss=1.2994341850280762
I0215 23:23:30.119312 140532797925120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.7394683957099915, loss=1.2997307777404785
I0215 23:24:52.536677 140532789532416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.729141891002655, loss=1.3112876415252686
I0215 23:26:13.422449 140532797925120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6957778334617615, loss=1.3253122568130493
I0215 23:27:34.970577 140532789532416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.7307759523391724, loss=1.3122804164886475
I0215 23:29:00.346144 140532797925120 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.7630748748779297, loss=1.2996490001678467
I0215 23:29:42.799466 140599226058560 spec.py:321] Evaluating on the training split.
I0215 23:30:36.684187 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 23:31:26.146076 140599226058560 spec.py:349] Evaluating on the test split.
I0215 23:31:51.358361 140599226058560 submission_runner.py:408] Time since start: 26845.64s, 	Step: 30958, 	{'train/ctc_loss': Array(0.20724949, dtype=float32), 'train/wer': 0.07801074446828589, 'validation/ctc_loss': Array(0.48233274, dtype=float32), 'validation/wer': 0.14484875985981444, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.286033, dtype=float32), 'test/wer': 0.09818617593890276, 'test/num_examples': 2472, 'score': 24520.68779182434, 'total_duration': 26845.64391064644, 'accumulated_submission_time': 24520.68779182434, 'accumulated_eval_time': 2322.8469140529633, 'accumulated_logging_time': 0.8701145648956299}
I0215 23:31:51.397893 140532797925120 logging_writer.py:48] [30958] accumulated_eval_time=2322.846914, accumulated_logging_time=0.870115, accumulated_submission_time=24520.687792, global_step=30958, preemption_count=0, score=24520.687792, test/ctc_loss=0.2860330045223236, test/num_examples=2472, test/wer=0.098186, total_duration=26845.643911, train/ctc_loss=0.2072494924068451, train/wer=0.078011, validation/ctc_loss=0.4823327362537384, validation/num_examples=5348, validation/wer=0.144849
I0215 23:32:23.729913 140532789532416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6049327850341797, loss=1.2744687795639038
I0215 23:33:38.878936 140532797925120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.9067134857177734, loss=1.2961993217468262
I0215 23:34:54.029200 140532789532416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.7011533379554749, loss=1.2366923093795776
I0215 23:36:09.544209 140532797925120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6374055743217468, loss=1.283563256263733
I0215 23:37:24.889985 140532789532416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.860808789730072, loss=1.3420543670654297
I0215 23:38:40.297569 140532797925120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7230063676834106, loss=1.277549147605896
I0215 23:39:55.871701 140532789532416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6093959212303162, loss=1.3061400651931763
I0215 23:41:18.019522 140532797925120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.6902251839637756, loss=1.2512880563735962
I0215 23:42:39.682329 140532789532416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.7483585476875305, loss=1.2809228897094727
I0215 23:44:02.410448 140532797925120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.7161157727241516, loss=1.306181788444519
I0215 23:45:23.157839 140532797925120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.581055760383606, loss=1.2081257104873657
I0215 23:46:38.463782 140532789532416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.8671501278877258, loss=1.2558376789093018
I0215 23:47:53.726519 140532797925120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.6393806338310242, loss=1.330438256263733
I0215 23:49:08.925097 140532789532416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5690445899963379, loss=1.2391902208328247
I0215 23:50:24.185719 140532797925120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6496862769126892, loss=1.2141509056091309
I0215 23:51:44.117618 140532789532416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.703827440738678, loss=1.267269253730774
I0215 23:53:05.555374 140532797925120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.7344438433647156, loss=1.2924344539642334
I0215 23:54:27.024622 140532789532416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.7506518363952637, loss=1.2996078729629517
I0215 23:55:48.868569 140532797925120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.6307854652404785, loss=1.2958102226257324
I0215 23:55:51.678201 140599226058560 spec.py:321] Evaluating on the training split.
I0215 23:56:44.874186 140599226058560 spec.py:333] Evaluating on the validation split.
I0215 23:57:34.844521 140599226058560 spec.py:349] Evaluating on the test split.
I0215 23:58:00.321182 140599226058560 submission_runner.py:408] Time since start: 28414.61s, 	Step: 32805, 	{'train/ctc_loss': Array(0.24316454, dtype=float32), 'train/wer': 0.08499948343456204, 'validation/ctc_loss': Array(0.4656841, dtype=float32), 'validation/wer': 0.14146963128879964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27722582, dtype=float32), 'test/wer': 0.09574878638311701, 'test/num_examples': 2472, 'score': 25960.884137392044, 'total_duration': 28414.607186079025, 'accumulated_submission_time': 25960.884137392044, 'accumulated_eval_time': 2451.4838552474976, 'accumulated_logging_time': 0.9259054660797119}
I0215 23:58:00.356663 140532797925120 logging_writer.py:48] [32805] accumulated_eval_time=2451.483855, accumulated_logging_time=0.925905, accumulated_submission_time=25960.884137, global_step=32805, preemption_count=0, score=25960.884137, test/ctc_loss=0.2772258222103119, test/num_examples=2472, test/wer=0.095749, total_duration=28414.607186, train/ctc_loss=0.2431645393371582, train/wer=0.084999, validation/ctc_loss=0.46568408608436584, validation/num_examples=5348, validation/wer=0.141470
I0215 23:59:12.413789 140532789532416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.8208089470863342, loss=1.247931718826294
I0216 00:00:31.248449 140532797925120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5838804244995117, loss=1.2193188667297363
I0216 00:01:46.612493 140532789532416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.6251596808433533, loss=1.219344973564148
I0216 00:03:02.117225 140532797925120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.6346854567527771, loss=1.2132130861282349
I0216 00:04:17.513239 140532789532416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.758825421333313, loss=1.297882318496704
I0216 00:05:32.989395 140532797925120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.7002370953559875, loss=1.2409162521362305
I0216 00:06:48.992587 140532789532416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6758010387420654, loss=1.258811593055725
I0216 00:08:10.326425 140532797925120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.8953177332878113, loss=1.2796621322631836
I0216 00:09:32.688946 140532789532416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.6872555017471313, loss=1.2304003238677979
I0216 00:10:55.015847 140532797925120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6196622848510742, loss=1.2933247089385986
I0216 00:12:17.458426 140532789532416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6944670677185059, loss=1.2299890518188477
I0216 00:13:41.582514 140532797925120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7465681433677673, loss=1.2145007848739624
I0216 00:14:56.707881 140532789532416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7453094720840454, loss=1.2524725198745728
I0216 00:16:11.871214 140532797925120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.6773641705513, loss=1.248740792274475
I0216 00:17:27.114004 140532789532416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6159155964851379, loss=1.2586863040924072
I0216 00:18:42.353706 140532797925120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7783781886100769, loss=1.2744204998016357
I0216 00:19:57.613060 140532789532416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7346658110618591, loss=1.2454882860183716
I0216 00:21:18.494807 140532797925120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.7497780323028564, loss=1.2000539302825928
I0216 00:22:00.892188 140599226058560 spec.py:321] Evaluating on the training split.
I0216 00:22:54.965553 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 00:23:45.208757 140599226058560 spec.py:349] Evaluating on the test split.
I0216 00:24:11.030762 140599226058560 submission_runner.py:408] Time since start: 29985.32s, 	Step: 34652, 	{'train/ctc_loss': Array(0.20290689, dtype=float32), 'train/wer': 0.07742401933621476, 'validation/ctc_loss': Array(0.46335202, dtype=float32), 'validation/wer': 0.13939388088089055, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.264707, dtype=float32), 'test/wer': 0.09221457152722767, 'test/num_examples': 2472, 'score': 27401.33624315262, 'total_duration': 29985.316709280014, 'accumulated_submission_time': 27401.33624315262, 'accumulated_eval_time': 2581.6163563728333, 'accumulated_logging_time': 0.9803595542907715}
I0216 00:24:11.066796 140532797925120 logging_writer.py:48] [34652] accumulated_eval_time=2581.616356, accumulated_logging_time=0.980360, accumulated_submission_time=27401.336243, global_step=34652, preemption_count=0, score=27401.336243, test/ctc_loss=0.2647069990634918, test/num_examples=2472, test/wer=0.092215, total_duration=29985.316709, train/ctc_loss=0.20290689170360565, train/wer=0.077424, validation/ctc_loss=0.4633520245552063, validation/num_examples=5348, validation/wer=0.139394
I0216 00:24:47.832756 140532789532416 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.7522151470184326, loss=1.3036261796951294
I0216 00:26:03.264678 140532797925120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.8230251669883728, loss=1.2147188186645508
I0216 00:27:18.463904 140532789532416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.661658763885498, loss=1.3038570880889893
I0216 00:28:33.767359 140532797925120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6392168402671814, loss=1.2589908838272095
I0216 00:29:52.579839 140532797925120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.6998502612113953, loss=1.2611119747161865
I0216 00:31:07.898216 140532789532416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6740505695343018, loss=1.1997349262237549
I0216 00:32:23.277857 140532797925120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6665840744972229, loss=1.2487633228302002
I0216 00:33:38.587882 140532789532416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.7963809967041016, loss=1.2686405181884766
I0216 00:34:54.011252 140532797925120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6529524922370911, loss=1.2502989768981934
I0216 00:36:11.889535 140532789532416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.6329976916313171, loss=1.2704750299453735
I0216 00:37:34.168138 140532797925120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.5938289761543274, loss=1.2012449502944946
I0216 00:38:56.690447 140532789532416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.7656365036964417, loss=1.3093076944351196
I0216 00:40:19.317491 140532797925120 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.5821520090103149, loss=1.2270475625991821
I0216 00:41:41.099266 140532789532416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.8182872533798218, loss=1.2799160480499268
I0216 00:43:04.125371 140532797925120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.6824216842651367, loss=1.2019366025924683
I0216 00:44:19.606837 140532789532416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.8763173222541809, loss=1.2477744817733765
I0216 00:45:35.013149 140532797925120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.9034029841423035, loss=1.2194632291793823
I0216 00:46:50.434392 140532789532416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7010616064071655, loss=1.2546882629394531
I0216 00:48:05.908597 140532797925120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6402165293693542, loss=1.2284423112869263
I0216 00:48:11.654946 140599226058560 spec.py:321] Evaluating on the training split.
I0216 00:49:06.084708 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 00:49:57.204696 140599226058560 spec.py:349] Evaluating on the test split.
I0216 00:50:22.861638 140599226058560 submission_runner.py:408] Time since start: 31557.15s, 	Step: 36509, 	{'train/ctc_loss': Array(0.18551725, dtype=float32), 'train/wer': 0.0709965430233595, 'validation/ctc_loss': Array(0.45179525, dtype=float32), 'validation/wer': 0.13585062320785501, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2665545, dtype=float32), 'test/wer': 0.08910689984360083, 'test/num_examples': 2472, 'score': 28841.840735912323, 'total_duration': 31557.14866900444, 'accumulated_submission_time': 28841.840735912323, 'accumulated_eval_time': 2712.818051815033, 'accumulated_logging_time': 1.0341756343841553}
I0216 00:50:22.901178 140532797925120 logging_writer.py:48] [36509] accumulated_eval_time=2712.818052, accumulated_logging_time=1.034176, accumulated_submission_time=28841.840736, global_step=36509, preemption_count=0, score=28841.840736, test/ctc_loss=0.26655450463294983, test/num_examples=2472, test/wer=0.089107, total_duration=31557.148669, train/ctc_loss=0.18551725149154663, train/wer=0.070997, validation/ctc_loss=0.4517952501773834, validation/num_examples=5348, validation/wer=0.135851
I0216 00:51:31.861084 140532789532416 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.641065239906311, loss=1.2189053297042847
I0216 00:52:47.057497 140532797925120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.6198972463607788, loss=1.1948217153549194
I0216 00:54:02.257362 140532789532416 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.917229175567627, loss=1.22869873046875
I0216 00:55:17.586531 140532797925120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.8621324300765991, loss=1.2559982538223267
I0216 00:56:36.181960 140532789532416 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6952605247497559, loss=1.2649192810058594
I0216 00:58:01.209862 140532797925120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.8053651452064514, loss=1.2258471250534058
I0216 00:59:16.379399 140532789532416 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.7362162470817566, loss=1.2395707368850708
I0216 01:00:31.714146 140532797925120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.7216827273368835, loss=1.2217239141464233
I0216 01:01:46.963100 140532789532416 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0339921712875366, loss=1.2279542684555054
I0216 01:03:02.277497 140532797925120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8437638878822327, loss=1.1902389526367188
I0216 01:04:17.482925 140532789532416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6157770156860352, loss=1.2519325017929077
I0216 01:05:38.517029 140532797925120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.6211574673652649, loss=1.2048879861831665
I0216 01:07:00.642740 140532789532416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.8089975118637085, loss=1.2294089794158936
I0216 01:08:23.795963 140532797925120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8920042514801025, loss=1.2522202730178833
I0216 01:09:46.614430 140532789532416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7948054671287537, loss=1.2426838874816895
I0216 01:11:08.817261 140532797925120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.7066829204559326, loss=1.2536710500717163
I0216 01:12:27.976630 140532797925120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.6870618462562561, loss=1.2182806730270386
I0216 01:13:43.244738 140532789532416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.7324811220169067, loss=1.2697800397872925
I0216 01:14:23.527770 140599226058560 spec.py:321] Evaluating on the training split.
I0216 01:15:16.796954 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 01:16:07.417863 140599226058560 spec.py:349] Evaluating on the test split.
I0216 01:16:33.231817 140599226058560 submission_runner.py:408] Time since start: 33127.52s, 	Step: 38355, 	{'train/ctc_loss': Array(0.17950557, dtype=float32), 'train/wer': 0.06721768942514683, 'validation/ctc_loss': Array(0.4428782, dtype=float32), 'validation/wer': 0.1336300530040453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26117116, dtype=float32), 'test/wer': 0.08843661771575975, 'test/num_examples': 2472, 'score': 30282.38442492485, 'total_duration': 33127.51769065857, 'accumulated_submission_time': 30282.38442492485, 'accumulated_eval_time': 2842.51593208313, 'accumulated_logging_time': 1.0907597541809082}
I0216 01:16:33.273925 140532797925120 logging_writer.py:48] [38355] accumulated_eval_time=2842.515932, accumulated_logging_time=1.090760, accumulated_submission_time=30282.384425, global_step=38355, preemption_count=0, score=30282.384425, test/ctc_loss=0.2611711621284485, test/num_examples=2472, test/wer=0.088437, total_duration=33127.517691, train/ctc_loss=0.17950557172298431, train/wer=0.067218, validation/ctc_loss=0.44287818670272827, validation/num_examples=5348, validation/wer=0.133630
I0216 01:17:07.864157 140532789532416 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.6398530006408691, loss=1.1587144136428833
I0216 01:18:23.123428 140532797925120 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8953443169593811, loss=1.2038366794586182
I0216 01:19:38.422635 140532789532416 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.6477341651916504, loss=1.2187801599502563
I0216 01:20:53.609481 140532797925120 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6870884299278259, loss=1.2142033576965332
I0216 01:22:08.892007 140532789532416 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7994205951690674, loss=1.2473716735839844
I0216 01:23:28.394071 140532797925120 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.6667040586471558, loss=1.2707816362380981
I0216 01:24:51.757568 140532789532416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.8372308611869812, loss=1.2283709049224854
I0216 01:26:15.114853 140532797925120 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.7332810163497925, loss=1.2038311958312988
I0216 01:27:36.171700 140532797925120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6842256188392639, loss=1.2083362340927124
I0216 01:28:51.739436 140532789532416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.692191481590271, loss=1.1906172037124634
I0216 01:30:06.999590 140532797925120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.7287001013755798, loss=1.2019742727279663
I0216 01:31:22.258520 140532789532416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7150089144706726, loss=1.216110110282898
I0216 01:32:37.570043 140532797925120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6601147651672363, loss=1.2014049291610718
I0216 01:33:52.854642 140532789532416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7215450406074524, loss=1.1714541912078857
I0216 01:35:14.898475 140532797925120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7226629257202148, loss=1.2292996644973755
I0216 01:36:37.521273 140532789532416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.6416689157485962, loss=1.2450618743896484
I0216 01:37:59.410196 140532797925120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6578179001808167, loss=1.1804016828536987
I0216 01:39:22.089669 140532789532416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6783434152603149, loss=1.1644383668899536
I0216 01:40:33.796645 140599226058560 spec.py:321] Evaluating on the training split.
I0216 01:41:27.335347 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 01:42:17.984355 140599226058560 spec.py:349] Evaluating on the test split.
I0216 01:42:43.508463 140599226058560 submission_runner.py:408] Time since start: 34697.79s, 	Step: 40185, 	{'train/ctc_loss': Array(0.16687053, dtype=float32), 'train/wer': 0.06488479758828596, 'validation/ctc_loss': Array(0.43387946, dtype=float32), 'validation/wer': 0.13033781631057087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25024888, dtype=float32), 'test/wer': 0.08461804074502874, 'test/num_examples': 2472, 'score': 31722.822843551636, 'total_duration': 34697.79404234886, 'accumulated_submission_time': 31722.822843551636, 'accumulated_eval_time': 2972.2214760780334, 'accumulated_logging_time': 1.150517463684082}
I0216 01:42:43.547747 140532797925120 logging_writer.py:48] [40185] accumulated_eval_time=2972.221476, accumulated_logging_time=1.150517, accumulated_submission_time=31722.822844, global_step=40185, preemption_count=0, score=31722.822844, test/ctc_loss=0.25024887919425964, test/num_examples=2472, test/wer=0.084618, total_duration=34697.794042, train/ctc_loss=0.16687053442001343, train/wer=0.064885, validation/ctc_loss=0.43387946486473083, validation/num_examples=5348, validation/wer=0.130338
I0216 01:42:55.599253 140532789532416 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7097951769828796, loss=1.1878269910812378
I0216 01:44:10.633807 140532797925120 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.7323599457740784, loss=1.1798603534698486
I0216 01:45:25.897884 140532789532416 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.7949317693710327, loss=1.216020941734314
I0216 01:46:41.419676 140532797925120 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.7668145895004272, loss=1.1738437414169312
I0216 01:47:56.706064 140532789532416 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.7018242478370667, loss=1.1937930583953857
I0216 01:49:12.023828 140532797925120 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.673105776309967, loss=1.167778491973877
I0216 01:50:27.216108 140532789532416 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.695106029510498, loss=1.1862215995788574
I0216 01:51:43.775610 140532797925120 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9222872853279114, loss=1.250593662261963
I0216 01:53:05.962498 140532789532416 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.727272629737854, loss=1.2200337648391724
I0216 01:54:28.210129 140532797925120 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.9034264087677002, loss=1.2104403972625732
I0216 01:55:54.504340 140532797925120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.8685030341148376, loss=1.1769726276397705
I0216 01:57:09.474651 140532789532416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.701162576675415, loss=1.1247183084487915
I0216 01:58:24.674360 140532797925120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.716794490814209, loss=1.1281203031539917
I0216 01:59:39.923110 140532789532416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7327502965927124, loss=1.238290548324585
I0216 02:00:55.465051 140532797925120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.7889625430107117, loss=1.2062872648239136
I0216 02:02:10.710974 140532789532416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9314224720001221, loss=1.181788682937622
I0216 02:03:29.896216 140532797925120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.7645185589790344, loss=1.1743199825286865
I0216 02:04:52.917313 140532789532416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.833369791507721, loss=1.2374855279922485
I0216 02:06:14.457650 140532797925120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.808928906917572, loss=1.189024567604065
I0216 02:06:43.505043 140599226058560 spec.py:321] Evaluating on the training split.
I0216 02:07:37.914548 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 02:08:28.666108 140599226058560 spec.py:349] Evaluating on the test split.
I0216 02:08:54.204948 140599226058560 submission_runner.py:408] Time since start: 36268.49s, 	Step: 42037, 	{'train/ctc_loss': Array(0.17537016, dtype=float32), 'train/wer': 0.06591017942010831, 'validation/ctc_loss': Array(0.42432463, dtype=float32), 'validation/wer': 0.12730625524971761, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24411249, dtype=float32), 'test/wer': 0.08364308492271444, 'test/num_examples': 2472, 'score': 33162.69558787346, 'total_duration': 36268.490728616714, 'accumulated_submission_time': 33162.69558787346, 'accumulated_eval_time': 3102.9151356220245, 'accumulated_logging_time': 1.2059061527252197}
I0216 02:08:54.248397 140532797925120 logging_writer.py:48] [42037] accumulated_eval_time=3102.915136, accumulated_logging_time=1.205906, accumulated_submission_time=33162.695588, global_step=42037, preemption_count=0, score=33162.695588, test/ctc_loss=0.24411249160766602, test/num_examples=2472, test/wer=0.083643, total_duration=36268.490729, train/ctc_loss=0.17537015676498413, train/wer=0.065910, validation/ctc_loss=0.424324631690979, validation/num_examples=5348, validation/wer=0.127306
I0216 02:09:42.336338 140532789532416 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.0070148706436157, loss=1.1849584579467773
I0216 02:10:57.521087 140532797925120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.7053371071815491, loss=1.1907278299331665
I0216 02:12:16.458554 140532797925120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7859992980957031, loss=1.1883742809295654
I0216 02:13:31.838717 140532789532416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7556803822517395, loss=1.14350163936615
I0216 02:14:47.245218 140532797925120 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.6877216696739197, loss=1.1481817960739136
I0216 02:16:02.707809 140532789532416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.7865188121795654, loss=1.216631531715393
I0216 02:17:18.125022 140532797925120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7000887393951416, loss=1.1372851133346558
I0216 02:18:38.562210 140532789532416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.7181674242019653, loss=1.1949704885482788
I0216 02:20:00.402714 140532797925120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.8525364995002747, loss=1.202138900756836
I0216 02:21:24.146276 140532789532416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.7098121643066406, loss=1.2612828016281128
I0216 02:22:46.704729 140532797925120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6457502245903015, loss=1.217032790184021
I0216 02:24:08.936421 140532789532416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.7588104009628296, loss=1.190527081489563
I0216 02:25:32.043931 140532797925120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.8804486393928528, loss=1.17178213596344
I0216 02:26:47.319800 140532789532416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.8239336013793945, loss=1.195944905281067
I0216 02:28:02.620833 140532797925120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7456179857254028, loss=1.1906334161758423
I0216 02:29:17.952663 140532789532416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.758123517036438, loss=1.0972645282745361
I0216 02:30:33.179305 140532797925120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.8032774329185486, loss=1.1571720838546753
I0216 02:31:48.567203 140532789532416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6748735308647156, loss=1.1093103885650635
I0216 02:32:54.655604 140599226058560 spec.py:321] Evaluating on the training split.
I0216 02:33:48.669416 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 02:34:39.174505 140599226058560 spec.py:349] Evaluating on the test split.
I0216 02:35:04.772955 140599226058560 submission_runner.py:408] Time since start: 37839.06s, 	Step: 43882, 	{'train/ctc_loss': Array(0.17138386, dtype=float32), 'train/wer': 0.06454534444719424, 'validation/ctc_loss': Array(0.4157252, dtype=float32), 'validation/wer': 0.124429168637825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23707676, dtype=float32), 'test/wer': 0.08059634797798225, 'test/num_examples': 2472, 'score': 34603.0143558979, 'total_duration': 37839.05934667587, 'accumulated_submission_time': 34603.0143558979, 'accumulated_eval_time': 3233.0268499851227, 'accumulated_logging_time': 1.269730806350708}
I0216 02:35:04.812159 140532797925120 logging_writer.py:48] [43882] accumulated_eval_time=3233.026850, accumulated_logging_time=1.269731, accumulated_submission_time=34603.014356, global_step=43882, preemption_count=0, score=34603.014356, test/ctc_loss=0.2370767593383789, test/num_examples=2472, test/wer=0.080596, total_duration=37839.059347, train/ctc_loss=0.17138385772705078, train/wer=0.064545, validation/ctc_loss=0.4157252013683319, validation/num_examples=5348, validation/wer=0.124429
I0216 02:35:19.146876 140532789532416 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.8442450165748596, loss=1.2281441688537598
I0216 02:36:34.238055 140532797925120 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.7010310292243958, loss=1.2012983560562134
I0216 02:37:49.554407 140532789532416 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.7470638751983643, loss=1.1653138399124146
I0216 02:39:04.850820 140532797925120 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.6302229762077332, loss=1.0798579454421997
I0216 02:40:24.990542 140532797925120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.686842143535614, loss=1.144929051399231
I0216 02:41:40.206410 140532789532416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.6798909306526184, loss=1.131635308265686
I0216 02:42:55.450426 140532797925120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7555591464042664, loss=1.1540319919586182
I0216 02:44:10.660487 140532789532416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.8769821524620056, loss=1.1590430736541748
I0216 02:45:25.952785 140532797925120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.7546330094337463, loss=1.134704828262329
I0216 02:46:41.254342 140532789532416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.7618481516838074, loss=1.224660873413086
I0216 02:48:01.921297 140532797925120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6705161333084106, loss=1.141011357307434
I0216 02:49:24.090275 140532789532416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.7770295143127441, loss=1.1927205324172974
I0216 02:50:46.024406 140532797925120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.854422390460968, loss=1.18956458568573
I0216 02:52:08.784320 140532789532416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.8377085328102112, loss=1.1827033758163452
I0216 02:53:31.411617 140532797925120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.6451027989387512, loss=1.0889339447021484
I0216 02:54:51.163728 140532797925120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.7040213346481323, loss=1.1459161043167114
I0216 02:56:06.391685 140532789532416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6826137900352478, loss=1.1701738834381104
I0216 02:57:21.602852 140532797925120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.707440972328186, loss=1.10734224319458
I0216 02:58:36.849426 140532789532416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6290699243545532, loss=1.1261177062988281
I0216 02:59:05.121854 140599226058560 spec.py:321] Evaluating on the training split.
I0216 02:59:59.585055 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 03:00:49.480375 140599226058560 spec.py:349] Evaluating on the test split.
I0216 03:01:15.517177 140599226058560 submission_runner.py:408] Time since start: 39409.80s, 	Step: 45739, 	{'train/ctc_loss': Array(0.15769911, dtype=float32), 'train/wer': 0.06066654914607464, 'validation/ctc_loss': Array(0.4071435, dtype=float32), 'validation/wer': 0.122372727536036, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2356796, dtype=float32), 'test/wer': 0.08029167428350903, 'test/num_examples': 2472, 'score': 36043.24026441574, 'total_duration': 39409.80263566971, 'accumulated_submission_time': 36043.24026441574, 'accumulated_eval_time': 3363.4156663417816, 'accumulated_logging_time': 1.3258774280548096}
I0216 03:01:15.565397 140532797925120 logging_writer.py:48] [45739] accumulated_eval_time=3363.415666, accumulated_logging_time=1.325877, accumulated_submission_time=36043.240264, global_step=45739, preemption_count=0, score=36043.240264, test/ctc_loss=0.23567959666252136, test/num_examples=2472, test/wer=0.080292, total_duration=39409.802636, train/ctc_loss=0.1576991081237793, train/wer=0.060667, validation/ctc_loss=0.4071435034275055, validation/num_examples=5348, validation/wer=0.122373
I0216 03:02:02.051526 140532789532416 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.8326336145401001, loss=1.1601769924163818
I0216 03:03:17.095303 140532797925120 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7924003005027771, loss=1.1317628622055054
I0216 03:04:32.292098 140532789532416 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.7678777575492859, loss=1.198953628540039
I0216 03:05:47.451173 140532797925120 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7402618527412415, loss=1.1619141101837158
I0216 03:07:03.968411 140532789532416 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7890444397926331, loss=1.1370224952697754
I0216 03:08:26.106545 140532797925120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.7339253425598145, loss=1.1431044340133667
I0216 03:09:48.468135 140532797925120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.8290808796882629, loss=1.101771354675293
I0216 03:11:03.810774 140532789532416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8274173736572266, loss=1.2022563219070435
I0216 03:12:19.079882 140532797925120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7893027067184448, loss=1.1660618782043457
I0216 03:13:34.355682 140532789532416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7926067113876343, loss=1.1309418678283691
I0216 03:14:49.574367 140532797925120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8398905396461487, loss=1.1581319570541382
I0216 03:16:07.749224 140532789532416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8498696684837341, loss=1.100508213043213
I0216 03:17:30.521983 140532797925120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.8648887276649475, loss=1.1204345226287842
I0216 03:18:52.176266 140532789532416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.7116703391075134, loss=1.1218647956848145
I0216 03:20:14.221732 140532797925120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.7671474814414978, loss=1.1014207601547241
I0216 03:21:35.893435 140532789532416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.7087501287460327, loss=1.1367846727371216
I0216 03:23:00.248655 140532797925120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7021981477737427, loss=1.144234299659729
I0216 03:24:15.335141 140532789532416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.7399302124977112, loss=1.1261355876922607
I0216 03:25:15.785783 140599226058560 spec.py:321] Evaluating on the training split.
I0216 03:26:10.584562 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 03:27:01.431495 140599226058560 spec.py:349] Evaluating on the test split.
I0216 03:27:27.415982 140599226058560 submission_runner.py:408] Time since start: 40981.70s, 	Step: 47582, 	{'train/ctc_loss': Array(0.15129596, dtype=float32), 'train/wer': 0.057782668328526805, 'validation/ctc_loss': Array(0.40430495, dtype=float32), 'validation/wer': 0.11943771300578314, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22712962, dtype=float32), 'test/wer': 0.07708244470172446, 'test/num_examples': 2472, 'score': 37483.37846851349, 'total_duration': 40981.70229816437, 'accumulated_submission_time': 37483.37846851349, 'accumulated_eval_time': 3495.040193796158, 'accumulated_logging_time': 1.3885679244995117}
I0216 03:27:27.460059 140532797925120 logging_writer.py:48] [47582] accumulated_eval_time=3495.040194, accumulated_logging_time=1.388568, accumulated_submission_time=37483.378469, global_step=47582, preemption_count=0, score=37483.378469, test/ctc_loss=0.22712962329387665, test/num_examples=2472, test/wer=0.077082, total_duration=40981.702298, train/ctc_loss=0.1512959599494934, train/wer=0.057783, validation/ctc_loss=0.40430495142936707, validation/num_examples=5348, validation/wer=0.119438
I0216 03:27:41.833756 140532789532416 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.8674894571304321, loss=1.1112860441207886
I0216 03:28:57.003841 140532797925120 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.7969544529914856, loss=1.0960612297058105
I0216 03:30:12.420503 140532789532416 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.6892994046211243, loss=1.1151739358901978
I0216 03:31:27.867465 140532797925120 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.8055616617202759, loss=1.1616688966751099
I0216 03:32:43.294704 140532789532416 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7621940970420837, loss=1.1729243993759155
I0216 03:34:00.528493 140532797925120 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.751150369644165, loss=1.1386518478393555
I0216 03:35:23.691903 140532789532416 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.8155784606933594, loss=1.1057933568954468
I0216 03:36:45.443688 140532797925120 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.7073877453804016, loss=1.0954768657684326
I0216 03:38:08.271014 140532789532416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8433637022972107, loss=1.125648856163025
I0216 03:39:27.862522 140532797925120 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0536139011383057, loss=1.0971782207489014
I0216 03:40:43.186257 140532789532416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.8680709004402161, loss=1.0936517715454102
I0216 03:41:58.664951 140532797925120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.7698705196380615, loss=1.1333616971969604
I0216 03:43:14.205013 140532789532416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.9233009219169617, loss=1.1301742792129517
I0216 03:44:29.647948 140532797925120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8516311049461365, loss=1.1576707363128662
I0216 03:45:51.432435 140532789532416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8992992639541626, loss=1.127098798751831
I0216 03:47:13.324181 140532797925120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.7336140871047974, loss=1.103590965270996
I0216 03:48:37.234792 140532789532416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.9131273031234741, loss=1.1373767852783203
I0216 03:49:59.158560 140532797925120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.816175103187561, loss=1.1311885118484497
I0216 03:51:21.908941 140532789532416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.7585374712944031, loss=1.1436562538146973
I0216 03:51:27.452756 140599226058560 spec.py:321] Evaluating on the training split.
I0216 03:52:22.086933 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 03:53:12.918928 140599226058560 spec.py:349] Evaluating on the test split.
I0216 03:53:39.163103 140599226058560 submission_runner.py:408] Time since start: 42553.45s, 	Step: 49408, 	{'train/ctc_loss': Array(0.13115764, dtype=float32), 'train/wer': 0.0500343243385964, 'validation/ctc_loss': Array(0.39019933, dtype=float32), 'validation/wer': 0.11601996582252817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21716937, dtype=float32), 'test/wer': 0.07543720675156906, 'test/num_examples': 2472, 'score': 38923.28881287575, 'total_duration': 42553.449315071106, 'accumulated_submission_time': 38923.28881287575, 'accumulated_eval_time': 3626.74472117424, 'accumulated_logging_time': 1.4479291439056396}
I0216 03:53:39.201725 140532797925120 logging_writer.py:48] [49408] accumulated_eval_time=3626.744721, accumulated_logging_time=1.447929, accumulated_submission_time=38923.288813, global_step=49408, preemption_count=0, score=38923.288813, test/ctc_loss=0.2171693742275238, test/num_examples=2472, test/wer=0.075437, total_duration=42553.449315, train/ctc_loss=0.13115763664245605, train/wer=0.050034, validation/ctc_loss=0.39019933342933655, validation/num_examples=5348, validation/wer=0.116020
I0216 03:54:52.856781 140532797925120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.7236404418945312, loss=1.0482518672943115
I0216 03:56:08.216701 140532789532416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9489737153053284, loss=1.0803850889205933
I0216 03:57:23.455768 140532797925120 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0585020780563354, loss=1.1035168170928955
I0216 03:58:38.664227 140532789532416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.8594698309898376, loss=1.0726641416549683
I0216 03:59:54.137967 140532797925120 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.8696627020835876, loss=1.1179206371307373
I0216 04:01:16.160793 140532789532416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.7718337178230286, loss=1.1570159196853638
I0216 04:02:38.436890 140532797925120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8616586923599243, loss=1.1155402660369873
I0216 04:04:02.212215 140532789532416 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0813943147659302, loss=1.1319200992584229
I0216 04:05:24.614760 140532797925120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7541231513023376, loss=1.121620535850525
I0216 04:06:48.181843 140532789532416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.8559849858283997, loss=1.1193972826004028
I0216 04:08:13.144707 140532797925120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.8440669775009155, loss=1.1095253229141235
I0216 04:09:28.273077 140532789532416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.740926206111908, loss=1.0480098724365234
I0216 04:10:43.450901 140532797925120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.8361624479293823, loss=1.1327217817306519
I0216 04:11:58.913447 140532789532416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.7950044870376587, loss=1.0725523233413696
I0216 04:13:14.108972 140532797925120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.9570944905281067, loss=1.0825769901275635
I0216 04:14:32.481317 140532789532416 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.8242319822311401, loss=1.076516032218933
I0216 04:15:55.537253 140532797925120 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.0020289421081543, loss=1.1400606632232666
I0216 04:17:18.557148 140532789532416 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8284463286399841, loss=1.0937364101409912
I0216 04:17:39.778103 140599226058560 spec.py:321] Evaluating on the training split.
I0216 04:18:35.805445 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 04:19:26.713607 140599226058560 spec.py:349] Evaluating on the test split.
I0216 04:19:52.347688 140599226058560 submission_runner.py:408] Time since start: 44126.63s, 	Step: 51227, 	{'train/ctc_loss': Array(0.13253723, dtype=float32), 'train/wer': 0.050379519241574466, 'validation/ctc_loss': Array(0.3822779, dtype=float32), 'validation/wer': 0.11255394537397298, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21561463, dtype=float32), 'test/wer': 0.07249202770499462, 'test/num_examples': 2472, 'score': 40363.78140926361, 'total_duration': 44126.6337416172, 'accumulated_submission_time': 40363.78140926361, 'accumulated_eval_time': 3759.30832862854, 'accumulated_logging_time': 1.5018019676208496}
I0216 04:19:52.395084 140532797925120 logging_writer.py:48] [51227] accumulated_eval_time=3759.308329, accumulated_logging_time=1.501802, accumulated_submission_time=40363.781409, global_step=51227, preemption_count=0, score=40363.781409, test/ctc_loss=0.21561463177204132, test/num_examples=2472, test/wer=0.072492, total_duration=44126.633742, train/ctc_loss=0.13253723084926605, train/wer=0.050380, validation/ctc_loss=0.3822779059410095, validation/num_examples=5348, validation/wer=0.112554
I0216 04:20:47.949429 140532789532416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.7629043459892273, loss=1.1291005611419678
I0216 04:22:03.217969 140532797925120 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.8293952941894531, loss=1.053766131401062
I0216 04:23:22.173030 140532797925120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.7628893852233887, loss=1.1048189401626587
I0216 04:24:37.300345 140532789532416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.8199494481086731, loss=1.1069079637527466
I0216 04:25:52.597202 140532797925120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.8028125166893005, loss=1.0436493158340454
I0216 04:27:07.823467 140532789532416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.9251728653907776, loss=1.0908924341201782
I0216 04:28:23.305863 140532797925120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.9763925075531006, loss=1.069142460823059
I0216 04:29:39.215427 140532789532416 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8401334881782532, loss=1.1288138628005981
I0216 04:31:01.617266 140532797925120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.935313880443573, loss=1.0997990369796753
I0216 04:32:24.070904 140532789532416 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.168973445892334, loss=1.1214359998703003
I0216 04:33:46.521090 140532797925120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.7967154383659363, loss=1.100723147392273
I0216 04:35:10.586258 140532789532416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.9063021540641785, loss=1.1319149732589722
I0216 04:36:32.938509 140532797925120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.750883936882019, loss=1.1047145128250122
I0216 04:37:53.588775 140532797925120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.9050777554512024, loss=1.1053335666656494
I0216 04:39:08.987793 140532789532416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.8245502710342407, loss=1.0463229417800903
I0216 04:40:24.334904 140532797925120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.7533506751060486, loss=1.0861061811447144
I0216 04:41:39.613261 140532789532416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.9129347205162048, loss=1.0830721855163574
I0216 04:42:55.003070 140532797925120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.789643406867981, loss=1.0934696197509766
I0216 04:43:52.584948 140599226058560 spec.py:321] Evaluating on the training split.
I0216 04:44:46.523276 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 04:45:37.263010 140599226058560 spec.py:349] Evaluating on the test split.
I0216 04:46:03.040936 140599226058560 submission_runner.py:408] Time since start: 45697.33s, 	Step: 53073, 	{'train/ctc_loss': Array(0.13393289, dtype=float32), 'train/wer': 0.0510575079009211, 'validation/ctc_loss': Array(0.3740585, dtype=float32), 'validation/wer': 0.11190708361895016, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20504084, dtype=float32), 'test/wer': 0.07046086974183982, 'test/num_examples': 2472, 'score': 41803.87991976738, 'total_duration': 45697.32681298256, 'accumulated_submission_time': 41803.87991976738, 'accumulated_eval_time': 3889.758169412613, 'accumulated_logging_time': 1.5712800025939941}
I0216 04:46:03.082201 140532797925120 logging_writer.py:48] [53073] accumulated_eval_time=3889.758169, accumulated_logging_time=1.571280, accumulated_submission_time=41803.879920, global_step=53073, preemption_count=0, score=41803.879920, test/ctc_loss=0.205040842294693, test/num_examples=2472, test/wer=0.070461, total_duration=45697.326813, train/ctc_loss=0.13393288850784302, train/wer=0.051058, validation/ctc_loss=0.3740585148334503, validation/num_examples=5348, validation/wer=0.111907
I0216 04:46:24.101338 140532789532416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.851845920085907, loss=1.1348602771759033
I0216 04:47:39.274098 140532797925120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.888059675693512, loss=1.1154054403305054
I0216 04:48:54.485041 140532789532416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8829305768013, loss=1.1029993295669556
I0216 04:50:09.756273 140532797925120 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8154760599136353, loss=1.0637483596801758
I0216 04:51:26.873484 140532789532416 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.1755073070526123, loss=1.1001545190811157
I0216 04:52:50.033699 140532797925120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8457656502723694, loss=1.0163809061050415
I0216 04:54:05.249520 140532789532416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.8384094834327698, loss=1.0724942684173584
I0216 04:55:20.497161 140532797925120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.7697382569313049, loss=1.097374677658081
I0216 04:56:35.757841 140532789532416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.7221665978431702, loss=1.0239866971969604
I0216 04:57:51.002812 140532797925120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9792921543121338, loss=0.9988080859184265
I0216 04:59:10.940608 140532789532416 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.0098261833190918, loss=1.087295651435852
I0216 05:00:33.456106 140532797925120 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.1486449241638184, loss=1.0513570308685303
I0216 05:01:56.248759 140532789532416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.838538646697998, loss=1.0545166730880737
I0216 05:03:19.509487 140532797925120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7694596648216248, loss=1.0562326908111572
I0216 05:04:42.444229 140532789532416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.9058942794799805, loss=1.0406700372695923
I0216 05:06:08.678505 140532797925120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.7898280620574951, loss=1.0190421342849731
I0216 05:07:23.849336 140532789532416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.8194951415061951, loss=1.0559266805648804
I0216 05:08:39.066493 140532797925120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.8453532457351685, loss=1.0537397861480713
I0216 05:09:54.303299 140532789532416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.9212629199028015, loss=1.0641095638275146
I0216 05:10:03.046842 140599226058560 spec.py:321] Evaluating on the training split.
I0216 05:10:57.270627 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 05:11:48.107666 140599226058560 spec.py:349] Evaluating on the test split.
I0216 05:12:13.577806 140599226058560 submission_runner.py:408] Time since start: 47267.86s, 	Step: 54913, 	{'train/ctc_loss': Array(0.11381011, dtype=float32), 'train/wer': 0.04320911095892627, 'validation/ctc_loss': Array(0.36113426, dtype=float32), 'validation/wer': 0.10776523745619201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20187275, dtype=float32), 'test/wer': 0.06686572014705584, 'test/num_examples': 2472, 'score': 43243.759679079056, 'total_duration': 47267.8637509346, 'accumulated_submission_time': 43243.759679079056, 'accumulated_eval_time': 4020.283055782318, 'accumulated_logging_time': 1.6276521682739258}
I0216 05:12:13.620685 140532797925120 logging_writer.py:48] [54913] accumulated_eval_time=4020.283056, accumulated_logging_time=1.627652, accumulated_submission_time=43243.759679, global_step=54913, preemption_count=0, score=43243.759679, test/ctc_loss=0.20187275111675262, test/num_examples=2472, test/wer=0.066866, total_duration=47267.863751, train/ctc_loss=0.11381011456251144, train/wer=0.043209, validation/ctc_loss=0.36113426089286804, validation/num_examples=5348, validation/wer=0.107765
I0216 05:13:19.570229 140532789532416 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.8163352012634277, loss=1.0434298515319824
I0216 05:14:34.763404 140532797925120 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.9571596384048462, loss=1.0744624137878418
I0216 05:15:49.976903 140532789532416 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.026910424232483, loss=1.0603089332580566
I0216 05:17:05.176853 140532797925120 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.9230635166168213, loss=1.0771465301513672
I0216 05:18:27.981751 140532789532416 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.9336215257644653, loss=1.1497578620910645
I0216 05:19:50.386279 140532797925120 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.8309193849563599, loss=1.0268925428390503
I0216 05:21:13.257697 140532789532416 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.8354127407073975, loss=1.021715521812439
I0216 05:22:33.253424 140532797925120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.8410050868988037, loss=1.0561333894729614
I0216 05:23:48.437887 140532789532416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.9999558925628662, loss=1.0418479442596436
I0216 05:25:03.745964 140532797925120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.8716398477554321, loss=1.0290226936340332
I0216 05:26:19.078735 140532789532416 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.2817633152008057, loss=1.07883882522583
I0216 05:27:34.423371 140532797925120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.957358717918396, loss=1.0263316631317139
I0216 05:28:54.719410 140532789532416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9914194345474243, loss=1.0372768640518188
I0216 05:30:17.266471 140532797925120 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.8842878341674805, loss=1.0543348789215088
I0216 05:31:40.177149 140532789532416 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.8974194526672363, loss=1.06157386302948
I0216 05:33:03.034386 140532797925120 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9547279477119446, loss=1.0483111143112183
I0216 05:34:25.687843 140532789532416 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.983610987663269, loss=1.0118565559387207
I0216 05:35:47.893429 140532797925120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.8791120052337646, loss=1.0793566703796387
I0216 05:36:13.861680 140599226058560 spec.py:321] Evaluating on the training split.
I0216 05:37:08.607259 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 05:37:58.872605 140599226058560 spec.py:349] Evaluating on the test split.
I0216 05:38:24.679551 140599226058560 submission_runner.py:408] Time since start: 48838.97s, 	Step: 56736, 	{'train/ctc_loss': Array(0.11524229, dtype=float32), 'train/wer': 0.04620441481158382, 'validation/ctc_loss': Array(0.35849932, dtype=float32), 'validation/wer': 0.10443438215047743, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19632728, dtype=float32), 'test/wer': 0.06710945910263441, 'test/num_examples': 2472, 'score': 44683.91825604439, 'total_duration': 48838.96536445618, 'accumulated_submission_time': 44683.91825604439, 'accumulated_eval_time': 4151.09471654892, 'accumulated_logging_time': 1.6856026649475098}
I0216 05:38:24.720858 140532797925120 logging_writer.py:48] [56736] accumulated_eval_time=4151.094717, accumulated_logging_time=1.685603, accumulated_submission_time=44683.918256, global_step=56736, preemption_count=0, score=44683.918256, test/ctc_loss=0.19632728397846222, test/num_examples=2472, test/wer=0.067109, total_duration=48838.965364, train/ctc_loss=0.11524228751659393, train/wer=0.046204, validation/ctc_loss=0.35849931836128235, validation/num_examples=5348, validation/wer=0.104434
I0216 05:39:13.443312 140532789532416 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.0460877418518066, loss=0.9497643709182739
I0216 05:40:28.621043 140532797925120 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.245928168296814, loss=1.0843195915222168
I0216 05:41:43.888149 140532789532416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.9033112525939941, loss=1.0408895015716553
I0216 05:42:59.061077 140532797925120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.8891807794570923, loss=1.0887768268585205
I0216 05:44:14.308177 140532789532416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9439234733581543, loss=1.0330610275268555
I0216 05:45:29.598207 140532797925120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.8188982605934143, loss=1.0132107734680176
I0216 05:46:50.264705 140532789532416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.8907104730606079, loss=1.0627816915512085
I0216 05:48:13.839398 140532797925120 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.9548018574714661, loss=1.0382236242294312
I0216 05:49:35.990548 140532789532416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.900812029838562, loss=1.0772347450256348
I0216 05:51:00.725506 140532797925120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.8819143176078796, loss=1.037095308303833
I0216 05:52:15.738572 140532789532416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.8730300664901733, loss=1.0349045991897583
I0216 05:53:30.795089 140532797925120 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.5813989639282227, loss=1.0250335931777954
I0216 05:54:45.895491 140532789532416 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0343965291976929, loss=0.9880966544151306
I0216 05:56:01.180654 140532797925120 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.1861075162887573, loss=1.0572620630264282
I0216 05:57:17.394491 140532789532416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.9753449559211731, loss=1.0011152029037476
I0216 05:58:40.387173 140532797925120 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.1635559797286987, loss=1.0016385316848755
I0216 06:00:03.424325 140532789532416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9404115676879883, loss=1.009535312652588
I0216 06:01:27.213916 140532797925120 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8785716891288757, loss=1.0453159809112549
I0216 06:02:24.675601 140599226058560 spec.py:321] Evaluating on the training split.
I0216 06:03:18.688875 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 06:04:09.382065 140599226058560 spec.py:349] Evaluating on the test split.
I0216 06:04:35.174030 140599226058560 submission_runner.py:408] Time since start: 50409.46s, 	Step: 58571, 	{'train/ctc_loss': Array(0.13256885, dtype=float32), 'train/wer': 0.04735210113979055, 'validation/ctc_loss': Array(0.35000604, dtype=float32), 'validation/wer': 0.10222346659972774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1881308, dtype=float32), 'test/wer': 0.06371742530416591, 'test/num_examples': 2472, 'score': 46123.78596878052, 'total_duration': 50409.45980882645, 'accumulated_submission_time': 46123.78596878052, 'accumulated_eval_time': 4281.586913585663, 'accumulated_logging_time': 1.746351718902588}
I0216 06:04:35.217565 140532797925120 logging_writer.py:48] [58571] accumulated_eval_time=4281.586914, accumulated_logging_time=1.746352, accumulated_submission_time=46123.785969, global_step=58571, preemption_count=0, score=46123.785969, test/ctc_loss=0.18813079595565796, test/num_examples=2472, test/wer=0.063717, total_duration=50409.459809, train/ctc_loss=0.1325688511133194, train/wer=0.047352, validation/ctc_loss=0.3500060439109802, validation/num_examples=5348, validation/wer=0.102223
I0216 06:04:57.758167 140532789532416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.976097583770752, loss=1.0085783004760742
I0216 06:06:12.837426 140532797925120 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.1388837099075317, loss=1.0477389097213745
I0216 06:07:31.923544 140532797925120 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1415880918502808, loss=1.0323972702026367
I0216 06:08:47.141057 140532789532416 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.8359048962593079, loss=0.9650664925575256
I0216 06:10:02.538011 140532797925120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9332624673843384, loss=1.0104984045028687
I0216 06:11:17.789178 140532789532416 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.9517513513565063, loss=1.0049210786819458
I0216 06:12:35.270560 140532797925120 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.0268638134002686, loss=1.024312973022461
I0216 06:13:58.797132 140532789532416 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.16147780418396, loss=1.0212512016296387
I0216 06:15:21.541645 140532797925120 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.1766228675842285, loss=0.9824215769767761
I0216 06:16:43.658676 140532789532416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.8859318494796753, loss=1.0106755495071411
I0216 06:18:06.129963 140532797925120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9763505458831787, loss=1.0202089548110962
I0216 06:19:29.054544 140532789532416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.9604483842849731, loss=0.9722127914428711
I0216 06:20:50.627123 140532797925120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.9585392475128174, loss=0.9481418132781982
I0216 06:22:06.003432 140532789532416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.9970775842666626, loss=1.0190635919570923
I0216 06:23:21.246700 140532797925120 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.0464117527008057, loss=0.9691196084022522
I0216 06:24:36.585428 140532789532416 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.1217236518859863, loss=1.0314370393753052
I0216 06:25:51.832356 140532797925120 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.8905594944953918, loss=0.9767615795135498
I0216 06:27:10.633295 140532789532416 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.237844467163086, loss=1.0268162488937378
I0216 06:28:33.819810 140532797925120 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.9489796757698059, loss=0.9552942514419556
I0216 06:28:35.803088 140599226058560 spec.py:321] Evaluating on the training split.
I0216 06:29:30.868506 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 06:30:21.838201 140599226058560 spec.py:349] Evaluating on the test split.
I0216 06:30:47.265294 140599226058560 submission_runner.py:408] Time since start: 51981.55s, 	Step: 60404, 	{'train/ctc_loss': Array(0.08553199, dtype=float32), 'train/wer': 0.033242987967639935, 'validation/ctc_loss': Array(0.34243563, dtype=float32), 'validation/wer': 0.0999546231306178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18706349, dtype=float32), 'test/wer': 0.06268153474295696, 'test/num_examples': 2472, 'score': 47564.2847366333, 'total_duration': 51981.551478385925, 'accumulated_submission_time': 47564.2847366333, 'accumulated_eval_time': 4413.043275594711, 'accumulated_logging_time': 1.8080739974975586}
I0216 06:30:47.311823 140532797925120 logging_writer.py:48] [60404] accumulated_eval_time=4413.043276, accumulated_logging_time=1.808074, accumulated_submission_time=47564.284737, global_step=60404, preemption_count=0, score=47564.284737, test/ctc_loss=0.18706348538398743, test/num_examples=2472, test/wer=0.062682, total_duration=51981.551478, train/ctc_loss=0.08553198724985123, train/wer=0.033243, validation/ctc_loss=0.3424356281757355, validation/num_examples=5348, validation/wer=0.099955
I0216 06:32:00.060407 140532789532416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9115344285964966, loss=1.0114623308181763
I0216 06:33:15.390740 140532797925120 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9280890226364136, loss=0.9700241088867188
I0216 06:34:30.901410 140532789532416 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9283953309059143, loss=0.9684212803840637
I0216 06:35:49.957448 140532797925120 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.0583546161651611, loss=1.0169787406921387
I0216 06:37:05.131731 140532789532416 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0197166204452515, loss=0.9660918712615967
I0216 06:38:20.330281 140532797925120 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.9433969259262085, loss=0.9505972266197205
I0216 06:39:35.801848 140532789532416 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2596466541290283, loss=1.0054750442504883
I0216 06:40:51.035308 140532797925120 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.0703011751174927, loss=0.9804680347442627
I0216 06:42:08.027142 140532789532416 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.1645575761795044, loss=0.9457173943519592
I0216 06:43:30.753677 140532797925120 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.0194051265716553, loss=1.0298447608947754
I0216 06:44:54.519240 140532789532416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.9107357859611511, loss=0.9789804816246033
I0216 06:46:17.387906 140532797925120 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.0019888877868652, loss=1.0185966491699219
I0216 06:47:40.701087 140532789532416 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.0943739414215088, loss=0.9661951065063477
I0216 06:49:08.042423 140532797925120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.9875587821006775, loss=0.9748438000679016
I0216 06:50:23.354967 140532789532416 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0679130554199219, loss=0.9641621112823486
I0216 06:51:38.721904 140532797925120 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0152721405029297, loss=1.0083531141281128
I0216 06:52:54.094979 140532789532416 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.502109169960022, loss=0.9931929111480713
I0216 06:54:09.837208 140532797925120 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0386465787887573, loss=0.9903540015220642
I0216 06:54:47.903569 140599226058560 spec.py:321] Evaluating on the training split.
I0216 06:55:42.525762 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 06:56:33.387448 140599226058560 spec.py:349] Evaluating on the test split.
I0216 06:56:58.903991 140599226058560 submission_runner.py:408] Time since start: 53553.19s, 	Step: 62252, 	{'train/ctc_loss': Array(0.08559206, dtype=float32), 'train/wer': 0.03227601847613499, 'validation/ctc_loss': Array(0.33397695, dtype=float32), 'validation/wer': 0.09791749133494888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1801457, dtype=float32), 'test/wer': 0.060264456766802754, 'test/num_examples': 2472, 'score': 49004.790016412735, 'total_duration': 53553.19096279144, 'accumulated_submission_time': 49004.790016412735, 'accumulated_eval_time': 4544.038645029068, 'accumulated_logging_time': 1.8715159893035889}
I0216 06:56:58.943917 140532797925120 logging_writer.py:48] [62252] accumulated_eval_time=4544.038645, accumulated_logging_time=1.871516, accumulated_submission_time=49004.790016, global_step=62252, preemption_count=0, score=49004.790016, test/ctc_loss=0.18014569580554962, test/num_examples=2472, test/wer=0.060264, total_duration=53553.190963, train/ctc_loss=0.08559206128120422, train/wer=0.032276, validation/ctc_loss=0.33397695422172546, validation/num_examples=5348, validation/wer=0.097917
I0216 06:57:35.746464 140532789532416 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.374672293663025, loss=0.9305692911148071
I0216 06:58:50.842010 140532797925120 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.0547900199890137, loss=1.0038259029388428
I0216 07:00:06.160367 140532789532416 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.7352938652038574, loss=1.0090701580047607
I0216 07:01:21.506797 140532797925120 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.0786391496658325, loss=0.9016301035881042
I0216 07:02:41.098048 140532789532416 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.9872351884841919, loss=0.9693834781646729
I0216 07:04:03.638845 140532797925120 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.0977520942687988, loss=0.95878666639328
I0216 07:05:24.361595 140532797925120 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.162112832069397, loss=0.9262680411338806
I0216 07:06:39.464484 140532789532416 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0074669122695923, loss=0.9691117405891418
I0216 07:07:54.635434 140532797925120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.8473162055015564, loss=0.9381741881370544
I0216 07:09:09.803303 140532789532416 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.0557656288146973, loss=0.9795675873756409
I0216 07:10:24.957714 140532797925120 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.0058586597442627, loss=0.967002272605896
I0216 07:11:46.748661 140532789532416 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.0582743883132935, loss=0.9478562474250793
I0216 07:13:09.833547 140532797925120 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.1630303859710693, loss=0.9734669327735901
I0216 07:14:33.573653 140532789532416 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.130728840827942, loss=0.9670681953430176
I0216 07:15:57.115735 140532797925120 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.1291753053665161, loss=0.9652384519577026
I0216 07:17:20.665463 140532789532416 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.0240516662597656, loss=0.922065258026123
I0216 07:18:43.947839 140532797925120 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.1511914730072021, loss=0.9317499399185181
I0216 07:19:59.080620 140532789532416 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.0290888547897339, loss=0.9690492153167725
I0216 07:20:59.577315 140599226058560 spec.py:321] Evaluating on the training split.
I0216 07:21:52.492332 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 07:22:43.545003 140599226058560 spec.py:349] Evaluating on the test split.
I0216 07:23:09.378660 140599226058560 submission_runner.py:408] Time since start: 55123.66s, 	Step: 64082, 	{'train/ctc_loss': Array(0.10317326, dtype=float32), 'train/wer': 0.040281857487045776, 'validation/ctc_loss': Array(0.32366726, dtype=float32), 'validation/wer': 0.09361151607017001, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17913373, dtype=float32), 'test/wer': 0.059472305161172384, 'test/num_examples': 2472, 'score': 50445.33984518051, 'total_duration': 55123.66427946091, 'accumulated_submission_time': 50445.33984518051, 'accumulated_eval_time': 4673.833595752716, 'accumulated_logging_time': 1.9265692234039307}
I0216 07:23:09.422442 140532797925120 logging_writer.py:48] [64082] accumulated_eval_time=4673.833596, accumulated_logging_time=1.926569, accumulated_submission_time=50445.339845, global_step=64082, preemption_count=0, score=50445.339845, test/ctc_loss=0.17913372814655304, test/num_examples=2472, test/wer=0.059472, total_duration=55123.664279, train/ctc_loss=0.10317325592041016, train/wer=0.040282, validation/ctc_loss=0.3236672580242157, validation/num_examples=5348, validation/wer=0.093612
I0216 07:23:23.805581 140532789532416 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.183316946029663, loss=0.9418799877166748
I0216 07:24:39.068509 140532797925120 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1189757585525513, loss=0.9061060547828674
I0216 07:25:54.547764 140532789532416 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.9982609748840332, loss=0.9561280608177185
I0216 07:27:09.916156 140532797925120 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1400740146636963, loss=0.9522706866264343
I0216 07:28:25.536257 140532789532416 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.9864892363548279, loss=1.0044865608215332
I0216 07:29:46.291440 140532797925120 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.9516050815582275, loss=0.9418199062347412
I0216 07:31:08.933850 140532789532416 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.9536601901054382, loss=0.9712092280387878
I0216 07:32:32.696340 140532797925120 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.1457791328430176, loss=0.9666432738304138
I0216 07:33:58.086673 140532797925120 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.3647377490997314, loss=0.9851967096328735
I0216 07:35:13.103986 140532789532416 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1371723413467407, loss=0.9348710775375366
I0216 07:36:28.275924 140532797925120 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.4460985660552979, loss=0.9334673285484314
I0216 07:37:43.512383 140532789532416 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.0617318153381348, loss=0.9635226726531982
I0216 07:38:58.714573 140532797925120 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.0218371152877808, loss=0.9208070635795593
I0216 07:40:15.478082 140532789532416 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.0202350616455078, loss=0.9197850823402405
I0216 07:41:38.504387 140532797925120 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.4800647497177124, loss=0.9392403364181519
I0216 07:43:01.104144 140532789532416 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.198090672492981, loss=0.9650958776473999
I0216 07:44:23.946208 140532797925120 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.259580373764038, loss=0.9431038498878479
I0216 07:45:47.173877 140532789532416 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.9656547904014587, loss=0.9309332370758057
I0216 07:47:10.172583 140532797925120 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.077031135559082, loss=0.8902789950370789
I0216 07:47:10.182327 140599226058560 spec.py:321] Evaluating on the training split.
I0216 07:48:02.518370 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 07:48:53.415143 140599226058560 spec.py:349] Evaluating on the test split.
I0216 07:49:18.863706 140599226058560 submission_runner.py:408] Time since start: 56693.15s, 	Step: 65901, 	{'train/ctc_loss': Array(0.09804582, dtype=float32), 'train/wer': 0.03701828627401346, 'validation/ctc_loss': Array(0.31444234, dtype=float32), 'validation/wer': 0.09114957953985924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16896814, dtype=float32), 'test/wer': 0.05573497450896756, 'test/num_examples': 2472, 'score': 51886.01150846481, 'total_duration': 56693.15033197403, 'accumulated_submission_time': 51886.01150846481, 'accumulated_eval_time': 4802.509584188461, 'accumulated_logging_time': 1.9893386363983154}
I0216 07:49:18.908728 140532797925120 logging_writer.py:48] [65901] accumulated_eval_time=4802.509584, accumulated_logging_time=1.989339, accumulated_submission_time=51886.011508, global_step=65901, preemption_count=0, score=51886.011508, test/ctc_loss=0.16896814107894897, test/num_examples=2472, test/wer=0.055735, total_duration=56693.150332, train/ctc_loss=0.09804581850767136, train/wer=0.037018, validation/ctc_loss=0.31444233655929565, validation/num_examples=5348, validation/wer=0.091150
I0216 07:50:37.813399 140532797925120 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.2384190559387207, loss=0.8806406855583191
I0216 07:51:53.109847 140532789532416 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.137186884880066, loss=0.9534454941749573
I0216 07:53:08.331709 140532797925120 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0692873001098633, loss=0.9270995855331421
I0216 07:54:23.667152 140532789532416 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3278661966323853, loss=0.9419631958007812
I0216 07:55:39.752038 140532797925120 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0756200551986694, loss=0.9410213232040405
I0216 07:57:02.309496 140532789532416 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2244065999984741, loss=0.9099035263061523
I0216 07:58:25.392055 140532797925120 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1349667310714722, loss=0.9254781603813171
I0216 07:59:48.965778 140532789532416 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.0195392370224, loss=0.9526492953300476
I0216 08:01:11.620626 140532797925120 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.4031455516815186, loss=0.9218043684959412
I0216 08:02:35.217330 140532789532416 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.0377781391143799, loss=0.9409182071685791
I0216 08:03:57.335271 140532797925120 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.1166682243347168, loss=0.8890461325645447
I0216 08:05:12.524613 140532789532416 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.147533893585205, loss=0.9715521335601807
I0216 08:06:27.796447 140532797925120 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.118104338645935, loss=0.9217333793640137
I0216 08:07:43.045211 140532789532416 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1411815881729126, loss=0.9583829045295715
I0216 08:08:58.327014 140532797925120 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.222204566001892, loss=0.8777964115142822
I0216 08:10:19.495911 140532789532416 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1705889701843262, loss=0.9008272886276245
I0216 08:11:42.774070 140532797925120 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3416913747787476, loss=0.9481199979782104
I0216 08:13:06.814350 140532789532416 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.0168113708496094, loss=0.8915604948997498
I0216 08:13:19.603664 140599226058560 spec.py:321] Evaluating on the training split.
I0216 08:14:11.955144 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 08:15:03.226161 140599226058560 spec.py:349] Evaluating on the test split.
I0216 08:15:29.295258 140599226058560 submission_runner.py:408] Time since start: 58263.58s, 	Step: 67717, 	{'train/ctc_loss': Array(0.11457752, dtype=float32), 'train/wer': 0.0450217154005156, 'validation/ctc_loss': Array(0.3129322, dtype=float32), 'validation/wer': 0.09053168174401653, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1688004, dtype=float32), 'test/wer': 0.05711616192391282, 'test/num_examples': 2472, 'score': 53326.61968564987, 'total_duration': 58263.58122968674, 'accumulated_submission_time': 53326.61968564987, 'accumulated_eval_time': 4932.19512963295, 'accumulated_logging_time': 2.0536038875579834}
I0216 08:15:29.336167 140532797925120 logging_writer.py:48] [67717] accumulated_eval_time=4932.195130, accumulated_logging_time=2.053604, accumulated_submission_time=53326.619686, global_step=67717, preemption_count=0, score=53326.619686, test/ctc_loss=0.16880039870738983, test/num_examples=2472, test/wer=0.057116, total_duration=58263.581230, train/ctc_loss=0.114577516913414, train/wer=0.045022, validation/ctc_loss=0.31293219327926636, validation/num_examples=5348, validation/wer=0.090532
I0216 08:16:32.300090 140532789532416 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.0596288442611694, loss=0.8791013956069946
I0216 08:17:47.532543 140532797925120 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.0818759202957153, loss=0.9290764927864075
I0216 08:19:06.824243 140532797925120 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1153188943862915, loss=0.9203768968582153
I0216 08:20:22.140757 140532789532416 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.2112915515899658, loss=0.921403169631958
I0216 08:21:37.562200 140532797925120 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1347692012786865, loss=0.9120641350746155
I0216 08:22:53.080257 140532789532416 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.1509790420532227, loss=0.912548303604126
I0216 08:24:08.627720 140532797925120 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.1602667570114136, loss=0.893473207950592
I0216 08:25:25.087687 140532789532416 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.1746352910995483, loss=0.905464231967926
I0216 08:26:47.933483 140532797925120 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.146325945854187, loss=0.8938746452331543
I0216 08:28:10.745501 140532789532416 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.221587061882019, loss=0.8955790996551514
I0216 08:29:34.304190 140532797925120 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2181211709976196, loss=0.9106197357177734
I0216 08:30:57.066226 140532789532416 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.307096242904663, loss=0.9022797346115112
I0216 08:32:20.471463 140532797925120 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1623260974884033, loss=0.9113028049468994
I0216 08:33:40.173189 140532797925120 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.13907790184021, loss=0.9171026945114136
I0216 08:34:55.456681 140532789532416 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3227943181991577, loss=0.855448305606842
I0216 08:36:11.157556 140532797925120 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2994576692581177, loss=0.9142423868179321
I0216 08:37:26.599417 140532789532416 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1977858543395996, loss=0.8994359374046326
I0216 08:38:42.165444 140532797925120 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1126824617385864, loss=0.8601006269454956
I0216 08:39:29.618491 140599226058560 spec.py:321] Evaluating on the training split.
I0216 08:40:21.478125 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 08:41:12.552397 140599226058560 spec.py:349] Evaluating on the test split.
I0216 08:41:38.413369 140599226058560 submission_runner.py:408] Time since start: 59832.70s, 	Step: 69559, 	{'train/ctc_loss': Array(0.09059269, dtype=float32), 'train/wer': 0.03353950845356968, 'validation/ctc_loss': Array(0.30794328, dtype=float32), 'validation/wer': 0.08759666721376368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16448735, dtype=float32), 'test/wer': 0.05524749659781041, 'test/num_examples': 2472, 'score': 54766.81351041794, 'total_duration': 59832.69885110855, 'accumulated_submission_time': 54766.81351041794, 'accumulated_eval_time': 5060.983474969864, 'accumulated_logging_time': 2.1116764545440674}
I0216 08:41:38.458400 140532797925120 logging_writer.py:48] [69559] accumulated_eval_time=5060.983475, accumulated_logging_time=2.111676, accumulated_submission_time=54766.813510, global_step=69559, preemption_count=0, score=54766.813510, test/ctc_loss=0.1644873470067978, test/num_examples=2472, test/wer=0.055247, total_duration=59832.698851, train/ctc_loss=0.09059268981218338, train/wer=0.033540, validation/ctc_loss=0.30794328451156616, validation/num_examples=5348, validation/wer=0.087597
I0216 08:42:09.992792 140532789532416 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.176168441772461, loss=0.922569751739502
I0216 08:43:25.274665 140532797925120 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.275675654411316, loss=0.8722434043884277
I0216 08:44:40.645543 140532789532416 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1182141304016113, loss=0.8862846493721008
I0216 08:45:55.825594 140532797925120 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.0550090074539185, loss=0.8843253254890442
I0216 08:47:17.776055 140532789532416 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.3026127815246582, loss=0.8713504672050476
I0216 08:48:40.088992 140532797925120 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.137840747833252, loss=0.8534073829650879
I0216 08:49:55.364555 140532789532416 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.4702527523040771, loss=0.8609363436698914
I0216 08:51:11.106621 140532797925120 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1639535427093506, loss=0.8738992810249329
I0216 08:52:26.620871 140532789532416 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1389890909194946, loss=0.8640680909156799
I0216 08:53:42.821997 140532797925120 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.2618520259857178, loss=0.8974526524543762
I0216 08:55:06.347069 140532789532416 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.5104120969772339, loss=0.869544506072998
I0216 08:56:29.053537 140532797925120 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1958354711532593, loss=0.8613832592964172
I0216 08:57:52.631628 140532789532416 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.0712212324142456, loss=0.8683896064758301
I0216 08:59:16.534050 140532797925120 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.379253625869751, loss=0.897652268409729
I0216 09:00:40.568775 140532789532416 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.0938518047332764, loss=0.8914125561714172
I0216 09:02:05.053469 140532797925120 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3070380687713623, loss=0.834465503692627
I0216 09:03:20.210390 140532789532416 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1887197494506836, loss=0.8763797879219055
I0216 09:04:35.469948 140532797925120 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.088320016860962, loss=0.8686002492904663
I0216 09:05:38.500173 140599226058560 spec.py:321] Evaluating on the training split.
I0216 09:06:30.512750 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 09:07:21.704691 140599226058560 spec.py:349] Evaluating on the test split.
I0216 09:07:47.791734 140599226058560 submission_runner.py:408] Time since start: 61402.08s, 	Step: 71385, 	{'train/ctc_loss': Array(0.08755792, dtype=float32), 'train/wer': 0.03389975091275122, 'validation/ctc_loss': Array(0.30202192, dtype=float32), 'validation/wer': 0.08579124709153577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16138907, dtype=float32), 'test/wer': 0.05392724392175979, 'test/num_examples': 2472, 'score': 56206.77034330368, 'total_duration': 61402.078199625015, 'accumulated_submission_time': 56206.77034330368, 'accumulated_eval_time': 5190.269483566284, 'accumulated_logging_time': 2.172675371170044}
I0216 09:07:47.835497 140532797925120 logging_writer.py:48] [71385] accumulated_eval_time=5190.269484, accumulated_logging_time=2.172675, accumulated_submission_time=56206.770343, global_step=71385, preemption_count=0, score=56206.770343, test/ctc_loss=0.1613890677690506, test/num_examples=2472, test/wer=0.053927, total_duration=61402.078200, train/ctc_loss=0.08755791932344437, train/wer=0.033900, validation/ctc_loss=0.30202192068099976, validation/num_examples=5348, validation/wer=0.085791
I0216 09:07:59.905863 140532789532416 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.25156569480896, loss=0.8587432503700256
I0216 09:09:15.075977 140532797925120 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3935223817825317, loss=0.908646285533905
I0216 09:10:30.404453 140532789532416 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2727195024490356, loss=0.8899776339530945
I0216 09:11:45.779695 140532797925120 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.2204835414886475, loss=0.901466429233551
I0216 09:13:08.145485 140532789532416 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.1787835359573364, loss=0.9211409091949463
I0216 09:14:33.486589 140532797925120 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1593152284622192, loss=0.8986483812332153
I0216 09:15:56.847677 140532789532416 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.272308111190796, loss=0.8922418355941772
I0216 09:17:23.442735 140532797925120 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.553436040878296, loss=0.8491749167442322
I0216 09:18:38.513160 140532789532416 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.1923480033874512, loss=0.8495351076126099
I0216 09:19:53.721740 140532797925120 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.5882387161254883, loss=0.8576757907867432
I0216 09:21:08.976463 140532789532416 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.224426507949829, loss=0.9199903011322021
I0216 09:22:24.516065 140532797925120 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.0821924209594727, loss=0.8707984089851379
I0216 09:23:44.316414 140532789532416 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.266399621963501, loss=0.8847715258598328
I0216 09:25:09.570316 140532797925120 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.0946483612060547, loss=0.8607264757156372
I0216 09:26:33.569639 140532789532416 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.2613340616226196, loss=0.8628002405166626
I0216 09:27:57.557137 140532797925120 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.088653802871704, loss=0.845264732837677
I0216 09:29:20.969394 140532789532416 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.0952668190002441, loss=0.875777006149292
I0216 09:30:45.528491 140532797925120 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.1709232330322266, loss=0.8599819540977478
I0216 09:31:47.828673 140599226058560 spec.py:321] Evaluating on the training split.
I0216 09:32:42.437644 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 09:33:34.195691 140599226058560 spec.py:349] Evaluating on the test split.
I0216 09:34:00.157684 140599226058560 submission_runner.py:408] Time since start: 62974.44s, 	Step: 73176, 	{'train/ctc_loss': Array(0.06485754, dtype=float32), 'train/wer': 0.024649648976517738, 'validation/ctc_loss': Array(0.29641142, dtype=float32), 'validation/wer': 0.08471958060187107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15961748, dtype=float32), 'test/wer': 0.053155403895760976, 'test/num_examples': 2472, 'score': 57646.677629470825, 'total_duration': 62974.443323373795, 'accumulated_submission_time': 57646.677629470825, 'accumulated_eval_time': 5322.592356920242, 'accumulated_logging_time': 2.23431396484375}
I0216 09:34:00.199157 140532797925120 logging_writer.py:48] [73176] accumulated_eval_time=5322.592357, accumulated_logging_time=2.234314, accumulated_submission_time=57646.677629, global_step=73176, preemption_count=0, score=57646.677629, test/ctc_loss=0.15961748361587524, test/num_examples=2472, test/wer=0.053155, total_duration=62974.443323, train/ctc_loss=0.06485754251480103, train/wer=0.024650, validation/ctc_loss=0.2964114248752594, validation/num_examples=5348, validation/wer=0.084720
I0216 09:34:19.036355 140532789532416 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.1883983612060547, loss=0.8737046718597412
I0216 09:35:34.153940 140532797925120 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.1047980785369873, loss=0.8808256387710571
I0216 09:36:49.527259 140532789532416 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.1932429075241089, loss=0.8365524411201477
I0216 09:38:04.859155 140532797925120 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.1457406282424927, loss=0.8514328598976135
I0216 09:39:20.040931 140532789532416 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.2621057033538818, loss=0.8721280097961426
I0216 09:40:35.567821 140532797925120 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.2248317003250122, loss=0.9011329412460327
I0216 09:41:57.420796 140532789532416 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.2622438669204712, loss=0.8638007044792175
I0216 09:43:21.001161 140532797925120 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.2948057651519775, loss=0.7985535860061646
I0216 09:44:45.605045 140532789532416 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.3044555187225342, loss=0.8705160021781921
I0216 09:46:09.987164 140532797925120 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.3104774951934814, loss=0.883487343788147
I0216 09:47:33.507459 140532797925120 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.1002247333526611, loss=0.8679153323173523
I0216 09:48:48.650463 140532789532416 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.292565107345581, loss=0.8566725850105286
I0216 09:50:03.704131 140532797925120 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.0278642177581787, loss=0.8655330538749695
I0216 09:51:18.909110 140532789532416 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3599646091461182, loss=0.8465717434883118
I0216 09:52:34.095784 140532797925120 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2390543222427368, loss=0.8643367886543274
I0216 09:53:56.214367 140532789532416 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3049722909927368, loss=0.8419930934906006
I0216 09:55:20.491610 140532797925120 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.0948443412780762, loss=0.8406550884246826
I0216 09:56:44.309579 140532789532416 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1133610010147095, loss=0.8408834934234619
I0216 09:58:00.343981 140599226058560 spec.py:321] Evaluating on the training split.
I0216 09:58:54.318287 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 09:59:45.281787 140599226058560 spec.py:349] Evaluating on the test split.
I0216 10:00:11.471866 140599226058560 submission_runner.py:408] Time since start: 64545.76s, 	Step: 74993, 	{'train/ctc_loss': Array(0.07795109, dtype=float32), 'train/wer': 0.03025211003951877, 'validation/ctc_loss': Array(0.29337308, dtype=float32), 'validation/wer': 0.08340654778570532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15675355, dtype=float32), 'test/wer': 0.051632035423394874, 'test/num_examples': 2472, 'score': 59086.733266830444, 'total_duration': 64545.75790762901, 'accumulated_submission_time': 59086.733266830444, 'accumulated_eval_time': 5453.71427154541, 'accumulated_logging_time': 2.295135021209717}
I0216 10:00:11.520104 140532797925120 logging_writer.py:48] [74993] accumulated_eval_time=5453.714272, accumulated_logging_time=2.295135, accumulated_submission_time=59086.733267, global_step=74993, preemption_count=0, score=59086.733267, test/ctc_loss=0.1567535549402237, test/num_examples=2472, test/wer=0.051632, total_duration=64545.757908, train/ctc_loss=0.0779510885477066, train/wer=0.030252, validation/ctc_loss=0.29337307810783386, validation/num_examples=5348, validation/wer=0.083407
I0216 10:00:17.612565 140532789532416 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.3533215522766113, loss=0.831876277923584
I0216 10:01:32.837982 140532797925120 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.3959009647369385, loss=0.9046738743782043
I0216 10:02:52.044331 140532797925120 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1157861948013306, loss=0.8985944390296936
I0216 10:04:07.354484 140532789532416 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.2781591415405273, loss=0.8684505224227905
I0216 10:05:22.617399 140532797925120 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.2196893692016602, loss=0.8589131832122803
I0216 10:06:37.979203 140532789532416 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1451692581176758, loss=0.8643836379051208
I0216 10:07:53.231436 140532797925120 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.373814582824707, loss=0.8619720935821533
I0216 10:09:12.034446 140532789532416 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.2627323865890503, loss=0.820969820022583
I0216 10:10:35.710035 140532797925120 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.3091685771942139, loss=0.8445173501968384
I0216 10:11:59.699319 140532789532416 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1276549100875854, loss=0.870719313621521
I0216 10:13:23.953766 140532797925120 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.4713326692581177, loss=0.8305090665817261
I0216 10:14:47.682774 140532789532416 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1478623151779175, loss=0.8534703254699707
I0216 10:16:10.806694 140532797925120 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.023432970046997, loss=0.880836546421051
I0216 10:17:31.420727 140532797925120 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.344204306602478, loss=0.8722925186157227
I0216 10:18:46.631709 140532789532416 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.2873682975769043, loss=0.8623814582824707
I0216 10:20:01.864477 140532797925120 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.2312557697296143, loss=0.8387479782104492
I0216 10:21:17.041327 140532789532416 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.1879016160964966, loss=0.8348926305770874
I0216 10:22:32.227020 140532797925120 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.960734486579895, loss=0.8473569750785828
I0216 10:23:55.072612 140532789532416 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.4551681280136108, loss=0.9004364609718323
I0216 10:24:12.597647 140599226058560 spec.py:321] Evaluating on the training split.
I0216 10:25:06.961269 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 10:25:58.261048 140599226058560 spec.py:349] Evaluating on the test split.
I0216 10:26:24.047826 140599226058560 submission_runner.py:408] Time since start: 66118.33s, 	Step: 76822, 	{'train/ctc_loss': Array(0.06765499, dtype=float32), 'train/wer': 0.02511690046760187, 'validation/ctc_loss': Array(0.29259604, dtype=float32), 'validation/wer': 0.08351274896936578, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15502998, dtype=float32), 'test/wer': 0.05106331119371153, 'test/num_examples': 2472, 'score': 60527.72573661804, 'total_duration': 66118.33410978317, 'accumulated_submission_time': 60527.72573661804, 'accumulated_eval_time': 5585.158769369125, 'accumulated_logging_time': 2.3584864139556885}
I0216 10:26:24.094573 140532797925120 logging_writer.py:48] [76822] accumulated_eval_time=5585.158769, accumulated_logging_time=2.358486, accumulated_submission_time=60527.725737, global_step=76822, preemption_count=0, score=60527.725737, test/ctc_loss=0.15502998232841492, test/num_examples=2472, test/wer=0.051063, total_duration=66118.334110, train/ctc_loss=0.06765498965978622, train/wer=0.025117, validation/ctc_loss=0.2925960421562195, validation/num_examples=5348, validation/wer=0.083513
I0216 10:27:23.303462 140532789532416 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1674323081970215, loss=0.8370955586433411
I0216 10:28:38.455888 140532797925120 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.1500513553619385, loss=0.8686473369598389
I0216 10:29:53.970724 140532789532416 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1022197008132935, loss=0.8378063440322876
I0216 10:31:10.837333 140532797925120 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2303974628448486, loss=0.8824408650398254
I0216 10:32:33.677774 140532797925120 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.294004201889038, loss=0.8595770001411438
I0216 10:33:48.946929 140532789532416 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.0843260288238525, loss=0.8603391647338867
I0216 10:35:04.326730 140532797925120 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.2855058908462524, loss=0.9021666049957275
I0216 10:35:25.121425 140532789532416 logging_writer.py:48] [77529] global_step=77529, preemption_count=0, score=61068.689002
I0216 10:35:26.031033 140599226058560 checkpoints.py:490] Saving checkpoint at step: 77529
I0216 10:35:27.459977 140599226058560 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_3/checkpoint_77529
I0216 10:35:27.491608 140599226058560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_3/checkpoint_77529.
I0216 10:35:31.013233 140599226058560 submission_runner.py:583] Tuning trial 3/5
I0216 10:35:31.013505 140599226058560 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0216 10:35:31.039839 140599226058560 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.871246, dtype=float32), 'train/wer': 1.3708653155889337, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 34.33066487312317, 'total_duration': 169.64612746238708, 'accumulated_submission_time': 34.33066487312317, 'accumulated_eval_time': 135.31539034843445, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1750, {'train/ctc_loss': Array(6.003965, dtype=float32), 'train/wer': 0.9391896477614642, 'validation/ctc_loss': Array(6.1192107, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0945807, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1474.825115442276, 'total_duration': 1719.9163784980774, 'accumulated_submission_time': 1474.825115442276, 'accumulated_eval_time': 244.99092769622803, 'accumulated_logging_time': 0.028472900390625, 'global_step': 1750, 'preemption_count': 0}), (3549, {'train/ctc_loss': Array(3.3109722, dtype=float32), 'train/wer': 0.7425314465408805, 'validation/ctc_loss': Array(3.2170153, dtype=float32), 'validation/wer': 0.7051565501993686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.8966093, dtype=float32), 'test/wer': 0.6532000893709504, 'test/num_examples': 2472, 'score': 2914.8816883563995, 'total_duration': 3281.0594849586487, 'accumulated_submission_time': 2914.8816883563995, 'accumulated_eval_time': 365.9432199001312, 'accumulated_logging_time': 0.08463764190673828, 'global_step': 3549, 'preemption_count': 0}), (5327, {'train/ctc_loss': Array(1.1042546, dtype=float32), 'train/wer': 0.34265022443973503, 'validation/ctc_loss': Array(1.1603082, dtype=float32), 'validation/wer': 0.33234212228583565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.85711485, dtype=float32), 'test/wer': 0.2735563544776877, 'test/num_examples': 2472, 'score': 4355.136976242065, 'total_duration': 4853.463213682175, 'accumulated_submission_time': 4355.136976242065, 'accumulated_eval_time': 497.9646122455597, 'accumulated_logging_time': 0.136824369430542, 'global_step': 5327, 'preemption_count': 0}), (7101, {'train/ctc_loss': Array(0.71781844, dtype=float32), 'train/wer': 0.23816366362119198, 'validation/ctc_loss': Array(0.856381, dtype=float32), 'validation/wer': 0.25759579829498824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.58395475, dtype=float32), 'test/wer': 0.19566144659070137, 'test/num_examples': 2472, 'score': 5796.596266269684, 'total_duration': 6426.853343486786, 'accumulated_submission_time': 5796.596266269684, 'accumulated_eval_time': 629.766964673996, 'accumulated_logging_time': 0.1887216567993164, 'global_step': 7101, 'preemption_count': 0}), (8886, {'train/ctc_loss': Array(0.6037436, dtype=float32), 'train/wer': 0.20082768397105394, 'validation/ctc_loss': Array(0.7638324, dtype=float32), 'validation/wer': 0.23031174874730875, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5030347, dtype=float32), 'test/wer': 0.16748928564174437, 'test/num_examples': 2472, 'score': 7236.706146240234, 'total_duration': 7999.423669338226, 'accumulated_submission_time': 7236.706146240234, 'accumulated_eval_time': 762.0983710289001, 'accumulated_logging_time': 0.2411792278289795, 'global_step': 8886, 'preemption_count': 0}), (10688, {'train/ctc_loss': Array(0.59205496, dtype=float32), 'train/wer': 0.20361274677653848, 'validation/ctc_loss': Array(0.702261, dtype=float32), 'validation/wer': 0.2120741091168889, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45312193, dtype=float32), 'test/wer': 0.15556638839802572, 'test/num_examples': 2472, 'score': 8676.956310272217, 'total_duration': 9573.624148845673, 'accumulated_submission_time': 8676.956310272217, 'accumulated_eval_time': 895.9172258377075, 'accumulated_logging_time': 0.2942352294921875, 'global_step': 10688, 'preemption_count': 0}), (12458, {'train/ctc_loss': Array(0.49941278, dtype=float32), 'train/wer': 0.17151479117353408, 'validation/ctc_loss': Array(0.64830595, dtype=float32), 'validation/wer': 0.19569981752705717, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4115406, dtype=float32), 'test/wer': 0.1393374362724189, 'test/num_examples': 2472, 'score': 10117.289225816727, 'total_duration': 11145.675840854645, 'accumulated_submission_time': 10117.289225816727, 'accumulated_eval_time': 1027.5034348964691, 'accumulated_logging_time': 0.3512380123138428, 'global_step': 12458, 'preemption_count': 0}), (14248, {'train/ctc_loss': Array(0.49112177, dtype=float32), 'train/wer': 0.16632042911473102, 'validation/ctc_loss': Array(0.6159968, dtype=float32), 'validation/wer': 0.18688511928323856, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38463014, dtype=float32), 'test/wer': 0.13159872443279913, 'test/num_examples': 2472, 'score': 11557.23063492775, 'total_duration': 12719.211951255798, 'accumulated_submission_time': 11557.23063492775, 'accumulated_eval_time': 1160.9660465717316, 'accumulated_logging_time': 0.408066987991333, 'global_step': 14248, 'preemption_count': 0}), (16095, {'train/ctc_loss': Array(0.41128248, dtype=float32), 'train/wer': 0.1451225741954266, 'validation/ctc_loss': Array(0.5923722, dtype=float32), 'validation/wer': 0.17998204234530832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36829486, dtype=float32), 'test/wer': 0.1253021347470193, 'test/num_examples': 2472, 'score': 12997.853597402573, 'total_duration': 14287.417057275772, 'accumulated_submission_time': 12997.853597402573, 'accumulated_eval_time': 1288.4258918762207, 'accumulated_logging_time': 0.45952868461608887, 'global_step': 16095, 'preemption_count': 0}), (17959, {'train/ctc_loss': Array(0.41662517, dtype=float32), 'train/wer': 0.14680189517323067, 'validation/ctc_loss': Array(0.57269543, dtype=float32), 'validation/wer': 0.17245141295847533, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35398287, dtype=float32), 'test/wer': 0.12046797879471087, 'test/num_examples': 2472, 'score': 14437.80846619606, 'total_duration': 15852.590964317322, 'accumulated_submission_time': 14437.80846619606, 'accumulated_eval_time': 1413.5200998783112, 'accumulated_logging_time': 0.5142014026641846, 'global_step': 17959, 'preemption_count': 0}), (19819, {'train/ctc_loss': Array(0.3939056, dtype=float32), 'train/wer': 0.13807692737617508, 'validation/ctc_loss': Array(0.55853176, dtype=float32), 'validation/wer': 0.16761443177539415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33988407, dtype=float32), 'test/wer': 0.11502447545345601, 'test/num_examples': 2472, 'score': 15878.370507478714, 'total_duration': 17418.529887914658, 'accumulated_submission_time': 15878.370507478714, 'accumulated_eval_time': 1538.7788639068604, 'accumulated_logging_time': 0.561873197555542, 'global_step': 19819, 'preemption_count': 0}), (21678, {'train/ctc_loss': Array(0.26414925, dtype=float32), 'train/wer': 0.0981504777709376, 'validation/ctc_loss': Array(0.5420662, dtype=float32), 'validation/wer': 0.16272917732701275, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32638323, dtype=float32), 'test/wer': 0.10870757418804461, 'test/num_examples': 2472, 'score': 17318.98425388336, 'total_duration': 18997.489067554474, 'accumulated_submission_time': 17318.98425388336, 'accumulated_eval_time': 1677.0056114196777, 'accumulated_logging_time': 0.6107773780822754, 'global_step': 21678, 'preemption_count': 0}), (23548, {'train/ctc_loss': Array(0.24399593, dtype=float32), 'train/wer': 0.08901873262866412, 'validation/ctc_loss': Array(0.5178802, dtype=float32), 'validation/wer': 0.15804667059289224, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3171775, dtype=float32), 'test/wer': 0.10893100156399163, 'test/num_examples': 2472, 'score': 18759.508040905, 'total_duration': 20568.534235477448, 'accumulated_submission_time': 18759.508040905, 'accumulated_eval_time': 1807.410040140152, 'accumulated_logging_time': 0.6587982177734375, 'global_step': 23548, 'preemption_count': 0}), (25401, {'train/ctc_loss': Array(0.2405506, dtype=float32), 'train/wer': 0.08822214860736464, 'validation/ctc_loss': Array(0.51276976, dtype=float32), 'validation/wer': 0.15382758720565376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31287903, dtype=float32), 'test/wer': 0.10529522880994455, 'test/num_examples': 2472, 'score': 20200.06555223465, 'total_duration': 22138.588984251022, 'accumulated_submission_time': 20200.06555223465, 'accumulated_eval_time': 1936.7809422016144, 'accumulated_logging_time': 0.7138986587524414, 'global_step': 25401, 'preemption_count': 0}), (27264, {'train/ctc_loss': Array(0.22414844, dtype=float32), 'train/wer': 0.08301188654325906, 'validation/ctc_loss': Array(0.50314236, dtype=float32), 'validation/wer': 0.1510277378182415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29457772, dtype=float32), 'test/wer': 0.10123291288363497, 'test/num_examples': 2472, 'score': 21640.561172246933, 'total_duration': 23708.775426387787, 'accumulated_submission_time': 21640.561172246933, 'accumulated_eval_time': 2066.3479647636414, 'accumulated_logging_time': 0.7682766914367676, 'global_step': 27264, 'preemption_count': 0}), (29119, {'train/ctc_loss': Array(0.22378603, dtype=float32), 'train/wer': 0.08479563335536555, 'validation/ctc_loss': Array(0.4944983, dtype=float32), 'validation/wer': 0.14928024561437384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29227978, dtype=float32), 'test/wer': 0.09968923283163732, 'test/num_examples': 2472, 'score': 23080.513560533524, 'total_duration': 25276.796125650406, 'accumulated_submission_time': 23080.513560533524, 'accumulated_eval_time': 2194.294489145279, 'accumulated_logging_time': 0.8199701309204102, 'global_step': 29119, 'preemption_count': 0}), (30958, {'train/ctc_loss': Array(0.20724949, dtype=float32), 'train/wer': 0.07801074446828589, 'validation/ctc_loss': Array(0.48233274, dtype=float32), 'validation/wer': 0.14484875985981444, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.286033, dtype=float32), 'test/wer': 0.09818617593890276, 'test/num_examples': 2472, 'score': 24520.68779182434, 'total_duration': 26845.64391064644, 'accumulated_submission_time': 24520.68779182434, 'accumulated_eval_time': 2322.8469140529633, 'accumulated_logging_time': 0.8701145648956299, 'global_step': 30958, 'preemption_count': 0}), (32805, {'train/ctc_loss': Array(0.24316454, dtype=float32), 'train/wer': 0.08499948343456204, 'validation/ctc_loss': Array(0.4656841, dtype=float32), 'validation/wer': 0.14146963128879964, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27722582, dtype=float32), 'test/wer': 0.09574878638311701, 'test/num_examples': 2472, 'score': 25960.884137392044, 'total_duration': 28414.607186079025, 'accumulated_submission_time': 25960.884137392044, 'accumulated_eval_time': 2451.4838552474976, 'accumulated_logging_time': 0.9259054660797119, 'global_step': 32805, 'preemption_count': 0}), (34652, {'train/ctc_loss': Array(0.20290689, dtype=float32), 'train/wer': 0.07742401933621476, 'validation/ctc_loss': Array(0.46335202, dtype=float32), 'validation/wer': 0.13939388088089055, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.264707, dtype=float32), 'test/wer': 0.09221457152722767, 'test/num_examples': 2472, 'score': 27401.33624315262, 'total_duration': 29985.316709280014, 'accumulated_submission_time': 27401.33624315262, 'accumulated_eval_time': 2581.6163563728333, 'accumulated_logging_time': 0.9803595542907715, 'global_step': 34652, 'preemption_count': 0}), (36509, {'train/ctc_loss': Array(0.18551725, dtype=float32), 'train/wer': 0.0709965430233595, 'validation/ctc_loss': Array(0.45179525, dtype=float32), 'validation/wer': 0.13585062320785501, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2665545, dtype=float32), 'test/wer': 0.08910689984360083, 'test/num_examples': 2472, 'score': 28841.840735912323, 'total_duration': 31557.14866900444, 'accumulated_submission_time': 28841.840735912323, 'accumulated_eval_time': 2712.818051815033, 'accumulated_logging_time': 1.0341756343841553, 'global_step': 36509, 'preemption_count': 0}), (38355, {'train/ctc_loss': Array(0.17950557, dtype=float32), 'train/wer': 0.06721768942514683, 'validation/ctc_loss': Array(0.4428782, dtype=float32), 'validation/wer': 0.1336300530040453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26117116, dtype=float32), 'test/wer': 0.08843661771575975, 'test/num_examples': 2472, 'score': 30282.38442492485, 'total_duration': 33127.51769065857, 'accumulated_submission_time': 30282.38442492485, 'accumulated_eval_time': 2842.51593208313, 'accumulated_logging_time': 1.0907597541809082, 'global_step': 38355, 'preemption_count': 0}), (40185, {'train/ctc_loss': Array(0.16687053, dtype=float32), 'train/wer': 0.06488479758828596, 'validation/ctc_loss': Array(0.43387946, dtype=float32), 'validation/wer': 0.13033781631057087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25024888, dtype=float32), 'test/wer': 0.08461804074502874, 'test/num_examples': 2472, 'score': 31722.822843551636, 'total_duration': 34697.79404234886, 'accumulated_submission_time': 31722.822843551636, 'accumulated_eval_time': 2972.2214760780334, 'accumulated_logging_time': 1.150517463684082, 'global_step': 40185, 'preemption_count': 0}), (42037, {'train/ctc_loss': Array(0.17537016, dtype=float32), 'train/wer': 0.06591017942010831, 'validation/ctc_loss': Array(0.42432463, dtype=float32), 'validation/wer': 0.12730625524971761, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24411249, dtype=float32), 'test/wer': 0.08364308492271444, 'test/num_examples': 2472, 'score': 33162.69558787346, 'total_duration': 36268.490728616714, 'accumulated_submission_time': 33162.69558787346, 'accumulated_eval_time': 3102.9151356220245, 'accumulated_logging_time': 1.2059061527252197, 'global_step': 42037, 'preemption_count': 0}), (43882, {'train/ctc_loss': Array(0.17138386, dtype=float32), 'train/wer': 0.06454534444719424, 'validation/ctc_loss': Array(0.4157252, dtype=float32), 'validation/wer': 0.124429168637825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23707676, dtype=float32), 'test/wer': 0.08059634797798225, 'test/num_examples': 2472, 'score': 34603.0143558979, 'total_duration': 37839.05934667587, 'accumulated_submission_time': 34603.0143558979, 'accumulated_eval_time': 3233.0268499851227, 'accumulated_logging_time': 1.269730806350708, 'global_step': 43882, 'preemption_count': 0}), (45739, {'train/ctc_loss': Array(0.15769911, dtype=float32), 'train/wer': 0.06066654914607464, 'validation/ctc_loss': Array(0.4071435, dtype=float32), 'validation/wer': 0.122372727536036, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2356796, dtype=float32), 'test/wer': 0.08029167428350903, 'test/num_examples': 2472, 'score': 36043.24026441574, 'total_duration': 39409.80263566971, 'accumulated_submission_time': 36043.24026441574, 'accumulated_eval_time': 3363.4156663417816, 'accumulated_logging_time': 1.3258774280548096, 'global_step': 45739, 'preemption_count': 0}), (47582, {'train/ctc_loss': Array(0.15129596, dtype=float32), 'train/wer': 0.057782668328526805, 'validation/ctc_loss': Array(0.40430495, dtype=float32), 'validation/wer': 0.11943771300578314, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22712962, dtype=float32), 'test/wer': 0.07708244470172446, 'test/num_examples': 2472, 'score': 37483.37846851349, 'total_duration': 40981.70229816437, 'accumulated_submission_time': 37483.37846851349, 'accumulated_eval_time': 3495.040193796158, 'accumulated_logging_time': 1.3885679244995117, 'global_step': 47582, 'preemption_count': 0}), (49408, {'train/ctc_loss': Array(0.13115764, dtype=float32), 'train/wer': 0.0500343243385964, 'validation/ctc_loss': Array(0.39019933, dtype=float32), 'validation/wer': 0.11601996582252817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21716937, dtype=float32), 'test/wer': 0.07543720675156906, 'test/num_examples': 2472, 'score': 38923.28881287575, 'total_duration': 42553.449315071106, 'accumulated_submission_time': 38923.28881287575, 'accumulated_eval_time': 3626.74472117424, 'accumulated_logging_time': 1.4479291439056396, 'global_step': 49408, 'preemption_count': 0}), (51227, {'train/ctc_loss': Array(0.13253723, dtype=float32), 'train/wer': 0.050379519241574466, 'validation/ctc_loss': Array(0.3822779, dtype=float32), 'validation/wer': 0.11255394537397298, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21561463, dtype=float32), 'test/wer': 0.07249202770499462, 'test/num_examples': 2472, 'score': 40363.78140926361, 'total_duration': 44126.6337416172, 'accumulated_submission_time': 40363.78140926361, 'accumulated_eval_time': 3759.30832862854, 'accumulated_logging_time': 1.5018019676208496, 'global_step': 51227, 'preemption_count': 0}), (53073, {'train/ctc_loss': Array(0.13393289, dtype=float32), 'train/wer': 0.0510575079009211, 'validation/ctc_loss': Array(0.3740585, dtype=float32), 'validation/wer': 0.11190708361895016, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20504084, dtype=float32), 'test/wer': 0.07046086974183982, 'test/num_examples': 2472, 'score': 41803.87991976738, 'total_duration': 45697.32681298256, 'accumulated_submission_time': 41803.87991976738, 'accumulated_eval_time': 3889.758169412613, 'accumulated_logging_time': 1.5712800025939941, 'global_step': 53073, 'preemption_count': 0}), (54913, {'train/ctc_loss': Array(0.11381011, dtype=float32), 'train/wer': 0.04320911095892627, 'validation/ctc_loss': Array(0.36113426, dtype=float32), 'validation/wer': 0.10776523745619201, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20187275, dtype=float32), 'test/wer': 0.06686572014705584, 'test/num_examples': 2472, 'score': 43243.759679079056, 'total_duration': 47267.8637509346, 'accumulated_submission_time': 43243.759679079056, 'accumulated_eval_time': 4020.283055782318, 'accumulated_logging_time': 1.6276521682739258, 'global_step': 54913, 'preemption_count': 0}), (56736, {'train/ctc_loss': Array(0.11524229, dtype=float32), 'train/wer': 0.04620441481158382, 'validation/ctc_loss': Array(0.35849932, dtype=float32), 'validation/wer': 0.10443438215047743, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19632728, dtype=float32), 'test/wer': 0.06710945910263441, 'test/num_examples': 2472, 'score': 44683.91825604439, 'total_duration': 48838.96536445618, 'accumulated_submission_time': 44683.91825604439, 'accumulated_eval_time': 4151.09471654892, 'accumulated_logging_time': 1.6856026649475098, 'global_step': 56736, 'preemption_count': 0}), (58571, {'train/ctc_loss': Array(0.13256885, dtype=float32), 'train/wer': 0.04735210113979055, 'validation/ctc_loss': Array(0.35000604, dtype=float32), 'validation/wer': 0.10222346659972774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1881308, dtype=float32), 'test/wer': 0.06371742530416591, 'test/num_examples': 2472, 'score': 46123.78596878052, 'total_duration': 50409.45980882645, 'accumulated_submission_time': 46123.78596878052, 'accumulated_eval_time': 4281.586913585663, 'accumulated_logging_time': 1.746351718902588, 'global_step': 58571, 'preemption_count': 0}), (60404, {'train/ctc_loss': Array(0.08553199, dtype=float32), 'train/wer': 0.033242987967639935, 'validation/ctc_loss': Array(0.34243563, dtype=float32), 'validation/wer': 0.0999546231306178, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18706349, dtype=float32), 'test/wer': 0.06268153474295696, 'test/num_examples': 2472, 'score': 47564.2847366333, 'total_duration': 51981.551478385925, 'accumulated_submission_time': 47564.2847366333, 'accumulated_eval_time': 4413.043275594711, 'accumulated_logging_time': 1.8080739974975586, 'global_step': 60404, 'preemption_count': 0}), (62252, {'train/ctc_loss': Array(0.08559206, dtype=float32), 'train/wer': 0.03227601847613499, 'validation/ctc_loss': Array(0.33397695, dtype=float32), 'validation/wer': 0.09791749133494888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1801457, dtype=float32), 'test/wer': 0.060264456766802754, 'test/num_examples': 2472, 'score': 49004.790016412735, 'total_duration': 53553.19096279144, 'accumulated_submission_time': 49004.790016412735, 'accumulated_eval_time': 4544.038645029068, 'accumulated_logging_time': 1.8715159893035889, 'global_step': 62252, 'preemption_count': 0}), (64082, {'train/ctc_loss': Array(0.10317326, dtype=float32), 'train/wer': 0.040281857487045776, 'validation/ctc_loss': Array(0.32366726, dtype=float32), 'validation/wer': 0.09361151607017001, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17913373, dtype=float32), 'test/wer': 0.059472305161172384, 'test/num_examples': 2472, 'score': 50445.33984518051, 'total_duration': 55123.66427946091, 'accumulated_submission_time': 50445.33984518051, 'accumulated_eval_time': 4673.833595752716, 'accumulated_logging_time': 1.9265692234039307, 'global_step': 64082, 'preemption_count': 0}), (65901, {'train/ctc_loss': Array(0.09804582, dtype=float32), 'train/wer': 0.03701828627401346, 'validation/ctc_loss': Array(0.31444234, dtype=float32), 'validation/wer': 0.09114957953985924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16896814, dtype=float32), 'test/wer': 0.05573497450896756, 'test/num_examples': 2472, 'score': 51886.01150846481, 'total_duration': 56693.15033197403, 'accumulated_submission_time': 51886.01150846481, 'accumulated_eval_time': 4802.509584188461, 'accumulated_logging_time': 1.9893386363983154, 'global_step': 65901, 'preemption_count': 0}), (67717, {'train/ctc_loss': Array(0.11457752, dtype=float32), 'train/wer': 0.0450217154005156, 'validation/ctc_loss': Array(0.3129322, dtype=float32), 'validation/wer': 0.09053168174401653, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1688004, dtype=float32), 'test/wer': 0.05711616192391282, 'test/num_examples': 2472, 'score': 53326.61968564987, 'total_duration': 58263.58122968674, 'accumulated_submission_time': 53326.61968564987, 'accumulated_eval_time': 4932.19512963295, 'accumulated_logging_time': 2.0536038875579834, 'global_step': 67717, 'preemption_count': 0}), (69559, {'train/ctc_loss': Array(0.09059269, dtype=float32), 'train/wer': 0.03353950845356968, 'validation/ctc_loss': Array(0.30794328, dtype=float32), 'validation/wer': 0.08759666721376368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16448735, dtype=float32), 'test/wer': 0.05524749659781041, 'test/num_examples': 2472, 'score': 54766.81351041794, 'total_duration': 59832.69885110855, 'accumulated_submission_time': 54766.81351041794, 'accumulated_eval_time': 5060.983474969864, 'accumulated_logging_time': 2.1116764545440674, 'global_step': 69559, 'preemption_count': 0}), (71385, {'train/ctc_loss': Array(0.08755792, dtype=float32), 'train/wer': 0.03389975091275122, 'validation/ctc_loss': Array(0.30202192, dtype=float32), 'validation/wer': 0.08579124709153577, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16138907, dtype=float32), 'test/wer': 0.05392724392175979, 'test/num_examples': 2472, 'score': 56206.77034330368, 'total_duration': 61402.078199625015, 'accumulated_submission_time': 56206.77034330368, 'accumulated_eval_time': 5190.269483566284, 'accumulated_logging_time': 2.172675371170044, 'global_step': 71385, 'preemption_count': 0}), (73176, {'train/ctc_loss': Array(0.06485754, dtype=float32), 'train/wer': 0.024649648976517738, 'validation/ctc_loss': Array(0.29641142, dtype=float32), 'validation/wer': 0.08471958060187107, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15961748, dtype=float32), 'test/wer': 0.053155403895760976, 'test/num_examples': 2472, 'score': 57646.677629470825, 'total_duration': 62974.443323373795, 'accumulated_submission_time': 57646.677629470825, 'accumulated_eval_time': 5322.592356920242, 'accumulated_logging_time': 2.23431396484375, 'global_step': 73176, 'preemption_count': 0}), (74993, {'train/ctc_loss': Array(0.07795109, dtype=float32), 'train/wer': 0.03025211003951877, 'validation/ctc_loss': Array(0.29337308, dtype=float32), 'validation/wer': 0.08340654778570532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15675355, dtype=float32), 'test/wer': 0.051632035423394874, 'test/num_examples': 2472, 'score': 59086.733266830444, 'total_duration': 64545.75790762901, 'accumulated_submission_time': 59086.733266830444, 'accumulated_eval_time': 5453.71427154541, 'accumulated_logging_time': 2.295135021209717, 'global_step': 74993, 'preemption_count': 0}), (76822, {'train/ctc_loss': Array(0.06765499, dtype=float32), 'train/wer': 0.02511690046760187, 'validation/ctc_loss': Array(0.29259604, dtype=float32), 'validation/wer': 0.08351274896936578, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.15502998, dtype=float32), 'test/wer': 0.05106331119371153, 'test/num_examples': 2472, 'score': 60527.72573661804, 'total_duration': 66118.33410978317, 'accumulated_submission_time': 60527.72573661804, 'accumulated_eval_time': 5585.158769369125, 'accumulated_logging_time': 2.3584864139556885, 'global_step': 76822, 'preemption_count': 0})], 'global_step': 77529}
I0216 10:35:31.040066 140599226058560 submission_runner.py:586] Timing: 61068.68900227547
I0216 10:35:31.040125 140599226058560 submission_runner.py:588] Total number of evals: 43
I0216 10:35:31.040172 140599226058560 submission_runner.py:589] ====================
I0216 10:35:31.040221 140599226058560 submission_runner.py:542] Using RNG seed 4151861576
I0216 10:35:31.043236 140599226058560 submission_runner.py:551] --- Tuning run 4/5 ---
I0216 10:35:31.043390 140599226058560 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_4.
I0216 10:35:31.045120 140599226058560 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_4/hparams.json.
I0216 10:35:31.046432 140599226058560 submission_runner.py:206] Initializing dataset.
I0216 10:35:31.046579 140599226058560 submission_runner.py:213] Initializing model.
I0216 10:35:34.796603 140599226058560 submission_runner.py:255] Initializing optimizer.
I0216 10:35:35.237519 140599226058560 submission_runner.py:262] Initializing metrics bundle.
I0216 10:35:35.237731 140599226058560 submission_runner.py:280] Initializing checkpoint and logger.
I0216 10:35:35.242248 140599226058560 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_4 with prefix checkpoint_
I0216 10:35:35.242413 140599226058560 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_4/meta_data_0.json.
I0216 10:35:35.242696 140599226058560 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 10:35:35.242768 140599226058560 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 10:35:36.115255 140599226058560 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 10:35:36.940592 140599226058560 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_4/flags_0.json.
I0216 10:35:36.960769 140599226058560 submission_runner.py:314] Starting training loop.
I0216 10:35:36.964310 140599226058560 input_pipeline.py:20] Loading split = train-clean-100
I0216 10:35:37.009591 140599226058560 input_pipeline.py:20] Loading split = train-clean-360
I0216 10:35:37.521450 140599226058560 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0216 10:36:12.036269 140427625584384 logging_writer.py:48] [0] global_step=0, grad_norm=41.225093841552734, loss=32.50017547607422
I0216 10:36:12.058455 140599226058560 spec.py:321] Evaluating on the training split.
I0216 10:37:11.039295 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 10:38:01.588624 140599226058560 spec.py:349] Evaluating on the test split.
I0216 10:38:27.900970 140599226058560 submission_runner.py:408] Time since start: 170.94s, 	Step: 1, 	{'train/ctc_loss': Array(31.98225, dtype=float32), 'train/wer': 1.3614880623081345, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585747801152765, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 35.09761643409729, 'total_duration': 170.93748545646667, 'accumulated_submission_time': 35.09761643409729, 'accumulated_eval_time': 135.83981275558472, 'accumulated_logging_time': 0}
I0216 10:38:27.918298 140532797925120 logging_writer.py:48] [1] accumulated_eval_time=135.839813, accumulated_logging_time=0, accumulated_submission_time=35.097616, global_step=1, preemption_count=0, score=35.097616, test/ctc_loss=31.23744010925293, test/num_examples=2472, test/wer=1.102350, total_duration=170.937485, train/ctc_loss=31.982250213623047, train/wer=1.361488, validation/ctc_loss=31.08704948425293, validation/num_examples=5348, validation/wer=1.058575
I0216 10:40:08.529337 140428012013312 logging_writer.py:48] [100] global_step=100, grad_norm=1.2876324653625488, loss=5.811514854431152
I0216 10:41:23.705421 140428020406016 logging_writer.py:48] [200] global_step=200, grad_norm=3.134495973587036, loss=5.778716564178467
I0216 10:42:38.972865 140428012013312 logging_writer.py:48] [300] global_step=300, grad_norm=4.45622444152832, loss=5.619658470153809
I0216 10:43:54.333086 140428020406016 logging_writer.py:48] [400] global_step=400, grad_norm=4.8541059494018555, loss=5.709814071655273
I0216 10:45:09.544484 140428012013312 logging_writer.py:48] [500] global_step=500, grad_norm=1.608568787574768, loss=5.574434757232666
I0216 10:46:27.258769 140428020406016 logging_writer.py:48] [600] global_step=600, grad_norm=1.5013859272003174, loss=5.526619911193848
I0216 10:47:51.346202 140428012013312 logging_writer.py:48] [700] global_step=700, grad_norm=1.9265676736831665, loss=5.531250953674316
I0216 10:49:15.770553 140428020406016 logging_writer.py:48] [800] global_step=800, grad_norm=2.6148605346679688, loss=5.513480186462402
I0216 10:50:39.492126 140428012013312 logging_writer.py:48] [900] global_step=900, grad_norm=1.0518553256988525, loss=5.508114337921143
I0216 10:52:03.578059 140428020406016 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.45549318194389343, loss=5.514954090118408
I0216 10:53:24.927140 140532797925120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5545591115951538, loss=5.555871963500977
I0216 10:54:39.803240 140532789532416 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.7306392192840576, loss=5.509141445159912
I0216 10:55:54.662711 140532797925120 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.6999168395996094, loss=5.550196647644043
I0216 10:57:09.809972 140532789532416 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.552957534790039, loss=5.582799911499023
I0216 10:58:26.612865 140532797925120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7168280482292175, loss=5.4950103759765625
I0216 10:59:50.001692 140532789532416 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.9646273851394653, loss=5.490772247314453
I0216 11:01:13.594267 140532797925120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8794819712638855, loss=5.483245849609375
I0216 11:02:28.919604 140599226058560 spec.py:321] Evaluating on the training split.
I0216 11:03:07.866857 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 11:03:54.152661 140599226058560 spec.py:349] Evaluating on the test split.
I0216 11:04:17.814730 140599226058560 submission_runner.py:408] Time since start: 1720.85s, 	Step: 1791, 	{'train/ctc_loss': Array(6.036521, dtype=float32), 'train/wer': 0.9413900245298447, 'validation/ctc_loss': Array(6.0377555, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0074077, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1476.0195994377136, 'total_duration': 1720.848159313202, 'accumulated_submission_time': 1476.0195994377136, 'accumulated_eval_time': 244.7291920185089, 'accumulated_logging_time': 0.02799057960510254}
I0216 11:04:17.850893 140532797925120 logging_writer.py:48] [1791] accumulated_eval_time=244.729192, accumulated_logging_time=0.027991, accumulated_submission_time=1476.019599, global_step=1791, preemption_count=0, score=1476.019599, test/ctc_loss=6.0074076652526855, test/num_examples=2472, test/wer=0.899580, total_duration=1720.848159, train/ctc_loss=6.036520957946777, train/wer=0.941390, validation/ctc_loss=6.037755489349365, validation/num_examples=5348, validation/wer=0.896618
I0216 11:04:25.456603 140532789532416 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.5402199029922485, loss=5.478538513183594
I0216 11:05:40.295901 140532797925120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.27711138129234314, loss=5.486117362976074
I0216 11:06:55.304368 140532789532416 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.9759154319763184, loss=5.509279251098633
I0216 11:08:14.350654 140532797925120 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.1522690057754517, loss=5.499337196350098
I0216 11:09:29.259745 140532789532416 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.198111057281494, loss=5.48519229888916
I0216 11:10:44.182457 140532797925120 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.8656408786773682, loss=5.465366363525391
I0216 11:11:59.116714 140532789532416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5751339197158813, loss=5.473080635070801
I0216 11:13:16.047333 140532797925120 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.7430346012115479, loss=5.489912509918213
I0216 11:14:39.321389 140532789532416 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.3307626247406006, loss=5.485404968261719
I0216 11:16:02.806756 140532797925120 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.1162171363830566, loss=5.472092628479004
I0216 11:17:26.481703 140532789532416 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.5716049671173096, loss=5.41157865524292
I0216 11:18:50.201405 140532797925120 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.6550376415252686, loss=5.197254657745361
I0216 11:20:13.571163 140532789532416 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7292660474777222, loss=4.803994655609131
I0216 11:21:39.923862 140532797925120 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.2000787258148193, loss=4.539493083953857
I0216 11:22:54.781874 140532789532416 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8921679854393005, loss=4.379403591156006
I0216 11:24:09.797410 140532797925120 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1938339471817017, loss=4.249993801116943
I0216 11:25:24.768001 140532789532416 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.4788542687892914, loss=4.10385799407959
I0216 11:26:39.697625 140532797925120 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.044075608253479, loss=3.9478070735931396
I0216 11:28:00.864316 140532789532416 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.0673861503601074, loss=3.930549144744873
I0216 11:28:18.487804 140599226058560 spec.py:321] Evaluating on the training split.
I0216 11:28:57.376967 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 11:29:43.377154 140599226058560 spec.py:349] Evaluating on the test split.
I0216 11:30:06.949455 140599226058560 submission_runner.py:408] Time since start: 3269.98s, 	Step: 3623, 	{'train/ctc_loss': Array(6.206665, dtype=float32), 'train/wer': 0.9382353276158586, 'validation/ctc_loss': Array(6.116562, dtype=float32), 'validation/wer': 0.8959904225841644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9581666, dtype=float32), 'test/wer': 0.8983405439441026, 'test/num_examples': 2472, 'score': 2916.5635409355164, 'total_duration': 3269.9817910194397, 'accumulated_submission_time': 2916.5635409355164, 'accumulated_eval_time': 353.1840178966522, 'accumulated_logging_time': 0.08708763122558594}
I0216 11:30:06.986998 140532797925120 logging_writer.py:48] [3623] accumulated_eval_time=353.184018, accumulated_logging_time=0.087088, accumulated_submission_time=2916.563541, global_step=3623, preemption_count=0, score=2916.563541, test/ctc_loss=5.958166599273682, test/num_examples=2472, test/wer=0.898341, total_duration=3269.981791, train/ctc_loss=6.2066650390625, train/wer=0.938235, validation/ctc_loss=6.1165618896484375, validation/num_examples=5348, validation/wer=0.895990
I0216 11:31:05.582730 140532789532416 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.313864827156067, loss=3.8190417289733887
I0216 11:32:20.495773 140532797925120 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.1777033805847168, loss=3.7482080459594727
I0216 11:33:35.532945 140532789532416 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6602829098701477, loss=3.728602170944214
I0216 11:34:52.784487 140532797925120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5258949995040894, loss=3.589127540588379
I0216 11:36:15.917268 140532789532416 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.386472225189209, loss=3.6336207389831543
I0216 11:37:36.832265 140532797925120 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.4028347730636597, loss=3.5323989391326904
I0216 11:38:51.852538 140532789532416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5234231948852539, loss=3.4425737857818604
I0216 11:40:06.804538 140532797925120 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.832397222518921, loss=3.462787389755249
I0216 11:41:21.802218 140532789532416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8264446258544922, loss=3.393052577972412
I0216 11:42:39.912591 140532797925120 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.1651123762130737, loss=3.378966808319092
I0216 11:44:02.718412 140532789532416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6925114989280701, loss=3.2717654705047607
I0216 11:45:26.303833 140532797925120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6996249556541443, loss=3.2199363708496094
I0216 11:46:49.822761 140532789532416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.9930499196052551, loss=3.283780574798584
I0216 11:48:12.758866 140532797925120 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.1754121780395508, loss=3.2547805309295654
I0216 11:49:36.547515 140532789532416 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.9367324113845825, loss=3.206744432449341
I0216 11:50:59.279318 140532797925120 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.9595025777816772, loss=3.218240261077881
I0216 11:52:14.217576 140532789532416 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.589276671409607, loss=3.1499533653259277
I0216 11:53:29.285067 140532797925120 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.4208711385726929, loss=3.1131744384765625
I0216 11:54:07.244566 140599226058560 spec.py:321] Evaluating on the training split.
I0216 11:54:46.019318 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 11:55:31.783082 140599226058560 spec.py:349] Evaluating on the test split.
I0216 11:55:55.696288 140599226058560 submission_runner.py:408] Time since start: 4818.73s, 	Step: 5452, 	{'train/ctc_loss': Array(12.30888, dtype=float32), 'train/wer': 0.939860390191516, 'validation/ctc_loss': Array(12.248377, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(12.230537, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4356.7338581085205, 'total_duration': 4818.730646371841, 'accumulated_submission_time': 4356.7338581085205, 'accumulated_eval_time': 461.63093519210815, 'accumulated_logging_time': 0.14055681228637695}
I0216 11:55:55.729241 140532797925120 logging_writer.py:48] [5452] accumulated_eval_time=461.630935, accumulated_logging_time=0.140557, accumulated_submission_time=4356.733858, global_step=5452, preemption_count=0, score=4356.733858, test/ctc_loss=12.230537414550781, test/num_examples=2472, test/wer=0.899580, total_duration=4818.730646, train/ctc_loss=12.308879852294922, train/wer=0.939860, validation/ctc_loss=12.248376846313477, validation/num_examples=5348, validation/wer=0.896618
I0216 11:56:32.616329 140532789532416 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.1445730924606323, loss=3.084808588027954
I0216 11:57:47.929221 140532797925120 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.1607578992843628, loss=3.097622871398926
I0216 11:59:03.404400 140532789532416 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.223272681236267, loss=3.062366485595703
I0216 12:00:18.617878 140532797925120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9395116567611694, loss=3.020533561706543
I0216 12:01:38.602913 140532789532416 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.1034681797027588, loss=2.9840540885925293
I0216 12:03:01.942834 140532797925120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6351391673088074, loss=2.9516890048980713
I0216 12:04:25.764476 140532789532416 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.9355107545852661, loss=2.9583070278167725
I0216 12:05:51.768185 140532797925120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5436499714851379, loss=2.914581775665283
I0216 12:07:06.847949 140532789532416 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.3776506185531616, loss=2.9723353385925293
I0216 12:08:21.921958 140532797925120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7954254150390625, loss=2.8820412158966064
I0216 12:09:37.101828 140532789532416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6892908811569214, loss=2.8726086616516113
I0216 12:10:52.223124 140532797925120 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.526537537574768, loss=2.8285889625549316
I0216 12:12:12.453391 140532789532416 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.3336535692214966, loss=2.912335157394409
I0216 12:13:35.588217 140532797925120 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.2913073301315308, loss=2.8740952014923096
I0216 12:14:59.860435 140532789532416 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.3217506408691406, loss=2.7961387634277344
I0216 12:16:23.712310 140532797925120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.753609299659729, loss=2.8529133796691895
I0216 12:17:47.119778 140532789532416 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.0503408908843994, loss=2.8341100215911865
I0216 12:19:11.008245 140532797925120 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.8485928773880005, loss=2.7589290142059326
I0216 12:19:55.699100 140599226058560 spec.py:321] Evaluating on the training split.
I0216 12:20:34.173948 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 12:21:20.350384 140599226058560 spec.py:349] Evaluating on the test split.
I0216 12:21:44.027484 140599226058560 submission_runner.py:408] Time since start: 6367.06s, 	Step: 7255, 	{'train/ctc_loss': Array(9.199602, dtype=float32), 'train/wer': 0.9406809116337576, 'validation/ctc_loss': Array(9.2184, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(9.05541, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5796.618039608002, 'total_duration': 6367.059968471527, 'accumulated_submission_time': 5796.618039608002, 'accumulated_eval_time': 569.9528002738953, 'accumulated_logging_time': 0.1885228157043457}
I0216 12:21:44.063904 140532797925120 logging_writer.py:48] [7255] accumulated_eval_time=569.952800, accumulated_logging_time=0.188523, accumulated_submission_time=5796.618040, global_step=7255, preemption_count=0, score=5796.618040, test/ctc_loss=9.055410385131836, test/num_examples=2472, test/wer=0.899580, total_duration=6367.059968, train/ctc_loss=9.199602127075195, train/wer=0.940681, validation/ctc_loss=9.218400001525879, validation/num_examples=5348, validation/wer=0.896618
I0216 12:22:18.541977 140532789532416 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.1890254020690918, loss=2.677192449569702
I0216 12:23:33.572600 140532797925120 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.178607702255249, loss=2.7569260597229004
I0216 12:24:48.577037 140532789532416 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.161948561668396, loss=2.7168357372283936
I0216 12:26:03.649400 140532797925120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8472594618797302, loss=2.737400531768799
I0216 12:27:18.571198 140532789532416 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0352935791015625, loss=2.6631217002868652
I0216 12:28:33.523032 140532797925120 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.5240479707717896, loss=2.735001802444458
I0216 12:29:56.877147 140532789532416 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.965470790863037, loss=2.6300270557403564
I0216 12:31:20.772490 140532797925120 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6886199116706848, loss=2.695431709289551
I0216 12:32:45.189865 140532789532416 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0391050577163696, loss=2.67572021484375
I0216 12:34:08.526112 140532797925120 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.2603431940078735, loss=2.6146481037139893
I0216 12:35:30.665869 140532797925120 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0041117668151855, loss=2.590308427810669
I0216 12:36:45.781275 140532789532416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8792269825935364, loss=2.6210989952087402
I0216 12:38:00.610070 140532797925120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.853420078754425, loss=2.560365676879883
I0216 12:39:15.522480 140532789532416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9452956914901733, loss=2.593857765197754
I0216 12:40:32.744639 140532797925120 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.2612625360488892, loss=2.4889564514160156
I0216 12:41:57.253309 140532789532416 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.7143096923828125, loss=2.520756721496582
I0216 12:43:21.138052 140532797925120 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.663619875907898, loss=2.5009100437164307
I0216 12:44:45.485077 140532789532416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8170557618141174, loss=2.4538447856903076
I0216 12:45:44.160741 140599226058560 spec.py:321] Evaluating on the training split.
I0216 12:46:36.521043 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 12:47:28.351426 140599226058560 spec.py:349] Evaluating on the test split.
I0216 12:47:54.726902 140599226058560 submission_runner.py:408] Time since start: 7937.76s, 	Step: 9072, 	{'train/ctc_loss': Array(1.9409115, dtype=float32), 'train/wer': 0.5133557669993037, 'validation/ctc_loss': Array(1.8373336, dtype=float32), 'validation/wer': 0.4703650424322002, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.4274611, dtype=float32), 'test/wer': 0.403936384132594, 'test/num_examples': 2472, 'score': 7236.628142595291, 'total_duration': 7937.760040521622, 'accumulated_submission_time': 7236.628142595291, 'accumulated_eval_time': 700.5129299163818, 'accumulated_logging_time': 0.24257588386535645}
I0216 12:47:54.759986 140532797925120 logging_writer.py:48] [9072] accumulated_eval_time=700.512930, accumulated_logging_time=0.242576, accumulated_submission_time=7236.628143, global_step=9072, preemption_count=0, score=7236.628143, test/ctc_loss=1.4274611473083496, test/num_examples=2472, test/wer=0.403936, total_duration=7937.760041, train/ctc_loss=1.9409115314483643, train/wer=0.513356, validation/ctc_loss=1.8373335599899292, validation/num_examples=5348, validation/wer=0.470365
I0216 12:48:16.436876 140532789532416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7113035917282104, loss=2.4476966857910156
I0216 12:49:31.263881 140532797925120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.6106435060501099, loss=2.5016603469848633
I0216 12:50:50.347669 140532797925120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5690950751304626, loss=2.474365234375
I0216 12:52:05.573719 140532789532416 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.048648715019226, loss=2.443634033203125
I0216 12:53:21.033351 140532797925120 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.5932828187942505, loss=2.456888437271118
I0216 12:54:36.537454 140532789532416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.7262468934059143, loss=2.3445637226104736
I0216 12:55:51.868826 140532797925120 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.1869573593139648, loss=2.394118309020996
I0216 12:57:11.039525 140532789532416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9094433188438416, loss=2.3261585235595703
I0216 12:58:34.301382 140532797925120 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.0279366970062256, loss=2.3791451454162598
I0216 12:59:58.497204 140532789532416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7884894013404846, loss=2.3879096508026123
I0216 13:01:21.878325 140532797925120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.5291112065315247, loss=2.417670726776123
I0216 13:02:45.687479 140532789532416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6216259598731995, loss=2.36458683013916
I0216 13:04:13.611393 140532797925120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9036465287208557, loss=2.296912670135498
I0216 13:05:28.943591 140532789532416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9579141736030579, loss=2.3195815086364746
I0216 13:06:44.392086 140532797925120 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.486555814743042, loss=2.430053949356079
I0216 13:07:59.669061 140532789532416 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.0665923357009888, loss=2.3454556465148926
I0216 13:09:15.030480 140532797925120 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.080073356628418, loss=2.281501054763794
I0216 13:10:32.430753 140532789532416 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.198371410369873, loss=2.2636609077453613
I0216 13:11:55.641420 140532797925120 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.7339348196983337, loss=2.339211940765381
I0216 13:11:55.650624 140599226058560 spec.py:321] Evaluating on the training split.
I0216 13:12:49.906405 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 13:13:41.841705 140599226058560 spec.py:349] Evaluating on the test split.
I0216 13:14:08.031522 140599226058560 submission_runner.py:408] Time since start: 9511.06s, 	Step: 10901, 	{'train/ctc_loss': Array(1.5330179, dtype=float32), 'train/wer': 0.4447662118572818, 'validation/ctc_loss': Array(1.5998214, dtype=float32), 'validation/wer': 0.4368054683954932, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2287663, dtype=float32), 'test/wer': 0.37527674527247984, 'test/num_examples': 2472, 'score': 8677.431626081467, 'total_duration': 9511.064285516739, 'accumulated_submission_time': 8677.431626081467, 'accumulated_eval_time': 832.8874080181122, 'accumulated_logging_time': 0.29271388053894043}
I0216 13:14:08.067115 140532797925120 logging_writer.py:48] [10901] accumulated_eval_time=832.887408, accumulated_logging_time=0.292714, accumulated_submission_time=8677.431626, global_step=10901, preemption_count=0, score=8677.431626, test/ctc_loss=1.2287663221359253, test/num_examples=2472, test/wer=0.375277, total_duration=9511.064286, train/ctc_loss=1.533017873764038, train/wer=0.444766, validation/ctc_loss=1.5998214483261108, validation/num_examples=5348, validation/wer=0.436805
I0216 13:15:22.701281 140532789532416 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.0690855979919434, loss=2.284534454345703
I0216 13:16:37.591865 140532797925120 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.3879399299621582, loss=2.275291681289673
I0216 13:17:52.540701 140532789532416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5910578370094299, loss=2.243450880050659
I0216 13:19:10.789575 140532797925120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.7910380959510803, loss=2.235382556915283
I0216 13:20:31.859183 140532797925120 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0794814825057983, loss=2.2683205604553223
I0216 13:21:46.750279 140532789532416 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.8573544025421143, loss=2.331040620803833
I0216 13:23:01.750273 140532797925120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9951426982879639, loss=2.281426191329956
I0216 13:24:16.711701 140532789532416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.7393370866775513, loss=2.2529430389404297
I0216 13:25:33.660967 140532797925120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6061647534370422, loss=2.2919864654541016
I0216 13:26:56.896347 140532789532416 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.1183899641036987, loss=2.2511212825775146
I0216 13:28:21.800091 140532797925120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.7272129654884338, loss=2.2747316360473633
I0216 13:29:45.280375 140532789532416 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.3002368211746216, loss=2.20794939994812
I0216 13:31:09.403309 140532797925120 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0870412588119507, loss=2.2198803424835205
I0216 13:32:32.390635 140532789532416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9936036467552185, loss=2.22985577583313
I0216 13:33:56.529308 140532797925120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.8071704506874084, loss=2.2048861980438232
I0216 13:35:11.529519 140532789532416 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.3012865781784058, loss=2.2025461196899414
I0216 13:36:26.562694 140532797925120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.8627979159355164, loss=2.2667605876922607
I0216 13:37:41.552797 140532789532416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.7431308627128601, loss=2.214007616043091
I0216 13:38:08.250149 140599226058560 spec.py:321] Evaluating on the training split.
I0216 13:39:01.436442 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 13:39:53.764507 140599226058560 spec.py:349] Evaluating on the test split.
I0216 13:40:20.411119 140599226058560 submission_runner.py:408] Time since start: 11083.44s, 	Step: 12737, 	{'train/ctc_loss': Array(1.5478851, dtype=float32), 'train/wer': 0.4351763341723465, 'validation/ctc_loss': Array(1.5292115, dtype=float32), 'validation/wer': 0.41630863994902345, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1772243, dtype=float32), 'test/wer': 0.3542745719334593, 'test/num_examples': 2472, 'score': 10117.528901100159, 'total_duration': 11083.444188117981, 'accumulated_submission_time': 10117.528901100159, 'accumulated_eval_time': 965.0422995090485, 'accumulated_logging_time': 0.34348201751708984}
I0216 13:40:20.445452 140532797925120 logging_writer.py:48] [12737] accumulated_eval_time=965.042300, accumulated_logging_time=0.343482, accumulated_submission_time=10117.528901, global_step=12737, preemption_count=0, score=10117.528901, test/ctc_loss=1.1772242784500122, test/num_examples=2472, test/wer=0.354275, total_duration=11083.444188, train/ctc_loss=1.5478850603103638, train/wer=0.435176, validation/ctc_loss=1.5292115211486816, validation/num_examples=5348, validation/wer=0.416309
I0216 13:41:08.307407 140532789532416 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9277955889701843, loss=2.2008485794067383
I0216 13:42:23.219808 140532797925120 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.0962506532669067, loss=2.264169692993164
I0216 13:43:38.286684 140532789532416 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9190430641174316, loss=2.1956987380981445
I0216 13:44:55.843929 140532797925120 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.1817073822021484, loss=2.2129065990448
I0216 13:46:18.954227 140532789532416 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.71779465675354, loss=2.2078614234924316
I0216 13:47:42.148782 140532797925120 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.6135741472244263, loss=2.2446844577789307
I0216 13:49:09.764996 140532797925120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8214699029922485, loss=2.1926662921905518
I0216 13:50:24.732578 140532789532416 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3857505321502686, loss=2.1871707439422607
I0216 13:51:39.775603 140532797925120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.615764319896698, loss=2.2074036598205566
I0216 13:52:54.760054 140532789532416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.8258517980575562, loss=2.129436492919922
I0216 13:54:09.867657 140532797925120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.6759927868843079, loss=2.1139941215515137
I0216 13:55:30.078454 140532789532416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.9441953301429749, loss=2.153175115585327
I0216 13:56:53.328469 140532797925120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9427561163902283, loss=2.1630196571350098
I0216 13:58:17.016837 140532789532416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5012771487236023, loss=2.1231253147125244
I0216 13:59:40.697083 140532797925120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.86952805519104, loss=2.1769495010375977
I0216 14:01:04.113251 140532789532416 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.4329954385757446, loss=2.1299824714660645
I0216 14:02:27.620419 140532797925120 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0764427185058594, loss=2.170830249786377
I0216 14:03:48.613856 140532797925120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8985648155212402, loss=2.0535008907318115
I0216 14:04:20.651087 140599226058560 spec.py:321] Evaluating on the training split.
I0216 14:05:12.850283 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 14:06:04.483854 140599226058560 spec.py:349] Evaluating on the test split.
I0216 14:06:31.094310 140599226058560 submission_runner.py:408] Time since start: 12654.13s, 	Step: 14544, 	{'train/ctc_loss': Array(1.2687769, dtype=float32), 'train/wer': 0.38057421766114674, 'validation/ctc_loss': Array(1.2760807, dtype=float32), 'validation/wer': 0.363468723751412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9283827, dtype=float32), 'test/wer': 0.2951881867852863, 'test/num_examples': 2472, 'score': 11557.644403457642, 'total_duration': 12654.127409219742, 'accumulated_submission_time': 11557.644403457642, 'accumulated_eval_time': 1095.4794552326202, 'accumulated_logging_time': 0.39788150787353516}
I0216 14:06:31.128865 140532797925120 logging_writer.py:48] [14544] accumulated_eval_time=1095.479455, accumulated_logging_time=0.397882, accumulated_submission_time=11557.644403, global_step=14544, preemption_count=0, score=11557.644403, test/ctc_loss=0.9283826947212219, test/num_examples=2472, test/wer=0.295188, total_duration=12654.127409, train/ctc_loss=1.2687768936157227, train/wer=0.380574, validation/ctc_loss=1.2760807275772095, validation/num_examples=5348, validation/wer=0.363469
I0216 14:07:13.757360 140532789532416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.7274430990219116, loss=2.165497303009033
I0216 14:08:28.707800 140532797925120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7786643505096436, loss=2.1474416255950928
I0216 14:09:43.830151 140532789532416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.6985086798667908, loss=2.166964054107666
I0216 14:10:58.872309 140532797925120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6269523501396179, loss=2.0970237255096436
I0216 14:12:13.935679 140532789532416 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.1251325607299805, loss=2.121638298034668
I0216 14:13:33.855448 140532797925120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9835066199302673, loss=2.118781566619873
I0216 14:14:57.251812 140532789532416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.557710587978363, loss=2.1388731002807617
I0216 14:16:20.650078 140532797925120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.8352131843566895, loss=2.051837921142578
I0216 14:17:44.114603 140532789532416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.8326206803321838, loss=2.046290636062622
I0216 14:19:07.364184 140532797925120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8029657602310181, loss=2.146803617477417
I0216 14:20:22.524367 140532789532416 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.1368577480316162, loss=2.101034164428711
I0216 14:21:37.768962 140532797925120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.7185680866241455, loss=2.080423593521118
I0216 14:22:53.062773 140532789532416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7331739068031311, loss=2.0554306507110596
I0216 14:24:08.240649 140532797925120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6745644807815552, loss=2.0812973976135254
I0216 14:25:30.282829 140532789532416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.8007782697677612, loss=2.0729997158050537
I0216 14:26:54.654664 140532797925120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6437403559684753, loss=2.0846095085144043
I0216 14:28:17.381911 140532789532416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.963808000087738, loss=2.099571466445923
I0216 14:29:40.400445 140532797925120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5494470596313477, loss=2.0294220447540283
I0216 14:30:31.464241 140599226058560 spec.py:321] Evaluating on the training split.
I0216 14:31:26.067573 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 14:32:17.626184 140599226058560 spec.py:349] Evaluating on the test split.
I0216 14:32:43.916340 140599226058560 submission_runner.py:408] Time since start: 14226.95s, 	Step: 16362, 	{'train/ctc_loss': Array(1.0706007, dtype=float32), 'train/wer': 0.3385405215249989, 'validation/ctc_loss': Array(1.1228197, dtype=float32), 'validation/wer': 0.33144423955125174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8068257, dtype=float32), 'test/wer': 0.2676659963845388, 'test/num_examples': 2472, 'score': 12997.894040346146, 'total_duration': 14226.949261188507, 'accumulated_submission_time': 12997.894040346146, 'accumulated_eval_time': 1227.9253115653992, 'accumulated_logging_time': 0.447887659072876}
I0216 14:32:43.953237 140532797925120 logging_writer.py:48] [16362] accumulated_eval_time=1227.925312, accumulated_logging_time=0.447888, accumulated_submission_time=12997.894040, global_step=16362, preemption_count=0, score=12997.894040, test/ctc_loss=0.8068256974220276, test/num_examples=2472, test/wer=0.267666, total_duration=14226.949261, train/ctc_loss=1.0706007480621338, train/wer=0.338541, validation/ctc_loss=1.1228196620941162, validation/num_examples=5348, validation/wer=0.331444
I0216 14:33:13.219006 140532789532416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.8834580183029175, loss=2.1479227542877197
I0216 14:34:32.325993 140532797925120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5965188145637512, loss=2.0545294284820557
I0216 14:35:47.752210 140532789532416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5270485877990723, loss=2.092965602874756
I0216 14:37:02.983196 140532797925120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.766230583190918, loss=2.036571741104126
I0216 14:38:18.142289 140532789532416 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.038921594619751, loss=2.0670342445373535
I0216 14:39:33.322828 140532797925120 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5044258236885071, loss=2.0840952396392822
I0216 14:40:55.183507 140532789532416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5808796882629395, loss=2.0532491207122803
I0216 14:42:18.949723 140532797925120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9204270839691162, loss=2.0894534587860107
I0216 14:43:42.764500 140532789532416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5274191498756409, loss=2.0896401405334473
I0216 14:45:06.374505 140532797925120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.9459673166275024, loss=2.009434461593628
I0216 14:46:30.538471 140532789532416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.9908249974250793, loss=2.1051716804504395
I0216 14:47:54.289453 140532797925120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8886914253234863, loss=1.992318034172058
I0216 14:49:14.197517 140532797925120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.973812460899353, loss=2.0924198627471924
I0216 14:50:29.330132 140532789532416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.48578763008117676, loss=2.0989885330200195
I0216 14:51:44.779028 140532797925120 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.1865832805633545, loss=2.0671885013580322
I0216 14:52:59.967099 140532789532416 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0931776762008667, loss=2.010438919067383
I0216 14:54:19.168923 140532797925120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7513918280601501, loss=2.0277762413024902
I0216 14:55:42.174986 140532789532416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5604361891746521, loss=2.001534938812256
I0216 14:56:44.141680 140599226058560 spec.py:321] Evaluating on the training split.
I0216 14:57:37.397015 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 14:58:29.159233 140599226058560 spec.py:349] Evaluating on the test split.
I0216 14:58:55.070687 140599226058560 submission_runner.py:408] Time since start: 15798.10s, 	Step: 18176, 	{'train/ctc_loss': Array(0.96476525, dtype=float32), 'train/wer': 0.3123660160216631, 'validation/ctc_loss': Array(1.0905648, dtype=float32), 'validation/wer': 0.3211427247361866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7684886, dtype=float32), 'test/wer': 0.2561696423130827, 'test/num_examples': 2472, 'score': 14437.993826389313, 'total_duration': 15798.103556871414, 'accumulated_submission_time': 14437.993826389313, 'accumulated_eval_time': 1358.848022699356, 'accumulated_logging_time': 0.5031416416168213}
I0216 14:58:55.105240 140532797925120 logging_writer.py:48] [18176] accumulated_eval_time=1358.848023, accumulated_logging_time=0.503142, accumulated_submission_time=14437.993826, global_step=18176, preemption_count=0, score=14437.993826, test/ctc_loss=0.7684885859489441, test/num_examples=2472, test/wer=0.256170, total_duration=15798.103557, train/ctc_loss=0.9647652506828308, train/wer=0.312366, validation/ctc_loss=1.0905648469924927, validation/num_examples=5348, validation/wer=0.321143
I0216 14:59:13.859560 140532789532416 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9208776354789734, loss=2.0341174602508545
I0216 15:00:29.017961 140532797925120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6971055865287781, loss=1.9663981199264526
I0216 15:01:44.322713 140532789532416 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.6177011728286743, loss=2.040696144104004
I0216 15:02:59.689838 140532797925120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.8592016100883484, loss=1.9885019063949585
I0216 15:04:19.059214 140532797925120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5001022815704346, loss=1.9635255336761475
I0216 15:05:34.313919 140532789532416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9326683878898621, loss=2.0545408725738525
I0216 15:06:49.702681 140532797925120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5305277705192566, loss=1.9550169706344604
I0216 15:08:05.030357 140532789532416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.5246685743331909, loss=1.9786359071731567
I0216 15:09:23.329330 140532797925120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8414838910102844, loss=1.9728407859802246
I0216 15:10:45.977021 140532789532416 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0870076417922974, loss=2.0078980922698975
I0216 15:12:09.644927 140532797925120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8143119215965271, loss=1.9574041366577148
I0216 15:13:33.598346 140532789532416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.9692122340202332, loss=2.034296989440918
I0216 15:14:57.637555 140532797925120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.7835074663162231, loss=2.0137033462524414
I0216 15:16:22.431248 140532789532416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.664095401763916, loss=1.9743040800094604
I0216 15:17:46.935229 140532797925120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.7920206189155579, loss=1.9935442209243774
I0216 15:19:01.980861 140532789532416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6623567342758179, loss=1.9849741458892822
I0216 15:20:17.074530 140532797925120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.530357837677002, loss=1.952703833580017
I0216 15:21:32.198979 140532789532416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.8736926913261414, loss=1.9881564378738403
I0216 15:22:47.384235 140532797925120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7166757583618164, loss=1.9443764686584473
I0216 15:22:55.371593 140599226058560 spec.py:321] Evaluating on the training split.
I0216 15:23:47.594126 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 15:24:38.847443 140599226058560 spec.py:349] Evaluating on the test split.
I0216 15:25:05.242994 140599226058560 submission_runner.py:408] Time since start: 17368.28s, 	Step: 20012, 	{'train/ctc_loss': Array(0.8736541, dtype=float32), 'train/wer': 0.2842924222705095, 'validation/ctc_loss': Array(1.0145409, dtype=float32), 'validation/wer': 0.30124448477943944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.715538, dtype=float32), 'test/wer': 0.23638616375195498, 'test/num_examples': 2472, 'score': 15878.171899795532, 'total_duration': 17368.276427268982, 'accumulated_submission_time': 15878.171899795532, 'accumulated_eval_time': 1488.713681936264, 'accumulated_logging_time': 0.5551145076751709}
I0216 15:25:05.277303 140532797925120 logging_writer.py:48] [20012] accumulated_eval_time=1488.713682, accumulated_logging_time=0.555115, accumulated_submission_time=15878.171900, global_step=20012, preemption_count=0, score=15878.171900, test/ctc_loss=0.7155380249023438, test/num_examples=2472, test/wer=0.236386, total_duration=17368.276427, train/ctc_loss=0.8736541271209717, train/wer=0.284292, validation/ctc_loss=1.0145409107208252, validation/num_examples=5348, validation/wer=0.301244
I0216 15:26:12.405929 140532789532416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.7337549924850464, loss=1.952457308769226
I0216 15:27:27.781406 140532797925120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.9373196959495544, loss=1.9939976930618286
I0216 15:28:43.168494 140532789532416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5820019841194153, loss=1.9002634286880493
I0216 15:29:59.619519 140532797925120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7132259607315063, loss=1.9766234159469604
I0216 15:31:23.770188 140532789532416 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6978312730789185, loss=2.0485165119171143
I0216 15:32:51.510849 140532797925120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.52980637550354, loss=1.895526647567749
I0216 15:34:06.795878 140532789532416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.8520155549049377, loss=1.9386062622070312
I0216 15:35:22.073090 140532797925120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6392400860786438, loss=2.0386383533477783
I0216 15:36:37.285784 140532789532416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.6782696843147278, loss=1.9508014917373657
I0216 15:37:52.453051 140532797925120 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.333138346672058, loss=1.9749276638031006
I0216 15:39:11.843389 140532789532416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8546817302703857, loss=1.9797600507736206
I0216 15:40:34.748056 140532797925120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7756649255752563, loss=1.9812647104263306
I0216 15:41:56.854282 140532789532416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.5597084164619446, loss=1.9091904163360596
I0216 15:43:20.554309 140532797925120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7398703694343567, loss=2.0088367462158203
I0216 15:44:43.825788 140532789532416 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0603301525115967, loss=1.9364368915557861
I0216 15:46:06.448940 140532797925120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8134307861328125, loss=1.9781073331832886
I0216 15:47:27.435200 140532797925120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5237166881561279, loss=1.9720789194107056
I0216 15:48:42.629264 140532789532416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.796097993850708, loss=1.9888867139816284
I0216 15:49:05.665840 140599226058560 spec.py:321] Evaluating on the training split.
I0216 15:49:58.795709 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 15:50:50.258227 140599226058560 spec.py:349] Evaluating on the test split.
I0216 15:51:17.078422 140599226058560 submission_runner.py:408] Time since start: 18940.11s, 	Step: 21832, 	{'train/ctc_loss': Array(0.9753044, dtype=float32), 'train/wer': 0.3114894714417061, 'validation/ctc_loss': Array(1.0329162, dtype=float32), 'validation/wer': 0.30400571555461153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.72071874, dtype=float32), 'test/wer': 0.23701582272053298, 'test/num_examples': 2472, 'score': 17318.47605085373, 'total_duration': 18940.11133503914, 'accumulated_submission_time': 17318.47605085373, 'accumulated_eval_time': 1620.1200077533722, 'accumulated_logging_time': 0.6046969890594482}
I0216 15:51:17.111044 140532797925120 logging_writer.py:48] [21832] accumulated_eval_time=1620.120008, accumulated_logging_time=0.604697, accumulated_submission_time=17318.476051, global_step=21832, preemption_count=0, score=17318.476051, test/ctc_loss=0.7207187414169312, test/num_examples=2472, test/wer=0.237016, total_duration=18940.111335, train/ctc_loss=0.9753044247627258, train/wer=0.311489, validation/ctc_loss=1.0329161882400513, validation/num_examples=5348, validation/wer=0.304006
I0216 15:52:08.842558 140532789532416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.7067624926567078, loss=1.9166947603225708
I0216 15:53:23.830892 140532797925120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.6638680100440979, loss=2.02474308013916
I0216 15:54:39.101207 140532789532416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.7513020634651184, loss=1.9888513088226318
I0216 15:55:54.283334 140532797925120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.5689351558685303, loss=1.9371508359909058
I0216 15:57:10.611068 140532789532416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6336575746536255, loss=2.007821798324585
I0216 15:58:34.321578 140532797925120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.6958770751953125, loss=1.8998661041259766
I0216 15:59:57.473434 140532789532416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5304962396621704, loss=1.9233098030090332
I0216 16:01:20.284963 140532797925120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5536155104637146, loss=1.9738153219223022
I0216 16:02:43.671488 140532797925120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.996366024017334, loss=1.9903757572174072
I0216 16:03:58.876202 140532789532416 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0772432088851929, loss=1.9623404741287231
I0216 16:05:14.051524 140532797925120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9475308656692505, loss=1.866550087928772
I0216 16:06:29.406111 140532789532416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.607802152633667, loss=1.9622684717178345
I0216 16:07:44.716166 140532797925120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.8643584251403809, loss=1.9167985916137695
I0216 16:09:04.571053 140532789532416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8648585081100464, loss=1.9446954727172852
I0216 16:10:28.065186 140532797925120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8356899619102478, loss=1.9775515794754028
I0216 16:11:52.656531 140532789532416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.7233278751373291, loss=1.8785569667816162
I0216 16:13:16.139066 140532797925120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9267418384552002, loss=1.8892070055007935
I0216 16:14:39.974453 140532789532416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.7581580877304077, loss=1.9306539297103882
I0216 16:15:17.419605 140599226058560 spec.py:321] Evaluating on the training split.
I0216 16:16:11.301358 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 16:17:03.354623 140599226058560 spec.py:349] Evaluating on the test split.
I0216 16:17:29.784164 140599226058560 submission_runner.py:408] Time since start: 20512.82s, 	Step: 23646, 	{'train/ctc_loss': Array(0.8934462, dtype=float32), 'train/wer': 0.28510904762442035, 'validation/ctc_loss': Array(0.9852448, dtype=float32), 'validation/wer': 0.29178292478059803, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6862627, dtype=float32), 'test/wer': 0.22722564133812687, 'test/num_examples': 2472, 'score': 18758.696970939636, 'total_duration': 20512.817051410675, 'accumulated_submission_time': 18758.696970939636, 'accumulated_eval_time': 1752.4782774448395, 'accumulated_logging_time': 0.654998779296875}
I0216 16:17:29.822338 140532797925120 logging_writer.py:48] [23646] accumulated_eval_time=1752.478277, accumulated_logging_time=0.654999, accumulated_submission_time=18758.696971, global_step=23646, preemption_count=0, score=18758.696971, test/ctc_loss=0.6862627267837524, test/num_examples=2472, test/wer=0.227226, total_duration=20512.817051, train/ctc_loss=0.8934462070465088, train/wer=0.285109, validation/ctc_loss=0.9852448105812073, validation/num_examples=5348, validation/wer=0.291783
I0216 16:18:15.166664 140532797925120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.7325564026832581, loss=1.9452310800552368
I0216 16:19:30.382181 140532789532416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.7326394319534302, loss=1.9448577165603638
I0216 16:20:45.680446 140532797925120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9160169959068298, loss=1.9149894714355469
I0216 16:22:01.043794 140532789532416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6854348182678223, loss=1.9234528541564941
I0216 16:23:16.422267 140532797925120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5725833773612976, loss=1.8831140995025635
I0216 16:24:38.066729 140532789532416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6924945712089539, loss=1.9231507778167725
I0216 16:26:00.917135 140532797925120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.7812680602073669, loss=2.053449869155884
I0216 16:27:24.717833 140532789532416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.9317803978919983, loss=1.9691048860549927
I0216 16:28:48.181362 140532797925120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5611037015914917, loss=1.859213948249817
I0216 16:30:12.173000 140532789532416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.6444629430770874, loss=1.9208054542541504
I0216 16:31:35.440575 140532797925120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.864995002746582, loss=1.9002885818481445
I0216 16:32:56.407003 140532797925120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6134578585624695, loss=1.9765470027923584
I0216 16:34:11.474884 140532789532416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.6408587098121643, loss=1.8831027746200562
I0216 16:35:26.666300 140532797925120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7569403648376465, loss=1.928767442703247
I0216 16:36:41.852372 140532789532416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.8293033242225647, loss=1.883784294128418
I0216 16:37:57.914466 140532797925120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.6882867217063904, loss=1.8803263902664185
I0216 16:39:21.778433 140532789532416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.6244137287139893, loss=1.8476324081420898
I0216 16:40:45.286051 140532797925120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.7303742170333862, loss=1.859825849533081
I0216 16:41:29.849463 140599226058560 spec.py:321] Evaluating on the training split.
I0216 16:42:23.789483 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 16:43:15.718848 140599226058560 spec.py:349] Evaluating on the test split.
I0216 16:43:42.106016 140599226058560 submission_runner.py:408] Time since start: 22085.14s, 	Step: 25454, 	{'train/ctc_loss': Array(0.8983038, dtype=float32), 'train/wer': 0.2872240604474295, 'validation/ctc_loss': Array(0.96987027, dtype=float32), 'validation/wer': 0.28998715931143015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66787153, dtype=float32), 'test/wer': 0.2244835780878679, 'test/num_examples': 2472, 'score': 20198.639166355133, 'total_duration': 22085.138073682785, 'accumulated_submission_time': 20198.639166355133, 'accumulated_eval_time': 1884.7277166843414, 'accumulated_logging_time': 0.7082116603851318}
I0216 16:43:42.140767 140532797925120 logging_writer.py:48] [25454] accumulated_eval_time=1884.727717, accumulated_logging_time=0.708212, accumulated_submission_time=20198.639166, global_step=25454, preemption_count=0, score=20198.639166, test/ctc_loss=0.6678715348243713, test/num_examples=2472, test/wer=0.224484, total_duration=22085.138074, train/ctc_loss=0.8983038067817688, train/wer=0.287224, validation/ctc_loss=0.9698702692985535, validation/num_examples=5348, validation/wer=0.289987
I0216 16:44:17.481440 140532789532416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8781152367591858, loss=1.9058247804641724
I0216 16:45:32.711884 140532797925120 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.030056118965149, loss=1.854867696762085
I0216 16:46:47.996643 140532789532416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.685461163520813, loss=1.8555614948272705
I0216 16:48:07.077976 140532797925120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.8519496321678162, loss=1.8688738346099854
I0216 16:49:22.066537 140532789532416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.7636652588844299, loss=1.8974906206130981
I0216 16:50:37.434043 140532797925120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7706483602523804, loss=1.8965247869491577
I0216 16:51:52.594290 140532789532416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6148083806037903, loss=1.844769835472107
I0216 16:53:07.966627 140532797925120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.5682249069213867, loss=1.947851300239563
I0216 16:54:30.317064 140532789532416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7564064264297485, loss=1.877763032913208
I0216 16:55:54.661809 140532797925120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8835342526435852, loss=1.8563342094421387
I0216 16:57:17.792105 140532789532416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5896455645561218, loss=1.8640378713607788
I0216 16:58:41.413536 140532797925120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.918613612651825, loss=1.8770467042922974
I0216 17:00:04.507020 140532789532416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.7126546502113342, loss=1.9373464584350586
I0216 17:01:30.616772 140532797925120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.5427477955818176, loss=1.8401137590408325
I0216 17:02:45.794503 140532789532416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5669901967048645, loss=1.8389595746994019
I0216 17:04:01.115449 140532797925120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6425662636756897, loss=1.8796398639678955
I0216 17:05:16.402348 140532789532416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.6985716223716736, loss=1.8568534851074219
I0216 17:06:31.915213 140532797925120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.5987120866775513, loss=1.89342200756073
I0216 17:07:42.528499 140599226058560 spec.py:321] Evaluating on the training split.
I0216 17:08:36.828960 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 17:09:28.616783 140599226058560 spec.py:349] Evaluating on the test split.
I0216 17:09:54.967159 140599226058560 submission_runner.py:408] Time since start: 23658.00s, 	Step: 27290, 	{'train/ctc_loss': Array(0.7946374, dtype=float32), 'train/wer': 0.26072332908574836, 'validation/ctc_loss': Array(0.9428824, dtype=float32), 'validation/wer': 0.2765865008640914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64174926, dtype=float32), 'test/wer': 0.2136778177238844, 'test/num_examples': 2472, 'score': 21638.938226938248, 'total_duration': 23658.000257968903, 'accumulated_submission_time': 21638.938226938248, 'accumulated_eval_time': 2017.1603062152863, 'accumulated_logging_time': 0.7608418464660645}
I0216 17:09:55.003799 140532797925120 logging_writer.py:48] [27290] accumulated_eval_time=2017.160306, accumulated_logging_time=0.760842, accumulated_submission_time=21638.938227, global_step=27290, preemption_count=0, score=21638.938227, test/ctc_loss=0.6417492628097534, test/num_examples=2472, test/wer=0.213678, total_duration=23658.000258, train/ctc_loss=0.7946373820304871, train/wer=0.260723, validation/ctc_loss=0.9428824186325073, validation/num_examples=5348, validation/wer=0.276587
I0216 17:10:03.346127 140532789532416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5988985896110535, loss=1.8659857511520386
I0216 17:11:18.341860 140532797925120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5792663097381592, loss=1.885467529296875
I0216 17:12:33.424353 140532789532416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5942610502243042, loss=1.830125093460083
I0216 17:13:48.589731 140532797925120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.8506959676742554, loss=1.9690338373184204
I0216 17:15:06.906139 140532789532416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.5209400653839111, loss=1.8643560409545898
I0216 17:16:30.054111 140532797925120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.637233316898346, loss=1.895587682723999
I0216 17:17:49.924309 140532797925120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.5851693153381348, loss=1.812071681022644
I0216 17:19:05.109685 140532789532416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.8572134375572205, loss=1.8965885639190674
I0216 17:20:20.290049 140532797925120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.6468956470489502, loss=1.905171275138855
I0216 17:21:35.588717 140532789532416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.8079184889793396, loss=1.8334858417510986
I0216 17:22:52.017566 140532797925120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5978752374649048, loss=1.8802157640457153
I0216 17:24:16.008695 140532789532416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.8802003264427185, loss=1.8960002660751343
I0216 17:25:39.224136 140532797925120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6709821224212646, loss=1.8885005712509155
I0216 17:27:02.776652 140532789532416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.631610095500946, loss=1.9278461933135986
I0216 17:28:26.068281 140532797925120 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.006377100944519, loss=1.8823844194412231
I0216 17:29:49.354362 140532789532416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.5796868205070496, loss=1.9036418199539185
I0216 17:31:11.911645 140532797925120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.591533362865448, loss=1.8739306926727295
I0216 17:32:27.153882 140532789532416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6707715392112732, loss=1.8187930583953857
I0216 17:33:42.367361 140532797925120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5683894157409668, loss=1.79352605342865
I0216 17:33:55.647699 140599226058560 spec.py:321] Evaluating on the training split.
I0216 17:34:48.966345 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 17:35:40.211227 140599226058560 spec.py:349] Evaluating on the test split.
I0216 17:36:06.815423 140599226058560 submission_runner.py:408] Time since start: 25229.85s, 	Step: 29119, 	{'train/ctc_loss': Array(0.7964099, dtype=float32), 'train/wer': 0.26791169391094555, 'validation/ctc_loss': Array(0.9283386, dtype=float32), 'validation/wer': 0.2796470258841249, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6265644, dtype=float32), 'test/wer': 0.21237787662746532, 'test/num_examples': 2472, 'score': 23079.49067544937, 'total_duration': 25229.84836292267, 'accumulated_submission_time': 23079.49067544937, 'accumulated_eval_time': 2148.3218274116516, 'accumulated_logging_time': 0.8176655769348145}
I0216 17:36:06.852390 140532797925120 logging_writer.py:48] [29119] accumulated_eval_time=2148.321827, accumulated_logging_time=0.817666, accumulated_submission_time=23079.490675, global_step=29119, preemption_count=0, score=23079.490675, test/ctc_loss=0.6265643835067749, test/num_examples=2472, test/wer=0.212378, total_duration=25229.848363, train/ctc_loss=0.7964099049568176, train/wer=0.267912, validation/ctc_loss=0.9283385872840881, validation/num_examples=5348, validation/wer=0.279647
I0216 17:37:08.339423 140532789532416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9783646464347839, loss=1.9400290250778198
I0216 17:38:23.550695 140532797925120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6722703576087952, loss=1.827079176902771
I0216 17:39:38.725951 140532789532416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.6685731410980225, loss=1.804905891418457
I0216 17:40:54.340610 140532797925120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.7046714425086975, loss=1.8151707649230957
I0216 17:42:16.831004 140532789532416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6864615678787231, loss=1.8687397241592407
I0216 17:43:40.183057 140532797925120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.8371204137802124, loss=1.8715708255767822
I0216 17:45:03.749552 140532789532416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.751227080821991, loss=1.7829205989837646
I0216 17:46:29.150387 140532797925120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.8811357021331787, loss=1.8127241134643555
I0216 17:47:44.289541 140532789532416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7354342937469482, loss=1.8015254735946655
I0216 17:48:59.476121 140532797925120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.5969581604003906, loss=1.8631404638290405
I0216 17:50:14.787567 140532789532416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.697532057762146, loss=1.8204045295715332
I0216 17:51:29.977429 140532797925120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.9526034593582153, loss=1.8412261009216309
I0216 17:52:47.594317 140532789532416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5894861817359924, loss=1.7890315055847168
I0216 17:54:10.826379 140532797925120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6227132081985474, loss=1.8461521863937378
I0216 17:55:33.563261 140532789532416 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.176735758781433, loss=1.7942578792572021
I0216 17:56:56.460127 140532797925120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6503598690032959, loss=1.858746886253357
I0216 17:58:19.533652 140532789532416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.9218605756759644, loss=1.8000943660736084
I0216 17:59:45.619959 140532797925120 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.7229353785514832, loss=1.8137646913528442
I0216 18:00:07.095533 140599226058560 spec.py:321] Evaluating on the training split.
I0216 18:00:59.388113 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 18:01:51.187056 140599226058560 spec.py:349] Evaluating on the test split.
I0216 18:02:17.845434 140599226058560 submission_runner.py:408] Time since start: 26800.88s, 	Step: 30930, 	{'train/ctc_loss': Array(0.81224275, dtype=float32), 'train/wer': 0.2673072213330903, 'validation/ctc_loss': Array(0.9076635, dtype=float32), 'validation/wer': 0.27059096131380517, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.614802, dtype=float32), 'test/wer': 0.2055531858712652, 'test/num_examples': 2472, 'score': 24519.64860892296, 'total_duration': 26800.878132104874, 'accumulated_submission_time': 24519.64860892296, 'accumulated_eval_time': 2279.065259218216, 'accumulated_logging_time': 0.8696136474609375}
I0216 18:02:17.885620 140532797925120 logging_writer.py:48] [30930] accumulated_eval_time=2279.065259, accumulated_logging_time=0.869614, accumulated_submission_time=24519.648609, global_step=30930, preemption_count=0, score=24519.648609, test/ctc_loss=0.6148020029067993, test/num_examples=2472, test/wer=0.205553, total_duration=26800.878132, train/ctc_loss=0.8122427463531494, train/wer=0.267307, validation/ctc_loss=0.9076635241508484, validation/num_examples=5348, validation/wer=0.270591
I0216 18:03:11.036709 140532789532416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6347264647483826, loss=1.7937536239624023
I0216 18:04:25.971310 140532797925120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.6473787426948547, loss=1.8025691509246826
I0216 18:05:41.015197 140532789532416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6464888453483582, loss=1.7535284757614136
I0216 18:06:56.076210 140532797925120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7875640392303467, loss=1.862385630607605
I0216 18:08:11.267746 140532789532416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.8916919827461243, loss=1.8702484369277954
I0216 18:09:26.377695 140532797925120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6641903519630432, loss=1.843883991241455
I0216 18:10:42.825201 140532789532416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.6984338760375977, loss=1.7808560132980347
I0216 18:12:06.620186 140532797925120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.7448503971099854, loss=1.8195761442184448
I0216 18:13:30.905833 140532789532416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.824586808681488, loss=1.8001021146774292
I0216 18:14:55.119925 140532797925120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.74595046043396, loss=1.9529350996017456
I0216 18:16:15.913382 140532797925120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6138212084770203, loss=1.7499521970748901
I0216 18:17:30.991981 140532789532416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6059197783470154, loss=1.8033119440078735
I0216 18:18:46.148908 140532797925120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.7206038236618042, loss=1.8375811576843262
I0216 18:20:01.328640 140532789532416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.8304389715194702, loss=1.7718783617019653
I0216 18:21:16.436283 140532797925120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.6341564059257507, loss=1.7525137662887573
I0216 18:22:31.559406 140532789532416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.7545130252838135, loss=1.8275200128555298
I0216 18:23:53.988946 140532797925120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.8705971837043762, loss=1.7983077764511108
I0216 18:25:16.966990 140532789532416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.6067653298377991, loss=1.8355343341827393
I0216 18:26:18.364593 140599226058560 spec.py:321] Evaluating on the training split.
I0216 18:27:21.666507 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 18:28:13.793256 140599226058560 spec.py:349] Evaluating on the test split.
I0216 18:28:40.243740 140599226058560 submission_runner.py:408] Time since start: 28383.28s, 	Step: 32775, 	{'train/ctc_loss': Array(0.5687455, dtype=float32), 'train/wer': 0.19706803332863815, 'validation/ctc_loss': Array(0.8738757, dtype=float32), 'validation/wer': 0.26107147339660347, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5819507, dtype=float32), 'test/wer': 0.1957630044888591, 'test/num_examples': 2472, 'score': 25960.040961503983, 'total_duration': 28383.277754068375, 'accumulated_submission_time': 25960.040961503983, 'accumulated_eval_time': 2420.939267396927, 'accumulated_logging_time': 0.9254543781280518}
I0216 18:28:40.284999 140532797925120 logging_writer.py:48] [32775] accumulated_eval_time=2420.939267, accumulated_logging_time=0.925454, accumulated_submission_time=25960.040962, global_step=32775, preemption_count=0, score=25960.040962, test/ctc_loss=0.5819507241249084, test/num_examples=2472, test/wer=0.195763, total_duration=28383.277754, train/ctc_loss=0.568745493888855, train/wer=0.197068, validation/ctc_loss=0.8738756775856018, validation/num_examples=5348, validation/wer=0.261071
I0216 18:28:59.849012 140532789532416 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.5627086162567139, loss=1.8417261838912964
I0216 18:30:14.883380 140532797925120 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.8420295715332031, loss=1.7414149045944214
I0216 18:31:33.783955 140532797925120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.6933443546295166, loss=1.8237658739089966
I0216 18:32:49.196249 140532789532416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.8371852040290833, loss=1.826172947883606
I0216 18:34:04.335685 140532797925120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.8519983291625977, loss=1.7972277402877808
I0216 18:35:19.968686 140532789532416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.8596349358558655, loss=1.8113601207733154
I0216 18:36:35.294358 140532797925120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.6432751417160034, loss=1.758538007736206
I0216 18:37:57.206949 140532789532416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7323410511016846, loss=1.7804615497589111
I0216 18:39:21.676629 140532797925120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6256271600723267, loss=1.7466068267822266
I0216 18:40:45.370035 140532789532416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.8956259489059448, loss=1.7758463621139526
I0216 18:42:07.912189 140532797925120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.6369485259056091, loss=1.804822564125061
I0216 18:43:31.752008 140532789532416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.75614994764328, loss=1.7680963277816772
I0216 18:44:57.704438 140532797925120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6424359679222107, loss=1.7422246932983398
I0216 18:46:12.578323 140532789532416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.6040301322937012, loss=1.746480107307434
I0216 18:47:27.892431 140532797925120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.7033511996269226, loss=1.7933752536773682
I0216 18:48:42.846197 140532789532416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.6106860041618347, loss=1.7281463146209717
I0216 18:49:57.911577 140532797925120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.8976962566375732, loss=1.7510703802108765
I0216 18:51:18.162977 140532789532416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.8103094100952148, loss=1.8002957105636597
I0216 18:52:41.108535 140532797925120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.6810784935951233, loss=1.8092381954193115
I0216 18:52:41.117915 140599226058560 spec.py:321] Evaluating on the training split.
I0216 18:53:37.498909 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 18:54:29.819665 140599226058560 spec.py:349] Evaluating on the test split.
I0216 18:54:56.496025 140599226058560 submission_runner.py:408] Time since start: 29959.53s, 	Step: 34601, 	{'train/ctc_loss': Array(0.52129424, dtype=float32), 'train/wer': 0.18640793793824897, 'validation/ctc_loss': Array(0.86678445, dtype=float32), 'validation/wer': 0.2615735153557257, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5760456, dtype=float32), 'test/wer': 0.19541770763512278, 'test/num_examples': 2472, 'score': 27400.785620450974, 'total_duration': 29959.528460025787, 'accumulated_submission_time': 27400.785620450974, 'accumulated_eval_time': 2556.310618162155, 'accumulated_logging_time': 0.9855234622955322}
I0216 18:54:56.535851 140532797925120 logging_writer.py:48] [34601] accumulated_eval_time=2556.310618, accumulated_logging_time=0.985523, accumulated_submission_time=27400.785620, global_step=34601, preemption_count=0, score=27400.785620, test/ctc_loss=0.576045572757721, test/num_examples=2472, test/wer=0.195418, total_duration=29959.528460, train/ctc_loss=0.5212942361831665, train/wer=0.186408, validation/ctc_loss=0.8667844533920288, validation/num_examples=5348, validation/wer=0.261574
I0216 18:56:11.555803 140532789532416 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.6383256316184998, loss=1.7371479272842407
I0216 18:57:26.810175 140532797925120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.6635974049568176, loss=1.7728768587112427
I0216 18:58:41.989702 140532789532416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.5309125185012817, loss=1.7929868698120117
I0216 18:59:59.437987 140532797925120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.8565769195556641, loss=1.7896661758422852
I0216 19:01:19.907605 140532797925120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.7888579964637756, loss=1.751505732536316
I0216 19:02:35.057503 140532789532416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6062674522399902, loss=1.753218650817871
I0216 19:03:50.295491 140532797925120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.8213014602661133, loss=1.7723805904388428
I0216 19:05:05.873599 140532789532416 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.025241494178772, loss=1.742141604423523
I0216 19:06:22.795655 140532797925120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6569538116455078, loss=1.7628074884414673
I0216 19:07:46.914623 140532789532416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5411736965179443, loss=1.7200253009796143
I0216 19:09:10.142732 140532797925120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.7764942049980164, loss=1.794199824333191
I0216 19:10:33.862311 140532789532416 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.0322847366333008, loss=1.7958523035049438
I0216 19:11:58.999958 140532797925120 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7923086881637573, loss=1.8014076948165894
I0216 19:13:23.455442 140532789532416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.7409377098083496, loss=1.7378941774368286
I0216 19:14:46.032472 140532797925120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.8035398721694946, loss=1.765870213508606
I0216 19:16:01.233725 140532789532416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.8531659245491028, loss=1.710548758506775
I0216 19:17:16.565990 140532797925120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.8818431496620178, loss=1.8197256326675415
I0216 19:18:31.785222 140532789532416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.7148253917694092, loss=1.7368731498718262
I0216 19:18:56.969662 140599226058560 spec.py:321] Evaluating on the training split.
I0216 19:19:52.337870 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 19:20:43.876464 140599226058560 spec.py:349] Evaluating on the test split.
I0216 19:21:10.488257 140599226058560 submission_runner.py:408] Time since start: 31533.52s, 	Step: 36435, 	{'train/ctc_loss': Array(0.4877663, dtype=float32), 'train/wer': 0.17128320788859458, 'validation/ctc_loss': Array(0.82872856, dtype=float32), 'validation/wer': 0.24930245131641193, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5492272, dtype=float32), 'test/wer': 0.18739463368066134, 'test/num_examples': 2472, 'score': 28841.119877815247, 'total_duration': 31533.52166032791, 'accumulated_submission_time': 28841.119877815247, 'accumulated_eval_time': 2689.823467731476, 'accumulated_logging_time': 1.0541713237762451}
I0216 19:21:10.526733 140532797925120 logging_writer.py:48] [36435] accumulated_eval_time=2689.823468, accumulated_logging_time=1.054171, accumulated_submission_time=28841.119878, global_step=36435, preemption_count=0, score=28841.119878, test/ctc_loss=0.5492271780967712, test/num_examples=2472, test/wer=0.187395, total_duration=31533.521660, train/ctc_loss=0.487766295671463, train/wer=0.171283, validation/ctc_loss=0.8287285566329956, validation/num_examples=5348, validation/wer=0.249302
I0216 19:21:59.940497 140532789532416 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.6290852427482605, loss=1.7336336374282837
I0216 19:23:15.332917 140532797925120 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.7299617528915405, loss=1.7327781915664673
I0216 19:24:30.475133 140532789532416 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.6065952181816101, loss=1.7452564239501953
I0216 19:25:45.545905 140532797925120 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6301127076148987, loss=1.730924129486084
I0216 19:27:07.606900 140532789532416 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.7288125157356262, loss=1.809078574180603
I0216 19:28:31.082844 140532797925120 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.583070695400238, loss=1.7816745042800903
I0216 19:29:55.085582 140532797925120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.830094039440155, loss=1.7323102951049805
I0216 19:31:10.172116 140532789532416 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.118208885192871, loss=1.7615491151809692
I0216 19:32:25.239883 140532797925120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.707266092300415, loss=1.7366619110107422
I0216 19:33:40.326155 140532789532416 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.8429292440414429, loss=1.7167035341262817
I0216 19:34:55.547421 140532797925120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6170663237571716, loss=1.692500352859497
I0216 19:36:15.209286 140532789532416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.6627535820007324, loss=1.7975356578826904
I0216 19:37:38.260715 140532797925120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5821940302848816, loss=1.7369091510772705
I0216 19:39:01.092562 140532789532416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.6648789048194885, loss=1.698669672012329
I0216 19:40:25.365992 140532797925120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.6954874396324158, loss=1.760304570198059
I0216 19:41:48.813289 140532789532416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8875816464424133, loss=1.7330185174942017
I0216 19:43:12.478407 140532797925120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.7969849109649658, loss=1.7646398544311523
I0216 19:44:31.781550 140532797925120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.9102259874343872, loss=1.6841953992843628
I0216 19:45:11.208087 140599226058560 spec.py:321] Evaluating on the training split.
I0216 19:46:06.850564 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 19:46:58.669229 140599226058560 spec.py:349] Evaluating on the test split.
I0216 19:47:25.514058 140599226058560 submission_runner.py:408] Time since start: 33108.55s, 	Step: 38254, 	{'train/ctc_loss': Array(0.45671743, dtype=float32), 'train/wer': 0.16296851962026954, 'validation/ctc_loss': Array(0.79590267, dtype=float32), 'validation/wer': 0.2387885341340259, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5308841, dtype=float32), 'test/wer': 0.17924969024841061, 'test/num_examples': 2472, 'score': 30281.715041160583, 'total_duration': 33108.547865867615, 'accumulated_submission_time': 30281.715041160583, 'accumulated_eval_time': 2824.1240861415863, 'accumulated_logging_time': 1.1081647872924805}
I0216 19:47:25.555338 140532797925120 logging_writer.py:48] [38254] accumulated_eval_time=2824.124086, accumulated_logging_time=1.108165, accumulated_submission_time=30281.715041, global_step=38254, preemption_count=0, score=30281.715041, test/ctc_loss=0.5308840870857239, test/num_examples=2472, test/wer=0.179250, total_duration=33108.547866, train/ctc_loss=0.45671743154525757, train/wer=0.162969, validation/ctc_loss=0.795902669429779, validation/num_examples=5348, validation/wer=0.238789
I0216 19:48:00.879936 140532789532416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.6624201536178589, loss=1.6937134265899658
I0216 19:49:16.053596 140532797925120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.7552467584609985, loss=1.7319062948226929
I0216 19:50:31.347226 140532789532416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7222725749015808, loss=1.742918848991394
I0216 19:51:46.551414 140532797925120 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.8170566558837891, loss=1.7276897430419922
I0216 19:53:01.713088 140532789532416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6553375720977783, loss=1.7200859785079956
I0216 19:54:22.127589 140532797925120 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.6345531940460205, loss=1.7351943254470825
I0216 19:55:46.346934 140532789532416 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7064387798309326, loss=1.784570336341858
I0216 19:57:09.651801 140532797925120 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7669071555137634, loss=1.6959155797958374
I0216 19:58:33.062639 140532789532416 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.5950130820274353, loss=1.6411926746368408
I0216 19:59:54.935051 140532797925120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6177268624305725, loss=1.7023876905441284
I0216 20:01:10.095363 140532789532416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.7158095836639404, loss=1.6752104759216309
I0216 20:02:25.198025 140532797925120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.6718127727508545, loss=1.6637263298034668
I0216 20:03:40.350560 140532789532416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.7404291033744812, loss=1.7221044301986694
I0216 20:04:56.639626 140532797925120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.784238874912262, loss=1.6321280002593994
I0216 20:06:20.029998 140532789532416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.7814539670944214, loss=1.685435175895691
I0216 20:07:43.833511 140532797925120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5784468054771423, loss=1.6975895166397095
I0216 20:09:07.375934 140532789532416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7077750563621521, loss=1.6968069076538086
I0216 20:10:30.513307 140532797925120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.647394061088562, loss=1.7033191919326782
I0216 20:11:25.776306 140599226058560 spec.py:321] Evaluating on the training split.
I0216 20:12:20.038854 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 20:13:11.954424 140599226058560 spec.py:349] Evaluating on the test split.
I0216 20:13:38.215831 140599226058560 submission_runner.py:408] Time since start: 34681.25s, 	Step: 40068, 	{'train/ctc_loss': Array(0.44774008, dtype=float32), 'train/wer': 0.15976669778082325, 'validation/ctc_loss': Array(0.7690591, dtype=float32), 'validation/wer': 0.23156685364511426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50365597, dtype=float32), 'test/wer': 0.16815956776958543, 'test/num_examples': 2472, 'score': 31721.85123872757, 'total_duration': 34681.24870944023, 'accumulated_submission_time': 31721.85123872757, 'accumulated_eval_time': 2956.557309150696, 'accumulated_logging_time': 1.1653995513916016}
I0216 20:13:38.257192 140532797925120 logging_writer.py:48] [40068] accumulated_eval_time=2956.557309, accumulated_logging_time=1.165400, accumulated_submission_time=31721.851239, global_step=40068, preemption_count=0, score=31721.851239, test/ctc_loss=0.5036559700965881, test/num_examples=2472, test/wer=0.168160, total_duration=34681.248709, train/ctc_loss=0.4477400779724121, train/wer=0.159767, validation/ctc_loss=0.7690591216087341, validation/num_examples=5348, validation/wer=0.231567
I0216 20:14:03.059370 140532789532416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.7740685939788818, loss=1.673774242401123
I0216 20:15:21.871661 140532797925120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.8184047937393188, loss=1.7083660364151
I0216 20:16:36.933411 140532789532416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6988505721092224, loss=1.6906719207763672
I0216 20:17:51.957741 140532797925120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6806684136390686, loss=1.7155300378799438
I0216 20:19:06.892870 140532789532416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6514641642570496, loss=1.6546413898468018
I0216 20:20:22.050733 140532797925120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.6874282956123352, loss=1.6665570735931396
I0216 20:21:44.781215 140532789532416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6455690264701843, loss=1.6312302350997925
I0216 20:23:09.551053 140532797925120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.8025676608085632, loss=1.694408893585205
I0216 20:24:32.964277 140532789532416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.732358992099762, loss=1.758326768875122
I0216 20:25:56.501989 140532797925120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.768764078617096, loss=1.7172738313674927
I0216 20:27:21.881637 140532789532416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5859224796295166, loss=1.666728138923645
I0216 20:28:48.245096 140532797925120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6761441826820374, loss=1.6463298797607422
I0216 20:30:03.661232 140532789532416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.63662189245224, loss=1.6706222295761108
I0216 20:31:18.804184 140532797925120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.6496556997299194, loss=1.6923260688781738
I0216 20:32:34.022351 140532789532416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8215423226356506, loss=1.6658977270126343
I0216 20:33:49.175267 140532797925120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.6629646420478821, loss=1.6584839820861816
I0216 20:35:08.172394 140532789532416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.6471409797668457, loss=1.6804630756378174
I0216 20:36:31.141157 140532797925120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.642210066318512, loss=1.6368027925491333
I0216 20:37:38.453877 140599226058560 spec.py:321] Evaluating on the training split.
I0216 20:38:34.558350 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 20:39:26.069310 140599226058560 spec.py:349] Evaluating on the test split.
I0216 20:39:52.427141 140599226058560 submission_runner.py:408] Time since start: 36255.46s, 	Step: 41881, 	{'train/ctc_loss': Array(0.42459473, dtype=float32), 'train/wer': 0.15427558055342772, 'validation/ctc_loss': Array(0.7605355, dtype=float32), 'validation/wer': 0.2281491064618593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5015915, dtype=float32), 'test/wer': 0.16994698677716166, 'test/num_examples': 2472, 'score': 33161.960060596466, 'total_duration': 36255.45990538597, 'accumulated_submission_time': 33161.960060596466, 'accumulated_eval_time': 3090.524173974991, 'accumulated_logging_time': 1.2250707149505615}
I0216 20:39:52.468914 140532797925120 logging_writer.py:48] [41881] accumulated_eval_time=3090.524174, accumulated_logging_time=1.225071, accumulated_submission_time=33161.960061, global_step=41881, preemption_count=0, score=33161.960061, test/ctc_loss=0.5015915036201477, test/num_examples=2472, test/wer=0.169947, total_duration=36255.459905, train/ctc_loss=0.4245947301387787, train/wer=0.154276, validation/ctc_loss=0.760535478591919, validation/num_examples=5348, validation/wer=0.228149
I0216 20:40:07.546140 140532789532416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.7631117105484009, loss=1.7229336500167847
I0216 20:41:22.820579 140532797925120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5960895419120789, loss=1.6589702367782593
I0216 20:42:38.084170 140532789532416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.7191451787948608, loss=1.676047682762146
I0216 20:43:53.252719 140532797925120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.8329732418060303, loss=1.7002032995224
I0216 20:45:12.359992 140532797925120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7792371511459351, loss=1.6314258575439453
I0216 20:46:27.576743 140532789532416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.7734920978546143, loss=1.6591037511825562
I0216 20:47:43.010371 140532797925120 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.703191876411438, loss=1.6302546262741089
I0216 20:48:58.228175 140532789532416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.64569091796875, loss=1.6262785196304321
I0216 20:50:14.557753 140532797925120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.6245221495628357, loss=1.68039071559906
I0216 20:51:37.661561 140532789532416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.6619340181350708, loss=1.6300466060638428
I0216 20:53:01.687154 140532797925120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.6231177449226379, loss=1.6760637760162354
I0216 20:54:25.727724 140532789532416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.602475106716156, loss=1.7246290445327759
I0216 20:55:49.362028 140532797925120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.7688785195350647, loss=1.687907338142395
I0216 20:57:14.306684 140532789532416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.706845223903656, loss=1.6788452863693237
I0216 20:58:38.502543 140532797925120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.6975039839744568, loss=1.636479139328003
I0216 20:59:53.685141 140532789532416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.831228494644165, loss=1.6024986505508423
I0216 21:01:08.946103 140532797925120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7092365026473999, loss=1.6723389625549316
I0216 21:02:24.491202 140532789532416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.7190229296684265, loss=1.6509562730789185
I0216 21:03:39.804332 140532797925120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6099505424499512, loss=1.597873330116272
I0216 21:03:52.876955 140599226058560 spec.py:321] Evaluating on the training split.
I0216 21:04:47.147424 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 21:05:38.830391 140599226058560 spec.py:349] Evaluating on the test split.
I0216 21:06:05.405996 140599226058560 submission_runner.py:408] Time since start: 37828.44s, 	Step: 43718, 	{'train/ctc_loss': Array(0.4951775, dtype=float32), 'train/wer': 0.17071944294139188, 'validation/ctc_loss': Array(0.7597602, dtype=float32), 'validation/wer': 0.22932697413518446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.492929, dtype=float32), 'test/wer': 0.1679767635529015, 'test/num_examples': 2472, 'score': 34602.280463695526, 'total_duration': 37828.440029382706, 'accumulated_submission_time': 34602.280463695526, 'accumulated_eval_time': 3223.048075437546, 'accumulated_logging_time': 1.28309965133667}
I0216 21:06:05.444167 140532797925120 logging_writer.py:48] [43718] accumulated_eval_time=3223.048075, accumulated_logging_time=1.283100, accumulated_submission_time=34602.280464, global_step=43718, preemption_count=0, score=34602.280464, test/ctc_loss=0.49292901158332825, test/num_examples=2472, test/wer=0.167977, total_duration=37828.440029, train/ctc_loss=0.4951775074005127, train/wer=0.170719, validation/ctc_loss=0.7597602009773254, validation/num_examples=5348, validation/wer=0.229327
I0216 21:07:07.561799 140532789532416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6716858148574829, loss=1.6043882369995117
I0216 21:08:22.800185 140532797925120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.7389186024665833, loss=1.631324291229248
I0216 21:09:38.000752 140532789532416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6939201951026917, loss=1.6389623880386353
I0216 21:10:54.048207 140532797925120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6864280104637146, loss=1.627302885055542
I0216 21:12:17.902150 140532789532416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.8806576728820801, loss=1.620100736618042
I0216 21:13:44.543445 140532797925120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7863995432853699, loss=1.6284559965133667
I0216 21:14:59.600549 140532789532416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.5791769623756409, loss=1.5762629508972168
I0216 21:16:14.682212 140532797925120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7628603577613831, loss=1.591400384902954
I0216 21:17:29.715228 140532789532416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.8044355511665344, loss=1.6358702182769775
I0216 21:18:45.032971 140532797925120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.681174635887146, loss=1.610365390777588
I0216 21:20:03.943207 140532789532416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.692877471446991, loss=1.6695268154144287
I0216 21:21:27.033424 140532797925120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.6897077560424805, loss=1.617730736732483
I0216 21:22:49.732429 140532789532416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.6874707937240601, loss=1.6065977811813354
I0216 21:24:13.212865 140532797925120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.8702516555786133, loss=1.634256362915039
I0216 21:25:37.049220 140532789532416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.6269150972366333, loss=1.6594983339309692
I0216 21:26:59.940781 140532797925120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7144308686256409, loss=1.5957223176956177
I0216 21:28:20.110397 140532797925120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.7163578867912292, loss=1.6695061922073364
I0216 21:29:35.276561 140532789532416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8069880604743958, loss=1.5983669757843018
I0216 21:30:05.797207 140599226058560 spec.py:321] Evaluating on the training split.
I0216 21:31:00.719824 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 21:31:51.586185 140599226058560 spec.py:349] Evaluating on the test split.
I0216 21:32:17.720107 140599226058560 submission_runner.py:408] Time since start: 39400.75s, 	Step: 45542, 	{'train/ctc_loss': Array(0.40693375, dtype=float32), 'train/wer': 0.14851573208885727, 'validation/ctc_loss': Array(0.7193552, dtype=float32), 'validation/wer': 0.21866823715689776, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46914527, dtype=float32), 'test/wer': 0.1600146243373347, 'test/num_examples': 2472, 'score': 36042.547214746475, 'total_duration': 39400.75337386131, 'accumulated_submission_time': 36042.547214746475, 'accumulated_eval_time': 3354.965092897415, 'accumulated_logging_time': 1.3371038436889648}
I0216 21:32:17.762509 140532797925120 logging_writer.py:48] [45542] accumulated_eval_time=3354.965093, accumulated_logging_time=1.337104, accumulated_submission_time=36042.547215, global_step=45542, preemption_count=0, score=36042.547215, test/ctc_loss=0.469145268201828, test/num_examples=2472, test/wer=0.160015, total_duration=39400.753374, train/ctc_loss=0.4069337546825409, train/wer=0.148516, validation/ctc_loss=0.7193552255630493, validation/num_examples=5348, validation/wer=0.218668
I0216 21:33:02.036518 140532789532416 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.556649386882782, loss=1.5464421510696411
I0216 21:34:17.098488 140532797925120 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.6396125555038452, loss=1.5717812776565552
I0216 21:35:32.476502 140532789532416 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.7464171648025513, loss=1.6064773797988892
I0216 21:36:47.664498 140532797925120 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7472675442695618, loss=1.6709344387054443
I0216 21:38:05.630592 140532789532416 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.6923423409461975, loss=1.6592971086502075
I0216 21:39:29.050415 140532797925120 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.7365798354148865, loss=1.6736578941345215
I0216 21:40:52.487624 140532789532416 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.6397485136985779, loss=1.6411434412002563
I0216 21:42:16.099088 140532797925120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.7171129584312439, loss=1.6337045431137085
I0216 21:43:39.574609 140532797925120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.6550126671791077, loss=1.537222981452942
I0216 21:44:54.958518 140532789532416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.67702716588974, loss=1.5804814100265503
I0216 21:46:10.419020 140532797925120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.7097536325454712, loss=1.5820655822753906
I0216 21:47:25.900456 140532789532416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.7055339813232422, loss=1.6180827617645264
I0216 21:48:41.415956 140532797925120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8170779943466187, loss=1.6204133033752441
I0216 21:50:03.752443 140532789532416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7475501894950867, loss=1.5244274139404297
I0216 21:51:27.590840 140532797925120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.6627004742622375, loss=1.5726501941680908
I0216 21:52:51.578068 140532789532416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.7010861039161682, loss=1.5722980499267578
I0216 21:54:15.900080 140532797925120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.8141754269599915, loss=1.5411288738250732
I0216 21:55:39.750735 140532789532416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.6693882942199707, loss=1.5407546758651733
I0216 21:56:18.229914 140599226058560 spec.py:321] Evaluating on the training split.
I0216 21:57:12.902752 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 21:58:04.154647 140599226058560 spec.py:349] Evaluating on the test split.
I0216 21:58:30.209824 140599226058560 submission_runner.py:408] Time since start: 40973.24s, 	Step: 47348, 	{'train/ctc_loss': Array(0.38006338, dtype=float32), 'train/wer': 0.13803585634282162, 'validation/ctc_loss': Array(0.69912624, dtype=float32), 'validation/wer': 0.2128078627494521, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4517525, dtype=float32), 'test/wer': 0.1527633904088721, 'test/num_examples': 2472, 'score': 37482.9290099144, 'total_duration': 40973.24291205406, 'accumulated_submission_time': 37482.9290099144, 'accumulated_eval_time': 3486.9389243125916, 'accumulated_logging_time': 1.3953540325164795}
I0216 21:58:30.252714 140532797925120 logging_writer.py:48] [47348] accumulated_eval_time=3486.938924, accumulated_logging_time=1.395354, accumulated_submission_time=37482.929010, global_step=47348, preemption_count=0, score=37482.929010, test/ctc_loss=0.45175251364707947, test/num_examples=2472, test/wer=0.152763, total_duration=40973.242912, train/ctc_loss=0.38006338477134705, train/wer=0.138036, validation/ctc_loss=0.6991262435913086, validation/num_examples=5348, validation/wer=0.212808
I0216 21:59:13.764421 140532797925120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.7953182458877563, loss=1.520444393157959
I0216 22:00:28.691916 140532789532416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8618977665901184, loss=1.5817137956619263
I0216 22:01:43.703495 140532797925120 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.6379421353340149, loss=1.5468701124191284
I0216 22:02:58.817513 140532789532416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.6994329690933228, loss=1.5182409286499023
I0216 22:04:14.216303 140532797925120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.5969492793083191, loss=1.5253582000732422
I0216 22:05:37.320749 140532789532416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.6901072859764099, loss=1.6412410736083984
I0216 22:07:01.253081 140532797925120 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7097162008285522, loss=1.5830938816070557
I0216 22:08:24.854352 140532789532416 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.6426712870597839, loss=1.5917856693267822
I0216 22:09:48.069259 140532797925120 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.6755041480064392, loss=1.586175799369812
I0216 22:11:10.743586 140532789532416 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.6988832950592041, loss=1.5854367017745972
I0216 22:12:34.037521 140532797925120 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.8321967720985413, loss=1.568169116973877
I0216 22:13:53.159176 140532797925120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.8068833947181702, loss=1.4902617931365967
I0216 22:15:08.352301 140532789532416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.7482663989067078, loss=1.540417194366455
I0216 22:16:23.463969 140532797925120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.6660477519035339, loss=1.6119329929351807
I0216 22:17:38.684884 140532789532416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.865812361240387, loss=1.503545880317688
I0216 22:18:56.048861 140532797925120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.7967139482498169, loss=1.5597580671310425
I0216 22:20:19.318579 140532789532416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.6972273588180542, loss=1.543097972869873
I0216 22:21:43.414900 140532797925120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.7412057518959045, loss=1.6089245080947876
I0216 22:22:30.938780 140599226058560 spec.py:321] Evaluating on the training split.
I0216 22:23:26.001601 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 22:24:17.510183 140599226058560 spec.py:349] Evaluating on the test split.
I0216 22:24:43.735370 140599226058560 submission_runner.py:408] Time since start: 42546.77s, 	Step: 49157, 	{'train/ctc_loss': Array(0.3494525, dtype=float32), 'train/wer': 0.12735696869363772, 'validation/ctc_loss': Array(0.67146903, dtype=float32), 'validation/wer': 0.20352974115875147, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43440887, dtype=float32), 'test/wer': 0.14622306176751365, 'test/num_examples': 2472, 'score': 38923.530285835266, 'total_duration': 42546.768450737, 'accumulated_submission_time': 38923.530285835266, 'accumulated_eval_time': 3619.7294538021088, 'accumulated_logging_time': 1.4531559944152832}
I0216 22:24:43.777475 140532797925120 logging_writer.py:48] [49157] accumulated_eval_time=3619.729454, accumulated_logging_time=1.453156, accumulated_submission_time=38923.530286, global_step=49157, preemption_count=0, score=38923.530286, test/ctc_loss=0.43440887331962585, test/num_examples=2472, test/wer=0.146223, total_duration=42546.768451, train/ctc_loss=0.34945249557495117, train/wer=0.127357, validation/ctc_loss=0.6714690327644348, validation/num_examples=5348, validation/wer=0.203530
I0216 22:25:16.797710 140532789532416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.6415959000587463, loss=1.4944490194320679
I0216 22:26:31.759106 140532797925120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.6963954567909241, loss=1.4976252317428589
I0216 22:27:46.950838 140532789532416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.6943454146385193, loss=1.555227279663086
I0216 22:29:06.262166 140532797925120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.8014983534812927, loss=1.5300050973892212
I0216 22:30:21.401350 140532789532416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6565423607826233, loss=1.495597243309021
I0216 22:31:36.578864 140532797925120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.7457071542739868, loss=1.5585728883743286
I0216 22:32:51.767259 140532789532416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.7379268407821655, loss=1.4897080659866333
I0216 22:34:08.809338 140532797925120 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.7948426008224487, loss=1.5724667310714722
I0216 22:35:31.074532 140532789532416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.7498459219932556, loss=1.589613437652588
I0216 22:36:54.014796 140532797925120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.8955320119857788, loss=1.5474873781204224
I0216 22:38:17.353836 140532789532416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.8417843580245972, loss=1.6105927228927612
I0216 22:39:39.843887 140532797925120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.7765539884567261, loss=1.5133000612258911
I0216 22:41:02.568420 140532789532416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.6956650614738464, loss=1.5432376861572266
I0216 22:42:27.511393 140532797925120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.686263918876648, loss=1.47295343875885
I0216 22:43:42.594361 140532789532416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.7017576694488525, loss=1.5241427421569824
I0216 22:44:58.007472 140532797925120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.7929529547691345, loss=1.5377496480941772
I0216 22:46:13.195767 140532789532416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.6504929661750793, loss=1.4907974004745483
I0216 22:47:28.353676 140532797925120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.802396833896637, loss=1.4948374032974243
I0216 22:48:44.022303 140599226058560 spec.py:321] Evaluating on the training split.
I0216 22:49:38.685940 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 22:50:30.017642 140599226058560 spec.py:349] Evaluating on the test split.
I0216 22:50:56.236850 140599226058560 submission_runner.py:408] Time since start: 44119.27s, 	Step: 50996, 	{'train/ctc_loss': Array(0.33166713, dtype=float32), 'train/wer': 0.12348912059722914, 'validation/ctc_loss': Array(0.6463883, dtype=float32), 'validation/wer': 0.1973024899350242, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4144835, dtype=float32), 'test/wer': 0.1407795584262588, 'test/num_examples': 2472, 'score': 40363.68802213669, 'total_duration': 44119.26902413368, 'accumulated_submission_time': 40363.68802213669, 'accumulated_eval_time': 3751.9370737075806, 'accumulated_logging_time': 1.511134386062622}
I0216 22:50:56.276258 140532797925120 logging_writer.py:48] [50996] accumulated_eval_time=3751.937074, accumulated_logging_time=1.511134, accumulated_submission_time=40363.688022, global_step=50996, preemption_count=0, score=40363.688022, test/ctc_loss=0.4144834876060486, test/num_examples=2472, test/wer=0.140780, total_duration=44119.269024, train/ctc_loss=0.33166712522506714, train/wer=0.123489, validation/ctc_loss=0.6463882923126221, validation/num_examples=5348, validation/wer=0.197302
I0216 22:51:00.125133 140532789532416 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7733533382415771, loss=1.4886378049850464
I0216 22:52:15.188788 140532797925120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.7147077322006226, loss=1.5325957536697388
I0216 22:53:30.349704 140532789532416 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.7375503778457642, loss=1.5242860317230225
I0216 22:54:45.563163 140532797925120 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.8370720744132996, loss=1.526541829109192
I0216 22:56:01.935236 140532789532416 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.741405725479126, loss=1.5871708393096924
I0216 22:57:29.621033 140532797925120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.6767356395721436, loss=1.5230921506881714
I0216 22:58:44.832262 140532789532416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.6402761936187744, loss=1.4791831970214844
I0216 22:59:59.949383 140532797925120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.7294104099273682, loss=1.5299140214920044
I0216 23:01:15.184211 140532789532416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.6966087222099304, loss=1.4953691959381104
I0216 23:02:30.631525 140532797925120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.7791818380355835, loss=1.5623724460601807
I0216 23:03:47.080054 140532789532416 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.788743793964386, loss=1.5947524309158325
I0216 23:05:10.445142 140532797925120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.7768247127532959, loss=1.5020532608032227
I0216 23:06:33.551229 140532789532416 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.7559837698936462, loss=1.530365228652954
I0216 23:07:58.590401 140532797925120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.7362756729125977, loss=1.5868287086486816
I0216 23:09:21.448154 140532789532416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.6298640966415405, loss=1.52456533908844
I0216 23:10:44.085638 140532797925120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.8180415034294128, loss=1.4694690704345703
I0216 23:12:05.327831 140532797925120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6968663334846497, loss=1.478281021118164
I0216 23:13:20.530856 140532789532416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.7035030126571655, loss=1.4761160612106323
I0216 23:14:35.703029 140532797925120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.773053765296936, loss=1.540940523147583
I0216 23:14:56.423744 140599226058560 spec.py:321] Evaluating on the training split.
I0216 23:15:50.554435 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 23:16:42.151705 140599226058560 spec.py:349] Evaluating on the test split.
I0216 23:17:08.187065 140599226058560 submission_runner.py:408] Time since start: 45691.22s, 	Step: 52829, 	{'train/ctc_loss': Array(0.3443252, dtype=float32), 'train/wer': 0.12496401573711763, 'validation/ctc_loss': Array(0.6351694, dtype=float32), 'validation/wer': 0.19351786593548762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40218315, dtype=float32), 'test/wer': 0.13468608453679443, 'test/num_examples': 2472, 'score': 41803.748430252075, 'total_duration': 45691.219561100006, 'accumulated_submission_time': 41803.748430252075, 'accumulated_eval_time': 3883.693731546402, 'accumulated_logging_time': 1.566605806350708}
I0216 23:17:08.226934 140532797925120 logging_writer.py:48] [52829] accumulated_eval_time=3883.693732, accumulated_logging_time=1.566606, accumulated_submission_time=41803.748430, global_step=52829, preemption_count=0, score=41803.748430, test/ctc_loss=0.4021831452846527, test/num_examples=2472, test/wer=0.134686, total_duration=45691.219561, train/ctc_loss=0.3443252146244049, train/wer=0.124964, validation/ctc_loss=0.6351693868637085, validation/num_examples=5348, validation/wer=0.193518
I0216 23:18:02.275322 140532789532416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.7665280103683472, loss=1.4580070972442627
I0216 23:19:17.987459 140532797925120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.8737831115722656, loss=1.49097740650177
I0216 23:20:33.265428 140532789532416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.9037396311759949, loss=1.5800460577011108
I0216 23:21:48.473689 140532797925120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.7930324673652649, loss=1.5697695016860962
I0216 23:23:09.186372 140532789532416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.7736021280288696, loss=1.503683090209961
I0216 23:24:31.734736 140532797925120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.8760793805122375, loss=1.5105128288269043
I0216 23:25:55.349411 140532789532416 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8656948804855347, loss=1.507322072982788
I0216 23:27:19.065683 140532797925120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.781798243522644, loss=1.4830089807510376
I0216 23:28:34.164058 140532789532416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.8163442015647888, loss=1.4986175298690796
I0216 23:29:49.326254 140532797925120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.7107992172241211, loss=1.5199724435806274
I0216 23:31:04.520256 140532789532416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.7477527856826782, loss=1.5187151432037354
I0216 23:32:19.765649 140532797925120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8168149590492249, loss=1.45891535282135
I0216 23:33:40.050937 140532789532416 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.6768469214439392, loss=1.5215097665786743
I0216 23:35:03.103371 140532797925120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.6843894720077515, loss=1.4896421432495117
I0216 23:36:26.571964 140532789532416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.7032139301300049, loss=1.4653608798980713
I0216 23:37:50.353083 140532797925120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7334943413734436, loss=1.515815258026123
I0216 23:39:14.343456 140532789532416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.7593411207199097, loss=1.457663893699646
I0216 23:40:39.875305 140532797925120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.7850018739700317, loss=1.439940094947815
I0216 23:41:08.771521 140599226058560 spec.py:321] Evaluating on the training split.
I0216 23:42:04.484282 140599226058560 spec.py:333] Evaluating on the validation split.
I0216 23:42:56.510326 140599226058560 spec.py:349] Evaluating on the test split.
I0216 23:43:23.063241 140599226058560 submission_runner.py:408] Time since start: 47266.10s, 	Step: 54640, 	{'train/ctc_loss': Array(0.33093026, dtype=float32), 'train/wer': 0.1206635390150001, 'validation/ctc_loss': Array(0.61256367, dtype=float32), 'validation/wer': 0.18805333230350368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38517037, dtype=float32), 'test/wer': 0.13178152864948306, 'test/num_examples': 2472, 'score': 43244.20497202873, 'total_duration': 47266.095905542374, 'accumulated_submission_time': 43244.20497202873, 'accumulated_eval_time': 4017.97895359993, 'accumulated_logging_time': 1.6243414878845215}
I0216 23:43:23.105165 140532797925120 logging_writer.py:48] [54640] accumulated_eval_time=4017.978954, accumulated_logging_time=1.624341, accumulated_submission_time=43244.204972, global_step=54640, preemption_count=0, score=43244.204972, test/ctc_loss=0.3851703703403473, test/num_examples=2472, test/wer=0.131782, total_duration=47266.095906, train/ctc_loss=0.33093026280403137, train/wer=0.120664, validation/ctc_loss=0.6125636696815491, validation/num_examples=5348, validation/wer=0.188053
I0216 23:44:08.890815 140532789532416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.6492047309875488, loss=1.4918662309646606
I0216 23:45:24.181543 140532797925120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.8106041550636292, loss=1.4813158512115479
I0216 23:46:39.462925 140532789532416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.8091493844985962, loss=1.5223461389541626
I0216 23:47:54.740700 140532797925120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.7324010133743286, loss=1.4523603916168213
I0216 23:49:10.109924 140532789532416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.7539286613464355, loss=1.407320499420166
I0216 23:50:25.335627 140532797925120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.7770840525627136, loss=1.4745382070541382
I0216 23:51:45.138965 140532789532416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.8444643616676331, loss=1.461272120475769
I0216 23:53:08.452875 140532797925120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.8581722974777222, loss=1.5041263103485107
I0216 23:54:31.917270 140532789532416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7875420451164246, loss=1.4485491514205933
I0216 23:55:54.551613 140532797925120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.9551683664321899, loss=1.485111951828003
I0216 23:57:14.804070 140532797925120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7620931267738342, loss=1.4879182577133179
I0216 23:58:30.012341 140532789532416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.7400791049003601, loss=1.4157049655914307
I0216 23:59:45.256326 140532797925120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.8082041144371033, loss=1.471514105796814
I0217 00:01:00.728089 140532789532416 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.8755671977996826, loss=1.4781841039657593
I0217 00:02:17.372663 140532797925120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.7878417372703552, loss=1.4693480730056763
I0217 00:03:40.856207 140532789532416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9033878445625305, loss=1.4487518072128296
I0217 00:05:03.668728 140532797925120 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.7270300388336182, loss=1.459028959274292
I0217 00:06:28.631491 140532789532416 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.7772079110145569, loss=1.4870517253875732
I0217 00:07:23.256793 140599226058560 spec.py:321] Evaluating on the training split.
I0217 00:08:19.020149 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 00:09:10.813180 140599226058560 spec.py:349] Evaluating on the test split.
I0217 00:09:37.042894 140599226058560 submission_runner.py:408] Time since start: 48840.08s, 	Step: 56468, 	{'train/ctc_loss': Array(0.31670836, dtype=float32), 'train/wer': 0.11514202916518154, 'validation/ctc_loss': Array(0.5975832, dtype=float32), 'validation/wer': 0.18225088581441826, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3706889, dtype=float32), 'test/wer': 0.1253630694859139, 'test/num_examples': 2472, 'score': 44684.27011537552, 'total_duration': 48840.0756649971, 'accumulated_submission_time': 44684.27011537552, 'accumulated_eval_time': 4151.758671045303, 'accumulated_logging_time': 1.6820077896118164}
I0217 00:09:37.084225 140532797925120 logging_writer.py:48] [56468] accumulated_eval_time=4151.758671, accumulated_logging_time=1.682008, accumulated_submission_time=44684.270115, global_step=56468, preemption_count=0, score=44684.270115, test/ctc_loss=0.37068888545036316, test/num_examples=2472, test/wer=0.125363, total_duration=48840.075665, train/ctc_loss=0.31670835614204407, train/wer=0.115142, validation/ctc_loss=0.5975831747055054, validation/num_examples=5348, validation/wer=0.182251
I0217 00:10:01.859470 140532789532416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.7492679357528687, loss=1.412359595298767
I0217 00:11:17.122502 140532797925120 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.7425031661987305, loss=1.4360331296920776
I0217 00:12:35.893900 140532797925120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.7839089632034302, loss=1.4661564826965332
I0217 00:13:50.931960 140532789532416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.7197211980819702, loss=1.4192777872085571
I0217 00:15:06.251899 140532797925120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.7364143133163452, loss=1.4538171291351318
I0217 00:16:21.462724 140532789532416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.911526083946228, loss=1.3923581838607788
I0217 00:17:38.993423 140532797925120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.766480028629303, loss=1.440380573272705
I0217 00:19:02.512397 140532789532416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.8784367442131042, loss=1.3914653062820435
I0217 00:20:26.031252 140532797925120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.7030272483825684, loss=1.4102942943572998
I0217 00:21:49.286856 140532789532416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.9745217561721802, loss=1.4781396389007568
I0217 00:23:11.954367 140532797925120 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.779740035533905, loss=1.4671943187713623
I0217 00:24:35.295431 140532789532416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.6794215440750122, loss=1.4005804061889648
I0217 00:26:01.540138 140532797925120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.681581437587738, loss=1.3775943517684937
I0217 00:27:16.579663 140532789532416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.8526474833488464, loss=1.4222627878189087
I0217 00:28:31.685219 140532797925120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.841972291469574, loss=1.396262526512146
I0217 00:29:46.697748 140532789532416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.6945807933807373, loss=1.366512656211853
I0217 00:31:01.810672 140532797925120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.764385998249054, loss=1.456377387046814
I0217 00:32:21.223563 140532789532416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.7454016208648682, loss=1.425331950187683
I0217 00:33:37.714144 140599226058560 spec.py:321] Evaluating on the training split.
I0217 00:34:32.792964 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 00:35:24.221807 140599226058560 spec.py:349] Evaluating on the test split.
I0217 00:35:50.470286 140599226058560 submission_runner.py:408] Time since start: 50413.50s, 	Step: 58294, 	{'train/ctc_loss': Array(0.2850827, dtype=float32), 'train/wer': 0.1052168831373953, 'validation/ctc_loss': Array(0.5681, dtype=float32), 'validation/wer': 0.17273139789721656, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35391665, dtype=float32), 'test/wer': 0.12004143562244836, 'test/num_examples': 2472, 'score': 46124.81386065483, 'total_duration': 50413.50209951401, 'accumulated_submission_time': 46124.81386065483, 'accumulated_eval_time': 4284.507459640503, 'accumulated_logging_time': 1.7384145259857178}
I0217 00:35:50.509547 140532797925120 logging_writer.py:48] [58294] accumulated_eval_time=4284.507460, accumulated_logging_time=1.738415, accumulated_submission_time=46124.813861, global_step=58294, preemption_count=0, score=46124.813861, test/ctc_loss=0.35391664505004883, test/num_examples=2472, test/wer=0.120041, total_duration=50413.502100, train/ctc_loss=0.28508269786834717, train/wer=0.105217, validation/ctc_loss=0.5680999755859375, validation/num_examples=5348, validation/wer=0.172731
I0217 00:35:55.853812 140532789532416 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.9150174856185913, loss=1.406950831413269
I0217 00:37:10.824160 140532797925120 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.7279937267303467, loss=1.4161980152130127
I0217 00:38:25.962428 140532789532416 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.6753888726234436, loss=1.4174737930297852
I0217 00:39:41.099209 140532797925120 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.7037504315376282, loss=1.3950940370559692
I0217 00:41:00.100749 140532789532416 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.8739595413208008, loss=1.4455819129943848
I0217 00:42:19.403649 140532797925120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7321965098381042, loss=1.4077460765838623
I0217 00:43:34.719409 140532789532416 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.7533473372459412, loss=1.3358746767044067
I0217 00:44:49.713649 140532797925120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9364174008369446, loss=1.4400644302368164
I0217 00:46:04.828743 140532789532416 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.9315891265869141, loss=1.3925895690917969
I0217 00:47:21.402035 140532797925120 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.7348055243492126, loss=1.4146039485931396
I0217 00:48:44.660340 140532789532416 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.8215118050575256, loss=1.3973218202590942
I0217 00:50:08.372197 140532797925120 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.7836216688156128, loss=1.414868950843811
I0217 00:51:31.463496 140532789532416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.7716733813285828, loss=1.405177116394043
I0217 00:52:54.723252 140532797925120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.9277761578559875, loss=1.4284110069274902
I0217 00:54:18.018095 140532789532416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.819318950176239, loss=1.393264651298523
I0217 00:55:40.279166 140532797925120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.8080229759216309, loss=1.347192645072937
I0217 00:56:55.515626 140532789532416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.7719241380691528, loss=1.396943211555481
I0217 00:58:10.695899 140532797925120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.9737228155136108, loss=1.4166384935379028
I0217 00:59:26.240795 140532789532416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.7366848587989807, loss=1.4261820316314697
I0217 00:59:50.725377 140599226058560 spec.py:321] Evaluating on the training split.
I0217 01:00:45.854603 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 01:01:37.758227 140599226058560 spec.py:349] Evaluating on the test split.
I0217 01:02:03.808302 140599226058560 submission_runner.py:408] Time since start: 51986.84s, 	Step: 60134, 	{'train/ctc_loss': Array(0.2588146, dtype=float32), 'train/wer': 0.09540699454244123, 'validation/ctc_loss': Array(0.5504874, dtype=float32), 'validation/wer': 0.168261293530417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33913124, dtype=float32), 'test/wer': 0.11404951963114171, 'test/num_examples': 2472, 'score': 47564.94284963608, 'total_duration': 51986.841499090195, 'accumulated_submission_time': 47564.94284963608, 'accumulated_eval_time': 4417.584407091141, 'accumulated_logging_time': 1.7928149700164795}
I0217 01:02:03.853039 140532797925120 logging_writer.py:48] [60134] accumulated_eval_time=4417.584407, accumulated_logging_time=1.792815, accumulated_submission_time=47564.942850, global_step=60134, preemption_count=0, score=47564.942850, test/ctc_loss=0.339131236076355, test/num_examples=2472, test/wer=0.114050, total_duration=51986.841499, train/ctc_loss=0.25881460309028625, train/wer=0.095407, validation/ctc_loss=0.5504873991012573, validation/num_examples=5348, validation/wer=0.168261
I0217 01:02:54.038716 140532789532416 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.7888805866241455, loss=1.399432897567749
I0217 01:04:09.299222 140532797925120 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.7703462839126587, loss=1.4409270286560059
I0217 01:05:24.393072 140532789532416 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.8873607516288757, loss=1.3708258867263794
I0217 01:06:39.540238 140532797925120 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.782462477684021, loss=1.4022266864776611
I0217 01:08:01.647404 140532789532416 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.7852739095687866, loss=1.3855477571487427
I0217 01:09:24.422021 140532797925120 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.7954585552215576, loss=1.3385154008865356
I0217 01:10:48.457947 140532797925120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.8180175423622131, loss=1.3804072141647339
I0217 01:12:03.604681 140532789532416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.6774921417236328, loss=1.3005776405334473
I0217 01:13:18.711734 140532797925120 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.7727444171905518, loss=1.3554401397705078
I0217 01:14:33.799977 140532789532416 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.0011687278747559, loss=1.367148756980896
I0217 01:15:48.937542 140532797925120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.870544970035553, loss=1.4116820096969604
I0217 01:17:07.730853 140532789532416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.7591549754142761, loss=1.3722906112670898
I0217 01:18:30.852190 140532797925120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.8108782768249512, loss=1.3747905492782593
I0217 01:19:53.948818 140532789532416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.8727450966835022, loss=1.3551957607269287
I0217 01:21:17.158923 140532797925120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.7933468818664551, loss=1.3804570436477661
I0217 01:22:40.170131 140532789532416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.9814175963401794, loss=1.4108473062515259
I0217 01:24:07.865941 140532797925120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.8698850274085999, loss=1.3443939685821533
I0217 01:25:23.017214 140532789532416 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.8145936131477356, loss=1.3539079427719116
I0217 01:26:03.836983 140599226058560 spec.py:321] Evaluating on the training split.
I0217 01:26:59.960967 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 01:27:51.215583 140599226058560 spec.py:349] Evaluating on the test split.
I0217 01:28:17.473599 140599226058560 submission_runner.py:408] Time since start: 53560.51s, 	Step: 61956, 	{'train/ctc_loss': Array(0.24200241, dtype=float32), 'train/wer': 0.08953588638092823, 'validation/ctc_loss': Array(0.52719754, dtype=float32), 'validation/wer': 0.16112650491904573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32282758, dtype=float32), 'test/wer': 0.10826071943615055, 'test/num_examples': 2472, 'score': 49004.83890962601, 'total_duration': 53560.50705432892, 'accumulated_submission_time': 49004.83890962601, 'accumulated_eval_time': 4551.215311527252, 'accumulated_logging_time': 1.8544585704803467}
I0217 01:28:17.516941 140532797925120 logging_writer.py:48] [61956] accumulated_eval_time=4551.215312, accumulated_logging_time=1.854459, accumulated_submission_time=49004.838910, global_step=61956, preemption_count=0, score=49004.838910, test/ctc_loss=0.3228275775909424, test/num_examples=2472, test/wer=0.108261, total_duration=53560.507054, train/ctc_loss=0.24200241267681122, train/wer=0.089536, validation/ctc_loss=0.527197539806366, validation/num_examples=5348, validation/wer=0.161127
I0217 01:28:51.213084 140532789532416 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.7503039240837097, loss=1.3385299444198608
I0217 01:30:06.287136 140532797925120 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.8249803185462952, loss=1.3226124048233032
I0217 01:31:21.420410 140532789532416 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.7878189086914062, loss=1.2785226106643677
I0217 01:32:36.536330 140532797925120 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.9531099200248718, loss=1.2945632934570312
I0217 01:33:52.015965 140532789532416 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.7569578289985657, loss=1.3335894346237183
I0217 01:35:11.150198 140532797925120 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.8542624115943909, loss=1.399422526359558
I0217 01:36:33.286357 140532789532416 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.8326846957206726, loss=1.35759437084198
I0217 01:37:56.464469 140532797925120 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.9098114967346191, loss=1.356096625328064
I0217 01:39:18.865595 140532789532416 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.8380085229873657, loss=1.3162163496017456
I0217 01:40:39.559219 140532797925120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.8231058716773987, loss=1.3293849229812622
I0217 01:41:54.826113 140532789532416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.8870033025741577, loss=1.3003418445587158
I0217 01:43:10.114302 140532797925120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.7701643705368042, loss=1.324865698814392
I0217 01:44:25.427582 140532789532416 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.916229784488678, loss=1.3178426027297974
I0217 01:45:40.764904 140532797925120 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.9269152879714966, loss=1.3818265199661255
I0217 01:47:00.565806 140532789532416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.7778028845787048, loss=1.3967911005020142
I0217 01:48:23.317130 140532797925120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.8230494856834412, loss=1.317510962486267
I0217 01:49:46.583568 140532789532416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.8909978270530701, loss=1.336991310119629
I0217 01:51:10.142965 140532797925120 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.8171466588973999, loss=1.317986011505127
I0217 01:52:18.262108 140599226058560 spec.py:321] Evaluating on the training split.
I0217 01:53:12.799401 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 01:54:05.235319 140599226058560 spec.py:349] Evaluating on the test split.
I0217 01:54:31.542525 140599226058560 submission_runner.py:408] Time since start: 55134.58s, 	Step: 63784, 	{'train/ctc_loss': Array(0.23850976, dtype=float32), 'train/wer': 0.0884111856169462, 'validation/ctc_loss': Array(0.50763637, dtype=float32), 'validation/wer': 0.1557971364299024, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30937302, dtype=float32), 'test/wer': 0.10517335933215526, 'test/num_examples': 2472, 'score': 50445.49860596657, 'total_duration': 55134.57596540451, 'accumulated_submission_time': 50445.49860596657, 'accumulated_eval_time': 4684.490033864975, 'accumulated_logging_time': 1.9127497673034668}
I0217 01:54:31.586066 140532797925120 logging_writer.py:48] [63784] accumulated_eval_time=4684.490034, accumulated_logging_time=1.912750, accumulated_submission_time=50445.498606, global_step=63784, preemption_count=0, score=50445.498606, test/ctc_loss=0.30937302112579346, test/num_examples=2472, test/wer=0.105173, total_duration=55134.575965, train/ctc_loss=0.23850975930690765, train/wer=0.088411, validation/ctc_loss=0.5076363682746887, validation/num_examples=5348, validation/wer=0.155797
I0217 01:54:44.392301 140532789532416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.8543605804443359, loss=1.312073826789856
I0217 01:56:03.365148 140532797925120 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0232077836990356, loss=1.30355703830719
I0217 01:57:18.662870 140532789532416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.9219251275062561, loss=1.3205187320709229
I0217 01:58:34.014257 140532797925120 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.9369574785232544, loss=1.3030924797058105
I0217 01:59:49.425503 140532789532416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.8433820605278015, loss=1.3169326782226562
I0217 02:01:04.757240 140532797925120 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.8271688222885132, loss=1.2490828037261963
I0217 02:02:24.755317 140532789532416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9161086678504944, loss=1.3260879516601562
I0217 02:03:47.461636 140532797925120 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.7618078589439392, loss=1.2758502960205078
I0217 02:05:09.987029 140532789532416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.8205560445785522, loss=1.360297441482544
I0217 02:06:32.366268 140532797925120 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.8585578203201294, loss=1.3502310514450073
I0217 02:07:55.528503 140532789532416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.8464535474777222, loss=1.3428210020065308
I0217 02:09:22.498065 140532797925120 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.816799521446228, loss=1.313502311706543
I0217 02:10:37.540104 140532789532416 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.0892726182937622, loss=1.3204447031021118
I0217 02:11:52.691989 140532797925120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.8817352652549744, loss=1.2304341793060303
I0217 02:13:07.840152 140532789532416 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.0550808906555176, loss=1.3679478168487549
I0217 02:14:23.032006 140532797925120 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1373695135116577, loss=1.307597279548645
I0217 02:15:40.999892 140532789532416 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.9049325585365295, loss=1.3753437995910645
I0217 02:17:03.770116 140532797925120 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.0897365808486938, loss=1.2836203575134277
I0217 02:18:27.122601 140532789532416 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.837017834186554, loss=1.3030915260314941
I0217 02:18:31.716160 140599226058560 spec.py:321] Evaluating on the training split.
I0217 02:19:26.108289 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 02:20:17.226204 140599226058560 spec.py:349] Evaluating on the test split.
I0217 02:20:43.666466 140599226058560 submission_runner.py:408] Time since start: 56706.70s, 	Step: 65607, 	{'train/ctc_loss': Array(0.21402435, dtype=float32), 'train/wer': 0.07830761787000699, 'validation/ctc_loss': Array(0.4928836, dtype=float32), 'validation/wer': 0.15193527520588548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29772234, dtype=float32), 'test/wer': 0.09940487071679564, 'test/num_examples': 2472, 'score': 51885.54228544235, 'total_duration': 56706.69957733154, 'accumulated_submission_time': 51885.54228544235, 'accumulated_eval_time': 4816.434291601181, 'accumulated_logging_time': 1.971311092376709}
I0217 02:20:43.711463 140532797925120 logging_writer.py:48] [65607] accumulated_eval_time=4816.434292, accumulated_logging_time=1.971311, accumulated_submission_time=51885.542285, global_step=65607, preemption_count=0, score=51885.542285, test/ctc_loss=0.29772233963012695, test/num_examples=2472, test/wer=0.099405, total_duration=56706.699577, train/ctc_loss=0.2140243500471115, train/wer=0.078308, validation/ctc_loss=0.4928835928440094, validation/num_examples=5348, validation/wer=0.151935
I0217 02:21:54.258625 140532789532416 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.0334951877593994, loss=1.3187971115112305
I0217 02:23:09.542046 140532797925120 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.8029369711875916, loss=1.2774479389190674
I0217 02:24:24.890934 140532789532416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.8901277184486389, loss=1.3219741582870483
I0217 02:25:44.133623 140532797925120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.9146963953971863, loss=1.235813856124878
I0217 02:26:59.463689 140532789532416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.9386261105537415, loss=1.3145791292190552
I0217 02:28:14.648048 140532797925120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.8838711380958557, loss=1.2586928606033325
I0217 02:29:29.914462 140532789532416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.9139986038208008, loss=1.3248757123947144
I0217 02:30:45.087839 140532797925120 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.8274450898170471, loss=1.2415714263916016
I0217 02:32:07.465245 140532789532416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9575238227844238, loss=1.2146273851394653
I0217 02:33:30.563229 140532797925120 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.8580994606018066, loss=1.3101269006729126
I0217 02:34:53.822959 140532789532416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.9597188830375671, loss=1.2600167989730835
I0217 02:36:17.524145 140532797925120 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.9461824893951416, loss=1.3449879884719849
I0217 02:37:39.477913 140532789532416 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.1453051567077637, loss=1.2505383491516113
I0217 02:39:01.933272 140532797925120 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.8475674390792847, loss=1.2299480438232422
I0217 02:40:17.360314 140532789532416 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.0277003049850464, loss=1.322628378868103
I0217 02:41:32.517856 140532797925120 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.4420796632766724, loss=1.342073678970337
I0217 02:42:47.738953 140532789532416 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.9123741388320923, loss=1.2794331312179565
I0217 02:44:05.200625 140532797925120 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.9394067525863647, loss=1.2698980569839478
I0217 02:44:43.970597 140599226058560 spec.py:321] Evaluating on the training split.
I0217 02:45:39.770708 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 02:46:31.260354 140599226058560 spec.py:349] Evaluating on the test split.
I0217 02:46:57.472413 140599226058560 submission_runner.py:408] Time since start: 58280.51s, 	Step: 67448, 	{'train/ctc_loss': Array(0.21193847, dtype=float32), 'train/wer': 0.07817700196970725, 'validation/ctc_loss': Array(0.4739441, dtype=float32), 'validation/wer': 0.14430809928845206, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28477463, dtype=float32), 'test/wer': 0.09597221375906405, 'test/num_examples': 2472, 'score': 53325.713799238205, 'total_duration': 58280.505524635315, 'accumulated_submission_time': 53325.713799238205, 'accumulated_eval_time': 4949.930072784424, 'accumulated_logging_time': 2.032210111618042}
I0217 02:46:57.515402 140532797925120 logging_writer.py:48] [67448] accumulated_eval_time=4949.930073, accumulated_logging_time=2.032210, accumulated_submission_time=53325.713799, global_step=67448, preemption_count=0, score=53325.713799, test/ctc_loss=0.28477463126182556, test/num_examples=2472, test/wer=0.095972, total_duration=58280.505525, train/ctc_loss=0.21193847060203552, train/wer=0.078177, validation/ctc_loss=0.4739440977573395, validation/num_examples=5348, validation/wer=0.144308
I0217 02:47:37.273390 140532789532416 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.8214328289031982, loss=1.2452147006988525
I0217 02:48:52.490182 140532797925120 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9507411122322083, loss=1.2445100545883179
I0217 02:50:07.714182 140532789532416 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.9361618161201477, loss=1.2821078300476074
I0217 02:51:22.873400 140532797925120 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.06880521774292, loss=1.2246767282485962
I0217 02:52:42.806641 140532789532416 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.9282827973365784, loss=1.2898770570755005
I0217 02:54:07.971773 140532797925120 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.2059009075164795, loss=1.2700648307800293
I0217 02:55:22.841200 140532789532416 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.012110710144043, loss=1.20036780834198
I0217 02:56:37.937633 140532797925120 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.9955548644065857, loss=1.2609976530075073
I0217 02:57:53.252011 140532789532416 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.1058257818222046, loss=1.2229316234588623
I0217 02:59:08.316470 140532797925120 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0955650806427002, loss=1.2482903003692627
I0217 03:00:26.923140 140532789532416 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0023713111877441, loss=1.2458477020263672
I0217 03:01:50.496151 140532797925120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.8382067084312439, loss=1.1966439485549927
I0217 03:03:14.763514 140532789532416 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.8945984840393066, loss=1.2789664268493652
I0217 03:04:37.490386 140532797925120 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.1744251251220703, loss=1.2680485248565674
I0217 03:06:00.678482 140532789532416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.8620521426200867, loss=1.2252641916275024
I0217 03:07:23.899291 140532797925120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.9338067770004272, loss=1.2585959434509277
I0217 03:08:43.391546 140532797925120 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.9017035961151123, loss=1.2120882272720337
I0217 03:09:58.641059 140532789532416 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.8692148923873901, loss=1.171355128288269
I0217 03:10:57.667889 140599226058560 spec.py:321] Evaluating on the training split.
I0217 03:11:53.228836 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 03:12:45.331610 140599226058560 spec.py:349] Evaluating on the test split.
I0217 03:13:12.645051 140599226058560 submission_runner.py:408] Time since start: 59855.68s, 	Step: 69280, 	{'train/ctc_loss': Array(0.22361787, dtype=float32), 'train/wer': 0.07778706122492147, 'validation/ctc_loss': Array(0.45318562, dtype=float32), 'validation/wer': 0.13958697394209138, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27006716, dtype=float32), 'test/wer': 0.09109743464749254, 'test/num_examples': 2472, 'score': 54765.77884674072, 'total_duration': 59855.67841768265, 'accumulated_submission_time': 54765.77884674072, 'accumulated_eval_time': 5084.901441574097, 'accumulated_logging_time': 2.091064214706421}
I0217 03:13:12.688692 140532797925120 logging_writer.py:48] [69280] accumulated_eval_time=5084.901442, accumulated_logging_time=2.091064, accumulated_submission_time=54765.778847, global_step=69280, preemption_count=0, score=54765.778847, test/ctc_loss=0.27006715536117554, test/num_examples=2472, test/wer=0.091097, total_duration=59855.678418, train/ctc_loss=0.22361786663532257, train/wer=0.077787, validation/ctc_loss=0.4531856179237366, validation/num_examples=5348, validation/wer=0.139587
I0217 03:13:28.467967 140532789532416 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.8183003067970276, loss=1.1968938112258911
I0217 03:14:43.493893 140532797925120 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.9296508431434631, loss=1.2382421493530273
I0217 03:15:58.776921 140532789532416 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.9214140176773071, loss=1.1798193454742432
I0217 03:17:13.805938 140532797925120 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.0082663297653198, loss=1.2163418531417847
I0217 03:18:30.507329 140532789532416 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.0204511880874634, loss=1.2030119895935059
I0217 03:19:53.406430 140532797925120 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.056750774383545, loss=1.2155300378799438
I0217 03:21:16.717945 140532789532416 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.889406681060791, loss=1.2258380651474
I0217 03:22:39.548175 140532797925120 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.9682375192642212, loss=1.255765676498413
I0217 03:24:01.684929 140532797925120 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.9430732727050781, loss=1.2077224254608154
I0217 03:25:16.857908 140532789532416 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.9725054502487183, loss=1.202370047569275
I0217 03:26:32.070216 140532797925120 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.8555891513824463, loss=1.1618328094482422
I0217 03:27:47.291025 140532789532416 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.0653046369552612, loss=1.1693123579025269
I0217 03:29:02.362755 140532797925120 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.9825494885444641, loss=1.2174360752105713
I0217 03:30:24.514678 140532789532416 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.1816872358322144, loss=1.2004315853118896
I0217 03:31:47.427502 140532797925120 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.0089348554611206, loss=1.1914244890213013
I0217 03:33:11.196422 140532789532416 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.9000536203384399, loss=1.2189087867736816
I0217 03:34:34.745360 140532797925120 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.9366719126701355, loss=1.1591342687606812
I0217 03:35:57.432118 140532789532416 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.2091864347457886, loss=1.2424038648605347
I0217 03:37:13.372784 140599226058560 spec.py:321] Evaluating on the training split.
I0217 03:38:09.043397 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 03:39:00.441111 140599226058560 spec.py:349] Evaluating on the test split.
I0217 03:39:26.326392 140599226058560 submission_runner.py:408] Time since start: 61429.36s, 	Step: 71090, 	{'train/ctc_loss': Array(0.15905462, dtype=float32), 'train/wer': 0.058698082203071156, 'validation/ctc_loss': Array(0.44548163, dtype=float32), 'validation/wer': 0.13546443708545333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26090908, dtype=float32), 'test/wer': 0.08786789348607642, 'test/num_examples': 2472, 'score': 56206.375772714615, 'total_duration': 61429.3595867157, 'accumulated_submission_time': 56206.375772714615, 'accumulated_eval_time': 5217.849287033081, 'accumulated_logging_time': 2.1515071392059326}
I0217 03:39:26.372621 140532797925120 logging_writer.py:48] [71090] accumulated_eval_time=5217.849287, accumulated_logging_time=2.151507, accumulated_submission_time=56206.375773, global_step=71090, preemption_count=0, score=56206.375773, test/ctc_loss=0.2609090805053711, test/num_examples=2472, test/wer=0.087868, total_duration=61429.359587, train/ctc_loss=0.15905462205410004, train/wer=0.058698, validation/ctc_loss=0.44548162817955017, validation/num_examples=5348, validation/wer=0.135464
I0217 03:39:34.697084 140532789532416 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.9790452122688293, loss=1.196070909500122
I0217 03:40:49.693870 140532797925120 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.9826686382293701, loss=1.1541038751602173
I0217 03:42:04.757033 140532789532416 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.024174690246582, loss=1.2458388805389404
I0217 03:43:19.934888 140532797925120 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0095436573028564, loss=1.1863893270492554
I0217 03:44:35.169147 140532789532416 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.9482829570770264, loss=1.1604628562927246
I0217 03:45:50.430914 140532797925120 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.2069082260131836, loss=1.2026065587997437
I0217 03:47:05.616231 140532789532416 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.2916622161865234, loss=1.1420941352844238
I0217 03:48:27.209867 140532797925120 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.108528971672058, loss=1.1981483697891235
I0217 03:49:51.414632 140532789532416 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.1372008323669434, loss=1.227765679359436
I0217 03:51:14.600009 140532797925120 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.229920744895935, loss=1.2137961387634277
I0217 03:52:41.318834 140532797925120 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.8851362466812134, loss=1.1382255554199219
I0217 03:53:56.320499 140532789532416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.8666898608207703, loss=1.1384954452514648
I0217 03:55:11.463722 140532797925120 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.1512867212295532, loss=1.1827139854431152
I0217 03:56:26.695103 140532789532416 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.0224330425262451, loss=1.2226563692092896
I0217 03:57:41.840737 140532797925120 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.0185476541519165, loss=1.184309720993042
I0217 03:58:57.037303 140532789532416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.8978033065795898, loss=1.1615086793899536
I0217 04:00:19.274720 140532797925120 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.0399856567382812, loss=1.1590510606765747
I0217 04:01:42.010463 140532789532416 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.0496914386749268, loss=1.1771243810653687
I0217 04:03:05.125620 140532797925120 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.0828511714935303, loss=1.1625312566757202
I0217 04:03:26.385365 140599226058560 spec.py:321] Evaluating on the training split.
I0217 04:04:21.850779 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 04:05:12.901780 140599226058560 spec.py:349] Evaluating on the test split.
I0217 04:05:39.321121 140599226058560 submission_runner.py:408] Time since start: 63002.35s, 	Step: 72927, 	{'train/ctc_loss': Array(0.15804432, dtype=float32), 'train/wer': 0.05804030998029963, 'validation/ctc_loss': Array(0.42757252, dtype=float32), 'validation/wer': 0.13049229075953156, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25125402, dtype=float32), 'test/wer': 0.08441492494871326, 'test/num_examples': 2472, 'score': 57646.29920887947, 'total_duration': 63002.354064941406, 'accumulated_submission_time': 57646.29920887947, 'accumulated_eval_time': 5350.778831243515, 'accumulated_logging_time': 2.2165493965148926}
I0217 04:05:39.368326 140532797925120 logging_writer.py:48] [72927] accumulated_eval_time=5350.778831, accumulated_logging_time=2.216549, accumulated_submission_time=57646.299209, global_step=72927, preemption_count=0, score=57646.299209, test/ctc_loss=0.25125402212142944, test/num_examples=2472, test/wer=0.084415, total_duration=63002.354065, train/ctc_loss=0.15804432332515717, train/wer=0.058040, validation/ctc_loss=0.4275725185871124, validation/num_examples=5348, validation/wer=0.130492
I0217 04:06:35.193222 140532789532416 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.088354468345642, loss=1.2416181564331055
I0217 04:07:50.223026 140532797925120 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.3490079641342163, loss=1.1230823993682861
I0217 04:09:09.267817 140532797925120 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.0706119537353516, loss=1.1967225074768066
I0217 04:10:24.541931 140532789532416 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.9835518002510071, loss=1.1885733604431152
I0217 04:11:39.783450 140532797925120 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.1293851137161255, loss=1.1426284313201904
I0217 04:12:55.124034 140532789532416 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.0302377939224243, loss=1.108774185180664
I0217 04:14:13.828043 140532797925120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9305532574653625, loss=1.155047059059143
I0217 04:15:35.866017 140532789532416 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0376379489898682, loss=1.1801916360855103
I0217 04:16:58.760295 140532797925120 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.151458740234375, loss=1.1724801063537598
I0217 04:18:21.445346 140532789532416 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.1497732400894165, loss=1.1186232566833496
I0217 04:19:44.655435 140532797925120 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2115001678466797, loss=1.112243890762329
I0217 04:21:08.441384 140532789532416 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.033187985420227, loss=1.203808307647705
I0217 04:22:32.134999 140532797925120 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.061542272567749, loss=1.124409794807434
I0217 04:23:47.166298 140532789532416 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1613093614578247, loss=1.1210523843765259
I0217 04:25:02.315918 140532797925120 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.063483476638794, loss=1.1245661973953247
I0217 04:26:17.472887 140532789532416 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.0533849000930786, loss=1.0806866884231567
I0217 04:27:32.736086 140532797925120 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2932583093643188, loss=1.1454453468322754
I0217 04:28:52.897860 140532789532416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.9698255062103271, loss=1.2008037567138672
I0217 04:29:39.416056 140599226058560 spec.py:321] Evaluating on the training split.
I0217 04:30:33.743488 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 04:31:25.294583 140599226058560 spec.py:349] Evaluating on the test split.
I0217 04:31:51.451626 140599226058560 submission_runner.py:408] Time since start: 64574.48s, 	Step: 74758, 	{'train/ctc_loss': Array(0.198389, dtype=float32), 'train/wer': 0.07393338989080758, 'validation/ctc_loss': Array(0.4172008, dtype=float32), 'validation/wer': 0.12731590990277764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24742267, dtype=float32), 'test/wer': 0.08222127434850608, 'test/num_examples': 2472, 'score': 59086.257048130035, 'total_duration': 64574.48491859436, 'accumulated_submission_time': 59086.257048130035, 'accumulated_eval_time': 5482.808529615402, 'accumulated_logging_time': 2.2834372520446777}
I0217 04:31:51.501007 140532797925120 logging_writer.py:48] [74758] accumulated_eval_time=5482.808530, accumulated_logging_time=2.283437, accumulated_submission_time=59086.257048, global_step=74758, preemption_count=0, score=59086.257048, test/ctc_loss=0.24742266535758972, test/num_examples=2472, test/wer=0.082221, total_duration=64574.484919, train/ctc_loss=0.1983889937400818, train/wer=0.073933, validation/ctc_loss=0.41720080375671387, validation/num_examples=5348, validation/wer=0.127316
I0217 04:32:23.770484 140532789532416 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.9267645478248596, loss=1.1561073064804077
I0217 04:33:38.870101 140532797925120 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.0312532186508179, loss=1.1778497695922852
I0217 04:34:54.109391 140532789532416 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.9681241512298584, loss=1.140280842781067
I0217 04:36:09.354123 140532797925120 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.9839330911636353, loss=1.157236099243164
I0217 04:37:32.959192 140532797925120 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.9637990593910217, loss=1.1636810302734375
I0217 04:38:48.281635 140532789532416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.9790790677070618, loss=1.165023922920227
I0217 04:40:03.349150 140532797925120 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.1350244283676147, loss=1.1079968214035034
I0217 04:41:18.614241 140532789532416 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1709363460540771, loss=1.1350011825561523
I0217 04:42:33.748318 140532797925120 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.2556055784225464, loss=1.1575219631195068
I0217 04:43:53.939827 140532789532416 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.99732506275177, loss=1.113299012184143
I0217 04:45:17.387908 140532797925120 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.116958498954773, loss=1.1595615148544312
I0217 04:46:40.727153 140532789532416 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1396350860595703, loss=1.1562247276306152
I0217 04:48:04.768337 140532797925120 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.0311893224716187, loss=1.1116808652877808
I0217 04:49:30.042137 140532789532416 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.052069067955017, loss=1.1791714429855347
I0217 04:50:53.744362 140532797925120 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.1893959045410156, loss=1.1594222784042358
I0217 04:52:13.846637 140532797925120 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.1344718933105469, loss=1.1295619010925293
I0217 04:53:28.982746 140532789532416 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.0663881301879883, loss=1.0986360311508179
I0217 04:54:44.342127 140532797925120 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.9373577237129211, loss=1.0787044763565063
I0217 04:55:51.485026 140599226058560 spec.py:321] Evaluating on the training split.
I0217 04:56:45.907920 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 04:57:37.460919 140599226058560 spec.py:349] Evaluating on the test split.
I0217 04:58:03.974409 140599226058560 submission_runner.py:408] Time since start: 66147.01s, 	Step: 76591, 	{'train/ctc_loss': Array(0.19787984, dtype=float32), 'train/wer': 0.07074304504809609, 'validation/ctc_loss': Array(0.41123158, dtype=float32), 'validation/wer': 0.1253656699846491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24104773, dtype=float32), 'test/wer': 0.0800276237482989, 'test/num_examples': 2472, 'score': 60526.15056729317, 'total_duration': 66147.00818371773, 'accumulated_submission_time': 60526.15056729317, 'accumulated_eval_time': 5615.292538166046, 'accumulated_logging_time': 2.351891279220581}
I0217 04:58:04.017492 140532797925120 logging_writer.py:48] [76591] accumulated_eval_time=5615.292538, accumulated_logging_time=2.351891, accumulated_submission_time=60526.150567, global_step=76591, preemption_count=0, score=60526.150567, test/ctc_loss=0.2410477250814438, test/num_examples=2472, test/wer=0.080028, total_duration=66147.008184, train/ctc_loss=0.1978798359632492, train/wer=0.070743, validation/ctc_loss=0.4112315773963928, validation/num_examples=5348, validation/wer=0.125366
I0217 04:58:11.603837 140532789532416 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.0736743211746216, loss=1.0920305252075195
I0217 04:59:26.490184 140532797925120 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.1077404022216797, loss=1.1122149229049683
I0217 05:00:41.501822 140532789532416 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.0691088438034058, loss=1.145450472831726
I0217 05:01:56.680505 140532797925120 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.0756440162658691, loss=1.1704329252243042
I0217 05:03:15.302345 140532789532416 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.0688636302947998, loss=1.1882431507110596
I0217 05:04:40.124588 140532797925120 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.980166494846344, loss=1.1244105100631714
I0217 05:06:03.176511 140532789532416 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.9244816303253174, loss=1.12279212474823
I0217 05:07:06.426773 140532797925120 logging_writer.py:48] [77276] global_step=77276, preemption_count=0, score=61068.489097
I0217 05:07:07.285789 140599226058560 checkpoints.py:490] Saving checkpoint at step: 77276
I0217 05:07:08.792035 140599226058560 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_4/checkpoint_77276
I0217 05:07:08.829668 140599226058560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_4/checkpoint_77276.
I0217 05:07:10.353628 140599226058560 submission_runner.py:583] Tuning trial 4/5
I0217 05:07:10.353888 140599226058560 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0217 05:07:10.383316 140599226058560 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.98225, dtype=float32), 'train/wer': 1.3614880623081345, 'validation/ctc_loss': Array(31.08705, dtype=float32), 'validation/wer': 1.0585747801152765, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.23744, dtype=float32), 'test/wer': 1.10235004976337, 'test/num_examples': 2472, 'score': 35.09761643409729, 'total_duration': 170.93748545646667, 'accumulated_submission_time': 35.09761643409729, 'accumulated_eval_time': 135.83981275558472, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1791, {'train/ctc_loss': Array(6.036521, dtype=float32), 'train/wer': 0.9413900245298447, 'validation/ctc_loss': Array(6.0377555, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0074077, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1476.0195994377136, 'total_duration': 1720.848159313202, 'accumulated_submission_time': 1476.0195994377136, 'accumulated_eval_time': 244.7291920185089, 'accumulated_logging_time': 0.02799057960510254, 'global_step': 1791, 'preemption_count': 0}), (3623, {'train/ctc_loss': Array(6.206665, dtype=float32), 'train/wer': 0.9382353276158586, 'validation/ctc_loss': Array(6.116562, dtype=float32), 'validation/wer': 0.8959904225841644, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9581666, dtype=float32), 'test/wer': 0.8983405439441026, 'test/num_examples': 2472, 'score': 2916.5635409355164, 'total_duration': 3269.9817910194397, 'accumulated_submission_time': 2916.5635409355164, 'accumulated_eval_time': 353.1840178966522, 'accumulated_logging_time': 0.08708763122558594, 'global_step': 3623, 'preemption_count': 0}), (5452, {'train/ctc_loss': Array(12.30888, dtype=float32), 'train/wer': 0.939860390191516, 'validation/ctc_loss': Array(12.248377, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(12.230537, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4356.7338581085205, 'total_duration': 4818.730646371841, 'accumulated_submission_time': 4356.7338581085205, 'accumulated_eval_time': 461.63093519210815, 'accumulated_logging_time': 0.14055681228637695, 'global_step': 5452, 'preemption_count': 0}), (7255, {'train/ctc_loss': Array(9.199602, dtype=float32), 'train/wer': 0.9406809116337576, 'validation/ctc_loss': Array(9.2184, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(9.05541, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5796.618039608002, 'total_duration': 6367.059968471527, 'accumulated_submission_time': 5796.618039608002, 'accumulated_eval_time': 569.9528002738953, 'accumulated_logging_time': 0.1885228157043457, 'global_step': 7255, 'preemption_count': 0}), (9072, {'train/ctc_loss': Array(1.9409115, dtype=float32), 'train/wer': 0.5133557669993037, 'validation/ctc_loss': Array(1.8373336, dtype=float32), 'validation/wer': 0.4703650424322002, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.4274611, dtype=float32), 'test/wer': 0.403936384132594, 'test/num_examples': 2472, 'score': 7236.628142595291, 'total_duration': 7937.760040521622, 'accumulated_submission_time': 7236.628142595291, 'accumulated_eval_time': 700.5129299163818, 'accumulated_logging_time': 0.24257588386535645, 'global_step': 9072, 'preemption_count': 0}), (10901, {'train/ctc_loss': Array(1.5330179, dtype=float32), 'train/wer': 0.4447662118572818, 'validation/ctc_loss': Array(1.5998214, dtype=float32), 'validation/wer': 0.4368054683954932, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2287663, dtype=float32), 'test/wer': 0.37527674527247984, 'test/num_examples': 2472, 'score': 8677.431626081467, 'total_duration': 9511.064285516739, 'accumulated_submission_time': 8677.431626081467, 'accumulated_eval_time': 832.8874080181122, 'accumulated_logging_time': 0.29271388053894043, 'global_step': 10901, 'preemption_count': 0}), (12737, {'train/ctc_loss': Array(1.5478851, dtype=float32), 'train/wer': 0.4351763341723465, 'validation/ctc_loss': Array(1.5292115, dtype=float32), 'validation/wer': 0.41630863994902345, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1772243, dtype=float32), 'test/wer': 0.3542745719334593, 'test/num_examples': 2472, 'score': 10117.528901100159, 'total_duration': 11083.444188117981, 'accumulated_submission_time': 10117.528901100159, 'accumulated_eval_time': 965.0422995090485, 'accumulated_logging_time': 0.34348201751708984, 'global_step': 12737, 'preemption_count': 0}), (14544, {'train/ctc_loss': Array(1.2687769, dtype=float32), 'train/wer': 0.38057421766114674, 'validation/ctc_loss': Array(1.2760807, dtype=float32), 'validation/wer': 0.363468723751412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9283827, dtype=float32), 'test/wer': 0.2951881867852863, 'test/num_examples': 2472, 'score': 11557.644403457642, 'total_duration': 12654.127409219742, 'accumulated_submission_time': 11557.644403457642, 'accumulated_eval_time': 1095.4794552326202, 'accumulated_logging_time': 0.39788150787353516, 'global_step': 14544, 'preemption_count': 0}), (16362, {'train/ctc_loss': Array(1.0706007, dtype=float32), 'train/wer': 0.3385405215249989, 'validation/ctc_loss': Array(1.1228197, dtype=float32), 'validation/wer': 0.33144423955125174, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8068257, dtype=float32), 'test/wer': 0.2676659963845388, 'test/num_examples': 2472, 'score': 12997.894040346146, 'total_duration': 14226.949261188507, 'accumulated_submission_time': 12997.894040346146, 'accumulated_eval_time': 1227.9253115653992, 'accumulated_logging_time': 0.447887659072876, 'global_step': 16362, 'preemption_count': 0}), (18176, {'train/ctc_loss': Array(0.96476525, dtype=float32), 'train/wer': 0.3123660160216631, 'validation/ctc_loss': Array(1.0905648, dtype=float32), 'validation/wer': 0.3211427247361866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7684886, dtype=float32), 'test/wer': 0.2561696423130827, 'test/num_examples': 2472, 'score': 14437.993826389313, 'total_duration': 15798.103556871414, 'accumulated_submission_time': 14437.993826389313, 'accumulated_eval_time': 1358.848022699356, 'accumulated_logging_time': 0.5031416416168213, 'global_step': 18176, 'preemption_count': 0}), (20012, {'train/ctc_loss': Array(0.8736541, dtype=float32), 'train/wer': 0.2842924222705095, 'validation/ctc_loss': Array(1.0145409, dtype=float32), 'validation/wer': 0.30124448477943944, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.715538, dtype=float32), 'test/wer': 0.23638616375195498, 'test/num_examples': 2472, 'score': 15878.171899795532, 'total_duration': 17368.276427268982, 'accumulated_submission_time': 15878.171899795532, 'accumulated_eval_time': 1488.713681936264, 'accumulated_logging_time': 0.5551145076751709, 'global_step': 20012, 'preemption_count': 0}), (21832, {'train/ctc_loss': Array(0.9753044, dtype=float32), 'train/wer': 0.3114894714417061, 'validation/ctc_loss': Array(1.0329162, dtype=float32), 'validation/wer': 0.30400571555461153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.72071874, dtype=float32), 'test/wer': 0.23701582272053298, 'test/num_examples': 2472, 'score': 17318.47605085373, 'total_duration': 18940.11133503914, 'accumulated_submission_time': 17318.47605085373, 'accumulated_eval_time': 1620.1200077533722, 'accumulated_logging_time': 0.6046969890594482, 'global_step': 21832, 'preemption_count': 0}), (23646, {'train/ctc_loss': Array(0.8934462, dtype=float32), 'train/wer': 0.28510904762442035, 'validation/ctc_loss': Array(0.9852448, dtype=float32), 'validation/wer': 0.29178292478059803, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6862627, dtype=float32), 'test/wer': 0.22722564133812687, 'test/num_examples': 2472, 'score': 18758.696970939636, 'total_duration': 20512.817051410675, 'accumulated_submission_time': 18758.696970939636, 'accumulated_eval_time': 1752.4782774448395, 'accumulated_logging_time': 0.654998779296875, 'global_step': 23646, 'preemption_count': 0}), (25454, {'train/ctc_loss': Array(0.8983038, dtype=float32), 'train/wer': 0.2872240604474295, 'validation/ctc_loss': Array(0.96987027, dtype=float32), 'validation/wer': 0.28998715931143015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.66787153, dtype=float32), 'test/wer': 0.2244835780878679, 'test/num_examples': 2472, 'score': 20198.639166355133, 'total_duration': 22085.138073682785, 'accumulated_submission_time': 20198.639166355133, 'accumulated_eval_time': 1884.7277166843414, 'accumulated_logging_time': 0.7082116603851318, 'global_step': 25454, 'preemption_count': 0}), (27290, {'train/ctc_loss': Array(0.7946374, dtype=float32), 'train/wer': 0.26072332908574836, 'validation/ctc_loss': Array(0.9428824, dtype=float32), 'validation/wer': 0.2765865008640914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.64174926, dtype=float32), 'test/wer': 0.2136778177238844, 'test/num_examples': 2472, 'score': 21638.938226938248, 'total_duration': 23658.000257968903, 'accumulated_submission_time': 21638.938226938248, 'accumulated_eval_time': 2017.1603062152863, 'accumulated_logging_time': 0.7608418464660645, 'global_step': 27290, 'preemption_count': 0}), (29119, {'train/ctc_loss': Array(0.7964099, dtype=float32), 'train/wer': 0.26791169391094555, 'validation/ctc_loss': Array(0.9283386, dtype=float32), 'validation/wer': 0.2796470258841249, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6265644, dtype=float32), 'test/wer': 0.21237787662746532, 'test/num_examples': 2472, 'score': 23079.49067544937, 'total_duration': 25229.84836292267, 'accumulated_submission_time': 23079.49067544937, 'accumulated_eval_time': 2148.3218274116516, 'accumulated_logging_time': 0.8176655769348145, 'global_step': 29119, 'preemption_count': 0}), (30930, {'train/ctc_loss': Array(0.81224275, dtype=float32), 'train/wer': 0.2673072213330903, 'validation/ctc_loss': Array(0.9076635, dtype=float32), 'validation/wer': 0.27059096131380517, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.614802, dtype=float32), 'test/wer': 0.2055531858712652, 'test/num_examples': 2472, 'score': 24519.64860892296, 'total_duration': 26800.878132104874, 'accumulated_submission_time': 24519.64860892296, 'accumulated_eval_time': 2279.065259218216, 'accumulated_logging_time': 0.8696136474609375, 'global_step': 30930, 'preemption_count': 0}), (32775, {'train/ctc_loss': Array(0.5687455, dtype=float32), 'train/wer': 0.19706803332863815, 'validation/ctc_loss': Array(0.8738757, dtype=float32), 'validation/wer': 0.26107147339660347, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5819507, dtype=float32), 'test/wer': 0.1957630044888591, 'test/num_examples': 2472, 'score': 25960.040961503983, 'total_duration': 28383.277754068375, 'accumulated_submission_time': 25960.040961503983, 'accumulated_eval_time': 2420.939267396927, 'accumulated_logging_time': 0.9254543781280518, 'global_step': 32775, 'preemption_count': 0}), (34601, {'train/ctc_loss': Array(0.52129424, dtype=float32), 'train/wer': 0.18640793793824897, 'validation/ctc_loss': Array(0.86678445, dtype=float32), 'validation/wer': 0.2615735153557257, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5760456, dtype=float32), 'test/wer': 0.19541770763512278, 'test/num_examples': 2472, 'score': 27400.785620450974, 'total_duration': 29959.528460025787, 'accumulated_submission_time': 27400.785620450974, 'accumulated_eval_time': 2556.310618162155, 'accumulated_logging_time': 0.9855234622955322, 'global_step': 34601, 'preemption_count': 0}), (36435, {'train/ctc_loss': Array(0.4877663, dtype=float32), 'train/wer': 0.17128320788859458, 'validation/ctc_loss': Array(0.82872856, dtype=float32), 'validation/wer': 0.24930245131641193, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5492272, dtype=float32), 'test/wer': 0.18739463368066134, 'test/num_examples': 2472, 'score': 28841.119877815247, 'total_duration': 31533.52166032791, 'accumulated_submission_time': 28841.119877815247, 'accumulated_eval_time': 2689.823467731476, 'accumulated_logging_time': 1.0541713237762451, 'global_step': 36435, 'preemption_count': 0}), (38254, {'train/ctc_loss': Array(0.45671743, dtype=float32), 'train/wer': 0.16296851962026954, 'validation/ctc_loss': Array(0.79590267, dtype=float32), 'validation/wer': 0.2387885341340259, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5308841, dtype=float32), 'test/wer': 0.17924969024841061, 'test/num_examples': 2472, 'score': 30281.715041160583, 'total_duration': 33108.547865867615, 'accumulated_submission_time': 30281.715041160583, 'accumulated_eval_time': 2824.1240861415863, 'accumulated_logging_time': 1.1081647872924805, 'global_step': 38254, 'preemption_count': 0}), (40068, {'train/ctc_loss': Array(0.44774008, dtype=float32), 'train/wer': 0.15976669778082325, 'validation/ctc_loss': Array(0.7690591, dtype=float32), 'validation/wer': 0.23156685364511426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50365597, dtype=float32), 'test/wer': 0.16815956776958543, 'test/num_examples': 2472, 'score': 31721.85123872757, 'total_duration': 34681.24870944023, 'accumulated_submission_time': 31721.85123872757, 'accumulated_eval_time': 2956.557309150696, 'accumulated_logging_time': 1.1653995513916016, 'global_step': 40068, 'preemption_count': 0}), (41881, {'train/ctc_loss': Array(0.42459473, dtype=float32), 'train/wer': 0.15427558055342772, 'validation/ctc_loss': Array(0.7605355, dtype=float32), 'validation/wer': 0.2281491064618593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5015915, dtype=float32), 'test/wer': 0.16994698677716166, 'test/num_examples': 2472, 'score': 33161.960060596466, 'total_duration': 36255.45990538597, 'accumulated_submission_time': 33161.960060596466, 'accumulated_eval_time': 3090.524173974991, 'accumulated_logging_time': 1.2250707149505615, 'global_step': 41881, 'preemption_count': 0}), (43718, {'train/ctc_loss': Array(0.4951775, dtype=float32), 'train/wer': 0.17071944294139188, 'validation/ctc_loss': Array(0.7597602, dtype=float32), 'validation/wer': 0.22932697413518446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.492929, dtype=float32), 'test/wer': 0.1679767635529015, 'test/num_examples': 2472, 'score': 34602.280463695526, 'total_duration': 37828.440029382706, 'accumulated_submission_time': 34602.280463695526, 'accumulated_eval_time': 3223.048075437546, 'accumulated_logging_time': 1.28309965133667, 'global_step': 43718, 'preemption_count': 0}), (45542, {'train/ctc_loss': Array(0.40693375, dtype=float32), 'train/wer': 0.14851573208885727, 'validation/ctc_loss': Array(0.7193552, dtype=float32), 'validation/wer': 0.21866823715689776, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46914527, dtype=float32), 'test/wer': 0.1600146243373347, 'test/num_examples': 2472, 'score': 36042.547214746475, 'total_duration': 39400.75337386131, 'accumulated_submission_time': 36042.547214746475, 'accumulated_eval_time': 3354.965092897415, 'accumulated_logging_time': 1.3371038436889648, 'global_step': 45542, 'preemption_count': 0}), (47348, {'train/ctc_loss': Array(0.38006338, dtype=float32), 'train/wer': 0.13803585634282162, 'validation/ctc_loss': Array(0.69912624, dtype=float32), 'validation/wer': 0.2128078627494521, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4517525, dtype=float32), 'test/wer': 0.1527633904088721, 'test/num_examples': 2472, 'score': 37482.9290099144, 'total_duration': 40973.24291205406, 'accumulated_submission_time': 37482.9290099144, 'accumulated_eval_time': 3486.9389243125916, 'accumulated_logging_time': 1.3953540325164795, 'global_step': 47348, 'preemption_count': 0}), (49157, {'train/ctc_loss': Array(0.3494525, dtype=float32), 'train/wer': 0.12735696869363772, 'validation/ctc_loss': Array(0.67146903, dtype=float32), 'validation/wer': 0.20352974115875147, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43440887, dtype=float32), 'test/wer': 0.14622306176751365, 'test/num_examples': 2472, 'score': 38923.530285835266, 'total_duration': 42546.768450737, 'accumulated_submission_time': 38923.530285835266, 'accumulated_eval_time': 3619.7294538021088, 'accumulated_logging_time': 1.4531559944152832, 'global_step': 49157, 'preemption_count': 0}), (50996, {'train/ctc_loss': Array(0.33166713, dtype=float32), 'train/wer': 0.12348912059722914, 'validation/ctc_loss': Array(0.6463883, dtype=float32), 'validation/wer': 0.1973024899350242, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4144835, dtype=float32), 'test/wer': 0.1407795584262588, 'test/num_examples': 2472, 'score': 40363.68802213669, 'total_duration': 44119.26902413368, 'accumulated_submission_time': 40363.68802213669, 'accumulated_eval_time': 3751.9370737075806, 'accumulated_logging_time': 1.511134386062622, 'global_step': 50996, 'preemption_count': 0}), (52829, {'train/ctc_loss': Array(0.3443252, dtype=float32), 'train/wer': 0.12496401573711763, 'validation/ctc_loss': Array(0.6351694, dtype=float32), 'validation/wer': 0.19351786593548762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40218315, dtype=float32), 'test/wer': 0.13468608453679443, 'test/num_examples': 2472, 'score': 41803.748430252075, 'total_duration': 45691.219561100006, 'accumulated_submission_time': 41803.748430252075, 'accumulated_eval_time': 3883.693731546402, 'accumulated_logging_time': 1.566605806350708, 'global_step': 52829, 'preemption_count': 0}), (54640, {'train/ctc_loss': Array(0.33093026, dtype=float32), 'train/wer': 0.1206635390150001, 'validation/ctc_loss': Array(0.61256367, dtype=float32), 'validation/wer': 0.18805333230350368, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38517037, dtype=float32), 'test/wer': 0.13178152864948306, 'test/num_examples': 2472, 'score': 43244.20497202873, 'total_duration': 47266.095905542374, 'accumulated_submission_time': 43244.20497202873, 'accumulated_eval_time': 4017.97895359993, 'accumulated_logging_time': 1.6243414878845215, 'global_step': 54640, 'preemption_count': 0}), (56468, {'train/ctc_loss': Array(0.31670836, dtype=float32), 'train/wer': 0.11514202916518154, 'validation/ctc_loss': Array(0.5975832, dtype=float32), 'validation/wer': 0.18225088581441826, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3706889, dtype=float32), 'test/wer': 0.1253630694859139, 'test/num_examples': 2472, 'score': 44684.27011537552, 'total_duration': 48840.0756649971, 'accumulated_submission_time': 44684.27011537552, 'accumulated_eval_time': 4151.758671045303, 'accumulated_logging_time': 1.6820077896118164, 'global_step': 56468, 'preemption_count': 0}), (58294, {'train/ctc_loss': Array(0.2850827, dtype=float32), 'train/wer': 0.1052168831373953, 'validation/ctc_loss': Array(0.5681, dtype=float32), 'validation/wer': 0.17273139789721656, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35391665, dtype=float32), 'test/wer': 0.12004143562244836, 'test/num_examples': 2472, 'score': 46124.81386065483, 'total_duration': 50413.50209951401, 'accumulated_submission_time': 46124.81386065483, 'accumulated_eval_time': 4284.507459640503, 'accumulated_logging_time': 1.7384145259857178, 'global_step': 58294, 'preemption_count': 0}), (60134, {'train/ctc_loss': Array(0.2588146, dtype=float32), 'train/wer': 0.09540699454244123, 'validation/ctc_loss': Array(0.5504874, dtype=float32), 'validation/wer': 0.168261293530417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33913124, dtype=float32), 'test/wer': 0.11404951963114171, 'test/num_examples': 2472, 'score': 47564.94284963608, 'total_duration': 51986.841499090195, 'accumulated_submission_time': 47564.94284963608, 'accumulated_eval_time': 4417.584407091141, 'accumulated_logging_time': 1.7928149700164795, 'global_step': 60134, 'preemption_count': 0}), (61956, {'train/ctc_loss': Array(0.24200241, dtype=float32), 'train/wer': 0.08953588638092823, 'validation/ctc_loss': Array(0.52719754, dtype=float32), 'validation/wer': 0.16112650491904573, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32282758, dtype=float32), 'test/wer': 0.10826071943615055, 'test/num_examples': 2472, 'score': 49004.83890962601, 'total_duration': 53560.50705432892, 'accumulated_submission_time': 49004.83890962601, 'accumulated_eval_time': 4551.215311527252, 'accumulated_logging_time': 1.8544585704803467, 'global_step': 61956, 'preemption_count': 0}), (63784, {'train/ctc_loss': Array(0.23850976, dtype=float32), 'train/wer': 0.0884111856169462, 'validation/ctc_loss': Array(0.50763637, dtype=float32), 'validation/wer': 0.1557971364299024, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30937302, dtype=float32), 'test/wer': 0.10517335933215526, 'test/num_examples': 2472, 'score': 50445.49860596657, 'total_duration': 55134.57596540451, 'accumulated_submission_time': 50445.49860596657, 'accumulated_eval_time': 4684.490033864975, 'accumulated_logging_time': 1.9127497673034668, 'global_step': 63784, 'preemption_count': 0}), (65607, {'train/ctc_loss': Array(0.21402435, dtype=float32), 'train/wer': 0.07830761787000699, 'validation/ctc_loss': Array(0.4928836, dtype=float32), 'validation/wer': 0.15193527520588548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29772234, dtype=float32), 'test/wer': 0.09940487071679564, 'test/num_examples': 2472, 'score': 51885.54228544235, 'total_duration': 56706.69957733154, 'accumulated_submission_time': 51885.54228544235, 'accumulated_eval_time': 4816.434291601181, 'accumulated_logging_time': 1.971311092376709, 'global_step': 65607, 'preemption_count': 0}), (67448, {'train/ctc_loss': Array(0.21193847, dtype=float32), 'train/wer': 0.07817700196970725, 'validation/ctc_loss': Array(0.4739441, dtype=float32), 'validation/wer': 0.14430809928845206, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28477463, dtype=float32), 'test/wer': 0.09597221375906405, 'test/num_examples': 2472, 'score': 53325.713799238205, 'total_duration': 58280.505524635315, 'accumulated_submission_time': 53325.713799238205, 'accumulated_eval_time': 4949.930072784424, 'accumulated_logging_time': 2.032210111618042, 'global_step': 67448, 'preemption_count': 0}), (69280, {'train/ctc_loss': Array(0.22361787, dtype=float32), 'train/wer': 0.07778706122492147, 'validation/ctc_loss': Array(0.45318562, dtype=float32), 'validation/wer': 0.13958697394209138, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27006716, dtype=float32), 'test/wer': 0.09109743464749254, 'test/num_examples': 2472, 'score': 54765.77884674072, 'total_duration': 59855.67841768265, 'accumulated_submission_time': 54765.77884674072, 'accumulated_eval_time': 5084.901441574097, 'accumulated_logging_time': 2.091064214706421, 'global_step': 69280, 'preemption_count': 0}), (71090, {'train/ctc_loss': Array(0.15905462, dtype=float32), 'train/wer': 0.058698082203071156, 'validation/ctc_loss': Array(0.44548163, dtype=float32), 'validation/wer': 0.13546443708545333, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26090908, dtype=float32), 'test/wer': 0.08786789348607642, 'test/num_examples': 2472, 'score': 56206.375772714615, 'total_duration': 61429.3595867157, 'accumulated_submission_time': 56206.375772714615, 'accumulated_eval_time': 5217.849287033081, 'accumulated_logging_time': 2.1515071392059326, 'global_step': 71090, 'preemption_count': 0}), (72927, {'train/ctc_loss': Array(0.15804432, dtype=float32), 'train/wer': 0.05804030998029963, 'validation/ctc_loss': Array(0.42757252, dtype=float32), 'validation/wer': 0.13049229075953156, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25125402, dtype=float32), 'test/wer': 0.08441492494871326, 'test/num_examples': 2472, 'score': 57646.29920887947, 'total_duration': 63002.354064941406, 'accumulated_submission_time': 57646.29920887947, 'accumulated_eval_time': 5350.778831243515, 'accumulated_logging_time': 2.2165493965148926, 'global_step': 72927, 'preemption_count': 0}), (74758, {'train/ctc_loss': Array(0.198389, dtype=float32), 'train/wer': 0.07393338989080758, 'validation/ctc_loss': Array(0.4172008, dtype=float32), 'validation/wer': 0.12731590990277764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24742267, dtype=float32), 'test/wer': 0.08222127434850608, 'test/num_examples': 2472, 'score': 59086.257048130035, 'total_duration': 64574.48491859436, 'accumulated_submission_time': 59086.257048130035, 'accumulated_eval_time': 5482.808529615402, 'accumulated_logging_time': 2.2834372520446777, 'global_step': 74758, 'preemption_count': 0}), (76591, {'train/ctc_loss': Array(0.19787984, dtype=float32), 'train/wer': 0.07074304504809609, 'validation/ctc_loss': Array(0.41123158, dtype=float32), 'validation/wer': 0.1253656699846491, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24104773, dtype=float32), 'test/wer': 0.0800276237482989, 'test/num_examples': 2472, 'score': 60526.15056729317, 'total_duration': 66147.00818371773, 'accumulated_submission_time': 60526.15056729317, 'accumulated_eval_time': 5615.292538166046, 'accumulated_logging_time': 2.351891279220581, 'global_step': 76591, 'preemption_count': 0})], 'global_step': 77276}
I0217 05:07:10.383615 140599226058560 submission_runner.py:586] Timing: 61068.48909711838
I0217 05:07:10.383682 140599226058560 submission_runner.py:588] Total number of evals: 43
I0217 05:07:10.383735 140599226058560 submission_runner.py:589] ====================
I0217 05:07:10.383791 140599226058560 submission_runner.py:542] Using RNG seed 4151861576
I0217 05:07:10.387288 140599226058560 submission_runner.py:551] --- Tuning run 5/5 ---
I0217 05:07:10.387449 140599226058560 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_5.
I0217 05:07:10.389114 140599226058560 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_5/hparams.json.
I0217 05:07:10.391313 140599226058560 submission_runner.py:206] Initializing dataset.
I0217 05:07:10.391438 140599226058560 submission_runner.py:213] Initializing model.
I0217 05:07:14.685007 140599226058560 submission_runner.py:255] Initializing optimizer.
I0217 05:07:15.127949 140599226058560 submission_runner.py:262] Initializing metrics bundle.
I0217 05:07:15.128167 140599226058560 submission_runner.py:280] Initializing checkpoint and logger.
I0217 05:07:15.132332 140599226058560 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_5 with prefix checkpoint_
I0217 05:07:15.132477 140599226058560 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_5/meta_data_0.json.
I0217 05:07:15.132700 140599226058560 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0217 05:07:15.132767 140599226058560 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0217 05:07:16.041708 140599226058560 logger_utils.py:220] Unable to record git information. Continuing without it.
I0217 05:07:16.928264 140599226058560 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_5/flags_0.json.
I0217 05:07:16.949042 140599226058560 submission_runner.py:314] Starting training loop.
I0217 05:07:16.952494 140599226058560 input_pipeline.py:20] Loading split = train-clean-100
I0217 05:07:17.000163 140599226058560 input_pipeline.py:20] Loading split = train-clean-360
I0217 05:07:17.137967 140599226058560 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0217 05:07:53.484528 140427168065280 logging_writer.py:48] [0] global_step=0, grad_norm=31.602928161621094, loss=33.002708435058594
I0217 05:07:53.513981 140599226058560 spec.py:321] Evaluating on the training split.
I0217 05:08:49.607201 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 05:09:40.230317 140599226058560 spec.py:349] Evaluating on the test split.
I0217 05:10:06.888588 140599226058560 submission_runner.py:408] Time since start: 169.94s, 	Step: 1, 	{'train/ctc_loss': Array(32.575264, dtype=float32), 'train/wer': 1.3520278272013182, 'validation/ctc_loss': Array(31.087046, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.237438, dtype=float32), 'test/wer': 1.1023297381837385, 'test/num_examples': 2472, 'score': 36.564847230911255, 'total_duration': 169.93663430213928, 'accumulated_submission_time': 36.564847230911255, 'accumulated_eval_time': 133.37171983718872, 'accumulated_logging_time': 0}
I0217 05:10:06.906627 140532797925120 logging_writer.py:48] [1] accumulated_eval_time=133.371720, accumulated_logging_time=0, accumulated_submission_time=36.564847, global_step=1, preemption_count=0, score=36.564847, test/ctc_loss=31.237438201904297, test/num_examples=2472, test/wer=1.102330, total_duration=169.936634, train/ctc_loss=32.57526397705078, train/wer=1.352028, validation/ctc_loss=31.087045669555664, validation/num_examples=5348, validation/wer=1.058565
I0217 05:11:49.383702 140427134494464 logging_writer.py:48] [100] global_step=100, grad_norm=1.1111317873001099, loss=6.025468826293945
I0217 05:13:05.443088 140427142887168 logging_writer.py:48] [200] global_step=200, grad_norm=0.5357886552810669, loss=5.832782745361328
I0217 05:14:21.458850 140427134494464 logging_writer.py:48] [300] global_step=300, grad_norm=1.040126085281372, loss=5.779180526733398
I0217 05:15:37.517162 140427142887168 logging_writer.py:48] [400] global_step=400, grad_norm=1.0732433795928955, loss=5.803054332733154
I0217 05:16:53.553695 140427134494464 logging_writer.py:48] [500] global_step=500, grad_norm=2.5742862224578857, loss=5.806151390075684
I0217 05:18:09.716892 140427142887168 logging_writer.py:48] [600] global_step=600, grad_norm=0.5501648783683777, loss=5.758833885192871
I0217 05:19:29.468479 140427134494464 logging_writer.py:48] [700] global_step=700, grad_norm=0.3067297637462616, loss=5.793675899505615
I0217 05:20:53.581122 140427142887168 logging_writer.py:48] [800] global_step=800, grad_norm=2.9358513355255127, loss=5.600902080535889
I0217 05:22:17.433606 140427134494464 logging_writer.py:48] [900] global_step=900, grad_norm=0.8309425711631775, loss=5.465376853942871
I0217 05:23:41.164670 140427142887168 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.496260643005371, loss=5.2033586502075195
I0217 05:25:02.978750 140532797925120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8159405589103699, loss=3.896624803543091
I0217 05:26:18.908673 140532789532416 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3236126899719238, loss=3.4096312522888184
I0217 05:27:34.789684 140532797925120 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.3257628679275513, loss=3.1328039169311523
I0217 05:28:50.601074 140532789532416 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.005112648010254, loss=3.1232078075408936
I0217 05:30:09.180845 140532797925120 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.272108554840088, loss=2.980215549468994
I0217 05:31:33.597092 140532789532416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7839556336402893, loss=2.737330198287964
I0217 05:32:57.045634 140532797925120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7906449437141418, loss=2.7563259601593018
I0217 05:34:07.049085 140599226058560 spec.py:321] Evaluating on the training split.
I0217 05:34:54.471806 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 05:35:44.754659 140599226058560 spec.py:349] Evaluating on the test split.
I0217 05:36:10.760049 140599226058560 submission_runner.py:408] Time since start: 1733.80s, 	Step: 1784, 	{'train/ctc_loss': Array(4.0700784, dtype=float32), 'train/wer': 0.6632959745030377, 'validation/ctc_loss': Array(3.8945782, dtype=float32), 'validation/wer': 0.6469486469003737, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.5804627, dtype=float32), 'test/wer': 0.5944590010765137, 'test/num_examples': 2472, 'score': 1476.6239953041077, 'total_duration': 1733.8049068450928, 'accumulated_submission_time': 1476.6239953041077, 'accumulated_eval_time': 257.0766489505768, 'accumulated_logging_time': 0.03051924705505371}
I0217 05:36:10.799131 140532797925120 logging_writer.py:48] [1784] accumulated_eval_time=257.076649, accumulated_logging_time=0.030519, accumulated_submission_time=1476.623995, global_step=1784, preemption_count=0, score=1476.623995, test/ctc_loss=3.580462694168091, test/num_examples=2472, test/wer=0.594459, total_duration=1733.804907, train/ctc_loss=4.070078372955322, train/wer=0.663296, validation/ctc_loss=3.894578218460083, validation/num_examples=5348, validation/wer=0.646949
I0217 05:36:23.708719 140532789532416 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9904401898384094, loss=2.6650915145874023
I0217 05:37:39.112929 140532797925120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6191192269325256, loss=2.3814921379089355
I0217 05:38:54.578611 140532789532416 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2429163455963135, loss=2.3514130115509033
I0217 05:40:14.173178 140532797925120 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9183548092842102, loss=2.3108675479888916
I0217 05:41:30.062527 140532789532416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6088218688964844, loss=2.269345998764038
I0217 05:42:45.578917 140532797925120 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7068114280700684, loss=2.202341318130493
I0217 05:44:01.151263 140532789532416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7689561247825623, loss=2.226228952407837
I0217 05:45:17.373045 140532797925120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5156540870666504, loss=2.1340646743774414
I0217 05:46:40.761173 140532789532416 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6038883328437805, loss=2.169796943664551
I0217 05:48:04.400094 140532797925120 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5384336709976196, loss=2.0663137435913086
I0217 05:49:27.701564 140532789532416 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6053292751312256, loss=2.092613697052002
I0217 05:50:51.246721 140532797925120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6014496088027954, loss=2.039494514465332
I0217 05:52:15.988479 140532789532416 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6621450185775757, loss=2.1079719066619873
I0217 05:53:42.240031 140532797925120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6100946068763733, loss=1.9963527917861938
I0217 05:54:57.590950 140532789532416 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5565285682678223, loss=1.9719387292861938
I0217 05:56:13.114304 140532797925120 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7105459570884705, loss=1.9391037225723267
I0217 05:57:28.871306 140532789532416 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7574368119239807, loss=2.0459647178649902
I0217 05:58:44.366105 140532797925120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5858682990074158, loss=1.982764720916748
I0217 06:00:02.270537 140532789532416 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.4564833641052246, loss=1.8262678384780884
I0217 06:00:11.855512 140599226058560 spec.py:321] Evaluating on the training split.
I0217 06:01:04.271228 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 06:01:55.314870 140599226058560 spec.py:349] Evaluating on the test split.
I0217 06:02:21.776741 140599226058560 submission_runner.py:408] Time since start: 3304.82s, 	Step: 3613, 	{'train/ctc_loss': Array(0.928824, dtype=float32), 'train/wer': 0.29132301859311877, 'validation/ctc_loss': Array(0.9600111, dtype=float32), 'validation/wer': 0.2800428666595866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6869836, dtype=float32), 'test/wer': 0.22074624743566307, 'test/num_examples': 2472, 'score': 2917.59091591835, 'total_duration': 3304.8210270404816, 'accumulated_submission_time': 2917.59091591835, 'accumulated_eval_time': 386.99128317832947, 'accumulated_logging_time': 0.0886681079864502}
I0217 06:02:21.814883 140532797925120 logging_writer.py:48] [3613] accumulated_eval_time=386.991283, accumulated_logging_time=0.088668, accumulated_submission_time=2917.590916, global_step=3613, preemption_count=0, score=2917.590916, test/ctc_loss=0.686983585357666, test/num_examples=2472, test/wer=0.220746, total_duration=3304.821027, train/ctc_loss=0.9288240075111389, train/wer=0.291323, validation/ctc_loss=0.9600111246109009, validation/num_examples=5348, validation/wer=0.280043
I0217 06:03:28.107411 140532789532416 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5285135507583618, loss=1.9340434074401855
I0217 06:04:43.700762 140532797925120 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.4613717794418335, loss=1.9013580083847046
I0217 06:05:59.282270 140532789532416 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.48554548621177673, loss=1.9100090265274048
I0217 06:07:14.938307 140532797925120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.46718189120292664, loss=1.8086268901824951
I0217 06:08:36.362750 140532789532416 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5801438689231873, loss=1.8826943635940552
I0217 06:09:56.704709 140532797925120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5836186408996582, loss=1.7977545261383057
I0217 06:11:12.353755 140532789532416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8217641115188599, loss=1.8199220895767212
I0217 06:12:27.961207 140532797925120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.4858357012271881, loss=1.8772661685943604
I0217 06:13:43.545440 140532789532416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7314131855964661, loss=1.7771114110946655
I0217 06:14:59.388547 140532797925120 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5291427969932556, loss=1.7348849773406982
I0217 06:16:21.624750 140532789532416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6004780530929565, loss=1.7821859121322632
I0217 06:17:45.874600 140532797925120 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6091508865356445, loss=1.7993427515029907
I0217 06:19:09.917427 140532789532416 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.1580538749694824, loss=1.7967592477798462
I0217 06:20:33.926812 140532797925120 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6097496747970581, loss=1.8422727584838867
I0217 06:21:58.128355 140532789532416 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7366995215415955, loss=1.7910995483398438
I0217 06:23:20.960046 140532797925120 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6180417537689209, loss=1.821862816810608
I0217 06:24:36.437432 140532789532416 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6710582971572876, loss=1.7666040658950806
I0217 06:25:51.897720 140532797925120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5334122180938721, loss=1.7546584606170654
I0217 06:26:21.778431 140599226058560 spec.py:321] Evaluating on the training split.
I0217 06:27:16.456740 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 06:28:07.868871 140599226058560 spec.py:349] Evaluating on the test split.
I0217 06:28:33.856402 140599226058560 submission_runner.py:408] Time since start: 4876.90s, 	Step: 5441, 	{'train/ctc_loss': Array(0.6443802, dtype=float32), 'train/wer': 0.2176905895966201, 'validation/ctc_loss': Array(0.83076894, dtype=float32), 'validation/wer': 0.24935072458171215, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55067784, dtype=float32), 'test/wer': 0.18333231775435174, 'test/num_examples': 2472, 'score': 4357.46843123436, 'total_duration': 4876.902319431305, 'accumulated_submission_time': 4357.46843123436, 'accumulated_eval_time': 519.0642786026001, 'accumulated_logging_time': 0.14203286170959473}
I0217 06:28:33.890043 140532797925120 logging_writer.py:48] [5441] accumulated_eval_time=519.064279, accumulated_logging_time=0.142033, accumulated_submission_time=4357.468431, global_step=5441, preemption_count=0, score=4357.468431, test/ctc_loss=0.5506778359413147, test/num_examples=2472, test/wer=0.183332, total_duration=4876.902319, train/ctc_loss=0.6443802118301392, train/wer=0.217691, validation/ctc_loss=0.8307689428329468, validation/num_examples=5348, validation/wer=0.249351
I0217 06:29:19.063720 140532789532416 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6498784422874451, loss=1.6825865507125854
I0217 06:30:34.598270 140532797925120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.4337829649448395, loss=1.7292026281356812
I0217 06:31:50.163135 140532789532416 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6356600522994995, loss=1.7460753917694092
I0217 06:33:06.108982 140532797925120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.47459912300109863, loss=1.7312067747116089
I0217 06:34:26.010847 140532789532416 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.9540112018585205, loss=1.7277956008911133
I0217 06:35:48.954801 140532797925120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5346854329109192, loss=1.7133936882019043
I0217 06:37:11.949976 140532789532416 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4185182750225067, loss=1.7601314783096313
I0217 06:38:37.964882 140532797925120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5057246088981628, loss=1.6969436407089233
I0217 06:39:53.327347 140532789532416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5374132394790649, loss=1.6587228775024414
I0217 06:41:08.799757 140532797925120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5537949204444885, loss=1.7595610618591309
I0217 06:42:24.322384 140532789532416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5641043186187744, loss=1.6394120454788208
I0217 06:43:39.813138 140532797925120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6759477257728577, loss=1.755283236503601
I0217 06:44:57.350478 140532789532416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5668618679046631, loss=1.7443809509277344
I0217 06:46:20.659082 140532797925120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5801516175270081, loss=1.6975153684616089
I0217 06:47:44.999722 140532789532416 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7068345546722412, loss=1.7698733806610107
I0217 06:49:09.177045 140532797925120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5803802609443665, loss=1.737980604171753
I0217 06:50:33.162238 140532789532416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6160405278205872, loss=1.748600721359253
I0217 06:51:56.616525 140532797925120 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5696513056755066, loss=1.7221715450286865
I0217 06:52:34.468604 140599226058560 spec.py:321] Evaluating on the training split.
I0217 06:53:28.504715 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 06:54:20.161361 140599226058560 spec.py:349] Evaluating on the test split.
I0217 06:54:46.120948 140599226058560 submission_runner.py:408] Time since start: 6449.17s, 	Step: 7246, 	{'train/ctc_loss': Array(0.5867469, dtype=float32), 'train/wer': 0.19511405655098257, 'validation/ctc_loss': Array(0.7002347, dtype=float32), 'validation/wer': 0.209592863280458, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45089793, dtype=float32), 'test/wer': 0.1516259419495054, 'test/num_examples': 2472, 'score': 5797.962051391602, 'total_duration': 6449.165390253067, 'accumulated_submission_time': 5797.962051391602, 'accumulated_eval_time': 650.7103319168091, 'accumulated_logging_time': 0.19083642959594727}
I0217 06:54:46.158148 140532797925120 logging_writer.py:48] [7246] accumulated_eval_time=650.710332, accumulated_logging_time=0.190836, accumulated_submission_time=5797.962051, global_step=7246, preemption_count=0, score=5797.962051, test/ctc_loss=0.4508979320526123, test/num_examples=2472, test/wer=0.151626, total_duration=6449.165390, train/ctc_loss=0.586746871471405, train/wer=0.195114, validation/ctc_loss=0.7002347111701965, validation/num_examples=5348, validation/wer=0.209593
I0217 06:55:27.489236 140532789532416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.44494009017944336, loss=1.630431056022644
I0217 06:56:42.911047 140532797925120 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.54888916015625, loss=1.6866658926010132
I0217 06:57:58.297232 140532789532416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4732632339000702, loss=1.6755096912384033
I0217 06:59:13.747326 140532797925120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4558637738227844, loss=1.6182003021240234
I0217 07:00:29.260420 140532789532416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.46713787317276, loss=1.6476174592971802
I0217 07:01:44.714863 140532797925120 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.39689451456069946, loss=1.6617172956466675
I0217 07:03:02.731447 140532789532416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5593388676643372, loss=1.5783946514129639
I0217 07:04:26.542461 140532797925120 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7437589168548584, loss=1.6850372552871704
I0217 07:05:49.273146 140532789532416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.6617213487625122, loss=1.537058711051941
I0217 07:07:13.147458 140532797925120 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4943324625492096, loss=1.6097248792648315
I0217 07:08:35.025374 140532797925120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4619073271751404, loss=1.5857864618301392
I0217 07:09:50.511183 140532789532416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.4575704336166382, loss=1.561903476715088
I0217 07:11:06.089578 140532797925120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5258591771125793, loss=1.610819697380066
I0217 07:12:21.629435 140532789532416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5780429840087891, loss=1.6231013536453247
I0217 07:13:38.563670 140532797925120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8576861619949341, loss=1.663784146308899
I0217 07:15:02.231464 140532789532416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5013875365257263, loss=1.646135926246643
I0217 07:16:25.312823 140532797925120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.5328883528709412, loss=1.6066207885742188
I0217 07:17:49.194067 140532789532416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5048539638519287, loss=1.6194318532943726
I0217 07:18:46.737695 140599226058560 spec.py:321] Evaluating on the training split.
I0217 07:19:41.456637 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 07:20:32.673636 140599226058560 spec.py:349] Evaluating on the test split.
I0217 07:20:58.833355 140599226058560 submission_runner.py:408] Time since start: 8021.88s, 	Step: 9070, 	{'train/ctc_loss': Array(0.4774239, dtype=float32), 'train/wer': 0.16464857563720095, 'validation/ctc_loss': Array(0.6440632, dtype=float32), 'validation/wer': 0.19560327099645675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.405954, dtype=float32), 'test/wer': 0.1373875246277903, 'test/num_examples': 2472, 'score': 7238.451512336731, 'total_duration': 8021.87784409523, 'accumulated_submission_time': 7238.451512336731, 'accumulated_eval_time': 782.7996170520782, 'accumulated_logging_time': 0.24584484100341797}
I0217 07:20:58.866471 140532797925120 logging_writer.py:48] [9070] accumulated_eval_time=782.799617, accumulated_logging_time=0.245845, accumulated_submission_time=7238.451512, global_step=9070, preemption_count=0, score=7238.451512, test/ctc_loss=0.4059540033340454, test/num_examples=2472, test/wer=0.137388, total_duration=8021.877844, train/ctc_loss=0.47742390632629395, train/wer=0.164649, validation/ctc_loss=0.6440631747245789, validation/num_examples=5348, validation/wer=0.195603
I0217 07:21:22.215541 140532789532416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4900905191898346, loss=1.6072654724121094
I0217 07:22:37.629929 140532797925120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.5275915265083313, loss=1.6194502115249634
I0217 07:23:56.796905 140532797925120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.479042112827301, loss=1.6176881790161133
I0217 07:25:12.464410 140532789532416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.4317319095134735, loss=1.5388929843902588
I0217 07:26:27.809954 140532797925120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.5563259124755859, loss=1.624462366104126
I0217 07:27:43.381727 140532789532416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.4547966420650482, loss=1.5289615392684937
I0217 07:28:59.021827 140532797925120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.4945554733276367, loss=1.5645428895950317
I0217 07:30:20.199362 140532789532416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5761000514030457, loss=1.5828056335449219
I0217 07:31:43.451326 140532797925120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.4450714588165283, loss=1.6233594417572021
I0217 07:33:06.642220 140532789532416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6087689995765686, loss=1.6308467388153076
I0217 07:34:30.729641 140532797925120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.48853540420532227, loss=1.5651085376739502
I0217 07:35:54.052600 140532789532416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.4062412679195404, loss=1.5128345489501953
I0217 07:37:22.398235 140532797925120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.5863297581672668, loss=1.5426247119903564
I0217 07:38:37.918395 140532789532416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.505581259727478, loss=1.5478276014328003
I0217 07:39:53.476320 140532797925120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.561187744140625, loss=1.6053001880645752
I0217 07:41:09.225867 140532789532416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.4365375339984894, loss=1.5021501779556274
I0217 07:42:24.756989 140532797925120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4403163194656372, loss=1.5380518436431885
I0217 07:43:43.063922 140532789532416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4953972101211548, loss=1.5619498491287231
I0217 07:44:59.163057 140599226058560 spec.py:321] Evaluating on the training split.
I0217 07:45:53.593596 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 07:46:45.131465 140599226058560 spec.py:349] Evaluating on the test split.
I0217 07:47:11.323310 140599226058560 submission_runner.py:408] Time since start: 9594.37s, 	Step: 10892, 	{'train/ctc_loss': Array(0.4616692, dtype=float32), 'train/wer': 0.16355717701154388, 'validation/ctc_loss': Array(0.62427807, dtype=float32), 'validation/wer': 0.18911534414010833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38973525, dtype=float32), 'test/wer': 0.13174090549021997, 'test/num_examples': 2472, 'score': 8678.661425828934, 'total_duration': 9594.367964029312, 'accumulated_submission_time': 8678.661425828934, 'accumulated_eval_time': 914.9536378383636, 'accumulated_logging_time': 0.29439759254455566}
I0217 07:47:11.358360 140532797925120 logging_writer.py:48] [10892] accumulated_eval_time=914.953638, accumulated_logging_time=0.294398, accumulated_submission_time=8678.661426, global_step=10892, preemption_count=0, score=8678.661426, test/ctc_loss=0.38973525166511536, test/num_examples=2472, test/wer=0.131741, total_duration=9594.367964, train/ctc_loss=0.4616692066192627, train/wer=0.163557, validation/ctc_loss=0.6242780685424805, validation/num_examples=5348, validation/wer=0.189115
I0217 07:47:18.225154 140532789532416 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.5642609000205994, loss=1.603989601135254
I0217 07:48:33.473097 140532797925120 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.590538740158081, loss=1.5376683473587036
I0217 07:49:48.873845 140532789532416 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.4298078715801239, loss=1.504211664199829
I0217 07:51:04.225407 140532797925120 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.5804749727249146, loss=1.49897038936615
I0217 07:52:22.732977 140532789532416 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.4811657667160034, loss=1.4967308044433594
I0217 07:53:44.299351 140532797925120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5042031407356262, loss=1.504414677619934
I0217 07:55:00.014682 140532789532416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.5589560270309448, loss=1.5145351886749268
I0217 07:56:15.656723 140532797925120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.5404307246208191, loss=1.5780082941055298
I0217 07:57:31.242400 140532789532416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5867503881454468, loss=1.5292041301727295
I0217 07:58:47.102719 140532797925120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5651140213012695, loss=1.5491517782211304
I0217 08:00:07.453596 140532789532416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5212663412094116, loss=1.4822533130645752
I0217 08:01:31.409112 140532797925120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4374554753303528, loss=1.582056999206543
I0217 08:02:54.588770 140532789532416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.6234789490699768, loss=1.5566959381103516
I0217 08:04:17.756012 140532797925120 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.5413187146186829, loss=1.531508445739746
I0217 08:05:41.609158 140532789532416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5130208134651184, loss=1.459682822227478
I0217 08:07:05.126801 140532797925120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.43070468306541443, loss=1.4652273654937744
I0217 08:08:20.655316 140532789532416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5802744030952454, loss=1.5257457494735718
I0217 08:09:36.151212 140532797925120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.523381769657135, loss=1.5189706087112427
I0217 08:10:51.838848 140532789532416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.49448034167289734, loss=1.5483686923980713
I0217 08:11:11.987109 140599226058560 spec.py:321] Evaluating on the training split.
I0217 08:12:05.423126 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 08:12:56.698815 140599226058560 spec.py:349] Evaluating on the test split.
I0217 08:13:22.691042 140599226058560 submission_runner.py:408] Time since start: 11165.74s, 	Step: 12728, 	{'train/ctc_loss': Array(0.42614308, dtype=float32), 'train/wer': 0.14673770491803279, 'validation/ctc_loss': Array(0.5865799, dtype=float32), 'validation/wer': 0.17795456520269945, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36318412, dtype=float32), 'test/wer': 0.12253975991712876, 'test/num_examples': 2472, 'score': 10119.200992584229, 'total_duration': 11165.735782384872, 'accumulated_submission_time': 10119.200992584229, 'accumulated_eval_time': 1045.6514217853546, 'accumulated_logging_time': 0.3471841812133789}
I0217 08:13:22.723659 140532797925120 logging_writer.py:48] [12728] accumulated_eval_time=1045.651422, accumulated_logging_time=0.347184, accumulated_submission_time=10119.200993, global_step=12728, preemption_count=0, score=10119.200993, test/ctc_loss=0.3631841242313385, test/num_examples=2472, test/wer=0.122540, total_duration=11165.735782, train/ctc_loss=0.426143079996109, train/wer=0.146738, validation/ctc_loss=0.5865799188613892, validation/num_examples=5348, validation/wer=0.177955
I0217 08:14:17.620096 140532789532416 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.5459581613540649, loss=1.5175336599349976
I0217 08:15:33.119610 140532797925120 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5817553400993347, loss=1.5068912506103516
I0217 08:16:48.852632 140532789532416 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.45659729838371277, loss=1.4885896444320679
I0217 08:18:04.287538 140532797925120 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.4342293441295624, loss=1.479851484298706
I0217 08:19:26.279126 140532789532416 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.5191413760185242, loss=1.5479683876037598
I0217 08:20:50.324807 140532797925120 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.4464399218559265, loss=1.4963048696517944
I0217 08:22:16.025033 140532797925120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.5663481950759888, loss=1.5277234315872192
I0217 08:23:31.460076 140532789532416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.43926599621772766, loss=1.4299627542495728
I0217 08:24:46.851255 140532797925120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.6051182746887207, loss=1.5209630727767944
I0217 08:26:02.309480 140532789532416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.45150285959243774, loss=1.4467841386795044
I0217 08:27:17.728448 140532797925120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.46978646516799927, loss=1.437312364578247
I0217 08:28:37.748655 140532789532416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5541063547134399, loss=1.5013427734375
I0217 08:30:01.713210 140532797925120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5259518027305603, loss=1.4548313617706299
I0217 08:31:26.272847 140532789532416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5672862529754639, loss=1.5142219066619873
I0217 08:32:49.026101 140532797925120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.5267939567565918, loss=1.492449402809143
I0217 08:34:11.059413 140532789532416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.49178531765937805, loss=1.503144383430481
I0217 08:35:34.218180 140532797925120 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.47695931792259216, loss=1.4879786968231201
I0217 08:36:54.774799 140532797925120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6239120960235596, loss=1.5349736213684082
I0217 08:37:23.165577 140599226058560 spec.py:321] Evaluating on the training split.
I0217 08:38:15.738706 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 08:39:06.774994 140599226058560 spec.py:349] Evaluating on the test split.
I0217 08:39:32.867209 140599226058560 submission_runner.py:408] Time since start: 12735.91s, 	Step: 14539, 	{'train/ctc_loss': Array(0.43106797, dtype=float32), 'train/wer': 0.15068626163046397, 'validation/ctc_loss': Array(0.58034813, dtype=float32), 'validation/wer': 0.17549262867238866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3582644, dtype=float32), 'test/wer': 0.1215444925151829, 'test/num_examples': 2472, 'score': 11559.555562257767, 'total_duration': 12735.91278553009, 'accumulated_submission_time': 11559.555562257767, 'accumulated_eval_time': 1175.3477528095245, 'accumulated_logging_time': 0.3970916271209717}
I0217 08:39:32.909480 140532797925120 logging_writer.py:48] [14539] accumulated_eval_time=1175.347753, accumulated_logging_time=0.397092, accumulated_submission_time=11559.555562, global_step=14539, preemption_count=0, score=11559.555562, test/ctc_loss=0.35826438665390015, test/num_examples=2472, test/wer=0.121544, total_duration=12735.912786, train/ctc_loss=0.43106797337532043, train/wer=0.150686, validation/ctc_loss=0.5803481340408325, validation/num_examples=5348, validation/wer=0.175493
I0217 08:40:19.618488 140532789532416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.5542355179786682, loss=1.5107675790786743
I0217 08:41:35.144872 140532797925120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.5620632171630859, loss=1.4683096408843994
I0217 08:42:50.783363 140532789532416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7135774493217468, loss=1.4650936126708984
I0217 08:44:06.415670 140532797925120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.601992130279541, loss=1.4838794469833374
I0217 08:45:22.026615 140532789532416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4495301842689514, loss=1.4936658143997192
I0217 08:46:39.352872 140532797925120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.6225218176841736, loss=1.4494853019714355
I0217 08:48:02.864506 140532789532416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.6314373016357422, loss=1.4659831523895264
I0217 08:49:26.236835 140532797925120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5562476515769958, loss=1.4832285642623901
I0217 08:50:49.503693 140532789532416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.461752325296402, loss=1.4419301748275757
I0217 08:52:12.839682 140532797925120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.5372937321662903, loss=1.4581587314605713
I0217 08:53:28.481007 140532789532416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.4887843132019043, loss=1.39873468875885
I0217 08:54:44.101879 140532797925120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.4652007222175598, loss=1.4527684450149536
I0217 08:55:59.725449 140532789532416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.5076950192451477, loss=1.474132776260376
I0217 08:57:15.373116 140532797925120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6578466892242432, loss=1.5268127918243408
I0217 08:58:38.333449 140532789532416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5660950541496277, loss=1.4213287830352783
I0217 09:00:01.086218 140532797925120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.69371098279953, loss=1.4991436004638672
I0217 09:01:24.438505 140532789532416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6053857207298279, loss=1.5160081386566162
I0217 09:02:49.217920 140532797925120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.6651231646537781, loss=1.522214651107788
I0217 09:03:33.929246 140599226058560 spec.py:321] Evaluating on the training split.
I0217 09:04:27.744663 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 09:05:19.573837 140599226058560 spec.py:349] Evaluating on the test split.
I0217 09:05:45.682260 140599226058560 submission_runner.py:408] Time since start: 14308.73s, 	Step: 16355, 	{'train/ctc_loss': Array(0.45272714, dtype=float32), 'train/wer': 0.15471028120091151, 'validation/ctc_loss': Array(0.5875047, dtype=float32), 'validation/wer': 0.17651602189675314, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3553724, dtype=float32), 'test/wer': 0.1196758271890805, 'test/num_examples': 2472, 'score': 13000.484461307526, 'total_duration': 14308.726927280426, 'accumulated_submission_time': 13000.484461307526, 'accumulated_eval_time': 1307.0945451259613, 'accumulated_logging_time': 0.4598226547241211}
I0217 09:05:45.722148 140532797925120 logging_writer.py:48] [16355] accumulated_eval_time=1307.094545, accumulated_logging_time=0.459823, accumulated_submission_time=13000.484461, global_step=16355, preemption_count=0, score=13000.484461, test/ctc_loss=0.3553723990917206, test/num_examples=2472, test/wer=0.119676, total_duration=14308.726927, train/ctc_loss=0.45272713899612427, train/wer=0.154710, validation/ctc_loss=0.5875046849250793, validation/num_examples=5348, validation/wer=0.176516
I0217 09:06:20.454924 140532789532416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.46087637543678284, loss=1.500045657157898
I0217 09:07:40.003208 140532797925120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.4236809313297272, loss=1.510168433189392
I0217 09:08:55.698180 140532789532416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.4939514398574829, loss=1.497377872467041
I0217 09:10:11.222016 140532797925120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.8750602602958679, loss=1.4713733196258545
I0217 09:11:26.859765 140532789532416 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.082231044769287, loss=1.4619290828704834
I0217 09:12:42.427079 140532797925120 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.915216863155365, loss=1.5545047521591187
I0217 09:14:04.060975 140532789532416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.47996383905410767, loss=1.3983421325683594
I0217 09:15:27.605834 140532797925120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5759764313697815, loss=1.462334394454956
I0217 09:16:50.450932 140532789532416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.47631606459617615, loss=1.4902414083480835
I0217 09:18:16.200698 140532797925120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.7697952389717102, loss=1.4301743507385254
I0217 09:19:40.245311 140532789532416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.6215478777885437, loss=1.473379135131836
I0217 09:21:03.435389 140532797925120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.40088850259780884, loss=1.4180325269699097
I0217 09:22:23.394381 140532797925120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.5436376333236694, loss=1.5165444612503052
I0217 09:23:38.864711 140532789532416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.7923452258110046, loss=1.5526890754699707
I0217 09:24:54.745619 140532797925120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.48811763525009155, loss=1.5118745565414429
I0217 09:26:10.232036 140532789532416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.4662702679634094, loss=1.4481018781661987
I0217 09:27:28.323940 140532797925120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.42892521619796753, loss=1.426894187927246
I0217 09:28:52.779044 140532789532416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.529453456401825, loss=1.4568865299224854
I0217 09:29:45.963875 140599226058560 spec.py:321] Evaluating on the training split.
I0217 09:30:39.816178 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 09:31:31.271935 140599226058560 spec.py:349] Evaluating on the test split.
I0217 09:31:57.771645 140599226058560 submission_runner.py:408] Time since start: 15880.82s, 	Step: 18164, 	{'train/ctc_loss': Array(0.43035367, dtype=float32), 'train/wer': 0.1507336027182378, 'validation/ctc_loss': Array(0.57992405, dtype=float32), 'validation/wer': 0.17502920532550661, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34942833, dtype=float32), 'test/wer': 0.11839619767229298, 'test/num_examples': 2472, 'score': 14440.638258457184, 'total_duration': 15880.81642794609, 'accumulated_submission_time': 14440.638258457184, 'accumulated_eval_time': 1438.896250963211, 'accumulated_logging_time': 0.5163545608520508}
I0217 09:31:57.809779 140532797925120 logging_writer.py:48] [18164] accumulated_eval_time=1438.896251, accumulated_logging_time=0.516355, accumulated_submission_time=14440.638258, global_step=18164, preemption_count=0, score=14440.638258, test/ctc_loss=0.34942832589149475, test/num_examples=2472, test/wer=0.118396, total_duration=15880.816428, train/ctc_loss=0.43035367131233215, train/wer=0.150734, validation/ctc_loss=0.5799240469932556, validation/num_examples=5348, validation/wer=0.175029
I0217 09:32:25.715104 140532789532416 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7390851378440857, loss=1.416477918624878
I0217 09:33:41.332904 140532797925120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5293729305267334, loss=1.4695806503295898
I0217 09:34:56.951701 140532789532416 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.4784559905529022, loss=1.4654372930526733
I0217 09:36:12.526399 140532797925120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4635230600833893, loss=1.3954788446426392
I0217 09:37:31.951527 140532797925120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5286649465560913, loss=1.4973180294036865
I0217 09:38:47.534097 140532789532416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.4974859356880188, loss=1.5010069608688354
I0217 09:40:03.045644 140532797925120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.5846558213233948, loss=1.4916820526123047
I0217 09:41:18.588886 140532789532416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.43074169754981995, loss=1.427189588546753
I0217 09:42:34.397225 140532797925120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5953646302223206, loss=1.4364900588989258
I0217 09:43:56.332681 140532789532416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5694507360458374, loss=1.4404908418655396
I0217 09:45:19.259442 140532797925120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.5865461826324463, loss=1.5083081722259521
I0217 09:46:42.394464 140532789532416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5790281295776367, loss=1.4692105054855347
I0217 09:48:05.875865 140532797925120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.6137593984603882, loss=1.4534465074539185
I0217 09:49:29.882847 140532789532416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5159305334091187, loss=1.4603749513626099
I0217 09:50:54.245988 140532797925120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5289775133132935, loss=1.4308699369430542
I0217 09:52:09.896727 140532789532416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.6674194931983948, loss=1.4813225269317627
I0217 09:53:25.622811 140532797925120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.8105933666229248, loss=1.4027020931243896
I0217 09:54:41.322357 140532789532416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5259135365486145, loss=1.4062809944152832
I0217 09:55:57.058169 140532797925120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.49956580996513367, loss=1.4528623819351196
I0217 09:55:58.289532 140599226058560 spec.py:321] Evaluating on the training split.
I0217 09:56:49.949574 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 09:57:40.940778 140599226058560 spec.py:349] Evaluating on the test split.
I0217 09:58:07.192005 140599226058560 submission_runner.py:408] Time since start: 17450.24s, 	Step: 20003, 	{'train/ctc_loss': Array(0.42334485, dtype=float32), 'train/wer': 0.14974843245075295, 'validation/ctc_loss': Array(0.537798, dtype=float32), 'validation/wer': 0.16227540863319076, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32704383, dtype=float32), 'test/wer': 0.10889037840472854, 'test/num_examples': 2472, 'score': 15881.029235124588, 'total_duration': 17450.236847639084, 'accumulated_submission_time': 15881.029235124588, 'accumulated_eval_time': 1567.7926728725433, 'accumulated_logging_time': 0.5708911418914795}
I0217 09:58:07.228909 140532797925120 logging_writer.py:48] [20003] accumulated_eval_time=1567.792673, accumulated_logging_time=0.570891, accumulated_submission_time=15881.029235, global_step=20003, preemption_count=0, score=15881.029235, test/ctc_loss=0.3270438313484192, test/num_examples=2472, test/wer=0.108890, total_duration=17450.236848, train/ctc_loss=0.42334485054016113, train/wer=0.149748, validation/ctc_loss=0.5377979874610901, validation/num_examples=5348, validation/wer=0.162275
I0217 09:59:20.951768 140532789532416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6642165184020996, loss=1.4035331010818481
I0217 10:00:36.712458 140532797925120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.46698522567749023, loss=1.4255023002624512
I0217 10:01:52.098662 140532789532416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5560294389724731, loss=1.3634763956069946
I0217 10:03:09.184067 140532797925120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.453230082988739, loss=1.373133659362793
I0217 10:04:32.262018 140532789532416 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.7151331901550293, loss=1.4776325225830078
I0217 10:05:59.599831 140532797925120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.6250527501106262, loss=1.360205888748169
I0217 10:07:15.011439 140532789532416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5685678124427795, loss=1.4165444374084473
I0217 10:08:30.520770 140532797925120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6277438402175903, loss=1.4217697381973267
I0217 10:09:46.048433 140532789532416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.5684449672698975, loss=1.3510628938674927
I0217 10:11:01.535802 140532797925120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.5516678690910339, loss=1.4119212627410889
I0217 10:12:19.131071 140532789532416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.5465281009674072, loss=1.3668957948684692
I0217 10:13:42.136363 140532797925120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4738143980503082, loss=1.3955093622207642
I0217 10:15:06.697315 140532789532416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.653144896030426, loss=1.4051249027252197
I0217 10:16:29.577904 140532797925120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.5458422899246216, loss=1.4476228952407837
I0217 10:17:53.031744 140532789532416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6226677894592285, loss=1.3861016035079956
I0217 10:19:16.673816 140532797925120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5908609628677368, loss=1.4241796731948853
I0217 10:20:38.794581 140532797925120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5846583247184753, loss=1.4061996936798096
I0217 10:21:54.379548 140532789532416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.48570001125335693, loss=1.4232968091964722
I0217 10:22:07.656491 140599226058560 spec.py:321] Evaluating on the training split.
I0217 10:23:01.985641 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 10:23:53.828502 140599226058560 spec.py:349] Evaluating on the test split.
I0217 10:24:20.195818 140599226058560 submission_runner.py:408] Time since start: 19023.24s, 	Step: 21819, 	{'train/ctc_loss': Array(0.35540873, dtype=float32), 'train/wer': 0.12463731411488127, 'validation/ctc_loss': Array(0.52952045, dtype=float32), 'validation/wer': 0.1576218658582504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31939596, dtype=float32), 'test/wer': 0.10728576361383625, 'test/num_examples': 2472, 'score': 17321.37030696869, 'total_duration': 19023.241703748703, 'accumulated_submission_time': 17321.37030696869, 'accumulated_eval_time': 1700.3270015716553, 'accumulated_logging_time': 0.6231732368469238}
I0217 10:24:20.232723 140532797925120 logging_writer.py:48] [21819] accumulated_eval_time=1700.327002, accumulated_logging_time=0.623173, accumulated_submission_time=17321.370307, global_step=21819, preemption_count=0, score=17321.370307, test/ctc_loss=0.3193959593772888, test/num_examples=2472, test/wer=0.107286, total_duration=19023.241704, train/ctc_loss=0.3554087281227112, train/wer=0.124637, validation/ctc_loss=0.5295204520225525, validation/num_examples=5348, validation/wer=0.157622
I0217 10:25:22.016043 140532789532416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5548255443572998, loss=1.4211705923080444
I0217 10:26:37.591114 140532797925120 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.4134998321533203, loss=1.43521249294281
I0217 10:27:53.170155 140532789532416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5060121417045593, loss=1.4242874383926392
I0217 10:29:08.710120 140532797925120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.5712320804595947, loss=1.423214077949524
I0217 10:30:28.404587 140532789532416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.6922634840011597, loss=1.417751431465149
I0217 10:31:51.429079 140532797925120 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.1465119123458862, loss=1.3204108476638794
I0217 10:33:15.381257 140532789532416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.7556806802749634, loss=1.3904531002044678
I0217 10:34:38.764535 140532797925120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.912283718585968, loss=1.4635508060455322
I0217 10:36:02.381129 140532797925120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.7245171666145325, loss=1.41964590549469
I0217 10:37:17.793622 140532789532416 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.222275972366333, loss=1.3405810594558716
I0217 10:38:33.331414 140532797925120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5613435506820679, loss=1.401066541671753
I0217 10:39:48.856612 140532789532416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4611721932888031, loss=1.4155489206314087
I0217 10:41:04.381493 140532797925120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.5411689281463623, loss=1.3226919174194336
I0217 10:42:25.539598 140532789532416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.4862135350704193, loss=1.3539944887161255
I0217 10:43:49.683947 140532797925120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.6379011869430542, loss=1.4520074129104614
I0217 10:45:14.033985 140532789532416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.4312800467014313, loss=1.3544427156448364
I0217 10:46:37.880064 140532797925120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8808856010437012, loss=1.3985339403152466
I0217 10:48:02.277110 140532789532416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.42200255393981934, loss=1.3335938453674316
I0217 10:48:20.487542 140599226058560 spec.py:321] Evaluating on the training split.
I0217 10:49:13.675036 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 10:50:05.163320 140599226058560 spec.py:349] Evaluating on the test split.
I0217 10:50:31.340164 140599226058560 submission_runner.py:408] Time since start: 20594.38s, 	Step: 23623, 	{'train/ctc_loss': Array(0.35186237, dtype=float32), 'train/wer': 0.1239148128052089, 'validation/ctc_loss': Array(0.5021615, dtype=float32), 'validation/wer': 0.1515297797773637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30010766, dtype=float32), 'test/wer': 0.10245160766152783, 'test/num_examples': 2472, 'score': 18761.539265871048, 'total_duration': 20594.3846950531, 'accumulated_submission_time': 18761.539265871048, 'accumulated_eval_time': 1831.173261165619, 'accumulated_logging_time': 0.6752007007598877}
I0217 10:50:31.383068 140532797925120 logging_writer.py:48] [23623] accumulated_eval_time=1831.173261, accumulated_logging_time=0.675201, accumulated_submission_time=18761.539266, global_step=23623, preemption_count=0, score=18761.539266, test/ctc_loss=0.3001076579093933, test/num_examples=2472, test/wer=0.102452, total_duration=20594.384695, train/ctc_loss=0.351862370967865, train/wer=0.123915, validation/ctc_loss=0.5021615028381348, validation/num_examples=5348, validation/wer=0.151530
I0217 10:51:34.229344 140532797925120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.5837472677230835, loss=1.3188447952270508
I0217 10:52:49.768724 140532789532416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.603224515914917, loss=1.4049264192581177
I0217 10:54:05.283259 140532797925120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.642625093460083, loss=1.3738871812820435
I0217 10:55:20.936557 140532789532416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.621732771396637, loss=1.435784101486206
I0217 10:56:36.623987 140532797925120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.5142518281936646, loss=1.4120365381240845
I0217 10:57:56.930958 140532789532416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.5864707827568054, loss=1.3971163034439087
I0217 10:59:20.081223 140532797925120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.6111005544662476, loss=1.3678938150405884
I0217 11:00:43.920287 140532789532416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.5185257792472839, loss=1.457008719444275
I0217 11:02:08.521746 140532797925120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.43183502554893494, loss=1.3324304819107056
I0217 11:03:31.885521 140532789532416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.5624997615814209, loss=1.4003500938415527
I0217 11:04:55.258095 140532797925120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7744267582893372, loss=1.3976914882659912
I0217 11:06:15.894798 140532797925120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.6559687852859497, loss=1.3704465627670288
I0217 11:07:31.453630 140532789532416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.5134965181350708, loss=1.4034514427185059
I0217 11:08:47.178570 140532797925120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5601018667221069, loss=1.3744652271270752
I0217 11:10:02.681571 140532789532416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.5776905417442322, loss=1.3778034448623657
I0217 11:11:18.618392 140532797925120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.718336820602417, loss=1.4210240840911865
I0217 11:12:42.877112 140532789532416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.8692370057106018, loss=1.3896313905715942
I0217 11:14:07.037731 140532797925120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.5376715660095215, loss=1.4127256870269775
I0217 11:14:32.067165 140599226058560 spec.py:321] Evaluating on the training split.
I0217 11:15:25.147047 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 11:16:16.645680 140599226058560 spec.py:349] Evaluating on the test split.
I0217 11:16:43.155977 140599226058560 submission_runner.py:408] Time since start: 22166.20s, 	Step: 25432, 	{'train/ctc_loss': Array(0.36941093, dtype=float32), 'train/wer': 0.13036828062065567, 'validation/ctc_loss': Array(0.5116533, dtype=float32), 'validation/wer': 0.1539530976954343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3096689, dtype=float32), 'test/wer': 0.10352812138199988, 'test/num_examples': 2472, 'score': 20202.135964870453, 'total_duration': 22166.200580835342, 'accumulated_submission_time': 20202.135964870453, 'accumulated_eval_time': 1962.2557699680328, 'accumulated_logging_time': 0.7344212532043457}
I0217 11:16:43.193929 140532797925120 logging_writer.py:48] [25432] accumulated_eval_time=1962.255770, accumulated_logging_time=0.734421, accumulated_submission_time=20202.135965, global_step=25432, preemption_count=0, score=20202.135965, test/ctc_loss=0.3096688985824585, test/num_examples=2472, test/wer=0.103528, total_duration=22166.200581, train/ctc_loss=0.3694109320640564, train/wer=0.130368, validation/ctc_loss=0.5116533041000366, validation/num_examples=5348, validation/wer=0.153953
I0217 11:17:35.083924 140532789532416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5403379797935486, loss=1.4151082038879395
I0217 11:18:50.458307 140532797925120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6005688905715942, loss=1.4075583219528198
I0217 11:20:05.855172 140532789532416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.5520775318145752, loss=1.3196252584457397
I0217 11:21:25.419173 140532797925120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4843461811542511, loss=1.3554797172546387
I0217 11:22:40.821829 140532789532416 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2415069341659546, loss=1.4423978328704834
I0217 11:23:56.227528 140532797925120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.47069239616394043, loss=1.3482307195663452
I0217 11:25:11.762773 140532789532416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.6011618971824646, loss=1.3899024724960327
I0217 11:26:28.503474 140532797925120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.5762352347373962, loss=1.3737374544143677
I0217 11:27:52.354655 140532789532416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4888979196548462, loss=1.38670814037323
I0217 11:29:15.348316 140532797925120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.6052155494689941, loss=1.4269896745681763
I0217 11:30:37.264996 140532789532416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5633151531219482, loss=1.3842967748641968
I0217 11:32:01.095895 140532797925120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.5233055949211121, loss=1.312700629234314
I0217 11:33:24.630394 140532789532416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.7867082357406616, loss=1.4264591932296753
I0217 11:34:50.576997 140532797925120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.6060754656791687, loss=1.4364570379257202
I0217 11:36:06.011672 140532789532416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.6927255392074585, loss=1.3613194227218628
I0217 11:37:21.545969 140532797925120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5388704538345337, loss=1.3525538444519043
I0217 11:38:37.014386 140532789532416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4208076596260071, loss=1.371074914932251
I0217 11:39:52.547511 140532797925120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8844957947731018, loss=1.3683840036392212
I0217 11:40:43.414515 140599226058560 spec.py:321] Evaluating on the training split.
I0217 11:41:37.603726 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 11:42:29.936247 140599226058560 spec.py:349] Evaluating on the test split.
I0217 11:42:56.394359 140599226058560 submission_runner.py:408] Time since start: 23739.44s, 	Step: 27267, 	{'train/ctc_loss': Array(0.35059008, dtype=float32), 'train/wer': 0.12348275975702187, 'validation/ctc_loss': Array(0.49381366, dtype=float32), 'validation/wer': 0.14929955492049393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29647806, dtype=float32), 'test/wer': 0.09820648751853432, 'test/num_examples': 2472, 'score': 21642.26782298088, 'total_duration': 23739.43984937668, 'accumulated_submission_time': 21642.26782298088, 'accumulated_eval_time': 2095.2302215099335, 'accumulated_logging_time': 0.7898995876312256}
I0217 11:42:56.431460 140532797925120 logging_writer.py:48] [27267] accumulated_eval_time=2095.230222, accumulated_logging_time=0.789900, accumulated_submission_time=21642.267823, global_step=27267, preemption_count=0, score=21642.267823, test/ctc_loss=0.2964780628681183, test/num_examples=2472, test/wer=0.098206, total_duration=23739.439849, train/ctc_loss=0.3505900800228119, train/wer=0.123483, validation/ctc_loss=0.4938136637210846, validation/num_examples=5348, validation/wer=0.149300
I0217 11:43:22.066374 140532789532416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.649187445640564, loss=1.3086737394332886
I0217 11:44:37.837985 140532797925120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.54520583152771, loss=1.4081027507781982
I0217 11:45:53.371533 140532789532416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.624902606010437, loss=1.285745620727539
I0217 11:47:08.788911 140532797925120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.6684991121292114, loss=1.4142378568649292
I0217 11:48:26.677829 140532789532416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4373987019062042, loss=1.342360258102417
I0217 11:49:50.215065 140532797925120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.6114307045936584, loss=1.3675768375396729
I0217 11:51:10.197611 140532797925120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.6437398791313171, loss=1.2956491708755493
I0217 11:52:25.793742 140532789532416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.47106197476387024, loss=1.2900538444519043
I0217 11:53:41.266376 140532797925120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.5181407332420349, loss=1.3285820484161377
I0217 11:54:56.826121 140532789532416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.6254003047943115, loss=1.3381304740905762
I0217 11:56:12.400659 140532797925120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5551865100860596, loss=1.3516584634780884
I0217 11:57:36.904389 140532789532416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.9925357699394226, loss=1.3464500904083252
I0217 11:58:59.548571 140532797925120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5317108035087585, loss=1.339372158050537
I0217 12:00:22.667446 140532789532416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.5113829374313354, loss=1.4252084493637085
I0217 12:01:46.340278 140532797925120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.9177077412605286, loss=1.3525258302688599
I0217 12:03:10.139109 140532789532416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.6954322457313538, loss=1.3526971340179443
I0217 12:04:32.417160 140532797925120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.495150089263916, loss=1.3436729907989502
I0217 12:05:48.064515 140532789532416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8113992214202881, loss=1.2815438508987427
I0217 12:06:57.103522 140599226058560 spec.py:321] Evaluating on the training split.
I0217 12:07:49.447691 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 12:08:41.111694 140599226058560 spec.py:349] Evaluating on the test split.
I0217 12:09:07.506035 140599226058560 submission_runner.py:408] Time since start: 25310.55s, 	Step: 29093, 	{'train/ctc_loss': Array(0.31172824, dtype=float32), 'train/wer': 0.11294081880826916, 'validation/ctc_loss': Array(0.48430818, dtype=float32), 'validation/wer': 0.14549562161483728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28592485, dtype=float32), 'test/wer': 0.09530193163122296, 'test/num_examples': 2472, 'score': 23082.85145688057, 'total_duration': 25310.55036687851, 'accumulated_submission_time': 23082.85145688057, 'accumulated_eval_time': 2225.6262097358704, 'accumulated_logging_time': 0.8439171314239502}
I0217 12:09:07.542366 140532797925120 logging_writer.py:48] [29093] accumulated_eval_time=2225.626210, accumulated_logging_time=0.843917, accumulated_submission_time=23082.851457, global_step=29093, preemption_count=0, score=23082.851457, test/ctc_loss=0.28592485189437866, test/num_examples=2472, test/wer=0.095302, total_duration=25310.550367, train/ctc_loss=0.31172823905944824, train/wer=0.112941, validation/ctc_loss=0.4843081831932068, validation/num_examples=5348, validation/wer=0.145496
I0217 12:09:13.675433 140532789532416 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5982519388198853, loss=1.3501131534576416
I0217 12:10:29.068309 140532797925120 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.8476515412330627, loss=1.2974553108215332
I0217 12:11:44.565454 140532789532416 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.5258945822715759, loss=1.3613572120666504
I0217 12:13:00.102962 140532797925120 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.5832278728485107, loss=1.3280130624771118
I0217 12:14:15.737980 140532789532416 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9216139316558838, loss=1.333441972732544
I0217 12:15:39.261287 140532797925120 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.5614407062530518, loss=1.3513981103897095
I0217 12:17:02.126438 140532789532416 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.5941117405891418, loss=1.289391040802002
I0217 12:18:25.226884 140532797925120 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.46508845686912537, loss=1.3059055805206299
I0217 12:19:50.543717 140532797925120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5935284495353699, loss=1.3056132793426514
I0217 12:21:06.039965 140532789532416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6574689745903015, loss=1.3502511978149414
I0217 12:22:21.521099 140532797925120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.6577470898628235, loss=1.3281556367874146
I0217 12:23:37.105060 140532789532416 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.0939162969589233, loss=1.3602451086044312
I0217 12:24:52.473508 140532797925120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.47425463795661926, loss=1.3251025676727295
I0217 12:26:12.577547 140532789532416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.9661639928817749, loss=1.2981629371643066
I0217 12:27:35.203183 140532797925120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6100384593009949, loss=1.3414462804794312
I0217 12:28:58.706862 140532789532416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.6675832271575928, loss=1.3504369258880615
I0217 12:30:22.513792 140532797925120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.7383177876472473, loss=1.3629748821258545
I0217 12:31:45.601344 140532789532416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.5462631583213806, loss=1.297229528427124
I0217 12:33:07.998779 140599226058560 spec.py:321] Evaluating on the training split.
I0217 12:33:59.376323 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 12:34:50.779665 140599226058560 spec.py:349] Evaluating on the test split.
I0217 12:35:17.052362 140599226058560 submission_runner.py:408] Time since start: 26880.10s, 	Step: 30900, 	{'train/ctc_loss': Array(0.27994934, dtype=float32), 'train/wer': 0.10009418614607415, 'validation/ctc_loss': Array(0.4781998, dtype=float32), 'validation/wer': 0.1433522886355079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27936098, dtype=float32), 'test/wer': 0.09432697580890866, 'test/num_examples': 2472, 'score': 24523.22280049324, 'total_duration': 26880.09740138054, 'accumulated_submission_time': 24523.22280049324, 'accumulated_eval_time': 2354.6739501953125, 'accumulated_logging_time': 0.8952944278717041}
I0217 12:35:17.091655 140532797925120 logging_writer.py:48] [30900] accumulated_eval_time=2354.673950, accumulated_logging_time=0.895294, accumulated_submission_time=24523.222800, global_step=30900, preemption_count=0, score=24523.222800, test/ctc_loss=0.27936097979545593, test/num_examples=2472, test/wer=0.094327, total_duration=26880.097401, train/ctc_loss=0.2799493372440338, train/wer=0.100094, validation/ctc_loss=0.4781998097896576, validation/num_examples=5348, validation/wer=0.143352
I0217 12:35:22.558809 140532797925120 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0561683177947998, loss=1.3233978748321533
I0217 12:36:37.955188 140532789532416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6433433294296265, loss=1.307444453239441
I0217 12:37:53.662200 140532797925120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.7266848683357239, loss=1.3052641153335571
I0217 12:39:09.211406 140532789532416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.6656382083892822, loss=1.329755187034607
I0217 12:40:24.827605 140532797925120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.6646480560302734, loss=1.37795889377594
I0217 12:41:44.141198 140532789532416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.47525331377983093, loss=1.3812168836593628
I0217 12:43:07.965465 140532797925120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.5379467606544495, loss=1.3361212015151978
I0217 12:44:32.438338 140532789532416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.48910343647003174, loss=1.3610306978225708
I0217 12:45:57.263002 140532797925120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.43444785475730896, loss=1.264574408531189
I0217 12:47:20.704230 140532789532416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.5949558615684509, loss=1.3096582889556885
I0217 12:48:44.223000 140532797925120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.5982890129089355, loss=1.369282603263855
I0217 12:50:05.613934 140532797925120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.46292564272880554, loss=1.2343943119049072
I0217 12:51:21.144727 140532789532416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.6274463534355164, loss=1.3845667839050293
I0217 12:52:36.693927 140532797925120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.5206885933876038, loss=1.360858678817749
I0217 12:53:52.635646 140532789532416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.5214515328407288, loss=1.2718991041183472
I0217 12:55:08.268367 140532797925120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.5693879127502441, loss=1.2943308353424072
I0217 12:56:31.347791 140532789532416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5784871578216553, loss=1.2480437755584717
I0217 12:57:55.161072 140532797925120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.644827663898468, loss=1.2968201637268066
I0217 12:59:17.963608 140599226058560 spec.py:321] Evaluating on the training split.
I0217 13:00:11.338259 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 13:01:02.432858 140599226058560 spec.py:349] Evaluating on the test split.
I0217 13:01:28.785717 140599226058560 submission_runner.py:408] Time since start: 28451.83s, 	Step: 32700, 	{'train/ctc_loss': Array(0.31628087, dtype=float32), 'train/wer': 0.114678250442593, 'validation/ctc_loss': Array(0.46717182, dtype=float32), 'validation/wer': 0.1399731600644931, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27516112, dtype=float32), 'test/wer': 0.0914224199215973, 'test/num_examples': 2472, 'score': 25964.00806760788, 'total_duration': 28451.8306787014, 'accumulated_submission_time': 25964.00806760788, 'accumulated_eval_time': 2485.4901201725006, 'accumulated_logging_time': 0.9515819549560547}
I0217 13:01:28.825926 140532797925120 logging_writer.py:48] [32700] accumulated_eval_time=2485.490120, accumulated_logging_time=0.951582, accumulated_submission_time=25964.008068, global_step=32700, preemption_count=0, score=25964.008068, test/ctc_loss=0.27516111731529236, test/num_examples=2472, test/wer=0.091422, total_duration=28451.830679, train/ctc_loss=0.31628087162971497, train/wer=0.114678, validation/ctc_loss=0.4671718180179596, validation/num_examples=5348, validation/wer=0.139973
I0217 13:01:29.694186 140532789532416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.5126681923866272, loss=1.3001195192337036
I0217 13:02:44.898454 140532797925120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.8943302035331726, loss=1.39922034740448
I0217 13:04:00.396602 140532789532416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.5845080018043518, loss=1.3141621351242065
I0217 13:05:19.805768 140532797925120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5060320496559143, loss=1.306673288345337
I0217 13:06:35.414242 140532789532416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.5981825590133667, loss=1.336562156677246
I0217 13:07:50.951034 140532797925120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.5622451305389404, loss=1.2938828468322754
I0217 13:09:06.522336 140532789532416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.48678579926490784, loss=1.2715637683868408
I0217 13:10:22.164864 140532797925120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.48080530762672424, loss=1.2880722284317017
I0217 13:11:45.419563 140532789532416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.4969196617603302, loss=1.294979453086853
I0217 13:13:10.047732 140532797925120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.6272752285003662, loss=1.3091267347335815
I0217 13:14:34.714282 140532789532416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.5431915521621704, loss=1.3324209451675415
I0217 13:15:59.213500 140532797925120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.4428858757019043, loss=1.3105541467666626
I0217 13:17:22.959610 140532789532416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.6126446723937988, loss=1.336358904838562
I0217 13:18:49.338446 140532797925120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6389886736869812, loss=1.2757550477981567
I0217 13:20:04.988729 140532789532416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.46530330181121826, loss=1.247743844985962
I0217 13:21:20.726514 140532797925120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.5014163851737976, loss=1.2689729928970337
I0217 13:22:36.733658 140532789532416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.544254720211029, loss=1.2893342971801758
I0217 13:23:52.483670 140532797925120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.7115737795829773, loss=1.2694134712219238
I0217 13:25:14.996558 140532789532416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7484619617462158, loss=1.33632493019104
I0217 13:25:28.822712 140599226058560 spec.py:321] Evaluating on the training split.
I0217 13:26:22.388037 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 13:27:13.973362 140599226058560 spec.py:349] Evaluating on the test split.
I0217 13:27:40.421630 140599226058560 submission_runner.py:408] Time since start: 30023.47s, 	Step: 34518, 	{'train/ctc_loss': Array(0.29610607, dtype=float32), 'train/wer': 0.10736932352761061, 'validation/ctc_loss': Array(0.47222495, dtype=float32), 'validation/wer': 0.139036658717669, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26905754, dtype=float32), 'test/wer': 0.09026465988259907, 'test/num_examples': 2472, 'score': 27403.91512775421, 'total_duration': 30023.466059207916, 'accumulated_submission_time': 27403.91512775421, 'accumulated_eval_time': 2617.0825748443604, 'accumulated_logging_time': 1.0097923278808594}
I0217 13:27:40.460778 140532797925120 logging_writer.py:48] [34518] accumulated_eval_time=2617.082575, accumulated_logging_time=1.009792, accumulated_submission_time=27403.915128, global_step=34518, preemption_count=0, score=27403.915128, test/ctc_loss=0.2690575420856476, test/num_examples=2472, test/wer=0.090265, total_duration=30023.466059, train/ctc_loss=0.2961060702800751, train/wer=0.107369, validation/ctc_loss=0.4722249507904053, validation/num_examples=5348, validation/wer=0.139037
I0217 13:28:42.970686 140532789532416 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.5551485419273376, loss=1.351833462715149
I0217 13:29:58.974622 140532797925120 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.6642495393753052, loss=1.2967417240142822
I0217 13:31:14.581517 140532789532416 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.3989144563674927, loss=1.2474696636199951
I0217 13:32:30.313428 140532797925120 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.6972586512565613, loss=1.274751901626587
I0217 13:33:53.948932 140532789532416 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7562656998634338, loss=1.2872884273529053
I0217 13:35:14.519197 140532797925120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.6627404093742371, loss=1.3035446405410767
I0217 13:36:30.105607 140532789532416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.669384241104126, loss=1.3190762996673584
I0217 13:37:45.738650 140532797925120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6242826581001282, loss=1.3146569728851318
I0217 13:39:01.405309 140532789532416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.8509411811828613, loss=1.3204492330551147
I0217 13:40:17.516833 140532797925120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6253007054328918, loss=1.2541254758834839
I0217 13:41:40.846159 140532789532416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.7761378288269043, loss=1.296473503112793
I0217 13:43:04.650856 140532797925120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.48655587434768677, loss=1.3412879705429077
I0217 13:44:28.851393 140532789532416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.8678984642028809, loss=1.3121695518493652
I0217 13:45:53.823919 140532797925120 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.7619645595550537, loss=1.2772924900054932
I0217 13:47:18.727679 140532789532416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5467894673347473, loss=1.2791842222213745
I0217 13:48:42.688014 140532797925120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.504713773727417, loss=1.2991867065429688
I0217 13:49:58.215358 140532789532416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.5602455139160156, loss=1.280848741531372
I0217 13:51:13.795113 140532797925120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.5076048374176025, loss=1.2665438652038574
I0217 13:51:40.676897 140599226058560 spec.py:321] Evaluating on the training split.
I0217 13:52:33.932122 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 13:53:25.308295 140599226058560 spec.py:349] Evaluating on the test split.
I0217 13:53:51.489872 140599226058560 submission_runner.py:408] Time since start: 31594.54s, 	Step: 36337, 	{'train/ctc_loss': Array(0.31838846, dtype=float32), 'train/wer': 0.10978509603818028, 'validation/ctc_loss': Array(0.4541082, dtype=float32), 'validation/wer': 0.1351844521467121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26998752, dtype=float32), 'test/wer': 0.08935063879917941, 'test/num_examples': 2472, 'score': 28844.04262661934, 'total_duration': 31594.535401821136, 'accumulated_submission_time': 28844.04262661934, 'accumulated_eval_time': 2747.890196323395, 'accumulated_logging_time': 1.0664589405059814}
I0217 13:53:51.525680 140532797925120 logging_writer.py:48] [36337] accumulated_eval_time=2747.890196, accumulated_logging_time=1.066459, accumulated_submission_time=28844.042627, global_step=36337, preemption_count=0, score=28844.042627, test/ctc_loss=0.2699875235557556, test/num_examples=2472, test/wer=0.089351, total_duration=31594.535402, train/ctc_loss=0.3183884620666504, train/wer=0.109785, validation/ctc_loss=0.45410820841789246, validation/num_examples=5348, validation/wer=0.135184
I0217 13:54:39.756961 140532789532416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.46266695857048035, loss=1.2623112201690674
I0217 13:55:55.381251 140532797925120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.5658997297286987, loss=1.2968751192092896
I0217 13:57:10.931110 140532789532416 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.6962751150131226, loss=1.2877038717269897
I0217 13:58:26.614818 140532797925120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.537133514881134, loss=1.2537567615509033
I0217 13:59:48.302160 140532789532416 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6905447244644165, loss=1.2914083003997803
I0217 14:01:11.038440 140532797925120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.621500551700592, loss=1.3221776485443115
I0217 14:02:35.366405 140532789532416 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.705847442150116, loss=1.2818183898925781
I0217 14:04:01.445277 140532797925120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.5784712433815002, loss=1.2621636390686035
I0217 14:05:16.949550 140532789532416 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.6743866205215454, loss=1.2361903190612793
I0217 14:06:32.501603 140532797925120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.6751423478126526, loss=1.257788896560669
I0217 14:07:48.130670 140532789532416 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.5129045844078064, loss=1.247448444366455
I0217 14:09:03.749587 140532797925120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5895009636878967, loss=1.2235291004180908
I0217 14:10:23.598506 140532789532416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.7694207429885864, loss=1.224841833114624
I0217 14:11:47.581440 140532797925120 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.517970085144043, loss=1.29010009765625
I0217 14:13:12.614561 140532789532416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.8316951990127563, loss=1.240525722503662
I0217 14:14:36.333191 140532797925120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.5350786447525024, loss=1.2676069736480713
I0217 14:16:00.325829 140532789532416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7138499021530151, loss=1.313504934310913
I0217 14:17:23.707102 140532797925120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.6875369548797607, loss=1.3110756874084473
I0217 14:17:51.907049 140599226058560 spec.py:321] Evaluating on the training split.
I0217 14:18:45.168055 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 14:19:36.142150 140599226058560 spec.py:349] Evaluating on the test split.
I0217 14:20:02.641125 140599226058560 submission_runner.py:408] Time since start: 33165.69s, 	Step: 38133, 	{'train/ctc_loss': Array(0.27280307, dtype=float32), 'train/wer': 0.09982278037894368, 'validation/ctc_loss': Array(0.47273842, dtype=float32), 'validation/wer': 0.14050416598279541, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27322602, dtype=float32), 'test/wer': 0.09150366624012349, 'test/num_examples': 2472, 'score': 30284.337661266327, 'total_duration': 33165.68579244614, 'accumulated_submission_time': 30284.337661266327, 'accumulated_eval_time': 2878.6182096004486, 'accumulated_logging_time': 1.1174898147583008}
I0217 14:20:02.685086 140532797925120 logging_writer.py:48] [38133] accumulated_eval_time=2878.618210, accumulated_logging_time=1.117490, accumulated_submission_time=30284.337661, global_step=38133, preemption_count=0, score=30284.337661, test/ctc_loss=0.2732260227203369, test/num_examples=2472, test/wer=0.091504, total_duration=33165.685792, train/ctc_loss=0.27280306816101074, train/wer=0.099823, validation/ctc_loss=0.4727384150028229, validation/num_examples=5348, validation/wer=0.140504
I0217 14:20:53.795452 140532789532416 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.492390900850296, loss=1.2714900970458984
I0217 14:22:09.557805 140532797925120 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5017869472503662, loss=1.2906627655029297
I0217 14:23:25.067908 140532789532416 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.629740834236145, loss=1.25221586227417
I0217 14:24:40.587854 140532797925120 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.5760183334350586, loss=1.3176418542861938
I0217 14:25:56.031937 140532789532416 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.49349871277809143, loss=1.2755452394485474
I0217 14:27:11.659766 140532797925120 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6011187434196472, loss=1.2671021223068237
I0217 14:28:32.738470 140532789532416 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.49673765897750854, loss=1.3048357963562012
I0217 14:29:54.914464 140532797925120 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.8055890798568726, loss=1.2633124589920044
I0217 14:31:18.966286 140532789532416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6077499389648438, loss=1.2909775972366333
I0217 14:32:42.582451 140532797925120 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.0983948707580566, loss=1.2564047574996948
I0217 14:34:04.782915 140532797925120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6475918889045715, loss=1.2394026517868042
I0217 14:35:20.186781 140532789532416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.7017144560813904, loss=1.2030632495880127
I0217 14:36:35.679484 140532797925120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.6598211526870728, loss=1.2786738872528076
I0217 14:37:51.356762 140532789532416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.49459534883499146, loss=1.2250182628631592
I0217 14:39:06.774914 140532797925120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.7037264108657837, loss=1.2887698411941528
I0217 14:40:28.199982 140532789532416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.747054934501648, loss=1.2050061225891113
I0217 14:41:51.924350 140532797925120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5717865824699402, loss=1.2595053911209106
I0217 14:43:15.464390 140532789532416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.7577630281448364, loss=1.2681760787963867
I0217 14:44:03.040241 140599226058560 spec.py:321] Evaluating on the training split.
I0217 14:44:56.143725 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 14:45:47.116744 140599226058560 spec.py:349] Evaluating on the test split.
I0217 14:46:13.159078 140599226058560 submission_runner.py:408] Time since start: 34736.20s, 	Step: 39958, 	{'train/ctc_loss': Array(0.23454855, dtype=float32), 'train/wer': 0.08544679623072911, 'validation/ctc_loss': Array(0.43293995, dtype=float32), 'validation/wer': 0.1286289427189434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2446584, dtype=float32), 'test/wer': 0.08248532488371621, 'test/num_examples': 2472, 'score': 31724.60327744484, 'total_duration': 34736.20393896103, 'accumulated_submission_time': 31724.60327744484, 'accumulated_eval_time': 3008.73100566864, 'accumulated_logging_time': 1.1783719062805176}
I0217 14:46:13.198223 140532797925120 logging_writer.py:48] [39958] accumulated_eval_time=3008.731006, accumulated_logging_time=1.178372, accumulated_submission_time=31724.603277, global_step=39958, preemption_count=0, score=31724.603277, test/ctc_loss=0.24465839564800262, test/num_examples=2472, test/wer=0.082485, total_duration=34736.203939, train/ctc_loss=0.23454855382442474, train/wer=0.085447, validation/ctc_loss=0.43293994665145874, validation/num_examples=5348, validation/wer=0.128629
I0217 14:46:45.680593 140532789532416 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.5833679437637329, loss=1.2665164470672607
I0217 14:48:01.232448 140532797925120 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.6085652112960815, loss=1.2523516416549683
I0217 14:49:20.443519 140532797925120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.5918862819671631, loss=1.2975549697875977
I0217 14:50:35.817401 140532789532416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6834633946418762, loss=1.1943631172180176
I0217 14:51:51.322715 140532797925120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.5986726880073547, loss=1.2473698854446411
I0217 14:53:06.764187 140532789532416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.605249285697937, loss=1.2674249410629272
I0217 14:54:22.313695 140532797925120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.5090441107749939, loss=1.222527027130127
I0217 14:55:41.217741 140532789532416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6818888187408447, loss=1.2513511180877686
I0217 14:57:06.307374 140532797925120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.6218979358673096, loss=1.176409125328064
I0217 14:58:30.280277 140532789532416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.691358208656311, loss=1.2806775569915771
I0217 14:59:54.419152 140532797925120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.8111242055892944, loss=1.1970709562301636
I0217 15:01:18.266664 140532789532416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5829287171363831, loss=1.2405391931533813
I0217 15:02:45.178241 140532797925120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.6769561171531677, loss=1.1970981359481812
I0217 15:04:00.655976 140532789532416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.5415766835212708, loss=1.2062077522277832
I0217 15:05:16.285024 140532797925120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.7336511611938477, loss=1.2020996809005737
I0217 15:06:31.847067 140532789532416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.7506018877029419, loss=1.2123419046401978
I0217 15:07:47.499913 140532797925120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.4871419370174408, loss=1.2188911437988281
I0217 15:09:03.208102 140532789532416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.5161720514297485, loss=1.2084726095199585
I0217 15:10:13.800975 140599226058560 spec.py:321] Evaluating on the training split.
I0217 15:11:06.007484 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 15:11:57.562111 140599226058560 spec.py:349] Evaluating on the test split.
I0217 15:12:23.783246 140599226058560 submission_runner.py:408] Time since start: 36306.83s, 	Step: 41786, 	{'train/ctc_loss': Array(0.25626796, dtype=float32), 'train/wer': 0.09345090317609794, 'validation/ctc_loss': Array(0.42910972, dtype=float32), 'validation/wer': 0.1268042132905954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24258377, dtype=float32), 'test/wer': 0.0816728616984543, 'test/num_examples': 2472, 'score': 33165.11921596527, 'total_duration': 36306.827905893326, 'accumulated_submission_time': 33165.11921596527, 'accumulated_eval_time': 3138.707051753998, 'accumulated_logging_time': 1.2340517044067383}
I0217 15:12:23.831912 140532797925120 logging_writer.py:48] [41786] accumulated_eval_time=3138.707052, accumulated_logging_time=1.234052, accumulated_submission_time=33165.119216, global_step=41786, preemption_count=0, score=33165.119216, test/ctc_loss=0.242583766579628, test/num_examples=2472, test/wer=0.081673, total_duration=36306.827906, train/ctc_loss=0.2562679648399353, train/wer=0.093451, validation/ctc_loss=0.42910972237586975, validation/num_examples=5348, validation/wer=0.126804
I0217 15:12:35.232752 140532789532416 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.9374319314956665, loss=1.2112071514129639
I0217 15:13:51.103006 140532797925120 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.6528753638267517, loss=1.24495530128479
I0217 15:15:06.630372 140532789532416 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5275900959968567, loss=1.227360486984253
I0217 15:16:22.247089 140532797925120 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.8668718338012695, loss=1.263113021850586
I0217 15:17:41.685304 140532789532416 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4713110625743866, loss=1.227039098739624
I0217 15:19:04.135034 140532797925120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.5413737893104553, loss=1.1934707164764404
I0217 15:20:19.972172 140532789532416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.5811048150062561, loss=1.2305396795272827
I0217 15:21:35.819935 140532797925120 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0399997234344482, loss=1.1698007583618164
I0217 15:22:51.559787 140532789532416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.5551011562347412, loss=1.2046120166778564
I0217 15:24:07.387566 140532797925120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.6435042023658752, loss=1.2264708280563354
I0217 15:25:23.095446 140532789532416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.559320867061615, loss=1.2093284130096436
I0217 15:26:45.255429 140532797925120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.5270611643791199, loss=1.186976671218872
I0217 15:28:08.037725 140532789532416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6360177397727966, loss=1.2677119970321655
I0217 15:29:32.302566 140532797925120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6518623232841492, loss=1.265429973602295
I0217 15:30:55.642003 140532789532416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.6398103833198547, loss=1.2431095838546753
I0217 15:32:20.000453 140532797925120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7118821144104004, loss=1.1540329456329346
I0217 15:33:35.404510 140532789532416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.525642454624176, loss=1.1711512804031372
I0217 15:34:50.793883 140532797925120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.6097262501716614, loss=1.1892013549804688
I0217 15:36:06.263475 140532789532416 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.4255298376083374, loss=1.178357481956482
I0217 15:36:24.079987 140599226058560 spec.py:321] Evaluating on the training split.
I0217 15:37:27.273458 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 15:38:19.047137 140599226058560 spec.py:349] Evaluating on the test split.
I0217 15:38:45.778357 140599226058560 submission_runner.py:408] Time since start: 37888.82s, 	Step: 43625, 	{'train/ctc_loss': Array(0.17323135, dtype=float32), 'train/wer': 0.06469077493987442, 'validation/ctc_loss': Array(0.4196442, dtype=float32), 'validation/wer': 0.12383058014810237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23896857, dtype=float32), 'test/wer': 0.0785448784351959, 'test/num_examples': 2472, 'score': 34605.279014348984, 'total_duration': 37888.82444357872, 'accumulated_submission_time': 34605.279014348984, 'accumulated_eval_time': 3280.400631427765, 'accumulated_logging_time': 1.2985985279083252}
I0217 15:38:45.818158 140532797925120 logging_writer.py:48] [43625] accumulated_eval_time=3280.400631, accumulated_logging_time=1.298599, accumulated_submission_time=34605.279014, global_step=43625, preemption_count=0, score=34605.279014, test/ctc_loss=0.23896856606006622, test/num_examples=2472, test/wer=0.078545, total_duration=37888.824444, train/ctc_loss=0.1732313483953476, train/wer=0.064691, validation/ctc_loss=0.41964420676231384, validation/num_examples=5348, validation/wer=0.123831
I0217 15:39:42.927167 140532789532416 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6551512479782104, loss=1.190147042274475
I0217 15:40:58.314452 140532797925120 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.6962888836860657, loss=1.1541390419006348
I0217 15:42:13.785223 140532789532416 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.61105877161026, loss=1.198620319366455
I0217 15:43:29.247485 140532797925120 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.07901930809021, loss=1.2580397129058838
I0217 15:44:47.235927 140532789532416 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.6363279223442078, loss=1.2183890342712402
I0217 15:46:10.360606 140532797925120 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.579149067401886, loss=1.2130768299102783
I0217 15:47:37.191800 140532797925120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.7068135142326355, loss=1.193374752998352
I0217 15:48:52.654880 140532789532416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.7118467092514038, loss=1.140730857849121
I0217 15:50:08.229418 140532797925120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.5539702773094177, loss=1.1391632556915283
I0217 15:51:23.774628 140532789532416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.4941348135471344, loss=1.1465586423873901
I0217 15:52:39.431697 140532797925120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6084893345832825, loss=1.2009528875350952
I0217 15:53:55.010018 140532789532416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.573680579662323, loss=1.2224963903427124
I0217 15:55:18.271103 140532797925120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.8197049498558044, loss=1.250028371810913
I0217 15:56:42.580743 140532789532416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.510042667388916, loss=1.208834171295166
I0217 15:58:06.765690 140532797925120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.6045854687690735, loss=1.187209129333496
I0217 15:59:29.714713 140532789532416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.5165591239929199, loss=1.2524300813674927
I0217 16:00:53.639277 140532797925120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.7111892104148865, loss=1.2219088077545166
I0217 16:02:14.375436 140532797925120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.5233500599861145, loss=1.2007734775543213
I0217 16:02:46.453744 140599226058560 spec.py:321] Evaluating on the training split.
I0217 16:03:41.585155 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 16:04:33.803217 140599226058560 spec.py:349] Evaluating on the test split.
I0217 16:05:00.390559 140599226058560 submission_runner.py:408] Time since start: 39463.43s, 	Step: 45444, 	{'train/ctc_loss': Array(0.1462238, dtype=float32), 'train/wer': 0.05505919084168076, 'validation/ctc_loss': Array(0.4182151, dtype=float32), 'validation/wer': 0.12126244243413113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23613536, dtype=float32), 'test/wer': 0.0795198342575102, 'test/num_examples': 2472, 'score': 36045.82629442215, 'total_duration': 39463.43499088287, 'accumulated_submission_time': 36045.82629442215, 'accumulated_eval_time': 3414.3309786319733, 'accumulated_logging_time': 1.3558061122894287}
I0217 16:05:00.429627 140532797925120 logging_writer.py:48] [45444] accumulated_eval_time=3414.330979, accumulated_logging_time=1.355806, accumulated_submission_time=36045.826294, global_step=45444, preemption_count=0, score=36045.826294, test/ctc_loss=0.2361353635787964, test/num_examples=2472, test/wer=0.079520, total_duration=39463.434991, train/ctc_loss=0.14622379839420319, train/wer=0.055059, validation/ctc_loss=0.4182150959968567, validation/num_examples=5348, validation/wer=0.121262
I0217 16:05:43.578640 140532789532416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.6201522350311279, loss=1.183103084564209
I0217 16:06:59.010493 140532797925120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.5205182433128357, loss=1.1440880298614502
I0217 16:08:14.415086 140532789532416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.5600447058677673, loss=1.1474409103393555
I0217 16:09:29.817593 140532797925120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.6665585041046143, loss=1.1865876913070679
I0217 16:10:45.287968 140532789532416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.7064144611358643, loss=1.206442952156067
I0217 16:12:03.283756 140532797925120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.5371288657188416, loss=1.1963881254196167
I0217 16:13:26.721315 140532789532416 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.599596381187439, loss=1.2570750713348389
I0217 16:14:50.411893 140532797925120 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.7826083302497864, loss=1.221575140953064
I0217 16:16:14.412684 140532789532416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.9847730398178101, loss=1.1684304475784302
I0217 16:17:37.718220 140532797925120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.7937487959861755, loss=1.177711844444275
I0217 16:18:53.061893 140532789532416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6647809743881226, loss=1.199064016342163
I0217 16:20:08.497702 140532797925120 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.2657420635223389, loss=1.2174791097640991
I0217 16:21:24.225083 140532789532416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.5687872767448425, loss=1.1931451559066772
I0217 16:22:39.638029 140532797925120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.51931232213974, loss=1.1736327409744263
I0217 16:24:01.517451 140532789532416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.7683325409889221, loss=1.1426717042922974
I0217 16:25:24.170143 140532797925120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.5793338418006897, loss=1.1677885055541992
I0217 16:26:46.930847 140532789532416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.6623090505599976, loss=1.126039743423462
I0217 16:28:10.107195 140532797925120 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.0354167222976685, loss=1.2300097942352295
I0217 16:29:00.529735 140599226058560 spec.py:321] Evaluating on the training split.
I0217 16:29:56.909500 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 16:30:49.179440 140599226058560 spec.py:349] Evaluating on the test split.
I0217 16:31:15.340880 140599226058560 submission_runner.py:408] Time since start: 41038.39s, 	Step: 47262, 	{'train/ctc_loss': Array(0.15300256, dtype=float32), 'train/wer': 0.05830980808577867, 'validation/ctc_loss': Array(0.41734868, dtype=float32), 'validation/wer': 0.12318371839307954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22954966, dtype=float32), 'test/wer': 0.07570125728677919, 'test/num_examples': 2472, 'score': 37485.838297605515, 'total_duration': 41038.38579106331, 'accumulated_submission_time': 37485.838297605515, 'accumulated_eval_time': 3549.1361298561096, 'accumulated_logging_time': 1.411712408065796}
I0217 16:31:15.380733 140532797925120 logging_writer.py:48] [47262] accumulated_eval_time=3549.136130, accumulated_logging_time=1.411712, accumulated_submission_time=37485.838298, global_step=47262, preemption_count=0, score=37485.838298, test/ctc_loss=0.22954966127872467, test/num_examples=2472, test/wer=0.075701, total_duration=41038.385791, train/ctc_loss=0.1530025601387024, train/wer=0.058310, validation/ctc_loss=0.4173486828804016, validation/num_examples=5348, validation/wer=0.123184
I0217 16:31:44.744772 140532789532416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.6930535435676575, loss=1.2336770296096802
I0217 16:33:04.303422 140532797925120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6941009759902954, loss=1.1684823036193848
I0217 16:34:19.614574 140532789532416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.8448968529701233, loss=1.1635255813598633
I0217 16:35:35.256710 140532797925120 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.0010254383087158, loss=1.1908726692199707
I0217 16:36:50.908492 140532789532416 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2164559364318848, loss=1.1328548192977905
I0217 16:38:06.510902 140532797925120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.5001441836357117, loss=1.1617839336395264
I0217 16:39:25.073815 140532789532416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.5725244283676147, loss=1.1949729919433594
I0217 16:40:47.958476 140532797925120 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6276117563247681, loss=1.19752037525177
I0217 16:42:11.211089 140532789532416 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.7153764367103577, loss=1.1930731534957886
I0217 16:43:35.170275 140532797925120 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.0163657665252686, loss=1.1647791862487793
I0217 16:44:57.886889 140532789532416 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.8921969532966614, loss=1.1567814350128174
I0217 16:46:21.261497 140532797925120 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.5222494006156921, loss=1.167044758796692
I0217 16:47:41.379980 140532797925120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.609632134437561, loss=1.1453182697296143
I0217 16:48:56.897069 140532789532416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.6592352986335754, loss=1.134567379951477
I0217 16:50:12.345841 140532797925120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.7734271287918091, loss=1.1673016548156738
I0217 16:51:27.880023 140532789532416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.7335253357887268, loss=1.1907079219818115
I0217 16:52:43.773317 140532797925120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.672189474105835, loss=1.2000339031219482
I0217 16:54:07.627294 140532789532416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.6079245209693909, loss=1.1521215438842773
I0217 16:55:16.485634 140599226058560 spec.py:321] Evaluating on the training split.
I0217 16:56:12.073173 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 16:57:04.270735 140599226058560 spec.py:349] Evaluating on the test split.
I0217 16:57:31.169940 140599226058560 submission_runner.py:408] Time since start: 42614.21s, 	Step: 49083, 	{'train/ctc_loss': Array(0.14905518, dtype=float32), 'train/wer': 0.056064947965695326, 'validation/ctc_loss': Array(0.39878234, dtype=float32), 'validation/wer': 0.11683095667957172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22368722, dtype=float32), 'test/wer': 0.07273576666057319, 'test/num_examples': 2472, 'score': 38926.85561108589, 'total_duration': 42614.214336395264, 'accumulated_submission_time': 38926.85561108589, 'accumulated_eval_time': 3683.8139424324036, 'accumulated_logging_time': 1.4672369956970215}
I0217 16:57:31.214105 140532797925120 logging_writer.py:48] [49083] accumulated_eval_time=3683.813942, accumulated_logging_time=1.467237, accumulated_submission_time=38926.855611, global_step=49083, preemption_count=0, score=38926.855611, test/ctc_loss=0.22368721663951874, test/num_examples=2472, test/wer=0.072736, total_duration=42614.214336, train/ctc_loss=0.14905518293380737, train/wer=0.056065, validation/ctc_loss=0.398782342672348, validation/num_examples=5348, validation/wer=0.116831
I0217 16:57:44.837644 140532789532416 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.5178178548812866, loss=1.1262754201889038
I0217 16:59:00.306239 140532797925120 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.5257577300071716, loss=1.1051691770553589
I0217 17:00:15.895434 140532789532416 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.5174817442893982, loss=1.1286498308181763
I0217 17:01:31.542182 140532797925120 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.5165032744407654, loss=1.1647642850875854
I0217 17:02:51.154852 140532797925120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.5789292454719543, loss=1.157651662826538
I0217 17:04:06.651941 140532789532416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6079224944114685, loss=1.1442978382110596
I0217 17:05:22.163740 140532797925120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.869188666343689, loss=1.1409274339675903
I0217 17:06:37.746167 140532789532416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.5973650813102722, loss=1.0924971103668213
I0217 17:07:53.411444 140532797925120 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.6340583562850952, loss=1.1188631057739258
I0217 17:09:16.049318 140532789532416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.6769099831581116, loss=1.1483360528945923
I0217 17:10:39.504577 140532797925120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.7731223702430725, loss=1.1406036615371704
I0217 17:12:03.179431 140532789532416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.6780187487602234, loss=1.1580690145492554
I0217 17:13:26.550678 140532797925120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.5559262633323669, loss=1.1428524255752563
I0217 17:14:50.320690 140532789532416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.7912814021110535, loss=1.1853371858596802
I0217 17:16:15.250377 140532797925120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.761660099029541, loss=1.1189522743225098
I0217 17:17:30.582005 140532789532416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6275432705879211, loss=1.187596082687378
I0217 17:18:46.044779 140532797925120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.5495095252990723, loss=1.1527493000030518
I0217 17:20:01.470532 140532789532416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.6663472652435303, loss=1.1466491222381592
I0217 17:21:16.849713 140532797925120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.5489535927772522, loss=1.094765305519104
I0217 17:21:31.656246 140599226058560 spec.py:321] Evaluating on the training split.
I0217 17:22:25.667283 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 17:23:17.311838 140599226058560 spec.py:349] Evaluating on the test split.
I0217 17:23:43.764553 140599226058560 submission_runner.py:408] Time since start: 44186.81s, 	Step: 50921, 	{'train/ctc_loss': Array(0.13042888, dtype=float32), 'train/wer': 0.05082385324107196, 'validation/ctc_loss': Array(0.38371804, dtype=float32), 'validation/wer': 0.11144366027206812, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21463418, dtype=float32), 'test/wer': 0.07143582556415412, 'test/num_examples': 2472, 'score': 40367.21039605141, 'total_duration': 44186.81035447121, 'accumulated_submission_time': 40367.21039605141, 'accumulated_eval_time': 3815.9171483516693, 'accumulated_logging_time': 1.5273003578186035}
I0217 17:23:43.803663 140532797925120 logging_writer.py:48] [50921] accumulated_eval_time=3815.917148, accumulated_logging_time=1.527300, accumulated_submission_time=40367.210396, global_step=50921, preemption_count=0, score=40367.210396, test/ctc_loss=0.21463418006896973, test/num_examples=2472, test/wer=0.071436, total_duration=44186.810354, train/ctc_loss=0.13042888045310974, train/wer=0.050824, validation/ctc_loss=0.3837180435657501, validation/num_examples=5348, validation/wer=0.111444
I0217 17:24:44.156738 140532789532416 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.8109633922576904, loss=1.124324917793274
I0217 17:25:59.903509 140532797925120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.967596709728241, loss=1.186872124671936
I0217 17:27:15.621123 140532789532416 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.5933054685592651, loss=1.1376230716705322
I0217 17:28:32.750336 140532797925120 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.03865385055542, loss=1.1615078449249268
I0217 17:29:56.116189 140532789532416 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6168063879013062, loss=1.1023838520050049
I0217 17:31:23.892453 140532797925120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.6949640512466431, loss=1.1914910078048706
I0217 17:32:39.376852 140532789532416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.5902677178382874, loss=1.1400566101074219
I0217 17:33:54.895189 140532797925120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.581935703754425, loss=1.1361982822418213
I0217 17:35:10.550151 140532789532416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.5863333940505981, loss=1.0993472337722778
I0217 17:36:26.053492 140532797925120 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.0054105520248413, loss=1.1726350784301758
I0217 17:37:43.601907 140532789532416 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.529956042766571, loss=1.190703272819519
I0217 17:39:07.590612 140532797925120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.8254182934761047, loss=1.1483604907989502
I0217 17:40:32.008507 140532789532416 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.5440349578857422, loss=1.1630475521087646
I0217 17:41:55.681604 140532797925120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.7607686519622803, loss=1.1493449211120605
I0217 17:43:18.600693 140532789532416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.8177406191825867, loss=1.1462504863739014
I0217 17:44:41.841412 140532797925120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7678350210189819, loss=1.1238019466400146
I0217 17:46:03.125018 140532797925120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.5444304347038269, loss=1.132339358329773
I0217 17:47:18.792767 140532789532416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.7877120971679688, loss=1.0715056657791138
I0217 17:47:44.129586 140599226058560 spec.py:321] Evaluating on the training split.
I0217 17:48:39.117886 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 17:49:30.372120 140599226058560 spec.py:349] Evaluating on the test split.
I0217 17:49:57.038162 140599226058560 submission_runner.py:408] Time since start: 45760.08s, 	Step: 52735, 	{'train/ctc_loss': Array(0.1234728, dtype=float32), 'train/wer': 0.04845024217383904, 'validation/ctc_loss': Array(0.38684124, dtype=float32), 'validation/wer': 0.11160778937408884, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21119802, dtype=float32), 'test/wer': 0.07029837710478744, 'test/num_examples': 2472, 'score': 41807.4494600296, 'total_duration': 45760.08290052414, 'accumulated_submission_time': 41807.4494600296, 'accumulated_eval_time': 3948.8195514678955, 'accumulated_logging_time': 1.5827083587646484}
I0217 17:49:57.077068 140532797925120 logging_writer.py:48] [52735] accumulated_eval_time=3948.819551, accumulated_logging_time=1.582708, accumulated_submission_time=41807.449460, global_step=52735, preemption_count=0, score=41807.449460, test/ctc_loss=0.21119801700115204, test/num_examples=2472, test/wer=0.070298, total_duration=45760.082901, train/ctc_loss=0.12347280234098434, train/wer=0.048450, validation/ctc_loss=0.3868412375450134, validation/num_examples=5348, validation/wer=0.111608
I0217 17:50:46.593816 140532789532416 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.162700891494751, loss=1.1689492464065552
I0217 17:52:01.972623 140532797925120 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.8941316604614258, loss=1.1523375511169434
I0217 17:53:17.557399 140532789532416 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.6023535132408142, loss=1.155285358428955
I0217 17:54:33.008858 140532797925120 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.8791643977165222, loss=1.1780483722686768
I0217 17:55:50.726650 140532789532416 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.6001442670822144, loss=1.1835784912109375
I0217 17:57:14.997648 140532797925120 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.8441479802131653, loss=1.1617295742034912
I0217 17:58:38.445299 140532789532416 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.884946882724762, loss=1.0937167406082153
I0217 18:00:02.132197 140532797925120 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.5336052179336548, loss=1.133001685142517
I0217 18:01:26.271544 140532797925120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.5601487755775452, loss=1.111391544342041
I0217 18:02:41.725507 140532789532416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.6626057624816895, loss=1.092328667640686
I0217 18:03:57.188454 140532797925120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.7975951433181763, loss=1.1104906797409058
I0217 18:05:12.928717 140532789532416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.5436174273490906, loss=1.0875403881072998
I0217 18:06:28.456403 140532797925120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.8304819464683533, loss=1.0820765495300293
I0217 18:07:50.560104 140532789532416 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.6040457487106323, loss=1.1198526620864868
I0217 18:09:13.778212 140532797925120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.7445922493934631, loss=1.103638768196106
I0217 18:10:37.983613 140532789532416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.581447184085846, loss=1.0988879203796387
I0217 18:12:00.800982 140532797925120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.7492327094078064, loss=1.1086081266403198
I0217 18:13:24.413978 140532789532416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.6203480362892151, loss=1.079838752746582
I0217 18:13:57.327531 140599226058560 spec.py:321] Evaluating on the training split.
I0217 18:14:51.651742 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 18:15:43.340040 140599226058560 spec.py:349] Evaluating on the test split.
I0217 18:16:09.490652 140599226058560 submission_runner.py:408] Time since start: 47332.54s, 	Step: 54541, 	{'train/ctc_loss': Array(0.13599047, dtype=float32), 'train/wer': 0.05029156920238527, 'validation/ctc_loss': Array(0.36117485, dtype=float32), 'validation/wer': 0.10539019280342161, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20237793, dtype=float32), 'test/wer': 0.06666260435074035, 'test/num_examples': 2472, 'score': 43247.61201763153, 'total_duration': 47332.53596878052, 'accumulated_submission_time': 43247.61201763153, 'accumulated_eval_time': 4080.977082490921, 'accumulated_logging_time': 1.6382241249084473}
I0217 18:16:09.532004 140532797925120 logging_writer.py:48] [54541] accumulated_eval_time=4080.977082, accumulated_logging_time=1.638224, accumulated_submission_time=43247.612018, global_step=54541, preemption_count=0, score=43247.612018, test/ctc_loss=0.20237793028354645, test/num_examples=2472, test/wer=0.066663, total_duration=47332.535969, train/ctc_loss=0.1359904706478119, train/wer=0.050292, validation/ctc_loss=0.3611748516559601, validation/num_examples=5348, validation/wer=0.105390
I0217 18:16:58.722354 140532797925120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.6591172814369202, loss=1.0850199460983276
I0217 18:18:14.010649 140532789532416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.6980214715003967, loss=1.0963003635406494
I0217 18:19:29.475885 140532797925120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.5973331928253174, loss=1.0684102773666382
I0217 18:20:45.028138 140532789532416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6628033518791199, loss=1.1127663850784302
I0217 18:22:00.487876 140532797925120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.5848706960678101, loss=1.127730131149292
I0217 18:23:21.621789 140532789532416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.6882511377334595, loss=1.0703871250152588
I0217 18:24:45.577083 140532797925120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.5454152226448059, loss=1.1105694770812988
I0217 18:26:09.697441 140532789532416 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.045289158821106, loss=1.1157437562942505
I0217 18:27:33.910481 140532797925120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.7555475234985352, loss=1.2081819772720337
I0217 18:28:57.076917 140532789532416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.7158094048500061, loss=1.110209584236145
I0217 18:30:20.056547 140532797925120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.6712936162948608, loss=1.1135584115982056
I0217 18:31:40.455759 140532797925120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7135366201400757, loss=1.060418963432312
I0217 18:32:56.144375 140532789532416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.6196655035018921, loss=1.093506097793579
I0217 18:34:11.674481 140532797925120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.5754197239875793, loss=1.087601661682129
I0217 18:35:27.231978 140532789532416 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.6197679042816162, loss=1.1099103689193726
I0217 18:36:43.261277 140532797925120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.6871907711029053, loss=1.0686372518539429
I0217 18:38:06.578630 140532789532416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9217256903648376, loss=1.09201180934906
I0217 18:39:29.995594 140532797925120 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.8019779920578003, loss=1.1364679336547852
I0217 18:40:09.950096 140599226058560 spec.py:321] Evaluating on the training split.
I0217 18:41:05.515783 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 18:41:56.930813 140599226058560 spec.py:349] Evaluating on the test split.
I0217 18:42:23.506985 140599226058560 submission_runner.py:408] Time since start: 48906.55s, 	Step: 56349, 	{'train/ctc_loss': Array(0.12227172, dtype=float32), 'train/wer': 0.04386070108296096, 'validation/ctc_loss': Array(0.3628258, dtype=float32), 'validation/wer': 0.10530330092588124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1969371, dtype=float32), 'test/wer': 0.06475331586537485, 'test/num_examples': 2472, 'score': 44687.94367861748, 'total_duration': 48906.5517706871, 'accumulated_submission_time': 44687.94367861748, 'accumulated_eval_time': 4214.5278577804565, 'accumulated_logging_time': 1.6951165199279785}
I0217 18:42:23.548156 140532797925120 logging_writer.py:48] [56349] accumulated_eval_time=4214.527858, accumulated_logging_time=1.695117, accumulated_submission_time=44687.943679, global_step=56349, preemption_count=0, score=44687.943679, test/ctc_loss=0.19693709909915924, test/num_examples=2472, test/wer=0.064753, total_duration=48906.551771, train/ctc_loss=0.12227171659469604, train/wer=0.043861, validation/ctc_loss=0.36282581090927124, validation/num_examples=5348, validation/wer=0.105303
I0217 18:43:02.786394 140532789532416 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.7368044257164001, loss=1.105009913444519
I0217 18:44:18.253114 140532797925120 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.5662207007408142, loss=1.1097413301467896
I0217 18:45:33.809093 140532789532416 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.830129861831665, loss=1.069759488105774
I0217 18:46:53.323210 140532797925120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.5935401320457458, loss=1.104514718055725
I0217 18:48:08.594255 140532789532416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.7011399865150452, loss=1.0170615911483765
I0217 18:49:24.083544 140532797925120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.8418261408805847, loss=1.1045364141464233
I0217 18:50:39.666838 140532789532416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.8677862286567688, loss=1.0616687536239624
I0217 18:51:56.725751 140532797925120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.6909481883049011, loss=1.128838300704956
I0217 18:53:20.334220 140532789532416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.782820999622345, loss=1.0774027109146118
I0217 18:54:43.326744 140532797925120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.5647777915000916, loss=1.0781214237213135
I0217 18:56:06.749238 140532789532416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.7016366720199585, loss=1.1126561164855957
I0217 18:57:31.916986 140532797925120 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.6518919467926025, loss=1.1014853715896606
I0217 18:58:55.147698 140532789532416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.756602942943573, loss=1.0505321025848389
I0217 19:00:20.809951 140532797925120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.612491250038147, loss=1.0245695114135742
I0217 19:01:36.142663 140532789532416 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.0042340755462646, loss=1.0152450799942017
I0217 19:02:51.750064 140532797925120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.7213875651359558, loss=1.0881906747817993
I0217 19:04:07.297458 140532789532416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.6729968190193176, loss=1.084126591682434
I0217 19:05:22.798719 140532797925120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.691369354724884, loss=1.0366498231887817
I0217 19:06:24.151606 140599226058560 spec.py:321] Evaluating on the training split.
I0217 19:07:18.800094 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 19:08:10.575729 140599226058560 spec.py:349] Evaluating on the test split.
I0217 19:08:37.208982 140599226058560 submission_runner.py:408] Time since start: 50480.25s, 	Step: 58181, 	{'train/ctc_loss': Array(0.11364357, dtype=float32), 'train/wer': 0.04371404219049648, 'validation/ctc_loss': Array(0.35202444, dtype=float32), 'validation/wer': 0.10140282108962415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19438767, dtype=float32), 'test/wer': 0.06304714317632483, 'test/num_examples': 2472, 'score': 46128.458505392075, 'total_duration': 50480.25352835655, 'accumulated_submission_time': 46128.458505392075, 'accumulated_eval_time': 4347.578888177872, 'accumulated_logging_time': 1.7528400421142578}
I0217 19:08:37.248107 140532797925120 logging_writer.py:48] [58181] accumulated_eval_time=4347.578888, accumulated_logging_time=1.752840, accumulated_submission_time=46128.458505, global_step=58181, preemption_count=0, score=46128.458505, test/ctc_loss=0.19438767433166504, test/num_examples=2472, test/wer=0.063047, total_duration=50480.253528, train/ctc_loss=0.1136435717344284, train/wer=0.043714, validation/ctc_loss=0.3520244359970093, validation/num_examples=5348, validation/wer=0.101403
I0217 19:08:52.344174 140532789532416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.6264593005180359, loss=1.085283875465393
I0217 19:10:07.607151 140532797925120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.8512450456619263, loss=1.09636652469635
I0217 19:11:23.166286 140532789532416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.8249721527099609, loss=1.0657013654708862
I0217 19:12:38.644008 140532797925120 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.6361246109008789, loss=1.0416179895401
I0217 19:13:55.936231 140532789532416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.6615703701972961, loss=1.0527126789093018
I0217 19:15:18.860644 140532797925120 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.7851451635360718, loss=1.0882048606872559
I0217 19:16:38.867868 140532797925120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7308015823364258, loss=1.0375447273254395
I0217 19:17:54.415845 140532789532416 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.6143673658370972, loss=1.0606181621551514
I0217 19:19:09.957615 140532797925120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.7034488320350647, loss=1.0548832416534424
I0217 19:20:25.522360 140532789532416 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.3002614974975586, loss=1.0692551136016846
I0217 19:21:41.004030 140532797925120 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.7575146555900574, loss=1.0730407238006592
I0217 19:23:03.596862 140532789532416 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.5557560324668884, loss=1.0768930912017822
I0217 19:24:26.883192 140532797925120 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.9036878347396851, loss=1.077805757522583
I0217 19:25:50.036507 140532789532416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.8183459043502808, loss=1.0345500707626343
I0217 19:27:13.684646 140532797925120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.7429505586624146, loss=1.048149824142456
I0217 19:28:38.173231 140532789532416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.7101540565490723, loss=1.0788815021514893
I0217 19:30:00.770593 140532797925120 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.5594468116760254, loss=1.042866587638855
I0217 19:31:16.628212 140532789532416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.5589547157287598, loss=1.0254985094070435
I0217 19:32:32.169612 140532797925120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.6288706660270691, loss=1.0268858671188354
I0217 19:32:37.924056 140599226058560 spec.py:321] Evaluating on the training split.
I0217 19:33:32.057715 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 19:34:23.267212 140599226058560 spec.py:349] Evaluating on the test split.
I0217 19:34:49.377763 140599226058560 submission_runner.py:408] Time since start: 52052.42s, 	Step: 60009, 	{'train/ctc_loss': Array(0.09262989, dtype=float32), 'train/wer': 0.035873062918113305, 'validation/ctc_loss': Array(0.34524652, dtype=float32), 'validation/wer': 0.09952981839597594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19161133, dtype=float32), 'test/wer': 0.062234679991062906, 'test/num_examples': 2472, 'score': 47569.04715514183, 'total_duration': 52052.42256188393, 'accumulated_submission_time': 47569.04715514183, 'accumulated_eval_time': 4479.026484251022, 'accumulated_logging_time': 1.8070552349090576}
I0217 19:34:49.421182 140532797925120 logging_writer.py:48] [60009] accumulated_eval_time=4479.026484, accumulated_logging_time=1.807055, accumulated_submission_time=47569.047155, global_step=60009, preemption_count=0, score=47569.047155, test/ctc_loss=0.19161133468151093, test/num_examples=2472, test/wer=0.062235, total_duration=52052.422562, train/ctc_loss=0.09262988716363907, train/wer=0.035873, validation/ctc_loss=0.3452465236186981, validation/num_examples=5348, validation/wer=0.099530
I0217 19:35:58.621969 140532789532416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.6895053386688232, loss=1.0619982481002808
I0217 19:37:13.998860 140532797925120 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.826360821723938, loss=1.0688402652740479
I0217 19:38:29.371424 140532789532416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.9270628690719604, loss=1.0108431577682495
I0217 19:39:44.644423 140532797925120 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.6357784271240234, loss=1.0295987129211426
I0217 19:41:08.490963 140532789532416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.5274041295051575, loss=1.074781894683838
I0217 19:42:31.493331 140532797925120 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.6241053342819214, loss=1.034315586090088
I0217 19:43:54.687937 140532789532416 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.6179927587509155, loss=1.0398781299591064
I0217 19:45:19.976002 140532797925120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.6237525343894958, loss=1.0645183324813843
I0217 19:46:35.292514 140532789532416 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0104466676712036, loss=0.9849939942359924
I0217 19:47:50.706547 140532797925120 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.5810189247131348, loss=1.0273336172103882
I0217 19:49:06.430727 140532789532416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.7481978535652161, loss=1.0191020965576172
I0217 19:50:21.795809 140532797925120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.701347291469574, loss=0.9875921010971069
I0217 19:51:40.725617 140532789532416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.7017706632614136, loss=1.052980899810791
I0217 19:53:05.035913 140532797925120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.745585024356842, loss=1.0189926624298096
I0217 19:54:28.830036 140532789532416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.6434776186943054, loss=1.0506923198699951
I0217 19:55:52.593365 140532797925120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.7554957270622253, loss=1.020946741104126
I0217 19:57:15.593816 140532789532416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.8461313843727112, loss=1.0403568744659424
I0217 19:58:42.362359 140532797925120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.675020158290863, loss=1.0181028842926025
I0217 19:58:49.636614 140599226058560 spec.py:321] Evaluating on the training split.
I0217 19:59:44.198536 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 20:00:34.916916 140599226058560 spec.py:349] Evaluating on the test split.
I0217 20:01:01.027335 140599226058560 submission_runner.py:408] Time since start: 53624.07s, 	Step: 61811, 	{'train/ctc_loss': Array(0.08787621, dtype=float32), 'train/wer': 0.034435677203232415, 'validation/ctc_loss': Array(0.34125715, dtype=float32), 'validation/wer': 0.09822644023287024, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18546745, dtype=float32), 'test/wer': 0.059655109377856314, 'test/num_examples': 2472, 'score': 49009.17750668526, 'total_duration': 53624.07222151756, 'accumulated_submission_time': 49009.17750668526, 'accumulated_eval_time': 4610.411191225052, 'accumulated_logging_time': 1.8653712272644043}
I0217 20:01:01.072305 140532797925120 logging_writer.py:48] [61811] accumulated_eval_time=4610.411191, accumulated_logging_time=1.865371, accumulated_submission_time=49009.177507, global_step=61811, preemption_count=0, score=49009.177507, test/ctc_loss=0.1854674518108368, test/num_examples=2472, test/wer=0.059655, total_duration=53624.072222, train/ctc_loss=0.08787620812654495, train/wer=0.034436, validation/ctc_loss=0.34125715494155884, validation/num_examples=5348, validation/wer=0.098226
I0217 20:02:08.780363 140532789532416 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.1116386651992798, loss=1.048754334449768
I0217 20:03:24.260711 140532797925120 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.6340748071670532, loss=1.0346592664718628
I0217 20:04:39.735390 140532789532416 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.7392094135284424, loss=1.069882869720459
I0217 20:05:55.233657 140532797925120 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.6034286618232727, loss=0.9931831955909729
I0217 20:07:11.060658 140532789532416 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.5675755739212036, loss=0.9947879910469055
I0217 20:08:26.536791 140532797925120 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.5723669528961182, loss=1.0419414043426514
I0217 20:09:46.666064 140532789532416 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.7418502569198608, loss=1.0393778085708618
I0217 20:11:12.164707 140532797925120 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.6860265135765076, loss=0.9725079536437988
I0217 20:12:35.689026 140532789532416 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.8898810744285583, loss=1.0494524240493774
I0217 20:13:58.879718 140532797925120 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.7201442122459412, loss=1.019983172416687
I0217 20:15:20.916053 140532797925120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.6711017489433289, loss=0.9887775182723999
I0217 20:16:36.485261 140532789532416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.6054131984710693, loss=1.0369476079940796
I0217 20:17:52.135520 140532797925120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.734681248664856, loss=1.0378365516662598
I0217 20:19:07.713422 140532789532416 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.024155616760254, loss=1.0378917455673218
I0217 20:20:23.318186 140532797925120 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.595456600189209, loss=0.9784238934516907
I0217 20:21:44.492043 140532789532416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.6773181557655334, loss=1.0317529439926147
I0217 20:23:07.508596 140532797925120 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.2279303073883057, loss=1.0456119775772095
I0217 20:24:30.324022 140532789532416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.9014604687690735, loss=0.9380697011947632
I0217 20:25:01.328673 140599226058560 spec.py:321] Evaluating on the training split.
I0217 20:25:55.764335 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 20:26:47.869628 140599226058560 spec.py:349] Evaluating on the test split.
I0217 20:27:14.317343 140599226058560 submission_runner.py:408] Time since start: 55197.36s, 	Step: 63638, 	{'train/ctc_loss': Array(0.09771829, dtype=float32), 'train/wer': 0.037992130240200636, 'validation/ctc_loss': Array(0.33584002, dtype=float32), 'validation/wer': 0.09558106529441865, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18322575, dtype=float32), 'test/wer': 0.05933012410375155, 'test/num_examples': 2472, 'score': 50449.347014427185, 'total_duration': 55197.36210608482, 'accumulated_submission_time': 50449.347014427185, 'accumulated_eval_time': 4743.393732786179, 'accumulated_logging_time': 1.925506830215454}
I0217 20:27:14.358593 140532797925120 logging_writer.py:48] [63638] accumulated_eval_time=4743.393733, accumulated_logging_time=1.925507, accumulated_submission_time=50449.347014, global_step=63638, preemption_count=0, score=50449.347014, test/ctc_loss=0.18322575092315674, test/num_examples=2472, test/wer=0.059330, total_duration=55197.362106, train/ctc_loss=0.09771829098463058, train/wer=0.037992, validation/ctc_loss=0.33584001660346985, validation/num_examples=5348, validation/wer=0.095581
I0217 20:28:01.804639 140532789532416 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.8102643489837646, loss=1.0637083053588867
I0217 20:29:17.156750 140532797925120 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.6330560445785522, loss=0.9737143516540527
I0217 20:30:36.761730 140532797925120 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.8684190511703491, loss=0.9617477655410767
I0217 20:31:52.169363 140532789532416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.8972994685173035, loss=1.0063261985778809
I0217 20:33:07.621533 140532797925120 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.962588906288147, loss=0.9667662382125854
I0217 20:34:23.125221 140532789532416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.6701908707618713, loss=0.98467618227005
I0217 20:35:38.612386 140532797925120 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.5568935871124268, loss=0.9758919477462769
I0217 20:36:58.668957 140532789532416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.6900990605354309, loss=0.9743085503578186
I0217 20:38:22.673536 140532797925120 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.7231031060218811, loss=1.0253814458847046
I0217 20:39:46.330071 140532789532416 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.3993233442306519, loss=1.0546542406082153
I0217 20:41:10.083133 140532797925120 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.969865620136261, loss=1.053945541381836
I0217 20:42:33.617544 140532789532416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.8177850246429443, loss=1.0376001596450806
I0217 20:43:59.121394 140532797925120 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.7786310315132141, loss=1.0536561012268066
I0217 20:45:14.501462 140532789532416 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.8910278677940369, loss=0.9930670261383057
I0217 20:46:30.057561 140532797925120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.9760472774505615, loss=0.9520221948623657
I0217 20:47:45.490956 140532789532416 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.8771430253982544, loss=1.016061782836914
I0217 20:49:00.998107 140532797925120 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.6687743067741394, loss=0.987272322177887
I0217 20:50:17.781276 140532789532416 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1357957124710083, loss=1.0067198276519775
I0217 20:51:14.833813 140599226058560 spec.py:321] Evaluating on the training split.
I0217 20:52:09.959651 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 20:53:00.820596 140599226058560 spec.py:349] Evaluating on the test split.
I0217 20:53:26.602362 140599226058560 submission_runner.py:408] Time since start: 56769.65s, 	Step: 65470, 	{'train/ctc_loss': Array(0.08789349, dtype=float32), 'train/wer': 0.0339759476169358, 'validation/ctc_loss': Array(0.32956982, dtype=float32), 'validation/wer': 0.09380460913137087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1784024, dtype=float32), 'test/wer': 0.05784737879064855, 'test/num_examples': 2472, 'score': 51889.73348236084, 'total_duration': 56769.64642548561, 'accumulated_submission_time': 51889.73348236084, 'accumulated_eval_time': 4875.155463933945, 'accumulated_logging_time': 1.9842355251312256}
I0217 20:53:26.646711 140532797925120 logging_writer.py:48] [65470] accumulated_eval_time=4875.155464, accumulated_logging_time=1.984236, accumulated_submission_time=51889.733482, global_step=65470, preemption_count=0, score=51889.733482, test/ctc_loss=0.1784023940563202, test/num_examples=2472, test/wer=0.057847, total_duration=56769.646425, train/ctc_loss=0.08789349347352982, train/wer=0.033976, validation/ctc_loss=0.32956981658935547, validation/num_examples=5348, validation/wer=0.093805
I0217 20:53:50.006639 140532789532416 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.6664837002754211, loss=1.0234161615371704
I0217 20:55:05.461982 140532797925120 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.8513952493667603, loss=1.0337401628494263
I0217 20:56:21.097418 140532789532416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.9332928657531738, loss=1.018711805343628
I0217 20:57:36.604634 140532797925120 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.6799079775810242, loss=1.0011321306228638
I0217 20:58:54.960102 140532789532416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.595538854598999, loss=1.009995937347412
I0217 21:00:15.337674 140532797925120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.7547937035560608, loss=0.9975512623786926
I0217 21:01:30.846248 140532789532416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.832971453666687, loss=1.0590732097625732
I0217 21:02:46.412487 140532797925120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.8244391083717346, loss=1.0153779983520508
I0217 21:04:01.886455 140532789532416 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2066656351089478, loss=0.9776343107223511
I0217 21:05:17.550840 140532797925120 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.5740200877189636, loss=0.9740352630615234
I0217 21:06:39.886126 140532789532416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9883649349212646, loss=0.936133861541748
I0217 21:08:03.030008 140532797925120 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.5128791332244873, loss=0.9819482564926147
I0217 21:09:26.138302 140532789532416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.7370510101318359, loss=1.0388398170471191
I0217 21:10:49.917455 140532797925120 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.6357225775718689, loss=1.0231047868728638
I0217 21:12:13.951110 140532789532416 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.6976282000541687, loss=0.9620524048805237
I0217 21:13:36.723905 140532797925120 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.6623730063438416, loss=0.9964897632598877
I0217 21:14:52.396298 140532789532416 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.622908353805542, loss=1.0103967189788818
I0217 21:16:07.917523 140532797925120 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.5939553380012512, loss=1.0183274745941162
I0217 21:17:23.424175 140532789532416 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.6392914056777954, loss=1.003106951713562
I0217 21:17:26.918351 140599226058560 spec.py:321] Evaluating on the training split.
I0217 21:18:23.923690 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 21:19:15.449561 140599226058560 spec.py:349] Evaluating on the test split.
I0217 21:19:41.593555 140599226058560 submission_runner.py:408] Time since start: 58344.64s, 	Step: 67306, 	{'train/ctc_loss': Array(0.08603654, dtype=float32), 'train/wer': 0.03281721838471094, 'validation/ctc_loss': Array(0.32403156, dtype=float32), 'validation/wer': 0.09274259729476621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1745069, dtype=float32), 'test/wer': 0.05711616192391282, 'test/num_examples': 2472, 'score': 53329.91587328911, 'total_duration': 58344.639607191086, 'accumulated_submission_time': 53329.91587328911, 'accumulated_eval_time': 5009.825822591782, 'accumulated_logging_time': 2.044980049133301}
I0217 21:19:41.639832 140532797925120 logging_writer.py:48] [67306] accumulated_eval_time=5009.825823, accumulated_logging_time=2.044980, accumulated_submission_time=53329.915873, global_step=67306, preemption_count=0, score=53329.915873, test/ctc_loss=0.17450690269470215, test/num_examples=2472, test/wer=0.057116, total_duration=58344.639607, train/ctc_loss=0.08603654056787491, train/wer=0.032817, validation/ctc_loss=0.3240315616130829, validation/num_examples=5348, validation/wer=0.092743
I0217 21:20:53.034451 140532789532416 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.6404010653495789, loss=0.9144002795219421
I0217 21:22:08.597010 140532797925120 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.7791199684143066, loss=0.9985056519508362
I0217 21:23:24.089983 140532789532416 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.6418719291687012, loss=0.9850990176200867
I0217 21:24:39.852504 140532797925120 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.617079496383667, loss=0.9702166318893433
I0217 21:26:03.122446 140532789532416 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.8371100425720215, loss=0.9496415257453918
I0217 21:27:27.164317 140532797925120 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.7308821082115173, loss=1.0059387683868408
I0217 21:28:52.271935 140532797925120 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.6969801187515259, loss=0.9801205992698669
I0217 21:30:07.741161 140532789532416 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.5869476199150085, loss=0.9203848242759705
I0217 21:31:23.227963 140532797925120 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.6583605408668518, loss=0.9728604555130005
I0217 21:32:38.999640 140532789532416 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.8340694904327393, loss=0.912969708442688
I0217 21:33:54.413040 140532797925120 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.6149793863296509, loss=0.9468655586242676
I0217 21:35:15.014845 140532789532416 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.6331990361213684, loss=0.9840843677520752
I0217 21:36:38.386534 140532797925120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.7587263584136963, loss=0.9926230907440186
I0217 21:38:03.456054 140532789532416 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.6205050349235535, loss=0.9707607626914978
I0217 21:39:26.356005 140532797925120 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2193143367767334, loss=0.9705120325088501
I0217 21:40:48.389653 140532789532416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.5905788540840149, loss=0.963947057723999
I0217 21:42:12.223622 140532797925120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.7728794813156128, loss=1.0261119604110718
I0217 21:43:31.910002 140532797925120 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.6677767038345337, loss=1.0102704763412476
I0217 21:43:42.195845 140599226058560 spec.py:321] Evaluating on the training split.
I0217 21:44:37.126136 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 21:45:28.741767 140599226058560 spec.py:349] Evaluating on the test split.
I0217 21:45:54.845875 140599226058560 submission_runner.py:408] Time since start: 59917.89s, 	Step: 69115, 	{'train/ctc_loss': Array(0.07548663, dtype=float32), 'train/wer': 0.029929910967986362, 'validation/ctc_loss': Array(0.31870773, dtype=float32), 'validation/wer': 0.09057995500931675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17174639, dtype=float32), 'test/wer': 0.05465846078849552, 'test/num_examples': 2472, 'score': 54770.38207864761, 'total_duration': 59917.89059305191, 'accumulated_submission_time': 54770.38207864761, 'accumulated_eval_time': 5142.46967458725, 'accumulated_logging_time': 2.1079823970794678}
I0217 21:45:54.888702 140532797925120 logging_writer.py:48] [69115] accumulated_eval_time=5142.469675, accumulated_logging_time=2.107982, accumulated_submission_time=54770.382079, global_step=69115, preemption_count=0, score=54770.382079, test/ctc_loss=0.1717463880777359, test/num_examples=2472, test/wer=0.054658, total_duration=59917.890593, train/ctc_loss=0.07548663020133972, train/wer=0.029930, validation/ctc_loss=0.31870773434638977, validation/num_examples=5348, validation/wer=0.090580
I0217 21:46:59.568145 140532789532416 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.6367363929748535, loss=0.9550588726997375
I0217 21:48:14.984690 140532797925120 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.0592197179794312, loss=0.9888025522232056
I0217 21:49:30.450595 140532789532416 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.7601442337036133, loss=0.9613386988639832
I0217 21:50:46.352542 140532797925120 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.8175445199012756, loss=0.9824550747871399
I0217 21:52:01.805766 140532789532416 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.7965877652168274, loss=0.9841237664222717
I0217 21:53:20.376383 140532797925120 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2644200325012207, loss=0.9031095504760742
I0217 21:54:43.942690 140532789532416 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.8110343217849731, loss=1.0200622081756592
I0217 21:56:07.561963 140532797925120 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.6505353450775146, loss=1.00204598903656
I0217 21:57:31.140109 140532789532416 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.6023062467575073, loss=1.0155701637268066
I0217 21:58:53.777249 140532797925120 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.7702513337135315, loss=0.942373514175415
I0217 22:00:09.248556 140532789532416 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.5773163437843323, loss=0.9483455419540405
I0217 22:01:24.742181 140532797925120 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.9878427386283875, loss=0.9433389902114868
I0217 22:02:40.264494 140532789532416 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.8541967868804932, loss=0.9325024485588074
I0217 22:03:55.768529 140532797925120 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.9736944437026978, loss=0.9624132513999939
I0217 22:05:17.618584 140532789532416 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.7048079371452332, loss=0.9955227971076965
I0217 22:06:41.664301 140532797925120 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.2941709756851196, loss=0.8937513828277588
I0217 22:08:05.355053 140532789532416 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.8585875630378723, loss=0.9534755945205688
I0217 22:09:28.879264 140532797925120 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.3979735374450684, loss=0.9221618175506592
I0217 22:09:54.913112 140599226058560 spec.py:321] Evaluating on the training split.
I0217 22:10:50.098632 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 22:11:42.064164 140599226058560 spec.py:349] Evaluating on the test split.
I0217 22:12:08.138140 140599226058560 submission_runner.py:408] Time since start: 61491.18s, 	Step: 70933, 	{'train/ctc_loss': Array(0.06823383, dtype=float32), 'train/wer': 0.026325386571083232, 'validation/ctc_loss': Array(0.31399453, dtype=float32), 'validation/wer': 0.08867798835648841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17032003, dtype=float32), 'test/wer': 0.05425222919586456, 'test/num_examples': 2472, 'score': 56210.31742787361, 'total_duration': 61491.18255186081, 'accumulated_submission_time': 56210.31742787361, 'accumulated_eval_time': 5275.688220500946, 'accumulated_logging_time': 2.1689844131469727}
I0217 22:12:08.184364 140532797925120 logging_writer.py:48] [70933] accumulated_eval_time=5275.688221, accumulated_logging_time=2.168984, accumulated_submission_time=56210.317428, global_step=70933, preemption_count=0, score=56210.317428, test/ctc_loss=0.1703200340270996, test/num_examples=2472, test/wer=0.054252, total_duration=61491.182552, train/ctc_loss=0.06823383271694183, train/wer=0.026325, validation/ctc_loss=0.31399452686309814, validation/num_examples=5348, validation/wer=0.088678
I0217 22:12:59.337433 140532789532416 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.2184364795684814, loss=0.9840043783187866
I0217 22:14:18.946022 140532797925120 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.7222881317138672, loss=0.9208380579948425
I0217 22:15:34.464070 140532789532416 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.9415476322174072, loss=0.9463951587677002
I0217 22:16:50.072976 140532797925120 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.6033142805099487, loss=0.9223114848136902
I0217 22:18:05.642441 140532789532416 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.3472431898117065, loss=0.9489649534225464
I0217 22:19:21.983034 140532797925120 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.7321791052818298, loss=0.9481487274169922
I0217 22:20:45.347595 140532789532416 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.7251399159431458, loss=0.9540644288063049
I0217 22:22:09.859923 140532797925120 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.841413140296936, loss=0.9811542630195618
I0217 22:23:34.221048 140532789532416 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.6579982042312622, loss=0.9708507657051086
I0217 22:24:58.070508 140532797925120 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.7417887449264526, loss=1.017016053199768
I0217 22:26:22.013928 140532789532416 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.682938814163208, loss=0.9628964066505432
I0217 22:27:49.297493 140532797925120 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.6070950031280518, loss=0.9305166006088257
I0217 22:29:04.684867 140532789532416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.6679742932319641, loss=0.8936286568641663
I0217 22:30:20.045356 140532797925120 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.7052584886550903, loss=0.9836006164550781
I0217 22:31:35.496727 140532789532416 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.8477387428283691, loss=0.9899929761886597
I0217 22:32:50.916099 140532797925120 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.9532842040061951, loss=1.0047252178192139
I0217 22:34:07.179718 140532789532416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.8310068845748901, loss=0.9633065462112427
I0217 22:35:31.088489 140532797925120 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.6605687141418457, loss=1.0013364553451538
I0217 22:36:08.140702 140599226058560 spec.py:321] Evaluating on the training split.
I0217 22:37:03.897269 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 22:37:55.274580 140599226058560 spec.py:349] Evaluating on the test split.
I0217 22:38:21.371037 140599226058560 submission_runner.py:408] Time since start: 63064.42s, 	Step: 72746, 	{'train/ctc_loss': Array(0.07446662, dtype=float32), 'train/wer': 0.027438337938798487, 'validation/ctc_loss': Array(0.31198248, dtype=float32), 'validation/wer': 0.08779941492802457, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16882586, dtype=float32), 'test/wer': 0.05437409867365385, 'test/num_examples': 2472, 'score': 57650.18681359291, 'total_duration': 63064.41569805145, 'accumulated_submission_time': 57650.18681359291, 'accumulated_eval_time': 5408.91233420372, 'accumulated_logging_time': 2.2319982051849365}
I0217 22:38:21.420265 140532797925120 logging_writer.py:48] [72746] accumulated_eval_time=5408.912334, accumulated_logging_time=2.231998, accumulated_submission_time=57650.186814, global_step=72746, preemption_count=0, score=57650.186814, test/ctc_loss=0.16882586479187012, test/num_examples=2472, test/wer=0.054374, total_duration=63064.415698, train/ctc_loss=0.07446662336587906, train/wer=0.027438, validation/ctc_loss=0.31198248267173767, validation/num_examples=5348, validation/wer=0.087799
I0217 22:39:02.869833 140532789532416 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.7558392286300659, loss=0.9750667810440063
I0217 22:40:18.421986 140532797925120 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.2512296438217163, loss=0.9105536937713623
I0217 22:41:34.080360 140532789532416 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3349183797836304, loss=1.0088640451431274
I0217 22:42:49.948147 140532797925120 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.9712461829185486, loss=0.9563839435577393
I0217 22:44:09.153378 140532797925120 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.6812974214553833, loss=0.9161370992660522
I0217 22:45:24.533094 140532789532416 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.8989540934562683, loss=0.9379457831382751
I0217 22:46:39.944086 140532797925120 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.9124472737312317, loss=0.9532055854797363
I0217 22:47:55.529983 140532789532416 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.5842369198799133, loss=0.9559446573257446
I0217 22:49:11.090827 140532797925120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.9897032976150513, loss=0.9961429834365845
I0217 22:50:33.637880 140532789532416 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.9119383096694946, loss=0.9264968037605286
I0217 22:51:57.062473 140532797925120 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.8012227416038513, loss=0.9480283260345459
I0217 22:53:20.381232 140532789532416 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.6521683931350708, loss=0.9483570456504822
I0217 22:54:43.508068 140532797925120 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.5653069615364075, loss=0.9447546601295471
I0217 22:56:07.419712 140532789532416 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.6636167168617249, loss=0.9692238569259644
I0217 22:57:30.894812 140532797925120 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.5627129077911377, loss=0.952214777469635
I0217 22:58:46.775615 140532789532416 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.2339942455291748, loss=0.9256859421730042
I0217 23:00:02.417883 140532797925120 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.6615651249885559, loss=0.9474824070930481
I0217 23:01:17.924835 140532789532416 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.7544106841087341, loss=0.9107511043548584
I0217 23:02:21.727381 140599226058560 spec.py:321] Evaluating on the training split.
I0217 23:03:15.532893 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 23:04:06.973233 140599226058560 spec.py:349] Evaluating on the test split.
I0217 23:04:32.915465 140599226058560 submission_runner.py:408] Time since start: 64635.96s, 	Step: 74586, 	{'train/ctc_loss': Array(0.07089892, dtype=float32), 'train/wer': 0.026582449321132615, 'validation/ctc_loss': Array(0.3090179, dtype=float32), 'validation/wer': 0.08693049615262076, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16789155, dtype=float32), 'test/wer': 0.05343976601060264, 'test/num_examples': 2472, 'score': 59090.40604901314, 'total_duration': 64635.95995926857, 'accumulated_submission_time': 59090.40604901314, 'accumulated_eval_time': 5540.094013929367, 'accumulated_logging_time': 2.298288345336914}
I0217 23:04:32.959708 140532797925120 logging_writer.py:48] [74586] accumulated_eval_time=5540.094014, accumulated_logging_time=2.298288, accumulated_submission_time=59090.406049, global_step=74586, preemption_count=0, score=59090.406049, test/ctc_loss=0.16789154708385468, test/num_examples=2472, test/wer=0.053440, total_duration=64635.959959, train/ctc_loss=0.07089892029762268, train/wer=0.026582, validation/ctc_loss=0.3090178966522217, validation/num_examples=5348, validation/wer=0.086930
I0217 23:04:44.303416 140532789532416 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.8646048903465271, loss=0.9227071404457092
I0217 23:05:59.724797 140532797925120 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4234923124313354, loss=0.9764875769615173
I0217 23:07:15.282943 140532789532416 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.8992937207221985, loss=0.9372460842132568
I0217 23:08:30.803075 140532797925120 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.686401903629303, loss=0.9332594275474548
I0217 23:09:47.728687 140532789532416 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.8526871204376221, loss=0.9475464224815369
I0217 23:11:12.854483 140532797925120 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.7072117924690247, loss=0.9904003143310547
I0217 23:12:39.442357 140532797925120 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.0011677742004395, loss=0.9624683260917664
I0217 23:13:54.693814 140532789532416 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.300569772720337, loss=1.0030699968338013
I0217 23:15:10.009294 140532797925120 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.092922568321228, loss=0.8839466571807861
I0217 23:16:25.647107 140532789532416 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.762179434299469, loss=0.9341694712638855
I0217 23:17:41.076009 140532797925120 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.6399227976799011, loss=0.9515371322631836
I0217 23:18:59.051169 140532789532416 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.7753552198410034, loss=0.9266424775123596
I0217 23:20:22.277076 140532797925120 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.6506649255752563, loss=0.9480229616165161
I0217 23:21:46.443679 140532789532416 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.6190934181213379, loss=0.9181393384933472
I0217 23:23:09.744354 140532797925120 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.7014040350914001, loss=0.9280232787132263
I0217 23:24:33.407526 140532789532416 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.8789410591125488, loss=0.9938811659812927
I0217 23:25:57.716301 140532797925120 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.9053622484207153, loss=0.9052297472953796
I0217 23:27:18.605582 140532797925120 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.3876230716705322, loss=0.9382412433624268
I0217 23:28:32.951400 140599226058560 spec.py:321] Evaluating on the training split.
I0217 23:29:27.252717 140599226058560 spec.py:333] Evaluating on the validation split.
I0217 23:30:18.365596 140599226058560 spec.py:349] Evaluating on the test split.
I0217 23:30:44.560935 140599226058560 submission_runner.py:408] Time since start: 66207.61s, 	Step: 76400, 	{'train/ctc_loss': Array(0.0723265, dtype=float32), 'train/wer': 0.02635920509936258, 'validation/ctc_loss': Array(0.3084427, dtype=float32), 'validation/wer': 0.08698842407098101, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16666134, dtype=float32), 'test/wer': 0.0529116649401824, 'test/num_examples': 2472, 'score': 60530.30942058563, 'total_duration': 66207.60625338554, 'accumulated_submission_time': 60530.30942058563, 'accumulated_eval_time': 5671.698009729385, 'accumulated_logging_time': 2.359781265258789}
I0217 23:30:44.601068 140532797925120 logging_writer.py:48] [76400] accumulated_eval_time=5671.698010, accumulated_logging_time=2.359781, accumulated_submission_time=60530.309421, global_step=76400, preemption_count=0, score=60530.309421, test/ctc_loss=0.166661337018013, test/num_examples=2472, test/wer=0.052912, total_duration=66207.606253, train/ctc_loss=0.07232649624347687, train/wer=0.026359, validation/ctc_loss=0.30844271183013916, validation/num_examples=5348, validation/wer=0.086988
I0217 23:30:45.479473 140532789532416 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.983487069606781, loss=0.9438508749008179
I0217 23:32:00.793906 140532797925120 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.5672374963760376, loss=0.9397620558738708
I0217 23:33:16.279667 140532789532416 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.6904560327529907, loss=0.9254563450813293
I0217 23:34:32.042434 140532797925120 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.526523470878601, loss=0.9345727562904358
I0217 23:35:47.579831 140532789532416 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.1314038038253784, loss=0.9033809900283813
I0217 23:37:07.252863 140532797925120 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.2969673871994019, loss=0.9215491414070129
I0217 23:38:30.522187 140532789532416 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.7169641256332397, loss=0.9750210046768188
I0217 23:39:42.930693 140532797925120 logging_writer.py:48] [77088] global_step=77088, preemption_count=0, score=61068.578021
I0217 23:39:43.767081 140599226058560 checkpoints.py:490] Saving checkpoint at step: 77088
I0217 23:39:45.257741 140599226058560 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_5/checkpoint_77088
I0217 23:39:45.292931 140599226058560 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_conformer_jax/trial_5/checkpoint_77088.
I0217 23:39:49.080592 140599226058560 submission_runner.py:583] Tuning trial 5/5
I0217 23:39:49.080867 140599226058560 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0217 23:39:49.112166 140599226058560 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.575264, dtype=float32), 'train/wer': 1.3520278272013182, 'validation/ctc_loss': Array(31.087046, dtype=float32), 'validation/wer': 1.0585651254622166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.237438, dtype=float32), 'test/wer': 1.1023297381837385, 'test/num_examples': 2472, 'score': 36.564847230911255, 'total_duration': 169.93663430213928, 'accumulated_submission_time': 36.564847230911255, 'accumulated_eval_time': 133.37171983718872, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1784, {'train/ctc_loss': Array(4.0700784, dtype=float32), 'train/wer': 0.6632959745030377, 'validation/ctc_loss': Array(3.8945782, dtype=float32), 'validation/wer': 0.6469486469003737, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.5804627, dtype=float32), 'test/wer': 0.5944590010765137, 'test/num_examples': 2472, 'score': 1476.6239953041077, 'total_duration': 1733.8049068450928, 'accumulated_submission_time': 1476.6239953041077, 'accumulated_eval_time': 257.0766489505768, 'accumulated_logging_time': 0.03051924705505371, 'global_step': 1784, 'preemption_count': 0}), (3613, {'train/ctc_loss': Array(0.928824, dtype=float32), 'train/wer': 0.29132301859311877, 'validation/ctc_loss': Array(0.9600111, dtype=float32), 'validation/wer': 0.2800428666595866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6869836, dtype=float32), 'test/wer': 0.22074624743566307, 'test/num_examples': 2472, 'score': 2917.59091591835, 'total_duration': 3304.8210270404816, 'accumulated_submission_time': 2917.59091591835, 'accumulated_eval_time': 386.99128317832947, 'accumulated_logging_time': 0.0886681079864502, 'global_step': 3613, 'preemption_count': 0}), (5441, {'train/ctc_loss': Array(0.6443802, dtype=float32), 'train/wer': 0.2176905895966201, 'validation/ctc_loss': Array(0.83076894, dtype=float32), 'validation/wer': 0.24935072458171215, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55067784, dtype=float32), 'test/wer': 0.18333231775435174, 'test/num_examples': 2472, 'score': 4357.46843123436, 'total_duration': 4876.902319431305, 'accumulated_submission_time': 4357.46843123436, 'accumulated_eval_time': 519.0642786026001, 'accumulated_logging_time': 0.14203286170959473, 'global_step': 5441, 'preemption_count': 0}), (7246, {'train/ctc_loss': Array(0.5867469, dtype=float32), 'train/wer': 0.19511405655098257, 'validation/ctc_loss': Array(0.7002347, dtype=float32), 'validation/wer': 0.209592863280458, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45089793, dtype=float32), 'test/wer': 0.1516259419495054, 'test/num_examples': 2472, 'score': 5797.962051391602, 'total_duration': 6449.165390253067, 'accumulated_submission_time': 5797.962051391602, 'accumulated_eval_time': 650.7103319168091, 'accumulated_logging_time': 0.19083642959594727, 'global_step': 7246, 'preemption_count': 0}), (9070, {'train/ctc_loss': Array(0.4774239, dtype=float32), 'train/wer': 0.16464857563720095, 'validation/ctc_loss': Array(0.6440632, dtype=float32), 'validation/wer': 0.19560327099645675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.405954, dtype=float32), 'test/wer': 0.1373875246277903, 'test/num_examples': 2472, 'score': 7238.451512336731, 'total_duration': 8021.87784409523, 'accumulated_submission_time': 7238.451512336731, 'accumulated_eval_time': 782.7996170520782, 'accumulated_logging_time': 0.24584484100341797, 'global_step': 9070, 'preemption_count': 0}), (10892, {'train/ctc_loss': Array(0.4616692, dtype=float32), 'train/wer': 0.16355717701154388, 'validation/ctc_loss': Array(0.62427807, dtype=float32), 'validation/wer': 0.18911534414010833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38973525, dtype=float32), 'test/wer': 0.13174090549021997, 'test/num_examples': 2472, 'score': 8678.661425828934, 'total_duration': 9594.367964029312, 'accumulated_submission_time': 8678.661425828934, 'accumulated_eval_time': 914.9536378383636, 'accumulated_logging_time': 0.29439759254455566, 'global_step': 10892, 'preemption_count': 0}), (12728, {'train/ctc_loss': Array(0.42614308, dtype=float32), 'train/wer': 0.14673770491803279, 'validation/ctc_loss': Array(0.5865799, dtype=float32), 'validation/wer': 0.17795456520269945, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36318412, dtype=float32), 'test/wer': 0.12253975991712876, 'test/num_examples': 2472, 'score': 10119.200992584229, 'total_duration': 11165.735782384872, 'accumulated_submission_time': 10119.200992584229, 'accumulated_eval_time': 1045.6514217853546, 'accumulated_logging_time': 0.3471841812133789, 'global_step': 12728, 'preemption_count': 0}), (14539, {'train/ctc_loss': Array(0.43106797, dtype=float32), 'train/wer': 0.15068626163046397, 'validation/ctc_loss': Array(0.58034813, dtype=float32), 'validation/wer': 0.17549262867238866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3582644, dtype=float32), 'test/wer': 0.1215444925151829, 'test/num_examples': 2472, 'score': 11559.555562257767, 'total_duration': 12735.91278553009, 'accumulated_submission_time': 11559.555562257767, 'accumulated_eval_time': 1175.3477528095245, 'accumulated_logging_time': 0.3970916271209717, 'global_step': 14539, 'preemption_count': 0}), (16355, {'train/ctc_loss': Array(0.45272714, dtype=float32), 'train/wer': 0.15471028120091151, 'validation/ctc_loss': Array(0.5875047, dtype=float32), 'validation/wer': 0.17651602189675314, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3553724, dtype=float32), 'test/wer': 0.1196758271890805, 'test/num_examples': 2472, 'score': 13000.484461307526, 'total_duration': 14308.726927280426, 'accumulated_submission_time': 13000.484461307526, 'accumulated_eval_time': 1307.0945451259613, 'accumulated_logging_time': 0.4598226547241211, 'global_step': 16355, 'preemption_count': 0}), (18164, {'train/ctc_loss': Array(0.43035367, dtype=float32), 'train/wer': 0.1507336027182378, 'validation/ctc_loss': Array(0.57992405, dtype=float32), 'validation/wer': 0.17502920532550661, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34942833, dtype=float32), 'test/wer': 0.11839619767229298, 'test/num_examples': 2472, 'score': 14440.638258457184, 'total_duration': 15880.81642794609, 'accumulated_submission_time': 14440.638258457184, 'accumulated_eval_time': 1438.896250963211, 'accumulated_logging_time': 0.5163545608520508, 'global_step': 18164, 'preemption_count': 0}), (20003, {'train/ctc_loss': Array(0.42334485, dtype=float32), 'train/wer': 0.14974843245075295, 'validation/ctc_loss': Array(0.537798, dtype=float32), 'validation/wer': 0.16227540863319076, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32704383, dtype=float32), 'test/wer': 0.10889037840472854, 'test/num_examples': 2472, 'score': 15881.029235124588, 'total_duration': 17450.236847639084, 'accumulated_submission_time': 15881.029235124588, 'accumulated_eval_time': 1567.7926728725433, 'accumulated_logging_time': 0.5708911418914795, 'global_step': 20003, 'preemption_count': 0}), (21819, {'train/ctc_loss': Array(0.35540873, dtype=float32), 'train/wer': 0.12463731411488127, 'validation/ctc_loss': Array(0.52952045, dtype=float32), 'validation/wer': 0.1576218658582504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31939596, dtype=float32), 'test/wer': 0.10728576361383625, 'test/num_examples': 2472, 'score': 17321.37030696869, 'total_duration': 19023.241703748703, 'accumulated_submission_time': 17321.37030696869, 'accumulated_eval_time': 1700.3270015716553, 'accumulated_logging_time': 0.6231732368469238, 'global_step': 21819, 'preemption_count': 0}), (23623, {'train/ctc_loss': Array(0.35186237, dtype=float32), 'train/wer': 0.1239148128052089, 'validation/ctc_loss': Array(0.5021615, dtype=float32), 'validation/wer': 0.1515297797773637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30010766, dtype=float32), 'test/wer': 0.10245160766152783, 'test/num_examples': 2472, 'score': 18761.539265871048, 'total_duration': 20594.3846950531, 'accumulated_submission_time': 18761.539265871048, 'accumulated_eval_time': 1831.173261165619, 'accumulated_logging_time': 0.6752007007598877, 'global_step': 23623, 'preemption_count': 0}), (25432, {'train/ctc_loss': Array(0.36941093, dtype=float32), 'train/wer': 0.13036828062065567, 'validation/ctc_loss': Array(0.5116533, dtype=float32), 'validation/wer': 0.1539530976954343, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3096689, dtype=float32), 'test/wer': 0.10352812138199988, 'test/num_examples': 2472, 'score': 20202.135964870453, 'total_duration': 22166.200580835342, 'accumulated_submission_time': 20202.135964870453, 'accumulated_eval_time': 1962.2557699680328, 'accumulated_logging_time': 0.7344212532043457, 'global_step': 25432, 'preemption_count': 0}), (27267, {'train/ctc_loss': Array(0.35059008, dtype=float32), 'train/wer': 0.12348275975702187, 'validation/ctc_loss': Array(0.49381366, dtype=float32), 'validation/wer': 0.14929955492049393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29647806, dtype=float32), 'test/wer': 0.09820648751853432, 'test/num_examples': 2472, 'score': 21642.26782298088, 'total_duration': 23739.43984937668, 'accumulated_submission_time': 21642.26782298088, 'accumulated_eval_time': 2095.2302215099335, 'accumulated_logging_time': 0.7898995876312256, 'global_step': 27267, 'preemption_count': 0}), (29093, {'train/ctc_loss': Array(0.31172824, dtype=float32), 'train/wer': 0.11294081880826916, 'validation/ctc_loss': Array(0.48430818, dtype=float32), 'validation/wer': 0.14549562161483728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28592485, dtype=float32), 'test/wer': 0.09530193163122296, 'test/num_examples': 2472, 'score': 23082.85145688057, 'total_duration': 25310.55036687851, 'accumulated_submission_time': 23082.85145688057, 'accumulated_eval_time': 2225.6262097358704, 'accumulated_logging_time': 0.8439171314239502, 'global_step': 29093, 'preemption_count': 0}), (30900, {'train/ctc_loss': Array(0.27994934, dtype=float32), 'train/wer': 0.10009418614607415, 'validation/ctc_loss': Array(0.4781998, dtype=float32), 'validation/wer': 0.1433522886355079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27936098, dtype=float32), 'test/wer': 0.09432697580890866, 'test/num_examples': 2472, 'score': 24523.22280049324, 'total_duration': 26880.09740138054, 'accumulated_submission_time': 24523.22280049324, 'accumulated_eval_time': 2354.6739501953125, 'accumulated_logging_time': 0.8952944278717041, 'global_step': 30900, 'preemption_count': 0}), (32700, {'train/ctc_loss': Array(0.31628087, dtype=float32), 'train/wer': 0.114678250442593, 'validation/ctc_loss': Array(0.46717182, dtype=float32), 'validation/wer': 0.1399731600644931, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27516112, dtype=float32), 'test/wer': 0.0914224199215973, 'test/num_examples': 2472, 'score': 25964.00806760788, 'total_duration': 28451.8306787014, 'accumulated_submission_time': 25964.00806760788, 'accumulated_eval_time': 2485.4901201725006, 'accumulated_logging_time': 0.9515819549560547, 'global_step': 32700, 'preemption_count': 0}), (34518, {'train/ctc_loss': Array(0.29610607, dtype=float32), 'train/wer': 0.10736932352761061, 'validation/ctc_loss': Array(0.47222495, dtype=float32), 'validation/wer': 0.139036658717669, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26905754, dtype=float32), 'test/wer': 0.09026465988259907, 'test/num_examples': 2472, 'score': 27403.91512775421, 'total_duration': 30023.466059207916, 'accumulated_submission_time': 27403.91512775421, 'accumulated_eval_time': 2617.0825748443604, 'accumulated_logging_time': 1.0097923278808594, 'global_step': 34518, 'preemption_count': 0}), (36337, {'train/ctc_loss': Array(0.31838846, dtype=float32), 'train/wer': 0.10978509603818028, 'validation/ctc_loss': Array(0.4541082, dtype=float32), 'validation/wer': 0.1351844521467121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26998752, dtype=float32), 'test/wer': 0.08935063879917941, 'test/num_examples': 2472, 'score': 28844.04262661934, 'total_duration': 31594.535401821136, 'accumulated_submission_time': 28844.04262661934, 'accumulated_eval_time': 2747.890196323395, 'accumulated_logging_time': 1.0664589405059814, 'global_step': 36337, 'preemption_count': 0}), (38133, {'train/ctc_loss': Array(0.27280307, dtype=float32), 'train/wer': 0.09982278037894368, 'validation/ctc_loss': Array(0.47273842, dtype=float32), 'validation/wer': 0.14050416598279541, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27322602, dtype=float32), 'test/wer': 0.09150366624012349, 'test/num_examples': 2472, 'score': 30284.337661266327, 'total_duration': 33165.68579244614, 'accumulated_submission_time': 30284.337661266327, 'accumulated_eval_time': 2878.6182096004486, 'accumulated_logging_time': 1.1174898147583008, 'global_step': 38133, 'preemption_count': 0}), (39958, {'train/ctc_loss': Array(0.23454855, dtype=float32), 'train/wer': 0.08544679623072911, 'validation/ctc_loss': Array(0.43293995, dtype=float32), 'validation/wer': 0.1286289427189434, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2446584, dtype=float32), 'test/wer': 0.08248532488371621, 'test/num_examples': 2472, 'score': 31724.60327744484, 'total_duration': 34736.20393896103, 'accumulated_submission_time': 31724.60327744484, 'accumulated_eval_time': 3008.73100566864, 'accumulated_logging_time': 1.1783719062805176, 'global_step': 39958, 'preemption_count': 0}), (41786, {'train/ctc_loss': Array(0.25626796, dtype=float32), 'train/wer': 0.09345090317609794, 'validation/ctc_loss': Array(0.42910972, dtype=float32), 'validation/wer': 0.1268042132905954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24258377, dtype=float32), 'test/wer': 0.0816728616984543, 'test/num_examples': 2472, 'score': 33165.11921596527, 'total_duration': 36306.827905893326, 'accumulated_submission_time': 33165.11921596527, 'accumulated_eval_time': 3138.707051753998, 'accumulated_logging_time': 1.2340517044067383, 'global_step': 41786, 'preemption_count': 0}), (43625, {'train/ctc_loss': Array(0.17323135, dtype=float32), 'train/wer': 0.06469077493987442, 'validation/ctc_loss': Array(0.4196442, dtype=float32), 'validation/wer': 0.12383058014810237, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23896857, dtype=float32), 'test/wer': 0.0785448784351959, 'test/num_examples': 2472, 'score': 34605.279014348984, 'total_duration': 37888.82444357872, 'accumulated_submission_time': 34605.279014348984, 'accumulated_eval_time': 3280.400631427765, 'accumulated_logging_time': 1.2985985279083252, 'global_step': 43625, 'preemption_count': 0}), (45444, {'train/ctc_loss': Array(0.1462238, dtype=float32), 'train/wer': 0.05505919084168076, 'validation/ctc_loss': Array(0.4182151, dtype=float32), 'validation/wer': 0.12126244243413113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23613536, dtype=float32), 'test/wer': 0.0795198342575102, 'test/num_examples': 2472, 'score': 36045.82629442215, 'total_duration': 39463.43499088287, 'accumulated_submission_time': 36045.82629442215, 'accumulated_eval_time': 3414.3309786319733, 'accumulated_logging_time': 1.3558061122894287, 'global_step': 45444, 'preemption_count': 0}), (47262, {'train/ctc_loss': Array(0.15300256, dtype=float32), 'train/wer': 0.05830980808577867, 'validation/ctc_loss': Array(0.41734868, dtype=float32), 'validation/wer': 0.12318371839307954, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22954966, dtype=float32), 'test/wer': 0.07570125728677919, 'test/num_examples': 2472, 'score': 37485.838297605515, 'total_duration': 41038.38579106331, 'accumulated_submission_time': 37485.838297605515, 'accumulated_eval_time': 3549.1361298561096, 'accumulated_logging_time': 1.411712408065796, 'global_step': 47262, 'preemption_count': 0}), (49083, {'train/ctc_loss': Array(0.14905518, dtype=float32), 'train/wer': 0.056064947965695326, 'validation/ctc_loss': Array(0.39878234, dtype=float32), 'validation/wer': 0.11683095667957172, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22368722, dtype=float32), 'test/wer': 0.07273576666057319, 'test/num_examples': 2472, 'score': 38926.85561108589, 'total_duration': 42614.214336395264, 'accumulated_submission_time': 38926.85561108589, 'accumulated_eval_time': 3683.8139424324036, 'accumulated_logging_time': 1.4672369956970215, 'global_step': 49083, 'preemption_count': 0}), (50921, {'train/ctc_loss': Array(0.13042888, dtype=float32), 'train/wer': 0.05082385324107196, 'validation/ctc_loss': Array(0.38371804, dtype=float32), 'validation/wer': 0.11144366027206812, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21463418, dtype=float32), 'test/wer': 0.07143582556415412, 'test/num_examples': 2472, 'score': 40367.21039605141, 'total_duration': 44186.81035447121, 'accumulated_submission_time': 40367.21039605141, 'accumulated_eval_time': 3815.9171483516693, 'accumulated_logging_time': 1.5273003578186035, 'global_step': 50921, 'preemption_count': 0}), (52735, {'train/ctc_loss': Array(0.1234728, dtype=float32), 'train/wer': 0.04845024217383904, 'validation/ctc_loss': Array(0.38684124, dtype=float32), 'validation/wer': 0.11160778937408884, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21119802, dtype=float32), 'test/wer': 0.07029837710478744, 'test/num_examples': 2472, 'score': 41807.4494600296, 'total_duration': 45760.08290052414, 'accumulated_submission_time': 41807.4494600296, 'accumulated_eval_time': 3948.8195514678955, 'accumulated_logging_time': 1.5827083587646484, 'global_step': 52735, 'preemption_count': 0}), (54541, {'train/ctc_loss': Array(0.13599047, dtype=float32), 'train/wer': 0.05029156920238527, 'validation/ctc_loss': Array(0.36117485, dtype=float32), 'validation/wer': 0.10539019280342161, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20237793, dtype=float32), 'test/wer': 0.06666260435074035, 'test/num_examples': 2472, 'score': 43247.61201763153, 'total_duration': 47332.53596878052, 'accumulated_submission_time': 43247.61201763153, 'accumulated_eval_time': 4080.977082490921, 'accumulated_logging_time': 1.6382241249084473, 'global_step': 54541, 'preemption_count': 0}), (56349, {'train/ctc_loss': Array(0.12227172, dtype=float32), 'train/wer': 0.04386070108296096, 'validation/ctc_loss': Array(0.3628258, dtype=float32), 'validation/wer': 0.10530330092588124, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1969371, dtype=float32), 'test/wer': 0.06475331586537485, 'test/num_examples': 2472, 'score': 44687.94367861748, 'total_duration': 48906.5517706871, 'accumulated_submission_time': 44687.94367861748, 'accumulated_eval_time': 4214.5278577804565, 'accumulated_logging_time': 1.6951165199279785, 'global_step': 56349, 'preemption_count': 0}), (58181, {'train/ctc_loss': Array(0.11364357, dtype=float32), 'train/wer': 0.04371404219049648, 'validation/ctc_loss': Array(0.35202444, dtype=float32), 'validation/wer': 0.10140282108962415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19438767, dtype=float32), 'test/wer': 0.06304714317632483, 'test/num_examples': 2472, 'score': 46128.458505392075, 'total_duration': 50480.25352835655, 'accumulated_submission_time': 46128.458505392075, 'accumulated_eval_time': 4347.578888177872, 'accumulated_logging_time': 1.7528400421142578, 'global_step': 58181, 'preemption_count': 0}), (60009, {'train/ctc_loss': Array(0.09262989, dtype=float32), 'train/wer': 0.035873062918113305, 'validation/ctc_loss': Array(0.34524652, dtype=float32), 'validation/wer': 0.09952981839597594, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19161133, dtype=float32), 'test/wer': 0.062234679991062906, 'test/num_examples': 2472, 'score': 47569.04715514183, 'total_duration': 52052.42256188393, 'accumulated_submission_time': 47569.04715514183, 'accumulated_eval_time': 4479.026484251022, 'accumulated_logging_time': 1.8070552349090576, 'global_step': 60009, 'preemption_count': 0}), (61811, {'train/ctc_loss': Array(0.08787621, dtype=float32), 'train/wer': 0.034435677203232415, 'validation/ctc_loss': Array(0.34125715, dtype=float32), 'validation/wer': 0.09822644023287024, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18546745, dtype=float32), 'test/wer': 0.059655109377856314, 'test/num_examples': 2472, 'score': 49009.17750668526, 'total_duration': 53624.07222151756, 'accumulated_submission_time': 49009.17750668526, 'accumulated_eval_time': 4610.411191225052, 'accumulated_logging_time': 1.8653712272644043, 'global_step': 61811, 'preemption_count': 0}), (63638, {'train/ctc_loss': Array(0.09771829, dtype=float32), 'train/wer': 0.037992130240200636, 'validation/ctc_loss': Array(0.33584002, dtype=float32), 'validation/wer': 0.09558106529441865, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18322575, dtype=float32), 'test/wer': 0.05933012410375155, 'test/num_examples': 2472, 'score': 50449.347014427185, 'total_duration': 55197.36210608482, 'accumulated_submission_time': 50449.347014427185, 'accumulated_eval_time': 4743.393732786179, 'accumulated_logging_time': 1.925506830215454, 'global_step': 63638, 'preemption_count': 0}), (65470, {'train/ctc_loss': Array(0.08789349, dtype=float32), 'train/wer': 0.0339759476169358, 'validation/ctc_loss': Array(0.32956982, dtype=float32), 'validation/wer': 0.09380460913137087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1784024, dtype=float32), 'test/wer': 0.05784737879064855, 'test/num_examples': 2472, 'score': 51889.73348236084, 'total_duration': 56769.64642548561, 'accumulated_submission_time': 51889.73348236084, 'accumulated_eval_time': 4875.155463933945, 'accumulated_logging_time': 1.9842355251312256, 'global_step': 65470, 'preemption_count': 0}), (67306, {'train/ctc_loss': Array(0.08603654, dtype=float32), 'train/wer': 0.03281721838471094, 'validation/ctc_loss': Array(0.32403156, dtype=float32), 'validation/wer': 0.09274259729476621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1745069, dtype=float32), 'test/wer': 0.05711616192391282, 'test/num_examples': 2472, 'score': 53329.91587328911, 'total_duration': 58344.639607191086, 'accumulated_submission_time': 53329.91587328911, 'accumulated_eval_time': 5009.825822591782, 'accumulated_logging_time': 2.044980049133301, 'global_step': 67306, 'preemption_count': 0}), (69115, {'train/ctc_loss': Array(0.07548663, dtype=float32), 'train/wer': 0.029929910967986362, 'validation/ctc_loss': Array(0.31870773, dtype=float32), 'validation/wer': 0.09057995500931675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17174639, dtype=float32), 'test/wer': 0.05465846078849552, 'test/num_examples': 2472, 'score': 54770.38207864761, 'total_duration': 59917.89059305191, 'accumulated_submission_time': 54770.38207864761, 'accumulated_eval_time': 5142.46967458725, 'accumulated_logging_time': 2.1079823970794678, 'global_step': 69115, 'preemption_count': 0}), (70933, {'train/ctc_loss': Array(0.06823383, dtype=float32), 'train/wer': 0.026325386571083232, 'validation/ctc_loss': Array(0.31399453, dtype=float32), 'validation/wer': 0.08867798835648841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17032003, dtype=float32), 'test/wer': 0.05425222919586456, 'test/num_examples': 2472, 'score': 56210.31742787361, 'total_duration': 61491.18255186081, 'accumulated_submission_time': 56210.31742787361, 'accumulated_eval_time': 5275.688220500946, 'accumulated_logging_time': 2.1689844131469727, 'global_step': 70933, 'preemption_count': 0}), (72746, {'train/ctc_loss': Array(0.07446662, dtype=float32), 'train/wer': 0.027438337938798487, 'validation/ctc_loss': Array(0.31198248, dtype=float32), 'validation/wer': 0.08779941492802457, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16882586, dtype=float32), 'test/wer': 0.05437409867365385, 'test/num_examples': 2472, 'score': 57650.18681359291, 'total_duration': 63064.41569805145, 'accumulated_submission_time': 57650.18681359291, 'accumulated_eval_time': 5408.91233420372, 'accumulated_logging_time': 2.2319982051849365, 'global_step': 72746, 'preemption_count': 0}), (74586, {'train/ctc_loss': Array(0.07089892, dtype=float32), 'train/wer': 0.026582449321132615, 'validation/ctc_loss': Array(0.3090179, dtype=float32), 'validation/wer': 0.08693049615262076, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16789155, dtype=float32), 'test/wer': 0.05343976601060264, 'test/num_examples': 2472, 'score': 59090.40604901314, 'total_duration': 64635.95995926857, 'accumulated_submission_time': 59090.40604901314, 'accumulated_eval_time': 5540.094013929367, 'accumulated_logging_time': 2.298288345336914, 'global_step': 74586, 'preemption_count': 0}), (76400, {'train/ctc_loss': Array(0.0723265, dtype=float32), 'train/wer': 0.02635920509936258, 'validation/ctc_loss': Array(0.3084427, dtype=float32), 'validation/wer': 0.08698842407098101, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.16666134, dtype=float32), 'test/wer': 0.0529116649401824, 'test/num_examples': 2472, 'score': 60530.30942058563, 'total_duration': 66207.60625338554, 'accumulated_submission_time': 60530.30942058563, 'accumulated_eval_time': 5671.698009729385, 'accumulated_logging_time': 2.359781265258789, 'global_step': 76400, 'preemption_count': 0})], 'global_step': 77088}
I0217 23:39:49.112493 140599226058560 submission_runner.py:586] Timing: 61068.57802128792
I0217 23:39:49.112573 140599226058560 submission_runner.py:588] Total number of evals: 43
I0217 23:39:49.112631 140599226058560 submission_runner.py:589] ====================
I0217 23:39:49.165740 140599226058560 submission_runner.py:673] Final librispeech_conformer score: 61068.13390135765
