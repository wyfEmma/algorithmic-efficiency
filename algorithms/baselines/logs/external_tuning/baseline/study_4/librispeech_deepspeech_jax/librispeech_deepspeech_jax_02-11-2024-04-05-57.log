python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_4 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=2216178884 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_02-11-2024-04-05-57.log
I0211 04:06:18.001473 140437690197824 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax.
I0211 04:06:19.066691 140437690197824 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0211 04:06:19.067338 140437690197824 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0211 04:06:19.067474 140437690197824 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0211 04:06:19.068558 140437690197824 submission_runner.py:542] Using RNG seed 2216178884
I0211 04:06:25.004731 140437690197824 submission_runner.py:551] --- Tuning run 1/5 ---
I0211 04:06:25.004970 140437690197824 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_1.
I0211 04:06:25.005141 140437690197824 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_1/hparams.json.
I0211 04:06:25.188274 140437690197824 submission_runner.py:206] Initializing dataset.
I0211 04:06:25.188498 140437690197824 submission_runner.py:213] Initializing model.
I0211 04:06:27.659099 140437690197824 submission_runner.py:255] Initializing optimizer.
I0211 04:06:28.347617 140437690197824 submission_runner.py:262] Initializing metrics bundle.
I0211 04:06:28.347799 140437690197824 submission_runner.py:280] Initializing checkpoint and logger.
I0211 04:06:28.348853 140437690197824 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0211 04:06:28.349000 140437690197824 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0211 04:06:28.349191 140437690197824 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 04:06:28.349250 140437690197824 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 04:06:28.757297 140437690197824 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 04:06:29.133424 140437690197824 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0211 04:06:29.147250 140437690197824 submission_runner.py:314] Starting training loop.
I0211 04:06:29.438349 140437690197824 input_pipeline.py:20] Loading split = train-clean-100
I0211 04:06:29.505388 140437690197824 input_pipeline.py:20] Loading split = train-clean-360
I0211 04:06:29.664008 140437690197824 input_pipeline.py:20] Loading split = train-other-500
2024-02-11 04:07:21.668215: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2024-02-11 04:07:23.796852: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 04:07:28.977197 140272736360192 logging_writer.py:48] [0] global_step=0, grad_norm=24.132890701293945, loss=33.28276443481445
I0211 04:07:29.013826 140437690197824 spec.py:321] Evaluating on the training split.
I0211 04:07:29.291976 140437690197824 input_pipeline.py:20] Loading split = train-clean-100
I0211 04:07:29.333136 140437690197824 input_pipeline.py:20] Loading split = train-clean-360
I0211 04:07:29.772316 140437690197824 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0211 04:09:31.427930 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 04:09:31.623983 140437690197824 input_pipeline.py:20] Loading split = dev-clean
I0211 04:09:31.629609 140437690197824 input_pipeline.py:20] Loading split = dev-other
I0211 04:10:46.834535 140437690197824 spec.py:349] Evaluating on the test split.
I0211 04:10:47.033428 140437690197824 input_pipeline.py:20] Loading split = test-clean
I0211 04:11:30.300232 140437690197824 submission_runner.py:408] Time since start: 301.15s, 	Step: 1, 	{'train/ctc_loss': Array(31.83468, dtype=float32), 'train/wer': 3.780360452283281, 'validation/ctc_loss': Array(30.891884, dtype=float32), 'validation/wer': 3.3249370033887833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942284, dtype=float32), 'test/wer': 3.6987589624845123, 'test/num_examples': 2472, 'score': 59.86646819114685, 'total_duration': 301.15058636665344, 'accumulated_submission_time': 59.86646819114685, 'accumulated_eval_time': 241.28403306007385, 'accumulated_logging_time': 0}
I0211 04:11:30.326741 140270704256768 logging_writer.py:48] [1] accumulated_eval_time=241.284033, accumulated_logging_time=0, accumulated_submission_time=59.866468, global_step=1, preemption_count=0, score=59.866468, test/ctc_loss=30.942283630371094, test/num_examples=2472, test/wer=3.698759, total_duration=301.150586, train/ctc_loss=31.834680557250977, train/wer=3.780360, validation/ctc_loss=30.891883850097656, validation/num_examples=5348, validation/wer=3.324937
I0211 04:12:54.607614 140279321650944 logging_writer.py:48] [100] global_step=100, grad_norm=6.973601818084717, loss=10.838057518005371
I0211 04:14:10.706432 140279330043648 logging_writer.py:48] [200] global_step=200, grad_norm=1.4295144081115723, loss=6.339452266693115
I0211 04:15:26.699069 140279321650944 logging_writer.py:48] [300] global_step=300, grad_norm=0.6353589296340942, loss=5.897637844085693
I0211 04:16:42.782417 140279330043648 logging_writer.py:48] [400] global_step=400, grad_norm=0.5205574631690979, loss=5.834236145019531
I0211 04:17:59.038358 140279321650944 logging_writer.py:48] [500] global_step=500, grad_norm=0.3048621118068695, loss=5.804798603057861
I0211 04:19:15.344766 140279330043648 logging_writer.py:48] [600] global_step=600, grad_norm=0.3109830319881439, loss=5.793020248413086
I0211 04:20:31.516294 140279321650944 logging_writer.py:48] [700] global_step=700, grad_norm=0.33709225058555603, loss=5.705035209655762
I0211 04:21:47.840300 140279330043648 logging_writer.py:48] [800] global_step=800, grad_norm=0.45744410157203674, loss=5.566648960113525
I0211 04:23:04.329193 140279321650944 logging_writer.py:48] [900] global_step=900, grad_norm=0.5654468536376953, loss=5.340003967285156
I0211 04:24:20.669125 140279330043648 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.859981894493103, loss=5.0257134437561035
I0211 04:25:39.956189 140280674334464 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.5764256715774536, loss=4.680764198303223
I0211 04:26:56.004892 140280665941760 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9357448816299438, loss=4.339832305908203
I0211 04:28:11.939462 140280674334464 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0200728178024292, loss=4.096503257751465
I0211 04:29:27.986662 140280665941760 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.6621174812316895, loss=3.867077112197876
I0211 04:30:44.021464 140280674334464 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.4015390872955322, loss=3.6621127128601074
I0211 04:31:59.992469 140280665941760 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.4418342113494873, loss=3.4585988521575928
I0211 04:33:19.027752 140280674334464 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.192131280899048, loss=3.2711589336395264
I0211 04:34:42.016544 140280665941760 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.5977002382278442, loss=3.2036244869232178
I0211 04:35:30.765244 140437690197824 spec.py:321] Evaluating on the training split.
I0211 04:36:10.510172 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 04:36:56.131915 140437690197824 spec.py:349] Evaluating on the test split.
I0211 04:37:18.779065 140437690197824 submission_runner.py:408] Time since start: 1849.63s, 	Step: 1861, 	{'train/ctc_loss': Array(6.043958, dtype=float32), 'train/wer': 0.9349704434512163, 'validation/ctc_loss': Array(5.9683733, dtype=float32), 'validation/wer': 0.8930167894416714, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.865311, dtype=float32), 'test/wer': 0.8945016553937399, 'test/num_examples': 2472, 'score': 1500.21915102005, 'total_duration': 1849.6291027069092, 'accumulated_submission_time': 1500.21915102005, 'accumulated_eval_time': 349.2952125072479, 'accumulated_logging_time': 0.04060792922973633}
I0211 04:37:18.805990 140281329694464 logging_writer.py:48] [1861] accumulated_eval_time=349.295213, accumulated_logging_time=0.040608, accumulated_submission_time=1500.219151, global_step=1861, preemption_count=0, score=1500.219151, test/ctc_loss=5.865311145782471, test/num_examples=2472, test/wer=0.894502, total_duration=1849.629103, train/ctc_loss=6.0439581871032715, train/wer=0.934970, validation/ctc_loss=5.9683732986450195, validation/num_examples=5348, validation/wer=0.893017
I0211 04:37:49.056169 140281321301760 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.2438981533050537, loss=3.0609593391418457
I0211 04:39:04.334234 140281329694464 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.6607933044433594, loss=2.898206949234009
I0211 04:40:23.348086 140281329694464 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.824780225753784, loss=2.836940050125122
I0211 04:41:38.611468 140281321301760 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.0146851539611816, loss=2.7630672454833984
I0211 04:42:53.806296 140281329694464 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.716064691543579, loss=2.695382833480835
I0211 04:44:09.179861 140281321301760 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.6143996715545654, loss=2.585888624191284
I0211 04:45:24.353971 140281329694464 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.4099173545837402, loss=2.5147507190704346
I0211 04:46:43.885772 140281321301760 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.648984909057617, loss=2.4714736938476562
I0211 04:48:06.041116 140281329694464 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.9737203121185303, loss=2.4700393676757812
I0211 04:49:27.924930 140281321301760 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.6829254627227783, loss=2.3740172386169434
I0211 04:50:49.962388 140281329694464 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.7775607109069824, loss=2.361030340194702
I0211 04:52:11.543829 140281321301760 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.279010772705078, loss=2.3439745903015137
I0211 04:53:36.296783 140281329694464 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.628521919250488, loss=2.341779947280884
I0211 04:54:51.148680 140281321301760 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.14538311958313, loss=2.2392210960388184
I0211 04:56:05.872288 140281329694464 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.957150459289551, loss=2.1795191764831543
I0211 04:57:20.760984 140281321301760 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.1600289344787598, loss=2.188457489013672
I0211 04:58:35.936507 140281329694464 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.004005193710327, loss=2.135619878768921
I0211 04:59:56.241183 140281321301760 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.689927101135254, loss=2.13796329498291
I0211 05:01:19.025263 140281329694464 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.6782031059265137, loss=2.125854015350342
I0211 05:01:19.031366 140437690197824 spec.py:321] Evaluating on the training split.
I0211 05:02:05.044887 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 05:02:52.232520 140437690197824 spec.py:349] Evaluating on the test split.
I0211 05:03:16.386115 140437690197824 submission_runner.py:408] Time since start: 3407.24s, 	Step: 3701, 	{'train/ctc_loss': Array(3.299461, dtype=float32), 'train/wer': 0.7144394427722878, 'validation/ctc_loss': Array(3.655602, dtype=float32), 'validation/wer': 0.7556600403564497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.1674793, dtype=float32), 'test/wer': 0.685657993622164, 'test/num_examples': 2472, 'score': 2940.3601796627045, 'total_duration': 3407.2356901168823, 'accumulated_submission_time': 2940.3601796627045, 'accumulated_eval_time': 466.6468231678009, 'accumulated_logging_time': 0.07855105400085449}
I0211 05:03:16.416727 140281329694464 logging_writer.py:48] [3701] accumulated_eval_time=466.646823, accumulated_logging_time=0.078551, accumulated_submission_time=2940.360180, global_step=3701, preemption_count=0, score=2940.360180, test/ctc_loss=3.1674792766571045, test/num_examples=2472, test/wer=0.685658, total_duration=3407.235690, train/ctc_loss=3.2994608879089355, train/wer=0.714439, validation/ctc_loss=3.655601978302002, validation/num_examples=5348, validation/wer=0.755660
I0211 05:04:31.535119 140281321301760 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.9817752838134766, loss=2.0990190505981445
I0211 05:05:46.814938 140281329694464 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.9464025497436523, loss=2.05297589302063
I0211 05:07:02.020318 140281321301760 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.283279895782471, loss=2.1370797157287598
I0211 05:08:17.334402 140281329694464 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.9281067848205566, loss=2.0989956855773926
I0211 05:09:36.541231 140281329694464 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.208580493927002, loss=2.0267703533172607
I0211 05:10:51.649507 140281321301760 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.944284915924072, loss=1.9949522018432617
I0211 05:12:06.675939 140281329694464 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.022922992706299, loss=2.0343918800354004
I0211 05:13:21.954779 140281321301760 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.1790664196014404, loss=1.968100666999817
I0211 05:14:38.931126 140281329694464 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.2416388988494873, loss=1.961379051208496
I0211 05:16:00.691508 140281321301760 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.146045446395874, loss=1.992058515548706
I0211 05:17:23.270397 140281329694464 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.0246570110321045, loss=2.022918939590454
I0211 05:18:45.062671 140281321301760 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.3875930309295654, loss=1.9290082454681396
I0211 05:20:07.921946 140281329694464 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.2021915912628174, loss=1.9357326030731201
I0211 05:21:30.937331 140281321301760 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.7202959060668945, loss=2.000232219696045
I0211 05:22:53.133434 140281329694464 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.230227470397949, loss=1.8843419551849365
I0211 05:24:08.426692 140281321301760 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.6086971759796143, loss=1.894846796989441
I0211 05:25:23.472295 140281329694464 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.422835111618042, loss=1.7877144813537598
I0211 05:26:38.606448 140281321301760 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.000411510467529, loss=1.8769193887710571
I0211 05:27:16.813736 140437690197824 spec.py:321] Evaluating on the training split.
I0211 05:28:16.539795 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 05:29:06.818178 140437690197824 spec.py:349] Evaluating on the test split.
I0211 05:29:31.901253 140437690197824 submission_runner.py:408] Time since start: 4982.75s, 	Step: 5552, 	{'train/ctc_loss': Array(0.5416918, dtype=float32), 'train/wer': 0.18918776454712694, 'validation/ctc_loss': Array(0.92900485, dtype=float32), 'validation/wer': 0.2685924481303764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6076796, dtype=float32), 'test/wer': 0.19813945930575022, 'test/num_examples': 2472, 'score': 4380.670278787613, 'total_duration': 4982.7510821819305, 'accumulated_submission_time': 4380.670278787613, 'accumulated_eval_time': 601.7314755916595, 'accumulated_logging_time': 0.12032938003540039}
I0211 05:29:31.927419 140280899614464 logging_writer.py:48] [5552] accumulated_eval_time=601.731476, accumulated_logging_time=0.120329, accumulated_submission_time=4380.670279, global_step=5552, preemption_count=0, score=4380.670279, test/ctc_loss=0.6076796054840088, test/num_examples=2472, test/wer=0.198139, total_duration=4982.751082, train/ctc_loss=0.541691780090332, train/wer=0.189188, validation/ctc_loss=0.9290048480033875, validation/num_examples=5348, validation/wer=0.268592
I0211 05:30:08.672460 140280891221760 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.4610235691070557, loss=1.859694004058838
I0211 05:31:23.615081 140280899614464 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.3910763263702393, loss=1.8747614622116089
I0211 05:32:38.750695 140280891221760 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.128918409347534, loss=1.832738995552063
I0211 05:33:53.956509 140280899614464 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.3198981285095215, loss=1.8548487424850464
I0211 05:35:12.059612 140280891221760 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.946429491043091, loss=1.8275541067123413
I0211 05:36:34.127761 140280899614464 logging_writer.py:48] [6100] global_step=6100, grad_norm=4.359184741973877, loss=1.8152666091918945
I0211 05:37:58.120320 140280899614464 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.8521029949188232, loss=1.793069839477539
I0211 05:39:13.070912 140280891221760 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.3026301860809326, loss=1.7060261964797974
I0211 05:40:28.027123 140280899614464 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.5555264949798584, loss=1.742618203163147
I0211 05:41:43.112888 140280891221760 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.786407232284546, loss=1.7120099067687988
I0211 05:42:58.199488 140280899614464 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.2836098670959473, loss=1.8021819591522217
I0211 05:44:18.080335 140280891221760 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.7911975383758545, loss=1.7755600214004517
I0211 05:45:40.529262 140280899614464 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.3075132369995117, loss=1.8476530313491821
I0211 05:47:03.513208 140280891221760 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.908726215362549, loss=1.7932286262512207
I0211 05:48:25.913886 140280899614464 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.252893447875977, loss=1.765651822090149
I0211 05:49:47.710007 140280891221760 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.8977062702178955, loss=1.7727433443069458
I0211 05:51:09.370202 140280899614464 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.317750453948975, loss=1.7901231050491333
I0211 05:52:28.501435 140280899614464 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.7343239784240723, loss=1.6912213563919067
I0211 05:53:32.590623 140437690197824 spec.py:321] Evaluating on the training split.
I0211 05:54:32.985127 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 05:55:22.759214 140437690197824 spec.py:349] Evaluating on the test split.
I0211 05:55:47.934046 140437690197824 submission_runner.py:408] Time since start: 6558.78s, 	Step: 7387, 	{'train/ctc_loss': Array(0.49491432, dtype=float32), 'train/wer': 0.16986773345620962, 'validation/ctc_loss': Array(0.8487835, dtype=float32), 'validation/wer': 0.2476708149492648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5291419, dtype=float32), 'test/wer': 0.17484207746836472, 'test/num_examples': 2472, 'score': 5821.245131254196, 'total_duration': 6558.784062385559, 'accumulated_submission_time': 5821.245131254196, 'accumulated_eval_time': 737.0722260475159, 'accumulated_logging_time': 0.1613924503326416}
I0211 05:55:47.958574 140281329694464 logging_writer.py:48] [7387] accumulated_eval_time=737.072226, accumulated_logging_time=0.161392, accumulated_submission_time=5821.245131, global_step=7387, preemption_count=0, score=5821.245131, test/ctc_loss=0.529141902923584, test/num_examples=2472, test/wer=0.174842, total_duration=6558.784062, train/ctc_loss=0.49491432309150696, train/wer=0.169868, validation/ctc_loss=0.8487834930419922, validation/num_examples=5348, validation/wer=0.247671
I0211 05:55:58.576742 140281321301760 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.2940099239349365, loss=1.746765375137329
I0211 05:57:13.454162 140281329694464 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.3900551795959473, loss=1.7108550071716309
I0211 05:58:28.401121 140281321301760 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.1690142154693604, loss=1.7463761568069458
I0211 05:59:43.288415 140281329694464 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.655635118484497, loss=1.7607126235961914
I0211 06:00:58.586236 140281321301760 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.0171971321105957, loss=1.790648102760315
I0211 06:02:16.586682 140281329694464 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.0344150066375732, loss=1.7527782917022705
I0211 06:03:38.881141 140281321301760 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.678908586502075, loss=1.675947904586792
I0211 06:05:01.266227 140281329694464 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.641914129257202, loss=1.7845277786254883
I0211 06:06:23.652503 140281321301760 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.0667803287506104, loss=1.6940137147903442
I0211 06:07:45.286945 140281329694464 logging_writer.py:48] [8300] global_step=8300, grad_norm=4.2801713943481445, loss=1.7092612981796265
I0211 06:09:00.433845 140281321301760 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.185337543487549, loss=1.7143588066101074
I0211 06:10:15.604371 140281329694464 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.423313617706299, loss=1.741980791091919
I0211 06:11:30.769405 140281321301760 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.9350290298461914, loss=1.7294878959655762
I0211 06:12:48.414773 140281329694464 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.19012451171875, loss=1.706868290901184
I0211 06:14:11.047330 140281321301760 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.7358522415161133, loss=1.6773779392242432
I0211 06:15:34.624318 140281329694464 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.0560264587402344, loss=1.663108229637146
I0211 06:16:56.882618 140281321301760 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.0043084621429443, loss=1.7347588539123535
I0211 06:18:19.888197 140281329694464 logging_writer.py:48] [9100] global_step=9100, grad_norm=4.957992076873779, loss=1.7019014358520508
I0211 06:19:41.976257 140281321301760 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.8453848361968994, loss=1.712232232093811
I0211 06:19:48.132553 140437690197824 spec.py:321] Evaluating on the training split.
I0211 06:20:44.236631 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 06:21:35.783627 140437690197824 spec.py:349] Evaluating on the test split.
I0211 06:22:01.599430 140437690197824 submission_runner.py:408] Time since start: 8132.45s, 	Step: 9209, 	{'train/ctc_loss': Array(0.42755538, dtype=float32), 'train/wer': 0.14786144512818283, 'validation/ctc_loss': Array(0.773474, dtype=float32), 'validation/wer': 0.22343763576855866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47206378, dtype=float32), 'test/wer': 0.15566794629618344, 'test/num_examples': 2472, 'score': 7261.33238196373, 'total_duration': 8132.448884963989, 'accumulated_submission_time': 7261.33238196373, 'accumulated_eval_time': 870.5358927249908, 'accumulated_logging_time': 0.2001349925994873}
I0211 06:22:01.629011 140280607774464 logging_writer.py:48] [9209] accumulated_eval_time=870.535893, accumulated_logging_time=0.200135, accumulated_submission_time=7261.332382, global_step=9209, preemption_count=0, score=7261.332382, test/ctc_loss=0.4720637798309326, test/num_examples=2472, test/wer=0.155668, total_duration=8132.448885, train/ctc_loss=0.4275553822517395, train/wer=0.147861, validation/ctc_loss=0.7734739780426025, validation/num_examples=5348, validation/wer=0.223438
I0211 06:23:14.346511 140280280094464 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.8430421352386475, loss=1.6607723236083984
I0211 06:24:29.352295 140280271701760 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.6533095836639404, loss=1.7373104095458984
I0211 06:25:44.573874 140280280094464 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.716732978820801, loss=1.644781231880188
I0211 06:26:59.759666 140280271701760 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.561363935470581, loss=1.6722007989883423
I0211 06:28:17.400980 140280280094464 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.0770151615142822, loss=1.6885919570922852
I0211 06:29:40.197959 140280271701760 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.2199440002441406, loss=1.6775602102279663
I0211 06:31:02.240614 140280280094464 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.1616787910461426, loss=1.66678786277771
I0211 06:32:24.250094 140280271701760 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.6621103286743164, loss=1.7439855337142944
I0211 06:33:46.670452 140280280094464 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.4155311584472656, loss=1.6701569557189941
I0211 06:35:08.875926 140280271701760 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.172572612762451, loss=1.6386088132858276
I0211 06:36:35.608118 140280607774464 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.4591619968414307, loss=1.5657713413238525
I0211 06:37:50.564727 140280599381760 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.354982614517212, loss=1.6304386854171753
I0211 06:39:05.538534 140280607774464 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.9872541427612305, loss=1.6178367137908936
I0211 06:40:20.428664 140280599381760 logging_writer.py:48] [10600] global_step=10600, grad_norm=4.5855021476745605, loss=1.6257216930389404
I0211 06:41:35.364406 140280607774464 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.7564709186553955, loss=1.6443685293197632
I0211 06:42:55.437936 140280599381760 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.8489882946014404, loss=1.6188676357269287
I0211 06:44:17.770907 140280607774464 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.4714462757110596, loss=1.6437492370605469
I0211 06:45:40.066295 140280599381760 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.251793146133423, loss=1.6821112632751465
I0211 06:46:01.947623 140437690197824 spec.py:321] Evaluating on the training split.
I0211 06:46:57.497860 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 06:47:48.429567 140437690197824 spec.py:349] Evaluating on the test split.
I0211 06:48:14.463305 140437690197824 submission_runner.py:408] Time since start: 9705.31s, 	Step: 11028, 	{'train/ctc_loss': Array(0.42699566, dtype=float32), 'train/wer': 0.1452166166091405, 'validation/ctc_loss': Array(0.7293994, dtype=float32), 'validation/wer': 0.21420778744315824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4390526, dtype=float32), 'test/wer': 0.1433591290394654, 'test/num_examples': 2472, 'score': 8701.565061092377, 'total_duration': 9705.31336402893, 'accumulated_submission_time': 8701.565061092377, 'accumulated_eval_time': 1003.0489454269409, 'accumulated_logging_time': 0.24153900146484375}
I0211 06:48:14.490575 140281037854464 logging_writer.py:48] [11028] accumulated_eval_time=1003.048945, accumulated_logging_time=0.241539, accumulated_submission_time=8701.565061, global_step=11028, preemption_count=0, score=8701.565061, test/ctc_loss=0.43905261158943176, test/num_examples=2472, test/wer=0.143359, total_duration=9705.313364, train/ctc_loss=0.4269956648349762, train/wer=0.145217, validation/ctc_loss=0.7293993830680847, validation/num_examples=5348, validation/wer=0.214208
I0211 06:49:09.259045 140281029461760 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.014669895172119, loss=1.6571590900421143
I0211 06:50:24.339020 140281037854464 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.01427960395813, loss=1.6270538568496704
I0211 06:51:39.359811 140281029461760 logging_writer.py:48] [11300] global_step=11300, grad_norm=4.174481391906738, loss=1.67440927028656
I0211 06:52:58.122852 140280710174464 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.6764166355133057, loss=1.6499732732772827
I0211 06:54:13.107317 140280701781760 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.2579152584075928, loss=1.6418906450271606
I0211 06:55:28.153222 140280710174464 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.8281970024108887, loss=1.584764838218689
I0211 06:56:43.430415 140280701781760 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.464022397994995, loss=1.6864778995513916
I0211 06:58:01.055659 140280710174464 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.5028140544891357, loss=1.5984103679656982
I0211 06:59:24.862381 140280701781760 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.098822832107544, loss=1.609287977218628
I0211 07:00:47.299943 140280710174464 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.0139479637146, loss=1.5773550271987915
I0211 07:02:10.029806 140280701781760 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.711641550064087, loss=1.584823489189148
I0211 07:03:33.528314 140280710174464 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.970078229904175, loss=1.6000995635986328
I0211 07:04:57.457339 140280701781760 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.6299831867218018, loss=1.5894386768341064
I0211 07:06:20.802968 140281037854464 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.113776922225952, loss=1.6367090940475464
I0211 07:07:35.764176 140281029461760 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.97424578666687, loss=1.6207759380340576
I0211 07:08:50.839771 140281037854464 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.7101640701293945, loss=1.612776279449463
I0211 07:10:06.023082 140281029461760 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.611353635787964, loss=1.6300362348556519
I0211 07:11:21.155596 140281037854464 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.0205225944519043, loss=1.6683628559112549
I0211 07:12:15.016903 140437690197824 spec.py:321] Evaluating on the training split.
I0211 07:13:16.201808 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 07:14:06.839206 140437690197824 spec.py:349] Evaluating on the test split.
I0211 07:14:32.705075 140437690197824 submission_runner.py:408] Time since start: 11283.55s, 	Step: 12867, 	{'train/ctc_loss': Array(0.37451404, dtype=float32), 'train/wer': 0.12834747835705018, 'validation/ctc_loss': Array(0.6910725, dtype=float32), 'validation/wer': 0.20170501173040348, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41152832, dtype=float32), 'test/wer': 0.13553917088131945, 'test/num_examples': 2472, 'score': 10142.004707336426, 'total_duration': 11283.554337739944, 'accumulated_submission_time': 10142.004707336426, 'accumulated_eval_time': 1140.7337267398834, 'accumulated_logging_time': 0.28154683113098145}
I0211 07:14:32.738401 140281037854464 logging_writer.py:48] [12867] accumulated_eval_time=1140.733727, accumulated_logging_time=0.281547, accumulated_submission_time=10142.004707, global_step=12867, preemption_count=0, score=10142.004707, test/ctc_loss=0.4115283191204071, test/num_examples=2472, test/wer=0.135539, total_duration=11283.554338, train/ctc_loss=0.37451404333114624, train/wer=0.128347, validation/ctc_loss=0.6910725235939026, validation/num_examples=5348, validation/wer=0.201705
I0211 07:14:58.287460 140281029461760 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.193887948989868, loss=1.5916692018508911
I0211 07:16:13.590775 140281037854464 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.061750650405884, loss=1.5779073238372803
I0211 07:17:28.635123 140281029461760 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.2679994106292725, loss=1.6167739629745483
I0211 07:18:43.696045 140281037854464 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.341536045074463, loss=1.59891676902771
I0211 07:19:59.850098 140281029461760 logging_writer.py:48] [13300] global_step=13300, grad_norm=5.605898380279541, loss=1.6152913570404053
I0211 07:21:26.036484 140281037854464 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.6400485038757324, loss=1.596776008605957
I0211 07:22:41.180111 140281029461760 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.007507085800171, loss=1.5939021110534668
I0211 07:23:56.236462 140281037854464 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.0234463214874268, loss=1.5918216705322266
I0211 07:25:11.115760 140281029461760 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.9826650619506836, loss=1.5157384872436523
I0211 07:26:25.876769 140281037854464 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.7567007541656494, loss=1.5374776124954224
I0211 07:27:48.143378 140281029461760 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.0666749477386475, loss=1.565615177154541
I0211 07:29:10.883201 140281037854464 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.270960807800293, loss=1.6831268072128296
I0211 07:30:34.010572 140281029461760 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.6852128505706787, loss=1.6152976751327515
I0211 07:31:57.779203 140281037854464 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.111100196838379, loss=1.6368484497070312
I0211 07:33:19.467660 140281029461760 logging_writer.py:48] [14300] global_step=14300, grad_norm=4.518137454986572, loss=1.570584774017334
I0211 07:34:41.347116 140281037854464 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.213972806930542, loss=1.5273301601409912
I0211 07:36:01.825083 140281037854464 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.121537446975708, loss=1.576247215270996
I0211 07:37:16.861606 140281029461760 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.273869514465332, loss=1.5633457899093628
I0211 07:38:32.174380 140281037854464 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.8381905555725098, loss=1.5350475311279297
I0211 07:38:32.778358 140437690197824 spec.py:321] Evaluating on the training split.
I0211 07:39:29.682493 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 07:40:21.262477 140437690197824 spec.py:349] Evaluating on the test split.
I0211 07:40:47.245190 140437690197824 submission_runner.py:408] Time since start: 12858.10s, 	Step: 14702, 	{'train/ctc_loss': Array(0.3195103, dtype=float32), 'train/wer': 0.11217140331261863, 'validation/ctc_loss': Array(0.6546651, dtype=float32), 'validation/wer': 0.19371095899668844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39033836, dtype=float32), 'test/wer': 0.12769890114354193, 'test/num_examples': 2472, 'score': 11581.955075740814, 'total_duration': 12858.095179080963, 'accumulated_submission_time': 11581.955075740814, 'accumulated_eval_time': 1275.1978507041931, 'accumulated_logging_time': 0.32950592041015625}
I0211 07:40:47.273392 140281037854464 logging_writer.py:48] [14702] accumulated_eval_time=1275.197851, accumulated_logging_time=0.329506, accumulated_submission_time=11581.955076, global_step=14702, preemption_count=0, score=11581.955076, test/ctc_loss=0.39033836126327515, test/num_examples=2472, test/wer=0.127699, total_duration=12858.095179, train/ctc_loss=0.3195103108882904, train/wer=0.112171, validation/ctc_loss=0.6546651124954224, validation/num_examples=5348, validation/wer=0.193711
I0211 07:42:01.633534 140281029461760 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.0849993228912354, loss=1.598970890045166
I0211 07:43:16.763890 140281037854464 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.057615280151367, loss=1.5000896453857422
I0211 07:44:31.855830 140281029461760 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.1981682777404785, loss=1.5352911949157715
I0211 07:45:48.083533 140281037854464 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.289189338684082, loss=1.562650442123413
I0211 07:47:10.401883 140281029461760 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.8686842918395996, loss=1.6218512058258057
I0211 07:48:33.352780 140281037854464 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.073192119598389, loss=1.562915325164795
I0211 07:49:56.461318 140281029461760 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.5904507637023926, loss=1.6026407480239868
I0211 07:51:19.528733 140280710174464 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.782431125640869, loss=1.563637614250183
I0211 07:52:34.651958 140280701781760 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.69315767288208, loss=1.5773234367370605
I0211 07:53:49.733321 140280710174464 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.151109218597412, loss=1.5681078433990479
I0211 07:55:04.705584 140280701781760 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.2677907943725586, loss=1.5786621570587158
I0211 07:56:20.880050 140280710174464 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.280291795730591, loss=1.5398160219192505
I0211 07:57:44.376554 140280701781760 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.088852643966675, loss=1.5433835983276367
I0211 07:59:07.475642 140280710174464 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.6277706623077393, loss=1.5109732151031494
I0211 08:00:29.947343 140280701781760 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.9798619747161865, loss=1.4631165266036987
I0211 08:01:52.789315 140280710174464 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.292534112930298, loss=1.5626720190048218
I0211 08:03:16.042159 140280701781760 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.0155417919158936, loss=1.5539848804473877
I0211 08:04:41.462799 140280710174464 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.2888340950012207, loss=1.4650472402572632
I0211 08:04:47.310190 140437690197824 spec.py:321] Evaluating on the training split.
I0211 08:05:47.122556 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 08:06:38.186234 140437690197824 spec.py:349] Evaluating on the test split.
I0211 08:07:03.540656 140437690197824 submission_runner.py:408] Time since start: 14434.39s, 	Step: 16509, 	{'train/ctc_loss': Array(0.3045598, dtype=float32), 'train/wer': 0.10615580228654725, 'validation/ctc_loss': Array(0.63967645, dtype=float32), 'validation/wer': 0.1875995636096817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37663108, dtype=float32), 'test/wer': 0.12349440415981151, 'test/num_examples': 2472, 'score': 13021.907640695572, 'total_duration': 14434.39049911499, 'accumulated_submission_time': 13021.907640695572, 'accumulated_eval_time': 1411.4254655838013, 'accumulated_logging_time': 0.36938047409057617}
I0211 08:07:03.570786 140280121370368 logging_writer.py:48] [16509] accumulated_eval_time=1411.425466, accumulated_logging_time=0.369380, accumulated_submission_time=13021.907641, global_step=16509, preemption_count=0, score=13021.907641, test/ctc_loss=0.37663108110427856, test/num_examples=2472, test/wer=0.123494, total_duration=14434.390499, train/ctc_loss=0.3045597970485687, train/wer=0.106156, validation/ctc_loss=0.6396764516830444, validation/num_examples=5348, validation/wer=0.187600
I0211 08:08:12.594428 140280112977664 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.631861686706543, loss=1.5331414937973022
I0211 08:09:27.583960 140280121370368 logging_writer.py:48] [16700] global_step=16700, grad_norm=3.444715976715088, loss=1.4815711975097656
I0211 08:10:42.631665 140280112977664 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.192265748977661, loss=1.5017261505126953
I0211 08:11:57.780269 140280121370368 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.5465352535247803, loss=1.5431768894195557
I0211 08:13:13.042964 140280112977664 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.0760369300842285, loss=1.5123825073242188
I0211 08:14:29.160857 140280121370368 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.687558650970459, loss=1.5088953971862793
I0211 08:15:51.309548 140280112977664 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.5665595531463623, loss=1.5117390155792236
I0211 08:17:14.795235 140280121370368 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.2775967121124268, loss=1.5805505514144897
I0211 08:18:37.452984 140280112977664 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.3831324577331543, loss=1.5001474618911743
I0211 08:19:59.454024 140280121370368 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.947502374649048, loss=1.5717360973358154
I0211 08:21:18.487642 140279793690368 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.911094903945923, loss=1.491012692451477
I0211 08:22:33.507857 140279785297664 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.871903896331787, loss=1.511971354484558
I0211 08:23:48.304347 140279793690368 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.7241909503936768, loss=1.543664574623108
I0211 08:25:03.481072 140279785297664 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.832263231277466, loss=1.5595896244049072
I0211 08:26:23.547167 140279793690368 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.2196848392486572, loss=1.5042994022369385
I0211 08:27:47.563902 140279785297664 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.3414924144744873, loss=1.4874818325042725
I0211 08:29:10.015338 140279793690368 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.2679450511932373, loss=1.4898275136947632
I0211 08:30:32.805661 140279785297664 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.902087688446045, loss=1.5138875246047974
I0211 08:31:03.750661 140437690197824 spec.py:321] Evaluating on the training split.
I0211 08:31:59.865609 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 08:32:50.336543 140437690197824 spec.py:349] Evaluating on the test split.
I0211 08:33:15.685399 140437690197824 submission_runner.py:408] Time since start: 16006.54s, 	Step: 18338, 	{'train/ctc_loss': Array(0.30234045, dtype=float32), 'train/wer': 0.10668375637512267, 'validation/ctc_loss': Array(0.6237858, dtype=float32), 'validation/wer': 0.18155575079409522, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35803446, dtype=float32), 'test/wer': 0.11723843763329474, 'test/num_examples': 2472, 'score': 14462.00174665451, 'total_duration': 16006.535148620605, 'accumulated_submission_time': 14462.00174665451, 'accumulated_eval_time': 1543.3572795391083, 'accumulated_logging_time': 0.41179323196411133}
I0211 08:33:15.712659 140279496726272 logging_writer.py:48] [18338] accumulated_eval_time=1543.357280, accumulated_logging_time=0.411793, accumulated_submission_time=14462.001747, global_step=18338, preemption_count=0, score=14462.001747, test/ctc_loss=0.3580344617366791, test/num_examples=2472, test/wer=0.117238, total_duration=16006.535149, train/ctc_loss=0.30234044790267944, train/wer=0.106684, validation/ctc_loss=0.6237857937812805, validation/num_examples=5348, validation/wer=0.181556
I0211 08:34:03.083754 140279488333568 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.39939546585083, loss=1.4823689460754395
I0211 08:35:18.191185 140279496726272 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.543790102005005, loss=1.5365726947784424
I0211 08:36:36.860213 140281037854464 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.460911273956299, loss=1.4536850452423096
I0211 08:37:51.846740 140281029461760 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.7809231281280518, loss=1.5212726593017578
I0211 08:39:06.792982 140281037854464 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.065343141555786, loss=1.514319896697998
I0211 08:40:21.556405 140281029461760 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.063896417617798, loss=1.5279525518417358
I0211 08:41:40.127601 140281037854464 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.6115102767944336, loss=1.4446974992752075
I0211 08:43:03.534148 140281029461760 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.1500089168548584, loss=1.5084879398345947
I0211 08:44:26.924178 140281037854464 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.6216297149658203, loss=1.5805323123931885
I0211 08:45:49.901093 140281029461760 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.1500847339630127, loss=1.5378730297088623
I0211 08:47:13.389710 140281037854464 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.950761556625366, loss=1.4298391342163086
I0211 08:48:37.593631 140281029461760 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.8327059745788574, loss=1.4274202585220337
I0211 08:50:02.478744 140279496726272 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.275481939315796, loss=1.5150516033172607
I0211 08:51:17.544870 140279488333568 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.617459774017334, loss=1.511034369468689
I0211 08:52:32.807161 140279496726272 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.1402790546417236, loss=1.5371900796890259
I0211 08:53:47.835345 140279488333568 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.7379953861236572, loss=1.4879977703094482
I0211 08:55:03.555919 140279496726272 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.6624832153320312, loss=1.4982264041900635
I0211 08:56:26.393712 140279488333568 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9972463846206665, loss=1.4727379083633423
I0211 08:57:15.954510 140437690197824 spec.py:321] Evaluating on the training split.
I0211 08:58:13.305861 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 08:59:04.595988 140437690197824 spec.py:349] Evaluating on the test split.
I0211 08:59:30.259738 140437690197824 submission_runner.py:408] Time since start: 17581.11s, 	Step: 20161, 	{'train/ctc_loss': Array(0.3074392, dtype=float32), 'train/wer': 0.10461408286839949, 'validation/ctc_loss': Array(0.59997153, dtype=float32), 'validation/wer': 0.17485542157042586, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3493241, dtype=float32), 'test/wer': 0.11380578067556314, 'test/num_examples': 2472, 'score': 15902.158890008926, 'total_duration': 17581.109792232513, 'accumulated_submission_time': 15902.158890008926, 'accumulated_eval_time': 1677.6598930358887, 'accumulated_logging_time': 0.45024919509887695}
I0211 08:59:30.286761 140280746014464 logging_writer.py:48] [20161] accumulated_eval_time=1677.659893, accumulated_logging_time=0.450249, accumulated_submission_time=15902.158890, global_step=20161, preemption_count=0, score=15902.158890, test/ctc_loss=0.349324107170105, test/num_examples=2472, test/wer=0.113806, total_duration=17581.109792, train/ctc_loss=0.3074392080307007, train/wer=0.104614, validation/ctc_loss=0.5999715328216553, validation/num_examples=5348, validation/wer=0.174855
I0211 09:00:00.594618 140280737621760 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.7835073471069336, loss=1.51711106300354
I0211 09:01:15.922397 140280746014464 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.632230043411255, loss=1.4945859909057617
I0211 09:02:31.146995 140280737621760 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.7389991283416748, loss=1.477968454360962
I0211 09:03:46.460094 140280746014464 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.095271587371826, loss=1.4342323541641235
I0211 09:05:07.731009 140280746014464 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.820706367492676, loss=1.4848060607910156
I0211 09:06:22.752673 140280737621760 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.7696704864501953, loss=1.5009515285491943
I0211 09:07:37.765558 140280746014464 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.386089324951172, loss=1.4544034004211426
I0211 09:08:52.918330 140280737621760 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.4124388694763184, loss=1.471854567527771
I0211 09:10:08.092192 140280746014464 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.268599271774292, loss=1.4946519136428833
I0211 09:11:30.672273 140280737621760 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.0495245456695557, loss=1.4569666385650635
I0211 09:12:53.179354 140280746014464 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.916017770767212, loss=1.5235235691070557
I0211 09:14:17.759061 140280737621760 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.3684308528900146, loss=1.461796760559082
I0211 09:15:40.321466 140280746014464 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.581533670425415, loss=1.4461725950241089
I0211 09:17:03.013479 140280737621760 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.9499967098236084, loss=1.4862775802612305
I0211 09:18:26.236681 140280746014464 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.9064643383026123, loss=1.461930751800537
I0211 09:19:46.931051 140280746014464 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.7869937419891357, loss=1.3612231016159058
I0211 09:21:01.788573 140280737621760 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.843294858932495, loss=1.5362143516540527
I0211 09:22:16.804570 140280746014464 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.3458335399627686, loss=1.4498721361160278
I0211 09:23:30.725353 140437690197824 spec.py:321] Evaluating on the training split.
I0211 09:24:27.293623 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 09:25:18.805206 140437690197824 spec.py:349] Evaluating on the test split.
I0211 09:25:44.666507 140437690197824 submission_runner.py:408] Time since start: 19155.52s, 	Step: 22000, 	{'train/ctc_loss': Array(0.30920258, dtype=float32), 'train/wer': 0.10613343181998047, 'validation/ctc_loss': Array(0.5998876, dtype=float32), 'validation/wer': 0.1756181391621692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3443247, dtype=float32), 'test/wer': 0.112688643795828, 'test/num_examples': 2472, 'score': 17342.512769937515, 'total_duration': 19155.51647043228, 'accumulated_submission_time': 17342.512769937515, 'accumulated_eval_time': 1811.5983428955078, 'accumulated_logging_time': 0.48813891410827637}
I0211 09:25:44.693824 140280899614464 logging_writer.py:48] [22000] accumulated_eval_time=1811.598343, accumulated_logging_time=0.488139, accumulated_submission_time=17342.512770, global_step=22000, preemption_count=0, score=17342.512770, test/ctc_loss=0.3443247079849243, test/num_examples=2472, test/wer=0.112689, total_duration=19155.516470, train/ctc_loss=0.3092025816440582, train/wer=0.106133, validation/ctc_loss=0.5998876094818115, validation/num_examples=5348, validation/wer=0.175618
I0211 09:25:45.582666 140280891221760 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.697188377380371, loss=1.4217860698699951
I0211 09:27:00.663106 140280899614464 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.5121231079101562, loss=1.4303518533706665
I0211 09:28:15.677848 140280891221760 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.1906371116638184, loss=1.4524258375167847
I0211 09:29:30.646531 140280899614464 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.111215353012085, loss=1.4806135892868042
I0211 09:30:47.017547 140280891221760 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.567704916000366, loss=1.4828420877456665
I0211 09:32:10.354068 140280899614464 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.4912824630737305, loss=1.47640061378479
I0211 09:33:33.429280 140280891221760 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.2713401317596436, loss=1.4413849115371704
I0211 09:34:57.410168 140280899614464 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.484558343887329, loss=1.4324908256530762
I0211 09:36:12.385788 140280891221760 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.208841323852539, loss=1.4460424184799194
I0211 09:37:27.412623 140280899614464 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.361454486846924, loss=1.4408464431762695
I0211 09:38:42.526391 140280891221760 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.9969801902770996, loss=1.5032131671905518
I0211 09:40:00.125771 140280899614464 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.195554494857788, loss=1.3881044387817383
I0211 09:41:23.705681 140280891221760 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.9938535690307617, loss=1.459342360496521
I0211 09:42:46.817804 140280899614464 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.7136921882629395, loss=1.4433364868164062
I0211 09:44:09.467133 140280891221760 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.536330223083496, loss=1.3992913961410522
I0211 09:45:31.872022 140280899614464 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.067186117172241, loss=1.4541053771972656
I0211 09:46:54.459130 140280891221760 logging_writer.py:48] [23600] global_step=23600, grad_norm=4.363247394561768, loss=1.4599589109420776
I0211 09:48:20.344518 140280244254464 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.251288414001465, loss=1.4065672159194946
I0211 09:49:35.127115 140280235861760 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.5997817516326904, loss=1.3878895044326782
I0211 09:49:44.687092 140437690197824 spec.py:321] Evaluating on the training split.
I0211 09:50:42.032143 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 09:51:33.384312 140437690197824 spec.py:349] Evaluating on the test split.
I0211 09:51:59.308929 140437690197824 submission_runner.py:408] Time since start: 20730.16s, 	Step: 23814, 	{'train/ctc_loss': Array(0.28427473, dtype=float32), 'train/wer': 0.09833316184792674, 'validation/ctc_loss': Array(0.5757108, dtype=float32), 'validation/wer': 0.16996051246898441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32907978, dtype=float32), 'test/wer': 0.10809822679909817, 'test/num_examples': 2472, 'score': 18782.419633626938, 'total_duration': 20730.158828258514, 'accumulated_submission_time': 18782.419633626938, 'accumulated_eval_time': 1946.2173926830292, 'accumulated_logging_time': 0.5289163589477539}
I0211 09:51:59.336599 140281329694464 logging_writer.py:48] [23814] accumulated_eval_time=1946.217393, accumulated_logging_time=0.528916, accumulated_submission_time=18782.419634, global_step=23814, preemption_count=0, score=18782.419634, test/ctc_loss=0.3290797770023346, test/num_examples=2472, test/wer=0.108098, total_duration=20730.158828, train/ctc_loss=0.28427472710609436, train/wer=0.098333, validation/ctc_loss=0.5757107734680176, validation/num_examples=5348, validation/wer=0.169961
I0211 09:53:04.690506 140281321301760 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.397315263748169, loss=1.393927812576294
I0211 09:54:19.551115 140281329694464 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.5650365352630615, loss=1.4119473695755005
I0211 09:55:34.690248 140281321301760 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.766152858734131, loss=1.4196568727493286
I0211 09:56:49.792581 140281329694464 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.979572057723999, loss=1.415245532989502
I0211 09:58:04.964187 140281321301760 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.304741382598877, loss=1.4827243089675903
I0211 09:59:27.968176 140281329694464 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.666792154312134, loss=1.3950077295303345
I0211 10:00:51.571581 140281321301760 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.378309726715088, loss=1.4644732475280762
I0211 10:02:14.635441 140281329694464 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.631317138671875, loss=1.4529916048049927
I0211 10:03:37.901985 140281321301760 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.7142210006713867, loss=1.4184749126434326
I0211 10:04:57.624008 140281329694464 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.537095308303833, loss=1.3691530227661133
I0211 10:06:12.619041 140281321301760 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.9118576049804688, loss=1.3811745643615723
I0211 10:07:27.572837 140281329694464 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.1620709896087646, loss=1.3588944673538208
I0211 10:08:42.442235 140281321301760 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.6170341968536377, loss=1.406084418296814
I0211 10:10:02.880271 140281329694464 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.2212564945220947, loss=1.4707612991333008
I0211 10:11:26.131814 140281321301760 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.685267210006714, loss=1.449353575706482
I0211 10:12:48.774927 140281329694464 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.741863965988159, loss=1.3808636665344238
I0211 10:14:11.698268 140281321301760 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.297722578048706, loss=1.3889186382293701
I0211 10:15:35.301532 140281329694464 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.4045348167419434, loss=1.4733505249023438
I0211 10:15:59.705300 140437690197824 spec.py:321] Evaluating on the training split.
I0211 10:16:56.290866 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 10:17:47.733099 140437690197824 spec.py:349] Evaluating on the test split.
I0211 10:18:13.574693 140437690197824 submission_runner.py:408] Time since start: 22304.42s, 	Step: 25630, 	{'train/ctc_loss': Array(0.2570293, dtype=float32), 'train/wer': 0.08909568852666695, 'validation/ctc_loss': Array(0.5563869, dtype=float32), 'validation/wer': 0.1642739218166195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3139176, dtype=float32), 'test/wer': 0.10314220136900047, 'test/num_examples': 2472, 'score': 20222.704214334488, 'total_duration': 22304.424744606018, 'accumulated_submission_time': 20222.704214334488, 'accumulated_eval_time': 2080.084167957306, 'accumulated_logging_time': 0.5681560039520264}
I0211 10:18:13.601395 140281329694464 logging_writer.py:48] [25630] accumulated_eval_time=2080.084168, accumulated_logging_time=0.568156, accumulated_submission_time=20222.704214, global_step=25630, preemption_count=0, score=20222.704214, test/ctc_loss=0.3139176070690155, test/num_examples=2472, test/wer=0.103142, total_duration=22304.424745, train/ctc_loss=0.25702929496765137, train/wer=0.089096, validation/ctc_loss=0.5563868880271912, validation/num_examples=5348, validation/wer=0.164274
I0211 10:19:06.976979 140281321301760 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.7583465576171875, loss=1.4014689922332764
I0211 10:20:25.666291 140280674334464 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.085373878479004, loss=1.3729112148284912
I0211 10:21:40.694700 140280665941760 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.2599470615386963, loss=1.382652759552002
I0211 10:22:55.827441 140280674334464 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.7967984676361084, loss=1.4573171138763428
I0211 10:24:11.006874 140280665941760 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.473494291305542, loss=1.3749955892562866
I0211 10:25:29.595651 140280674334464 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.225152015686035, loss=1.3682371377944946
I0211 10:26:53.161969 140280665941760 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.514773368835449, loss=1.480674147605896
I0211 10:28:16.105855 140280674334464 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.2768666744232178, loss=1.3776363134384155
I0211 10:29:39.028705 140280665941760 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.5216012001037598, loss=1.377296805381775
I0211 10:31:03.149059 140280674334464 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.170659303665161, loss=1.4136167764663696
I0211 10:32:26.770888 140280665941760 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.9633140563964844, loss=1.3125958442687988
I0211 10:33:52.488168 140281329694464 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.7186532020568848, loss=1.4166425466537476
I0211 10:35:07.718101 140281321301760 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.5366597175598145, loss=1.3493876457214355
I0211 10:36:22.990206 140281329694464 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.5567851066589355, loss=1.3760920763015747
I0211 10:37:38.187659 140281321301760 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.4289207458496094, loss=1.425246000289917
I0211 10:38:53.895847 140281329694464 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.4650895595550537, loss=1.3572261333465576
I0211 10:40:16.940328 140281321301760 logging_writer.py:48] [27300] global_step=27300, grad_norm=4.404293537139893, loss=1.367832064628601
I0211 10:41:40.568531 140281329694464 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.9125266075134277, loss=1.3659981489181519
I0211 10:42:13.952958 140437690197824 spec.py:321] Evaluating on the training split.
I0211 10:43:11.743821 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 10:44:03.398564 140437690197824 spec.py:349] Evaluating on the test split.
I0211 10:44:29.276352 140437690197824 submission_runner.py:408] Time since start: 23880.13s, 	Step: 27442, 	{'train/ctc_loss': Array(0.24057451, dtype=float32), 'train/wer': 0.08563081979185685, 'validation/ctc_loss': Array(0.54482305, dtype=float32), 'validation/wer': 0.16042171524566265, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3053218, dtype=float32), 'test/wer': 0.09928300123900635, 'test/num_examples': 2472, 'score': 21662.970312833786, 'total_duration': 23880.126302480698, 'accumulated_submission_time': 21662.970312833786, 'accumulated_eval_time': 2215.4049229621887, 'accumulated_logging_time': 0.6067309379577637}
I0211 10:44:29.304147 140281329694464 logging_writer.py:48] [27442] accumulated_eval_time=2215.404923, accumulated_logging_time=0.606731, accumulated_submission_time=21662.970313, global_step=27442, preemption_count=0, score=21662.970313, test/ctc_loss=0.3053218126296997, test/num_examples=2472, test/wer=0.099283, total_duration=23880.126302, train/ctc_loss=0.24057450890541077, train/wer=0.085631, validation/ctc_loss=0.5448230504989624, validation/num_examples=5348, validation/wer=0.160422
I0211 10:45:13.642975 140281321301760 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.5345442295074463, loss=1.3304733037948608
I0211 10:46:28.837241 140281329694464 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.5715134143829346, loss=1.4205987453460693
I0211 10:47:43.840802 140281321301760 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.4551820755004883, loss=1.4091683626174927
I0211 10:48:59.143847 140281329694464 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.398158311843872, loss=1.381239891052246
I0211 10:50:18.226514 140281329694464 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.691636085510254, loss=1.4316213130950928
I0211 10:51:33.450944 140281321301760 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.38619327545166, loss=1.3403898477554321
I0211 10:52:48.679609 140281329694464 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.083364009857178, loss=1.3731931447982788
I0211 10:54:03.924109 140281321301760 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.3647048473358154, loss=1.3500014543533325
I0211 10:55:24.457772 140281329694464 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.2612240314483643, loss=1.3182201385498047
I0211 10:56:47.814365 140281321301760 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.764587163925171, loss=1.4014376401901245
I0211 10:58:11.254645 140281329694464 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.998744010925293, loss=1.4071507453918457
I0211 10:59:34.494189 140281321301760 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.5200629234313965, loss=1.3787609338760376
I0211 11:00:57.225297 140281329694464 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.6840577125549316, loss=1.4245073795318604
I0211 11:02:20.718665 140281321301760 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.510300636291504, loss=1.4477297067642212
I0211 11:03:42.436975 140281329694464 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.0541224479675293, loss=1.360832691192627
I0211 11:04:57.460076 140281321301760 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.4347784519195557, loss=1.2655668258666992
I0211 11:06:12.717234 140281329694464 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.1921231746673584, loss=1.329576849937439
I0211 11:07:27.848005 140281321301760 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.9562346935272217, loss=1.406943678855896
I0211 11:08:29.862706 140437690197824 spec.py:321] Evaluating on the training split.
I0211 11:09:27.261160 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 11:10:19.116342 140437690197824 spec.py:349] Evaluating on the test split.
I0211 11:10:45.296195 140437690197824 submission_runner.py:408] Time since start: 25456.15s, 	Step: 29281, 	{'train/ctc_loss': Array(0.22545318, dtype=float32), 'train/wer': 0.07815712225506541, 'validation/ctc_loss': Array(0.5211732, dtype=float32), 'validation/wer': 0.15219595083850662, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29165354, dtype=float32), 'test/wer': 0.09505819267564439, 'test/num_examples': 2472, 'score': 23103.443352222443, 'total_duration': 25456.14620923996, 'accumulated_submission_time': 23103.443352222443, 'accumulated_eval_time': 2350.8357470035553, 'accumulated_logging_time': 0.6452951431274414}
I0211 11:10:45.327005 140280530974464 logging_writer.py:48] [29281] accumulated_eval_time=2350.835747, accumulated_logging_time=0.645295, accumulated_submission_time=23103.443352, global_step=29281, preemption_count=0, score=23103.443352, test/ctc_loss=0.2916535437107086, test/num_examples=2472, test/wer=0.095058, total_duration=25456.146209, train/ctc_loss=0.225453183054924, train/wer=0.078157, validation/ctc_loss=0.5211731791496277, validation/num_examples=5348, validation/wer=0.152196
I0211 11:11:00.426674 140280522581760 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.518406867980957, loss=1.3391003608703613
I0211 11:12:15.560148 140280530974464 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.3596103191375732, loss=1.3749908208847046
I0211 11:13:30.805822 140280522581760 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.0479331016540527, loss=1.4074633121490479
I0211 11:14:45.956134 140280530974464 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.568582773208618, loss=1.3633108139038086
I0211 11:16:03.068227 140280522581760 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.2569580078125, loss=1.2951276302337646
I0211 11:17:26.490032 140280530974464 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.3487014770507812, loss=1.3521671295166016
I0211 11:18:51.290859 140280530974464 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.139573574066162, loss=1.3170884847640991
I0211 11:20:06.663094 140280522581760 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.064466953277588, loss=1.2913074493408203
I0211 11:21:21.809423 140280530974464 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.0406906604766846, loss=1.355772614479065
I0211 11:22:37.120245 140280522581760 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.4211971759796143, loss=1.3968795537948608
I0211 11:23:53.028088 140280530974464 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.645461082458496, loss=1.346994161605835
I0211 11:25:15.469477 140280522581760 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.8198554515838623, loss=1.3222157955169678
I0211 11:26:39.079581 140280530974464 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.084583044052124, loss=1.375671625137329
I0211 11:28:03.001046 140280522581760 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.683607816696167, loss=1.3613146543502808
I0211 11:29:26.062847 140280530974464 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.1492502689361572, loss=1.3510198593139648
I0211 11:30:49.257555 140280522581760 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.112933874130249, loss=1.3444181680679321
I0211 11:32:16.189863 140280530974464 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.011232614517212, loss=1.2918369770050049
I0211 11:33:31.371315 140280522581760 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.619774580001831, loss=1.3647737503051758
I0211 11:34:45.612202 140437690197824 spec.py:321] Evaluating on the training split.
I0211 11:35:41.516148 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 11:36:32.940886 140437690197824 spec.py:349] Evaluating on the test split.
I0211 11:36:59.279394 140437690197824 submission_runner.py:408] Time since start: 27030.13s, 	Step: 31100, 	{'train/ctc_loss': Array(0.23854691, dtype=float32), 'train/wer': 0.08268872421563529, 'validation/ctc_loss': Array(0.50667065, dtype=float32), 'validation/wer': 0.14983056083879626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2836596, dtype=float32), 'test/wer': 0.09276298417727946, 'test/num_examples': 2472, 'score': 24543.64266705513, 'total_duration': 27030.12952065468, 'accumulated_submission_time': 24543.64266705513, 'accumulated_eval_time': 2484.500387430191, 'accumulated_logging_time': 0.6883645057678223}
I0211 11:36:59.310246 140280018970368 logging_writer.py:48] [31100] accumulated_eval_time=2484.500387, accumulated_logging_time=0.688365, accumulated_submission_time=24543.642667, global_step=31100, preemption_count=0, score=24543.642667, test/ctc_loss=0.28365960717201233, test/num_examples=2472, test/wer=0.092763, total_duration=27030.129521, train/ctc_loss=0.23854690790176392, train/wer=0.082689, validation/ctc_loss=0.5066706538200378, validation/num_examples=5348, validation/wer=0.149831
I0211 11:37:00.184572 140280010577664 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.2456204891204834, loss=1.3434251546859741
I0211 11:38:15.289744 140280018970368 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.7499191761016846, loss=1.3246487379074097
I0211 11:39:30.350288 140280010577664 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.3034861087799072, loss=1.2902805805206299
I0211 11:40:45.371126 140280018970368 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.4825661182403564, loss=1.286098599433899
I0211 11:42:00.440371 140280010577664 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.8062584400177, loss=1.2959808111190796
I0211 11:43:20.184221 140280018970368 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.0687804222106934, loss=1.3198463916778564
I0211 11:44:43.528102 140280010577664 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.620534658432007, loss=1.2866945266723633
I0211 11:46:05.918338 140280018970368 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.7695043087005615, loss=1.394079327583313
I0211 11:47:29.374704 140280010577664 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.353790760040283, loss=1.328947901725769
I0211 11:48:50.211038 140280018970368 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.037033796310425, loss=1.3064377307891846
I0211 11:50:05.276706 140280010577664 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.8210926055908203, loss=1.325527548789978
I0211 11:51:20.343118 140280018970368 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.269188165664673, loss=1.301455020904541
I0211 11:52:35.434122 140280010577664 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.188767910003662, loss=1.2732120752334595
I0211 11:53:54.210519 140280018970368 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.8585400581359863, loss=1.3092145919799805
I0211 11:55:17.042068 140280010577664 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.253230333328247, loss=1.2818150520324707
I0211 11:56:40.088699 140280018970368 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.5443222522735596, loss=1.3289333581924438
I0211 11:58:02.758437 140280010577664 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.782395601272583, loss=1.2477178573608398
I0211 11:59:25.545937 140280018970368 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.362277030944824, loss=1.323195457458496
I0211 12:00:48.651104 140280010577664 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.231363534927368, loss=1.3128256797790527
I0211 12:00:59.316575 140437690197824 spec.py:321] Evaluating on the training split.
I0211 12:01:54.877180 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 12:02:46.638931 140437690197824 spec.py:349] Evaluating on the test split.
I0211 12:03:12.430399 140437690197824 submission_runner.py:408] Time since start: 28603.28s, 	Step: 32914, 	{'train/ctc_loss': Array(0.21479145, dtype=float32), 'train/wer': 0.07341331206354891, 'validation/ctc_loss': Array(0.4919465, dtype=float32), 'validation/wer': 0.14370951079872946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2696539, dtype=float32), 'test/wer': 0.08790851664533951, 'test/num_examples': 2472, 'score': 25983.562262773514, 'total_duration': 28603.279937028885, 'accumulated_submission_time': 25983.562262773514, 'accumulated_eval_time': 2617.6110711097717, 'accumulated_logging_time': 0.7320413589477539}
I0211 12:03:12.460939 140280018970368 logging_writer.py:48] [32914] accumulated_eval_time=2617.611071, accumulated_logging_time=0.732041, accumulated_submission_time=25983.562263, global_step=32914, preemption_count=0, score=25983.562263, test/ctc_loss=0.26965388655662537, test/num_examples=2472, test/wer=0.087909, total_duration=28603.279937, train/ctc_loss=0.2147914469242096, train/wer=0.073413, validation/ctc_loss=0.4919464886188507, validation/num_examples=5348, validation/wer=0.143710
I0211 12:04:21.684247 140280018970368 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.008439302444458, loss=1.2763087749481201
I0211 12:05:36.805255 140280010577664 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.160170793533325, loss=1.2758036851882935
I0211 12:06:52.013251 140280018970368 logging_writer.py:48] [33200] global_step=33200, grad_norm=4.2783379554748535, loss=1.2755165100097656
I0211 12:08:07.390170 140280010577664 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.4315028190612793, loss=1.300294280052185
I0211 12:09:26.555096 140280018970368 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.0825307369232178, loss=1.2893730401992798
I0211 12:10:50.992640 140280010577664 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.622922420501709, loss=1.2931853532791138
I0211 12:12:13.949205 140280018970368 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.9173357486724854, loss=1.2734341621398926
I0211 12:13:36.769703 140280010577664 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.448111057281494, loss=1.2715394496917725
I0211 12:15:00.298080 140280018970368 logging_writer.py:48] [33800] global_step=33800, grad_norm=4.279665470123291, loss=1.2397807836532593
I0211 12:16:23.185846 140280010577664 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.68849778175354, loss=1.273543357849121
I0211 12:17:49.978577 140280018970368 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.2319207191467285, loss=1.231905460357666
I0211 12:19:04.758577 140280010577664 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.7995808124542236, loss=1.279415488243103
I0211 12:20:19.894215 140280018970368 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.4755327701568604, loss=1.2227792739868164
I0211 12:21:34.959720 140280010577664 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.0285022258758545, loss=1.257545828819275
I0211 12:22:50.721508 140280018970368 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.760542392730713, loss=1.2795681953430176
I0211 12:24:14.382800 140280010577664 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.368910074234009, loss=1.2867653369903564
I0211 12:25:38.502447 140280018970368 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.4501519203186035, loss=1.2301985025405884
I0211 12:27:01.741338 140280010577664 logging_writer.py:48] [34700] global_step=34700, grad_norm=5.083950519561768, loss=1.2677253484725952
I0211 12:27:13.407222 140437690197824 spec.py:321] Evaluating on the training split.
I0211 12:28:11.236218 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 12:29:02.053844 140437690197824 spec.py:349] Evaluating on the test split.
I0211 12:29:28.275590 140437690197824 submission_runner.py:408] Time since start: 30179.13s, 	Step: 34715, 	{'train/ctc_loss': Array(0.2172395, dtype=float32), 'train/wer': 0.07446486086639373, 'validation/ctc_loss': Array(0.47097364, dtype=float32), 'validation/wer': 0.13800361084024446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25954035, dtype=float32), 'test/wer': 0.08390713545792457, 'test/num_examples': 2472, 'score': 27424.42139530182, 'total_duration': 30179.125629663467, 'accumulated_submission_time': 27424.42139530182, 'accumulated_eval_time': 2752.476796388626, 'accumulated_logging_time': 0.7754151821136475}
I0211 12:29:28.306565 140279722006272 logging_writer.py:48] [34715] accumulated_eval_time=2752.476796, accumulated_logging_time=0.775415, accumulated_submission_time=27424.421395, global_step=34715, preemption_count=0, score=27424.421395, test/ctc_loss=0.2595403492450714, test/num_examples=2472, test/wer=0.083907, total_duration=30179.125630, train/ctc_loss=0.21723949909210205, train/wer=0.074465, validation/ctc_loss=0.4709736406803131, validation/num_examples=5348, validation/wer=0.138004
I0211 12:30:32.811766 140279713613568 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.617288589477539, loss=1.2066781520843506
I0211 12:31:47.827902 140279722006272 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.403742790222168, loss=1.2979947328567505
I0211 12:33:02.879091 140279713613568 logging_writer.py:48] [35000] global_step=35000, grad_norm=4.28790807723999, loss=1.2795778512954712
I0211 12:34:21.869412 140279722006272 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.2091288566589355, loss=1.2846328020095825
I0211 12:35:36.936724 140279713613568 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.384655475616455, loss=1.2653640508651733
I0211 12:36:52.242616 140279722006272 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.9658124446868896, loss=1.20796537399292
I0211 12:38:07.371323 140279713613568 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.5271806716918945, loss=1.2200535535812378
I0211 12:39:27.481100 140279722006272 logging_writer.py:48] [35500] global_step=35500, grad_norm=4.128967761993408, loss=1.2313416004180908
I0211 12:40:51.433103 140279713613568 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.722395658493042, loss=1.2541954517364502
I0211 12:42:15.257820 140279722006272 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.720331907272339, loss=1.1810983419418335
I0211 12:43:38.830768 140279713613568 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.966501474380493, loss=1.246193528175354
I0211 12:45:01.512327 140279722006272 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.2976880073547363, loss=1.2309633493423462
I0211 12:46:25.483751 140279713613568 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.082820177078247, loss=1.268493890762329
I0211 12:47:48.423286 140279066646272 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.382117748260498, loss=1.2121577262878418
I0211 12:49:03.383708 140279058253568 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.137340545654297, loss=1.2487679719924927
I0211 12:50:18.577082 140279066646272 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.3987770080566406, loss=1.215128779411316
I0211 12:51:33.735717 140279058253568 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.960819959640503, loss=1.2024633884429932
I0211 12:52:52.458969 140279066646272 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.55533766746521, loss=1.2146564722061157
I0211 12:53:28.786059 140437690197824 spec.py:321] Evaluating on the training split.
I0211 12:54:25.634357 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 12:55:16.811555 140437690197824 spec.py:349] Evaluating on the test split.
I0211 12:55:42.827243 140437690197824 submission_runner.py:408] Time since start: 31753.68s, 	Step: 36545, 	{'train/ctc_loss': Array(0.20283253, dtype=float32), 'train/wer': 0.0670258000575466, 'validation/ctc_loss': Array(0.4575766, dtype=float32), 'validation/wer': 0.13517479749365208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24819948, dtype=float32), 'test/wer': 0.08043385534092987, 'test/num_examples': 2472, 'score': 28864.815721035004, 'total_duration': 31753.677274227142, 'accumulated_submission_time': 28864.815721035004, 'accumulated_eval_time': 2886.515316724777, 'accumulated_logging_time': 0.8175325393676758}
I0211 12:55:42.857367 140279066646272 logging_writer.py:48] [36545] accumulated_eval_time=2886.515317, accumulated_logging_time=0.817533, accumulated_submission_time=28864.815721, global_step=36545, preemption_count=0, score=28864.815721, test/ctc_loss=0.2481994777917862, test/num_examples=2472, test/wer=0.080434, total_duration=31753.677274, train/ctc_loss=0.20283253490924835, train/wer=0.067026, validation/ctc_loss=0.45757660269737244, validation/num_examples=5348, validation/wer=0.135175
I0211 12:56:25.050577 140279058253568 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.631065845489502, loss=1.2503293752670288
I0211 12:57:40.409698 140279066646272 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.135603189468384, loss=1.2640923261642456
I0211 12:58:55.525946 140279058253568 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.0827131271362305, loss=1.2109038829803467
I0211 13:00:10.852616 140279066646272 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.295086145401001, loss=1.23685622215271
I0211 13:01:33.386627 140279058253568 logging_writer.py:48] [37000] global_step=37000, grad_norm=4.065083980560303, loss=1.2167963981628418
I0211 13:02:59.379463 140279722006272 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.6065731048583984, loss=1.2317067384719849
I0211 13:04:14.452522 140279713613568 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.639380693435669, loss=1.208065390586853
I0211 13:05:29.550727 140279722006272 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.531463861465454, loss=1.1884065866470337
I0211 13:06:44.266093 140279713613568 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.6948039531707764, loss=1.24203622341156
I0211 13:08:00.516386 140279722006272 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.1847221851348877, loss=1.1908973455429077
I0211 13:09:24.087720 140279713613568 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.198134183883667, loss=1.2138080596923828
I0211 13:10:46.700379 140279722006272 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.8428211212158203, loss=1.2561359405517578
I0211 13:12:10.172925 140279713613568 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.7114920616149902, loss=1.2027029991149902
I0211 13:13:33.607812 140279722006272 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.5296144485473633, loss=1.2679320573806763
I0211 13:14:56.378201 140279713613568 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.5724666118621826, loss=1.2213579416275024
I0211 13:16:20.161625 140279722006272 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.8216283321380615, loss=1.188246488571167
I0211 13:17:39.768943 140279722006272 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.4635701179504395, loss=1.1322311162948608
I0211 13:18:54.628682 140279713613568 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.5634162425994873, loss=1.186289668083191
I0211 13:19:43.341042 140437690197824 spec.py:321] Evaluating on the training split.
I0211 13:20:48.264075 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 13:21:39.651941 140437690197824 spec.py:349] Evaluating on the test split.
I0211 13:22:05.296833 140437690197824 submission_runner.py:408] Time since start: 33336.15s, 	Step: 38366, 	{'train/ctc_loss': Array(0.15163435, dtype=float32), 'train/wer': 0.05331939362258233, 'validation/ctc_loss': Array(0.44698212, dtype=float32), 'validation/wer': 0.13183428753487744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2412294, dtype=float32), 'test/wer': 0.07844332053703816, 'test/num_examples': 2472, 'score': 30305.212210178375, 'total_duration': 33336.14661240578, 'accumulated_submission_time': 30305.212210178375, 'accumulated_eval_time': 3028.4682157039642, 'accumulated_logging_time': 0.8610215187072754}
I0211 13:22:05.329838 140279722006272 logging_writer.py:48] [38366] accumulated_eval_time=3028.468216, accumulated_logging_time=0.861022, accumulated_submission_time=30305.212210, global_step=38366, preemption_count=0, score=30305.212210, test/ctc_loss=0.24122940003871918, test/num_examples=2472, test/wer=0.078443, total_duration=33336.146612, train/ctc_loss=0.1516343504190445, train/wer=0.053319, validation/ctc_loss=0.44698211550712585, validation/num_examples=5348, validation/wer=0.131834
I0211 13:22:31.628565 140279713613568 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.5394928455352783, loss=1.1753994226455688
I0211 13:23:46.567076 140279722006272 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.729572296142578, loss=1.1664433479309082
I0211 13:25:01.439646 140279713613568 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.296945095062256, loss=1.1789427995681763
I0211 13:26:16.526495 140279722006272 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.2350080013275146, loss=1.2744022607803345
I0211 13:27:36.457259 140279713613568 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.4696319103240967, loss=1.203377366065979
I0211 13:28:59.823684 140279722006272 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.392524242401123, loss=1.2199463844299316
I0211 13:30:22.661009 140279713613568 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.876518964767456, loss=1.230115532875061
I0211 13:31:45.989567 140279722006272 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.9710075855255127, loss=1.2295985221862793
I0211 13:33:08.038642 140279394326272 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.6521358489990234, loss=1.1860884428024292
I0211 13:34:23.154985 140279385933568 logging_writer.py:48] [39300] global_step=39300, grad_norm=4.4939284324646, loss=1.1449592113494873
I0211 13:35:38.070351 140279394326272 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.8270294666290283, loss=1.1885051727294922
I0211 13:36:53.301689 140279385933568 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.4054980278015137, loss=1.16367506980896
I0211 13:38:12.364286 140279394326272 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.66361403465271, loss=1.177001714706421
I0211 13:39:36.269094 140279385933568 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.936142921447754, loss=1.1790670156478882
I0211 13:40:58.718942 140279394326272 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.8128528594970703, loss=1.1484358310699463
I0211 13:42:22.064306 140279385933568 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.7218260765075684, loss=1.1477843523025513
I0211 13:43:44.691661 140279394326272 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.7979514598846436, loss=1.1253907680511475
I0211 13:45:07.035307 140279385933568 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.846456527709961, loss=1.1910079717636108
I0211 13:46:09.335690 140437690197824 spec.py:321] Evaluating on the training split.
I0211 13:47:06.248586 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 13:47:57.102922 140437690197824 spec.py:349] Evaluating on the test split.
I0211 13:48:23.099182 140437690197824 submission_runner.py:408] Time since start: 34913.95s, 	Step: 40171, 	{'train/ctc_loss': Array(0.16758403, dtype=float32), 'train/wer': 0.057807649257671435, 'validation/ctc_loss': Array(0.43268782, dtype=float32), 'validation/wer': 0.12699730635179624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23414363, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 31749.13193511963, 'total_duration': 34913.94862866402, 'accumulated_submission_time': 31749.13193511963, 'accumulated_eval_time': 3162.2286465168, 'accumulated_logging_time': 0.9062769412994385}
I0211 13:48:23.133637 140280899614464 logging_writer.py:48] [40171] accumulated_eval_time=3162.228647, accumulated_logging_time=0.906277, accumulated_submission_time=31749.131935, global_step=40171, preemption_count=0, score=31749.131935, test/ctc_loss=0.23414362967014313, test/num_examples=2472, test/wer=0.075965, total_duration=34913.948629, train/ctc_loss=0.16758403182029724, train/wer=0.057808, validation/ctc_loss=0.43268781900405884, validation/num_examples=5348, validation/wer=0.126997
I0211 13:48:45.752956 140280891221760 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.4673783779144287, loss=1.0698139667510986
I0211 13:50:01.010717 140280899614464 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.876157760620117, loss=1.161483883857727
I0211 13:51:16.168176 140280891221760 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.5099828243255615, loss=1.1552995443344116
I0211 13:52:31.465391 140280899614464 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.3375184535980225, loss=1.1605700254440308
I0211 13:53:46.705167 140280891221760 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.5069470405578613, loss=1.1710928678512573
I0211 13:55:02.012022 140280899614464 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.4615612030029297, loss=1.1275724172592163
I0211 13:56:19.749224 140280891221760 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.819148540496826, loss=1.160958170890808
I0211 13:57:43.385215 140280899614464 logging_writer.py:48] [40900] global_step=40900, grad_norm=4.053442001342773, loss=1.179797649383545
I0211 13:59:06.920174 140280891221760 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.4291937351226807, loss=1.1762020587921143
I0211 14:00:30.708017 140280899614464 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.004897117614746, loss=1.1893435716629028
I0211 14:01:58.364511 140280899614464 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.738323926925659, loss=1.144882321357727
I0211 14:03:13.547307 140280891221760 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.2068464756011963, loss=1.1238248348236084
I0211 14:04:28.438750 140280899614464 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.177725315093994, loss=1.1414155960083008
I0211 14:05:43.516001 140280891221760 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.516028642654419, loss=1.1896049976348877
I0211 14:06:58.436633 140280899614464 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.7956061363220215, loss=1.1568219661712646
I0211 14:08:19.445104 140280891221760 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.9803144931793213, loss=1.1534565687179565
I0211 14:09:42.590467 140280899614464 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.656245470046997, loss=1.1538623571395874
I0211 14:11:06.450555 140280891221760 logging_writer.py:48] [41900] global_step=41900, grad_norm=4.3160552978515625, loss=1.1321009397506714
I0211 14:12:23.131420 140437690197824 spec.py:321] Evaluating on the training split.
I0211 14:13:23.105909 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 14:14:13.619693 140437690197824 spec.py:349] Evaluating on the test split.
I0211 14:14:39.365790 140437690197824 submission_runner.py:408] Time since start: 36490.22s, 	Step: 41994, 	{'train/ctc_loss': Array(0.20786917, dtype=float32), 'train/wer': 0.07257963908159447, 'validation/ctc_loss': Array(0.4219524, dtype=float32), 'validation/wer': 0.12310648116859921, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22567298, dtype=float32), 'test/wer': 0.0731216866735726, 'test/num_examples': 2472, 'score': 33189.040801525116, 'total_duration': 36490.21577978134, 'accumulated_submission_time': 33189.040801525116, 'accumulated_eval_time': 3298.4603476524353, 'accumulated_logging_time': 0.9550228118896484}
I0211 14:14:39.397329 140281329694464 logging_writer.py:48] [41994] accumulated_eval_time=3298.460348, accumulated_logging_time=0.955023, accumulated_submission_time=33189.040802, global_step=41994, preemption_count=0, score=33189.040802, test/ctc_loss=0.22567297518253326, test/num_examples=2472, test/wer=0.073122, total_duration=36490.215780, train/ctc_loss=0.20786917209625244, train/wer=0.072580, validation/ctc_loss=0.42195239663124084, validation/num_examples=5348, validation/wer=0.123106
I0211 14:14:44.748373 140281321301760 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.715022087097168, loss=1.1827880144119263
I0211 14:15:59.518687 140281329694464 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.22977352142334, loss=1.146307349205017
I0211 14:17:14.507087 140281321301760 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.187299966812134, loss=1.1987740993499756
I0211 14:18:33.789799 140281329694464 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.5698421001434326, loss=1.1234452724456787
I0211 14:19:48.900334 140281321301760 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.6995952129364014, loss=1.1381323337554932
I0211 14:21:04.332077 140281329694464 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.010986089706421, loss=1.1023104190826416
I0211 14:22:19.929471 140281321301760 logging_writer.py:48] [42600] global_step=42600, grad_norm=4.7238593101501465, loss=1.1727232933044434
I0211 14:23:39.267660 140281329694464 logging_writer.py:48] [42700] global_step=42700, grad_norm=4.4900312423706055, loss=1.1361382007598877
I0211 14:25:04.073264 140281321301760 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.868173360824585, loss=1.0950992107391357
I0211 14:26:27.170080 140281329694464 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.243705749511719, loss=1.159517526626587
I0211 14:27:50.647168 140281321301760 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.8019607067108154, loss=1.1211711168289185
I0211 14:29:13.958962 140281329694464 logging_writer.py:48] [43100] global_step=43100, grad_norm=4.14989709854126, loss=1.1189724206924438
I0211 14:30:36.589562 140281321301760 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.4064979553222656, loss=1.1142746210098267
I0211 14:31:59.482682 140280674334464 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.765145778656006, loss=1.1191740036010742
I0211 14:33:14.550500 140280665941760 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.8791565895080566, loss=1.1138681173324585
I0211 14:34:29.441236 140280674334464 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.2424023151397705, loss=1.1859207153320312
I0211 14:35:44.265266 140280665941760 logging_writer.py:48] [43600] global_step=43600, grad_norm=5.010179042816162, loss=1.112827181816101
I0211 14:37:02.887488 140280674334464 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.7573015689849854, loss=1.143314003944397
I0211 14:38:25.278023 140280665941760 logging_writer.py:48] [43800] global_step=43800, grad_norm=4.050187587738037, loss=1.111241102218628
I0211 14:38:40.093277 140437690197824 spec.py:321] Evaluating on the training split.
I0211 14:39:35.027577 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 14:40:26.520051 140437690197824 spec.py:349] Evaluating on the test split.
I0211 14:40:52.742896 140437690197824 submission_runner.py:408] Time since start: 38063.59s, 	Step: 43820, 	{'train/ctc_loss': Array(0.21207216, dtype=float32), 'train/wer': 0.07294448114490637, 'validation/ctc_loss': Array(0.41716936, dtype=float32), 'validation/wer': 0.12211205190341486, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22200587, dtype=float32), 'test/wer': 0.07249202770499462, 'test/num_examples': 2472, 'score': 34629.64849734306, 'total_duration': 38063.593044281006, 'accumulated_submission_time': 34629.64849734306, 'accumulated_eval_time': 3431.107417821884, 'accumulated_logging_time': 1.0002148151397705}
I0211 14:40:52.775160 140280459294464 logging_writer.py:48] [43820] accumulated_eval_time=3431.107418, accumulated_logging_time=1.000215, accumulated_submission_time=34629.648497, global_step=43820, preemption_count=0, score=34629.648497, test/ctc_loss=0.22200587391853333, test/num_examples=2472, test/wer=0.072492, total_duration=38063.593044, train/ctc_loss=0.21207216382026672, train/wer=0.072944, validation/ctc_loss=0.41716936230659485, validation/num_examples=5348, validation/wer=0.122112
I0211 14:41:53.620096 140280450901760 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.4637153148651123, loss=1.146629810333252
I0211 14:43:08.810972 140280459294464 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.7646703720092773, loss=1.1497222185134888
I0211 14:44:23.852679 140280450901760 logging_writer.py:48] [44100] global_step=44100, grad_norm=4.730400085449219, loss=1.1061654090881348
I0211 14:45:39.362427 140280459294464 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.020113945007324, loss=1.1054742336273193
I0211 14:47:04.324354 140280459294464 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.0230352878570557, loss=1.161211371421814
I0211 14:48:19.696160 140280450901760 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.0640971660614014, loss=1.1085073947906494
I0211 14:49:34.940206 140280459294464 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.7673394680023193, loss=1.1186048984527588
I0211 14:50:50.234177 140280450901760 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.3176686763763428, loss=1.1438162326812744
I0211 14:52:05.554506 140280459294464 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.607639789581299, loss=1.1327741146087646
I0211 14:53:26.646100 140280450901760 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.7180726528167725, loss=1.1062220335006714
I0211 14:54:49.847799 140280459294464 logging_writer.py:48] [44900] global_step=44900, grad_norm=5.12550687789917, loss=1.070517897605896
I0211 14:56:13.185776 140280450901760 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.063503265380859, loss=1.167617678642273
I0211 14:57:36.275099 140280459294464 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.4693150520324707, loss=1.1868075132369995
I0211 14:59:00.154114 140280450901760 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.7221357822418213, loss=1.1861916780471802
I0211 15:00:23.293920 140280459294464 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.2949278354644775, loss=1.1288660764694214
I0211 15:01:43.256744 140280459294464 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.946841239929199, loss=1.1384239196777344
I0211 15:02:58.145992 140280450901760 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.6131560802459717, loss=1.0879666805267334
I0211 15:04:13.085990 140280459294464 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.4136321544647217, loss=1.0918121337890625
I0211 15:04:53.295590 140437690197824 spec.py:321] Evaluating on the training split.
I0211 15:05:46.503235 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 15:06:37.715474 140437690197824 spec.py:349] Evaluating on the test split.
I0211 15:07:03.448071 140437690197824 submission_runner.py:408] Time since start: 39634.30s, 	Step: 45655, 	{'train/ctc_loss': Array(0.24418886, dtype=float32), 'train/wer': 0.08487227334148381, 'validation/ctc_loss': Array(0.4126984, dtype=float32), 'validation/wer': 0.12063488998522838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21940964, dtype=float32), 'test/wer': 0.07174049925862734, 'test/num_examples': 2472, 'score': 36070.083124399185, 'total_duration': 39634.29805088043, 'accumulated_submission_time': 36070.083124399185, 'accumulated_eval_time': 3561.2571907043457, 'accumulated_logging_time': 1.044391393661499}
I0211 15:07:03.480761 140279875614464 logging_writer.py:48] [45655] accumulated_eval_time=3561.257191, accumulated_logging_time=1.044391, accumulated_submission_time=36070.083124, global_step=45655, preemption_count=0, score=36070.083124, test/ctc_loss=0.21940964460372925, test/num_examples=2472, test/wer=0.071740, total_duration=39634.298051, train/ctc_loss=0.24418886005878448, train/wer=0.084872, validation/ctc_loss=0.4126983880996704, validation/num_examples=5348, validation/wer=0.120635
I0211 15:07:38.006645 140279867221760 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.719714879989624, loss=1.1132656335830688
I0211 15:08:53.029018 140279875614464 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.794222831726074, loss=1.0570075511932373
I0211 15:10:07.811486 140279867221760 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.4014244079589844, loss=1.0952396392822266
I0211 15:11:23.022342 140279875614464 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.844465970993042, loss=1.1331535577774048
I0211 15:12:47.196351 140279867221760 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.617100238800049, loss=1.1025649309158325
I0211 15:14:12.990559 140279875614464 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.4621529579162598, loss=1.103717565536499
I0211 15:15:37.578727 140279867221760 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.579524517059326, loss=1.0776309967041016
I0211 15:17:00.856385 140279875614464 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.0557687282562256, loss=1.0897787809371948
I0211 15:18:16.189324 140279867221760 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.6729650497436523, loss=1.1427514553070068
I0211 15:19:31.430651 140279875614464 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.105149984359741, loss=1.094122290611267
I0211 15:20:47.546551 140279867221760 logging_writer.py:48] [46700] global_step=46700, grad_norm=4.385889053344727, loss=1.1387840509414673
I0211 15:22:09.519602 140279875614464 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.289881467819214, loss=1.1230210065841675
I0211 15:23:34.171771 140279867221760 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.6825854778289795, loss=1.0654674768447876
I0211 15:24:58.016905 140279875614464 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.179368495941162, loss=1.0853683948516846
I0211 15:26:22.458256 140279867221760 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.8303346633911133, loss=1.1614950895309448
I0211 15:27:45.580098 140279875614464 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.1214168071746826, loss=1.1221646070480347
I0211 15:29:09.071673 140279867221760 logging_writer.py:48] [47300] global_step=47300, grad_norm=4.14286470413208, loss=1.1102482080459595
I0211 15:30:35.188630 140279875614464 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.8519105911254883, loss=1.131096601486206
I0211 15:31:03.479267 140437690197824 spec.py:321] Evaluating on the training split.
I0211 15:32:05.913207 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 15:32:57.310101 140437690197824 spec.py:349] Evaluating on the test split.
I0211 15:33:23.383296 140437690197824 submission_runner.py:408] Time since start: 41214.23s, 	Step: 47439, 	{'train/ctc_loss': Array(0.22175954, dtype=float32), 'train/wer': 0.0753316180842984, 'validation/ctc_loss': Array(0.41188765, dtype=float32), 'validation/wer': 0.12020043059752648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21931432, dtype=float32), 'test/wer': 0.07153738346231187, 'test/num_examples': 2472, 'score': 37509.996856451035, 'total_duration': 41214.233224630356, 'accumulated_submission_time': 37509.996856451035, 'accumulated_eval_time': 3701.1584811210632, 'accumulated_logging_time': 1.0890610218048096}
I0211 15:33:23.416292 140279875614464 logging_writer.py:48] [47439] accumulated_eval_time=3701.158481, accumulated_logging_time=1.089061, accumulated_submission_time=37509.996856, global_step=47439, preemption_count=0, score=37509.996856, test/ctc_loss=0.2193143218755722, test/num_examples=2472, test/wer=0.071537, total_duration=41214.233225, train/ctc_loss=0.22175954282283783, train/wer=0.075332, validation/ctc_loss=0.41188764572143555, validation/num_examples=5348, validation/wer=0.120200
I0211 15:34:10.086572 140279867221760 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.184367656707764, loss=1.1115756034851074
I0211 15:35:25.184561 140279875614464 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.8112170696258545, loss=1.1058461666107178
I0211 15:36:40.288204 140279867221760 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.5444514751434326, loss=1.0804868936538696
I0211 15:37:55.335304 140279875614464 logging_writer.py:48] [47800] global_step=47800, grad_norm=6.330637454986572, loss=1.055716633796692
I0211 15:39:10.523761 140279867221760 logging_writer.py:48] [47900] global_step=47900, grad_norm=4.960631370544434, loss=1.118259310722351
I0211 15:40:29.256909 140437690197824 spec.py:321] Evaluating on the training split.
I0211 15:41:34.255018 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 15:42:26.783507 140437690197824 spec.py:349] Evaluating on the test split.
I0211 15:42:53.110596 140437690197824 submission_runner.py:408] Time since start: 41783.96s, 	Step: 48000, 	{'train/ctc_loss': Array(0.20823953, dtype=float32), 'train/wer': 0.07322993330586886, 'validation/ctc_loss': Array(0.41214904, dtype=float32), 'validation/wer': 0.12032594108730703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21926215, dtype=float32), 'test/wer': 0.07163894136046961, 'test/num_examples': 2472, 'score': 37935.802810907364, 'total_duration': 41783.96035575867, 'accumulated_submission_time': 37935.802810907364, 'accumulated_eval_time': 3845.009263277054, 'accumulated_logging_time': 1.1332612037658691}
I0211 15:42:53.146412 140281329694464 logging_writer.py:48] [48000] accumulated_eval_time=3845.009263, accumulated_logging_time=1.133261, accumulated_submission_time=37935.802811, global_step=48000, preemption_count=0, score=37935.802811, test/ctc_loss=0.21926215291023254, test/num_examples=2472, test/wer=0.071639, total_duration=41783.960356, train/ctc_loss=0.20823952555656433, train/wer=0.073230, validation/ctc_loss=0.412149041891098, validation/num_examples=5348, validation/wer=0.120326
I0211 15:42:53.171944 140281321301760 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=37935.802811
I0211 15:42:53.393839 140437690197824 checkpoints.py:490] Saving checkpoint at step: 48000
I0211 15:42:54.337061 140437690197824 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_1/checkpoint_48000
I0211 15:42:54.358540 140437690197824 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_1/checkpoint_48000.
I0211 15:42:55.851548 140437690197824 submission_runner.py:583] Tuning trial 1/5
I0211 15:42:55.851816 140437690197824 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0211 15:42:55.864137 140437690197824 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.83468, dtype=float32), 'train/wer': 3.780360452283281, 'validation/ctc_loss': Array(30.891884, dtype=float32), 'validation/wer': 3.3249370033887833, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942284, dtype=float32), 'test/wer': 3.6987589624845123, 'test/num_examples': 2472, 'score': 59.86646819114685, 'total_duration': 301.15058636665344, 'accumulated_submission_time': 59.86646819114685, 'accumulated_eval_time': 241.28403306007385, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1861, {'train/ctc_loss': Array(6.043958, dtype=float32), 'train/wer': 0.9349704434512163, 'validation/ctc_loss': Array(5.9683733, dtype=float32), 'validation/wer': 0.8930167894416714, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.865311, dtype=float32), 'test/wer': 0.8945016553937399, 'test/num_examples': 2472, 'score': 1500.21915102005, 'total_duration': 1849.6291027069092, 'accumulated_submission_time': 1500.21915102005, 'accumulated_eval_time': 349.2952125072479, 'accumulated_logging_time': 0.04060792922973633, 'global_step': 1861, 'preemption_count': 0}), (3701, {'train/ctc_loss': Array(3.299461, dtype=float32), 'train/wer': 0.7144394427722878, 'validation/ctc_loss': Array(3.655602, dtype=float32), 'validation/wer': 0.7556600403564497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.1674793, dtype=float32), 'test/wer': 0.685657993622164, 'test/num_examples': 2472, 'score': 2940.3601796627045, 'total_duration': 3407.2356901168823, 'accumulated_submission_time': 2940.3601796627045, 'accumulated_eval_time': 466.6468231678009, 'accumulated_logging_time': 0.07855105400085449, 'global_step': 3701, 'preemption_count': 0}), (5552, {'train/ctc_loss': Array(0.5416918, dtype=float32), 'train/wer': 0.18918776454712694, 'validation/ctc_loss': Array(0.92900485, dtype=float32), 'validation/wer': 0.2685924481303764, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6076796, dtype=float32), 'test/wer': 0.19813945930575022, 'test/num_examples': 2472, 'score': 4380.670278787613, 'total_duration': 4982.7510821819305, 'accumulated_submission_time': 4380.670278787613, 'accumulated_eval_time': 601.7314755916595, 'accumulated_logging_time': 0.12032938003540039, 'global_step': 5552, 'preemption_count': 0}), (7387, {'train/ctc_loss': Array(0.49491432, dtype=float32), 'train/wer': 0.16986773345620962, 'validation/ctc_loss': Array(0.8487835, dtype=float32), 'validation/wer': 0.2476708149492648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5291419, dtype=float32), 'test/wer': 0.17484207746836472, 'test/num_examples': 2472, 'score': 5821.245131254196, 'total_duration': 6558.784062385559, 'accumulated_submission_time': 5821.245131254196, 'accumulated_eval_time': 737.0722260475159, 'accumulated_logging_time': 0.1613924503326416, 'global_step': 7387, 'preemption_count': 0}), (9209, {'train/ctc_loss': Array(0.42755538, dtype=float32), 'train/wer': 0.14786144512818283, 'validation/ctc_loss': Array(0.773474, dtype=float32), 'validation/wer': 0.22343763576855866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47206378, dtype=float32), 'test/wer': 0.15566794629618344, 'test/num_examples': 2472, 'score': 7261.33238196373, 'total_duration': 8132.448884963989, 'accumulated_submission_time': 7261.33238196373, 'accumulated_eval_time': 870.5358927249908, 'accumulated_logging_time': 0.2001349925994873, 'global_step': 9209, 'preemption_count': 0}), (11028, {'train/ctc_loss': Array(0.42699566, dtype=float32), 'train/wer': 0.1452166166091405, 'validation/ctc_loss': Array(0.7293994, dtype=float32), 'validation/wer': 0.21420778744315824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4390526, dtype=float32), 'test/wer': 0.1433591290394654, 'test/num_examples': 2472, 'score': 8701.565061092377, 'total_duration': 9705.31336402893, 'accumulated_submission_time': 8701.565061092377, 'accumulated_eval_time': 1003.0489454269409, 'accumulated_logging_time': 0.24153900146484375, 'global_step': 11028, 'preemption_count': 0}), (12867, {'train/ctc_loss': Array(0.37451404, dtype=float32), 'train/wer': 0.12834747835705018, 'validation/ctc_loss': Array(0.6910725, dtype=float32), 'validation/wer': 0.20170501173040348, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41152832, dtype=float32), 'test/wer': 0.13553917088131945, 'test/num_examples': 2472, 'score': 10142.004707336426, 'total_duration': 11283.554337739944, 'accumulated_submission_time': 10142.004707336426, 'accumulated_eval_time': 1140.7337267398834, 'accumulated_logging_time': 0.28154683113098145, 'global_step': 12867, 'preemption_count': 0}), (14702, {'train/ctc_loss': Array(0.3195103, dtype=float32), 'train/wer': 0.11217140331261863, 'validation/ctc_loss': Array(0.6546651, dtype=float32), 'validation/wer': 0.19371095899668844, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39033836, dtype=float32), 'test/wer': 0.12769890114354193, 'test/num_examples': 2472, 'score': 11581.955075740814, 'total_duration': 12858.095179080963, 'accumulated_submission_time': 11581.955075740814, 'accumulated_eval_time': 1275.1978507041931, 'accumulated_logging_time': 0.32950592041015625, 'global_step': 14702, 'preemption_count': 0}), (16509, {'train/ctc_loss': Array(0.3045598, dtype=float32), 'train/wer': 0.10615580228654725, 'validation/ctc_loss': Array(0.63967645, dtype=float32), 'validation/wer': 0.1875995636096817, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37663108, dtype=float32), 'test/wer': 0.12349440415981151, 'test/num_examples': 2472, 'score': 13021.907640695572, 'total_duration': 14434.39049911499, 'accumulated_submission_time': 13021.907640695572, 'accumulated_eval_time': 1411.4254655838013, 'accumulated_logging_time': 0.36938047409057617, 'global_step': 16509, 'preemption_count': 0}), (18338, {'train/ctc_loss': Array(0.30234045, dtype=float32), 'train/wer': 0.10668375637512267, 'validation/ctc_loss': Array(0.6237858, dtype=float32), 'validation/wer': 0.18155575079409522, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35803446, dtype=float32), 'test/wer': 0.11723843763329474, 'test/num_examples': 2472, 'score': 14462.00174665451, 'total_duration': 16006.535148620605, 'accumulated_submission_time': 14462.00174665451, 'accumulated_eval_time': 1543.3572795391083, 'accumulated_logging_time': 0.41179323196411133, 'global_step': 18338, 'preemption_count': 0}), (20161, {'train/ctc_loss': Array(0.3074392, dtype=float32), 'train/wer': 0.10461408286839949, 'validation/ctc_loss': Array(0.59997153, dtype=float32), 'validation/wer': 0.17485542157042586, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3493241, dtype=float32), 'test/wer': 0.11380578067556314, 'test/num_examples': 2472, 'score': 15902.158890008926, 'total_duration': 17581.109792232513, 'accumulated_submission_time': 15902.158890008926, 'accumulated_eval_time': 1677.6598930358887, 'accumulated_logging_time': 0.45024919509887695, 'global_step': 20161, 'preemption_count': 0}), (22000, {'train/ctc_loss': Array(0.30920258, dtype=float32), 'train/wer': 0.10613343181998047, 'validation/ctc_loss': Array(0.5998876, dtype=float32), 'validation/wer': 0.1756181391621692, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3443247, dtype=float32), 'test/wer': 0.112688643795828, 'test/num_examples': 2472, 'score': 17342.512769937515, 'total_duration': 19155.51647043228, 'accumulated_submission_time': 17342.512769937515, 'accumulated_eval_time': 1811.5983428955078, 'accumulated_logging_time': 0.48813891410827637, 'global_step': 22000, 'preemption_count': 0}), (23814, {'train/ctc_loss': Array(0.28427473, dtype=float32), 'train/wer': 0.09833316184792674, 'validation/ctc_loss': Array(0.5757108, dtype=float32), 'validation/wer': 0.16996051246898441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32907978, dtype=float32), 'test/wer': 0.10809822679909817, 'test/num_examples': 2472, 'score': 18782.419633626938, 'total_duration': 20730.158828258514, 'accumulated_submission_time': 18782.419633626938, 'accumulated_eval_time': 1946.2173926830292, 'accumulated_logging_time': 0.5289163589477539, 'global_step': 23814, 'preemption_count': 0}), (25630, {'train/ctc_loss': Array(0.2570293, dtype=float32), 'train/wer': 0.08909568852666695, 'validation/ctc_loss': Array(0.5563869, dtype=float32), 'validation/wer': 0.1642739218166195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3139176, dtype=float32), 'test/wer': 0.10314220136900047, 'test/num_examples': 2472, 'score': 20222.704214334488, 'total_duration': 22304.424744606018, 'accumulated_submission_time': 20222.704214334488, 'accumulated_eval_time': 2080.084167957306, 'accumulated_logging_time': 0.5681560039520264, 'global_step': 25630, 'preemption_count': 0}), (27442, {'train/ctc_loss': Array(0.24057451, dtype=float32), 'train/wer': 0.08563081979185685, 'validation/ctc_loss': Array(0.54482305, dtype=float32), 'validation/wer': 0.16042171524566265, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3053218, dtype=float32), 'test/wer': 0.09928300123900635, 'test/num_examples': 2472, 'score': 21662.970312833786, 'total_duration': 23880.126302480698, 'accumulated_submission_time': 21662.970312833786, 'accumulated_eval_time': 2215.4049229621887, 'accumulated_logging_time': 0.6067309379577637, 'global_step': 27442, 'preemption_count': 0}), (29281, {'train/ctc_loss': Array(0.22545318, dtype=float32), 'train/wer': 0.07815712225506541, 'validation/ctc_loss': Array(0.5211732, dtype=float32), 'validation/wer': 0.15219595083850662, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29165354, dtype=float32), 'test/wer': 0.09505819267564439, 'test/num_examples': 2472, 'score': 23103.443352222443, 'total_duration': 25456.14620923996, 'accumulated_submission_time': 23103.443352222443, 'accumulated_eval_time': 2350.8357470035553, 'accumulated_logging_time': 0.6452951431274414, 'global_step': 29281, 'preemption_count': 0}), (31100, {'train/ctc_loss': Array(0.23854691, dtype=float32), 'train/wer': 0.08268872421563529, 'validation/ctc_loss': Array(0.50667065, dtype=float32), 'validation/wer': 0.14983056083879626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2836596, dtype=float32), 'test/wer': 0.09276298417727946, 'test/num_examples': 2472, 'score': 24543.64266705513, 'total_duration': 27030.12952065468, 'accumulated_submission_time': 24543.64266705513, 'accumulated_eval_time': 2484.500387430191, 'accumulated_logging_time': 0.6883645057678223, 'global_step': 31100, 'preemption_count': 0}), (32914, {'train/ctc_loss': Array(0.21479145, dtype=float32), 'train/wer': 0.07341331206354891, 'validation/ctc_loss': Array(0.4919465, dtype=float32), 'validation/wer': 0.14370951079872946, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2696539, dtype=float32), 'test/wer': 0.08790851664533951, 'test/num_examples': 2472, 'score': 25983.562262773514, 'total_duration': 28603.279937028885, 'accumulated_submission_time': 25983.562262773514, 'accumulated_eval_time': 2617.6110711097717, 'accumulated_logging_time': 0.7320413589477539, 'global_step': 32914, 'preemption_count': 0}), (34715, {'train/ctc_loss': Array(0.2172395, dtype=float32), 'train/wer': 0.07446486086639373, 'validation/ctc_loss': Array(0.47097364, dtype=float32), 'validation/wer': 0.13800361084024446, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25954035, dtype=float32), 'test/wer': 0.08390713545792457, 'test/num_examples': 2472, 'score': 27424.42139530182, 'total_duration': 30179.125629663467, 'accumulated_submission_time': 27424.42139530182, 'accumulated_eval_time': 2752.476796388626, 'accumulated_logging_time': 0.7754151821136475, 'global_step': 34715, 'preemption_count': 0}), (36545, {'train/ctc_loss': Array(0.20283253, dtype=float32), 'train/wer': 0.0670258000575466, 'validation/ctc_loss': Array(0.4575766, dtype=float32), 'validation/wer': 0.13517479749365208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24819948, dtype=float32), 'test/wer': 0.08043385534092987, 'test/num_examples': 2472, 'score': 28864.815721035004, 'total_duration': 31753.677274227142, 'accumulated_submission_time': 28864.815721035004, 'accumulated_eval_time': 2886.515316724777, 'accumulated_logging_time': 0.8175325393676758, 'global_step': 36545, 'preemption_count': 0}), (38366, {'train/ctc_loss': Array(0.15163435, dtype=float32), 'train/wer': 0.05331939362258233, 'validation/ctc_loss': Array(0.44698212, dtype=float32), 'validation/wer': 0.13183428753487744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2412294, dtype=float32), 'test/wer': 0.07844332053703816, 'test/num_examples': 2472, 'score': 30305.212210178375, 'total_duration': 33336.14661240578, 'accumulated_submission_time': 30305.212210178375, 'accumulated_eval_time': 3028.4682157039642, 'accumulated_logging_time': 0.8610215187072754, 'global_step': 38366, 'preemption_count': 0}), (40171, {'train/ctc_loss': Array(0.16758403, dtype=float32), 'train/wer': 0.057807649257671435, 'validation/ctc_loss': Array(0.43268782, dtype=float32), 'validation/wer': 0.12699730635179624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23414363, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 31749.13193511963, 'total_duration': 34913.94862866402, 'accumulated_submission_time': 31749.13193511963, 'accumulated_eval_time': 3162.2286465168, 'accumulated_logging_time': 0.9062769412994385, 'global_step': 40171, 'preemption_count': 0}), (41994, {'train/ctc_loss': Array(0.20786917, dtype=float32), 'train/wer': 0.07257963908159447, 'validation/ctc_loss': Array(0.4219524, dtype=float32), 'validation/wer': 0.12310648116859921, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22567298, dtype=float32), 'test/wer': 0.0731216866735726, 'test/num_examples': 2472, 'score': 33189.040801525116, 'total_duration': 36490.21577978134, 'accumulated_submission_time': 33189.040801525116, 'accumulated_eval_time': 3298.4603476524353, 'accumulated_logging_time': 0.9550228118896484, 'global_step': 41994, 'preemption_count': 0}), (43820, {'train/ctc_loss': Array(0.21207216, dtype=float32), 'train/wer': 0.07294448114490637, 'validation/ctc_loss': Array(0.41716936, dtype=float32), 'validation/wer': 0.12211205190341486, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22200587, dtype=float32), 'test/wer': 0.07249202770499462, 'test/num_examples': 2472, 'score': 34629.64849734306, 'total_duration': 38063.593044281006, 'accumulated_submission_time': 34629.64849734306, 'accumulated_eval_time': 3431.107417821884, 'accumulated_logging_time': 1.0002148151397705, 'global_step': 43820, 'preemption_count': 0}), (45655, {'train/ctc_loss': Array(0.24418886, dtype=float32), 'train/wer': 0.08487227334148381, 'validation/ctc_loss': Array(0.4126984, dtype=float32), 'validation/wer': 0.12063488998522838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21940964, dtype=float32), 'test/wer': 0.07174049925862734, 'test/num_examples': 2472, 'score': 36070.083124399185, 'total_duration': 39634.29805088043, 'accumulated_submission_time': 36070.083124399185, 'accumulated_eval_time': 3561.2571907043457, 'accumulated_logging_time': 1.044391393661499, 'global_step': 45655, 'preemption_count': 0}), (47439, {'train/ctc_loss': Array(0.22175954, dtype=float32), 'train/wer': 0.0753316180842984, 'validation/ctc_loss': Array(0.41188765, dtype=float32), 'validation/wer': 0.12020043059752648, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21931432, dtype=float32), 'test/wer': 0.07153738346231187, 'test/num_examples': 2472, 'score': 37509.996856451035, 'total_duration': 41214.233224630356, 'accumulated_submission_time': 37509.996856451035, 'accumulated_eval_time': 3701.1584811210632, 'accumulated_logging_time': 1.0890610218048096, 'global_step': 47439, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.20823953, dtype=float32), 'train/wer': 0.07322993330586886, 'validation/ctc_loss': Array(0.41214904, dtype=float32), 'validation/wer': 0.12032594108730703, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21926215, dtype=float32), 'test/wer': 0.07163894136046961, 'test/num_examples': 2472, 'score': 37935.802810907364, 'total_duration': 41783.96035575867, 'accumulated_submission_time': 37935.802810907364, 'accumulated_eval_time': 3845.009263277054, 'accumulated_logging_time': 1.1332612037658691, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0211 15:42:55.864396 140437690197824 submission_runner.py:586] Timing: 37935.802810907364
I0211 15:42:55.864462 140437690197824 submission_runner.py:588] Total number of evals: 28
I0211 15:42:55.864515 140437690197824 submission_runner.py:589] ====================
I0211 15:42:55.864583 140437690197824 submission_runner.py:542] Using RNG seed 2216178884
I0211 15:42:55.867818 140437690197824 submission_runner.py:551] --- Tuning run 2/5 ---
I0211 15:42:55.867972 140437690197824 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_2.
I0211 15:42:55.868331 140437690197824 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_2/hparams.json.
I0211 15:42:55.869699 140437690197824 submission_runner.py:206] Initializing dataset.
I0211 15:42:55.869828 140437690197824 submission_runner.py:213] Initializing model.
I0211 15:42:57.005823 140437690197824 submission_runner.py:255] Initializing optimizer.
I0211 15:42:57.143284 140437690197824 submission_runner.py:262] Initializing metrics bundle.
I0211 15:42:57.143471 140437690197824 submission_runner.py:280] Initializing checkpoint and logger.
I0211 15:42:57.147717 140437690197824 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_2 with prefix checkpoint_
I0211 15:42:57.147861 140437690197824 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_2/meta_data_0.json.
I0211 15:42:57.148231 140437690197824 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 15:42:57.148326 140437690197824 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 15:42:57.862085 140437690197824 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 15:42:58.557868 140437690197824 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_2/flags_0.json.
I0211 15:42:58.590142 140437690197824 submission_runner.py:314] Starting training loop.
I0211 15:42:58.593510 140437690197824 input_pipeline.py:20] Loading split = train-clean-100
I0211 15:42:58.640824 140437690197824 input_pipeline.py:20] Loading split = train-clean-360
I0211 15:42:59.200543 140437690197824 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 15:43:14.918357 140279358486272 logging_writer.py:48] [0] global_step=0, grad_norm=21.787630081176758, loss=32.91376495361328
I0211 15:43:14.932816 140437690197824 spec.py:321] Evaluating on the training split.
I0211 15:44:53.538477 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 15:45:55.391458 140437690197824 spec.py:349] Evaluating on the test split.
I0211 15:46:27.669458 140437690197824 submission_runner.py:408] Time since start: 209.08s, 	Step: 1, 	{'train/ctc_loss': Array(32.25542, dtype=float32), 'train/wer': 3.4835125993757408, 'validation/ctc_loss': Array(30.891924, dtype=float32), 'validation/wer': 3.325159060409164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942331, dtype=float32), 'test/wer': 3.6988402088030385, 'test/num_examples': 2472, 'score': 16.34257674217224, 'total_duration': 209.07660150527954, 'accumulated_submission_time': 16.34257674217224, 'accumulated_eval_time': 192.73394989967346, 'accumulated_logging_time': 0}
I0211 15:46:27.687046 140280641345280 logging_writer.py:48] [1] accumulated_eval_time=192.733950, accumulated_logging_time=0, accumulated_submission_time=16.342577, global_step=1, preemption_count=0, score=16.342577, test/ctc_loss=30.942331314086914, test/num_examples=2472, test/wer=3.698840, total_duration=209.076602, train/ctc_loss=32.25542068481445, train/wer=3.483513, validation/ctc_loss=30.891923904418945, validation/num_examples=5348, validation/wer=3.325159
I0211 15:47:52.762891 140280565810944 logging_writer.py:48] [100] global_step=100, grad_norm=4.4774932861328125, loss=7.249282360076904
I0211 15:49:09.221003 140280574203648 logging_writer.py:48] [200] global_step=200, grad_norm=1.3349207639694214, loss=5.969944477081299
I0211 15:50:25.608543 140280565810944 logging_writer.py:48] [300] global_step=300, grad_norm=0.7663012742996216, loss=5.824243545532227
I0211 15:51:42.047804 140280574203648 logging_writer.py:48] [400] global_step=400, grad_norm=0.6974452137947083, loss=5.733412742614746
I0211 15:52:58.220442 140280565810944 logging_writer.py:48] [500] global_step=500, grad_norm=1.2205541133880615, loss=5.678002834320068
I0211 15:54:14.542526 140280574203648 logging_writer.py:48] [600] global_step=600, grad_norm=1.0400323867797852, loss=5.472447395324707
I0211 15:55:31.672758 140280565810944 logging_writer.py:48] [700] global_step=700, grad_norm=1.6835964918136597, loss=5.1179633140563965
I0211 15:56:54.541261 140280574203648 logging_writer.py:48] [800] global_step=800, grad_norm=1.7046167850494385, loss=4.752006530761719
I0211 15:58:18.198616 140280565810944 logging_writer.py:48] [900] global_step=900, grad_norm=1.4046685695648193, loss=4.312763690948486
I0211 15:59:41.318032 140280574203648 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.794309616088867, loss=4.037564754486084
I0211 16:01:03.060691 140280641345280 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.0079381465911865, loss=3.778750419616699
I0211 16:02:18.849972 140280632952576 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.8147770166397095, loss=3.5943806171417236
I0211 16:03:34.726067 140280641345280 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.5461575984954834, loss=3.430901288986206
I0211 16:04:50.615176 140280632952576 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.736719846725464, loss=3.3179047107696533
I0211 16:06:08.854033 140280641345280 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.4837136268615723, loss=3.2482385635375977
I0211 16:07:32.972240 140280632952576 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.007474422454834, loss=3.0865631103515625
I0211 16:08:56.595530 140280641345280 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.6230554580688477, loss=2.963935136795044
I0211 16:10:19.580549 140280632952576 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.8228085041046143, loss=2.8663198947906494
I0211 16:10:28.313746 140437690197824 spec.py:321] Evaluating on the training split.
I0211 16:11:13.168019 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 16:11:59.096997 140437690197824 spec.py:349] Evaluating on the test split.
I0211 16:12:23.185220 140437690197824 submission_runner.py:408] Time since start: 1764.59s, 	Step: 1812, 	{'train/ctc_loss': Array(6.4745483, dtype=float32), 'train/wer': 0.9413882809148034, 'validation/ctc_loss': Array(6.38616, dtype=float32), 'validation/wer': 0.8965214285024667, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.340809, dtype=float32), 'test/wer': 0.899396746084943, 'test/num_examples': 2472, 'score': 1456.8843939304352, 'total_duration': 1764.5923347473145, 'accumulated_submission_time': 1456.8843939304352, 'accumulated_eval_time': 307.60274863243103, 'accumulated_logging_time': 0.028884410858154297}
I0211 16:12:23.214349 140281071425280 logging_writer.py:48] [1812] accumulated_eval_time=307.602749, accumulated_logging_time=0.028884, accumulated_submission_time=1456.884394, global_step=1812, preemption_count=0, score=1456.884394, test/ctc_loss=6.340808868408203, test/num_examples=2472, test/wer=0.899397, total_duration=1764.592335, train/ctc_loss=6.47454833984375, train/wer=0.941388, validation/ctc_loss=6.386159896850586, validation/num_examples=5348, validation/wer=0.896521
I0211 16:13:30.328282 140281063032576 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.2509424686431885, loss=2.813770294189453
I0211 16:14:45.902970 140281071425280 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.894075870513916, loss=2.7320895195007324
I0211 16:16:05.040554 140281071425280 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.935585618019104, loss=2.5936779975891113
I0211 16:17:20.369079 140281063032576 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.579005241394043, loss=2.5557854175567627
I0211 16:18:35.654553 140281071425280 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.906075954437256, loss=2.5428006649017334
I0211 16:19:51.168962 140281063032576 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.173020601272583, loss=2.40535569190979
I0211 16:21:08.447008 140281071425280 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.73158597946167, loss=2.428467273712158
I0211 16:22:32.004918 140281063032576 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.4432740211486816, loss=2.3378264904022217
I0211 16:23:55.057889 140281071425280 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.3771073818206787, loss=2.32712459564209
I0211 16:25:17.860374 140281063032576 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.173496723175049, loss=2.3115296363830566
I0211 16:26:40.279925 140281071425280 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.3372678756713867, loss=2.2732255458831787
I0211 16:28:03.430537 140281063032576 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.227771043777466, loss=2.1976897716522217
I0211 16:29:29.354778 140281071425280 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.1249208450317383, loss=2.1815803050994873
I0211 16:30:44.340604 140281063032576 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.167397975921631, loss=2.168992757797241
I0211 16:31:59.614989 140281071425280 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.988006114959717, loss=2.1290159225463867
I0211 16:33:15.043727 140281063032576 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.40324068069458, loss=2.1411404609680176
I0211 16:34:30.178653 140281071425280 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.72309684753418, loss=2.0866641998291016
I0211 16:35:49.860274 140281063032576 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.9210891723632812, loss=2.0583560466766357
I0211 16:36:23.477931 140437690197824 spec.py:321] Evaluating on the training split.
I0211 16:37:10.708703 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 16:37:59.791902 140437690197824 spec.py:349] Evaluating on the test split.
I0211 16:38:25.238303 140437690197824 submission_runner.py:408] Time since start: 3326.64s, 	Step: 3641, 	{'train/ctc_loss': Array(3.3249242, dtype=float32), 'train/wer': 0.7415177804500945, 'validation/ctc_loss': Array(3.0489736, dtype=float32), 'validation/wer': 0.6717804145708024, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.53634, dtype=float32), 'test/wer': 0.5975869843397721, 'test/num_examples': 2472, 'score': 2897.0594935417175, 'total_duration': 3326.6420726776123, 'accumulated_submission_time': 2897.0594935417175, 'accumulated_eval_time': 429.35713052749634, 'accumulated_logging_time': 0.06998872756958008}
I0211 16:38:25.271936 140281071425280 logging_writer.py:48] [3641] accumulated_eval_time=429.357131, accumulated_logging_time=0.069989, accumulated_submission_time=2897.059494, global_step=3641, preemption_count=0, score=2897.059494, test/ctc_loss=2.5363399982452393, test/num_examples=2472, test/wer=0.597587, total_duration=3326.642073, train/ctc_loss=3.3249242305755615, train/wer=0.741518, validation/ctc_loss=3.048973560333252, validation/num_examples=5348, validation/wer=0.671780
I0211 16:39:10.408512 140281063032576 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.831249475479126, loss=2.0300943851470947
I0211 16:40:25.391830 140281071425280 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.9960448741912842, loss=1.950111746788025
I0211 16:41:40.447892 140281063032576 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.568415403366089, loss=2.0301196575164795
I0211 16:42:55.875094 140281071425280 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.5394678115844727, loss=2.036139488220215
I0211 16:44:17.751912 140281063032576 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.0333778858184814, loss=1.96902596950531
I0211 16:45:37.942266 140281071425280 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.4869704246520996, loss=1.948384404182434
I0211 16:46:52.959593 140281063032576 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.262587308883667, loss=1.956181526184082
I0211 16:48:07.792232 140281071425280 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.587967872619629, loss=1.9705051183700562
I0211 16:49:22.826083 140281063032576 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.948613405227661, loss=1.8689724206924438
I0211 16:50:42.200787 140281071425280 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.7956578731536865, loss=1.9277416467666626
I0211 16:52:05.921927 140281063032576 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.389479398727417, loss=1.8656678199768066
I0211 16:53:30.429268 140281071425280 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.0455360412597656, loss=1.8618041276931763
I0211 16:54:54.973206 140281063032576 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.4142184257507324, loss=1.8365001678466797
I0211 16:56:19.216196 140281071425280 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.992753028869629, loss=1.877764344215393
I0211 16:57:43.339929 140281063032576 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.9889144897460938, loss=1.8094829320907593
I0211 16:59:05.904061 140281071425280 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.5895485877990723, loss=1.7672251462936401
I0211 17:00:20.764526 140281063032576 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.8685662746429443, loss=1.813419222831726
I0211 17:01:35.735690 140281071425280 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.0525853633880615, loss=1.7304638624191284
I0211 17:02:25.972063 140437690197824 spec.py:321] Evaluating on the training split.
I0211 17:03:21.708516 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 17:04:12.926829 140437690197824 spec.py:349] Evaluating on the test split.
I0211 17:04:38.949558 140437690197824 submission_runner.py:408] Time since start: 4900.36s, 	Step: 5468, 	{'train/ctc_loss': Array(0.7368655, dtype=float32), 'train/wer': 0.23889931546230703, 'validation/ctc_loss': Array(0.8666504, dtype=float32), 'validation/wer': 0.2528746729486276, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.56013215, dtype=float32), 'test/wer': 0.18442914305445535, 'test/num_examples': 2472, 'score': 4337.671471118927, 'total_duration': 4900.35649228096, 'accumulated_submission_time': 4337.671471118927, 'accumulated_eval_time': 562.3317792415619, 'accumulated_logging_time': 0.11525559425354004}
I0211 17:04:38.987853 140281071425280 logging_writer.py:48] [5468] accumulated_eval_time=562.331779, accumulated_logging_time=0.115256, accumulated_submission_time=4337.671471, global_step=5468, preemption_count=0, score=4337.671471, test/ctc_loss=0.5601321458816528, test/num_examples=2472, test/wer=0.184429, total_duration=4900.356492, train/ctc_loss=0.7368655204772949, train/wer=0.238899, validation/ctc_loss=0.866650402545929, validation/num_examples=5348, validation/wer=0.252875
I0211 17:05:03.925775 140281063032576 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.4201250076293945, loss=1.7427170276641846
I0211 17:06:19.129644 140281071425280 logging_writer.py:48] [5600] global_step=5600, grad_norm=5.076394557952881, loss=1.8251540660858154
I0211 17:07:34.108987 140281063032576 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.546138763427734, loss=1.792391300201416
I0211 17:08:49.194031 140281071425280 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.6023101806640625, loss=1.8366966247558594
I0211 17:10:10.233409 140281063032576 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.1438982486724854, loss=1.785961627960205
I0211 17:11:33.061576 140281071425280 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.500972032546997, loss=1.8049006462097168
I0211 17:12:56.902220 140281063032576 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.090049982070923, loss=1.7355376482009888
I0211 17:14:23.081112 140280743745280 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.5828216075897217, loss=1.699514389038086
I0211 17:15:38.314410 140280735352576 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.7536970376968384, loss=1.6995360851287842
I0211 17:16:53.543491 140280743745280 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.638296365737915, loss=1.7380397319793701
I0211 17:18:08.624700 140280735352576 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.8703253269195557, loss=1.7740304470062256
I0211 17:19:23.970996 140280743745280 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.7492475509643555, loss=1.7139683961868286
I0211 17:20:42.896511 140280735352576 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.3538589477539062, loss=1.6888006925582886
I0211 17:22:07.393152 140280743745280 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.2886440753936768, loss=1.7070049047470093
I0211 17:23:31.705320 140280735352576 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.6505825519561768, loss=1.7049779891967773
I0211 17:24:56.354380 140280743745280 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.4917333126068115, loss=1.6217846870422363
I0211 17:26:21.394079 140280735352576 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.246232032775879, loss=1.6481820344924927
I0211 17:27:45.616425 140280743745280 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.7352728843688965, loss=1.711765170097351
I0211 17:28:39.303950 140437690197824 spec.py:321] Evaluating on the training split.
I0211 17:29:36.444605 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 17:30:28.553807 140437690197824 spec.py:349] Evaluating on the test split.
I0211 17:30:55.089749 140437690197824 submission_runner.py:408] Time since start: 6476.49s, 	Step: 7267, 	{'train/ctc_loss': Array(0.63280827, dtype=float32), 'train/wer': 0.20651483781918564, 'validation/ctc_loss': Array(0.77585036, dtype=float32), 'validation/wer': 0.22672987246203308, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47255045, dtype=float32), 'test/wer': 0.15680539475555014, 'test/num_examples': 2472, 'score': 5777.897728443146, 'total_duration': 6476.492695569992, 'accumulated_submission_time': 5777.897728443146, 'accumulated_eval_time': 698.1108963489532, 'accumulated_logging_time': 0.1686549186706543}
I0211 17:30:55.127606 140280743745280 logging_writer.py:48] [7267] accumulated_eval_time=698.110896, accumulated_logging_time=0.168655, accumulated_submission_time=5777.897728, global_step=7267, preemption_count=0, score=5777.897728, test/ctc_loss=0.4725504517555237, test/num_examples=2472, test/wer=0.156805, total_duration=6476.492696, train/ctc_loss=0.632808268070221, train/wer=0.206515, validation/ctc_loss=0.7758503556251526, validation/num_examples=5348, validation/wer=0.226730
I0211 17:31:20.625858 140280735352576 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.984682559967041, loss=1.6202906370162964
I0211 17:32:35.744963 140280743745280 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.5476551055908203, loss=1.6288591623306274
I0211 17:33:51.141487 140280735352576 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.9117093086242676, loss=1.6328871250152588
I0211 17:35:06.182749 140280743745280 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.4439613819122314, loss=1.6563019752502441
I0211 17:36:21.429674 140280735352576 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.483525037765503, loss=1.6545599699020386
I0211 17:37:36.578641 140280743745280 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.729774236679077, loss=1.692842721939087
I0211 17:39:00.260957 140280735352576 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.0344526767730713, loss=1.6395847797393799
I0211 17:40:24.875837 140280743745280 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.595639228820801, loss=1.6775082349777222
I0211 17:41:49.434714 140280735352576 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.627241134643555, loss=1.657021164894104
I0211 17:43:15.031693 140280743745280 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.598250150680542, loss=1.690181016921997
I0211 17:44:37.275062 140280743745280 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.028935432434082, loss=1.6942756175994873
I0211 17:45:52.380126 140280735352576 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.7173354625701904, loss=1.6504359245300293
I0211 17:47:07.609179 140280743745280 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.0500454902648926, loss=1.6930513381958008
I0211 17:48:22.690968 140280735352576 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.1839442253112793, loss=1.6516798734664917
I0211 17:49:39.030814 140280743745280 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.553077220916748, loss=1.6549403667449951
I0211 17:51:03.877936 140280735352576 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.6962943077087402, loss=1.6445425748825073
I0211 17:52:28.318447 140280743745280 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.1019318103790283, loss=1.619633674621582
I0211 17:53:53.173314 140280735352576 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.0093376636505127, loss=1.628875494003296
I0211 17:54:55.093770 140437690197824 spec.py:321] Evaluating on the training split.
I0211 17:55:50.869014 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 17:56:42.751396 140437690197824 spec.py:349] Evaluating on the test split.
I0211 17:57:09.285252 140437690197824 submission_runner.py:408] Time since start: 8050.69s, 	Step: 9075, 	{'train/ctc_loss': Array(0.5822703, dtype=float32), 'train/wer': 0.19117933221157699, 'validation/ctc_loss': Array(0.71564424, dtype=float32), 'validation/wer': 0.2078743350357705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42079455, dtype=float32), 'test/wer': 0.13990616050210225, 'test/num_examples': 2472, 'score': 7217.773640632629, 'total_duration': 8050.689073801041, 'accumulated_submission_time': 7217.773640632629, 'accumulated_eval_time': 832.2964282035828, 'accumulated_logging_time': 0.2222280502319336}
I0211 17:57:09.322075 140280743745280 logging_writer.py:48] [9075] accumulated_eval_time=832.296428, accumulated_logging_time=0.222228, accumulated_submission_time=7217.773641, global_step=9075, preemption_count=0, score=7217.773641, test/ctc_loss=0.4207945466041565, test/num_examples=2472, test/wer=0.139906, total_duration=8050.689074, train/ctc_loss=0.5822703242301941, train/wer=0.191179, validation/ctc_loss=0.7156442403793335, validation/num_examples=5348, validation/wer=0.207874
I0211 17:57:28.989686 140280735352576 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.3033981323242188, loss=1.6807352304458618
I0211 17:58:44.067404 140280743745280 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.2340281009674072, loss=1.5820404291152954
I0211 18:00:02.914334 140280743745280 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.6465587615966797, loss=1.6072574853897095
I0211 18:01:18.256253 140280735352576 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.3157856464385986, loss=1.615061640739441
I0211 18:02:33.577328 140280743745280 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.290956735610962, loss=1.5318547487258911
I0211 18:03:48.870955 140280735352576 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.90095591545105, loss=1.6483701467514038
I0211 18:05:04.509574 140280743745280 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.54306960105896, loss=1.5606229305267334
I0211 18:06:28.501739 140280735352576 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.3875203132629395, loss=1.6192666292190552
I0211 18:07:53.030898 140280743745280 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.262739419937134, loss=1.5615594387054443
I0211 18:09:17.654498 140280735352576 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.6557748317718506, loss=1.5799014568328857
I0211 18:10:42.547411 140280743745280 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.014265537261963, loss=1.5700819492340088
I0211 18:12:07.425395 140280735352576 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.516308307647705, loss=1.6178410053253174
I0211 18:13:35.208289 140280743745280 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.677415609359741, loss=1.5036431550979614
I0211 18:14:50.386368 140280735352576 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.0314605236053467, loss=1.5556836128234863
I0211 18:16:05.531810 140280743745280 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.478811025619507, loss=1.584274172782898
I0211 18:17:20.694397 140280735352576 logging_writer.py:48] [10600] global_step=10600, grad_norm=7.764841079711914, loss=1.580814242362976
I0211 18:18:35.738729 140280743745280 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.5303537845611572, loss=1.5771610736846924
I0211 18:19:56.997322 140280735352576 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.4700746536254883, loss=1.5026010274887085
I0211 18:21:09.302220 140437690197824 spec.py:321] Evaluating on the training split.
I0211 18:22:06.259404 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 18:22:58.353863 140437690197824 spec.py:349] Evaluating on the test split.
I0211 18:23:24.571455 140437690197824 submission_runner.py:408] Time since start: 9625.98s, 	Step: 10886, 	{'train/ctc_loss': Array(0.5224885, dtype=float32), 'train/wer': 0.1712662956468705, 'validation/ctc_loss': Array(0.66655123, dtype=float32), 'validation/wer': 0.19406818115991, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3917676, dtype=float32), 'test/wer': 0.12985192858448602, 'test/num_examples': 2472, 'score': 8657.661759138107, 'total_duration': 9625.975238323212, 'accumulated_submission_time': 8657.661759138107, 'accumulated_eval_time': 967.5596957206726, 'accumulated_logging_time': 0.2755579948425293}
I0211 18:23:24.608889 140280743745280 logging_writer.py:48] [10886] accumulated_eval_time=967.559696, accumulated_logging_time=0.275558, accumulated_submission_time=8657.661759, global_step=10886, preemption_count=0, score=8657.661759, test/ctc_loss=0.39176759123802185, test/num_examples=2472, test/wer=0.129852, total_duration=9625.975238, train/ctc_loss=0.5224884748458862, train/wer=0.171266, validation/ctc_loss=0.6665512323379517, validation/num_examples=5348, validation/wer=0.194068
I0211 18:23:35.971075 140280735352576 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.924701690673828, loss=1.569400429725647
I0211 18:24:50.877489 140280743745280 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.2594680786132812, loss=1.5609793663024902
I0211 18:26:05.880951 140280735352576 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.9558823108673096, loss=1.4531418085098267
I0211 18:27:21.026814 140280743745280 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.173940420150757, loss=1.4964097738265991
I0211 18:28:44.243055 140280735352576 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.5093822479248047, loss=1.521755576133728
I0211 18:30:05.033403 140281071425280 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.088768243789673, loss=1.521176815032959
I0211 18:31:19.992526 140281063032576 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.9614102840423584, loss=1.5200183391571045
I0211 18:32:35.138457 140281071425280 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.827552556991577, loss=1.5456570386886597
I0211 18:33:50.290480 140281063032576 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.8187097311019897, loss=1.557715654373169
I0211 18:35:06.805509 140281071425280 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.8365001678466797, loss=1.5842986106872559
I0211 18:36:31.663155 140281063032576 logging_writer.py:48] [11900] global_step=11900, grad_norm=3.2004170417785645, loss=1.6130000352859497
I0211 18:37:57.724450 140281071425280 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.8142073154449463, loss=1.5172932147979736
I0211 18:39:22.460744 140281063032576 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.483734369277954, loss=1.4470139741897583
I0211 18:40:47.623776 140281071425280 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.6515674591064453, loss=1.515486240386963
I0211 18:42:12.020252 140281063032576 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.6248230934143066, loss=1.5901347398757935
I0211 18:43:37.321044 140280743745280 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.8812692165374756, loss=1.5277975797653198
I0211 18:44:52.487151 140280735352576 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.391047716140747, loss=1.5517873764038086
I0211 18:46:07.635362 140280743745280 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.9726290702819824, loss=1.543381929397583
I0211 18:47:22.524541 140280735352576 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.566082715988159, loss=1.5028022527694702
I0211 18:47:24.651789 140437690197824 spec.py:321] Evaluating on the training split.
I0211 18:48:21.012773 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 18:49:13.280879 140437690197824 spec.py:349] Evaluating on the test split.
I0211 18:49:39.882520 140437690197824 submission_runner.py:408] Time since start: 11201.29s, 	Step: 12704, 	{'train/ctc_loss': Array(0.47296602, dtype=float32), 'train/wer': 0.15854574394153384, 'validation/ctc_loss': Array(0.6403958, dtype=float32), 'validation/wer': 0.1855720864670728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36725986, dtype=float32), 'test/wer': 0.12052891353360551, 'test/num_examples': 2472, 'score': 10097.6127679348, 'total_duration': 11201.28601717949, 'accumulated_submission_time': 10097.6127679348, 'accumulated_eval_time': 1102.7841680049896, 'accumulated_logging_time': 0.329695463180542}
I0211 18:49:39.915842 140280933185280 logging_writer.py:48] [12704] accumulated_eval_time=1102.784168, accumulated_logging_time=0.329695, accumulated_submission_time=10097.612768, global_step=12704, preemption_count=0, score=10097.612768, test/ctc_loss=0.3672598600387573, test/num_examples=2472, test/wer=0.120529, total_duration=11201.286017, train/ctc_loss=0.4729660153388977, train/wer=0.158546, validation/ctc_loss=0.6403958201408386, validation/num_examples=5348, validation/wer=0.185572
I0211 18:50:53.036251 140280924792576 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.155553340911865, loss=1.4905110597610474
I0211 18:52:08.749398 140280933185280 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6572421789169312, loss=1.4932581186294556
I0211 18:53:24.137054 140280924792576 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.0698392391204834, loss=1.556743860244751
I0211 18:54:44.050246 140280933185280 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.4811530113220215, loss=1.5982812643051147
I0211 18:56:08.204125 140280924792576 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.099210023880005, loss=1.493030309677124
I0211 18:57:32.177254 140280933185280 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.70666766166687, loss=1.5392909049987793
I0211 18:58:59.858985 140280933185280 logging_writer.py:48] [13400] global_step=13400, grad_norm=3.6483237743377686, loss=1.4807350635528564
I0211 19:00:15.102037 140280924792576 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.2602837085723877, loss=1.4720836877822876
I0211 19:01:30.333406 140280933185280 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.544429063796997, loss=1.519341230392456
I0211 19:02:45.609960 140280924792576 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.8812247514724731, loss=1.4445793628692627
I0211 19:04:00.657116 140280933185280 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.0388081073760986, loss=1.4438093900680542
I0211 19:05:20.813444 140280924792576 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.9310932159423828, loss=1.5072911977767944
I0211 19:06:45.109118 140280933185280 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.7403645515441895, loss=1.5025936365127563
I0211 19:08:10.053715 140280924792576 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.76271915435791, loss=1.4567078351974487
I0211 19:09:34.463555 140280933185280 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.645390748977661, loss=1.4911068677902222
I0211 19:10:58.930577 140280924792576 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.183894157409668, loss=1.4991989135742188
I0211 19:12:22.709839 140280933185280 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.567010402679443, loss=1.4763967990875244
I0211 19:13:40.437285 140437690197824 spec.py:321] Evaluating on the training split.
I0211 19:14:34.780161 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 19:15:25.887330 140437690197824 spec.py:349] Evaluating on the test split.
I0211 19:15:51.755746 140437690197824 submission_runner.py:408] Time since start: 12773.16s, 	Step: 14498, 	{'train/ctc_loss': Array(0.43832353, dtype=float32), 'train/wer': 0.14844142211674502, 'validation/ctc_loss': Array(0.60433036, dtype=float32), 'validation/wer': 0.17553124728462882, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3434072, dtype=float32), 'test/wer': 0.11447606280340422, 'test/num_examples': 2472, 'score': 11538.042315006256, 'total_duration': 12773.159367084503, 'accumulated_submission_time': 11538.042315006256, 'accumulated_eval_time': 1234.0966272354126, 'accumulated_logging_time': 0.3797941207885742}
I0211 19:15:51.790324 140281363265280 logging_writer.py:48] [14498] accumulated_eval_time=1234.096627, accumulated_logging_time=0.379794, accumulated_submission_time=11538.042315, global_step=14498, preemption_count=0, score=11538.042315, test/ctc_loss=0.34340721368789673, test/num_examples=2472, test/wer=0.114476, total_duration=12773.159367, train/ctc_loss=0.4383235275745392, train/wer=0.148441, validation/ctc_loss=0.6043303608894348, validation/num_examples=5348, validation/wer=0.175531
I0211 19:15:54.203273 140281354872576 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.5772922039031982, loss=1.4849246740341187
I0211 19:17:09.002156 140281363265280 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.175812244415283, loss=1.472644329071045
I0211 19:18:24.027094 140281354872576 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.6362733840942383, loss=1.480198860168457
I0211 19:19:39.138672 140281363265280 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.751066207885742, loss=1.5239468812942505
I0211 19:20:54.104660 140281354872576 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.6535422801971436, loss=1.4230326414108276
I0211 19:22:09.392378 140281363265280 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.520134210586548, loss=1.4244205951690674
I0211 19:23:31.624923 140281354872576 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.5496180057525635, loss=1.4906424283981323
I0211 19:24:57.032547 140281363265280 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.4739580154418945, loss=1.4426857233047485
I0211 19:26:21.271075 140281354872576 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.3825130462646484, loss=1.441556453704834
I0211 19:27:45.848859 140281363265280 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.0460400581359863, loss=1.5103061199188232
I0211 19:29:08.733676 140281363265280 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.226353883743286, loss=1.4942330121994019
I0211 19:30:24.096470 140281354872576 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.5359392166137695, loss=1.4347785711288452
I0211 19:31:39.248485 140281363265280 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.193418025970459, loss=1.510152816772461
I0211 19:32:54.417718 140281354872576 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.7136309146881104, loss=1.4846092462539673
I0211 19:34:12.989283 140281363265280 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.9313405752182007, loss=1.4957400560379028
I0211 19:35:37.591762 140281354872576 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.0915520191192627, loss=1.4894185066223145
I0211 19:37:02.788873 140281363265280 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.7771518230438232, loss=1.4875818490982056
I0211 19:38:27.634187 140281354872576 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.359701156616211, loss=1.4488391876220703
I0211 19:39:51.984173 140437690197824 spec.py:321] Evaluating on the training split.
I0211 19:40:48.451412 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 19:41:40.673212 140437690197824 spec.py:349] Evaluating on the test split.
I0211 19:42:06.707052 140437690197824 submission_runner.py:408] Time since start: 14348.11s, 	Step: 16299, 	{'train/ctc_loss': Array(0.42478886, dtype=float32), 'train/wer': 0.14485225193054502, 'validation/ctc_loss': Array(0.6073105, dtype=float32), 'validation/wer': 0.1771242650395358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34683496, dtype=float32), 'test/wer': 0.11295269433103812, 'test/num_examples': 2472, 'score': 12978.146248579025, 'total_duration': 14348.110781908035, 'accumulated_submission_time': 12978.146248579025, 'accumulated_eval_time': 1368.8134655952454, 'accumulated_logging_time': 0.42960095405578613}
I0211 19:42:06.740792 140280779585280 logging_writer.py:48] [16299] accumulated_eval_time=1368.813466, accumulated_logging_time=0.429601, accumulated_submission_time=12978.146249, global_step=16299, preemption_count=0, score=12978.146249, test/ctc_loss=0.3468349575996399, test/num_examples=2472, test/wer=0.112953, total_duration=14348.110782, train/ctc_loss=0.42478886246681213, train/wer=0.144852, validation/ctc_loss=0.6073104739189148, validation/num_examples=5348, validation/wer=0.177124
I0211 19:42:08.369795 140280771192576 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.441218376159668, loss=1.5606024265289307
I0211 19:43:23.560255 140280779585280 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.7535572052001953, loss=1.3937979936599731
I0211 19:44:42.647244 140280779585280 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.527472496032715, loss=1.413805603981018
I0211 19:45:57.838067 140280771192576 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.9609637260437012, loss=1.4367876052856445
I0211 19:47:12.883795 140280779585280 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.2480406761169434, loss=1.4071123600006104
I0211 19:48:27.748778 140280771192576 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.0367329120635986, loss=1.4411526918411255
I0211 19:49:46.355854 140280779585280 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.6197288036346436, loss=1.43712317943573
I0211 19:51:10.802742 140280771192576 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.4969382286071777, loss=1.4194879531860352
I0211 19:52:36.294818 140280779585280 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.154015302658081, loss=1.4609416723251343
I0211 19:54:01.536078 140280771192576 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.7534873485565186, loss=1.3986960649490356
I0211 19:55:27.321032 140280779585280 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.8188087940216064, loss=1.4367737770080566
I0211 19:56:52.916527 140280771192576 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.643589735031128, loss=1.428727149963379
I0211 19:58:16.811604 140280779585280 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.630774974822998, loss=1.4365761280059814
I0211 19:59:36.565032 140280779585280 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.5065739154815674, loss=1.396456241607666
I0211 20:00:51.524718 140280771192576 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.376404285430908, loss=1.4277170896530151
I0211 20:02:07.008508 140280779585280 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.0924670696258545, loss=1.4831823110580444
I0211 20:03:22.381337 140280771192576 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.281741142272949, loss=1.4669123888015747
I0211 20:04:43.711307 140280779585280 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.0819482803344727, loss=1.3683826923370361
I0211 20:06:07.340162 140437690197824 spec.py:321] Evaluating on the training split.
I0211 20:07:02.990610 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 20:07:54.959542 140437690197824 spec.py:349] Evaluating on the test split.
I0211 20:08:21.049014 140437690197824 submission_runner.py:408] Time since start: 15922.45s, 	Step: 18099, 	{'train/ctc_loss': Array(0.41303357, dtype=float32), 'train/wer': 0.1393726834323064, 'validation/ctc_loss': Array(0.57551175, dtype=float32), 'validation/wer': 0.16836749471407744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3229248, dtype=float32), 'test/wer': 0.10594519935815408, 'test/num_examples': 2472, 'score': 14418.65439915657, 'total_duration': 15922.452051639557, 'accumulated_submission_time': 14418.65439915657, 'accumulated_eval_time': 1502.5155773162842, 'accumulated_logging_time': 0.478834867477417}
I0211 20:08:21.086435 140280779585280 logging_writer.py:48] [18099] accumulated_eval_time=1502.515577, accumulated_logging_time=0.478835, accumulated_submission_time=14418.654399, global_step=18099, preemption_count=0, score=14418.654399, test/ctc_loss=0.32292479276657104, test/num_examples=2472, test/wer=0.105945, total_duration=15922.452052, train/ctc_loss=0.4130335748195648, train/wer=0.139373, validation/ctc_loss=0.5755117535591125, validation/num_examples=5348, validation/wer=0.168367
I0211 20:08:22.715755 140280771192576 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.0825793743133545, loss=1.413945198059082
I0211 20:09:37.824737 140280779585280 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.1380584239959717, loss=1.4165650606155396
I0211 20:10:53.020023 140280771192576 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.829699993133545, loss=1.4453721046447754
I0211 20:12:08.493778 140280779585280 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.0842223167419434, loss=1.3988169431686401
I0211 20:13:31.418467 140280771192576 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.1667447090148926, loss=1.387560248374939
I0211 20:14:53.323038 140280779585280 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.801008701324463, loss=1.373321533203125
I0211 20:16:08.407481 140280771192576 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.8633031845092773, loss=1.4137150049209595
I0211 20:17:23.410697 140280779585280 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.8663814067840576, loss=1.3510291576385498
I0211 20:18:38.581461 140280771192576 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.006415843963623, loss=1.3572179079055786
I0211 20:19:56.827231 140280779585280 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.1937999725341797, loss=1.3794376850128174
I0211 20:21:21.938677 140280771192576 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.19903564453125, loss=1.4039582014083862
I0211 20:22:46.538683 140280779585280 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.213252067565918, loss=1.4518202543258667
I0211 20:24:11.714918 140280771192576 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.0994203090667725, loss=1.3572131395339966
I0211 20:25:37.036079 140280779585280 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.1086180210113525, loss=1.4196264743804932
I0211 20:27:02.196776 140280771192576 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.052103281021118, loss=1.3572779893875122
I0211 20:28:27.954837 140280779585280 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.6516335010528564, loss=1.3940938711166382
I0211 20:29:42.913732 140280771192576 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.3004813194274902, loss=1.4349020719528198
I0211 20:30:57.867344 140280779585280 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.5968644618988037, loss=1.3967567682266235
I0211 20:32:13.129136 140280771192576 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.1151554584503174, loss=1.393433928489685
I0211 20:32:21.279943 140437690197824 spec.py:321] Evaluating on the training split.
I0211 20:33:17.180955 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 20:34:09.633412 140437690197824 spec.py:349] Evaluating on the test split.
I0211 20:34:36.170755 140437690197824 submission_runner.py:408] Time since start: 17497.57s, 	Step: 19912, 	{'train/ctc_loss': Array(0.37763608, dtype=float32), 'train/wer': 0.12719974955837302, 'validation/ctc_loss': Array(0.54934216, dtype=float32), 'validation/wer': 0.1614644177761472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31086192, dtype=float32), 'test/wer': 0.10172039079479211, 'test/num_examples': 2472, 'score': 15858.754093170166, 'total_duration': 17497.57366490364, 'accumulated_submission_time': 15858.754093170166, 'accumulated_eval_time': 1637.3995015621185, 'accumulated_logging_time': 0.5337679386138916}
I0211 20:34:36.210529 140280779585280 logging_writer.py:48] [19912] accumulated_eval_time=1637.399502, accumulated_logging_time=0.533768, accumulated_submission_time=15858.754093, global_step=19912, preemption_count=0, score=15858.754093, test/ctc_loss=0.3108619153499603, test/num_examples=2472, test/wer=0.101720, total_duration=17497.573665, train/ctc_loss=0.3776360750198364, train/wer=0.127200, validation/ctc_loss=0.549342155456543, validation/num_examples=5348, validation/wer=0.161464
I0211 20:35:43.318051 140280771192576 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.5187156200408936, loss=1.3730742931365967
I0211 20:36:58.735968 140280779585280 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.5232086181640625, loss=1.3710836172103882
I0211 20:38:13.645361 140280771192576 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.9735963344573975, loss=1.389552116394043
I0211 20:39:36.021486 140280779585280 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.3040642738342285, loss=1.3871655464172363
I0211 20:41:00.319783 140280771192576 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.4932422637939453, loss=1.3395473957061768
I0211 20:42:25.802759 140280779585280 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.05147647857666, loss=1.3428139686584473
I0211 20:43:53.622441 140280779585280 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.228710651397705, loss=1.3473519086837769
I0211 20:45:08.998953 140280771192576 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.5220940113067627, loss=1.324211835861206
I0211 20:46:24.214396 140280779585280 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.4351067543029785, loss=1.373100757598877
I0211 20:47:39.373124 140280771192576 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.211716890335083, loss=1.3645817041397095
I0211 20:48:54.209194 140280779585280 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.981667160987854, loss=1.3777344226837158
I0211 20:50:14.854590 140280771192576 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.1821818351745605, loss=1.386446237564087
I0211 20:51:39.539242 140280779585280 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.2473888397216797, loss=1.3966819047927856
I0211 20:53:03.805861 140280771192576 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.9304172992706299, loss=1.3934717178344727
I0211 20:54:28.292257 140280779585280 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.7974584102630615, loss=1.4030423164367676
I0211 20:55:53.252087 140280771192576 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.630442500114441, loss=1.3888038396835327
I0211 20:57:17.951997 140280779585280 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.2908987998962402, loss=1.3667680025100708
I0211 20:58:36.506947 140437690197824 spec.py:321] Evaluating on the training split.
I0211 20:59:32.537050 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 21:00:23.883105 140437690197824 spec.py:349] Evaluating on the test split.
I0211 21:00:49.624440 140437690197824 submission_runner.py:408] Time since start: 19071.03s, 	Step: 21698, 	{'train/ctc_loss': Array(0.36806503, dtype=float32), 'train/wer': 0.1274199439912334, 'validation/ctc_loss': Array(0.5384376, dtype=float32), 'validation/wer': 0.15590333761356287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30279443, dtype=float32), 'test/wer': 0.09853147279263909, 'test/num_examples': 2472, 'score': 17298.95994949341, 'total_duration': 19071.027629852295, 'accumulated_submission_time': 17298.95994949341, 'accumulated_eval_time': 1770.5106179714203, 'accumulated_logging_time': 0.5898373126983643}
I0211 21:00:49.665326 140280933185280 logging_writer.py:48] [21698] accumulated_eval_time=1770.510618, accumulated_logging_time=0.589837, accumulated_submission_time=17298.959949, global_step=21698, preemption_count=0, score=17298.959949, test/ctc_loss=0.3027944266796112, test/num_examples=2472, test/wer=0.098531, total_duration=19071.027630, train/ctc_loss=0.3680650293827057, train/wer=0.127420, validation/ctc_loss=0.5384376049041748, validation/num_examples=5348, validation/wer=0.155903
I0211 21:00:52.032882 140280924792576 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.6697254180908203, loss=1.3282947540283203
I0211 21:02:07.072187 140280933185280 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.39752459526062, loss=1.4406682252883911
I0211 21:03:22.182890 140280924792576 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.2200608253479004, loss=1.3449350595474243
I0211 21:04:37.114857 140280933185280 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.6450881958007812, loss=1.3448251485824585
I0211 21:05:52.295774 140280924792576 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.230626106262207, loss=1.3039977550506592
I0211 21:07:07.365247 140280933185280 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.0931315422058105, loss=1.319101333618164
I0211 21:08:31.464131 140280924792576 logging_writer.py:48] [22300] global_step=22300, grad_norm=4.250385284423828, loss=1.3747732639312744
I0211 21:09:55.487227 140280933185280 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.1089327335357666, loss=1.3622264862060547
I0211 21:11:19.978154 140280924792576 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.0316967964172363, loss=1.3837355375289917
I0211 21:12:44.517513 140280933185280 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.7239909172058105, loss=1.3814271688461304
I0211 21:14:08.480302 140280933185280 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.4201130867004395, loss=1.317138671875
I0211 21:15:23.418268 140280924792576 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.6787471771240234, loss=1.3250033855438232
I0211 21:16:38.560312 140280933185280 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.890979290008545, loss=1.3289926052093506
I0211 21:17:53.779958 140280924792576 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.438507556915283, loss=1.3896156549453735
I0211 21:19:09.343657 140280933185280 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.8941181898117065, loss=1.2851899862289429
I0211 21:20:33.665904 140280924792576 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.0497801303863525, loss=1.2808133363723755
I0211 21:21:57.527278 140280933185280 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.065765142440796, loss=1.3604944944381714
I0211 21:23:22.267567 140280924792576 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.3214733600616455, loss=1.3027381896972656
I0211 21:24:46.638847 140280933185280 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.185044050216675, loss=1.3039153814315796
I0211 21:24:49.694158 140437690197824 spec.py:321] Evaluating on the training split.
I0211 21:25:44.859051 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 21:26:36.590875 140437690197824 spec.py:349] Evaluating on the test split.
I0211 21:27:02.908844 140437690197824 submission_runner.py:408] Time since start: 20644.31s, 	Step: 23505, 	{'train/ctc_loss': Array(0.32012546, dtype=float32), 'train/wer': 0.10897084348656946, 'validation/ctc_loss': Array(0.52728623, dtype=float32), 'validation/wer': 0.15230215202216707, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2927988, dtype=float32), 'test/wer': 0.09410354843296163, 'test/num_examples': 2472, 'score': 18738.897290468216, 'total_duration': 20644.312327861786, 'accumulated_submission_time': 18738.897290468216, 'accumulated_eval_time': 1903.7190117835999, 'accumulated_logging_time': 0.6465094089508057}
I0211 21:27:02.944880 140280933185280 logging_writer.py:48] [23505] accumulated_eval_time=1903.719012, accumulated_logging_time=0.646509, accumulated_submission_time=18738.897290, global_step=23505, preemption_count=0, score=18738.897290, test/ctc_loss=0.292798787355423, test/num_examples=2472, test/wer=0.094104, total_duration=20644.312328, train/ctc_loss=0.3201254606246948, train/wer=0.108971, validation/ctc_loss=0.5272862315177917, validation/num_examples=5348, validation/wer=0.152302
I0211 21:28:15.176348 140280924792576 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.0648980140686035, loss=1.2938836812973022
I0211 21:29:33.747759 140280933185280 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.992458701133728, loss=1.3249409198760986
I0211 21:30:48.772795 140280924792576 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.264068841934204, loss=1.2740283012390137
I0211 21:32:03.732745 140280933185280 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.636098861694336, loss=1.3143539428710938
I0211 21:33:18.761034 140280924792576 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.5016732215881348, loss=1.3768048286437988
I0211 21:34:33.650834 140280933185280 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.359884262084961, loss=1.279594898223877
I0211 21:35:56.447696 140280924792576 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.2908143997192383, loss=1.3542455434799194
I0211 21:37:20.338245 140280933185280 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.5886166095733643, loss=1.3670201301574707
I0211 21:38:44.840558 140280924792576 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.8484610319137573, loss=1.2969255447387695
I0211 21:40:09.410140 140280933185280 logging_writer.py:48] [24500] global_step=24500, grad_norm=4.312530994415283, loss=1.3381037712097168
I0211 21:41:34.215661 140280924792576 logging_writer.py:48] [24600] global_step=24600, grad_norm=4.151177406311035, loss=1.311170220375061
I0211 21:42:58.834670 140280933185280 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.0887293815612793, loss=1.2906118631362915
I0211 21:44:19.246613 140280933185280 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.6052823066711426, loss=1.3893256187438965
I0211 21:45:34.295025 140280924792576 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.0414936542510986, loss=1.3362398147583008
I0211 21:46:49.345261 140280933185280 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.990713596343994, loss=1.2803560495376587
I0211 21:48:04.501243 140280924792576 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.902187705039978, loss=1.3220075368881226
I0211 21:49:25.934733 140280933185280 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.1594903469085693, loss=1.371492862701416
I0211 21:50:51.039906 140280924792576 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.4150166511535645, loss=1.3509185314178467
I0211 21:51:03.065425 140437690197824 spec.py:321] Evaluating on the training split.
I0211 21:51:58.223619 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 21:52:49.915442 140437690197824 spec.py:349] Evaluating on the test split.
I0211 21:53:16.331263 140437690197824 submission_runner.py:408] Time since start: 22217.73s, 	Step: 25316, 	{'train/ctc_loss': Array(0.28483084, dtype=float32), 'train/wer': 0.09780632444648349, 'validation/ctc_loss': Array(0.5107498, dtype=float32), 'validation/wer': 0.14767757320640681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2822816, dtype=float32), 'test/wer': 0.09306765787175268, 'test/num_examples': 2472, 'score': 20178.92725777626, 'total_duration': 22217.73483490944, 'accumulated_submission_time': 20178.92725777626, 'accumulated_eval_time': 2036.9786303043365, 'accumulated_logging_time': 0.6984333992004395}
I0211 21:53:16.368671 140281363265280 logging_writer.py:48] [25316] accumulated_eval_time=2036.978630, accumulated_logging_time=0.698433, accumulated_submission_time=20178.927258, global_step=25316, preemption_count=0, score=20178.927258, test/ctc_loss=0.2822816073894501, test/num_examples=2472, test/wer=0.093068, total_duration=22217.734835, train/ctc_loss=0.28483083844184875, train/wer=0.097806, validation/ctc_loss=0.5107498168945312, validation/num_examples=5348, validation/wer=0.147678
I0211 21:54:20.574259 140281354872576 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.626075506210327, loss=1.3170123100280762
I0211 21:55:35.787445 140281363265280 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.3421905040740967, loss=1.3211537599563599
I0211 21:56:50.919311 140281354872576 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.526776075363159, loss=1.3329298496246338
I0211 21:58:10.844774 140281363265280 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.2314186096191406, loss=1.330641746520996
I0211 21:59:33.137881 140280707905280 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.6536439657211304, loss=1.3201472759246826
I0211 22:00:48.326796 140280699512576 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.404283046722412, loss=1.3019579648971558
I0211 22:02:03.644880 140280707905280 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.543480396270752, loss=1.349412202835083
I0211 22:03:19.054465 140280699512576 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.7111071348190308, loss=1.311646819114685
I0211 22:04:35.822700 140280707905280 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.84042489528656, loss=1.286134123802185
I0211 22:06:00.905339 140280699512576 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.148407220840454, loss=1.3124817609786987
I0211 22:07:26.331755 140280707905280 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3898452520370483, loss=1.3374334573745728
I0211 22:08:51.138502 140280699512576 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.9526824951171875, loss=1.2969719171524048
I0211 22:10:17.315700 140280707905280 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.9733335971832275, loss=1.3027666807174683
I0211 22:11:42.730965 140280699512576 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.288217782974243, loss=1.3198304176330566
I0211 22:13:09.360009 140281363265280 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.7968477010726929, loss=1.2580108642578125
I0211 22:14:24.806109 140281354872576 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.39632511138916, loss=1.2801134586334229
I0211 22:15:40.104171 140281363265280 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.00649094581604, loss=1.2927436828613281
I0211 22:16:55.173336 140281354872576 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.3190500736236572, loss=1.2889100313186646
I0211 22:17:16.806555 140437690197824 spec.py:321] Evaluating on the training split.
I0211 22:18:11.788107 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 22:19:03.916635 140437690197824 spec.py:349] Evaluating on the test split.
I0211 22:19:30.613430 140437690197824 submission_runner.py:408] Time since start: 23792.02s, 	Step: 27130, 	{'train/ctc_loss': Array(0.3214634, dtype=float32), 'train/wer': 0.11328561590480599, 'validation/ctc_loss': Array(0.5000843, dtype=float32), 'validation/wer': 0.1443853365129324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27239585, dtype=float32), 'test/wer': 0.08857879877318059, 'test/num_examples': 2472, 'score': 21619.27502799034, 'total_duration': 23792.016908168793, 'accumulated_submission_time': 21619.27502799034, 'accumulated_eval_time': 2170.779187440872, 'accumulated_logging_time': 0.7506082057952881}
I0211 22:19:30.651247 140281363265280 logging_writer.py:48] [27130] accumulated_eval_time=2170.779187, accumulated_logging_time=0.750608, accumulated_submission_time=21619.275028, global_step=27130, preemption_count=0, score=21619.275028, test/ctc_loss=0.2723958492279053, test/num_examples=2472, test/wer=0.088579, total_duration=23792.016908, train/ctc_loss=0.321463406085968, train/wer=0.113286, validation/ctc_loss=0.5000842809677124, validation/num_examples=5348, validation/wer=0.144385
I0211 22:20:24.228904 140281354872576 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.9587926864624023, loss=1.2112231254577637
I0211 22:21:39.499824 140281363265280 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.9936884641647339, loss=1.2615574598312378
I0211 22:22:54.682166 140281354872576 logging_writer.py:48] [27400] global_step=27400, grad_norm=4.252588272094727, loss=1.28074312210083
I0211 22:24:13.215941 140281363265280 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.9075942039489746, loss=1.3411835432052612
I0211 22:25:38.752510 140281354872576 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.353078842163086, loss=1.3171380758285522
I0211 22:27:03.190315 140281363265280 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.6436922550201416, loss=1.267653226852417
I0211 22:28:27.904327 140281354872576 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.7301478385925293, loss=1.3269604444503784
I0211 22:29:47.162108 140281363265280 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.4293129444122314, loss=1.3083919286727905
I0211 22:31:02.521766 140281354872576 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.907213568687439, loss=1.2591352462768555
I0211 22:32:17.604012 140281363265280 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.253246784210205, loss=1.2637220621109009
I0211 22:33:32.807663 140281354872576 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.390775680541992, loss=1.28919517993927
I0211 22:34:52.046272 140281363265280 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.860027551651001, loss=1.220605731010437
I0211 22:36:16.707031 140281354872576 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.0006561279296875, loss=1.2798612117767334
I0211 22:37:41.224457 140281363265280 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.7456042766571045, loss=1.2657183408737183
I0211 22:39:06.254453 140281354872576 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.8239452838897705, loss=1.333204984664917
I0211 22:40:32.226595 140281363265280 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.28863787651062, loss=1.327734351158142
I0211 22:41:57.269307 140281354872576 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.593169689178467, loss=1.3379080295562744
I0211 22:43:19.495762 140281363265280 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.2561299800872803, loss=1.2958183288574219
I0211 22:43:31.277317 140437690197824 spec.py:321] Evaluating on the training split.
I0211 22:44:26.073806 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 22:45:17.021103 140437690197824 spec.py:349] Evaluating on the test split.
I0211 22:45:43.251634 140437690197824 submission_runner.py:408] Time since start: 25364.65s, 	Step: 28917, 	{'train/ctc_loss': Array(0.29604378, dtype=float32), 'train/wer': 0.1013367463026166, 'validation/ctc_loss': Array(0.4922317, dtype=float32), 'validation/wer': 0.14277300945190535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26738203, dtype=float32), 'test/wer': 0.08662888712855199, 'test/num_examples': 2472, 'score': 23059.811488866806, 'total_duration': 25364.654709339142, 'accumulated_submission_time': 23059.811488866806, 'accumulated_eval_time': 2302.746828556061, 'accumulated_logging_time': 0.8038895130157471}
I0211 22:45:43.288964 140281363265280 logging_writer.py:48] [28917] accumulated_eval_time=2302.746829, accumulated_logging_time=0.803890, accumulated_submission_time=23059.811489, global_step=28917, preemption_count=0, score=23059.811489, test/ctc_loss=0.26738202571868896, test/num_examples=2472, test/wer=0.086629, total_duration=25364.654709, train/ctc_loss=0.2960437834262848, train/wer=0.101337, validation/ctc_loss=0.49223169684410095, validation/num_examples=5348, validation/wer=0.142773
I0211 22:46:46.293993 140281354872576 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.8743667602539062, loss=1.2604773044586182
I0211 22:48:01.327863 140281363265280 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.173987865447998, loss=1.2616890668869019
I0211 22:49:16.198757 140281354872576 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.3419580459594727, loss=1.2824536561965942
I0211 22:50:31.238539 140281363265280 logging_writer.py:48] [29300] global_step=29300, grad_norm=4.184690952301025, loss=1.2745437622070312
I0211 22:51:46.364167 140281354872576 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.396205186843872, loss=1.2461771965026855
I0211 22:53:08.831307 140281363265280 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.739803433418274, loss=1.2759571075439453
I0211 22:54:32.980288 140281354872576 logging_writer.py:48] [29600] global_step=29600, grad_norm=5.227293491363525, loss=1.259694218635559
I0211 22:55:57.842981 140281363265280 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.793412208557129, loss=1.2540041208267212
I0211 22:57:21.204128 140281354872576 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.4154040813446045, loss=1.1776129007339478
I0211 22:58:46.078010 140281363265280 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.1491804122924805, loss=1.2467362880706787
I0211 23:00:01.049812 140281354872576 logging_writer.py:48] [30000] global_step=30000, grad_norm=4.947057723999023, loss=1.1983152627944946
I0211 23:01:16.595778 140281363265280 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.2071540355682373, loss=1.2364696264266968
I0211 23:02:31.602842 140281354872576 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.4903931617736816, loss=1.277767539024353
I0211 23:03:46.431227 140281363265280 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.7220417261123657, loss=1.2473419904708862
I0211 23:05:07.850235 140281354872576 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.9669504165649414, loss=1.2148094177246094
I0211 23:06:32.917829 140281363265280 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.998666524887085, loss=1.2872681617736816
I0211 23:07:57.995501 140281354872576 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.074361562728882, loss=1.2583822011947632
I0211 23:09:22.495725 140281363265280 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.388770818710327, loss=1.2941750288009644
I0211 23:09:43.916969 140437690197824 spec.py:321] Evaluating on the training split.
I0211 23:10:39.949415 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 23:11:31.385066 140437690197824 spec.py:349] Evaluating on the test split.
I0211 23:11:57.374426 140437690197824 submission_runner.py:408] Time since start: 26938.78s, 	Step: 30727, 	{'train/ctc_loss': Array(0.2781695, dtype=float32), 'train/wer': 0.09525280944559189, 'validation/ctc_loss': Array(0.48324794, dtype=float32), 'validation/wer': 0.13978972165635228, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25862825, dtype=float32), 'test/wer': 0.08293217963561027, 'test/num_examples': 2472, 'score': 24500.349231004715, 'total_duration': 26938.776789188385, 'accumulated_submission_time': 24500.349231004715, 'accumulated_eval_time': 2436.196858882904, 'accumulated_logging_time': 0.8572819232940674}
I0211 23:11:57.412149 140281363265280 logging_writer.py:48] [30727] accumulated_eval_time=2436.196859, accumulated_logging_time=0.857282, accumulated_submission_time=24500.349231, global_step=30727, preemption_count=0, score=24500.349231, test/ctc_loss=0.258628249168396, test/num_examples=2472, test/wer=0.082932, total_duration=26938.776789, train/ctc_loss=0.27816951274871826, train/wer=0.095253, validation/ctc_loss=0.48324793577194214, validation/num_examples=5348, validation/wer=0.139790
I0211 23:12:53.047564 140281354872576 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.910796046257019, loss=1.2468239068984985
I0211 23:14:11.732528 140281363265280 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.0474507808685303, loss=1.2380093336105347
I0211 23:15:26.734814 140281354872576 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.347522735595703, loss=1.2734471559524536
I0211 23:16:41.964888 140281363265280 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.8778102397918701, loss=1.2324755191802979
I0211 23:17:57.134359 140281354872576 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.4013590812683105, loss=1.1858330965042114
I0211 23:19:12.277881 140281363265280 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.6616718769073486, loss=1.2596243619918823
I0211 23:20:33.158822 140281354872576 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.9396767616271973, loss=1.2359949350357056
I0211 23:21:59.215909 140281363265280 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.655327081680298, loss=1.177846908569336
I0211 23:23:24.447637 140281354872576 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.190945863723755, loss=1.2386773824691772
I0211 23:24:48.982577 140281363265280 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.2371628284454346, loss=1.2275294065475464
I0211 23:26:13.554545 140281354872576 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.9661509990692139, loss=1.262618064880371
I0211 23:27:38.843881 140281363265280 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.1807448863983154, loss=1.1560945510864258
I0211 23:29:00.243470 140281363265280 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.166393280029297, loss=1.2058476209640503
I0211 23:30:15.721207 140281354872576 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.0655741691589355, loss=1.237286925315857
I0211 23:31:30.969406 140281363265280 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.9955612421035767, loss=1.1937206983566284
I0211 23:32:46.430332 140281354872576 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.990582227706909, loss=1.2499197721481323
I0211 23:34:04.964028 140281363265280 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.274930715560913, loss=1.2726705074310303
I0211 23:35:29.565436 140281354872576 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.5147533416748047, loss=1.237037181854248
I0211 23:35:57.595501 140437690197824 spec.py:321] Evaluating on the training split.
I0211 23:36:53.610661 140437690197824 spec.py:333] Evaluating on the validation split.
I0211 23:37:45.194601 140437690197824 spec.py:349] Evaluating on the test split.
I0211 23:38:11.617613 140437690197824 submission_runner.py:408] Time since start: 28513.02s, 	Step: 32535, 	{'train/ctc_loss': Array(0.24887384, dtype=float32), 'train/wer': 0.08832523735924044, 'validation/ctc_loss': Array(0.46815404, dtype=float32), 'validation/wer': 0.13549340104463345, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25137788, dtype=float32), 'test/wer': 0.08238376698555847, 'test/num_examples': 2472, 'score': 25940.439158916473, 'total_duration': 28513.021070718765, 'accumulated_submission_time': 25940.439158916473, 'accumulated_eval_time': 2570.2126417160034, 'accumulated_logging_time': 0.9131293296813965}
I0211 23:38:11.653307 140281363265280 logging_writer.py:48] [32535] accumulated_eval_time=2570.212642, accumulated_logging_time=0.913129, accumulated_submission_time=25940.439159, global_step=32535, preemption_count=0, score=25940.439159, test/ctc_loss=0.2513778805732727, test/num_examples=2472, test/wer=0.082384, total_duration=28513.021071, train/ctc_loss=0.24887384474277496, train/wer=0.088325, validation/ctc_loss=0.46815404295921326, validation/num_examples=5348, validation/wer=0.135493
I0211 23:39:01.258397 140281354872576 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.3547022342681885, loss=1.2369030714035034
I0211 23:40:16.335001 140281363265280 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.377936363220215, loss=1.251474142074585
I0211 23:41:31.120208 140281354872576 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.874706268310547, loss=1.2059364318847656
I0211 23:42:48.882722 140281363265280 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.433342456817627, loss=1.2005784511566162
I0211 23:44:12.208527 140280743745280 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.29453706741333, loss=1.1799368858337402
I0211 23:45:27.200304 140280735352576 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.3867578506469727, loss=1.2621166706085205
I0211 23:46:42.110141 140280743745280 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.945859432220459, loss=1.2295647859573364
I0211 23:47:57.333140 140280735352576 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.346346855163574, loss=1.259230136871338
I0211 23:49:12.344781 140280743745280 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.5953242778778076, loss=1.2091587781906128
I0211 23:50:35.571755 140280735352576 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.0423920154571533, loss=1.230946660041809
I0211 23:52:00.310420 140280743745280 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.775395631790161, loss=1.233385443687439
I0211 23:53:24.943802 140280735352576 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.529987335205078, loss=1.2073533535003662
I0211 23:54:49.948816 140280743745280 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.726827621459961, loss=1.1707597970962524
I0211 23:56:15.292542 140280735352576 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.7414007186889648, loss=1.165848970413208
I0211 23:57:42.552180 140280743745280 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.2352635860443115, loss=1.2055872678756714
I0211 23:58:57.753664 140280735352576 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.9568002223968506, loss=1.2609068155288696
I0212 00:00:12.773319 140280743745280 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.819289207458496, loss=1.1855336427688599
I0212 00:01:27.768483 140280735352576 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.3410747051239014, loss=1.2451236248016357
I0212 00:02:12.007503 140437690197824 spec.py:321] Evaluating on the training split.
I0212 00:03:05.735294 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 00:03:57.037109 140437690197824 spec.py:349] Evaluating on the test split.
I0212 00:04:23.058829 140437690197824 submission_runner.py:408] Time since start: 30084.46s, 	Step: 34360, 	{'train/ctc_loss': Array(0.23843347, dtype=float32), 'train/wer': 0.08565072302558398, 'validation/ctc_loss': Array(0.45768866, dtype=float32), 'validation/wer': 0.1327418249225214, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24266852, dtype=float32), 'test/wer': 0.0781183352629334, 'test/num_examples': 2472, 'score': 27380.701331615448, 'total_duration': 30084.456929922104, 'accumulated_submission_time': 27380.701331615448, 'accumulated_eval_time': 2701.2522921562195, 'accumulated_logging_time': 0.965153694152832}
I0212 00:04:23.098487 140280856385280 logging_writer.py:48] [34360] accumulated_eval_time=2701.252292, accumulated_logging_time=0.965154, accumulated_submission_time=27380.701332, global_step=34360, preemption_count=0, score=27380.701332, test/ctc_loss=0.2426685243844986, test/num_examples=2472, test/wer=0.078118, total_duration=30084.456930, train/ctc_loss=0.23843346536159515, train/wer=0.085651, validation/ctc_loss=0.45768865942955017, validation/num_examples=5348, validation/wer=0.132742
I0212 00:04:54.077483 140280847992576 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.4120380878448486, loss=1.193703293800354
I0212 00:06:09.415178 140280856385280 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.5503039360046387, loss=1.1865040063858032
I0212 00:07:25.193413 140280847992576 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.1371400356292725, loss=1.205849289894104
I0212 00:08:40.804374 140280856385280 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.5047125816345215, loss=1.189812183380127
I0212 00:10:06.210335 140280847992576 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.2013041973114014, loss=1.1742618083953857
I0212 00:11:30.043132 140280856385280 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.954566478729248, loss=1.247994065284729
I0212 00:12:54.561215 140280847992576 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.730921983718872, loss=1.2196093797683716
I0212 00:14:14.907411 140280856385280 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.857734203338623, loss=1.2060757875442505
I0212 00:15:30.331763 140280847992576 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.3643643856048584, loss=1.154367446899414
I0212 00:16:45.284697 140280856385280 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.2400734424591064, loss=1.1409661769866943
I0212 00:18:00.369827 140280847992576 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.7991697788238525, loss=1.1407395601272583
I0212 00:19:15.452309 140280856385280 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.0792975425720215, loss=1.2110350131988525
I0212 00:20:39.990499 140280847992576 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.964099645614624, loss=1.1625059843063354
I0212 00:22:04.588432 140280856385280 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.637451648712158, loss=1.1915682554244995
I0212 00:23:28.881666 140280847992576 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.7224111557006836, loss=1.1326818466186523
I0212 00:24:54.268739 140280856385280 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.91866934299469, loss=1.1754990816116333
I0212 00:26:18.717802 140280847992576 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.7415287494659424, loss=1.1885958909988403
I0212 00:27:42.565868 140280856385280 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7507392168045044, loss=1.1602197885513306
I0212 00:28:23.644547 140437690197824 spec.py:321] Evaluating on the training split.
I0212 00:29:19.491863 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 00:30:10.077355 140437690197824 spec.py:349] Evaluating on the test split.
I0212 00:30:36.176119 140437690197824 submission_runner.py:408] Time since start: 31657.58s, 	Step: 36156, 	{'train/ctc_loss': Array(0.22635128, dtype=float32), 'train/wer': 0.08115605409302812, 'validation/ctc_loss': Array(0.4513795, dtype=float32), 'validation/wer': 0.13103295133089393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23776175, dtype=float32), 'test/wer': 0.07759023419251315, 'test/num_examples': 2472, 'score': 28821.154938220978, 'total_duration': 31657.57952785492, 'accumulated_submission_time': 28821.154938220978, 'accumulated_eval_time': 2833.7774863243103, 'accumulated_logging_time': 1.021730661392212}
I0212 00:30:36.217342 140281071425280 logging_writer.py:48] [36156] accumulated_eval_time=2833.777486, accumulated_logging_time=1.021731, accumulated_submission_time=28821.154938, global_step=36156, preemption_count=0, score=28821.154938, test/ctc_loss=0.2377617508172989, test/num_examples=2472, test/wer=0.077590, total_duration=31657.579528, train/ctc_loss=0.2263512760400772, train/wer=0.081156, validation/ctc_loss=0.4513795077800751, validation/num_examples=5348, validation/wer=0.131033
I0212 00:31:09.995781 140281063032576 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.686185598373413, loss=1.171797513961792
I0212 00:32:24.984961 140281071425280 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.6874804496765137, loss=1.1631193161010742
I0212 00:33:39.879467 140281063032576 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.8600566387176514, loss=1.1978353261947632
I0212 00:34:54.940277 140281071425280 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.676574468612671, loss=1.1801562309265137
I0212 00:36:10.267918 140281063032576 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.019138813018799, loss=1.2006467580795288
I0212 00:37:25.442330 140281071425280 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.5787386894226074, loss=1.152748942375183
I0212 00:38:48.746329 140281063032576 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.0800552368164062, loss=1.1231458187103271
I0212 00:40:13.278015 140281071425280 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.9881800413131714, loss=1.1253459453582764
I0212 00:41:37.157377 140281063032576 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.6534671783447266, loss=1.1793804168701172
I0212 00:43:03.885548 140280743745280 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.414318561553955, loss=1.1662266254425049
I0212 00:44:18.905836 140280735352576 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.4640417098999023, loss=1.1678262948989868
I0212 00:45:34.061022 140280743745280 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.5064194202423096, loss=1.1839253902435303
I0212 00:46:49.219657 140280735352576 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.928022027015686, loss=1.174633502960205
I0212 00:48:04.402202 140280743745280 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.561441421508789, loss=1.126199722290039
I0212 00:49:19.688545 140280735352576 logging_writer.py:48] [37600] global_step=37600, grad_norm=4.564932823181152, loss=1.1961573362350464
I0212 00:50:40.471089 140280743745280 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.2035772800445557, loss=1.2247453927993774
I0212 00:52:04.714986 140280735352576 logging_writer.py:48] [37800] global_step=37800, grad_norm=5.325834274291992, loss=1.151851773262024
I0212 00:53:29.298577 140280743745280 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.1552231311798096, loss=1.148534893989563
I0212 00:54:36.636114 140437690197824 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0212 00:55:53.986411 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 00:56:45.367954 140437690197824 spec.py:349] Evaluating on the test split.
I0212 00:57:11.679189 140437690197824 submission_runner.py:408] Time since start: 33253.08s, 	Step: 37981, 	{'train/ctc_loss': Array(0.13815072, dtype=float32), 'train/wer': 0.0506929093691549, 'validation/ctc_loss': Array(0.4403994, dtype=float32), 'validation/wer': 0.12775036929047956, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2351409, dtype=float32), 'test/wer': 0.07588406150346312, 'test/num_examples': 2472, 'score': 30261.48275089264, 'total_duration': 33253.08378100395, 'accumulated_submission_time': 30261.48275089264, 'accumulated_eval_time': 2988.8153936862946, 'accumulated_logging_time': 1.078145980834961}
I0212 00:57:11.716890 140280743745280 logging_writer.py:48] [37981] accumulated_eval_time=2988.815394, accumulated_logging_time=1.078146, accumulated_submission_time=30261.482751, global_step=37981, preemption_count=0, score=30261.482751, test/ctc_loss=0.23514090478420258, test/num_examples=2472, test/wer=0.075884, total_duration=33253.083781, train/ctc_loss=0.13815072178840637, train/wer=0.050693, validation/ctc_loss=0.4403994083404541, validation/num_examples=5348, validation/wer=0.127750
I0212 00:57:26.875674 140280735352576 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.840667247772217, loss=1.1942657232284546
I0212 00:58:41.937387 140280743745280 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.8016934394836426, loss=1.1572556495666504
I0212 01:00:00.734249 140281071425280 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.219133138656616, loss=1.1758373975753784
I0212 01:01:16.046221 140281063032576 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.6824166774749756, loss=1.1792935132980347
I0212 01:02:31.459146 140281071425280 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.197925329208374, loss=1.2254527807235718
I0212 01:03:46.725386 140281063032576 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.4643313884735107, loss=1.1610153913497925
I0212 01:05:07.392279 140281071425280 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.000121593475342, loss=1.1789501905441284
I0212 01:06:31.208833 140281063032576 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.4066832065582275, loss=1.1617045402526855
I0212 01:07:55.285835 140281071425280 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.9592751264572144, loss=1.1479582786560059
I0212 01:09:20.285166 140281063032576 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.901694893836975, loss=1.17897367477417
I0212 01:10:45.987504 140281071425280 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.292210578918457, loss=1.1729614734649658
I0212 01:12:10.177371 140281063032576 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.3913285732269287, loss=1.139302134513855
I0212 01:13:32.238443 140281071425280 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.8740150928497314, loss=1.1629971265792847
I0212 01:14:47.296351 140281063032576 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.9434276819229126, loss=1.1034984588623047
I0212 01:16:02.286294 140281071425280 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.22255277633667, loss=1.1592931747436523
I0212 01:17:17.238257 140281063032576 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.764329195022583, loss=1.1364964246749878
I0212 01:18:32.269397 140281071425280 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.9059524536132812, loss=1.1920859813690186
I0212 01:19:55.396976 140281063032576 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.206453800201416, loss=1.1355043649673462
I0212 01:21:12.364309 140437690197824 spec.py:321] Evaluating on the training split.
I0212 01:22:09.719698 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 01:23:02.634982 140437690197824 spec.py:349] Evaluating on the test split.
I0212 01:23:28.588202 140437690197824 submission_runner.py:408] Time since start: 34829.99s, 	Step: 39793, 	{'train/ctc_loss': Array(0.13410744, dtype=float32), 'train/wer': 0.047232886390673946, 'validation/ctc_loss': Array(0.43591997, dtype=float32), 'validation/wer': 0.12621527945393282, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23304227, dtype=float32), 'test/wer': 0.0750715983182012, 'test/num_examples': 2472, 'score': 31702.03856253624, 'total_duration': 34829.99032139778, 'accumulated_submission_time': 31702.03856253624, 'accumulated_eval_time': 3125.0316421985626, 'accumulated_logging_time': 1.131788969039917}
I0212 01:23:28.628299 140281071425280 logging_writer.py:48] [39793] accumulated_eval_time=3125.031642, accumulated_logging_time=1.131789, accumulated_submission_time=31702.038563, global_step=39793, preemption_count=0, score=31702.038563, test/ctc_loss=0.23304226994514465, test/num_examples=2472, test/wer=0.075072, total_duration=34829.990321, train/ctc_loss=0.13410744071006775, train/wer=0.047233, validation/ctc_loss=0.43591997027397156, validation/num_examples=5348, validation/wer=0.126215
I0212 01:23:34.802995 140281063032576 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.907356023788452, loss=1.1973203420639038
I0212 01:24:49.885279 140281071425280 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.048090934753418, loss=1.1628382205963135
I0212 01:26:05.021775 140281063032576 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.8332582712173462, loss=1.096993088722229
I0212 01:27:20.573510 140281071425280 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.9743787050247192, loss=1.1224185228347778
I0212 01:28:44.864435 140281071425280 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.8556668758392334, loss=1.123216986656189
I0212 01:30:00.305527 140281063032576 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.030543327331543, loss=1.103610634803772
I0212 01:31:15.325886 140281071425280 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.7781524658203125, loss=1.1739081144332886
I0212 01:32:30.277515 140281063032576 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.972782850265503, loss=1.1549925804138184
I0212 01:33:46.567011 140281071425280 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.9829493761062622, loss=1.1526601314544678
I0212 01:35:09.186070 140281063032576 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.8708661794662476, loss=1.143985629081726
I0212 01:36:34.120210 140281071425280 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.0030763149261475, loss=1.1044081449508667
I0212 01:37:59.206679 140281063032576 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.3224613666534424, loss=1.1589380502700806
I0212 01:39:23.115729 140281071425280 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.0908546447753906, loss=1.1112221479415894
I0212 01:40:48.126188 140281063032576 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.6537351608276367, loss=1.1191810369491577
I0212 01:42:17.092032 140280743745280 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.2420289516448975, loss=1.151583194732666
I0212 01:43:32.193372 140280735352576 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.1310276985168457, loss=1.109669804573059
I0212 01:44:47.142854 140280743745280 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.5031089782714844, loss=1.146195411682129
I0212 01:46:02.309669 140280735352576 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.309084177017212, loss=1.1375383138656616
I0212 01:47:17.599740 140280743745280 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.3289501667022705, loss=1.1331337690353394
I0212 01:47:28.747365 140437690197824 spec.py:321] Evaluating on the training split.
I0212 01:48:26.000127 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 01:49:17.268501 140437690197824 spec.py:349] Evaluating on the test split.
I0212 01:49:43.376990 140437690197824 submission_runner.py:408] Time since start: 36404.78s, 	Step: 41616, 	{'train/ctc_loss': Array(0.14266028, dtype=float32), 'train/wer': 0.05091043632231296, 'validation/ctc_loss': Array(0.4308139, dtype=float32), 'validation/wer': 0.1242457302296842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22871488, dtype=float32), 'test/wer': 0.07320293299209879, 'test/num_examples': 2472, 'score': 33142.06616973877, 'total_duration': 36404.780586481094, 'accumulated_submission_time': 33142.06616973877, 'accumulated_eval_time': 3259.6550998687744, 'accumulated_logging_time': 1.1871211528778076}
I0212 01:49:43.415978 140281363265280 logging_writer.py:48] [41616] accumulated_eval_time=3259.655100, accumulated_logging_time=1.187121, accumulated_submission_time=33142.066170, global_step=41616, preemption_count=0, score=33142.066170, test/ctc_loss=0.22871488332748413, test/num_examples=2472, test/wer=0.073203, total_duration=36404.780586, train/ctc_loss=0.14266027510166168, train/wer=0.050910, validation/ctc_loss=0.43081390857696533, validation/num_examples=5348, validation/wer=0.124246
I0212 01:50:47.445612 140281354872576 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.356257200241089, loss=1.0695428848266602
I0212 01:52:02.732695 140281363265280 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.414691209793091, loss=1.0782967805862427
I0212 01:53:17.824161 140281354872576 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.18560528755188, loss=1.1255398988723755
I0212 01:54:37.921985 140281363265280 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9609664678573608, loss=1.1601455211639404
I0212 01:56:02.156284 140281354872576 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.593477249145508, loss=1.1480984687805176
I0212 01:57:26.302298 140281363265280 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.035999059677124, loss=1.1535699367523193
I0212 01:58:47.402475 140281363265280 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.441736936569214, loss=1.139743447303772
I0212 02:00:02.365601 140281354872576 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.184516191482544, loss=1.1164264678955078
I0212 02:01:17.384022 140281363265280 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.0572826862335205, loss=1.075451374053955
I0212 02:02:32.457299 140281354872576 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.724914073944092, loss=1.1899691820144653
I0212 02:03:50.007917 140281363265280 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.9430763721466064, loss=1.1248410940170288
I0212 02:05:14.659108 140281354872576 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.0197913646698, loss=1.115083932876587
I0212 02:06:39.461667 140281363265280 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.532193183898926, loss=1.1305729150772095
I0212 02:08:04.592178 140281354872576 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.172600746154785, loss=1.1038998365402222
I0212 02:09:29.066939 140281363265280 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.3670802116394043, loss=1.141448974609375
I0212 02:10:53.569974 140281354872576 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.4524056911468506, loss=1.1750776767730713
I0212 02:12:18.392413 140280707905280 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.5557217597961426, loss=1.1497719287872314
I0212 02:13:33.435244 140280699512576 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.4587087631225586, loss=1.1287198066711426
I0212 02:13:43.797509 140437690197824 spec.py:321] Evaluating on the training split.
I0212 02:14:41.740923 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 02:15:34.175074 140437690197824 spec.py:349] Evaluating on the test split.
I0212 02:16:00.160047 140437690197824 submission_runner.py:408] Time since start: 37981.56s, 	Step: 43415, 	{'train/ctc_loss': Array(0.12480412, dtype=float32), 'train/wer': 0.0457206361131981, 'validation/ctc_loss': Array(0.42841774, dtype=float32), 'validation/wer': 0.12410091043378356, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2267602, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 34582.3564324379, 'total_duration': 37981.5632686615, 'accumulated_submission_time': 34582.3564324379, 'accumulated_eval_time': 3396.0110619068146, 'accumulated_logging_time': 1.2427010536193848}
I0212 02:16:00.202251 140280779585280 logging_writer.py:48] [43415] accumulated_eval_time=3396.011062, accumulated_logging_time=1.242701, accumulated_submission_time=34582.356432, global_step=43415, preemption_count=0, score=34582.356432, test/ctc_loss=0.22676019370555878, test/num_examples=2472, test/wer=0.072715, total_duration=37981.563269, train/ctc_loss=0.12480412423610687, train/wer=0.045721, validation/ctc_loss=0.42841774225234985, validation/num_examples=5348, validation/wer=0.124101
I0212 02:17:04.760636 140280771192576 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.3521196842193604, loss=1.1165701150894165
I0212 02:18:19.889801 140280779585280 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.603381633758545, loss=1.14922297000885
I0212 02:19:35.005673 140280771192576 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.229865074157715, loss=1.1397840976715088
I0212 02:20:50.228565 140280779585280 logging_writer.py:48] [43800] global_step=43800, grad_norm=4.970867156982422, loss=1.1402665376663208
I0212 02:22:11.067282 140280771192576 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.465156316757202, loss=1.1284675598144531
I0212 02:23:36.247516 140280779585280 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.8197672367095947, loss=1.1287362575531006
I0212 02:25:00.738976 140280771192576 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6448602676391602, loss=1.1159263849258423
I0212 02:26:26.167965 140280779585280 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.425328493118286, loss=1.0376023054122925
I0212 02:27:53.630300 140280779585280 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.0130863189697266, loss=1.1797125339508057
I0212 02:29:08.558225 140280771192576 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.103473663330078, loss=1.1336151361465454
I0212 02:30:23.469969 140280779585280 logging_writer.py:48] [44500] global_step=44500, grad_norm=6.7117438316345215, loss=1.103002667427063
I0212 02:31:38.331331 140280771192576 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.748230218887329, loss=1.1424510478973389
I0212 02:32:54.600452 140280779585280 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.9299943447113037, loss=1.1189926862716675
I0212 02:34:18.697706 140280771192576 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.659147262573242, loss=1.122254729270935
I0212 02:35:44.339827 140280779585280 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.8166592121124268, loss=1.1347935199737549
I0212 02:37:09.888464 140280771192576 logging_writer.py:48] [45000] global_step=45000, grad_norm=5.024527549743652, loss=1.1558091640472412
I0212 02:38:35.180029 140280779585280 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.0316221714019775, loss=1.1205179691314697
I0212 02:40:00.373679 140280771192576 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.044473648071289, loss=1.0907015800476074
I0212 02:40:00.379715 140437690197824 spec.py:321] Evaluating on the training split.
I0212 02:40:58.432464 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 02:41:49.758508 140437690197824 spec.py:349] Evaluating on the test split.
I0212 02:42:15.785432 140437690197824 submission_runner.py:408] Time since start: 39557.19s, 	Step: 45201, 	{'train/ctc_loss': Array(0.13416459, dtype=float32), 'train/wer': 0.04780056618560438, 'validation/ctc_loss': Array(0.42583966, dtype=float32), 'validation/wer': 0.12288442414821824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22566003, dtype=float32), 'test/wer': 0.07285763613836248, 'test/num_examples': 2472, 'score': 36022.44448566437, 'total_duration': 39557.187906980515, 'accumulated_submission_time': 36022.44448566437, 'accumulated_eval_time': 3531.409448623657, 'accumulated_logging_time': 1.2997477054595947}
I0212 02:42:15.825268 140280779585280 logging_writer.py:48] [45201] accumulated_eval_time=3531.409449, accumulated_logging_time=1.299748, accumulated_submission_time=36022.444486, global_step=45201, preemption_count=0, score=36022.444486, test/ctc_loss=0.2256600260734558, test/num_examples=2472, test/wer=0.072858, total_duration=39557.187907, train/ctc_loss=0.13416458666324615, train/wer=0.047801, validation/ctc_loss=0.4258396625518799, validation/num_examples=5348, validation/wer=0.122884
I0212 02:43:30.539454 140280771192576 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.5277488231658936, loss=1.0696641206741333
I0212 02:44:49.178115 140280779585280 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.8897532224655151, loss=1.1023447513580322
I0212 02:46:04.271178 140280771192576 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.450291156768799, loss=1.066914677619934
I0212 02:47:19.421813 140280779585280 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.301150321960449, loss=1.0700334310531616
I0212 02:48:35.182026 140280771192576 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.980929136276245, loss=1.0913195610046387
I0212 02:49:58.811495 140280779585280 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.858083963394165, loss=1.061623454093933
I0212 02:51:24.091349 140280771192576 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.8326764106750488, loss=1.1172220706939697
I0212 02:52:49.532051 140280779585280 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.5069639682769775, loss=1.1065161228179932
I0212 02:54:15.683661 140280771192576 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.9190447330474854, loss=1.0937767028808594
I0212 02:55:40.953628 140280779585280 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.307570695877075, loss=1.1550014019012451
I0212 02:57:07.005964 140280771192576 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.608276605606079, loss=1.1076018810272217
I0212 02:58:31.336751 140280779585280 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.497053623199463, loss=1.1083390712738037
I0212 02:59:46.330256 140280771192576 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.8709967136383057, loss=1.1402348279953003
I0212 03:01:01.438004 140280779585280 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.3136050701141357, loss=1.0616430044174194
I0212 03:02:16.689888 140280771192576 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.7416610717773438, loss=1.1237871646881104
I0212 03:03:34.380900 140280779585280 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.7368557453155518, loss=1.118796706199646
I0212 03:04:59.947253 140280771192576 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.0823609828948975, loss=1.0612542629241943
I0212 03:06:16.512245 140437690197824 spec.py:321] Evaluating on the training split.
I0212 03:07:13.721193 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 03:08:06.310325 140437690197824 spec.py:349] Evaluating on the test split.
I0212 03:08:32.995337 140437690197824 submission_runner.py:408] Time since start: 41134.40s, 	Step: 46991, 	{'train/ctc_loss': Array(0.12691087, dtype=float32), 'train/wer': 0.04664109331424236, 'validation/ctc_loss': Array(0.42552513, dtype=float32), 'validation/wer': 0.12275891365843769, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22541308, dtype=float32), 'test/wer': 0.07226860032904758, 'test/num_examples': 2472, 'score': 37463.04101443291, 'total_duration': 41134.39881324768, 'accumulated_submission_time': 37463.04101443291, 'accumulated_eval_time': 3667.8862357139587, 'accumulated_logging_time': 1.3547301292419434}
I0212 03:08:33.040644 140281363265280 logging_writer.py:48] [46991] accumulated_eval_time=3667.886236, accumulated_logging_time=1.354730, accumulated_submission_time=37463.041014, global_step=46991, preemption_count=0, score=37463.041014, test/ctc_loss=0.22541308403015137, test/num_examples=2472, test/wer=0.072269, total_duration=41134.398813, train/ctc_loss=0.12691086530685425, train/wer=0.046641, validation/ctc_loss=0.42552512884140015, validation/num_examples=5348, validation/wer=0.122759
I0212 03:08:40.693589 140281354872576 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.5369205474853516, loss=1.1377493143081665
I0212 03:09:55.812497 140281363265280 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.0934603214263916, loss=1.1525051593780518
I0212 03:11:11.308875 140281354872576 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.7920522689819336, loss=1.1348447799682617
I0212 03:12:26.852266 140281363265280 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.7602729797363281, loss=1.0908843278884888
I0212 03:13:53.285261 140280707905280 logging_writer.py:48] [47400] global_step=47400, grad_norm=5.635400772094727, loss=1.1535329818725586
I0212 03:15:08.450896 140280699512576 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.741837024688721, loss=1.1389966011047363
I0212 03:16:23.654012 140280707905280 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.44033145904541, loss=1.2013888359069824
I0212 03:17:38.955783 140280699512576 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.421898603439331, loss=1.0489776134490967
I0212 03:18:56.018360 140280707905280 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.2872188091278076, loss=1.1193805932998657
I0212 03:20:21.288431 140280699512576 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.7132015228271484, loss=1.0743777751922607
I0212 03:21:45.293064 140437690197824 spec.py:321] Evaluating on the training split.
I0212 03:22:42.613020 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 03:23:34.454323 140437690197824 spec.py:349] Evaluating on the test split.
I0212 03:24:00.955678 140437690197824 submission_runner.py:408] Time since start: 42062.36s, 	Step: 48000, 	{'train/ctc_loss': Array(0.15317775, dtype=float32), 'train/wer': 0.05205918963001116, 'validation/ctc_loss': Array(0.4256426, dtype=float32), 'validation/wer': 0.12272994969925756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22542043, dtype=float32), 'test/wer': 0.07234984664757378, 'test/num_examples': 2472, 'score': 38255.23158955574, 'total_duration': 42062.36226296425, 'accumulated_submission_time': 38255.23158955574, 'accumulated_eval_time': 3803.54567193985, 'accumulated_logging_time': 1.418626070022583}
I0212 03:24:00.987411 140281363265280 logging_writer.py:48] [48000] accumulated_eval_time=3803.545672, accumulated_logging_time=1.418626, accumulated_submission_time=38255.231590, global_step=48000, preemption_count=0, score=38255.231590, test/ctc_loss=0.22542043030261993, test/num_examples=2472, test/wer=0.072350, total_duration=42062.362263, train/ctc_loss=0.15317775309085846, train/wer=0.052059, validation/ctc_loss=0.42564260959625244, validation/num_examples=5348, validation/wer=0.122730
I0212 03:24:01.018435 140281354872576 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=38255.231590
I0212 03:24:01.219894 140437690197824 checkpoints.py:490] Saving checkpoint at step: 48000
I0212 03:24:02.218995 140437690197824 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_2/checkpoint_48000
I0212 03:24:02.239571 140437690197824 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_2/checkpoint_48000.
I0212 03:24:03.572824 140437690197824 submission_runner.py:583] Tuning trial 2/5
I0212 03:24:03.573181 140437690197824 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0212 03:24:03.588840 140437690197824 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.25542, dtype=float32), 'train/wer': 3.4835125993757408, 'validation/ctc_loss': Array(30.891924, dtype=float32), 'validation/wer': 3.325159060409164, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942331, dtype=float32), 'test/wer': 3.6988402088030385, 'test/num_examples': 2472, 'score': 16.34257674217224, 'total_duration': 209.07660150527954, 'accumulated_submission_time': 16.34257674217224, 'accumulated_eval_time': 192.73394989967346, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1812, {'train/ctc_loss': Array(6.4745483, dtype=float32), 'train/wer': 0.9413882809148034, 'validation/ctc_loss': Array(6.38616, dtype=float32), 'validation/wer': 0.8965214285024667, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.340809, dtype=float32), 'test/wer': 0.899396746084943, 'test/num_examples': 2472, 'score': 1456.8843939304352, 'total_duration': 1764.5923347473145, 'accumulated_submission_time': 1456.8843939304352, 'accumulated_eval_time': 307.60274863243103, 'accumulated_logging_time': 0.028884410858154297, 'global_step': 1812, 'preemption_count': 0}), (3641, {'train/ctc_loss': Array(3.3249242, dtype=float32), 'train/wer': 0.7415177804500945, 'validation/ctc_loss': Array(3.0489736, dtype=float32), 'validation/wer': 0.6717804145708024, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.53634, dtype=float32), 'test/wer': 0.5975869843397721, 'test/num_examples': 2472, 'score': 2897.0594935417175, 'total_duration': 3326.6420726776123, 'accumulated_submission_time': 2897.0594935417175, 'accumulated_eval_time': 429.35713052749634, 'accumulated_logging_time': 0.06998872756958008, 'global_step': 3641, 'preemption_count': 0}), (5468, {'train/ctc_loss': Array(0.7368655, dtype=float32), 'train/wer': 0.23889931546230703, 'validation/ctc_loss': Array(0.8666504, dtype=float32), 'validation/wer': 0.2528746729486276, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.56013215, dtype=float32), 'test/wer': 0.18442914305445535, 'test/num_examples': 2472, 'score': 4337.671471118927, 'total_duration': 4900.35649228096, 'accumulated_submission_time': 4337.671471118927, 'accumulated_eval_time': 562.3317792415619, 'accumulated_logging_time': 0.11525559425354004, 'global_step': 5468, 'preemption_count': 0}), (7267, {'train/ctc_loss': Array(0.63280827, dtype=float32), 'train/wer': 0.20651483781918564, 'validation/ctc_loss': Array(0.77585036, dtype=float32), 'validation/wer': 0.22672987246203308, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47255045, dtype=float32), 'test/wer': 0.15680539475555014, 'test/num_examples': 2472, 'score': 5777.897728443146, 'total_duration': 6476.492695569992, 'accumulated_submission_time': 5777.897728443146, 'accumulated_eval_time': 698.1108963489532, 'accumulated_logging_time': 0.1686549186706543, 'global_step': 7267, 'preemption_count': 0}), (9075, {'train/ctc_loss': Array(0.5822703, dtype=float32), 'train/wer': 0.19117933221157699, 'validation/ctc_loss': Array(0.71564424, dtype=float32), 'validation/wer': 0.2078743350357705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42079455, dtype=float32), 'test/wer': 0.13990616050210225, 'test/num_examples': 2472, 'score': 7217.773640632629, 'total_duration': 8050.689073801041, 'accumulated_submission_time': 7217.773640632629, 'accumulated_eval_time': 832.2964282035828, 'accumulated_logging_time': 0.2222280502319336, 'global_step': 9075, 'preemption_count': 0}), (10886, {'train/ctc_loss': Array(0.5224885, dtype=float32), 'train/wer': 0.1712662956468705, 'validation/ctc_loss': Array(0.66655123, dtype=float32), 'validation/wer': 0.19406818115991, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3917676, dtype=float32), 'test/wer': 0.12985192858448602, 'test/num_examples': 2472, 'score': 8657.661759138107, 'total_duration': 9625.975238323212, 'accumulated_submission_time': 8657.661759138107, 'accumulated_eval_time': 967.5596957206726, 'accumulated_logging_time': 0.2755579948425293, 'global_step': 10886, 'preemption_count': 0}), (12704, {'train/ctc_loss': Array(0.47296602, dtype=float32), 'train/wer': 0.15854574394153384, 'validation/ctc_loss': Array(0.6403958, dtype=float32), 'validation/wer': 0.1855720864670728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36725986, dtype=float32), 'test/wer': 0.12052891353360551, 'test/num_examples': 2472, 'score': 10097.6127679348, 'total_duration': 11201.28601717949, 'accumulated_submission_time': 10097.6127679348, 'accumulated_eval_time': 1102.7841680049896, 'accumulated_logging_time': 0.329695463180542, 'global_step': 12704, 'preemption_count': 0}), (14498, {'train/ctc_loss': Array(0.43832353, dtype=float32), 'train/wer': 0.14844142211674502, 'validation/ctc_loss': Array(0.60433036, dtype=float32), 'validation/wer': 0.17553124728462882, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3434072, dtype=float32), 'test/wer': 0.11447606280340422, 'test/num_examples': 2472, 'score': 11538.042315006256, 'total_duration': 12773.159367084503, 'accumulated_submission_time': 11538.042315006256, 'accumulated_eval_time': 1234.0966272354126, 'accumulated_logging_time': 0.3797941207885742, 'global_step': 14498, 'preemption_count': 0}), (16299, {'train/ctc_loss': Array(0.42478886, dtype=float32), 'train/wer': 0.14485225193054502, 'validation/ctc_loss': Array(0.6073105, dtype=float32), 'validation/wer': 0.1771242650395358, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34683496, dtype=float32), 'test/wer': 0.11295269433103812, 'test/num_examples': 2472, 'score': 12978.146248579025, 'total_duration': 14348.110781908035, 'accumulated_submission_time': 12978.146248579025, 'accumulated_eval_time': 1368.8134655952454, 'accumulated_logging_time': 0.42960095405578613, 'global_step': 16299, 'preemption_count': 0}), (18099, {'train/ctc_loss': Array(0.41303357, dtype=float32), 'train/wer': 0.1393726834323064, 'validation/ctc_loss': Array(0.57551175, dtype=float32), 'validation/wer': 0.16836749471407744, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3229248, dtype=float32), 'test/wer': 0.10594519935815408, 'test/num_examples': 2472, 'score': 14418.65439915657, 'total_duration': 15922.452051639557, 'accumulated_submission_time': 14418.65439915657, 'accumulated_eval_time': 1502.5155773162842, 'accumulated_logging_time': 0.478834867477417, 'global_step': 18099, 'preemption_count': 0}), (19912, {'train/ctc_loss': Array(0.37763608, dtype=float32), 'train/wer': 0.12719974955837302, 'validation/ctc_loss': Array(0.54934216, dtype=float32), 'validation/wer': 0.1614644177761472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31086192, dtype=float32), 'test/wer': 0.10172039079479211, 'test/num_examples': 2472, 'score': 15858.754093170166, 'total_duration': 17497.57366490364, 'accumulated_submission_time': 15858.754093170166, 'accumulated_eval_time': 1637.3995015621185, 'accumulated_logging_time': 0.5337679386138916, 'global_step': 19912, 'preemption_count': 0}), (21698, {'train/ctc_loss': Array(0.36806503, dtype=float32), 'train/wer': 0.1274199439912334, 'validation/ctc_loss': Array(0.5384376, dtype=float32), 'validation/wer': 0.15590333761356287, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30279443, dtype=float32), 'test/wer': 0.09853147279263909, 'test/num_examples': 2472, 'score': 17298.95994949341, 'total_duration': 19071.027629852295, 'accumulated_submission_time': 17298.95994949341, 'accumulated_eval_time': 1770.5106179714203, 'accumulated_logging_time': 0.5898373126983643, 'global_step': 21698, 'preemption_count': 0}), (23505, {'train/ctc_loss': Array(0.32012546, dtype=float32), 'train/wer': 0.10897084348656946, 'validation/ctc_loss': Array(0.52728623, dtype=float32), 'validation/wer': 0.15230215202216707, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2927988, dtype=float32), 'test/wer': 0.09410354843296163, 'test/num_examples': 2472, 'score': 18738.897290468216, 'total_duration': 20644.312327861786, 'accumulated_submission_time': 18738.897290468216, 'accumulated_eval_time': 1903.7190117835999, 'accumulated_logging_time': 0.6465094089508057, 'global_step': 23505, 'preemption_count': 0}), (25316, {'train/ctc_loss': Array(0.28483084, dtype=float32), 'train/wer': 0.09780632444648349, 'validation/ctc_loss': Array(0.5107498, dtype=float32), 'validation/wer': 0.14767757320640681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2822816, dtype=float32), 'test/wer': 0.09306765787175268, 'test/num_examples': 2472, 'score': 20178.92725777626, 'total_duration': 22217.73483490944, 'accumulated_submission_time': 20178.92725777626, 'accumulated_eval_time': 2036.9786303043365, 'accumulated_logging_time': 0.6984333992004395, 'global_step': 25316, 'preemption_count': 0}), (27130, {'train/ctc_loss': Array(0.3214634, dtype=float32), 'train/wer': 0.11328561590480599, 'validation/ctc_loss': Array(0.5000843, dtype=float32), 'validation/wer': 0.1443853365129324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27239585, dtype=float32), 'test/wer': 0.08857879877318059, 'test/num_examples': 2472, 'score': 21619.27502799034, 'total_duration': 23792.016908168793, 'accumulated_submission_time': 21619.27502799034, 'accumulated_eval_time': 2170.779187440872, 'accumulated_logging_time': 0.7506082057952881, 'global_step': 27130, 'preemption_count': 0}), (28917, {'train/ctc_loss': Array(0.29604378, dtype=float32), 'train/wer': 0.1013367463026166, 'validation/ctc_loss': Array(0.4922317, dtype=float32), 'validation/wer': 0.14277300945190535, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26738203, dtype=float32), 'test/wer': 0.08662888712855199, 'test/num_examples': 2472, 'score': 23059.811488866806, 'total_duration': 25364.654709339142, 'accumulated_submission_time': 23059.811488866806, 'accumulated_eval_time': 2302.746828556061, 'accumulated_logging_time': 0.8038895130157471, 'global_step': 28917, 'preemption_count': 0}), (30727, {'train/ctc_loss': Array(0.2781695, dtype=float32), 'train/wer': 0.09525280944559189, 'validation/ctc_loss': Array(0.48324794, dtype=float32), 'validation/wer': 0.13978972165635228, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25862825, dtype=float32), 'test/wer': 0.08293217963561027, 'test/num_examples': 2472, 'score': 24500.349231004715, 'total_duration': 26938.776789188385, 'accumulated_submission_time': 24500.349231004715, 'accumulated_eval_time': 2436.196858882904, 'accumulated_logging_time': 0.8572819232940674, 'global_step': 30727, 'preemption_count': 0}), (32535, {'train/ctc_loss': Array(0.24887384, dtype=float32), 'train/wer': 0.08832523735924044, 'validation/ctc_loss': Array(0.46815404, dtype=float32), 'validation/wer': 0.13549340104463345, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25137788, dtype=float32), 'test/wer': 0.08238376698555847, 'test/num_examples': 2472, 'score': 25940.439158916473, 'total_duration': 28513.021070718765, 'accumulated_submission_time': 25940.439158916473, 'accumulated_eval_time': 2570.2126417160034, 'accumulated_logging_time': 0.9131293296813965, 'global_step': 32535, 'preemption_count': 0}), (34360, {'train/ctc_loss': Array(0.23843347, dtype=float32), 'train/wer': 0.08565072302558398, 'validation/ctc_loss': Array(0.45768866, dtype=float32), 'validation/wer': 0.1327418249225214, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24266852, dtype=float32), 'test/wer': 0.0781183352629334, 'test/num_examples': 2472, 'score': 27380.701331615448, 'total_duration': 30084.456929922104, 'accumulated_submission_time': 27380.701331615448, 'accumulated_eval_time': 2701.2522921562195, 'accumulated_logging_time': 0.965153694152832, 'global_step': 34360, 'preemption_count': 0}), (36156, {'train/ctc_loss': Array(0.22635128, dtype=float32), 'train/wer': 0.08115605409302812, 'validation/ctc_loss': Array(0.4513795, dtype=float32), 'validation/wer': 0.13103295133089393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23776175, dtype=float32), 'test/wer': 0.07759023419251315, 'test/num_examples': 2472, 'score': 28821.154938220978, 'total_duration': 31657.57952785492, 'accumulated_submission_time': 28821.154938220978, 'accumulated_eval_time': 2833.7774863243103, 'accumulated_logging_time': 1.021730661392212, 'global_step': 36156, 'preemption_count': 0}), (37981, {'train/ctc_loss': Array(0.13815072, dtype=float32), 'train/wer': 0.0506929093691549, 'validation/ctc_loss': Array(0.4403994, dtype=float32), 'validation/wer': 0.12775036929047956, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2351409, dtype=float32), 'test/wer': 0.07588406150346312, 'test/num_examples': 2472, 'score': 30261.48275089264, 'total_duration': 33253.08378100395, 'accumulated_submission_time': 30261.48275089264, 'accumulated_eval_time': 2988.8153936862946, 'accumulated_logging_time': 1.078145980834961, 'global_step': 37981, 'preemption_count': 0}), (39793, {'train/ctc_loss': Array(0.13410744, dtype=float32), 'train/wer': 0.047232886390673946, 'validation/ctc_loss': Array(0.43591997, dtype=float32), 'validation/wer': 0.12621527945393282, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23304227, dtype=float32), 'test/wer': 0.0750715983182012, 'test/num_examples': 2472, 'score': 31702.03856253624, 'total_duration': 34829.99032139778, 'accumulated_submission_time': 31702.03856253624, 'accumulated_eval_time': 3125.0316421985626, 'accumulated_logging_time': 1.131788969039917, 'global_step': 39793, 'preemption_count': 0}), (41616, {'train/ctc_loss': Array(0.14266028, dtype=float32), 'train/wer': 0.05091043632231296, 'validation/ctc_loss': Array(0.4308139, dtype=float32), 'validation/wer': 0.1242457302296842, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22871488, dtype=float32), 'test/wer': 0.07320293299209879, 'test/num_examples': 2472, 'score': 33142.06616973877, 'total_duration': 36404.780586481094, 'accumulated_submission_time': 33142.06616973877, 'accumulated_eval_time': 3259.6550998687744, 'accumulated_logging_time': 1.1871211528778076, 'global_step': 41616, 'preemption_count': 0}), (43415, {'train/ctc_loss': Array(0.12480412, dtype=float32), 'train/wer': 0.0457206361131981, 'validation/ctc_loss': Array(0.42841774, dtype=float32), 'validation/wer': 0.12410091043378356, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2267602, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 34582.3564324379, 'total_duration': 37981.5632686615, 'accumulated_submission_time': 34582.3564324379, 'accumulated_eval_time': 3396.0110619068146, 'accumulated_logging_time': 1.2427010536193848, 'global_step': 43415, 'preemption_count': 0}), (45201, {'train/ctc_loss': Array(0.13416459, dtype=float32), 'train/wer': 0.04780056618560438, 'validation/ctc_loss': Array(0.42583966, dtype=float32), 'validation/wer': 0.12288442414821824, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22566003, dtype=float32), 'test/wer': 0.07285763613836248, 'test/num_examples': 2472, 'score': 36022.44448566437, 'total_duration': 39557.187906980515, 'accumulated_submission_time': 36022.44448566437, 'accumulated_eval_time': 3531.409448623657, 'accumulated_logging_time': 1.2997477054595947, 'global_step': 45201, 'preemption_count': 0}), (46991, {'train/ctc_loss': Array(0.12691087, dtype=float32), 'train/wer': 0.04664109331424236, 'validation/ctc_loss': Array(0.42552513, dtype=float32), 'validation/wer': 0.12275891365843769, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22541308, dtype=float32), 'test/wer': 0.07226860032904758, 'test/num_examples': 2472, 'score': 37463.04101443291, 'total_duration': 41134.39881324768, 'accumulated_submission_time': 37463.04101443291, 'accumulated_eval_time': 3667.8862357139587, 'accumulated_logging_time': 1.3547301292419434, 'global_step': 46991, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.15317775, dtype=float32), 'train/wer': 0.05205918963001116, 'validation/ctc_loss': Array(0.4256426, dtype=float32), 'validation/wer': 0.12272994969925756, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22542043, dtype=float32), 'test/wer': 0.07234984664757378, 'test/num_examples': 2472, 'score': 38255.23158955574, 'total_duration': 42062.36226296425, 'accumulated_submission_time': 38255.23158955574, 'accumulated_eval_time': 3803.54567193985, 'accumulated_logging_time': 1.418626070022583, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0212 03:24:03.589093 140437690197824 submission_runner.py:586] Timing: 38255.23158955574
I0212 03:24:03.589165 140437690197824 submission_runner.py:588] Total number of evals: 28
I0212 03:24:03.589222 140437690197824 submission_runner.py:589] ====================
I0212 03:24:03.589292 140437690197824 submission_runner.py:542] Using RNG seed 2216178884
I0212 03:24:03.593208 140437690197824 submission_runner.py:551] --- Tuning run 3/5 ---
I0212 03:24:03.593365 140437690197824 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_3.
I0212 03:24:03.595211 140437690197824 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_3/hparams.json.
I0212 03:24:03.597702 140437690197824 submission_runner.py:206] Initializing dataset.
I0212 03:24:03.597829 140437690197824 submission_runner.py:213] Initializing model.
I0212 03:24:04.811374 140437690197824 submission_runner.py:255] Initializing optimizer.
I0212 03:24:04.971180 140437690197824 submission_runner.py:262] Initializing metrics bundle.
I0212 03:24:04.971432 140437690197824 submission_runner.py:280] Initializing checkpoint and logger.
I0212 03:24:04.976021 140437690197824 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_3 with prefix checkpoint_
I0212 03:24:04.976244 140437690197824 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_3/meta_data_0.json.
I0212 03:24:04.976660 140437690197824 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 03:24:04.976763 140437690197824 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 03:24:05.826713 140437690197824 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 03:24:06.603519 140437690197824 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_3/flags_0.json.
I0212 03:24:06.624389 140437690197824 submission_runner.py:314] Starting training loop.
I0212 03:24:06.627943 140437690197824 input_pipeline.py:20] Loading split = train-clean-100
I0212 03:24:06.670892 140437690197824 input_pipeline.py:20] Loading split = train-clean-360
I0212 03:24:06.805253 140437690197824 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0212 03:24:22.451036 140280611206912 logging_writer.py:48] [0] global_step=0, grad_norm=21.000577926635742, loss=32.45863342285156
I0212 03:24:22.471675 140437690197824 spec.py:321] Evaluating on the training split.
I0212 03:26:05.590752 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 03:27:08.074100 140437690197824 spec.py:349] Evaluating on the test split.
I0212 03:27:40.330039 140437690197824 submission_runner.py:408] Time since start: 213.70s, 	Step: 1, 	{'train/ctc_loss': Array(30.26521, dtype=float32), 'train/wer': 3.632442748091603, 'validation/ctc_loss': Array(30.89189, dtype=float32), 'validation/wer': 3.325081823184684, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942295, dtype=float32), 'test/wer': 3.6990636361789857, 'test/num_examples': 2472, 'score': 15.847177743911743, 'total_duration': 213.7028248310089, 'accumulated_submission_time': 15.847177743911743, 'accumulated_eval_time': 197.85556888580322, 'accumulated_logging_time': 0}
I0212 03:27:40.348197 140280933185280 logging_writer.py:48] [1] accumulated_eval_time=197.855569, accumulated_logging_time=0, accumulated_submission_time=15.847178, global_step=1, preemption_count=0, score=15.847178, test/ctc_loss=30.94229507446289, test/num_examples=2472, test/wer=3.699064, total_duration=213.702825, train/ctc_loss=30.265209197998047, train/wer=3.632443, validation/ctc_loss=30.891889572143555, validation/num_examples=5348, validation/wer=3.325082
I0212 03:29:05.168832 140280857650944 logging_writer.py:48] [100] global_step=100, grad_norm=7.24969482421875, loss=10.579131126403809
I0212 03:30:21.769898 140280866043648 logging_writer.py:48] [200] global_step=200, grad_norm=1.3991256952285767, loss=6.31775426864624
I0212 03:31:38.181314 140280857650944 logging_writer.py:48] [300] global_step=300, grad_norm=0.3911665380001068, loss=5.919961452484131
I0212 03:32:54.615951 140280866043648 logging_writer.py:48] [400] global_step=400, grad_norm=0.6269755959510803, loss=5.8376145362854
I0212 03:34:11.198482 140280857650944 logging_writer.py:48] [500] global_step=500, grad_norm=0.37736058235168457, loss=5.802742004394531
I0212 03:35:27.833879 140280866043648 logging_writer.py:48] [600] global_step=600, grad_norm=0.34086668491363525, loss=5.783568859100342
I0212 03:36:48.764490 140280857650944 logging_writer.py:48] [700] global_step=700, grad_norm=0.8078939914703369, loss=5.713744640350342
I0212 03:38:12.967468 140280866043648 logging_writer.py:48] [800] global_step=800, grad_norm=0.5243511199951172, loss=5.588911056518555
I0212 03:39:37.874998 140280857650944 logging_writer.py:48] [900] global_step=900, grad_norm=1.1410356760025024, loss=5.354381561279297
I0212 03:41:02.836892 140280866043648 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5941844582557678, loss=5.044914245605469
I0212 03:42:25.211735 140280933185280 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.8398035764694214, loss=4.728827476501465
I0212 03:43:41.416219 140280924792576 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8062567710876465, loss=4.364038467407227
I0212 03:44:57.481604 140280933185280 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.3668330907821655, loss=4.107498645782471
I0212 03:46:13.507726 140280924792576 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.331129789352417, loss=3.848742961883545
I0212 03:47:29.487411 140280933185280 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.103476047515869, loss=3.7081117630004883
I0212 03:48:53.265514 140280924792576 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.348965883255005, loss=3.4250752925872803
I0212 03:50:18.014472 140280933185280 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.255011558532715, loss=3.2696146965026855
I0212 03:51:41.331868 140437690197824 spec.py:321] Evaluating on the training split.
I0212 03:52:22.023813 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 03:53:08.492362 140437690197824 spec.py:349] Evaluating on the test split.
I0212 03:53:32.236280 140437690197824 submission_runner.py:408] Time since start: 1765.61s, 	Step: 1800, 	{'train/ctc_loss': Array(5.9749413, dtype=float32), 'train/wer': 0.9422576974075737, 'validation/ctc_loss': Array(5.959475, dtype=float32), 'validation/wer': 0.8962607528698456, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.869004, dtype=float32), 'test/wer': 0.898990514492312, 'test/num_examples': 2472, 'score': 1456.7463419437408, 'total_duration': 1765.605096578598, 'accumulated_submission_time': 1456.7463419437408, 'accumulated_eval_time': 308.75326776504517, 'accumulated_logging_time': 0.029464244842529297}
I0212 03:53:32.274392 140280933185280 logging_writer.py:48] [1800] accumulated_eval_time=308.753268, accumulated_logging_time=0.029464, accumulated_submission_time=1456.746342, global_step=1800, preemption_count=0, score=1456.746342, test/ctc_loss=5.869003772735596, test/num_examples=2472, test/wer=0.898991, total_duration=1765.605097, train/ctc_loss=5.974941253662109, train/wer=0.942258, validation/ctc_loss=5.959475040435791, validation/num_examples=5348, validation/wer=0.896261
I0212 03:53:33.159442 140280924792576 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.9647417068481445, loss=3.150855541229248
I0212 03:54:48.738845 140280933185280 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.1865756511688232, loss=3.0718600749969482
I0212 03:56:04.270744 140280924792576 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.584266424179077, loss=2.897386074066162
I0212 03:57:23.628212 140280933185280 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.0803229808807373, loss=2.8147804737091064
I0212 03:58:38.973447 140280924792576 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.9914650917053223, loss=2.7607529163360596
I0212 03:59:54.157307 140280933185280 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.6581249237060547, loss=2.6928653717041016
I0212 04:01:09.503867 140280924792576 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.4039876461029053, loss=2.5915310382843018
I0212 04:02:25.143034 140280933185280 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.770207405090332, loss=2.557921886444092
I0212 04:03:49.186722 140280924792576 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.4617056846618652, loss=2.495648145675659
I0212 04:05:13.217743 140280933185280 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.3946120738983154, loss=2.4706056118011475
I0212 04:06:37.369986 140280924792576 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.9394044876098633, loss=2.3780598640441895
I0212 04:08:01.723881 140280933185280 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.1548125743865967, loss=2.310826063156128
I0212 04:09:26.009460 140280924792576 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.788161277770996, loss=2.291325092315674
I0212 04:10:53.584448 140280933185280 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.210454225540161, loss=2.2446651458740234
I0212 04:12:08.668697 140280924792576 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.0704545974731445, loss=2.235989570617676
I0212 04:13:23.650364 140280933185280 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.371785879135132, loss=2.1845617294311523
I0212 04:14:38.731199 140280924792576 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.9781241416931152, loss=2.187309980392456
I0212 04:15:53.881771 140280933185280 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.691005229949951, loss=2.1332473754882812
I0212 04:17:15.502202 140280924792576 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.2097887992858887, loss=2.121961832046509
I0212 04:17:32.781949 140437690197824 spec.py:321] Evaluating on the training split.
I0212 04:18:18.216212 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 04:19:05.723285 140437690197824 spec.py:349] Evaluating on the test split.
I0212 04:19:30.421007 140437690197824 submission_runner.py:408] Time since start: 3323.79s, 	Step: 3622, 	{'train/ctc_loss': Array(3.9224856, dtype=float32), 'train/wer': 0.8321735789451216, 'validation/ctc_loss': Array(4.115441, dtype=float32), 'validation/wer': 0.8269982718171023, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.7333565, dtype=float32), 'test/wer': 0.7959092478622063, 'test/num_examples': 2472, 'score': 2897.1611421108246, 'total_duration': 3323.7894248962402, 'accumulated_submission_time': 2897.1611421108246, 'accumulated_eval_time': 426.3852117061615, 'accumulated_logging_time': 0.08456134796142578}
I0212 04:19:30.459222 140281363265280 logging_writer.py:48] [3622] accumulated_eval_time=426.385212, accumulated_logging_time=0.084561, accumulated_submission_time=2897.161142, global_step=3622, preemption_count=0, score=2897.161142, test/ctc_loss=3.733356475830078, test/num_examples=2472, test/wer=0.795909, total_duration=3323.789425, train/ctc_loss=3.922485589981079, train/wer=0.832174, validation/ctc_loss=4.115440845489502, validation/num_examples=5348, validation/wer=0.826998
I0212 04:20:29.944197 140281354872576 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.86339807510376, loss=2.1370248794555664
I0212 04:21:45.400755 140281363265280 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.418569087982178, loss=2.0391621589660645
I0212 04:23:00.550193 140281354872576 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.545072555541992, loss=2.0385923385620117
I0212 04:24:18.877821 140281363265280 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.9565234184265137, loss=2.123311758041382
I0212 04:25:43.125368 140281354872576 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.5074822902679443, loss=2.0480895042419434
I0212 04:27:03.556788 140281363265280 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.965068817138672, loss=2.038918972015381
I0212 04:28:18.723475 140281354872576 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.412034749984741, loss=2.0291574001312256
I0212 04:29:33.895395 140281363265280 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.565084934234619, loss=2.0482723712921143
I0212 04:30:48.970171 140281354872576 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.2011234760284424, loss=2.0028254985809326
I0212 04:32:07.369160 140281363265280 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.9578874111175537, loss=2.026318311691284
I0212 04:33:31.825915 140281354872576 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.395929336547852, loss=2.061715602874756
I0212 04:34:56.626146 140281363265280 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.922126531600952, loss=1.9913028478622437
I0212 04:36:21.270975 140281354872576 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.8772177696228027, loss=1.977050542831421
I0212 04:37:46.360586 140281363265280 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.879936695098877, loss=1.9304020404815674
I0212 04:39:11.190059 140281354872576 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.238692045211792, loss=1.9893139600753784
I0212 04:40:34.649609 140280745424640 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.5099687576293945, loss=1.9223718643188477
I0212 04:41:49.708246 140280737031936 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.610748529434204, loss=1.9222813844680786
I0212 04:43:05.128930 140280745424640 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.839907646179199, loss=1.862914800643921
I0212 04:43:30.479000 140437690197824 spec.py:321] Evaluating on the training split.
I0212 04:44:27.086599 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 04:45:18.663305 140437690197824 spec.py:349] Evaluating on the test split.
I0212 04:45:45.208483 140437690197824 submission_runner.py:408] Time since start: 4898.58s, 	Step: 5435, 	{'train/ctc_loss': Array(0.58773357, dtype=float32), 'train/wer': 0.2038345540715209, 'validation/ctc_loss': Array(0.9542844, dtype=float32), 'validation/wer': 0.2757272367417477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6223996, dtype=float32), 'test/wer': 0.20675156906952655, 'test/num_examples': 2472, 'score': 4337.0904133319855, 'total_duration': 4898.577420711517, 'accumulated_submission_time': 4337.0904133319855, 'accumulated_eval_time': 561.1081030368805, 'accumulated_logging_time': 0.13887548446655273}
I0212 04:45:45.243809 140280933185280 logging_writer.py:48] [5435] accumulated_eval_time=561.108103, accumulated_logging_time=0.138875, accumulated_submission_time=4337.090413, global_step=5435, preemption_count=0, score=4337.090413, test/ctc_loss=0.622399628162384, test/num_examples=2472, test/wer=0.206752, total_duration=4898.577421, train/ctc_loss=0.5877335667610168, train/wer=0.203835, validation/ctc_loss=0.9542844295501709, validation/num_examples=5348, validation/wer=0.275727
I0212 04:46:35.033886 140280924792576 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.5230660438537598, loss=1.8043928146362305
I0212 04:47:50.281765 140280933185280 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.1463444232940674, loss=1.8665112257003784
I0212 04:49:05.506437 140280924792576 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.3814215660095215, loss=1.8913904428482056
I0212 04:50:24.690932 140280933185280 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.9276065826416016, loss=1.828053593635559
I0212 04:51:48.297263 140280924792576 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.3606083393096924, loss=1.8195161819458008
I0212 04:53:12.166926 140280933185280 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.111603021621704, loss=1.832114338874817
I0212 04:54:36.241933 140280924792576 logging_writer.py:48] [6100] global_step=6100, grad_norm=7.581784725189209, loss=1.8064919710159302
I0212 04:56:02.354681 140280933185280 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.65342116355896, loss=1.8122681379318237
I0212 04:57:17.183796 140280924792576 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.989715814590454, loss=1.775581955909729
I0212 04:58:32.170429 140280933185280 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.6872310638427734, loss=1.7538056373596191
I0212 04:59:47.262428 140280924792576 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.5226850509643555, loss=1.7329001426696777
I0212 05:01:02.393652 140280933185280 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.1044318675994873, loss=1.769248127937317
I0212 05:02:25.506742 140280924792576 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.943892002105713, loss=1.7080618143081665
I0212 05:03:50.218245 140280933185280 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.640719890594482, loss=1.8032559156417847
I0212 05:05:15.028950 140280924792576 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.9589321613311768, loss=1.762812852859497
I0212 05:06:39.947578 140280933185280 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.8684661388397217, loss=1.7756317853927612
I0212 05:08:04.817168 140280924792576 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.9693312644958496, loss=1.785072684288025
I0212 05:09:30.305269 140280933185280 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.869887351989746, loss=1.7783007621765137
I0212 05:09:45.913876 140437690197824 spec.py:321] Evaluating on the training split.
I0212 05:10:44.661078 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 05:11:37.134718 140437690197824 spec.py:349] Evaluating on the test split.
I0212 05:12:03.517444 140437690197824 submission_runner.py:408] Time since start: 6476.89s, 	Step: 7216, 	{'train/ctc_loss': Array(0.47603443, dtype=float32), 'train/wer': 0.16325389943480395, 'validation/ctc_loss': Array(0.834246, dtype=float32), 'validation/wer': 0.24146287303165762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5297743, dtype=float32), 'test/wer': 0.17311559319968314, 'test/num_examples': 2472, 'score': 5777.671191692352, 'total_duration': 6476.886586666107, 'accumulated_submission_time': 5777.671191692352, 'accumulated_eval_time': 698.7054476737976, 'accumulated_logging_time': 0.18925881385803223}
I0212 05:12:03.553617 140280933185280 logging_writer.py:48] [7216] accumulated_eval_time=698.705448, accumulated_logging_time=0.189259, accumulated_submission_time=5777.671192, global_step=7216, preemption_count=0, score=5777.671192, test/ctc_loss=0.5297743082046509, test/num_examples=2472, test/wer=0.173116, total_duration=6476.886587, train/ctc_loss=0.4760344326496124, train/wer=0.163254, validation/ctc_loss=0.8342459797859192, validation/num_examples=5348, validation/wer=0.241463
I0212 05:13:07.533725 140280924792576 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.6803855895996094, loss=1.7692052125930786
I0212 05:14:22.732527 140280933185280 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.202958822250366, loss=1.7160447835922241
I0212 05:15:38.020016 140280924792576 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.1442999839782715, loss=1.8175444602966309
I0212 05:16:53.101833 140280933185280 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.733222007751465, loss=1.7106103897094727
I0212 05:18:08.144734 140280924792576 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.0932700634002686, loss=1.6692650318145752
I0212 05:19:23.887959 140280933185280 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.6500768661499023, loss=1.7789745330810547
I0212 05:20:48.695925 140280924792576 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.7053308486938477, loss=1.6916383504867554
I0212 05:22:13.575031 140280933185280 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.7120583057403564, loss=1.6923766136169434
I0212 05:23:38.257568 140280924792576 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.0427088737487793, loss=1.6795549392700195
I0212 05:25:02.683963 140280933185280 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.8775010108947754, loss=1.6877365112304688
I0212 05:26:25.035805 140280933185280 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.650761365890503, loss=1.7179913520812988
I0212 05:27:40.155848 140280924792576 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.3538451194763184, loss=1.6753796339035034
I0212 05:28:55.291270 140280933185280 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.212892532348633, loss=1.6987260580062866
I0212 05:30:10.502404 140280924792576 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.248920440673828, loss=1.674972653388977
I0212 05:31:26.823215 140280933185280 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.8503787517547607, loss=1.6596980094909668
I0212 05:32:50.903058 140280924792576 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.5211434364318848, loss=1.6634236574172974
I0212 05:34:16.109687 140280933185280 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.2911298274993896, loss=1.681732177734375
I0212 05:35:40.437105 140280924792576 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.2555413246154785, loss=1.6615135669708252
I0212 05:36:04.240190 140437690197824 spec.py:321] Evaluating on the training split.
I0212 05:37:01.676101 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 05:37:53.394092 140437690197824 spec.py:349] Evaluating on the test split.
I0212 05:38:19.772223 140437690197824 submission_runner.py:408] Time since start: 8053.14s, 	Step: 9030, 	{'train/ctc_loss': Array(0.42759815, dtype=float32), 'train/wer': 0.14474628882741727, 'validation/ctc_loss': Array(0.7569138, dtype=float32), 'validation/wer': 0.22058951311584618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4683264, dtype=float32), 'test/wer': 0.15215404301992566, 'test/num_examples': 2472, 'score': 7218.265148639679, 'total_duration': 8053.141635894775, 'accumulated_submission_time': 7218.265148639679, 'accumulated_eval_time': 834.2313735485077, 'accumulated_logging_time': 0.2436678409576416}
I0212 05:38:19.809811 140281363265280 logging_writer.py:48] [9030] accumulated_eval_time=834.231374, accumulated_logging_time=0.243668, accumulated_submission_time=7218.265149, global_step=9030, preemption_count=0, score=7218.265149, test/ctc_loss=0.4683263897895813, test/num_examples=2472, test/wer=0.152154, total_duration=8053.141636, train/ctc_loss=0.42759814858436584, train/wer=0.144746, validation/ctc_loss=0.7569137811660767, validation/num_examples=5348, validation/wer=0.220590
I0212 05:39:13.055435 140281354872576 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.9760451316833496, loss=1.7006473541259766
I0212 05:40:28.188279 140281363265280 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.307293176651001, loss=1.6624373197555542
I0212 05:41:47.095656 140280745424640 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.9871416091918945, loss=1.6262534856796265
I0212 05:43:02.308352 140280737031936 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.301894187927246, loss=1.652051329612732
I0212 05:44:17.306037 140280745424640 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.4338557720184326, loss=1.6265136003494263
I0212 05:45:32.487250 140280737031936 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.521090030670166, loss=1.6413695812225342
I0212 05:46:48.614210 140280745424640 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.9861063957214355, loss=1.675510287284851
I0212 05:48:12.125512 140280737031936 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.3711867332458496, loss=1.6809046268463135
I0212 05:49:37.318373 140280745424640 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.649343490600586, loss=1.6393096446990967
I0212 05:51:02.324480 140280737031936 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.9929919242858887, loss=1.642916202545166
I0212 05:52:26.795439 140280745424640 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.6914515495300293, loss=1.6455429792404175
I0212 05:53:50.804591 140280737031936 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.4327874183654785, loss=1.5744013786315918
I0212 05:55:19.053404 140281363265280 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.5457170009613037, loss=1.5936890840530396
I0212 05:56:34.204426 140281354872576 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.4786336421966553, loss=1.6063871383666992
I0212 05:57:49.418528 140281363265280 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.387031078338623, loss=1.6408053636550903
I0212 05:59:04.734199 140281354872576 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.1874330043792725, loss=1.6417288780212402
I0212 06:00:19.786103 140281363265280 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.7547125816345215, loss=1.6074416637420654
I0212 06:01:41.458914 140281354872576 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.70918345451355, loss=1.6188157796859741
I0212 06:02:19.946991 140437690197824 spec.py:321] Evaluating on the training split.
I0212 06:03:18.381822 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 06:04:10.588661 140437690197824 spec.py:349] Evaluating on the test split.
I0212 06:04:37.148946 140437690197824 submission_runner.py:408] Time since start: 9630.52s, 	Step: 10847, 	{'train/ctc_loss': Array(0.40683183, dtype=float32), 'train/wer': 0.13968561039068522, 'validation/ctc_loss': Array(0.72036105, dtype=float32), 'validation/wer': 0.2091584038927561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4358419, dtype=float32), 'test/wer': 0.14053581947068022, 'test/num_examples': 2472, 'score': 8658.311223983765, 'total_duration': 9630.516981840134, 'accumulated_submission_time': 8658.311223983765, 'accumulated_eval_time': 971.4258334636688, 'accumulated_logging_time': 0.29752111434936523}
I0212 06:04:37.187430 140280856385280 logging_writer.py:48] [10847] accumulated_eval_time=971.425833, accumulated_logging_time=0.297521, accumulated_submission_time=8658.311224, global_step=10847, preemption_count=0, score=8658.311224, test/ctc_loss=0.4358418881893158, test/num_examples=2472, test/wer=0.140536, total_duration=9630.516982, train/ctc_loss=0.406831830739975, train/wer=0.139686, validation/ctc_loss=0.720361053943634, validation/num_examples=5348, validation/wer=0.209158
I0212 06:05:17.780462 140280847992576 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.248389720916748, loss=1.6142499446868896
I0212 06:06:32.924167 140280856385280 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.8435771465301514, loss=1.6444624662399292
I0212 06:07:47.967216 140280847992576 logging_writer.py:48] [11100] global_step=11100, grad_norm=3.1511454582214355, loss=1.582995891571045
I0212 06:09:06.123492 140280856385280 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.1352643966674805, loss=1.6223304271697998
I0212 06:10:30.193850 140280847992576 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.8672657012939453, loss=1.6485459804534912
I0212 06:11:51.231925 140280856385280 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.903397560119629, loss=1.6029553413391113
I0212 06:13:06.330574 140280847992576 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.8832154273986816, loss=1.5359681844711304
I0212 06:14:21.486584 140280856385280 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.421590805053711, loss=1.5535356998443604
I0212 06:15:36.459457 140280847992576 logging_writer.py:48] [11700] global_step=11700, grad_norm=5.3824920654296875, loss=1.6237194538116455
I0212 06:16:53.757952 140280856385280 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.4291529655456543, loss=1.643886923789978
I0212 06:18:18.417379 140280847992576 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.33424711227417, loss=1.6065442562103271
I0212 06:19:42.906527 140280856385280 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.109433650970459, loss=1.5960763692855835
I0212 06:21:07.262988 140280847992576 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.401681423187256, loss=1.5543715953826904
I0212 06:22:32.238756 140280856385280 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.566086769104004, loss=1.61945641040802
I0212 06:23:56.582214 140280847992576 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.093044281005859, loss=1.6253162622451782
I0212 06:25:20.857455 140280856385280 logging_writer.py:48] [12400] global_step=12400, grad_norm=3.153122901916504, loss=1.6543359756469727
I0212 06:26:35.858483 140280847992576 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.453373432159424, loss=1.5817075967788696
I0212 06:27:50.815846 140280856385280 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.9742414951324463, loss=1.6201415061950684
I0212 06:28:37.815788 140437690197824 spec.py:321] Evaluating on the training split.
I0212 06:29:34.970645 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 06:30:26.853623 140437690197824 spec.py:349] Evaluating on the test split.
I0212 06:30:53.446471 140437690197824 submission_runner.py:408] Time since start: 11206.82s, 	Step: 12664, 	{'train/ctc_loss': Array(0.36989608, dtype=float32), 'train/wer': 0.12554296283488167, 'validation/ctc_loss': Array(0.67882633, dtype=float32), 'validation/wer': 0.19839346573080896, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40882304, dtype=float32), 'test/wer': 0.1320862023439563, 'test/num_examples': 2472, 'score': 10098.84746146202, 'total_duration': 11206.81583738327, 'accumulated_submission_time': 10098.84746146202, 'accumulated_eval_time': 1107.0503504276276, 'accumulated_logging_time': 0.35266804695129395}
I0212 06:30:53.485046 140280856385280 logging_writer.py:48] [12664] accumulated_eval_time=1107.050350, accumulated_logging_time=0.352668, accumulated_submission_time=10098.847461, global_step=12664, preemption_count=0, score=10098.847461, test/ctc_loss=0.40882304310798645, test/num_examples=2472, test/wer=0.132086, total_duration=11206.815837, train/ctc_loss=0.3698960840702057, train/wer=0.125543, validation/ctc_loss=0.6788263320922852, validation/num_examples=5348, validation/wer=0.198393
I0212 06:31:21.573076 140280847992576 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.9614970684051514, loss=1.6011850833892822
I0212 06:32:36.705642 140280856385280 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.1461031436920166, loss=1.6693594455718994
I0212 06:33:51.896688 140280847992576 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.7456276416778564, loss=1.5808558464050293
I0212 06:35:08.850413 140280856385280 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.863485336303711, loss=1.5717990398406982
I0212 06:36:32.995043 140280847992576 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.866988182067871, loss=1.5922960042953491
I0212 06:37:58.084826 140280856385280 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.252157211303711, loss=1.5803924798965454
I0212 06:39:22.891020 140280847992576 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.286954164505005, loss=1.627962589263916
I0212 06:40:50.353305 140280856385280 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.4997804164886475, loss=1.561056137084961
I0212 06:42:05.495493 140280847992576 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.0671353340148926, loss=1.5421613454818726
I0212 06:43:20.639302 140280856385280 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.686800956726074, loss=1.5853224992752075
I0212 06:44:35.801455 140280847992576 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.5531909465789795, loss=1.5608035326004028
I0212 06:45:51.048303 140280856385280 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.4521477222442627, loss=1.5160243511199951
I0212 06:47:15.101907 140280847992576 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.761321783065796, loss=1.5759092569351196
I0212 06:48:40.310860 140280856385280 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.3468401432037354, loss=1.5731775760650635
I0212 06:50:06.024294 140280847992576 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.9660844802856445, loss=1.55689537525177
I0212 06:51:31.103149 140280856385280 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.8656258583068848, loss=1.5780582427978516
I0212 06:52:56.687129 140280847992576 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.1721155643463135, loss=1.5668894052505493
I0212 06:54:21.912384 140280856385280 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.2157962322235107, loss=1.5625864267349243
I0212 06:54:54.133599 140437690197824 spec.py:321] Evaluating on the training split.
I0212 06:55:51.634306 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 06:56:42.927195 140437690197824 spec.py:349] Evaluating on the test split.
I0212 06:57:09.757603 140437690197824 submission_runner.py:408] Time since start: 12783.13s, 	Step: 14437, 	{'train/ctc_loss': Array(0.33032325, dtype=float32), 'train/wer': 0.11631720189169667, 'validation/ctc_loss': Array(0.66513646, dtype=float32), 'validation/wer': 0.1929965146702453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3992169, dtype=float32), 'test/wer': 0.13003473280116995, 'test/num_examples': 2472, 'score': 11539.406084775925, 'total_duration': 12783.127084732056, 'accumulated_submission_time': 11539.406084775925, 'accumulated_eval_time': 1242.6684551239014, 'accumulated_logging_time': 0.4081716537475586}
I0212 06:57:09.793138 140280856385280 logging_writer.py:48] [14437] accumulated_eval_time=1242.668455, accumulated_logging_time=0.408172, accumulated_submission_time=11539.406085, global_step=14437, preemption_count=0, score=11539.406085, test/ctc_loss=0.399216890335083, test/num_examples=2472, test/wer=0.130035, total_duration=12783.127085, train/ctc_loss=0.3303232491016388, train/wer=0.116317, validation/ctc_loss=0.665136456489563, validation/num_examples=5348, validation/wer=0.192997
I0212 06:57:57.758305 140280847992576 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.5133559703826904, loss=1.583292007446289
I0212 06:59:12.920904 140280856385280 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.061936616897583, loss=1.6106760501861572
I0212 07:00:27.857808 140280847992576 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.3748931884765625, loss=1.540453314781189
I0212 07:01:43.165315 140280856385280 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.2901041507720947, loss=1.5433624982833862
I0212 07:02:58.596297 140280847992576 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.298828125, loss=1.55843985080719
I0212 07:04:15.977870 140280856385280 logging_writer.py:48] [15000] global_step=15000, grad_norm=5.370967864990234, loss=1.5397354364395142
I0212 07:05:41.336705 140280847992576 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.1059482097625732, loss=1.5707255601882935
I0212 07:07:06.147686 140280856385280 logging_writer.py:48] [15200] global_step=15200, grad_norm=6.178559303283691, loss=1.4801987409591675
I0212 07:08:31.618971 140280847992576 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.8126258850097656, loss=1.5962674617767334
I0212 07:09:57.026268 140280856385280 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.9945443868637085, loss=1.5782536268234253
I0212 07:11:20.519532 140280856385280 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.141829490661621, loss=1.542465329170227
I0212 07:12:35.633782 140280847992576 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.2139358520507812, loss=1.4772520065307617
I0212 07:13:50.661401 140280856385280 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.2058892250061035, loss=1.5916144847869873
I0212 07:15:05.546274 140280847992576 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.538329839706421, loss=1.541165828704834
I0212 07:16:24.080734 140280856385280 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.997814655303955, loss=1.4892852306365967
I0212 07:17:49.521349 140280847992576 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.3676469326019287, loss=1.5531644821166992
I0212 07:19:14.514589 140280856385280 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.803592562675476, loss=1.542726993560791
I0212 07:20:39.547077 140280847992576 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.2415804862976074, loss=1.495851993560791
I0212 07:21:09.781422 140437690197824 spec.py:321] Evaluating on the training split.
I0212 07:22:08.830160 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 07:23:01.641093 140437690197824 spec.py:349] Evaluating on the test split.
I0212 07:23:28.404233 140437690197824 submission_runner.py:408] Time since start: 14361.77s, 	Step: 16237, 	{'train/ctc_loss': Array(0.31750143, dtype=float32), 'train/wer': 0.10809319153937746, 'validation/ctc_loss': Array(0.641495, dtype=float32), 'validation/wer': 0.1859099993241743, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38088343, dtype=float32), 'test/wer': 0.12294599150975971, 'test/num_examples': 2472, 'score': 12979.301619768143, 'total_duration': 14361.773697376251, 'accumulated_submission_time': 12979.301619768143, 'accumulated_eval_time': 1381.285228252411, 'accumulated_logging_time': 0.46169400215148926}
I0212 07:23:28.438413 140280856385280 logging_writer.py:48] [16237] accumulated_eval_time=1381.285228, accumulated_logging_time=0.461694, accumulated_submission_time=12979.301620, global_step=16237, preemption_count=0, score=12979.301620, test/ctc_loss=0.38088342547416687, test/num_examples=2472, test/wer=0.122946, total_duration=14361.773697, train/ctc_loss=0.317501425743103, train/wer=0.108093, validation/ctc_loss=0.6414949893951416, validation/num_examples=5348, validation/wer=0.185910
I0212 07:24:16.761353 140280847992576 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.1064364910125732, loss=1.5206815004348755
I0212 07:25:32.088272 140280856385280 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.3086206912994385, loss=1.482835292816162
I0212 07:26:51.014893 140280856385280 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.028024435043335, loss=1.4994953870773315
I0212 07:28:06.192910 140280847992576 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.72240948677063, loss=1.5297266244888306
I0212 07:29:21.372027 140280856385280 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.703758478164673, loss=1.5231306552886963
I0212 07:30:36.470580 140280847992576 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.5298750400543213, loss=1.538619875907898
I0212 07:31:52.462260 140280856385280 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.3727481365203857, loss=1.4947291612625122
I0212 07:33:16.604501 140280847992576 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.545358419418335, loss=1.499143123626709
I0212 07:34:41.772363 140280856385280 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.0552287101745605, loss=1.5519033670425415
I0212 07:36:05.764266 140280847992576 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.170145273208618, loss=1.4699809551239014
I0212 07:37:29.719404 140280856385280 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.900315523147583, loss=1.537925362586975
I0212 07:38:54.540555 140280847992576 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.029404401779175, loss=1.5083142518997192
I0212 07:40:18.463292 140280856385280 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.8108294010162354, loss=1.5623140335083008
I0212 07:41:37.550770 140280856385280 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.4250423908233643, loss=1.4966013431549072
I0212 07:42:52.603155 140280847992576 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.2140161991119385, loss=1.5429773330688477
I0212 07:44:07.638279 140280856385280 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.4756014347076416, loss=1.5318021774291992
I0212 07:45:22.531095 140280847992576 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.707017421722412, loss=1.534612774848938
I0212 07:46:43.323764 140280856385280 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.884225606918335, loss=1.519829273223877
I0212 07:47:28.410042 140437690197824 spec.py:321] Evaluating on the training split.
I0212 07:48:25.226853 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 07:49:17.625597 140437690197824 spec.py:349] Evaluating on the test split.
I0212 07:49:44.286937 140437690197824 submission_runner.py:408] Time since start: 15937.65s, 	Step: 18055, 	{'train/ctc_loss': Array(0.33247074, dtype=float32), 'train/wer': 0.11449361610041117, 'validation/ctc_loss': Array(0.62319475, dtype=float32), 'validation/wer': 0.18160402405939544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36559522, dtype=float32), 'test/wer': 0.11841650925192453, 'test/num_examples': 2472, 'score': 14419.182766199112, 'total_duration': 15937.654762268066, 'accumulated_submission_time': 14419.182766199112, 'accumulated_eval_time': 1517.154412984848, 'accumulated_logging_time': 0.5125668048858643}
I0212 07:49:44.329736 140280856385280 logging_writer.py:48] [18055] accumulated_eval_time=1517.154413, accumulated_logging_time=0.512567, accumulated_submission_time=14419.182766, global_step=18055, preemption_count=0, score=14419.182766, test/ctc_loss=0.3655952215194702, test/num_examples=2472, test/wer=0.118417, total_duration=15937.654762, train/ctc_loss=0.33247074484825134, train/wer=0.114494, validation/ctc_loss=0.6231947541236877, validation/num_examples=5348, validation/wer=0.181604
I0212 07:50:18.957407 140280847992576 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.791961908340454, loss=1.5080891847610474
I0212 07:51:33.992229 140280856385280 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.591496229171753, loss=1.4818466901779175
I0212 07:52:49.168387 140280847992576 logging_writer.py:48] [18300] global_step=18300, grad_norm=4.078909873962402, loss=1.4900565147399902
I0212 07:54:05.526115 140280856385280 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.764752149581909, loss=1.518094539642334
I0212 07:55:29.030772 140280847992576 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.244481325149536, loss=1.5847541093826294
I0212 07:56:51.345510 140280856385280 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.2008228302001953, loss=1.4539579153060913
I0212 07:58:06.378579 140280847992576 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.7589517831802368, loss=1.4641307592391968
I0212 07:59:21.302165 140280856385280 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.820446729660034, loss=1.4498382806777954
I0212 08:00:36.356485 140280847992576 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.9777719974517822, loss=1.5275259017944336
I0212 08:01:52.539944 140280856385280 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.5497138500213623, loss=1.4466286897659302
I0212 08:03:16.552772 140280847992576 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.5093510150909424, loss=1.4140493869781494
I0212 08:04:41.926000 140280856385280 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.4907686710357666, loss=1.477989673614502
I0212 08:06:07.250495 140280847992576 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.2746148109436035, loss=1.5265092849731445
I0212 08:07:32.441786 140280856385280 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.2725632190704346, loss=1.4512090682983398
I0212 08:08:56.339986 140280847992576 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.1658029556274414, loss=1.438528299331665
I0212 08:10:22.053750 140280856385280 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.663487195968628, loss=1.533484935760498
I0212 08:11:37.251760 140280847992576 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.8965110778808594, loss=1.4764891862869263
I0212 08:12:52.424349 140280856385280 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.245706558227539, loss=1.4364378452301025
I0212 08:13:44.785494 140437690197824 spec.py:321] Evaluating on the training split.
I0212 08:14:41.493631 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 08:15:33.286860 140437690197824 spec.py:349] Evaluating on the test split.
I0212 08:15:59.694869 140437690197824 submission_runner.py:408] Time since start: 17513.06s, 	Step: 19871, 	{'train/ctc_loss': Array(0.3052099, dtype=float32), 'train/wer': 0.10275043443681192, 'validation/ctc_loss': Array(0.6104939, dtype=float32), 'validation/wer': 0.1763712021008525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35727885, dtype=float32), 'test/wer': 0.11703532183697926, 'test/num_examples': 2472, 'score': 15859.548187494278, 'total_duration': 17513.06376671791, 'accumulated_submission_time': 15859.548187494278, 'accumulated_eval_time': 1652.0571603775024, 'accumulated_logging_time': 0.5724399089813232}
I0212 08:15:59.731567 140280856385280 logging_writer.py:48] [19871] accumulated_eval_time=1652.057160, accumulated_logging_time=0.572440, accumulated_submission_time=15859.548187, global_step=19871, preemption_count=0, score=15859.548187, test/ctc_loss=0.35727885365486145, test/num_examples=2472, test/wer=0.117035, total_duration=17513.063767, train/ctc_loss=0.3052099049091339, train/wer=0.102750, validation/ctc_loss=0.6104938983917236, validation/num_examples=5348, validation/wer=0.176371
I0212 08:16:22.237439 140280847992576 logging_writer.py:48] [19900] global_step=19900, grad_norm=5.480851650238037, loss=1.4841563701629639
I0212 08:17:37.528033 140280856385280 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.9828317165374756, loss=1.450264573097229
I0212 08:18:52.481346 140280847992576 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.424954891204834, loss=1.4684301614761353
I0212 08:20:07.673877 140280856385280 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.1900436878204346, loss=1.478658676147461
I0212 08:21:30.445313 140280847992576 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.7070486545562744, loss=1.4457587003707886
I0212 08:22:55.529615 140280856385280 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.643428087234497, loss=1.5361295938491821
I0212 08:24:19.873439 140280847992576 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.826148509979248, loss=1.5192160606384277
I0212 08:25:48.725235 140280856385280 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.4655416011810303, loss=1.5184329748153687
I0212 08:27:04.169245 140280847992576 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.5891659259796143, loss=1.4953346252441406
I0212 08:28:19.216431 140280856385280 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.818814277648926, loss=1.45353364944458
I0212 08:29:34.445284 140280847992576 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.84148907661438, loss=1.4299256801605225
I0212 08:30:49.553218 140280856385280 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.9171608686447144, loss=1.4353814125061035
I0212 08:32:10.096343 140280847992576 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.558746337890625, loss=1.5000158548355103
I0212 08:33:35.475757 140280856385280 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.5142416954040527, loss=1.545074462890625
I0212 08:35:00.775009 140280847992576 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.132081031799316, loss=1.4746676683425903
I0212 08:36:26.847071 140280856385280 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.6426167488098145, loss=1.4996107816696167
I0212 08:37:52.296159 140280847992576 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.666910171508789, loss=1.5003231763839722
I0212 08:39:17.405013 140280856385280 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.709380626678467, loss=1.493715763092041
I0212 08:39:59.930296 140437690197824 spec.py:321] Evaluating on the training split.
I0212 08:40:58.805605 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 08:41:50.702595 140437690197824 spec.py:349] Evaluating on the test split.
I0212 08:42:16.947249 140437690197824 submission_runner.py:408] Time since start: 19090.32s, 	Step: 21649, 	{'train/ctc_loss': Array(0.28996828, dtype=float32), 'train/wer': 0.10099517800348826, 'validation/ctc_loss': Array(0.59253216, dtype=float32), 'validation/wer': 0.1723065931625747, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3434821, dtype=float32), 'test/wer': 0.11110434058456727, 'test/num_examples': 2472, 'score': 17299.65816307068, 'total_duration': 19090.31603384018, 'accumulated_submission_time': 17299.65816307068, 'accumulated_eval_time': 1789.0675375461578, 'accumulated_logging_time': 0.6251442432403564}
I0212 08:42:16.984303 140280856385280 logging_writer.py:48] [21649] accumulated_eval_time=1789.067538, accumulated_logging_time=0.625144, accumulated_submission_time=17299.658163, global_step=21649, preemption_count=0, score=17299.658163, test/ctc_loss=0.343482106924057, test/num_examples=2472, test/wer=0.111104, total_duration=19090.316034, train/ctc_loss=0.2899682819843292, train/wer=0.100995, validation/ctc_loss=0.5925321578979492, validation/num_examples=5348, validation/wer=0.172307
I0212 08:42:56.029078 140280847992576 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.883901357650757, loss=1.3902192115783691
I0212 08:44:11.091472 140280856385280 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.1949710845947266, loss=1.4648314714431763
I0212 08:45:26.060518 140280847992576 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.6203560829162598, loss=1.3947407007217407
I0212 08:46:41.157664 140280856385280 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.164707899093628, loss=1.4360474348068237
I0212 08:47:56.403310 140280847992576 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.402190685272217, loss=1.4079536199569702
I0212 08:49:13.052958 140280856385280 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.152339220046997, loss=1.399340271949768
I0212 08:50:37.998152 140280847992576 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.0396230220794678, loss=1.4774402379989624
I0212 08:52:03.375849 140280856385280 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.733030319213867, loss=1.4503484964370728
I0212 08:53:28.658483 140280847992576 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.5246222019195557, loss=1.4160469770431519
I0212 08:54:53.188030 140280856385280 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.801968574523926, loss=1.4512144327163696
I0212 08:56:18.172370 140280856385280 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.93914532661438, loss=1.4089266061782837
I0212 08:57:33.273710 140280847992576 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.458834409713745, loss=1.4894605875015259
I0212 08:58:48.079341 140280856385280 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.9267988204956055, loss=1.4589577913284302
I0212 09:00:02.995852 140280847992576 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.8532330989837646, loss=1.4397279024124146
I0212 09:01:21.745992 140280856385280 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.0628108978271484, loss=1.4646480083465576
I0212 09:02:46.294813 140280847992576 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.0635361671447754, loss=1.5223627090454102
I0212 09:04:11.399521 140280856385280 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.318204164505005, loss=1.4683046340942383
I0212 09:05:36.734541 140280847992576 logging_writer.py:48] [23400] global_step=23400, grad_norm=4.906735420227051, loss=1.4581702947616577
I0212 09:06:17.961144 140437690197824 spec.py:321] Evaluating on the training split.
I0212 09:07:16.388400 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 09:08:08.695438 140437690197824 spec.py:349] Evaluating on the test split.
I0212 09:08:35.037026 140437690197824 submission_runner.py:408] Time since start: 20668.41s, 	Step: 23449, 	{'train/ctc_loss': Array(0.32587433, dtype=float32), 'train/wer': 0.10602337875065147, 'validation/ctc_loss': Array(0.5905726, dtype=float32), 'validation/wer': 0.17074253936684786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3419359, dtype=float32), 'test/wer': 0.11142932585867203, 'test/num_examples': 2472, 'score': 18740.54448866844, 'total_duration': 20668.405680656433, 'accumulated_submission_time': 18740.54448866844, 'accumulated_eval_time': 1926.1365442276, 'accumulated_logging_time': 0.6778745651245117}
I0212 09:08:35.079305 140280856385280 logging_writer.py:48] [23449] accumulated_eval_time=1926.136544, accumulated_logging_time=0.677875, accumulated_submission_time=18740.544489, global_step=23449, preemption_count=0, score=18740.544489, test/ctc_loss=0.3419359028339386, test/num_examples=2472, test/wer=0.111429, total_duration=20668.405681, train/ctc_loss=0.32587432861328125, train/wer=0.106023, validation/ctc_loss=0.5905725955963135, validation/num_examples=5348, validation/wer=0.170743
I0212 09:09:14.142206 140280847992576 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.529677629470825, loss=1.4356404542922974
I0212 09:10:29.193601 140280856385280 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.617422342300415, loss=1.4121419191360474
I0212 09:11:48.090306 140280856385280 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.0282211303710938, loss=1.4666715860366821
I0212 09:13:03.287181 140280847992576 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.3894143104553223, loss=1.444915771484375
I0212 09:14:18.405036 140280856385280 logging_writer.py:48] [23900] global_step=23900, grad_norm=4.488415241241455, loss=1.4998197555541992
I0212 09:15:33.457297 140280847992576 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.3745150566101074, loss=1.4455647468566895
I0212 09:16:49.645772 140280856385280 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.1740236282348633, loss=1.3932245969772339
I0212 09:18:13.655553 140280847992576 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.0430519580841064, loss=1.4401761293411255
I0212 09:19:38.238216 140280856385280 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.771843194961548, loss=1.4552100896835327
I0212 09:21:03.751996 140280847992576 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.559018850326538, loss=1.3857262134552002
I0212 09:22:29.911021 140280856385280 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.1899662017822266, loss=1.4483261108398438
I0212 09:23:55.019344 140280847992576 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.737518072128296, loss=1.4891811609268188
I0212 09:25:21.475349 140280856385280 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.826145052909851, loss=1.433090329170227
I0212 09:26:41.952660 140280856385280 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.8059375286102295, loss=1.3834726810455322
I0212 09:27:56.802588 140280847992576 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.25704288482666, loss=1.4332537651062012
I0212 09:29:12.057257 140280856385280 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.1404736042022705, loss=1.3445115089416504
I0212 09:30:27.417541 140280847992576 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.5787112712860107, loss=1.4200377464294434
I0212 09:31:50.033843 140280856385280 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.283726692199707, loss=1.448807716369629
I0212 09:32:35.565141 140437690197824 spec.py:321] Evaluating on the training split.
I0212 09:33:34.383681 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 09:34:26.702296 140437690197824 spec.py:349] Evaluating on the test split.
I0212 09:34:53.316149 140437690197824 submission_runner.py:408] Time since start: 22246.69s, 	Step: 25254, 	{'train/ctc_loss': Array(0.23896928, dtype=float32), 'train/wer': 0.08361363496523576, 'validation/ctc_loss': Array(0.5691403, dtype=float32), 'validation/wer': 0.16544213483688464, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32006806, dtype=float32), 'test/wer': 0.10419840350984096, 'test/num_examples': 2472, 'score': 20180.9346408844, 'total_duration': 22246.685710906982, 'accumulated_submission_time': 20180.9346408844, 'accumulated_eval_time': 2063.8816010951996, 'accumulated_logging_time': 0.741168737411499}
I0212 09:34:53.365267 140280856385280 logging_writer.py:48] [25254] accumulated_eval_time=2063.881601, accumulated_logging_time=0.741169, accumulated_submission_time=20180.934641, global_step=25254, preemption_count=0, score=20180.934641, test/ctc_loss=0.3200680613517761, test/num_examples=2472, test/wer=0.104198, total_duration=22246.685711, train/ctc_loss=0.23896928131580353, train/wer=0.083614, validation/ctc_loss=0.5691403150558472, validation/num_examples=5348, validation/wer=0.165442
I0212 09:35:28.576950 140280847992576 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.018578052520752, loss=1.4224028587341309
I0212 09:36:43.755513 140280856385280 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.3156421184539795, loss=1.4433881044387817
I0212 09:37:58.766068 140280847992576 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.2986900806427, loss=1.4628500938415527
I0212 09:39:17.540502 140280856385280 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.9660751819610596, loss=1.4123363494873047
I0212 09:40:43.150192 140280847992576 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.472791910171509, loss=1.473923683166504
I0212 09:42:07.228880 140280856385280 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.555983781814575, loss=1.3722864389419556
I0212 09:43:22.714913 140280847992576 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.3885982036590576, loss=1.3695156574249268
I0212 09:44:38.120034 140280856385280 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.6977109909057617, loss=1.4555256366729736
I0212 09:45:53.376695 140280847992576 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.055368423461914, loss=1.420575499534607
I0212 09:47:13.057186 140280856385280 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.282630681991577, loss=1.3696014881134033
I0212 09:48:38.217279 140280847992576 logging_writer.py:48] [26300] global_step=26300, grad_norm=4.228216171264648, loss=1.4028416872024536
I0212 09:50:04.411399 140280856385280 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.103408336639404, loss=1.4137461185455322
I0212 09:51:29.276540 140280847992576 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.7223944664001465, loss=1.3906832933425903
I0212 09:52:55.368735 140280856385280 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.770045518875122, loss=1.360547423362732
I0212 09:54:20.752203 140280847992576 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.690021514892578, loss=1.373124122619629
I0212 09:55:47.814239 140280856385280 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.664823055267334, loss=1.3723996877670288
I0212 09:57:02.791300 140280847992576 logging_writer.py:48] [26900] global_step=26900, grad_norm=12.497639656066895, loss=1.3857178688049316
I0212 09:58:17.932442 140280856385280 logging_writer.py:48] [27000] global_step=27000, grad_norm=4.372695446014404, loss=1.3826112747192383
I0212 09:58:53.779347 140437690197824 spec.py:321] Evaluating on the training split.
I0212 09:59:52.114459 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 10:00:44.302848 140437690197824 spec.py:349] Evaluating on the test split.
I0212 10:01:11.349339 140437690197824 submission_runner.py:408] Time since start: 23824.72s, 	Step: 27049, 	{'train/ctc_loss': Array(0.25387847, dtype=float32), 'train/wer': 0.08802480043076634, 'validation/ctc_loss': Array(0.55523735, dtype=float32), 'validation/wer': 0.16125201540882628, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31684002, dtype=float32), 'test/wer': 0.10348749822273678, 'test/num_examples': 2472, 'score': 21621.25813126564, 'total_duration': 23824.718508720398, 'accumulated_submission_time': 21621.25813126564, 'accumulated_eval_time': 2201.445280790329, 'accumulated_logging_time': 0.8065252304077148}
I0212 10:01:11.394320 140280856385280 logging_writer.py:48] [27049] accumulated_eval_time=2201.445281, accumulated_logging_time=0.806525, accumulated_submission_time=21621.258131, global_step=27049, preemption_count=0, score=21621.258131, test/ctc_loss=0.3168400228023529, test/num_examples=2472, test/wer=0.103487, total_duration=23824.718509, train/ctc_loss=0.25387847423553467, train/wer=0.088025, validation/ctc_loss=0.555237352848053, validation/num_examples=5348, validation/wer=0.161252
I0212 10:01:50.638464 140280847992576 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.265652894973755, loss=1.4287903308868408
I0212 10:03:05.967721 140280856385280 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.932368516921997, loss=1.4178298711776733
I0212 10:04:21.142531 140280847992576 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.69085431098938, loss=1.4226734638214111
I0212 10:05:39.630901 140280856385280 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.9295347929000854, loss=1.3671493530273438
I0212 10:07:04.353687 140280847992576 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.513181447982788, loss=1.3753132820129395
I0212 10:08:29.537107 140280856385280 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.2525863647460938, loss=1.390848159790039
I0212 10:09:54.292453 140280847992576 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.567673444747925, loss=1.4720860719680786
I0212 10:11:20.010694 140280856385280 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.087925910949707, loss=1.436837077140808
I0212 10:12:39.462889 140280856385280 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.4549663066864014, loss=1.4435116052627563
I0212 10:13:54.570678 140280847992576 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.936069130897522, loss=1.3148276805877686
I0212 10:15:09.409942 140280856385280 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.581153392791748, loss=1.4021549224853516
I0212 10:16:25.067154 140280847992576 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.075094699859619, loss=1.3203428983688354
I0212 10:17:47.792353 140280856385280 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.2795536518096924, loss=1.3492687940597534
I0212 10:19:14.413225 140280847992576 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.615675210952759, loss=1.3968884944915771
I0212 10:20:39.607525 140280856385280 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.2428858280181885, loss=1.3826029300689697
I0212 10:22:04.950391 140280847992576 logging_writer.py:48] [28600] global_step=28600, grad_norm=4.230747222900391, loss=1.4703770875930786
I0212 10:23:30.281332 140280856385280 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.876871943473816, loss=1.370623230934143
I0212 10:24:56.499373 140280847992576 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.4345998764038086, loss=1.4100418090820312
I0212 10:25:11.599446 140437690197824 spec.py:321] Evaluating on the training split.
I0212 10:26:07.249025 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 10:26:59.425543 140437690197824 spec.py:349] Evaluating on the test split.
I0212 10:27:25.943616 140437690197824 submission_runner.py:408] Time since start: 25399.31s, 	Step: 28819, 	{'train/ctc_loss': Array(0.32222232, dtype=float32), 'train/wer': 0.11008639831160551, 'validation/ctc_loss': Array(0.52448064, dtype=float32), 'validation/wer': 0.15450341291985673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29585266, dtype=float32), 'test/wer': 0.09544411268864379, 'test/num_examples': 2472, 'score': 23061.37408900261, 'total_duration': 25399.31268310547, 'accumulated_submission_time': 23061.37408900261, 'accumulated_eval_time': 2335.782987356186, 'accumulated_logging_time': 0.8676490783691406}
I0212 10:27:25.982518 140280856385280 logging_writer.py:48] [28819] accumulated_eval_time=2335.782987, accumulated_logging_time=0.867649, accumulated_submission_time=23061.374089, global_step=28819, preemption_count=0, score=23061.374089, test/ctc_loss=0.2958526611328125, test/num_examples=2472, test/wer=0.095444, total_duration=25399.312683, train/ctc_loss=0.3222223222255707, train/wer=0.110086, validation/ctc_loss=0.5244806408882141, validation/num_examples=5348, validation/wer=0.154503
I0212 10:28:31.445062 140280856385280 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.367316961288452, loss=1.3482471704483032
I0212 10:29:46.457667 140280847992576 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.5366885662078857, loss=1.3219691514968872
I0212 10:31:01.562353 140280856385280 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.5348448753356934, loss=1.3426786661148071
I0212 10:32:18.267230 140280847992576 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.9006636142730713, loss=1.3431682586669922
I0212 10:33:41.058271 140280856385280 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.610380172729492, loss=1.3077938556671143
I0212 10:35:06.001100 140280847992576 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.5056073665618896, loss=1.3810328245162964
I0212 10:36:31.023045 140280856385280 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.5458831787109375, loss=1.358132243156433
I0212 10:37:56.180577 140280847992576 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.553687334060669, loss=1.4029483795166016
I0212 10:39:21.423605 140280856385280 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.4017224311828613, loss=1.3013176918029785
I0212 10:40:47.443651 140280847992576 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.831770658493042, loss=1.3330192565917969
I0212 10:42:12.860465 140280856385280 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.22023344039917, loss=1.3432741165161133
I0212 10:43:27.939919 140280847992576 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.0221519470214844, loss=1.340813159942627
I0212 10:44:43.219252 140280856385280 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.673027992248535, loss=1.3028709888458252
I0212 10:45:58.313154 140280847992576 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.430910110473633, loss=1.4010388851165771
I0212 10:47:14.776280 140280856385280 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.362267017364502, loss=1.3572548627853394
I0212 10:48:39.232257 140280847992576 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.398977518081665, loss=1.2798007726669312
I0212 10:50:04.334616 140280856385280 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.416010618209839, loss=1.3125524520874023
I0212 10:51:26.055129 140437690197824 spec.py:321] Evaluating on the training split.
I0212 10:52:22.448033 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 10:53:14.501718 140437690197824 spec.py:349] Evaluating on the test split.
I0212 10:53:41.587325 140437690197824 submission_runner.py:408] Time since start: 26974.96s, 	Step: 30598, 	{'train/ctc_loss': Array(0.33865708, dtype=float32), 'train/wer': 0.1133199141616622, 'validation/ctc_loss': Array(0.5113746, dtype=float32), 'validation/wer': 0.14911611651235313, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28533491, dtype=float32), 'test/wer': 0.0923973757439116, 'test/num_examples': 2472, 'score': 24501.35665678978, 'total_duration': 26974.95587038994, 'accumulated_submission_time': 24501.35665678978, 'accumulated_eval_time': 2471.308205604553, 'accumulated_logging_time': 0.9225730895996094}
I0212 10:53:41.626874 140280856385280 logging_writer.py:48] [30598] accumulated_eval_time=2471.308206, accumulated_logging_time=0.922573, accumulated_submission_time=24501.356657, global_step=30598, preemption_count=0, score=24501.356657, test/ctc_loss=0.28533491492271423, test/num_examples=2472, test/wer=0.092397, total_duration=26974.955870, train/ctc_loss=0.33865708112716675, train/wer=0.113320, validation/ctc_loss=0.5113745927810669, validation/num_examples=5348, validation/wer=0.149116
I0212 10:53:44.006047 140280847992576 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.5146398544311523, loss=1.3133162260055542
I0212 10:54:59.124275 140280856385280 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.373534917831421, loss=1.3506203889846802
I0212 10:56:14.157634 140280847992576 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.724142551422119, loss=1.3517636060714722
I0212 10:57:32.991912 140280856385280 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.44677996635437, loss=1.3276923894882202
I0212 10:58:47.893590 140280847992576 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.9472949504852295, loss=1.3242406845092773
I0212 11:00:03.118319 140280856385280 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.775130271911621, loss=1.315285563468933
I0212 11:01:18.381947 140280847992576 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.040884017944336, loss=1.2988860607147217
I0212 11:02:34.149870 140280856385280 logging_writer.py:48] [31300] global_step=31300, grad_norm=4.000020503997803, loss=1.3022381067276
I0212 11:03:57.387655 140280847992576 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.3923709392547607, loss=1.2915637493133545
I0212 11:05:22.246626 140280856385280 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.1752564907073975, loss=1.3342139720916748
I0212 11:06:47.095633 140280847992576 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.384739875793457, loss=1.3575371503829956
I0212 11:08:12.401105 140280856385280 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.004359483718872, loss=1.3630740642547607
I0212 11:09:37.390866 140280847992576 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.3342444896698, loss=1.3745993375778198
I0212 11:11:03.002156 140280856385280 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.552018880844116, loss=1.3088418245315552
I0212 11:12:25.052724 140280856385280 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.5269322395324707, loss=1.3373922109603882
I0212 11:13:40.224231 140280847992576 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.851311683654785, loss=1.3355209827423096
I0212 11:14:55.287917 140280856385280 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.505434513092041, loss=1.3463102579116821
I0212 11:16:10.288309 140280847992576 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.3152668476104736, loss=1.3119640350341797
I0212 11:17:29.730669 140280856385280 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.4063804149627686, loss=1.316867709159851
I0212 11:17:42.072032 140437690197824 spec.py:321] Evaluating on the training split.
I0212 11:18:36.594619 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 11:19:28.504795 140437690197824 spec.py:349] Evaluating on the test split.
I0212 11:19:54.822489 140437690197824 submission_runner.py:408] Time since start: 28548.19s, 	Step: 32416, 	{'train/ctc_loss': Array(0.3645432, dtype=float32), 'train/wer': 0.12535386114720623, 'validation/ctc_loss': Array(0.5027272, dtype=float32), 'validation/wer': 0.14581422516581866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2788384, dtype=float32), 'test/wer': 0.09026465988259907, 'test/num_examples': 2472, 'score': 25941.709431886673, 'total_duration': 28548.19150686264, 'accumulated_submission_time': 25941.709431886673, 'accumulated_eval_time': 2604.052140712738, 'accumulated_logging_time': 0.9797863960266113}
I0212 11:19:54.862408 140280856385280 logging_writer.py:48] [32416] accumulated_eval_time=2604.052141, accumulated_logging_time=0.979786, accumulated_submission_time=25941.709432, global_step=32416, preemption_count=0, score=25941.709432, test/ctc_loss=0.2788383960723877, test/num_examples=2472, test/wer=0.090265, total_duration=28548.191507, train/ctc_loss=0.36454319953918457, train/wer=0.125354, validation/ctc_loss=0.502727210521698, validation/num_examples=5348, validation/wer=0.145814
I0212 11:20:59.028974 140280847992576 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.8065683841705322, loss=1.330718755722046
I0212 11:22:14.229016 140280856385280 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.160102128982544, loss=1.2168552875518799
I0212 11:23:29.570492 140280847992576 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.188342809677124, loss=1.2908803224563599
I0212 11:24:50.128348 140280856385280 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.3066086769104004, loss=1.3184677362442017
I0212 11:26:15.085510 140280847992576 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.9522674083709717, loss=1.3276194334030151
I0212 11:27:39.450776 140280856385280 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.8268394470214844, loss=1.3034552335739136
I0212 11:28:54.587857 140280847992576 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.357374668121338, loss=1.3003822565078735
I0212 11:30:09.619291 140280856385280 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.9299509525299072, loss=1.318994164466858
I0212 11:31:24.847345 140280847992576 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.7203633785247803, loss=1.3381508588790894
I0212 11:32:40.947252 140280856385280 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.8640894889831543, loss=1.309043526649475
I0212 11:34:05.410919 140280847992576 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.550934314727783, loss=1.2963159084320068
I0212 11:35:30.274512 140280856385280 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.9030568599700928, loss=1.3079326152801514
I0212 11:36:56.039057 140280847992576 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.675323009490967, loss=1.3316606283187866
I0212 11:38:21.042042 140280856385280 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.6119866371154785, loss=1.2923126220703125
I0212 11:39:46.134272 140280847992576 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.702094554901123, loss=1.2576419115066528
I0212 11:41:14.025909 140280856385280 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.4987337589263916, loss=1.2573996782302856
I0212 11:42:29.252456 140280847992576 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.7106382846832275, loss=1.255157232284546
I0212 11:43:44.491665 140280856385280 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.5241103172302246, loss=1.2740168571472168
I0212 11:43:54.934303 140437690197824 spec.py:321] Evaluating on the training split.
I0212 11:44:49.912311 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 11:45:41.797603 140437690197824 spec.py:349] Evaluating on the test split.
I0212 11:46:08.632872 140437690197824 submission_runner.py:408] Time since start: 30122.00s, 	Step: 34215, 	{'train/ctc_loss': Array(0.31370944, dtype=float32), 'train/wer': 0.10446648667684, 'validation/ctc_loss': Array(0.4884356, dtype=float32), 'validation/wer': 0.14066829508481613, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2685194, dtype=float32), 'test/wer': 0.08486177970060732, 'test/num_examples': 2472, 'score': 27381.691277742386, 'total_duration': 30122.002299785614, 'accumulated_submission_time': 27381.691277742386, 'accumulated_eval_time': 2737.744610309601, 'accumulated_logging_time': 1.035611629486084}
I0212 11:46:08.671130 140280856385280 logging_writer.py:48] [34215] accumulated_eval_time=2737.744610, accumulated_logging_time=1.035612, accumulated_submission_time=27381.691278, global_step=34215, preemption_count=0, score=27381.691278, test/ctc_loss=0.26851940155029297, test/num_examples=2472, test/wer=0.084862, total_duration=30122.002300, train/ctc_loss=0.31370943784713745, train/wer=0.104466, validation/ctc_loss=0.4884355962276459, validation/num_examples=5348, validation/wer=0.140668
I0212 11:47:13.393167 140280847992576 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.527981996536255, loss=1.2902135848999023
I0212 11:48:28.643222 140280856385280 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.825789213180542, loss=1.2766395807266235
I0212 11:49:43.947318 140280847992576 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.527167797088623, loss=1.276628851890564
I0212 11:51:01.338646 140280856385280 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.384040355682373, loss=1.2417370080947876
I0212 11:52:26.462253 140280847992576 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.5708086490631104, loss=1.2640416622161865
I0212 11:53:51.209661 140280856385280 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.074357509613037, loss=1.2215766906738281
I0212 11:55:16.895915 140280847992576 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.6068460941314697, loss=1.298672080039978
I0212 11:56:41.927531 140280856385280 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.554163932800293, loss=1.2794084548950195
I0212 11:58:03.071124 140280856385280 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.470710039138794, loss=1.2487045526504517
I0212 11:59:18.186526 140280847992576 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.9534194469451904, loss=1.280801773071289
I0212 12:00:33.379613 140280856385280 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.011920928955078, loss=1.2596498727798462
I0212 12:01:48.628294 140280847992576 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.920239210128784, loss=1.212456464767456
I0212 12:03:08.041759 140280856385280 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.524315595626831, loss=1.2236213684082031
I0212 12:04:33.912494 140280847992576 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.8925747871398926, loss=1.2258195877075195
I0212 12:05:58.975908 140280856385280 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.3261659145355225, loss=1.2577859163284302
I0212 12:07:25.113324 140280847992576 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.609103202819824, loss=1.2448821067810059
I0212 12:08:51.411587 140280856385280 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.593442678451538, loss=1.2208380699157715
I0212 12:10:09.354826 140437690197824 spec.py:321] Evaluating on the training split.
I0212 12:11:04.652675 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 12:11:56.996419 140437690197824 spec.py:349] Evaluating on the test split.
I0212 12:12:23.795887 140437690197824 submission_runner.py:408] Time since start: 31697.16s, 	Step: 35991, 	{'train/ctc_loss': Array(0.2759164, dtype=float32), 'train/wer': 0.09577486949241118, 'validation/ctc_loss': Array(0.46581116, dtype=float32), 'validation/wer': 0.13578304063643473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25631005, dtype=float32), 'test/wer': 0.08289155647634716, 'test/num_examples': 2472, 'score': 28822.284260749817, 'total_duration': 31697.164457798004, 'accumulated_submission_time': 28822.284260749817, 'accumulated_eval_time': 2872.178744316101, 'accumulated_logging_time': 1.0904130935668945}
I0212 12:12:23.836285 140280856385280 logging_writer.py:48] [35991] accumulated_eval_time=2872.178744, accumulated_logging_time=1.090413, accumulated_submission_time=28822.284261, global_step=35991, preemption_count=0, score=28822.284261, test/ctc_loss=0.25631004571914673, test/num_examples=2472, test/wer=0.082892, total_duration=31697.164458, train/ctc_loss=0.2759163975715637, train/wer=0.095775, validation/ctc_loss=0.465811163187027, validation/num_examples=5348, validation/wer=0.135783
I0212 12:12:31.513147 140280847992576 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.2198057174682617, loss=1.295809030532837
I0212 12:13:51.023456 140280856385280 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.1849007606506348, loss=1.199516773223877
I0212 12:15:06.726187 140280847992576 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.3844916820526123, loss=1.1864498853683472
I0212 12:16:22.023720 140280856385280 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.057168483734131, loss=1.2506550550460815
I0212 12:17:39.516622 140280847992576 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.67409086227417, loss=1.2453105449676514
I0212 12:19:02.671335 140280856385280 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.7412407398223877, loss=1.2454156875610352
I0212 12:20:27.353339 140280847992576 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.916767120361328, loss=1.2947683334350586
I0212 12:21:53.222257 140280856385280 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.528212308883667, loss=1.24189293384552
I0212 12:23:18.639063 140280847992576 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.1987922191619873, loss=1.2541989088058472
I0212 12:24:43.803198 140280856385280 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.7388932704925537, loss=1.186029076576233
I0212 12:26:09.142870 140280847992576 logging_writer.py:48] [37000] global_step=37000, grad_norm=4.4514665603637695, loss=1.205482840538025
I0212 12:27:35.421690 140280856385280 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.7640488147735596, loss=1.1732006072998047
I0212 12:28:50.709852 140280847992576 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.148926258087158, loss=1.2037482261657715
I0212 12:30:05.976605 140280856385280 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.4885246753692627, loss=1.2121384143829346
I0212 12:31:21.294699 140280847992576 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.449838638305664, loss=1.1921024322509766
I0212 12:32:40.137259 140280856385280 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.441767692565918, loss=1.1803667545318604
I0212 12:34:05.691563 140280847992576 logging_writer.py:48] [37600] global_step=37600, grad_norm=4.4770355224609375, loss=1.2399742603302002
I0212 12:35:31.660342 140280856385280 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.8459279537200928, loss=1.2383733987808228
I0212 12:36:23.822216 140437690197824 spec.py:321] Evaluating on the training split.
I0212 12:37:20.325862 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 12:38:12.543519 140437690197824 spec.py:349] Evaluating on the test split.
I0212 12:38:39.367457 140437690197824 submission_runner.py:408] Time since start: 33272.74s, 	Step: 37763, 	{'train/ctc_loss': Array(0.2330637, dtype=float32), 'train/wer': 0.08139264530152533, 'validation/ctc_loss': Array(0.4498272, dtype=float32), 'validation/wer': 0.13042470818811125, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25000796, dtype=float32), 'test/wer': 0.0806166595576138, 'test/num_examples': 2472, 'score': 30262.179399967194, 'total_duration': 33272.73663830757, 'accumulated_submission_time': 30262.179399967194, 'accumulated_eval_time': 3007.717676639557, 'accumulated_logging_time': 1.1477704048156738}
I0212 12:38:39.408603 140280856385280 logging_writer.py:48] [37763] accumulated_eval_time=3007.717677, accumulated_logging_time=1.147770, accumulated_submission_time=30262.179400, global_step=37763, preemption_count=0, score=30262.179400, test/ctc_loss=0.2500079572200775, test/num_examples=2472, test/wer=0.080617, total_duration=33272.736638, train/ctc_loss=0.2330636978149414, train/wer=0.081393, validation/ctc_loss=0.4498271942138672, validation/num_examples=5348, validation/wer=0.130425
I0212 12:39:08.003081 140280847992576 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.436387538909912, loss=1.1911218166351318
I0212 12:40:23.290361 140280856385280 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.1864991188049316, loss=1.2678453922271729
I0212 12:41:38.375257 140280847992576 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.66882586479187, loss=1.2406853437423706
I0212 12:42:55.814694 140280856385280 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.9775795936584473, loss=1.214468240737915
I0212 12:44:15.303498 140280856385280 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.348663091659546, loss=1.1941330432891846
I0212 12:45:30.393541 140280847992576 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.4398458003997803, loss=1.275214672088623
I0212 12:46:45.447992 140280856385280 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.696371555328369, loss=1.1730390787124634
I0212 12:48:01.353587 140280847992576 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.5454885959625244, loss=1.1733258962631226
I0212 12:49:25.941223 140280856385280 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.4148764610290527, loss=1.1832197904586792
I0212 12:50:51.395516 140280847992576 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.626239061355591, loss=1.253828763961792
I0212 12:52:17.318054 140280856385280 logging_writer.py:48] [38800] global_step=38800, grad_norm=4.128517150878906, loss=1.22029447555542
I0212 12:53:42.984056 140280847992576 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.2759249210357666, loss=1.2160264253616333
I0212 12:55:08.778660 140280856385280 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.2813560962677, loss=1.1932774782180786
I0212 12:56:34.372305 140280847992576 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.742790460586548, loss=1.1949557065963745
I0212 12:57:57.091115 140280856385280 logging_writer.py:48] [39200] global_step=39200, grad_norm=4.488079071044922, loss=1.1935032606124878
I0212 12:59:12.170175 140280847992576 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.079956531524658, loss=1.1538845300674438
I0212 13:00:27.403323 140280856385280 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.6779115200042725, loss=1.1420902013778687
I0212 13:01:42.983309 140280847992576 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.4636967182159424, loss=1.1506304740905762
I0212 13:02:39.810553 140437690197824 spec.py:321] Evaluating on the training split.
I0212 13:03:36.167845 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 13:04:28.145507 140437690197824 spec.py:349] Evaluating on the test split.
I0212 13:04:54.580750 140437690197824 submission_runner.py:408] Time since start: 34847.95s, 	Step: 39574, 	{'train/ctc_loss': Array(0.26204503, dtype=float32), 'train/wer': 0.0910593137147968, 'validation/ctc_loss': Array(0.4422166, dtype=float32), 'validation/wer': 0.12872548924954383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24149479, dtype=float32), 'test/wer': 0.07698088680356671, 'test/num_examples': 2472, 'score': 31702.490131378174, 'total_duration': 34847.95053982735, 'accumulated_submission_time': 31702.490131378174, 'accumulated_eval_time': 3142.482138156891, 'accumulated_logging_time': 1.2042455673217773}
I0212 13:04:54.627920 140280856385280 logging_writer.py:48] [39574] accumulated_eval_time=3142.482138, accumulated_logging_time=1.204246, accumulated_submission_time=31702.490131, global_step=39574, preemption_count=0, score=31702.490131, test/ctc_loss=0.2414947897195816, test/num_examples=2472, test/wer=0.076981, total_duration=34847.950540, train/ctc_loss=0.2620450258255005, train/wer=0.091059, validation/ctc_loss=0.4422166049480438, validation/num_examples=5348, validation/wer=0.128725
I0212 13:05:15.012331 140280847992576 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.6583712100982666, loss=1.2808831930160522
I0212 13:06:30.287730 140280856385280 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.2781920433044434, loss=1.1248759031295776
I0212 13:07:45.555757 140280847992576 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.0251219272613525, loss=1.1966454982757568
I0212 13:09:02.478931 140280856385280 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.9279913902282715, loss=1.1530343294143677
I0212 13:10:27.395954 140280847992576 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.838618278503418, loss=1.12819242477417
I0212 13:11:52.970642 140280856385280 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.894390821456909, loss=1.180659532546997
I0212 13:13:19.664396 140280856385280 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.900740623474121, loss=1.1628791093826294
I0212 13:14:34.855906 140280847992576 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.697840452194214, loss=1.1100302934646606
I0212 13:15:49.988135 140280856385280 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.3010637760162354, loss=1.1899420022964478
I0212 13:17:05.322450 140280847992576 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.344137191772461, loss=1.1738598346710205
I0212 13:18:24.024408 140280856385280 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.9956583976745605, loss=1.1788792610168457
I0212 13:19:48.484642 140280847992576 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.6556005477905273, loss=1.150819182395935
I0212 13:21:14.300702 140280856385280 logging_writer.py:48] [40800] global_step=40800, grad_norm=4.487297534942627, loss=1.1825448274612427
I0212 13:22:39.918529 140280847992576 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.77873158454895, loss=1.15926992893219
I0212 13:24:06.157971 140280856385280 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.4830446243286133, loss=1.189257264137268
I0212 13:25:32.159857 140280847992576 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.5584614276885986, loss=1.1955653429031372
I0212 13:27:01.470421 140280856385280 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.156233787536621, loss=1.1789361238479614
I0212 13:28:16.554820 140280847992576 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.014521360397339, loss=1.169133186340332
I0212 13:28:54.662879 140437690197824 spec.py:321] Evaluating on the training split.
I0212 13:29:51.866922 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 13:30:44.564667 140437690197824 spec.py:349] Evaluating on the test split.
I0212 13:31:10.928399 140437690197824 submission_runner.py:408] Time since start: 36424.30s, 	Step: 41352, 	{'train/ctc_loss': Array(0.22383782, dtype=float32), 'train/wer': 0.07813602823227463, 'validation/ctc_loss': Array(0.43122214, dtype=float32), 'validation/wer': 0.12468984427044615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23231012, dtype=float32), 'test/wer': 0.07523409095525359, 'test/num_examples': 2472, 'score': 33142.4312608242, 'total_duration': 36424.297860622406, 'accumulated_submission_time': 33142.4312608242, 'accumulated_eval_time': 3278.741602897644, 'accumulated_logging_time': 1.27134108543396}
I0212 13:31:10.969619 140280856385280 logging_writer.py:48] [41352] accumulated_eval_time=3278.741603, accumulated_logging_time=1.271341, accumulated_submission_time=33142.431261, global_step=41352, preemption_count=0, score=33142.431261, test/ctc_loss=0.23231011629104614, test/num_examples=2472, test/wer=0.075234, total_duration=36424.297861, train/ctc_loss=0.22383782267570496, train/wer=0.078136, validation/ctc_loss=0.431222140789032, validation/num_examples=5348, validation/wer=0.124690
I0212 13:31:47.792643 140280847992576 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.9533421993255615, loss=1.2048758268356323
I0212 13:33:02.836001 140280856385280 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.259965181350708, loss=1.1484078168869019
I0212 13:34:17.929686 140280847992576 logging_writer.py:48] [41600] global_step=41600, grad_norm=4.092648506164551, loss=1.1640987396240234
I0212 13:35:33.187940 140280856385280 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.294712543487549, loss=1.129801630973816
I0212 13:36:51.894304 140280847992576 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.2625513076782227, loss=1.2048043012619019
I0212 13:38:16.882537 140280856385280 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.561624765396118, loss=1.1669408082962036
I0212 13:39:41.421502 140280847992576 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.951955795288086, loss=1.1796493530273438
I0212 13:41:06.613085 140280856385280 logging_writer.py:48] [42100] global_step=42100, grad_norm=4.363100528717041, loss=1.185779333114624
I0212 13:42:31.411406 140280847992576 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.1201982498168945, loss=1.1494359970092773
I0212 13:43:52.789792 140280856385280 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.142092227935791, loss=1.125335931777954
I0212 13:45:07.704416 140280847992576 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.1697933673858643, loss=1.138269066810608
I0212 13:46:22.590403 140280856385280 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.566556453704834, loss=1.1652634143829346
I0212 13:47:37.683913 140280847992576 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.865558624267578, loss=1.1909959316253662
I0212 13:48:58.069130 140280856385280 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.900662422180176, loss=1.1098285913467407
I0212 13:50:23.169482 140280847992576 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.9249916076660156, loss=1.1843507289886475
I0212 13:51:49.462034 140280856385280 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.174365520477295, loss=1.1673197746276855
I0212 13:53:14.626646 140280847992576 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.980829119682312, loss=1.120314359664917
I0212 13:54:39.845878 140280856385280 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.4094574451446533, loss=1.0955058336257935
I0212 13:55:11.015471 140437690197824 spec.py:321] Evaluating on the training split.
I0212 13:56:07.925750 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 13:56:59.684164 140437690197824 spec.py:349] Evaluating on the test split.
I0212 13:57:25.824304 140437690197824 submission_runner.py:408] Time since start: 37999.19s, 	Step: 43138, 	{'train/ctc_loss': Array(0.21453248, dtype=float32), 'train/wer': 0.0768669198206493, 'validation/ctc_loss': Array(0.42354012, dtype=float32), 'validation/wer': 0.12261409386253705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2263191, dtype=float32), 'test/wer': 0.0726951435013101, 'test/num_examples': 2472, 'score': 34582.384875535965, 'total_duration': 37999.1936173439, 'accumulated_submission_time': 34582.384875535965, 'accumulated_eval_time': 3413.544237613678, 'accumulated_logging_time': 1.331352949142456}
I0212 13:57:25.863302 140280856385280 logging_writer.py:48] [43138] accumulated_eval_time=3413.544238, accumulated_logging_time=1.331353, accumulated_submission_time=34582.384876, global_step=43138, preemption_count=0, score=34582.384876, test/ctc_loss=0.2263191044330597, test/num_examples=2472, test/wer=0.072695, total_duration=37999.193617, train/ctc_loss=0.21453247964382172, train/wer=0.076867, validation/ctc_loss=0.4235401153564453, validation/num_examples=5348, validation/wer=0.122614
I0212 13:58:13.274205 140280847992576 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.4176523685455322, loss=1.1250091791152954
I0212 13:59:32.127977 140280856385280 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.419556140899658, loss=1.1486109495162964
I0212 14:00:47.194835 140280847992576 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.273624897003174, loss=1.1500401496887207
I0212 14:02:02.161043 140280856385280 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.5954623222351074, loss=1.1121962070465088
I0212 14:03:17.286335 140280847992576 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.9452643394470215, loss=1.1704591512680054
I0212 14:04:38.738383 140280856385280 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.465827226638794, loss=1.0956364870071411
I0212 14:06:04.024047 140280847992576 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.4280638694763184, loss=1.161257028579712
I0212 14:07:30.202957 140280856385280 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.265634298324585, loss=1.18997323513031
I0212 14:08:56.337502 140280847992576 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.264721632003784, loss=1.1237893104553223
I0212 14:10:21.626512 140280856385280 logging_writer.py:48] [44100] global_step=44100, grad_norm=5.5656232833862305, loss=1.1085233688354492
I0212 14:11:47.283537 140280847992576 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.459961175918579, loss=1.0507314205169678
I0212 14:13:15.280844 140280856385280 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.9413187503814697, loss=1.1453566551208496
I0212 14:14:30.246232 140280847992576 logging_writer.py:48] [44400] global_step=44400, grad_norm=4.481436252593994, loss=1.1317601203918457
I0212 14:15:45.338413 140280856385280 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.7806880474090576, loss=1.1148475408554077
I0212 14:17:00.335682 140280847992576 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.8092589378356934, loss=1.1411610841751099
I0212 14:18:16.273572 140280856385280 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.2876510620117188, loss=1.105838656425476
I0212 14:19:39.856297 140280847992576 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.9691078662872314, loss=1.162493109703064
I0212 14:21:04.798238 140280856385280 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.333712577819824, loss=1.1444921493530273
I0212 14:21:25.989482 140437690197824 spec.py:321] Evaluating on the training split.
I0212 14:22:22.924346 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 14:23:15.193139 140437690197824 spec.py:349] Evaluating on the test split.
I0212 14:23:41.946577 140437690197824 submission_runner.py:408] Time since start: 39575.32s, 	Step: 44926, 	{'train/ctc_loss': Array(0.21462, dtype=float32), 'train/wer': 0.07510113246434327, 'validation/ctc_loss': Array(0.41773412, dtype=float32), 'validation/wer': 0.12063488998522838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2226167, dtype=float32), 'test/wer': 0.07228891190867914, 'test/num_examples': 2472, 'score': 36022.423419713974, 'total_duration': 39575.315915346146, 'accumulated_submission_time': 36022.423419713974, 'accumulated_eval_time': 3549.495146036148, 'accumulated_logging_time': 1.3847966194152832}
I0212 14:23:41.989335 140280856385280 logging_writer.py:48] [44926] accumulated_eval_time=3549.495146, accumulated_logging_time=1.384797, accumulated_submission_time=36022.423420, global_step=44926, preemption_count=0, score=36022.423420, test/ctc_loss=0.22261670231819153, test/num_examples=2472, test/wer=0.072289, total_duration=39575.315915, train/ctc_loss=0.21461999416351318, train/wer=0.075101, validation/ctc_loss=0.4177341163158417, validation/num_examples=5348, validation/wer=0.120635
I0212 14:24:38.442663 140280847992576 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.6254374980926514, loss=1.1414121389389038
I0212 14:25:53.789591 140280856385280 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.760821580886841, loss=1.1219514608383179
I0212 14:27:09.088723 140280847992576 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.788379192352295, loss=1.087235927581787
I0212 14:28:30.819672 140280856385280 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.5577380657196045, loss=1.126084804534912
I0212 14:29:51.358484 140280856385280 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.236401319503784, loss=1.1508352756500244
I0212 14:31:06.442395 140280847992576 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.57802414894104, loss=1.110683798789978
I0212 14:32:21.568355 140280856385280 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.375103712081909, loss=1.078940749168396
I0212 14:33:36.646097 140280847992576 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.7137601375579834, loss=1.1747137308120728
I0212 14:34:58.084611 140280856385280 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.5746355056762695, loss=1.0824633836746216
I0212 14:36:23.050782 140280847992576 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.0369696617126465, loss=1.1289793252944946
I0212 14:37:48.918631 140280856385280 logging_writer.py:48] [46000] global_step=46000, grad_norm=4.650227069854736, loss=1.1283854246139526
I0212 14:39:13.697026 140280847992576 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.4959611892700195, loss=1.1460919380187988
I0212 14:40:38.981348 140280856385280 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.4779412746429443, loss=1.1492807865142822
I0212 14:42:04.597566 140280847992576 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.531752586364746, loss=1.1216620206832886
I0212 14:43:29.168607 140280856385280 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.609678030014038, loss=1.131826639175415
I0212 14:44:44.408515 140280847992576 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.345555543899536, loss=1.147412657737732
I0212 14:45:59.718267 140280856385280 logging_writer.py:48] [46600] global_step=46600, grad_norm=4.080974578857422, loss=1.094542145729065
I0212 14:47:15.118677 140280847992576 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.362079381942749, loss=1.1191270351409912
I0212 14:47:42.063294 140437690197824 spec.py:321] Evaluating on the training split.
I0212 14:48:37.395980 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 14:49:28.895762 140437690197824 spec.py:349] Evaluating on the test split.
I0212 14:49:55.064432 140437690197824 submission_runner.py:408] Time since start: 41148.43s, 	Step: 46737, 	{'train/ctc_loss': Array(0.22253259, dtype=float32), 'train/wer': 0.07865858910677884, 'validation/ctc_loss': Array(0.41605195, dtype=float32), 'validation/wer': 0.12045145157708757, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2220608, dtype=float32), 'test/wer': 0.07198423821420592, 'test/num_examples': 2472, 'score': 37462.40724658966, 'total_duration': 41148.4344394207, 'accumulated_submission_time': 37462.40724658966, 'accumulated_eval_time': 3682.490786552429, 'accumulated_logging_time': 1.443239688873291}
I0212 14:49:55.110837 140280856385280 logging_writer.py:48] [46737] accumulated_eval_time=3682.490787, accumulated_logging_time=1.443240, accumulated_submission_time=37462.407247, global_step=46737, preemption_count=0, score=37462.407247, test/ctc_loss=0.22206079959869385, test/num_examples=2472, test/wer=0.071984, total_duration=41148.434439, train/ctc_loss=0.22253258526325226, train/wer=0.078659, validation/ctc_loss=0.4160519540309906, validation/num_examples=5348, validation/wer=0.120451
I0212 14:50:43.230462 140280847992576 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.6650969982147217, loss=1.168215036392212
I0212 14:51:58.326441 140280856385280 logging_writer.py:48] [46900] global_step=46900, grad_norm=4.4028544425964355, loss=1.1258392333984375
I0212 14:53:13.804251 140280847992576 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.245408296585083, loss=1.1064438819885254
I0212 14:54:35.108933 140280856385280 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.5155980587005615, loss=1.1256648302078247
I0212 14:56:00.195376 140280847992576 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.87671160697937, loss=1.1183398962020874
I0212 14:57:25.268324 140280856385280 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.2364423274993896, loss=1.123225212097168
I0212 14:58:52.111308 140280856385280 logging_writer.py:48] [47400] global_step=47400, grad_norm=4.367217063903809, loss=1.113265037536621
I0212 15:00:07.445650 140280847992576 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.9716718196868896, loss=1.1467283964157104
I0212 15:01:22.578686 140280856385280 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.6441943645477295, loss=1.1537656784057617
I0212 15:02:37.612210 140280847992576 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.3412628173828125, loss=1.1590864658355713
I0212 15:03:52.741108 140280856385280 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.732379913330078, loss=1.104468822479248
I0212 15:05:16.472315 140280847992576 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.020733118057251, loss=1.0827094316482544
I0212 15:06:39.859119 140437690197824 spec.py:321] Evaluating on the training split.
I0212 15:07:36.620556 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 15:08:29.183302 140437690197824 spec.py:349] Evaluating on the test split.
I0212 15:08:55.774422 140437690197824 submission_runner.py:408] Time since start: 42289.14s, 	Step: 48000, 	{'train/ctc_loss': Array(0.23057689, dtype=float32), 'train/wer': 0.07855449653186469, 'validation/ctc_loss': Array(0.41583607, dtype=float32), 'validation/wer': 0.12014250267916622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22198159, dtype=float32), 'test/wer': 0.071801433997522, 'test/num_examples': 2472, 'score': 38467.08563065529, 'total_duration': 42289.14387321472, 'accumulated_submission_time': 38467.08563065529, 'accumulated_eval_time': 3818.4000244140625, 'accumulated_logging_time': 1.5066735744476318}
I0212 15:08:55.817669 140280856385280 logging_writer.py:48] [48000] accumulated_eval_time=3818.400024, accumulated_logging_time=1.506674, accumulated_submission_time=38467.085631, global_step=48000, preemption_count=0, score=38467.085631, test/ctc_loss=0.22198158502578735, test/num_examples=2472, test/wer=0.071801, total_duration=42289.143873, train/ctc_loss=0.23057688772678375, train/wer=0.078554, validation/ctc_loss=0.41583606600761414, validation/num_examples=5348, validation/wer=0.120143
I0212 15:08:55.844752 140280847992576 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=38467.085631
I0212 15:08:56.033479 140437690197824 checkpoints.py:490] Saving checkpoint at step: 48000
I0212 15:08:56.981059 140437690197824 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_3/checkpoint_48000
I0212 15:08:57.000914 140437690197824 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_3/checkpoint_48000.
I0212 15:08:58.329358 140437690197824 submission_runner.py:583] Tuning trial 3/5
I0212 15:08:58.329612 140437690197824 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0212 15:08:58.343339 140437690197824 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.26521, dtype=float32), 'train/wer': 3.632442748091603, 'validation/ctc_loss': Array(30.89189, dtype=float32), 'validation/wer': 3.325081823184684, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942295, dtype=float32), 'test/wer': 3.6990636361789857, 'test/num_examples': 2472, 'score': 15.847177743911743, 'total_duration': 213.7028248310089, 'accumulated_submission_time': 15.847177743911743, 'accumulated_eval_time': 197.85556888580322, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1800, {'train/ctc_loss': Array(5.9749413, dtype=float32), 'train/wer': 0.9422576974075737, 'validation/ctc_loss': Array(5.959475, dtype=float32), 'validation/wer': 0.8962607528698456, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.869004, dtype=float32), 'test/wer': 0.898990514492312, 'test/num_examples': 2472, 'score': 1456.7463419437408, 'total_duration': 1765.605096578598, 'accumulated_submission_time': 1456.7463419437408, 'accumulated_eval_time': 308.75326776504517, 'accumulated_logging_time': 0.029464244842529297, 'global_step': 1800, 'preemption_count': 0}), (3622, {'train/ctc_loss': Array(3.9224856, dtype=float32), 'train/wer': 0.8321735789451216, 'validation/ctc_loss': Array(4.115441, dtype=float32), 'validation/wer': 0.8269982718171023, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.7333565, dtype=float32), 'test/wer': 0.7959092478622063, 'test/num_examples': 2472, 'score': 2897.1611421108246, 'total_duration': 3323.7894248962402, 'accumulated_submission_time': 2897.1611421108246, 'accumulated_eval_time': 426.3852117061615, 'accumulated_logging_time': 0.08456134796142578, 'global_step': 3622, 'preemption_count': 0}), (5435, {'train/ctc_loss': Array(0.58773357, dtype=float32), 'train/wer': 0.2038345540715209, 'validation/ctc_loss': Array(0.9542844, dtype=float32), 'validation/wer': 0.2757272367417477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6223996, dtype=float32), 'test/wer': 0.20675156906952655, 'test/num_examples': 2472, 'score': 4337.0904133319855, 'total_duration': 4898.577420711517, 'accumulated_submission_time': 4337.0904133319855, 'accumulated_eval_time': 561.1081030368805, 'accumulated_logging_time': 0.13887548446655273, 'global_step': 5435, 'preemption_count': 0}), (7216, {'train/ctc_loss': Array(0.47603443, dtype=float32), 'train/wer': 0.16325389943480395, 'validation/ctc_loss': Array(0.834246, dtype=float32), 'validation/wer': 0.24146287303165762, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5297743, dtype=float32), 'test/wer': 0.17311559319968314, 'test/num_examples': 2472, 'score': 5777.671191692352, 'total_duration': 6476.886586666107, 'accumulated_submission_time': 5777.671191692352, 'accumulated_eval_time': 698.7054476737976, 'accumulated_logging_time': 0.18925881385803223, 'global_step': 7216, 'preemption_count': 0}), (9030, {'train/ctc_loss': Array(0.42759815, dtype=float32), 'train/wer': 0.14474628882741727, 'validation/ctc_loss': Array(0.7569138, dtype=float32), 'validation/wer': 0.22058951311584618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4683264, dtype=float32), 'test/wer': 0.15215404301992566, 'test/num_examples': 2472, 'score': 7218.265148639679, 'total_duration': 8053.141635894775, 'accumulated_submission_time': 7218.265148639679, 'accumulated_eval_time': 834.2313735485077, 'accumulated_logging_time': 0.2436678409576416, 'global_step': 9030, 'preemption_count': 0}), (10847, {'train/ctc_loss': Array(0.40683183, dtype=float32), 'train/wer': 0.13968561039068522, 'validation/ctc_loss': Array(0.72036105, dtype=float32), 'validation/wer': 0.2091584038927561, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4358419, dtype=float32), 'test/wer': 0.14053581947068022, 'test/num_examples': 2472, 'score': 8658.311223983765, 'total_duration': 9630.516981840134, 'accumulated_submission_time': 8658.311223983765, 'accumulated_eval_time': 971.4258334636688, 'accumulated_logging_time': 0.29752111434936523, 'global_step': 10847, 'preemption_count': 0}), (12664, {'train/ctc_loss': Array(0.36989608, dtype=float32), 'train/wer': 0.12554296283488167, 'validation/ctc_loss': Array(0.67882633, dtype=float32), 'validation/wer': 0.19839346573080896, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40882304, dtype=float32), 'test/wer': 0.1320862023439563, 'test/num_examples': 2472, 'score': 10098.84746146202, 'total_duration': 11206.81583738327, 'accumulated_submission_time': 10098.84746146202, 'accumulated_eval_time': 1107.0503504276276, 'accumulated_logging_time': 0.35266804695129395, 'global_step': 12664, 'preemption_count': 0}), (14437, {'train/ctc_loss': Array(0.33032325, dtype=float32), 'train/wer': 0.11631720189169667, 'validation/ctc_loss': Array(0.66513646, dtype=float32), 'validation/wer': 0.1929965146702453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3992169, dtype=float32), 'test/wer': 0.13003473280116995, 'test/num_examples': 2472, 'score': 11539.406084775925, 'total_duration': 12783.127084732056, 'accumulated_submission_time': 11539.406084775925, 'accumulated_eval_time': 1242.6684551239014, 'accumulated_logging_time': 0.4081716537475586, 'global_step': 14437, 'preemption_count': 0}), (16237, {'train/ctc_loss': Array(0.31750143, dtype=float32), 'train/wer': 0.10809319153937746, 'validation/ctc_loss': Array(0.641495, dtype=float32), 'validation/wer': 0.1859099993241743, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38088343, dtype=float32), 'test/wer': 0.12294599150975971, 'test/num_examples': 2472, 'score': 12979.301619768143, 'total_duration': 14361.773697376251, 'accumulated_submission_time': 12979.301619768143, 'accumulated_eval_time': 1381.285228252411, 'accumulated_logging_time': 0.46169400215148926, 'global_step': 16237, 'preemption_count': 0}), (18055, {'train/ctc_loss': Array(0.33247074, dtype=float32), 'train/wer': 0.11449361610041117, 'validation/ctc_loss': Array(0.62319475, dtype=float32), 'validation/wer': 0.18160402405939544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36559522, dtype=float32), 'test/wer': 0.11841650925192453, 'test/num_examples': 2472, 'score': 14419.182766199112, 'total_duration': 15937.654762268066, 'accumulated_submission_time': 14419.182766199112, 'accumulated_eval_time': 1517.154412984848, 'accumulated_logging_time': 0.5125668048858643, 'global_step': 18055, 'preemption_count': 0}), (19871, {'train/ctc_loss': Array(0.3052099, dtype=float32), 'train/wer': 0.10275043443681192, 'validation/ctc_loss': Array(0.6104939, dtype=float32), 'validation/wer': 0.1763712021008525, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35727885, dtype=float32), 'test/wer': 0.11703532183697926, 'test/num_examples': 2472, 'score': 15859.548187494278, 'total_duration': 17513.06376671791, 'accumulated_submission_time': 15859.548187494278, 'accumulated_eval_time': 1652.0571603775024, 'accumulated_logging_time': 0.5724399089813232, 'global_step': 19871, 'preemption_count': 0}), (21649, {'train/ctc_loss': Array(0.28996828, dtype=float32), 'train/wer': 0.10099517800348826, 'validation/ctc_loss': Array(0.59253216, dtype=float32), 'validation/wer': 0.1723065931625747, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3434821, dtype=float32), 'test/wer': 0.11110434058456727, 'test/num_examples': 2472, 'score': 17299.65816307068, 'total_duration': 19090.31603384018, 'accumulated_submission_time': 17299.65816307068, 'accumulated_eval_time': 1789.0675375461578, 'accumulated_logging_time': 0.6251442432403564, 'global_step': 21649, 'preemption_count': 0}), (23449, {'train/ctc_loss': Array(0.32587433, dtype=float32), 'train/wer': 0.10602337875065147, 'validation/ctc_loss': Array(0.5905726, dtype=float32), 'validation/wer': 0.17074253936684786, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3419359, dtype=float32), 'test/wer': 0.11142932585867203, 'test/num_examples': 2472, 'score': 18740.54448866844, 'total_duration': 20668.405680656433, 'accumulated_submission_time': 18740.54448866844, 'accumulated_eval_time': 1926.1365442276, 'accumulated_logging_time': 0.6778745651245117, 'global_step': 23449, 'preemption_count': 0}), (25254, {'train/ctc_loss': Array(0.23896928, dtype=float32), 'train/wer': 0.08361363496523576, 'validation/ctc_loss': Array(0.5691403, dtype=float32), 'validation/wer': 0.16544213483688464, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32006806, dtype=float32), 'test/wer': 0.10419840350984096, 'test/num_examples': 2472, 'score': 20180.9346408844, 'total_duration': 22246.685710906982, 'accumulated_submission_time': 20180.9346408844, 'accumulated_eval_time': 2063.8816010951996, 'accumulated_logging_time': 0.741168737411499, 'global_step': 25254, 'preemption_count': 0}), (27049, {'train/ctc_loss': Array(0.25387847, dtype=float32), 'train/wer': 0.08802480043076634, 'validation/ctc_loss': Array(0.55523735, dtype=float32), 'validation/wer': 0.16125201540882628, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31684002, dtype=float32), 'test/wer': 0.10348749822273678, 'test/num_examples': 2472, 'score': 21621.25813126564, 'total_duration': 23824.718508720398, 'accumulated_submission_time': 21621.25813126564, 'accumulated_eval_time': 2201.445280790329, 'accumulated_logging_time': 0.8065252304077148, 'global_step': 27049, 'preemption_count': 0}), (28819, {'train/ctc_loss': Array(0.32222232, dtype=float32), 'train/wer': 0.11008639831160551, 'validation/ctc_loss': Array(0.52448064, dtype=float32), 'validation/wer': 0.15450341291985673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29585266, dtype=float32), 'test/wer': 0.09544411268864379, 'test/num_examples': 2472, 'score': 23061.37408900261, 'total_duration': 25399.31268310547, 'accumulated_submission_time': 23061.37408900261, 'accumulated_eval_time': 2335.782987356186, 'accumulated_logging_time': 0.8676490783691406, 'global_step': 28819, 'preemption_count': 0}), (30598, {'train/ctc_loss': Array(0.33865708, dtype=float32), 'train/wer': 0.1133199141616622, 'validation/ctc_loss': Array(0.5113746, dtype=float32), 'validation/wer': 0.14911611651235313, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28533491, dtype=float32), 'test/wer': 0.0923973757439116, 'test/num_examples': 2472, 'score': 24501.35665678978, 'total_duration': 26974.95587038994, 'accumulated_submission_time': 24501.35665678978, 'accumulated_eval_time': 2471.308205604553, 'accumulated_logging_time': 0.9225730895996094, 'global_step': 30598, 'preemption_count': 0}), (32416, {'train/ctc_loss': Array(0.3645432, dtype=float32), 'train/wer': 0.12535386114720623, 'validation/ctc_loss': Array(0.5027272, dtype=float32), 'validation/wer': 0.14581422516581866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2788384, dtype=float32), 'test/wer': 0.09026465988259907, 'test/num_examples': 2472, 'score': 25941.709431886673, 'total_duration': 28548.19150686264, 'accumulated_submission_time': 25941.709431886673, 'accumulated_eval_time': 2604.052140712738, 'accumulated_logging_time': 0.9797863960266113, 'global_step': 32416, 'preemption_count': 0}), (34215, {'train/ctc_loss': Array(0.31370944, dtype=float32), 'train/wer': 0.10446648667684, 'validation/ctc_loss': Array(0.4884356, dtype=float32), 'validation/wer': 0.14066829508481613, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2685194, dtype=float32), 'test/wer': 0.08486177970060732, 'test/num_examples': 2472, 'score': 27381.691277742386, 'total_duration': 30122.002299785614, 'accumulated_submission_time': 27381.691277742386, 'accumulated_eval_time': 2737.744610309601, 'accumulated_logging_time': 1.035611629486084, 'global_step': 34215, 'preemption_count': 0}), (35991, {'train/ctc_loss': Array(0.2759164, dtype=float32), 'train/wer': 0.09577486949241118, 'validation/ctc_loss': Array(0.46581116, dtype=float32), 'validation/wer': 0.13578304063643473, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25631005, dtype=float32), 'test/wer': 0.08289155647634716, 'test/num_examples': 2472, 'score': 28822.284260749817, 'total_duration': 31697.164457798004, 'accumulated_submission_time': 28822.284260749817, 'accumulated_eval_time': 2872.178744316101, 'accumulated_logging_time': 1.0904130935668945, 'global_step': 35991, 'preemption_count': 0}), (37763, {'train/ctc_loss': Array(0.2330637, dtype=float32), 'train/wer': 0.08139264530152533, 'validation/ctc_loss': Array(0.4498272, dtype=float32), 'validation/wer': 0.13042470818811125, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25000796, dtype=float32), 'test/wer': 0.0806166595576138, 'test/num_examples': 2472, 'score': 30262.179399967194, 'total_duration': 33272.73663830757, 'accumulated_submission_time': 30262.179399967194, 'accumulated_eval_time': 3007.717676639557, 'accumulated_logging_time': 1.1477704048156738, 'global_step': 37763, 'preemption_count': 0}), (39574, {'train/ctc_loss': Array(0.26204503, dtype=float32), 'train/wer': 0.0910593137147968, 'validation/ctc_loss': Array(0.4422166, dtype=float32), 'validation/wer': 0.12872548924954383, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24149479, dtype=float32), 'test/wer': 0.07698088680356671, 'test/num_examples': 2472, 'score': 31702.490131378174, 'total_duration': 34847.95053982735, 'accumulated_submission_time': 31702.490131378174, 'accumulated_eval_time': 3142.482138156891, 'accumulated_logging_time': 1.2042455673217773, 'global_step': 39574, 'preemption_count': 0}), (41352, {'train/ctc_loss': Array(0.22383782, dtype=float32), 'train/wer': 0.07813602823227463, 'validation/ctc_loss': Array(0.43122214, dtype=float32), 'validation/wer': 0.12468984427044615, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23231012, dtype=float32), 'test/wer': 0.07523409095525359, 'test/num_examples': 2472, 'score': 33142.4312608242, 'total_duration': 36424.297860622406, 'accumulated_submission_time': 33142.4312608242, 'accumulated_eval_time': 3278.741602897644, 'accumulated_logging_time': 1.27134108543396, 'global_step': 41352, 'preemption_count': 0}), (43138, {'train/ctc_loss': Array(0.21453248, dtype=float32), 'train/wer': 0.0768669198206493, 'validation/ctc_loss': Array(0.42354012, dtype=float32), 'validation/wer': 0.12261409386253705, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2263191, dtype=float32), 'test/wer': 0.0726951435013101, 'test/num_examples': 2472, 'score': 34582.384875535965, 'total_duration': 37999.1936173439, 'accumulated_submission_time': 34582.384875535965, 'accumulated_eval_time': 3413.544237613678, 'accumulated_logging_time': 1.331352949142456, 'global_step': 43138, 'preemption_count': 0}), (44926, {'train/ctc_loss': Array(0.21462, dtype=float32), 'train/wer': 0.07510113246434327, 'validation/ctc_loss': Array(0.41773412, dtype=float32), 'validation/wer': 0.12063488998522838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2226167, dtype=float32), 'test/wer': 0.07228891190867914, 'test/num_examples': 2472, 'score': 36022.423419713974, 'total_duration': 39575.315915346146, 'accumulated_submission_time': 36022.423419713974, 'accumulated_eval_time': 3549.495146036148, 'accumulated_logging_time': 1.3847966194152832, 'global_step': 44926, 'preemption_count': 0}), (46737, {'train/ctc_loss': Array(0.22253259, dtype=float32), 'train/wer': 0.07865858910677884, 'validation/ctc_loss': Array(0.41605195, dtype=float32), 'validation/wer': 0.12045145157708757, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2220608, dtype=float32), 'test/wer': 0.07198423821420592, 'test/num_examples': 2472, 'score': 37462.40724658966, 'total_duration': 41148.4344394207, 'accumulated_submission_time': 37462.40724658966, 'accumulated_eval_time': 3682.490786552429, 'accumulated_logging_time': 1.443239688873291, 'global_step': 46737, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.23057689, dtype=float32), 'train/wer': 0.07855449653186469, 'validation/ctc_loss': Array(0.41583607, dtype=float32), 'validation/wer': 0.12014250267916622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22198159, dtype=float32), 'test/wer': 0.071801433997522, 'test/num_examples': 2472, 'score': 38467.08563065529, 'total_duration': 42289.14387321472, 'accumulated_submission_time': 38467.08563065529, 'accumulated_eval_time': 3818.4000244140625, 'accumulated_logging_time': 1.5066735744476318, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0212 15:08:58.343577 140437690197824 submission_runner.py:586] Timing: 38467.08563065529
I0212 15:08:58.343638 140437690197824 submission_runner.py:588] Total number of evals: 28
I0212 15:08:58.343690 140437690197824 submission_runner.py:589] ====================
I0212 15:08:58.343752 140437690197824 submission_runner.py:542] Using RNG seed 2216178884
I0212 15:08:58.347246 140437690197824 submission_runner.py:551] --- Tuning run 4/5 ---
I0212 15:08:58.347425 140437690197824 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_4.
I0212 15:08:58.349073 140437690197824 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_4/hparams.json.
I0212 15:08:58.351652 140437690197824 submission_runner.py:206] Initializing dataset.
I0212 15:08:58.351782 140437690197824 submission_runner.py:213] Initializing model.
I0212 15:08:59.550182 140437690197824 submission_runner.py:255] Initializing optimizer.
I0212 15:08:59.689711 140437690197824 submission_runner.py:262] Initializing metrics bundle.
I0212 15:08:59.689883 140437690197824 submission_runner.py:280] Initializing checkpoint and logger.
I0212 15:08:59.693756 140437690197824 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_4 with prefix checkpoint_
I0212 15:08:59.693896 140437690197824 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_4/meta_data_0.json.
I0212 15:08:59.694227 140437690197824 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 15:08:59.694342 140437690197824 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 15:09:00.483069 140437690197824 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 15:09:01.239327 140437690197824 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_4/flags_0.json.
I0212 15:09:01.259068 140437690197824 submission_runner.py:314] Starting training loop.
I0212 15:09:01.263859 140437690197824 input_pipeline.py:20] Loading split = train-clean-100
I0212 15:09:01.801028 140437690197824 input_pipeline.py:20] Loading split = train-clean-360
I0212 15:09:01.937630 140437690197824 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0212 15:09:17.527171 140280737031936 logging_writer.py:48] [0] global_step=0, grad_norm=21.54698944091797, loss=32.616371154785156
I0212 15:09:17.540631 140437690197824 spec.py:321] Evaluating on the training split.
I0212 15:10:55.236227 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 15:11:57.575915 140437690197824 spec.py:349] Evaluating on the test split.
I0212 15:12:30.574830 140437690197824 submission_runner.py:408] Time since start: 209.31s, 	Step: 1, 	{'train/ctc_loss': Array(31.853792, dtype=float32), 'train/wer': 3.525787909704684, 'validation/ctc_loss': Array(30.891838, dtype=float32), 'validation/wer': 3.324647363796982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942242, dtype=float32), 'test/wer': 3.698637093006723, 'test/num_examples': 2472, 'score': 16.281517267227173, 'total_duration': 209.31295228004456, 'accumulated_submission_time': 16.281517267227173, 'accumulated_eval_time': 193.03134417533875, 'accumulated_logging_time': 0}
I0212 15:12:30.591602 140280856385280 logging_writer.py:48] [1] accumulated_eval_time=193.031344, accumulated_logging_time=0, accumulated_submission_time=16.281517, global_step=1, preemption_count=0, score=16.281517, test/ctc_loss=30.942241668701172, test/num_examples=2472, test/wer=3.698637, total_duration=209.312952, train/ctc_loss=31.853792190551758, train/wer=3.525788, validation/ctc_loss=30.89183807373047, validation/num_examples=5348, validation/wer=3.324647
I0212 15:13:56.089023 140280745424640 logging_writer.py:48] [100] global_step=100, grad_norm=0.9575855731964111, loss=5.735672950744629
I0212 15:15:11.668557 140280822814464 logging_writer.py:48] [200] global_step=200, grad_norm=1.3971943855285645, loss=4.528076648712158
I0212 15:16:27.113562 140280745424640 logging_writer.py:48] [300] global_step=300, grad_norm=63.2688102722168, loss=4.2843708992004395
I0212 15:17:42.523273 140280822814464 logging_writer.py:48] [400] global_step=400, grad_norm=2.3799197673797607, loss=3.19700026512146
I0212 15:18:57.608637 140280745424640 logging_writer.py:48] [500] global_step=500, grad_norm=2.963663339614868, loss=2.9168386459350586
I0212 15:20:16.285572 140280822814464 logging_writer.py:48] [600] global_step=600, grad_norm=2.1578369140625, loss=2.816291093826294
I0212 15:21:40.856929 140280745424640 logging_writer.py:48] [700] global_step=700, grad_norm=2.354945421218872, loss=2.6472091674804688
I0212 15:23:06.580218 140280822814464 logging_writer.py:48] [800] global_step=800, grad_norm=2.51082706451416, loss=2.5534768104553223
I0212 15:24:32.285927 140280745424640 logging_writer.py:48] [900] global_step=900, grad_norm=3.235349416732788, loss=2.5338666439056396
I0212 15:25:58.244093 140280822814464 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.451162815093994, loss=2.5096170902252197
I0212 15:27:19.784979 140280856385280 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.3512344360351562, loss=2.420074939727783
I0212 15:28:34.824345 140280847992576 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.029106616973877, loss=2.343738079071045
I0212 15:29:50.075946 140280856385280 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.56032657623291, loss=2.3237733840942383
I0212 15:31:05.597260 140280847992576 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.572573661804199, loss=2.2372450828552246
I0212 15:32:24.362293 140280856385280 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.9105186462402344, loss=2.2393853664398193
I0212 15:33:49.475434 140280847992576 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.477511167526245, loss=2.213995933532715
I0212 15:35:16.161912 140280856385280 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.425678014755249, loss=2.184753894805908
I0212 15:36:31.081774 140437690197824 spec.py:321] Evaluating on the training split.
I0212 15:37:24.015686 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 15:38:15.633927 140437690197824 spec.py:349] Evaluating on the test split.
I0212 15:38:41.854403 140437690197824 submission_runner.py:408] Time since start: 1780.59s, 	Step: 1789, 	{'train/ctc_loss': Array(1.6250488, dtype=float32), 'train/wer': 0.45638047836305823, 'validation/ctc_loss': Array(1.6839274, dtype=float32), 'validation/wer': 0.4397694468849262, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2356836, dtype=float32), 'test/wer': 0.3786890906505799, 'test/num_examples': 2472, 'score': 1456.6854193210602, 'total_duration': 1780.5886964797974, 'accumulated_submission_time': 1456.6854193210602, 'accumulated_eval_time': 323.79737067222595, 'accumulated_logging_time': 0.028933048248291016}
I0212 15:38:41.890275 140280856385280 logging_writer.py:48] [1789] accumulated_eval_time=323.797371, accumulated_logging_time=0.028933, accumulated_submission_time=1456.685419, global_step=1789, preemption_count=0, score=1456.685419, test/ctc_loss=1.235683560371399, test/num_examples=2472, test/wer=0.378689, total_duration=1780.588696, train/ctc_loss=1.6250487565994263, train/wer=0.456380, validation/ctc_loss=1.6839274168014526, validation/num_examples=5348, validation/wer=0.439769
I0212 15:38:50.967401 140280847992576 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.8834116458892822, loss=2.189542293548584
I0212 15:40:06.007260 140280856385280 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.2313008308410645, loss=2.21348237991333
I0212 15:41:21.185477 140280847992576 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.5670902729034424, loss=2.120616912841797
I0212 15:42:40.524013 140280856385280 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.3321189880371094, loss=2.118478775024414
I0212 15:43:55.692304 140280847992576 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.653341054916382, loss=2.139179229736328
I0212 15:45:10.735420 140280856385280 logging_writer.py:48] [2300] global_step=2300, grad_norm=6.116793632507324, loss=2.0331053733825684
I0212 15:46:25.864268 140280847992576 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.3007898330688477, loss=2.056459426879883
I0212 15:47:46.571395 140280856385280 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.4525911808013916, loss=2.0562314987182617
I0212 15:49:11.094469 140280847992576 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.3196017742156982, loss=2.039388418197632
I0212 15:50:36.810949 140280856385280 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.585493326187134, loss=2.1081173419952393
I0212 15:52:02.258406 140280847992576 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.1057040691375732, loss=2.0359067916870117
I0212 15:53:27.963664 140280856385280 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.7365869283676147, loss=2.021432638168335
I0212 15:54:53.603976 140280847992576 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.245469808578491, loss=1.947249174118042
I0212 15:56:22.064260 140280856385280 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.2280497550964355, loss=2.0595293045043945
I0212 15:57:36.981940 140280847992576 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.9597842693328857, loss=1.9566110372543335
I0212 15:58:51.876627 140280856385280 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.9170053005218506, loss=1.997744083404541
I0212 16:00:06.822468 140280847992576 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.095397472381592, loss=2.0075225830078125
I0212 16:01:24.595098 140280856385280 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.4822304248809814, loss=1.9963215589523315
I0212 16:02:42.457498 140437690197824 spec.py:321] Evaluating on the training split.
I0212 16:03:39.690103 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 16:04:32.224501 140437690197824 spec.py:349] Evaluating on the test split.
I0212 16:04:58.562762 140437690197824 submission_runner.py:408] Time since start: 3357.30s, 	Step: 3594, 	{'train/ctc_loss': Array(0.87861687, dtype=float32), 'train/wer': 0.26925837981114364, 'validation/ctc_loss': Array(1.0104467, dtype=float32), 'validation/wer': 0.2839530011489037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6614137, dtype=float32), 'test/wer': 0.21085450815509924, 'test/num_examples': 2472, 'score': 2897.1606023311615, 'total_duration': 3357.2956132888794, 'accumulated_submission_time': 2897.1606023311615, 'accumulated_eval_time': 459.8945791721344, 'accumulated_logging_time': 0.08021688461303711}
I0212 16:04:58.602113 140280856385280 logging_writer.py:48] [3594] accumulated_eval_time=459.894579, accumulated_logging_time=0.080217, accumulated_submission_time=2897.160602, global_step=3594, preemption_count=0, score=2897.160602, test/ctc_loss=0.6614137291908264, test/num_examples=2472, test/wer=0.210855, total_duration=3357.295613, train/ctc_loss=0.8786168694496155, train/wer=0.269258, validation/ctc_loss=1.0104466676712036, validation/num_examples=5348, validation/wer=0.283953
I0212 16:05:03.989705 140280847992576 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.260009765625, loss=2.107994318008423
I0212 16:06:19.191073 140280856385280 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.9879884719848633, loss=1.9829822778701782
I0212 16:07:34.238691 140280847992576 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.599384307861328, loss=1.9725738763809204
I0212 16:08:49.558800 140280856385280 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.063369035720825, loss=1.9701520204544067
I0212 16:10:13.446091 140280847992576 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.006502151489258, loss=1.985581636428833
I0212 16:11:38.695875 140280856385280 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.963667869567871, loss=1.9882481098175049
I0212 16:12:59.274930 140280856385280 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.748675584793091, loss=2.009500741958618
I0212 16:14:14.138217 140280847992576 logging_writer.py:48] [4300] global_step=4300, grad_norm=6.688135147094727, loss=1.927420735359192
I0212 16:15:29.141743 140280856385280 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.5467212200164795, loss=1.9648401737213135
I0212 16:16:43.841317 140280847992576 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.003100872039795, loss=1.9181702136993408
I0212 16:18:06.439274 140280856385280 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.0083460807800293, loss=1.9643069505691528
I0212 16:19:31.734081 140280847992576 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.3915488719940186, loss=1.946433424949646
I0212 16:20:57.261254 140280856385280 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.261763095855713, loss=1.9002771377563477
I0212 16:22:23.256275 140280847992576 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.5270519256591797, loss=1.9471017122268677
I0212 16:23:48.362995 140280856385280 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.6386139392852783, loss=1.9105314016342163
I0212 16:25:15.103028 140280847992576 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.4373629093170166, loss=1.992387056350708
I0212 16:26:38.637405 140280856385280 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.1353375911712646, loss=1.961539387702942
I0212 16:27:53.401963 140280847992576 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.6386537551879883, loss=1.9867024421691895
I0212 16:28:59.012470 140437690197824 spec.py:321] Evaluating on the training split.
I0212 16:29:54.636177 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 16:30:47.192953 140437690197824 spec.py:349] Evaluating on the test split.
I0212 16:31:14.071801 140437690197824 submission_runner.py:408] Time since start: 4932.81s, 	Step: 5389, 	{'train/ctc_loss': Array(0.98097146, dtype=float32), 'train/wer': 0.28369680790896545, 'validation/ctc_loss': Array(1.0103266, dtype=float32), 'validation/wer': 0.279453932822924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.646533, dtype=float32), 'test/wer': 0.20019092884853656, 'test/num_examples': 2472, 'score': 4337.480946302414, 'total_duration': 4932.8055765628815, 'accumulated_submission_time': 4337.480946302414, 'accumulated_eval_time': 594.9467799663544, 'accumulated_logging_time': 0.13591408729553223}
I0212 16:31:14.109233 140280856385280 logging_writer.py:48] [5389] accumulated_eval_time=594.946780, accumulated_logging_time=0.135914, accumulated_submission_time=4337.480946, global_step=5389, preemption_count=0, score=4337.480946, test/ctc_loss=0.6465330123901367, test/num_examples=2472, test/wer=0.200191, total_duration=4932.805577, train/ctc_loss=0.9809714555740356, train/wer=0.283697, validation/ctc_loss=1.010326623916626, validation/num_examples=5348, validation/wer=0.279454
I0212 16:31:23.200319 140280847992576 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.8803119659423828, loss=1.879089117050171
I0212 16:32:38.211944 140280856385280 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.9137744903564453, loss=1.9270809888839722
I0212 16:33:53.355764 140280847992576 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.3702046871185303, loss=1.9213491678237915
I0212 16:35:08.515406 140280856385280 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.0901432037353516, loss=1.9981696605682373
I0212 16:36:31.043728 140280847992576 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.7937202453613281, loss=1.963547706604004
I0212 16:37:56.835979 140280856385280 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.8418266773223877, loss=1.9392197132110596
I0212 16:39:23.016712 140280847992576 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.786587953567505, loss=1.9234713315963745
I0212 16:40:48.135689 140280856385280 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.554914951324463, loss=1.9467358589172363
I0212 16:42:15.648044 140280856385280 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.3107872009277344, loss=1.8904765844345093
I0212 16:43:30.574285 140280847992576 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.111661911010742, loss=2.0554730892181396
I0212 16:44:45.384216 140280856385280 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.9960010051727295, loss=1.9436376094818115
I0212 16:46:00.092856 140280847992576 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.46142578125, loss=1.9157586097717285
I0212 16:47:18.881352 140280856385280 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.283853769302368, loss=1.9538472890853882
I0212 16:48:43.163245 140280847992576 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.4728119373321533, loss=1.859053611755371
I0212 16:50:08.211495 140280856385280 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.455214023590088, loss=1.9492734670639038
I0212 16:51:33.619785 140280847992576 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.180541515350342, loss=1.8874132633209229
I0212 16:52:59.947328 140280856385280 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.1230685710906982, loss=1.8721134662628174
I0212 16:54:26.290229 140280847992576 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.4933061599731445, loss=1.835074543952942
I0212 16:55:14.071171 140437690197824 spec.py:321] Evaluating on the training split.
I0212 16:56:10.000177 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 16:57:02.108824 140437690197824 spec.py:349] Evaluating on the test split.
I0212 16:57:28.891065 140437690197824 submission_runner.py:408] Time since start: 6507.63s, 	Step: 7157, 	{'train/ctc_loss': Array(0.8190737, dtype=float32), 'train/wer': 0.24712488769092542, 'validation/ctc_loss': Array(0.892246, dtype=float32), 'validation/wer': 0.2521023007038242, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57268214, dtype=float32), 'test/wer': 0.18081368188003982, 'test/num_examples': 2472, 'score': 5777.349337816238, 'total_duration': 6507.6251838207245, 'accumulated_submission_time': 5777.349337816238, 'accumulated_eval_time': 729.7598900794983, 'accumulated_logging_time': 0.19289159774780273}
I0212 16:57:28.925512 140280856385280 logging_writer.py:48] [7157] accumulated_eval_time=729.759890, accumulated_logging_time=0.192892, accumulated_submission_time=5777.349338, global_step=7157, preemption_count=0, score=5777.349338, test/ctc_loss=0.5726821422576904, test/num_examples=2472, test/wer=0.180814, total_duration=6507.625184, train/ctc_loss=0.8190736770629883, train/wer=0.247125, validation/ctc_loss=0.8922460079193115, validation/num_examples=5348, validation/wer=0.252102
I0212 16:58:01.833105 140280847992576 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.132622480392456, loss=1.8570938110351562
I0212 16:59:20.744964 140280856385280 logging_writer.py:48] [7300] global_step=7300, grad_norm=4.798473358154297, loss=1.902414321899414
I0212 17:00:35.867608 140280847992576 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.0409188270568848, loss=1.883856177330017
I0212 17:01:51.111236 140280856385280 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.08805251121521, loss=1.935476541519165
I0212 17:03:09.121038 140280847992576 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.9142215251922607, loss=1.9053291082382202
I0212 17:04:32.369757 140280856385280 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.888010263442993, loss=1.9137370586395264
I0212 17:05:56.646594 140280847992576 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.0445659160614014, loss=1.9178556203842163
I0212 17:07:21.525452 140280856385280 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.943107843399048, loss=1.882655143737793
I0212 17:08:46.889960 140280847992576 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.2730586528778076, loss=1.8652831315994263
I0212 17:10:12.103192 140280856385280 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.0343542098999023, loss=1.8882144689559937
I0212 17:11:37.284067 140280847992576 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.9699997901916504, loss=1.8281540870666504
I0212 17:13:00.038430 140280856385280 logging_writer.py:48] [8300] global_step=8300, grad_norm=3.18780779838562, loss=1.8343315124511719
I0212 17:14:15.044591 140280847992576 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.938652992248535, loss=1.9458962678909302
I0212 17:15:30.002202 140280856385280 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.459364652633667, loss=1.8647702932357788
I0212 17:16:44.966799 140280847992576 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.9552717208862305, loss=1.8894579410552979
I0212 17:18:04.638626 140280856385280 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.154287576675415, loss=1.9189964532852173
I0212 17:19:29.804670 140280847992576 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.081667423248291, loss=1.8405349254608154
I0212 17:20:54.425513 140280856385280 logging_writer.py:48] [8900] global_step=8900, grad_norm=3.201284408569336, loss=1.8455746173858643
I0212 17:21:29.145763 140437690197824 spec.py:321] Evaluating on the training split.
I0212 17:22:26.023478 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 17:23:19.339885 140437690197824 spec.py:349] Evaluating on the test split.
I0212 17:23:45.766753 140437690197824 submission_runner.py:408] Time since start: 8084.50s, 	Step: 8942, 	{'train/ctc_loss': Array(0.79178494, dtype=float32), 'train/wer': 0.24491464615281927, 'validation/ctc_loss': Array(0.8644864, dtype=float32), 'validation/wer': 0.24434961429661026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54710823, dtype=float32), 'test/wer': 0.17362338269047184, 'test/num_examples': 2472, 'score': 7217.478336572647, 'total_duration': 8084.5014843940735, 'accumulated_submission_time': 7217.478336572647, 'accumulated_eval_time': 866.3746891021729, 'accumulated_logging_time': 0.244370698928833}
I0212 17:23:45.800082 140280856385280 logging_writer.py:48] [8942] accumulated_eval_time=866.374689, accumulated_logging_time=0.244371, accumulated_submission_time=7217.478337, global_step=8942, preemption_count=0, score=7217.478337, test/ctc_loss=0.5471082329750061, test/num_examples=2472, test/wer=0.173623, total_duration=8084.501484, train/ctc_loss=0.791784942150116, train/wer=0.244915, validation/ctc_loss=0.8644863963127136, validation/num_examples=5348, validation/wer=0.244350
I0212 17:24:30.127965 140280847992576 logging_writer.py:48] [9000] global_step=9000, grad_norm=5.697096824645996, loss=2.4496941566467285
I0212 17:25:45.194360 140280856385280 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.9782416820526123, loss=1.9364938735961914
I0212 17:27:00.402730 140280847992576 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.2802164554595947, loss=1.8564114570617676
I0212 17:28:21.085196 140280856385280 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.412952184677124, loss=1.9177093505859375
I0212 17:29:36.161232 140280847992576 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.3283069133758545, loss=1.916325569152832
I0212 17:30:51.267386 140280856385280 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.9666874408721924, loss=1.7855198383331299
I0212 17:32:06.455563 140280847992576 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.9597337245941162, loss=1.868477702140808
I0212 17:33:25.719275 140280856385280 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.430190086364746, loss=1.914961576461792
I0212 17:34:50.133063 140280847992576 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.3141260147094727, loss=1.8836082220077515
I0212 17:36:15.403306 140280856385280 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.9577648639678955, loss=1.8711576461791992
I0212 17:37:40.272965 140280847992576 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.415024995803833, loss=1.8390820026397705
I0212 17:39:06.058721 140280856385280 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.192708969116211, loss=1.8291888236999512
I0212 17:40:30.491831 140280847992576 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.276191473007202, loss=1.779569387435913
I0212 17:41:59.146402 140280856385280 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.134004831314087, loss=1.8398123979568481
I0212 17:43:14.211418 140280847992576 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.6437480449676514, loss=1.9180030822753906
I0212 17:44:29.166570 140280856385280 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.920362710952759, loss=1.8580400943756104
I0212 17:45:44.179230 140280847992576 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.9858615398406982, loss=1.9255781173706055
I0212 17:46:59.409141 140280856385280 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.0292017459869385, loss=1.874085545539856
I0212 17:47:46.813212 140437690197824 spec.py:321] Evaluating on the training split.
I0212 17:48:43.682600 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 17:49:37.013171 140437690197824 spec.py:349] Evaluating on the test split.
I0212 17:50:04.338154 140437690197824 submission_runner.py:408] Time since start: 9663.07s, 	Step: 10758, 	{'train/ctc_loss': Array(0.7392486, dtype=float32), 'train/wer': 0.22892540944191442, 'validation/ctc_loss': Array(0.8693898, dtype=float32), 'validation/wer': 0.24681155082692102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55628407, dtype=float32), 'test/wer': 0.17457802693315458, 'test/num_examples': 2472, 'score': 8658.395947933197, 'total_duration': 9663.073320865631, 'accumulated_submission_time': 8658.395947933197, 'accumulated_eval_time': 1003.8939211368561, 'accumulated_logging_time': 0.29659557342529297}
I0212 17:50:04.375683 140280856385280 logging_writer.py:48] [10758] accumulated_eval_time=1003.893921, accumulated_logging_time=0.296596, accumulated_submission_time=8658.395948, global_step=10758, preemption_count=0, score=8658.395948, test/ctc_loss=0.5562840700149536, test/num_examples=2472, test/wer=0.174578, total_duration=9663.073321, train/ctc_loss=0.7392485737800598, train/wer=0.228925, validation/ctc_loss=0.8693897724151611, validation/num_examples=5348, validation/wer=0.246812
I0212 17:50:36.995413 140280847992576 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.0767881870269775, loss=1.8317219018936157
I0212 17:51:52.484104 140280856385280 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.333390951156616, loss=1.8100208044052124
I0212 17:53:08.072907 140280847992576 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.201993465423584, loss=1.8188763856887817
I0212 17:54:26.854930 140280856385280 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.152597665786743, loss=1.832669973373413
I0212 17:55:51.535742 140280847992576 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.1199212074279785, loss=1.7932169437408447
I0212 17:57:16.729579 140280856385280 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.9827682971954346, loss=1.8612923622131348
I0212 17:58:38.434428 140280856385280 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.6836252212524414, loss=1.8735612630844116
I0212 17:59:53.844993 140280847992576 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.8299405574798584, loss=1.8202824592590332
I0212 18:01:08.999282 140280856385280 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.2429280281066895, loss=1.8007371425628662
I0212 18:02:24.155834 140280847992576 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.432490348815918, loss=1.838646650314331
I0212 18:03:42.927390 140280856385280 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.9142098426818848, loss=1.8768868446350098
I0212 18:05:08.855012 140280847992576 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.284168243408203, loss=1.8976272344589233
I0212 18:06:33.752182 140280856385280 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.473494291305542, loss=1.7915024757385254
I0212 18:07:58.692410 140280847992576 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.796867847442627, loss=1.712517261505127
I0212 18:09:24.419098 140280856385280 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.8634042739868164, loss=1.7997851371765137
I0212 18:10:50.839603 140280847992576 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.24899959564209, loss=1.8070831298828125
I0212 18:12:15.625114 140280856385280 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.8853542804718018, loss=1.905073881149292
I0212 18:13:30.678245 140280847992576 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.225465774536133, loss=1.8467680215835571
I0212 18:14:04.982996 140437690197824 spec.py:321] Evaluating on the training split.
I0212 18:15:00.983939 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 18:15:53.941410 140437690197824 spec.py:349] Evaluating on the test split.
I0212 18:16:20.759441 140437690197824 submission_runner.py:408] Time since start: 11239.49s, 	Step: 12547, 	{'train/ctc_loss': Array(0.65287876, dtype=float32), 'train/wer': 0.20319984452345294, 'validation/ctc_loss': Array(0.8087663, dtype=float32), 'validation/wer': 0.2305241511146297, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5140238, dtype=float32), 'test/wer': 0.16482846871001158, 'test/num_examples': 2472, 'score': 10098.912323236465, 'total_duration': 11239.493403196335, 'accumulated_submission_time': 10098.912323236465, 'accumulated_eval_time': 1139.6634182929993, 'accumulated_logging_time': 0.3509695529937744}
I0212 18:16:20.794135 140280856385280 logging_writer.py:48] [12547] accumulated_eval_time=1139.663418, accumulated_logging_time=0.350970, accumulated_submission_time=10098.912323, global_step=12547, preemption_count=0, score=10098.912323, test/ctc_loss=0.5140237808227539, test/num_examples=2472, test/wer=0.164828, total_duration=11239.493403, train/ctc_loss=0.6528787612915039, train/wer=0.203200, validation/ctc_loss=0.8087663054466248, validation/num_examples=5348, validation/wer=0.230524
I0212 18:17:01.434617 140280847992576 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.3850135803222656, loss=1.736491322517395
I0212 18:18:16.636353 140280856385280 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.5615758895874023, loss=1.7796390056610107
I0212 18:19:31.807511 140280847992576 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.1554088592529297, loss=1.8661412000656128
I0212 18:20:47.152237 140280856385280 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.5713319778442383, loss=1.808657169342041
I0212 18:22:07.913784 140280847992576 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.399813413619995, loss=1.7421108484268188
I0212 18:23:33.610273 140280856385280 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.805100440979004, loss=1.8409265279769897
I0212 18:24:59.322390 140280847992576 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.617110013961792, loss=1.8245888948440552
I0212 18:26:24.985810 140280856385280 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.383347988128662, loss=1.8451359272003174
I0212 18:27:52.827319 140280856385280 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.4686827659606934, loss=1.7744765281677246
I0212 18:29:08.014228 140280847992576 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.8218698501586914, loss=1.823502779006958
I0212 18:30:23.130359 140280856385280 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.219871759414673, loss=1.8023433685302734
I0212 18:31:38.249317 140280847992576 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.4012653827667236, loss=1.770255208015442
I0212 18:32:54.898778 140280856385280 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.82787024974823, loss=1.7366622686386108
I0212 18:34:19.560700 140280847992576 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.660468101501465, loss=1.780968189239502
I0212 18:35:44.747964 140280856385280 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.887056827545166, loss=1.7749272584915161
I0212 18:37:10.302656 140280847992576 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.0742199420928955, loss=1.693555474281311
I0212 18:38:35.639335 140280856385280 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.9420510530471802, loss=1.8187918663024902
I0212 18:40:00.517662 140280847992576 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.6889612674713135, loss=1.8009811639785767
I0212 18:40:21.097144 140437690197824 spec.py:321] Evaluating on the training split.
I0212 18:41:17.574186 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 18:42:10.163754 140437690197824 spec.py:349] Evaluating on the test split.
I0212 18:42:36.602261 140437690197824 submission_runner.py:408] Time since start: 12815.34s, 	Step: 14325, 	{'train/ctc_loss': Array(0.71331424, dtype=float32), 'train/wer': 0.22321453630990137, 'validation/ctc_loss': Array(0.78945225, dtype=float32), 'validation/wer': 0.22622783050291087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48911905, dtype=float32), 'test/wer': 0.1573334958259704, 'test/num_examples': 2472, 'score': 11539.125578641891, 'total_duration': 12815.336868524551, 'accumulated_submission_time': 11539.125578641891, 'accumulated_eval_time': 1275.162227153778, 'accumulated_logging_time': 0.4020240306854248}
I0212 18:42:36.640032 140280856385280 logging_writer.py:48] [14325] accumulated_eval_time=1275.162227, accumulated_logging_time=0.402024, accumulated_submission_time=11539.125579, global_step=14325, preemption_count=0, score=11539.125579, test/ctc_loss=0.4891190528869629, test/num_examples=2472, test/wer=0.157333, total_duration=12815.336869, train/ctc_loss=0.7133142352104187, train/wer=0.223215, validation/ctc_loss=0.7894522547721863, validation/num_examples=5348, validation/wer=0.226228
I0212 18:43:33.897914 140280847992576 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.097637414932251, loss=1.7557777166366577
I0212 18:44:52.946937 140280856385280 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.044434070587158, loss=1.7306406497955322
I0212 18:46:08.221595 140280847992576 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.9758577346801758, loss=1.7132549285888672
I0212 18:47:23.622586 140280856385280 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.9199883937835693, loss=1.7886979579925537
I0212 18:48:39.206222 140280847992576 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.497516632080078, loss=1.7594938278198242
I0212 18:50:02.078787 140280856385280 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.7001593112945557, loss=1.7215958833694458
I0212 18:51:27.113784 140280847992576 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.3231282234191895, loss=1.763120174407959
I0212 18:52:52.351697 140280856385280 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.9373245239257812, loss=1.773209571838379
I0212 18:54:17.178464 140280847992576 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.5395864248275757, loss=1.773126482963562
I0212 18:55:42.180044 140280856385280 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.439159870147705, loss=1.8399213552474976
I0212 18:57:07.581789 140280847992576 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.990873098373413, loss=1.7710354328155518
I0212 18:58:31.021643 140280856385280 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.20230770111084, loss=1.7900882959365845
I0212 18:59:46.039718 140280847992576 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.4172325134277344, loss=1.802008867263794
I0212 19:01:00.712780 140280856385280 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.4329662322998047, loss=1.7561414241790771
I0212 19:02:15.827064 140280847992576 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.9667261838912964, loss=1.756895899772644
I0212 19:03:33.574560 140280856385280 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.574406862258911, loss=1.6917346715927124
I0212 19:04:58.039564 140280847992576 logging_writer.py:48] [16000] global_step=16000, grad_norm=5.852506160736084, loss=1.7668930292129517
I0212 19:06:23.884757 140280856385280 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.389939069747925, loss=1.6958481073379517
I0212 19:06:37.546159 140437690197824 spec.py:321] Evaluating on the training split.
I0212 19:07:34.112200 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 19:08:26.832124 140437690197824 spec.py:349] Evaluating on the test split.
I0212 19:08:53.040714 140437690197824 submission_runner.py:408] Time since start: 14391.77s, 	Step: 16117, 	{'train/ctc_loss': Array(0.62898093, dtype=float32), 'train/wer': 0.19862913810704388, 'validation/ctc_loss': Array(0.76331466, dtype=float32), 'validation/wer': 0.22087915270764746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47254232, dtype=float32), 'test/wer': 0.1539414620275019, 'test/num_examples': 2472, 'score': 12979.941724300385, 'total_duration': 14391.774739265442, 'accumulated_submission_time': 12979.941724300385, 'accumulated_eval_time': 1410.6499030590057, 'accumulated_logging_time': 0.4547863006591797}
I0212 19:08:53.077909 140280856385280 logging_writer.py:48] [16117] accumulated_eval_time=1410.649903, accumulated_logging_time=0.454786, accumulated_submission_time=12979.941724, global_step=16117, preemption_count=0, score=12979.941724, test/ctc_loss=0.47254231572151184, test/num_examples=2472, test/wer=0.153941, total_duration=14391.774739, train/ctc_loss=0.6289809346199036, train/wer=0.198629, validation/ctc_loss=0.7633146643638611, validation/num_examples=5348, validation/wer=0.220879
I0212 19:09:56.322357 140280847992576 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.872924566268921, loss=1.651036024093628
I0212 19:11:11.675765 140280856385280 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.3016443252563477, loss=1.7319128513336182
I0212 19:12:26.936564 140280847992576 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.9993213415145874, loss=1.6611592769622803
I0212 19:13:52.088712 140280856385280 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.171804428100586, loss=1.6390165090560913
I0212 19:15:07.361098 140280847992576 logging_writer.py:48] [16600] global_step=16600, grad_norm=3.589682102203369, loss=1.735130786895752
I0212 19:16:22.297697 140280856385280 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.2982120513916016, loss=1.7113656997680664
I0212 19:17:37.473920 140280847992576 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.5371556282043457, loss=1.8053193092346191
I0212 19:18:53.617761 140280856385280 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.4267187118530273, loss=1.7268160581588745
I0212 19:20:16.931064 140280847992576 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.155064344406128, loss=1.7126544713974
I0212 19:21:41.320206 140280856385280 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.819333791732788, loss=1.7004588842391968
I0212 19:23:07.189480 140280847992576 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.237304449081421, loss=1.6724886894226074
I0212 19:24:32.524718 140280856385280 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.3006062507629395, loss=1.686885952949524
I0212 19:25:57.651896 140280847992576 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.834124803543091, loss=1.7748688459396362
I0212 19:27:24.049522 140280856385280 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.828831195831299, loss=1.754825472831726
I0212 19:28:43.872138 140280856385280 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.277226448059082, loss=1.6646298170089722
I0212 19:29:58.800286 140280847992576 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.0347096920013428, loss=1.7182989120483398
I0212 19:31:13.804792 140280856385280 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.264185667037964, loss=1.7614705562591553
I0212 19:32:28.645178 140280847992576 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.4905691146850586, loss=1.6852574348449707
I0212 19:32:53.631733 140437690197824 spec.py:321] Evaluating on the training split.
I0212 19:33:50.068571 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 19:34:42.487088 140437690197824 spec.py:349] Evaluating on the test split.
I0212 19:35:09.171508 140437690197824 submission_runner.py:408] Time since start: 15967.91s, 	Step: 17933, 	{'train/ctc_loss': Array(0.6621947, dtype=float32), 'train/wer': 0.20311503880058915, 'validation/ctc_loss': Array(0.7468117, dtype=float32), 'validation/wer': 0.21476775732064068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45915446, dtype=float32), 'test/wer': 0.1470558365324071, 'test/num_examples': 2472, 'score': 14420.404343128204, 'total_duration': 15967.906356573105, 'accumulated_submission_time': 14420.404343128204, 'accumulated_eval_time': 1546.1836066246033, 'accumulated_logging_time': 0.5079362392425537}
I0212 19:35:09.207738 140280856385280 logging_writer.py:48] [17933] accumulated_eval_time=1546.183607, accumulated_logging_time=0.507936, accumulated_submission_time=14420.404343, global_step=17933, preemption_count=0, score=14420.404343, test/ctc_loss=0.4591544568538666, test/num_examples=2472, test/wer=0.147056, total_duration=15967.906357, train/ctc_loss=0.6621947288513184, train/wer=0.203115, validation/ctc_loss=0.7468116879463196, validation/num_examples=5348, validation/wer=0.214768
I0212 19:36:00.300279 140280847992576 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.128169298171997, loss=1.7189944982528687
I0212 19:37:15.283135 140280856385280 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.035421848297119, loss=1.694883108139038
I0212 19:38:30.650911 140280847992576 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.1184005737304688, loss=1.6702783107757568
I0212 19:39:51.389611 140280856385280 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.8143419027328491, loss=1.6649643182754517
I0212 19:41:16.855769 140280847992576 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.735671043395996, loss=1.7446811199188232
I0212 19:42:43.014989 140280856385280 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.621947765350342, loss=1.7219825983047485
I0212 19:44:05.255786 140280856385280 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.865382194519043, loss=1.7176231145858765
I0212 19:45:20.307072 140280847992576 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.459892272949219, loss=1.6428489685058594
I0212 19:46:35.368645 140280856385280 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.336395025253296, loss=1.6884857416152954
I0212 19:47:50.377717 140280847992576 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.282036781311035, loss=1.7198137044906616
I0212 19:49:09.759513 140280856385280 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.3936078548431396, loss=1.6795192956924438
I0212 19:50:34.792711 140280847992576 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.789449691772461, loss=1.738975167274475
I0212 19:52:01.214973 140280856385280 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.5446383953094482, loss=1.677680492401123
I0212 19:53:26.988577 140280847992576 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.8186354637145996, loss=1.6935564279556274
I0212 19:54:53.130665 140280856385280 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.2519707679748535, loss=1.6695313453674316
I0212 19:56:18.822574 140280847992576 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.00410795211792, loss=1.5961805582046509
I0212 19:57:44.742360 140280856385280 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.6647510528564453, loss=1.6658830642700195
I0212 19:58:59.700617 140280847992576 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.738248825073242, loss=1.69053316116333
I0212 19:59:09.528172 140437690197824 spec.py:321] Evaluating on the training split.
I0212 20:00:05.779443 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 20:00:58.109037 140437690197824 spec.py:349] Evaluating on the test split.
I0212 20:01:24.823932 140437690197824 submission_runner.py:408] Time since start: 17543.56s, 	Step: 19714, 	{'train/ctc_loss': Array(0.5638722, dtype=float32), 'train/wer': 0.17971140272338276, 'validation/ctc_loss': Array(0.7201087, dtype=float32), 'validation/wer': 0.20510344960753835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4412394, dtype=float32), 'test/wer': 0.14057644262994334, 'test/num_examples': 2472, 'score': 15860.634415864944, 'total_duration': 17543.55829191208, 'accumulated_submission_time': 15860.634415864944, 'accumulated_eval_time': 1681.472809791565, 'accumulated_logging_time': 0.5603365898132324}
I0212 20:01:24.864155 140280856385280 logging_writer.py:48] [19714] accumulated_eval_time=1681.472810, accumulated_logging_time=0.560337, accumulated_submission_time=15860.634416, global_step=19714, preemption_count=0, score=15860.634416, test/ctc_loss=0.4412393867969513, test/num_examples=2472, test/wer=0.140576, total_duration=17543.558292, train/ctc_loss=0.563872218132019, train/wer=0.179711, validation/ctc_loss=0.720108687877655, validation/num_examples=5348, validation/wer=0.205103
I0212 20:02:30.295945 140280847992576 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.1019253730773926, loss=1.643816590309143
I0212 20:03:45.511744 140280856385280 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.7134931087493896, loss=1.6178144216537476
I0212 20:05:00.907765 140280847992576 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.4739162921905518, loss=1.7397592067718506
I0212 20:06:16.117273 140280856385280 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.207533359527588, loss=1.6467113494873047
I0212 20:07:36.641972 140280847992576 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.578547477722168, loss=1.7338812351226807
I0212 20:09:02.855265 140280856385280 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.972027063369751, loss=1.6817713975906372
I0212 20:10:28.586568 140280847992576 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.0613853931427, loss=1.6991502046585083
I0212 20:11:53.035194 140280856385280 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.328765630722046, loss=1.6984648704528809
I0212 20:13:22.000774 140280856385280 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.866621971130371, loss=1.6297658681869507
I0212 20:14:37.004413 140280847992576 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.300673723220825, loss=1.6250931024551392
I0212 20:15:52.278742 140280856385280 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.2270307540893555, loss=1.6624315977096558
I0212 20:17:07.940677 140280847992576 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.216444492340088, loss=1.6731019020080566
I0212 20:18:23.414335 140280856385280 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.649789571762085, loss=1.7032750844955444
I0212 20:19:47.478013 140280847992576 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.492021083831787, loss=1.6267931461334229
I0212 20:21:12.459399 140280856385280 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.469646453857422, loss=1.6721495389938354
I0212 20:22:38.760707 140280847992576 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.499765634536743, loss=1.6319464445114136
I0212 20:24:05.847239 140280856385280 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.5189567804336548, loss=1.6348925828933716
I0212 20:25:25.171383 140437690197824 spec.py:321] Evaluating on the training split.
I0212 20:26:20.565327 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 20:27:13.053726 140437690197824 spec.py:349] Evaluating on the test split.
I0212 20:27:39.355535 140437690197824 submission_runner.py:408] Time since start: 19118.09s, 	Step: 21494, 	{'train/ctc_loss': Array(0.5838771, dtype=float32), 'train/wer': 0.1874216988223503, 'validation/ctc_loss': Array(0.70004, dtype=float32), 'validation/wer': 0.20001544744489608, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42382613, dtype=float32), 'test/wer': 0.1354782361424248, 'test/num_examples': 2472, 'score': 17300.85076236725, 'total_duration': 19118.08944129944, 'accumulated_submission_time': 17300.85076236725, 'accumulated_eval_time': 1815.649961233139, 'accumulated_logging_time': 0.6182372570037842}
I0212 20:27:39.394205 140280856385280 logging_writer.py:48] [21494] accumulated_eval_time=1815.649961, accumulated_logging_time=0.618237, accumulated_submission_time=17300.850762, global_step=21494, preemption_count=0, score=17300.850762, test/ctc_loss=0.4238261282444, test/num_examples=2472, test/wer=0.135478, total_duration=19118.089441, train/ctc_loss=0.5838770866394043, train/wer=0.187422, validation/ctc_loss=0.7000399827957153, validation/num_examples=5348, validation/wer=0.200015
I0212 20:27:44.766481 140280847992576 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.9542967081069946, loss=1.7041678428649902
I0212 20:29:00.021018 140280856385280 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.319944858551025, loss=1.667635202407837
I0212 20:30:18.659968 140280856385280 logging_writer.py:48] [21700] global_step=21700, grad_norm=8.582472801208496, loss=1.5390725135803223
I0212 20:31:33.617149 140280847992576 logging_writer.py:48] [21800] global_step=21800, grad_norm=4.072018146514893, loss=1.7071982622146606
I0212 20:32:48.836568 140280856385280 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.4985501766204834, loss=1.6639339923858643
I0212 20:34:03.942705 140280847992576 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.392507314682007, loss=1.6065192222595215
I0212 20:35:23.196056 140280856385280 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.3643782138824463, loss=1.5952821969985962
I0212 20:36:47.231829 140280847992576 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.6425323486328125, loss=1.6227412223815918
I0212 20:38:11.364710 140280856385280 logging_writer.py:48] [22300] global_step=22300, grad_norm=4.246852874755859, loss=1.657879114151001
I0212 20:39:36.972756 140280847992576 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.5256893634796143, loss=1.6392217874526978
I0212 20:41:03.479191 140280856385280 logging_writer.py:48] [22500] global_step=22500, grad_norm=4.091907978057861, loss=1.680125117301941
I0212 20:42:28.531052 140280847992576 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.9727869033813477, loss=1.5738033056259155
I0212 20:43:53.248725 140280856385280 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.363797426223755, loss=1.5555182695388794
I0212 20:45:07.975696 140280847992576 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.1003594398498535, loss=1.5903732776641846
I0212 20:46:22.789527 140280856385280 logging_writer.py:48] [22900] global_step=22900, grad_norm=4.7020158767700195, loss=1.6145907640457153
I0212 20:47:37.990386 140280847992576 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.281813621520996, loss=1.5871127843856812
I0212 20:48:52.848376 140280856385280 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.791229724884033, loss=1.5887818336486816
I0212 20:50:14.859162 140280847992576 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.2546489238739014, loss=1.6600497961044312
I0212 20:51:40.278737 140280856385280 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.782458782196045, loss=1.6845072507858276
I0212 20:51:40.284752 140437690197824 spec.py:321] Evaluating on the training split.
I0212 20:52:36.035746 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 20:53:28.125501 140437690197824 spec.py:349] Evaluating on the test split.
I0212 20:53:54.852202 140437690197824 submission_runner.py:408] Time since start: 20693.59s, 	Step: 23301, 	{'train/ctc_loss': Array(0.5520931, dtype=float32), 'train/wer': 0.17591464437017248, 'validation/ctc_loss': Array(0.67959964, dtype=float32), 'validation/wer': 0.1954101779352559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4096471, dtype=float32), 'test/wer': 0.13155810127353604, 'test/num_examples': 2472, 'score': 18741.65048289299, 'total_duration': 20693.58714914322, 'accumulated_submission_time': 18741.65048289299, 'accumulated_eval_time': 1950.2114119529724, 'accumulated_logging_time': 0.6731538772583008}
I0212 20:53:54.890327 140280856385280 logging_writer.py:48] [23301] accumulated_eval_time=1950.211412, accumulated_logging_time=0.673154, accumulated_submission_time=18741.650483, global_step=23301, preemption_count=0, score=18741.650483, test/ctc_loss=0.4096471071243286, test/num_examples=2472, test/wer=0.131558, total_duration=20693.587149, train/ctc_loss=0.5520930886268616, train/wer=0.175915, validation/ctc_loss=0.6795996427536011, validation/num_examples=5348, validation/wer=0.195410
I0212 20:55:10.053108 140280847992576 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.0950701236724854, loss=1.6407579183578491
I0212 20:56:25.204979 140280856385280 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.13196063041687, loss=1.6015621423721313
I0212 20:57:42.228918 140280847992576 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.457404375076294, loss=1.6132255792617798
I0212 20:59:10.302707 140280856385280 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.314424753189087, loss=1.6132181882858276
I0212 21:00:25.598188 140280847992576 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.656749963760376, loss=1.627581000328064
I0212 21:01:40.647053 140280856385280 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.590715169906616, loss=1.660239577293396
I0212 21:02:56.004096 140280847992576 logging_writer.py:48] [24000] global_step=24000, grad_norm=4.8632659912109375, loss=1.6496691703796387
I0212 21:04:11.345116 140280856385280 logging_writer.py:48] [24100] global_step=24100, grad_norm=4.105412006378174, loss=1.5984224081039429
I0212 21:05:28.748069 140280847992576 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.836656093597412, loss=1.6169589757919312
I0212 21:06:54.576487 140280856385280 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.898740291595459, loss=1.6398704051971436
I0212 21:08:20.640352 140280847992576 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.3400373458862305, loss=1.5440150499343872
I0212 21:09:46.138422 140280856385280 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.1050820350646973, loss=1.612298846244812
I0212 21:11:12.401851 140280847992576 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.2418785095214844, loss=1.637969970703125
I0212 21:12:37.630294 140280856385280 logging_writer.py:48] [24700] global_step=24700, grad_norm=4.420376300811768, loss=1.5326868295669556
I0212 21:13:58.074368 140280856385280 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.20090913772583, loss=1.6183240413665771
I0212 21:15:13.480081 140280847992576 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.712268829345703, loss=1.5830824375152588
I0212 21:16:28.775515 140280856385280 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.318633556365967, loss=1.53022301197052
I0212 21:17:44.243691 140280847992576 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.8767951726913452, loss=1.5562208890914917
I0212 21:17:55.412506 140437690197824 spec.py:321] Evaluating on the training split.
I0212 21:19:00.975003 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 21:19:52.814245 140437690197824 spec.py:349] Evaluating on the test split.
I0212 21:20:19.193591 140437690197824 submission_runner.py:408] Time since start: 22277.93s, 	Step: 25116, 	{'train/ctc_loss': Array(0.33693334, dtype=float32), 'train/wer': 0.11385134230882471, 'validation/ctc_loss': Array(0.645735, dtype=float32), 'validation/wer': 0.1852052096507912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3809952, dtype=float32), 'test/wer': 0.12373814311539008, 'test/num_examples': 2472, 'score': 20182.07522916794, 'total_duration': 22277.928158044815, 'accumulated_submission_time': 20182.07522916794, 'accumulated_eval_time': 2093.9861640930176, 'accumulated_logging_time': 0.7318322658538818}
I0212 21:20:19.231907 140280856385280 logging_writer.py:48] [25116] accumulated_eval_time=2093.986164, accumulated_logging_time=0.731832, accumulated_submission_time=20182.075229, global_step=25116, preemption_count=0, score=20182.075229, test/ctc_loss=0.3809952139854431, test/num_examples=2472, test/wer=0.123738, total_duration=22277.928158, train/ctc_loss=0.33693334460258484, train/wer=0.113851, validation/ctc_loss=0.6457350254058838, validation/num_examples=5348, validation/wer=0.185205
I0212 21:21:23.083872 140280847992576 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.2075557708740234, loss=1.586026668548584
I0212 21:22:38.159403 140280856385280 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.374044895172119, loss=1.6234643459320068
I0212 21:23:53.526456 140280847992576 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.479154348373413, loss=1.5941715240478516
I0212 21:25:09.864034 140280856385280 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.9935450553894043, loss=1.6255097389221191
I0212 21:26:34.079289 140280847992576 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.461432933807373, loss=1.6195517778396606
I0212 21:27:58.902004 140280856385280 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.408703088760376, loss=1.5627484321594238
I0212 21:29:22.751416 140280856385280 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.9434773921966553, loss=1.5803327560424805
I0212 21:30:37.792287 140280847992576 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.276787281036377, loss=1.562413215637207
I0212 21:31:52.947370 140280856385280 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.5099775791168213, loss=1.594696044921875
I0212 21:33:08.180498 140280847992576 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.7477844953536987, loss=1.6113961935043335
I0212 21:34:24.537863 140280856385280 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.7859392166137695, loss=1.5766801834106445
I0212 21:35:49.711222 140280847992576 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.3834080696105957, loss=1.6370009183883667
I0212 21:37:15.319934 140280856385280 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.002528190612793, loss=1.5165824890136719
I0212 21:38:40.163754 140280847992576 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.4207558631896973, loss=1.4841500520706177
I0212 21:40:05.034003 140280856385280 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.861664056777954, loss=1.5696009397506714
I0212 21:41:29.598395 140280847992576 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.6786274909973145, loss=1.536669373512268
I0212 21:42:56.160074 140280856385280 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.831573724746704, loss=1.582483172416687
I0212 21:44:11.089742 140280847992576 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.576414108276367, loss=1.4980696439743042
I0212 21:44:19.198776 140437690197824 spec.py:321] Evaluating on the training split.
I0212 21:45:17.269695 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 21:46:08.714852 140437690197824 spec.py:349] Evaluating on the test split.
I0212 21:46:34.938953 140437690197824 submission_runner.py:408] Time since start: 23853.67s, 	Step: 26912, 	{'train/ctc_loss': Array(0.31949133, dtype=float32), 'train/wer': 0.10684211070125951, 'validation/ctc_loss': Array(0.6205764, dtype=float32), 'validation/wer': 0.180821997161532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3652431, dtype=float32), 'test/wer': 0.11914772611866024, 'test/num_examples': 2472, 'score': 21621.946141719818, 'total_duration': 23853.673711776733, 'accumulated_submission_time': 21621.946141719818, 'accumulated_eval_time': 2229.720189809799, 'accumulated_logging_time': 0.790858268737793}
I0212 21:46:34.981086 140280856385280 logging_writer.py:48] [26912] accumulated_eval_time=2229.720190, accumulated_logging_time=0.790858, accumulated_submission_time=21621.946142, global_step=26912, preemption_count=0, score=21621.946142, test/ctc_loss=0.3652431070804596, test/num_examples=2472, test/wer=0.119148, total_duration=23853.673712, train/ctc_loss=0.31949132680892944, train/wer=0.106842, validation/ctc_loss=0.6205763816833496, validation/num_examples=5348, validation/wer=0.180822
I0212 21:47:41.804291 140280847992576 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.6711704730987549, loss=1.504007339477539
I0212 21:48:56.791391 140280856385280 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.5294920206069946, loss=1.5455849170684814
I0212 21:50:11.954567 140280847992576 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.285820960998535, loss=1.4800165891647339
I0212 21:51:27.053919 140280856385280 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.9500811100006104, loss=1.5504387617111206
I0212 21:52:46.292200 140280847992576 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.0054104328155518, loss=1.4912775754928589
I0212 21:54:11.350174 140280856385280 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.6576695442199707, loss=1.5324039459228516
I0212 21:55:36.246856 140280847992576 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.179879903793335, loss=1.5228382349014282
I0212 21:57:01.790868 140280856385280 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.639737606048584, loss=1.53005850315094
I0212 21:58:27.415148 140280847992576 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.53125262260437, loss=1.527482032775879
I0212 21:59:46.745567 140280856385280 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.8676173686981201, loss=1.4772565364837646
I0212 22:01:01.994577 140280847992576 logging_writer.py:48] [28000] global_step=28000, grad_norm=4.514863014221191, loss=1.5700089931488037
I0212 22:02:16.933499 140280856385280 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.7404825687408447, loss=1.588767409324646
I0212 22:03:32.115168 140280847992576 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.919923782348633, loss=1.5400899648666382
I0212 22:04:52.790517 140280856385280 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.32529354095459, loss=1.4875448942184448
I0212 22:06:17.948393 140280847992576 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.071763038635254, loss=1.563339114189148
I0212 22:07:43.805797 140280856385280 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.749706745147705, loss=1.5236587524414062
I0212 22:09:08.065358 140280847992576 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.2667629718780518, loss=1.5236010551452637
I0212 22:10:33.144485 140280856385280 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.1790201663970947, loss=1.532347321510315
I0212 22:10:35.488144 140437690197824 spec.py:321] Evaluating on the training split.
I0212 22:11:32.933756 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 22:12:24.435382 140437690197824 spec.py:349] Evaluating on the test split.
I0212 22:12:50.916621 140437690197824 submission_runner.py:408] Time since start: 25429.65s, 	Step: 28704, 	{'train/ctc_loss': Array(0.30595523, dtype=float32), 'train/wer': 0.10328690339101583, 'validation/ctc_loss': Array(0.6050423, dtype=float32), 'validation/wer': 0.17387064695830157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3550856, dtype=float32), 'test/wer': 0.11575569232019174, 'test/num_examples': 2472, 'score': 23062.360239744186, 'total_duration': 25429.651166677475, 'accumulated_submission_time': 23062.360239744186, 'accumulated_eval_time': 2365.1423330307007, 'accumulated_logging_time': 0.8519728183746338}
I0212 22:12:50.954327 140280856385280 logging_writer.py:48] [28704] accumulated_eval_time=2365.142333, accumulated_logging_time=0.851973, accumulated_submission_time=23062.360240, global_step=28704, preemption_count=0, score=23062.360240, test/ctc_loss=0.3550856113433838, test/num_examples=2472, test/wer=0.115756, total_duration=25429.651167, train/ctc_loss=0.3059552311897278, train/wer=0.103287, validation/ctc_loss=0.6050422787666321, validation/num_examples=5348, validation/wer=0.173871
I0212 22:14:03.997396 140280847992576 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.0626049041748047, loss=1.5174000263214111
I0212 22:15:23.152356 140280856385280 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.9917099475860596, loss=1.5385578870773315
I0212 22:16:38.308901 140280847992576 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.131500720977783, loss=1.4591820240020752
I0212 22:17:53.136287 140280856385280 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.165959596633911, loss=1.4904736280441284
I0212 22:19:08.272857 140280847992576 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.081097364425659, loss=1.5023889541625977
I0212 22:20:29.349920 140280856385280 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.262805461883545, loss=1.4698683023452759
I0212 22:21:52.835271 140280847992576 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.76461124420166, loss=1.515292763710022
I0212 22:23:17.666921 140280856385280 logging_writer.py:48] [29500] global_step=29500, grad_norm=4.467475414276123, loss=1.5515230894088745
I0212 22:24:43.007257 140280847992576 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.6607235670089722, loss=1.4933677911758423
I0212 22:26:07.524538 140280856385280 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.160763740539551, loss=1.4983400106430054
I0212 22:27:32.846355 140280847992576 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.7831575870513916, loss=1.4955902099609375
I0212 22:28:59.365561 140280856385280 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.844595432281494, loss=1.455999732017517
I0212 22:30:14.435583 140280847992576 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.3502745628356934, loss=1.4033071994781494
I0212 22:31:29.305287 140280856385280 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.9316043853759766, loss=1.4403893947601318
I0212 22:32:44.250537 140280847992576 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.3136181831359863, loss=1.4765604734420776
I0212 22:33:59.265658 140280856385280 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.879434585571289, loss=1.4998139142990112
I0212 22:35:22.769922 140280847992576 logging_writer.py:48] [30400] global_step=30400, grad_norm=4.30596923828125, loss=1.4315037727355957
I0212 22:36:47.060648 140280856385280 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.9486442804336548, loss=1.475014090538025
I0212 22:36:52.053987 140437690197824 spec.py:321] Evaluating on the training split.
I0212 22:37:49.486128 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 22:38:41.258721 140437690197824 spec.py:349] Evaluating on the test split.
I0212 22:39:07.773831 140437690197824 submission_runner.py:408] Time since start: 27006.51s, 	Step: 30507, 	{'train/ctc_loss': Array(0.2792333, dtype=float32), 'train/wer': 0.09602524056099453, 'validation/ctc_loss': Array(0.5789832, dtype=float32), 'validation/wer': 0.16743099336725334, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33707365, dtype=float32), 'test/wer': 0.10915442893993865, 'test/num_examples': 2472, 'score': 24503.368768692017, 'total_duration': 27006.50775051117, 'accumulated_submission_time': 24503.368768692017, 'accumulated_eval_time': 2500.8551738262177, 'accumulated_logging_time': 0.9049315452575684}
I0212 22:39:07.813896 140280856385280 logging_writer.py:48] [30507] accumulated_eval_time=2500.855174, accumulated_logging_time=0.904932, accumulated_submission_time=24503.368769, global_step=30507, preemption_count=0, score=24503.368769, test/ctc_loss=0.3370736539363861, test/num_examples=2472, test/wer=0.109154, total_duration=27006.507751, train/ctc_loss=0.27923330664634705, train/wer=0.096025, validation/ctc_loss=0.5789831876754761, validation/num_examples=5348, validation/wer=0.167431
I0212 22:40:18.541631 140280847992576 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.7370853424072266, loss=1.539881706237793
I0212 22:41:33.743110 140280856385280 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.2587337493896484, loss=1.4963022470474243
I0212 22:42:48.937489 140280847992576 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.5663055181503296, loss=1.4776946306228638
I0212 22:44:14.734284 140280856385280 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.5251553058624268, loss=1.495894193649292
I0212 22:45:29.885493 140280847992576 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.896247625350952, loss=1.4739816188812256
I0212 22:46:44.896005 140280856385280 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.725782632827759, loss=1.5078332424163818
I0212 22:48:00.152653 140280847992576 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.8734700679779053, loss=1.460806965827942
I0212 22:49:15.462881 140280856385280 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.3989946842193604, loss=1.431158423423767
I0212 22:50:35.880533 140280847992576 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.8615773916244507, loss=1.4425337314605713
I0212 22:52:00.958677 140280856385280 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.3977904319763184, loss=1.4599685668945312
I0212 22:53:26.541784 140280847992576 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.596091389656067, loss=1.3917744159698486
I0212 22:54:52.413040 140280856385280 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.9713490009307861, loss=1.4857125282287598
I0212 22:56:17.672998 140280847992576 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.7631916999816895, loss=1.5086153745651245
I0212 22:57:43.127352 140280856385280 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.5959291458129883, loss=1.3562216758728027
I0212 22:59:04.146390 140280856385280 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.2772655487060547, loss=1.3690876960754395
I0212 23:00:19.620932 140280847992576 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.0310888290405273, loss=1.453662395477295
I0212 23:01:34.502475 140280856385280 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.7010302543640137, loss=1.4456321001052856
I0212 23:02:49.392983 140280847992576 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.771264672279358, loss=1.3969322443008423
I0212 23:03:07.920974 140437690197824 spec.py:321] Evaluating on the training split.
I0212 23:04:04.888010 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 23:04:56.460096 140437690197824 spec.py:349] Evaluating on the test split.
I0212 23:05:22.623103 140437690197824 submission_runner.py:408] Time since start: 28581.36s, 	Step: 32326, 	{'train/ctc_loss': Array(0.26147306, dtype=float32), 'train/wer': 0.08957165092428733, 'validation/ctc_loss': Array(0.5437209, dtype=float32), 'validation/wer': 0.15865491373567492, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31017295, dtype=float32), 'test/wer': 0.0994658054556903, 'test/num_examples': 2472, 'score': 25943.38379716873, 'total_duration': 28581.357215881348, 'accumulated_submission_time': 25943.38379716873, 'accumulated_eval_time': 2635.5505130290985, 'accumulated_logging_time': 0.9616458415985107}
I0212 23:05:22.665993 140280856385280 logging_writer.py:48] [32326] accumulated_eval_time=2635.550513, accumulated_logging_time=0.961646, accumulated_submission_time=25943.383797, global_step=32326, preemption_count=0, score=25943.383797, test/ctc_loss=0.3101729452610016, test/num_examples=2472, test/wer=0.099466, total_duration=28581.357216, train/ctc_loss=0.26147305965423584, train/wer=0.089572, validation/ctc_loss=0.5437209010124207, validation/num_examples=5348, validation/wer=0.158655
I0212 23:06:19.032465 140280847992576 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.1949925422668457, loss=1.4715042114257812
I0212 23:07:34.165194 140280856385280 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.5745491981506348, loss=1.4251664876937866
I0212 23:08:49.107321 140280847992576 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.7661561965942383, loss=1.3397549390792847
I0212 23:10:09.062603 140280856385280 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.593228816986084, loss=1.3629624843597412
I0212 23:11:33.701357 140280847992576 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.992924451828003, loss=1.4021475315093994
I0212 23:12:58.509428 140280856385280 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.8471522331237793, loss=1.3900421857833862
I0212 23:14:23.022576 140280856385280 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.5382118225097656, loss=1.3974723815917969
I0212 23:15:38.313440 140280847992576 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.8578038215637207, loss=1.4240630865097046
I0212 23:16:53.216217 140280856385280 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.9299921989440918, loss=1.4451379776000977
I0212 23:18:08.194177 140280847992576 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.047077178955078, loss=1.39431631565094
I0212 23:19:23.543369 140280856385280 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.420058488845825, loss=1.3333107233047485
I0212 23:20:48.839527 140280847992576 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.493196964263916, loss=1.3902556896209717
I0212 23:22:14.158262 140280856385280 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.302345037460327, loss=1.370270013809204
I0212 23:23:38.402028 140280847992576 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.7316783666610718, loss=1.3296819925308228
I0212 23:25:04.572218 140280856385280 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.1108338832855225, loss=1.3936256170272827
I0212 23:26:30.624239 140280847992576 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.1921253204345703, loss=1.3560068607330322
I0212 23:27:57.607186 140280856385280 logging_writer.py:48] [34000] global_step=34000, grad_norm=4.877035617828369, loss=1.390305519104004
I0212 23:29:12.678348 140280847992576 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.7707327604293823, loss=1.3621940612792969
I0212 23:29:23.044769 140437690197824 spec.py:321] Evaluating on the training split.
I0212 23:30:20.332650 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 23:31:12.730108 140437690197824 spec.py:349] Evaluating on the test split.
I0212 23:31:39.052669 140437690197824 submission_runner.py:408] Time since start: 30157.79s, 	Step: 34115, 	{'train/ctc_loss': Array(0.23398308, dtype=float32), 'train/wer': 0.08175391055414674, 'validation/ctc_loss': Array(0.51874906, dtype=float32), 'validation/wer': 0.15006227251223728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29474744, dtype=float32), 'test/wer': 0.09528162005159141, 'test/num_examples': 2472, 'score': 27383.673309326172, 'total_duration': 30157.786672353745, 'accumulated_submission_time': 27383.673309326172, 'accumulated_eval_time': 2771.551521062851, 'accumulated_logging_time': 1.019352912902832}
I0212 23:31:39.091866 140280856385280 logging_writer.py:48] [34115] accumulated_eval_time=2771.551521, accumulated_logging_time=1.019353, accumulated_submission_time=27383.673309, global_step=34115, preemption_count=0, score=27383.673309, test/ctc_loss=0.2947474420070648, test/num_examples=2472, test/wer=0.095282, total_duration=30157.786672, train/ctc_loss=0.2339830845594406, train/wer=0.081754, validation/ctc_loss=0.5187490582466125, validation/num_examples=5348, validation/wer=0.150062
I0212 23:32:43.519485 140280847992576 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.382946491241455, loss=1.3546730279922485
I0212 23:33:58.529476 140280856385280 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.3488521575927734, loss=1.377341866493225
I0212 23:35:13.477791 140280847992576 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.1941559314727783, loss=1.3601433038711548
I0212 23:36:28.746895 140280856385280 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.1184422969818115, loss=1.365920066833496
I0212 23:37:46.543999 140280847992576 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.612959861755371, loss=1.359860897064209
I0212 23:39:11.147989 140280856385280 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.6443785429000854, loss=1.3623114824295044
I0212 23:40:35.721381 140280847992576 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.737911581993103, loss=1.3407233953475952
I0212 23:42:00.040169 140280856385280 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.5853590965270996, loss=1.418556809425354
I0212 23:43:25.541410 140280847992576 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.9152532815933228, loss=1.370752215385437
I0212 23:44:45.837736 140280856385280 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.4309298992156982, loss=1.3749953508377075
I0212 23:46:00.834084 140280847992576 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.9972388744354248, loss=1.308070421218872
I0212 23:47:16.069542 140280856385280 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.9811780452728271, loss=1.3461201190948486
I0212 23:48:31.138389 140280847992576 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.2937004566192627, loss=1.3561089038848877
I0212 23:49:49.142828 140280856385280 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.4277942180633545, loss=1.3595736026763916
I0212 23:51:14.569956 140280847992576 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.2316155433654785, loss=1.323898196220398
I0212 23:52:39.022496 140280856385280 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.8096721172332764, loss=1.3486604690551758
I0212 23:54:04.297800 140280847992576 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.8846209049224854, loss=1.303073525428772
I0212 23:55:28.235030 140280856385280 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.6321297883987427, loss=1.3576561212539673
I0212 23:55:39.522461 140437690197824 spec.py:321] Evaluating on the training split.
I0212 23:56:36.728044 140437690197824 spec.py:333] Evaluating on the validation split.
I0212 23:57:28.623183 140437690197824 spec.py:349] Evaluating on the test split.
I0212 23:57:55.513942 140437690197824 submission_runner.py:408] Time since start: 31734.25s, 	Step: 35915, 	{'train/ctc_loss': Array(0.25606412, dtype=float32), 'train/wer': 0.08354766267733148, 'validation/ctc_loss': Array(0.49699828, dtype=float32), 'validation/wer': 0.14366123753342924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27777967, dtype=float32), 'test/wer': 0.09046777567891455, 'test/num_examples': 2472, 'score': 28824.013179302216, 'total_duration': 31734.248168230057, 'accumulated_submission_time': 28824.013179302216, 'accumulated_eval_time': 2907.5363302230835, 'accumulated_logging_time': 1.075251579284668}
I0212 23:57:55.555903 140280856385280 logging_writer.py:48] [35915] accumulated_eval_time=2907.536330, accumulated_logging_time=1.075252, accumulated_submission_time=28824.013179, global_step=35915, preemption_count=0, score=28824.013179, test/ctc_loss=0.2777796685695648, test/num_examples=2472, test/wer=0.090468, total_duration=31734.248168, train/ctc_loss=0.25606411695480347, train/wer=0.083548, validation/ctc_loss=0.49699828028678894, validation/num_examples=5348, validation/wer=0.143661
I0212 23:58:59.933619 140280847992576 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.1549909114837646, loss=1.3818472623825073
I0213 00:00:18.750846 140280856385280 logging_writer.py:48] [36100] global_step=36100, grad_norm=6.468932628631592, loss=1.2870562076568604
I0213 00:01:33.807704 140280847992576 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.829615592956543, loss=1.276571273803711
I0213 00:02:48.812576 140280856385280 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.325862169265747, loss=1.361825704574585
I0213 00:04:03.919937 140280847992576 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.331467628479004, loss=1.2969821691513062
I0213 00:05:22.574798 140280856385280 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.3526259660720825, loss=1.338753581047058
I0213 00:06:46.643963 140280847992576 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.3348989486694336, loss=1.37039315700531
I0213 00:08:11.157581 140280856385280 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.543081283569336, loss=1.2943280935287476
I0213 00:09:35.657782 140280847992576 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.0539743900299072, loss=1.2882964611053467
I0213 00:11:00.682195 140280856385280 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7224652767181396, loss=1.298516035079956
I0213 00:12:26.165378 140280847992576 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.777531862258911, loss=1.289100170135498
I0213 00:13:51.878419 140280856385280 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.892553448677063, loss=1.302000641822815
I0213 00:15:07.209607 140280847992576 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.1100032329559326, loss=1.2778342962265015
I0213 00:16:22.698339 140280856385280 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.765626311302185, loss=1.2982068061828613
I0213 00:17:37.967732 140280847992576 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.8303000926971436, loss=1.3363457918167114
I0213 00:18:53.366357 140280856385280 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.7455260753631592, loss=1.2712994813919067
I0213 00:20:14.281758 140280847992576 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.9364880323410034, loss=1.3402405977249146
I0213 00:21:39.194917 140280856385280 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.453613758087158, loss=1.300963044166565
I0213 00:21:55.833545 140437690197824 spec.py:321] Evaluating on the training split.
I0213 00:22:53.968388 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 00:23:46.123950 140437690197824 spec.py:349] Evaluating on the test split.
I0213 00:24:12.845764 140437690197824 submission_runner.py:408] Time since start: 33311.58s, 	Step: 37721, 	{'train/ctc_loss': Array(0.20800158, dtype=float32), 'train/wer': 0.07206750787644665, 'validation/ctc_loss': Array(0.47449413, dtype=float32), 'validation/wer': 0.13778155381986348, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26118982, dtype=float32), 'test/wer': 0.08331809964860967, 'test/num_examples': 2472, 'score': 30264.20079755783, 'total_duration': 33311.580392599106, 'accumulated_submission_time': 30264.20079755783, 'accumulated_eval_time': 3044.5422701835632, 'accumulated_logging_time': 1.1326208114624023}
I0213 00:24:12.884117 140280856385280 logging_writer.py:48] [37721] accumulated_eval_time=3044.542270, accumulated_logging_time=1.132621, accumulated_submission_time=30264.200798, global_step=37721, preemption_count=0, score=30264.200798, test/ctc_loss=0.2611898183822632, test/num_examples=2472, test/wer=0.083318, total_duration=33311.580393, train/ctc_loss=0.20800158381462097, train/wer=0.072068, validation/ctc_loss=0.4744941294193268, validation/num_examples=5348, validation/wer=0.137782
I0213 00:25:13.108642 140280847992576 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.1587681770324707, loss=1.3662511110305786
I0213 00:26:28.258084 140280856385280 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.026113986968994, loss=1.3491421937942505
I0213 00:27:43.586091 140280847992576 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.4121198654174805, loss=1.2719473838806152
I0213 00:29:02.689051 140280856385280 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.3575279712677, loss=1.3116806745529175
I0213 00:30:22.160508 140280856385280 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.0899319648742676, loss=1.277262568473816
I0213 00:31:37.183466 140280847992576 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.542877197265625, loss=1.261457085609436
I0213 00:32:52.493092 140280856385280 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.6725423336029053, loss=1.252321481704712
I0213 00:34:07.570251 140280847992576 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.0544540882110596, loss=1.2905995845794678
I0213 00:35:27.780946 140280856385280 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.441690683364868, loss=1.3162301778793335
I0213 00:36:52.520269 140280847992576 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.0414140224456787, loss=1.28581702709198
I0213 00:38:17.348713 140280856385280 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.1194770336151123, loss=1.2648720741271973
I0213 00:39:42.328673 140280847992576 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.67638897895813, loss=1.2280129194259644
I0213 00:41:07.588029 140280856385280 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.8962934017181396, loss=1.315886378288269
I0213 00:42:32.090329 140280847992576 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.4464972019195557, loss=1.3155170679092407
I0213 00:43:55.356203 140280856385280 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.90887188911438, loss=1.2324453592300415
I0213 00:45:09.997663 140280847992576 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.168253183364868, loss=1.2373815774917603
I0213 00:46:24.953156 140280856385280 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.1243462562561035, loss=1.285523533821106
I0213 00:47:39.920983 140280847992576 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.1344380378723145, loss=1.2179582118988037
I0213 00:48:13.572258 140437690197824 spec.py:321] Evaluating on the training split.
I0213 00:49:11.754755 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 00:50:03.373983 140437690197824 spec.py:349] Evaluating on the test split.
I0213 00:50:29.939171 140437690197824 submission_runner.py:408] Time since start: 34888.67s, 	Step: 39546, 	{'train/ctc_loss': Array(0.17795631, dtype=float32), 'train/wer': 0.06375735066542866, 'validation/ctc_loss': Array(0.45436054, dtype=float32), 'validation/wer': 0.1326259690858009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24431218, dtype=float32), 'test/wer': 0.07939796477972091, 'test/num_examples': 2472, 'score': 31704.79609966278, 'total_duration': 34888.67411828041, 'accumulated_submission_time': 31704.79609966278, 'accumulated_eval_time': 3180.9032578468323, 'accumulated_logging_time': 1.188124656677246}
I0213 00:50:29.977867 140280856385280 logging_writer.py:48] [39546] accumulated_eval_time=3180.903258, accumulated_logging_time=1.188125, accumulated_submission_time=31704.796100, global_step=39546, preemption_count=0, score=31704.796100, test/ctc_loss=0.24431218206882477, test/num_examples=2472, test/wer=0.079398, total_duration=34888.674118, train/ctc_loss=0.17795631289482117, train/wer=0.063757, validation/ctc_loss=0.4543605446815491, validation/num_examples=5348, validation/wer=0.132626
I0213 00:51:11.284530 140280847992576 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.311096668243408, loss=1.305112361907959
I0213 00:52:26.549407 140280856385280 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.627406358718872, loss=1.2565491199493408
I0213 00:53:41.507492 140280847992576 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.601402521133423, loss=1.2649931907653809
I0213 00:54:58.738379 140280856385280 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.706042528152466, loss=1.2555735111236572
I0213 00:56:23.476351 140280847992576 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.7134603261947632, loss=1.2702323198318481
I0213 00:57:47.297137 140280856385280 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.6637144088745117, loss=1.267067790031433
I0213 00:59:12.993885 140280856385280 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.8506228923797607, loss=1.241801142692566
I0213 01:00:28.030099 140280847992576 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.9462199211120605, loss=1.1266465187072754
I0213 01:01:43.139229 140280856385280 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.651616334915161, loss=1.2378963232040405
I0213 01:02:58.361396 140280847992576 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.4750957489013672, loss=1.2052003145217896
I0213 01:04:13.435547 140280856385280 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.890162706375122, loss=1.250133991241455
I0213 01:05:35.451799 140280847992576 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.280034065246582, loss=1.1544970273971558
I0213 01:07:00.273856 140280856385280 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.0306191444396973, loss=1.2374827861785889
I0213 01:08:24.493222 140280847992576 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.5891988277435303, loss=1.2381969690322876
I0213 01:09:48.963957 140280856385280 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.087489128112793, loss=1.2160735130310059
I0213 01:11:13.452440 140280847992576 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.845851182937622, loss=1.2052762508392334
I0213 01:12:42.346246 140280856385280 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.5274431705474854, loss=1.1797995567321777
I0213 01:13:57.400953 140280847992576 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.2692277431488037, loss=1.13435697555542
I0213 01:14:30.364984 140437690197824 spec.py:321] Evaluating on the training split.
I0213 01:15:26.860604 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 01:16:18.803601 140437690197824 spec.py:349] Evaluating on the test split.
I0213 01:16:45.264890 140437690197824 submission_runner.py:408] Time since start: 36464.00s, 	Step: 41345, 	{'train/ctc_loss': Array(0.16376394, dtype=float32), 'train/wer': 0.05626279001780636, 'validation/ctc_loss': Array(0.43039808, dtype=float32), 'validation/wer': 0.1254139432499493, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23327044, dtype=float32), 'test/wer': 0.07456380882741251, 'test/num_examples': 2472, 'score': 33145.08502173424, 'total_duration': 36463.99954032898, 'accumulated_submission_time': 33145.08502173424, 'accumulated_eval_time': 3315.796919107437, 'accumulated_logging_time': 1.2503759860992432}
I0213 01:16:45.310376 140280856385280 logging_writer.py:48] [41345] accumulated_eval_time=3315.796919, accumulated_logging_time=1.250376, accumulated_submission_time=33145.085022, global_step=41345, preemption_count=0, score=33145.085022, test/ctc_loss=0.23327043652534485, test/num_examples=2472, test/wer=0.074564, total_duration=36463.999540, train/ctc_loss=0.16376394033432007, train/wer=0.056263, validation/ctc_loss=0.4303980767726898, validation/num_examples=5348, validation/wer=0.125414
I0213 01:17:27.451448 140280847992576 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.0263218879699707, loss=1.2159897089004517
I0213 01:18:42.433552 140280856385280 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.5507991313934326, loss=1.155377984046936
I0213 01:19:57.485518 140280847992576 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.863992214202881, loss=1.1885567903518677
I0213 01:21:12.652786 140280856385280 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.1319587230682373, loss=1.1488265991210938
I0213 01:22:30.070362 140280847992576 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.0263540744781494, loss=1.158318281173706
I0213 01:23:54.848243 140280856385280 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.7501775026321411, loss=1.1770071983337402
I0213 01:25:19.632193 140280847992576 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.0011515617370605, loss=1.2596434354782104
I0213 01:26:43.584569 140280856385280 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.0139212608337402, loss=1.1940052509307861
I0213 01:28:08.097861 140280847992576 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.200169801712036, loss=1.2031432390213013
I0213 01:29:29.494681 140280856385280 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.8263959884643555, loss=1.2108455896377563
I0213 01:30:44.573662 140280847992576 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.2989768981933594, loss=1.1856350898742676
I0213 01:31:59.685021 140280856385280 logging_writer.py:48] [42500] global_step=42500, grad_norm=4.389649868011475, loss=1.1544302701950073
I0213 01:33:14.801369 140280847992576 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.6833484172821045, loss=1.172603964805603
I0213 01:34:34.019703 140280856385280 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.142072916030884, loss=1.1577048301696777
I0213 01:35:58.214810 140280847992576 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.124016046524048, loss=1.2044169902801514
I0213 01:37:23.603594 140280856385280 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.9044127464294434, loss=1.1886646747589111
I0213 01:38:48.181078 140280847992576 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.8424960374832153, loss=1.1479641199111938
I0213 01:40:13.167778 140280856385280 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.9290399551391602, loss=1.144098162651062
I0213 01:40:45.592738 140437690197824 spec.py:321] Evaluating on the training split.
I0213 01:41:42.138691 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 01:42:34.417893 140437690197824 spec.py:349] Evaluating on the test split.
I0213 01:43:00.950783 140437690197824 submission_runner.py:408] Time since start: 38039.68s, 	Step: 43139, 	{'train/ctc_loss': Array(0.14803201, dtype=float32), 'train/wer': 0.05255167958656331, 'validation/ctc_loss': Array(0.41873074, dtype=float32), 'validation/wer': 0.1214844994545121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22326158, dtype=float32), 'test/wer': 0.07161862978083805, 'test/num_examples': 2472, 'score': 34585.27427482605, 'total_duration': 38039.684517383575, 'accumulated_submission_time': 34585.27427482605, 'accumulated_eval_time': 3451.147789478302, 'accumulated_logging_time': 1.3133165836334229}
I0213 01:43:00.996400 140280856385280 logging_writer.py:48] [43139] accumulated_eval_time=3451.147789, accumulated_logging_time=1.313317, accumulated_submission_time=34585.274275, global_step=43139, preemption_count=0, score=34585.274275, test/ctc_loss=0.22326157987117767, test/num_examples=2472, test/wer=0.071619, total_duration=38039.684517, train/ctc_loss=0.14803200960159302, train/wer=0.052552, validation/ctc_loss=0.4187307357788086, validation/num_examples=5348, validation/wer=0.121484
I0213 01:43:47.608918 140280847992576 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.1065313816070557, loss=1.2208820581436157
I0213 01:45:06.384492 140280856385280 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.1360177993774414, loss=1.179173231124878
I0213 01:46:21.397688 140280847992576 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.95673406124115, loss=1.1600309610366821
I0213 01:47:36.357455 140280856385280 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.4616403579711914, loss=1.176143765449524
I0213 01:48:51.296880 140280847992576 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.4854068756103516, loss=1.1167446374893188
I0213 01:50:07.278914 140280856385280 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.4046549797058105, loss=1.1665608882904053
I0213 01:51:30.979450 140280847992576 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.2132413387298584, loss=1.137986421585083
I0213 01:52:54.735218 140280856385280 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.2170324325561523, loss=1.1395456790924072
I0213 01:54:20.737289 140280847992576 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.0296597480773926, loss=1.1873195171356201
I0213 01:55:47.176719 140280856385280 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6787991523742676, loss=1.1556460857391357
I0213 01:57:11.332314 140280847992576 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.274570941925049, loss=1.157049298286438
I0213 01:58:38.265067 140280856385280 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.7600247859954834, loss=1.1501091718673706
I0213 01:59:53.541370 140280847992576 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.297997236251831, loss=1.1255097389221191
I0213 02:01:08.945715 140280856385280 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.4415366649627686, loss=1.1320199966430664
I0213 02:02:24.429130 140280847992576 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.106520175933838, loss=1.156783103942871
I0213 02:03:39.699948 140280856385280 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.6358225345611572, loss=1.1487005949020386
I0213 02:05:00.837280 140280847992576 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.592042088508606, loss=1.1372101306915283
I0213 02:06:26.508535 140280856385280 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.617029905319214, loss=1.1990102529525757
I0213 02:07:01.129531 140437690197824 spec.py:321] Evaluating on the training split.
I0213 02:07:58.681167 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 02:08:51.067070 140437690197824 spec.py:349] Evaluating on the test split.
I0213 02:09:17.439364 140437690197824 submission_runner.py:408] Time since start: 39616.17s, 	Step: 44943, 	{'train/ctc_loss': Array(0.16079587, dtype=float32), 'train/wer': 0.05456287072173304, 'validation/ctc_loss': Array(0.4065272, dtype=float32), 'validation/wer': 0.11763229288355523, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2159714, dtype=float32), 'test/wer': 0.06940466760099932, 'test/num_examples': 2472, 'score': 36025.31555771828, 'total_duration': 39616.17360472679, 'accumulated_submission_time': 36025.31555771828, 'accumulated_eval_time': 3587.4509439468384, 'accumulated_logging_time': 1.3760840892791748}
I0213 02:09:17.477879 140280856385280 logging_writer.py:48] [44943] accumulated_eval_time=3587.450944, accumulated_logging_time=1.376084, accumulated_submission_time=36025.315558, global_step=44943, preemption_count=0, score=36025.315558, test/ctc_loss=0.21597139537334442, test/num_examples=2472, test/wer=0.069405, total_duration=39616.173605, train/ctc_loss=0.16079586744308472, train/wer=0.054563, validation/ctc_loss=0.40652719140052795, validation/num_examples=5348, validation/wer=0.117632
I0213 02:10:01.093396 140280847992576 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.2345550060272217, loss=1.1661127805709839
I0213 02:11:16.142981 140280856385280 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.9934654235839844, loss=1.090862512588501
I0213 02:12:31.481258 140280847992576 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.205026626586914, loss=1.1344897747039795
I0213 02:13:50.771937 140280856385280 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.413419246673584, loss=1.1243520975112915
I0213 02:15:11.151881 140280856385280 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.584815263748169, loss=1.1590934991836548
I0213 02:16:26.176256 140280847992576 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.2225797176361084, loss=1.1247996091842651
I0213 02:17:41.576763 140280856385280 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.7916481494903564, loss=1.1271545886993408
I0213 02:18:56.862372 140280847992576 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.6027915477752686, loss=1.1605470180511475
I0213 02:20:13.358486 140280856385280 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.750826120376587, loss=1.0997449159622192
I0213 02:21:37.845204 140280847992576 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.7265284061431885, loss=1.080612301826477
I0213 02:23:02.144033 140280856385280 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.1106762886047363, loss=1.1745781898498535
I0213 02:24:26.375922 140280847992576 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.6759397983551025, loss=1.1553971767425537
I0213 02:25:51.040194 140280856385280 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.0716397762298584, loss=1.18686044216156
I0213 02:27:16.559343 140280847992576 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.3186089992523193, loss=1.1103485822677612
I0213 02:28:40.194478 140280856385280 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.8633941411972046, loss=1.0828502178192139
I0213 02:29:55.466192 140280847992576 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.576509714126587, loss=1.1088758707046509
I0213 02:31:10.814940 140280856385280 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.0812220573425293, loss=1.1007026433944702
I0213 02:32:26.112989 140280847992576 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.1766512393951416, loss=1.1318039894104004
I0213 02:33:17.835700 140437690197824 spec.py:321] Evaluating on the training split.
I0213 02:34:15.876066 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 02:35:08.115862 140437690197824 spec.py:349] Evaluating on the test split.
I0213 02:35:34.241723 140437690197824 submission_runner.py:408] Time since start: 41192.98s, 	Step: 46770, 	{'train/ctc_loss': Array(0.15611503, dtype=float32), 'train/wer': 0.052918968159104435, 'validation/ctc_loss': Array(0.40304145, dtype=float32), 'validation/wer': 0.11674406480203134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2135498, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 37465.57762217522, 'total_duration': 41192.976047992706, 'accumulated_submission_time': 37465.57762217522, 'accumulated_eval_time': 3723.850387096405, 'accumulated_logging_time': 1.433948278427124}
I0213 02:35:34.282577 140280856385280 logging_writer.py:48] [46770] accumulated_eval_time=3723.850387, accumulated_logging_time=1.433948, accumulated_submission_time=37465.577622, global_step=46770, preemption_count=0, score=37465.577622, test/ctc_loss=0.21354979276657104, test/num_examples=2472, test/wer=0.068348, total_duration=41192.976048, train/ctc_loss=0.15611502528190613, train/wer=0.052919, validation/ctc_loss=0.40304145216941833, validation/num_examples=5348, validation/wer=0.116744
I0213 02:35:57.659215 140280847992576 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.687011241912842, loss=1.1133848428726196
I0213 02:37:12.728254 140280856385280 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.723076581954956, loss=1.0830717086791992
I0213 02:38:27.820555 140280847992576 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.2738938331604004, loss=1.1139296293258667
I0213 02:39:43.297381 140280856385280 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.3843801021575928, loss=1.089089274406433
I0213 02:41:07.636662 140280847992576 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.6940861940383911, loss=1.1408336162567139
I0213 02:42:32.627959 140280856385280 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.7835679054260254, loss=1.138642430305481
I0213 02:43:58.147486 140280856385280 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.8112502098083496, loss=1.162512183189392
I0213 02:45:13.318109 140280847992576 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.5330952405929565, loss=1.1742044687271118
I0213 02:46:28.507588 140280856385280 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.2977943420410156, loss=1.1339857578277588
I0213 02:47:43.787724 140280847992576 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.6669890880584717, loss=1.0942851305007935
I0213 02:48:58.866832 140280856385280 logging_writer.py:48] [47800] global_step=47800, grad_norm=4.200168132781982, loss=1.1620668172836304
I0213 02:50:19.376230 140280847992576 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.688516139984131, loss=1.0706108808517456
I0213 02:51:41.801308 140437690197824 spec.py:321] Evaluating on the training split.
I0213 02:52:40.787843 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 02:53:32.778262 140437690197824 spec.py:349] Evaluating on the test split.
I0213 02:53:59.463629 140437690197824 submission_runner.py:408] Time since start: 42298.20s, 	Step: 48000, 	{'train/ctc_loss': Array(0.147639, dtype=float32), 'train/wer': 0.05145846511193836, 'validation/ctc_loss': Array(0.40327126, dtype=float32), 'validation/wer': 0.11670544618979117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21367131, dtype=float32), 'test/wer': 0.06851095809721122, 'test/num_examples': 2472, 'score': 38433.03055810928, 'total_duration': 42298.19808506966, 'accumulated_submission_time': 38433.03055810928, 'accumulated_eval_time': 3861.5062580108643, 'accumulated_logging_time': 1.4898200035095215}
I0213 02:53:59.506150 140280856385280 logging_writer.py:48] [48000] accumulated_eval_time=3861.506258, accumulated_logging_time=1.489820, accumulated_submission_time=38433.030558, global_step=48000, preemption_count=0, score=38433.030558, test/ctc_loss=0.21367131173610687, test/num_examples=2472, test/wer=0.068511, total_duration=42298.198085, train/ctc_loss=0.14763900637626648, train/wer=0.051458, validation/ctc_loss=0.40327125787734985, validation/num_examples=5348, validation/wer=0.116705
I0213 02:53:59.535677 140280847992576 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=38433.030558
I0213 02:53:59.713684 140437690197824 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 02:54:00.591523 140437690197824 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_4/checkpoint_48000
I0213 02:54:00.611795 140437690197824 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_4/checkpoint_48000.
I0213 02:54:01.996231 140437690197824 submission_runner.py:583] Tuning trial 4/5
I0213 02:54:01.996479 140437690197824 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0213 02:54:02.010330 140437690197824 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.853792, dtype=float32), 'train/wer': 3.525787909704684, 'validation/ctc_loss': Array(30.891838, dtype=float32), 'validation/wer': 3.324647363796982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942242, dtype=float32), 'test/wer': 3.698637093006723, 'test/num_examples': 2472, 'score': 16.281517267227173, 'total_duration': 209.31295228004456, 'accumulated_submission_time': 16.281517267227173, 'accumulated_eval_time': 193.03134417533875, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1789, {'train/ctc_loss': Array(1.6250488, dtype=float32), 'train/wer': 0.45638047836305823, 'validation/ctc_loss': Array(1.6839274, dtype=float32), 'validation/wer': 0.4397694468849262, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2356836, dtype=float32), 'test/wer': 0.3786890906505799, 'test/num_examples': 2472, 'score': 1456.6854193210602, 'total_duration': 1780.5886964797974, 'accumulated_submission_time': 1456.6854193210602, 'accumulated_eval_time': 323.79737067222595, 'accumulated_logging_time': 0.028933048248291016, 'global_step': 1789, 'preemption_count': 0}), (3594, {'train/ctc_loss': Array(0.87861687, dtype=float32), 'train/wer': 0.26925837981114364, 'validation/ctc_loss': Array(1.0104467, dtype=float32), 'validation/wer': 0.2839530011489037, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6614137, dtype=float32), 'test/wer': 0.21085450815509924, 'test/num_examples': 2472, 'score': 2897.1606023311615, 'total_duration': 3357.2956132888794, 'accumulated_submission_time': 2897.1606023311615, 'accumulated_eval_time': 459.8945791721344, 'accumulated_logging_time': 0.08021688461303711, 'global_step': 3594, 'preemption_count': 0}), (5389, {'train/ctc_loss': Array(0.98097146, dtype=float32), 'train/wer': 0.28369680790896545, 'validation/ctc_loss': Array(1.0103266, dtype=float32), 'validation/wer': 0.279453932822924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.646533, dtype=float32), 'test/wer': 0.20019092884853656, 'test/num_examples': 2472, 'score': 4337.480946302414, 'total_duration': 4932.8055765628815, 'accumulated_submission_time': 4337.480946302414, 'accumulated_eval_time': 594.9467799663544, 'accumulated_logging_time': 0.13591408729553223, 'global_step': 5389, 'preemption_count': 0}), (7157, {'train/ctc_loss': Array(0.8190737, dtype=float32), 'train/wer': 0.24712488769092542, 'validation/ctc_loss': Array(0.892246, dtype=float32), 'validation/wer': 0.2521023007038242, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.57268214, dtype=float32), 'test/wer': 0.18081368188003982, 'test/num_examples': 2472, 'score': 5777.349337816238, 'total_duration': 6507.6251838207245, 'accumulated_submission_time': 5777.349337816238, 'accumulated_eval_time': 729.7598900794983, 'accumulated_logging_time': 0.19289159774780273, 'global_step': 7157, 'preemption_count': 0}), (8942, {'train/ctc_loss': Array(0.79178494, dtype=float32), 'train/wer': 0.24491464615281927, 'validation/ctc_loss': Array(0.8644864, dtype=float32), 'validation/wer': 0.24434961429661026, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.54710823, dtype=float32), 'test/wer': 0.17362338269047184, 'test/num_examples': 2472, 'score': 7217.478336572647, 'total_duration': 8084.5014843940735, 'accumulated_submission_time': 7217.478336572647, 'accumulated_eval_time': 866.3746891021729, 'accumulated_logging_time': 0.244370698928833, 'global_step': 8942, 'preemption_count': 0}), (10758, {'train/ctc_loss': Array(0.7392486, dtype=float32), 'train/wer': 0.22892540944191442, 'validation/ctc_loss': Array(0.8693898, dtype=float32), 'validation/wer': 0.24681155082692102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55628407, dtype=float32), 'test/wer': 0.17457802693315458, 'test/num_examples': 2472, 'score': 8658.395947933197, 'total_duration': 9663.073320865631, 'accumulated_submission_time': 8658.395947933197, 'accumulated_eval_time': 1003.8939211368561, 'accumulated_logging_time': 0.29659557342529297, 'global_step': 10758, 'preemption_count': 0}), (12547, {'train/ctc_loss': Array(0.65287876, dtype=float32), 'train/wer': 0.20319984452345294, 'validation/ctc_loss': Array(0.8087663, dtype=float32), 'validation/wer': 0.2305241511146297, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5140238, dtype=float32), 'test/wer': 0.16482846871001158, 'test/num_examples': 2472, 'score': 10098.912323236465, 'total_duration': 11239.493403196335, 'accumulated_submission_time': 10098.912323236465, 'accumulated_eval_time': 1139.6634182929993, 'accumulated_logging_time': 0.3509695529937744, 'global_step': 12547, 'preemption_count': 0}), (14325, {'train/ctc_loss': Array(0.71331424, dtype=float32), 'train/wer': 0.22321453630990137, 'validation/ctc_loss': Array(0.78945225, dtype=float32), 'validation/wer': 0.22622783050291087, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48911905, dtype=float32), 'test/wer': 0.1573334958259704, 'test/num_examples': 2472, 'score': 11539.125578641891, 'total_duration': 12815.336868524551, 'accumulated_submission_time': 11539.125578641891, 'accumulated_eval_time': 1275.162227153778, 'accumulated_logging_time': 0.4020240306854248, 'global_step': 14325, 'preemption_count': 0}), (16117, {'train/ctc_loss': Array(0.62898093, dtype=float32), 'train/wer': 0.19862913810704388, 'validation/ctc_loss': Array(0.76331466, dtype=float32), 'validation/wer': 0.22087915270764746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47254232, dtype=float32), 'test/wer': 0.1539414620275019, 'test/num_examples': 2472, 'score': 12979.941724300385, 'total_duration': 14391.774739265442, 'accumulated_submission_time': 12979.941724300385, 'accumulated_eval_time': 1410.6499030590057, 'accumulated_logging_time': 0.4547863006591797, 'global_step': 16117, 'preemption_count': 0}), (17933, {'train/ctc_loss': Array(0.6621947, dtype=float32), 'train/wer': 0.20311503880058915, 'validation/ctc_loss': Array(0.7468117, dtype=float32), 'validation/wer': 0.21476775732064068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.45915446, dtype=float32), 'test/wer': 0.1470558365324071, 'test/num_examples': 2472, 'score': 14420.404343128204, 'total_duration': 15967.906356573105, 'accumulated_submission_time': 14420.404343128204, 'accumulated_eval_time': 1546.1836066246033, 'accumulated_logging_time': 0.5079362392425537, 'global_step': 17933, 'preemption_count': 0}), (19714, {'train/ctc_loss': Array(0.5638722, dtype=float32), 'train/wer': 0.17971140272338276, 'validation/ctc_loss': Array(0.7201087, dtype=float32), 'validation/wer': 0.20510344960753835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4412394, dtype=float32), 'test/wer': 0.14057644262994334, 'test/num_examples': 2472, 'score': 15860.634415864944, 'total_duration': 17543.55829191208, 'accumulated_submission_time': 15860.634415864944, 'accumulated_eval_time': 1681.472809791565, 'accumulated_logging_time': 0.5603365898132324, 'global_step': 19714, 'preemption_count': 0}), (21494, {'train/ctc_loss': Array(0.5838771, dtype=float32), 'train/wer': 0.1874216988223503, 'validation/ctc_loss': Array(0.70004, dtype=float32), 'validation/wer': 0.20001544744489608, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42382613, dtype=float32), 'test/wer': 0.1354782361424248, 'test/num_examples': 2472, 'score': 17300.85076236725, 'total_duration': 19118.08944129944, 'accumulated_submission_time': 17300.85076236725, 'accumulated_eval_time': 1815.649961233139, 'accumulated_logging_time': 0.6182372570037842, 'global_step': 21494, 'preemption_count': 0}), (23301, {'train/ctc_loss': Array(0.5520931, dtype=float32), 'train/wer': 0.17591464437017248, 'validation/ctc_loss': Array(0.67959964, dtype=float32), 'validation/wer': 0.1954101779352559, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4096471, dtype=float32), 'test/wer': 0.13155810127353604, 'test/num_examples': 2472, 'score': 18741.65048289299, 'total_duration': 20693.58714914322, 'accumulated_submission_time': 18741.65048289299, 'accumulated_eval_time': 1950.2114119529724, 'accumulated_logging_time': 0.6731538772583008, 'global_step': 23301, 'preemption_count': 0}), (25116, {'train/ctc_loss': Array(0.33693334, dtype=float32), 'train/wer': 0.11385134230882471, 'validation/ctc_loss': Array(0.645735, dtype=float32), 'validation/wer': 0.1852052096507912, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3809952, dtype=float32), 'test/wer': 0.12373814311539008, 'test/num_examples': 2472, 'score': 20182.07522916794, 'total_duration': 22277.928158044815, 'accumulated_submission_time': 20182.07522916794, 'accumulated_eval_time': 2093.9861640930176, 'accumulated_logging_time': 0.7318322658538818, 'global_step': 25116, 'preemption_count': 0}), (26912, {'train/ctc_loss': Array(0.31949133, dtype=float32), 'train/wer': 0.10684211070125951, 'validation/ctc_loss': Array(0.6205764, dtype=float32), 'validation/wer': 0.180821997161532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3652431, dtype=float32), 'test/wer': 0.11914772611866024, 'test/num_examples': 2472, 'score': 21621.946141719818, 'total_duration': 23853.673711776733, 'accumulated_submission_time': 21621.946141719818, 'accumulated_eval_time': 2229.720189809799, 'accumulated_logging_time': 0.790858268737793, 'global_step': 26912, 'preemption_count': 0}), (28704, {'train/ctc_loss': Array(0.30595523, dtype=float32), 'train/wer': 0.10328690339101583, 'validation/ctc_loss': Array(0.6050423, dtype=float32), 'validation/wer': 0.17387064695830157, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3550856, dtype=float32), 'test/wer': 0.11575569232019174, 'test/num_examples': 2472, 'score': 23062.360239744186, 'total_duration': 25429.651166677475, 'accumulated_submission_time': 23062.360239744186, 'accumulated_eval_time': 2365.1423330307007, 'accumulated_logging_time': 0.8519728183746338, 'global_step': 28704, 'preemption_count': 0}), (30507, {'train/ctc_loss': Array(0.2792333, dtype=float32), 'train/wer': 0.09602524056099453, 'validation/ctc_loss': Array(0.5789832, dtype=float32), 'validation/wer': 0.16743099336725334, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33707365, dtype=float32), 'test/wer': 0.10915442893993865, 'test/num_examples': 2472, 'score': 24503.368768692017, 'total_duration': 27006.50775051117, 'accumulated_submission_time': 24503.368768692017, 'accumulated_eval_time': 2500.8551738262177, 'accumulated_logging_time': 0.9049315452575684, 'global_step': 30507, 'preemption_count': 0}), (32326, {'train/ctc_loss': Array(0.26147306, dtype=float32), 'train/wer': 0.08957165092428733, 'validation/ctc_loss': Array(0.5437209, dtype=float32), 'validation/wer': 0.15865491373567492, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31017295, dtype=float32), 'test/wer': 0.0994658054556903, 'test/num_examples': 2472, 'score': 25943.38379716873, 'total_duration': 28581.357215881348, 'accumulated_submission_time': 25943.38379716873, 'accumulated_eval_time': 2635.5505130290985, 'accumulated_logging_time': 0.9616458415985107, 'global_step': 32326, 'preemption_count': 0}), (34115, {'train/ctc_loss': Array(0.23398308, dtype=float32), 'train/wer': 0.08175391055414674, 'validation/ctc_loss': Array(0.51874906, dtype=float32), 'validation/wer': 0.15006227251223728, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29474744, dtype=float32), 'test/wer': 0.09528162005159141, 'test/num_examples': 2472, 'score': 27383.673309326172, 'total_duration': 30157.786672353745, 'accumulated_submission_time': 27383.673309326172, 'accumulated_eval_time': 2771.551521062851, 'accumulated_logging_time': 1.019352912902832, 'global_step': 34115, 'preemption_count': 0}), (35915, {'train/ctc_loss': Array(0.25606412, dtype=float32), 'train/wer': 0.08354766267733148, 'validation/ctc_loss': Array(0.49699828, dtype=float32), 'validation/wer': 0.14366123753342924, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27777967, dtype=float32), 'test/wer': 0.09046777567891455, 'test/num_examples': 2472, 'score': 28824.013179302216, 'total_duration': 31734.248168230057, 'accumulated_submission_time': 28824.013179302216, 'accumulated_eval_time': 2907.5363302230835, 'accumulated_logging_time': 1.075251579284668, 'global_step': 35915, 'preemption_count': 0}), (37721, {'train/ctc_loss': Array(0.20800158, dtype=float32), 'train/wer': 0.07206750787644665, 'validation/ctc_loss': Array(0.47449413, dtype=float32), 'validation/wer': 0.13778155381986348, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26118982, dtype=float32), 'test/wer': 0.08331809964860967, 'test/num_examples': 2472, 'score': 30264.20079755783, 'total_duration': 33311.580392599106, 'accumulated_submission_time': 30264.20079755783, 'accumulated_eval_time': 3044.5422701835632, 'accumulated_logging_time': 1.1326208114624023, 'global_step': 37721, 'preemption_count': 0}), (39546, {'train/ctc_loss': Array(0.17795631, dtype=float32), 'train/wer': 0.06375735066542866, 'validation/ctc_loss': Array(0.45436054, dtype=float32), 'validation/wer': 0.1326259690858009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24431218, dtype=float32), 'test/wer': 0.07939796477972091, 'test/num_examples': 2472, 'score': 31704.79609966278, 'total_duration': 34888.67411828041, 'accumulated_submission_time': 31704.79609966278, 'accumulated_eval_time': 3180.9032578468323, 'accumulated_logging_time': 1.188124656677246, 'global_step': 39546, 'preemption_count': 0}), (41345, {'train/ctc_loss': Array(0.16376394, dtype=float32), 'train/wer': 0.05626279001780636, 'validation/ctc_loss': Array(0.43039808, dtype=float32), 'validation/wer': 0.1254139432499493, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23327044, dtype=float32), 'test/wer': 0.07456380882741251, 'test/num_examples': 2472, 'score': 33145.08502173424, 'total_duration': 36463.99954032898, 'accumulated_submission_time': 33145.08502173424, 'accumulated_eval_time': 3315.796919107437, 'accumulated_logging_time': 1.2503759860992432, 'global_step': 41345, 'preemption_count': 0}), (43139, {'train/ctc_loss': Array(0.14803201, dtype=float32), 'train/wer': 0.05255167958656331, 'validation/ctc_loss': Array(0.41873074, dtype=float32), 'validation/wer': 0.1214844994545121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22326158, dtype=float32), 'test/wer': 0.07161862978083805, 'test/num_examples': 2472, 'score': 34585.27427482605, 'total_duration': 38039.684517383575, 'accumulated_submission_time': 34585.27427482605, 'accumulated_eval_time': 3451.147789478302, 'accumulated_logging_time': 1.3133165836334229, 'global_step': 43139, 'preemption_count': 0}), (44943, {'train/ctc_loss': Array(0.16079587, dtype=float32), 'train/wer': 0.05456287072173304, 'validation/ctc_loss': Array(0.4065272, dtype=float32), 'validation/wer': 0.11763229288355523, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2159714, dtype=float32), 'test/wer': 0.06940466760099932, 'test/num_examples': 2472, 'score': 36025.31555771828, 'total_duration': 39616.17360472679, 'accumulated_submission_time': 36025.31555771828, 'accumulated_eval_time': 3587.4509439468384, 'accumulated_logging_time': 1.3760840892791748, 'global_step': 44943, 'preemption_count': 0}), (46770, {'train/ctc_loss': Array(0.15611503, dtype=float32), 'train/wer': 0.052918968159104435, 'validation/ctc_loss': Array(0.40304145, dtype=float32), 'validation/wer': 0.11674406480203134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2135498, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 37465.57762217522, 'total_duration': 41192.976047992706, 'accumulated_submission_time': 37465.57762217522, 'accumulated_eval_time': 3723.850387096405, 'accumulated_logging_time': 1.433948278427124, 'global_step': 46770, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.147639, dtype=float32), 'train/wer': 0.05145846511193836, 'validation/ctc_loss': Array(0.40327126, dtype=float32), 'validation/wer': 0.11670544618979117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21367131, dtype=float32), 'test/wer': 0.06851095809721122, 'test/num_examples': 2472, 'score': 38433.03055810928, 'total_duration': 42298.19808506966, 'accumulated_submission_time': 38433.03055810928, 'accumulated_eval_time': 3861.5062580108643, 'accumulated_logging_time': 1.4898200035095215, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 02:54:02.010565 140437690197824 submission_runner.py:586] Timing: 38433.03055810928
I0213 02:54:02.010619 140437690197824 submission_runner.py:588] Total number of evals: 28
I0213 02:54:02.010666 140437690197824 submission_runner.py:589] ====================
I0213 02:54:02.010742 140437690197824 submission_runner.py:542] Using RNG seed 2216178884
I0213 02:54:02.014295 140437690197824 submission_runner.py:551] --- Tuning run 5/5 ---
I0213 02:54:02.014436 140437690197824 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_5.
I0213 02:54:02.016225 140437690197824 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_5/hparams.json.
I0213 02:54:02.018802 140437690197824 submission_runner.py:206] Initializing dataset.
I0213 02:54:02.018929 140437690197824 submission_runner.py:213] Initializing model.
I0213 02:54:03.227623 140437690197824 submission_runner.py:255] Initializing optimizer.
I0213 02:54:03.369740 140437690197824 submission_runner.py:262] Initializing metrics bundle.
I0213 02:54:03.369936 140437690197824 submission_runner.py:280] Initializing checkpoint and logger.
I0213 02:54:03.373593 140437690197824 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_5 with prefix checkpoint_
I0213 02:54:03.373739 140437690197824 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_5/meta_data_0.json.
I0213 02:54:03.374173 140437690197824 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0213 02:54:03.374319 140437690197824 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0213 02:54:04.176551 140437690197824 logger_utils.py:220] Unable to record git information. Continuing without it.
I0213 02:54:04.950781 140437690197824 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_5/flags_0.json.
I0213 02:54:04.968832 140437690197824 submission_runner.py:314] Starting training loop.
I0213 02:54:04.972736 140437690197824 input_pipeline.py:20] Loading split = train-clean-100
I0213 02:54:05.020814 140437690197824 input_pipeline.py:20] Loading split = train-clean-360
I0213 02:54:05.663827 140437690197824 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0213 02:54:21.702177 140280737031936 logging_writer.py:48] [0] global_step=0, grad_norm=20.48473358154297, loss=33.180511474609375
I0213 02:54:21.715385 140437690197824 spec.py:321] Evaluating on the training split.
I0213 02:56:09.786735 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 02:57:12.414120 140437690197824 spec.py:349] Evaluating on the test split.
I0213 02:57:45.125875 140437690197824 submission_runner.py:408] Time since start: 220.15s, 	Step: 1, 	{'train/ctc_loss': Array(31.380108, dtype=float32), 'train/wer': 3.862109067393991, 'validation/ctc_loss': Array(30.891895, dtype=float32), 'validation/wer': 3.324869420817363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942286, dtype=float32), 'test/wer': 3.698819897223407, 'test/num_examples': 2472, 'score': 16.74648356437683, 'total_duration': 220.1540470123291, 'accumulated_submission_time': 16.74648356437683, 'accumulated_eval_time': 203.40749049186707, 'accumulated_logging_time': 0}
I0213 02:57:45.144961 140280856385280 logging_writer.py:48] [1] accumulated_eval_time=203.407490, accumulated_logging_time=0, accumulated_submission_time=16.746484, global_step=1, preemption_count=0, score=16.746484, test/ctc_loss=30.942285537719727, test/num_examples=2472, test/wer=3.698820, total_duration=220.154047, train/ctc_loss=31.380107879638672, train/wer=3.862109, validation/ctc_loss=30.891895294189453, validation/num_examples=5348, validation/wer=3.324869
I0213 02:59:10.549318 140280737031936 logging_writer.py:48] [100] global_step=100, grad_norm=0.7421338558197021, loss=5.983426570892334
I0213 03:00:26.988794 140280745424640 logging_writer.py:48] [200] global_step=200, grad_norm=0.313499391078949, loss=5.8143815994262695
I0213 03:01:43.358952 140280737031936 logging_writer.py:48] [300] global_step=300, grad_norm=1.216873288154602, loss=5.6298112869262695
I0213 03:02:59.630346 140280745424640 logging_writer.py:48] [400] global_step=400, grad_norm=1.3223073482513428, loss=5.120920181274414
I0213 03:04:15.830434 140280737031936 logging_writer.py:48] [500] global_step=500, grad_norm=1.4219549894332886, loss=4.349756240844727
I0213 03:05:31.871417 140280745424640 logging_writer.py:48] [600] global_step=600, grad_norm=1.5679104328155518, loss=3.7516403198242188
I0213 03:06:51.900000 140280737031936 logging_writer.py:48] [700] global_step=700, grad_norm=2.0190203189849854, loss=3.4216132164001465
I0213 03:08:16.771568 140280745424640 logging_writer.py:48] [800] global_step=800, grad_norm=2.1610629558563232, loss=3.0890331268310547
I0213 03:09:41.696294 140280737031936 logging_writer.py:48] [900] global_step=900, grad_norm=3.2241172790527344, loss=3.050762176513672
I0213 03:11:06.221282 140280745424640 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.4136972427368164, loss=2.8234121799468994
I0213 03:12:27.419295 140280856385280 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.961841106414795, loss=2.716892957687378
I0213 03:13:42.229532 140280847992576 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.8294408321380615, loss=2.589650869369507
I0213 03:14:57.293162 140280856385280 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.803769826889038, loss=2.5187230110168457
I0213 03:16:12.207315 140280847992576 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.0711798667907715, loss=2.4661734104156494
I0213 03:17:29.704725 140280856385280 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.2929112911224365, loss=2.361210823059082
I0213 03:18:54.585755 140280847992576 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.8432629108428955, loss=2.364246368408203
I0213 03:20:19.811301 140280856385280 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.2799909114837646, loss=2.3476741313934326
I0213 03:21:44.253435 140280847992576 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.6625401973724365, loss=2.3193392753601074
I0213 03:21:45.832848 140437690197824 spec.py:321] Evaluating on the training split.
I0213 03:22:33.474316 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 03:23:21.918331 140437690197824 spec.py:349] Evaluating on the test split.
I0213 03:23:46.991943 140437690197824 submission_runner.py:408] Time since start: 1782.02s, 	Step: 1803, 	{'train/ctc_loss': Array(3.9911628, dtype=float32), 'train/wer': 0.7952104345989333, 'validation/ctc_loss': Array(4.117805, dtype=float32), 'validation/wer': 0.7905519565154426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.7096946, dtype=float32), 'test/wer': 0.7506753600227489, 'test/num_examples': 2472, 'score': 1457.3477289676666, 'total_duration': 1782.0162966251373, 'accumulated_submission_time': 1457.3477289676666, 'accumulated_eval_time': 324.55981945991516, 'accumulated_logging_time': 0.031755924224853516}
I0213 03:23:47.029830 140280856385280 logging_writer.py:48] [1803] accumulated_eval_time=324.559819, accumulated_logging_time=0.031756, accumulated_submission_time=1457.347729, global_step=1803, preemption_count=0, score=1457.347729, test/ctc_loss=3.7096946239471436, test/num_examples=2472, test/wer=0.750675, total_duration=1782.016297, train/ctc_loss=3.9911627769470215, train/wer=0.795210, validation/ctc_loss=4.117805004119873, validation/num_examples=5348, validation/wer=0.790552
I0213 03:25:00.507046 140280847992576 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.886530876159668, loss=2.279789447784424
I0213 03:26:15.670608 140280856385280 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.883068561553955, loss=2.2672808170318604
I0213 03:27:34.530728 140280856385280 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.244476079940796, loss=2.1668879985809326
I0213 03:28:49.953136 140280847992576 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.3636233806610107, loss=2.1775896549224854
I0213 03:30:05.018725 140280856385280 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.403247833251953, loss=2.141169309616089
I0213 03:31:19.967786 140280847992576 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.4775631427764893, loss=2.157580614089966
I0213 03:32:37.172367 140280856385280 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.163160562515259, loss=2.012538194656372
I0213 03:34:01.067882 140280847992576 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.897140026092529, loss=2.0674145221710205
I0213 03:35:25.313823 140280856385280 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.9258646965026855, loss=2.0390379428863525
I0213 03:36:49.614747 140280847992576 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.483426332473755, loss=1.9755131006240845
I0213 03:38:13.173754 140280856385280 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.6924476623535156, loss=1.9870903491973877
I0213 03:39:38.204065 140280847992576 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.6437337398529053, loss=2.007395029067993
I0213 03:41:06.419203 140280856385280 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.8935348987579346, loss=1.9859551191329956
I0213 03:42:21.496014 140280847992576 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.0869812965393066, loss=1.9112776517868042
I0213 03:43:36.455687 140280856385280 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.6231789588928223, loss=1.9507485628128052
I0213 03:44:51.448517 140280847992576 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.116560459136963, loss=1.9084739685058594
I0213 03:46:06.502750 140280856385280 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.6150047779083252, loss=1.9393080472946167
I0213 03:47:28.446686 140280847992576 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.7823729515075684, loss=1.9490574598312378
I0213 03:47:47.293122 140437690197824 spec.py:321] Evaluating on the training split.
I0213 03:48:46.430486 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 03:49:38.937077 140437690197824 spec.py:349] Evaluating on the test split.
I0213 03:50:05.324691 140437690197824 submission_runner.py:408] Time since start: 3360.35s, 	Step: 3624, 	{'train/ctc_loss': Array(0.5607584, dtype=float32), 'train/wer': 0.18567327632882813, 'validation/ctc_loss': Array(0.92604756, dtype=float32), 'validation/wer': 0.2639678693146162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.59962285, dtype=float32), 'test/wer': 0.19145694960697093, 'test/num_examples': 2472, 'score': 2897.5168826580048, 'total_duration': 3360.348862886429, 'accumulated_submission_time': 2897.5168826580048, 'accumulated_eval_time': 462.58443236351013, 'accumulated_logging_time': 0.08944153785705566}
I0213 03:50:05.364284 140280856385280 logging_writer.py:48] [3624] accumulated_eval_time=462.584432, accumulated_logging_time=0.089442, accumulated_submission_time=2897.516883, global_step=3624, preemption_count=0, score=2897.516883, test/ctc_loss=0.5996228456497192, test/num_examples=2472, test/wer=0.191457, total_duration=3360.348863, train/ctc_loss=0.5607584118843079, train/wer=0.185673, validation/ctc_loss=0.9260475635528564, validation/num_examples=5348, validation/wer=0.263968
I0213 03:51:03.252028 140280847992576 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.4302635192871094, loss=1.9222825765609741
I0213 03:52:18.400939 140280856385280 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.7222765684127808, loss=1.8209114074707031
I0213 03:53:33.829429 140280847992576 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.048614740371704, loss=1.851781964302063
I0213 03:54:50.961757 140280856385280 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.1441540718078613, loss=1.8710929155349731
I0213 03:56:13.869970 140280847992576 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.9942562580108643, loss=1.8273566961288452
I0213 03:57:33.841398 140280856385280 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.50215482711792, loss=1.8555206060409546
I0213 03:58:49.092317 140280847992576 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.6245529651641846, loss=1.8424584865570068
I0213 04:00:03.899742 140280856385280 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.2767655849456787, loss=1.8539667129516602
I0213 04:01:18.736473 140280847992576 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.117933750152588, loss=1.852117657661438
I0213 04:02:36.388062 140280856385280 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.9324779510498047, loss=1.8701841831207275
I0213 04:03:59.240531 140280847992576 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.949484348297119, loss=1.7680035829544067
I0213 04:05:24.243756 140280856385280 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.1669816970825195, loss=1.7684941291809082
I0213 04:06:48.431676 140280847992576 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.8423430919647217, loss=1.699866771697998
I0213 04:08:12.116889 140280856385280 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.1230576038360596, loss=1.8089263439178467
I0213 04:09:36.536484 140280847992576 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.623867154121399, loss=1.7453041076660156
I0213 04:11:00.408688 140280856385280 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.2856879234313965, loss=1.7287806272506714
I0213 04:12:15.404199 140280847992576 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.9826093912124634, loss=1.7482527494430542
I0213 04:13:30.496395 140280856385280 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.37204647064209, loss=1.7556309700012207
I0213 04:14:05.507942 140437690197824 spec.py:321] Evaluating on the training split.
I0213 04:15:02.041220 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 04:15:54.384440 140437690197824 spec.py:349] Evaluating on the test split.
I0213 04:16:21.639078 140437690197824 submission_runner.py:408] Time since start: 4936.66s, 	Step: 5448, 	{'train/ctc_loss': Array(0.46810666, dtype=float32), 'train/wer': 0.1586667026822614, 'validation/ctc_loss': Array(0.79226905, dtype=float32), 'validation/wer': 0.23071724417583053, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.493212, dtype=float32), 'test/wer': 0.15786159689639062, 'test/num_examples': 2472, 'score': 4337.565103292465, 'total_duration': 4936.6641318798065, 'accumulated_submission_time': 4337.565103292465, 'accumulated_eval_time': 598.7094995975494, 'accumulated_logging_time': 0.14923548698425293}
I0213 04:16:21.671779 140280856385280 logging_writer.py:48] [5448] accumulated_eval_time=598.709500, accumulated_logging_time=0.149235, accumulated_submission_time=4337.565103, global_step=5448, preemption_count=0, score=4337.565103, test/ctc_loss=0.4932120144367218, test/num_examples=2472, test/wer=0.157862, total_duration=4936.664132, train/ctc_loss=0.4681066572666168, train/wer=0.158667, validation/ctc_loss=0.7922690510749817, validation/num_examples=5348, validation/wer=0.230717
I0213 04:17:01.407750 140280847992576 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.263733148574829, loss=1.7373216152191162
I0213 04:18:16.570191 140280856385280 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.4813995361328125, loss=1.7088967561721802
I0213 04:19:31.750363 140280847992576 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.2396135330200195, loss=1.7360872030258179
I0213 04:20:48.263573 140280856385280 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.4064276218414307, loss=1.7973839044570923
I0213 04:22:12.310889 140280847992576 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.4702818393707275, loss=1.6778523921966553
I0213 04:23:36.521808 140280856385280 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.3211846351623535, loss=1.660103440284729
I0213 04:24:59.905902 140280847992576 logging_writer.py:48] [6100] global_step=6100, grad_norm=4.826280117034912, loss=1.6988310813903809
I0213 04:26:26.583448 140280856385280 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.6297404766082764, loss=1.6607755422592163
I0213 04:27:41.596723 140280847992576 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.7572813034057617, loss=1.6555906534194946
I0213 04:28:57.077386 140280856385280 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.458024263381958, loss=1.6703780889511108
I0213 04:30:11.922854 140280847992576 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.432936668395996, loss=1.707194209098816
I0213 04:31:27.041071 140280856385280 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.1095542907714844, loss=1.684596061706543
I0213 04:32:49.131847 140280847992576 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.6276018619537354, loss=1.725256323814392
I0213 04:34:14.605353 140280856385280 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.017439603805542, loss=1.7607723474502563
I0213 04:35:40.797893 140280847992576 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.9617644548416138, loss=1.6370692253112793
I0213 04:37:06.609319 140280856385280 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.948967456817627, loss=1.684501051902771
I0213 04:38:31.697111 140280847992576 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.8764755725860596, loss=1.6904886960983276
I0213 04:39:55.803258 140280856385280 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.487846612930298, loss=1.6961003541946411
I0213 04:40:21.783136 140437690197824 spec.py:321] Evaluating on the training split.
I0213 04:41:19.794800 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 04:42:12.654731 140437690197824 spec.py:349] Evaluating on the test split.
I0213 04:42:39.080817 140437690197824 submission_runner.py:408] Time since start: 6514.11s, 	Step: 7230, 	{'train/ctc_loss': Array(0.40930718, dtype=float32), 'train/wer': 0.13781296003709956, 'validation/ctc_loss': Array(0.7724912, dtype=float32), 'validation/wer': 0.2218735819728318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47449654, dtype=float32), 'test/wer': 0.15371803465155484, 'test/num_examples': 2472, 'score': 5777.571934938431, 'total_duration': 6514.106091976166, 'accumulated_submission_time': 5777.571934938431, 'accumulated_eval_time': 736.0015232563019, 'accumulated_logging_time': 0.21187639236450195}
I0213 04:42:39.114560 140280856385280 logging_writer.py:48] [7230] accumulated_eval_time=736.001523, accumulated_logging_time=0.211876, accumulated_submission_time=5777.571935, global_step=7230, preemption_count=0, score=5777.571935, test/ctc_loss=0.4744965434074402, test/num_examples=2472, test/wer=0.153718, total_duration=6514.106092, train/ctc_loss=0.40930718183517456, train/wer=0.137813, validation/ctc_loss=0.7724912166595459, validation/num_examples=5348, validation/wer=0.221874
I0213 04:43:32.296905 140280847992576 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.623934507369995, loss=1.66034996509552
I0213 04:44:47.655649 140280856385280 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.102375030517578, loss=1.6462880373001099
I0213 04:46:02.794770 140280847992576 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.394176483154297, loss=1.6848036050796509
I0213 04:47:18.083058 140280856385280 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.6547224521636963, loss=1.6596927642822266
I0213 04:48:33.147956 140280847992576 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.9297852516174316, loss=1.678518533706665
I0213 04:49:49.157781 140280856385280 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.7488253116607666, loss=1.7039309740066528
I0213 04:51:13.716203 140280847992576 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.169715404510498, loss=1.6783983707427979
I0213 04:52:37.780706 140280856385280 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.0410678386688232, loss=1.6423360109329224
I0213 04:54:02.369470 140280847992576 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.015056133270264, loss=1.6410349607467651
I0213 04:55:26.626042 140280856385280 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.5925337076187134, loss=1.59450101852417
I0213 04:56:49.194294 140280856385280 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.7834339141845703, loss=1.602820634841919
I0213 04:58:04.133320 140280847992576 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.8319904804229736, loss=1.6765180826187134
I0213 04:59:19.314308 140280856385280 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.666954517364502, loss=1.6176598072052002
I0213 05:00:34.492224 140280847992576 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.4418599605560303, loss=1.7106021642684937
I0213 05:01:52.032559 140280856385280 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.4386439323425293, loss=1.688127040863037
I0213 05:03:16.810577 140280847992576 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.8529396057128906, loss=1.604090929031372
I0213 05:04:41.922128 140280856385280 logging_writer.py:48] [8900] global_step=8900, grad_norm=5.088569164276123, loss=1.5476049184799194
I0213 05:06:07.608011 140280847992576 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.8790438175201416, loss=1.6763254404067993
I0213 05:06:40.099184 140437690197824 spec.py:321] Evaluating on the training split.
I0213 05:07:38.578887 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 05:08:31.300013 140437690197824 spec.py:349] Evaluating on the test split.
I0213 05:08:57.837857 140437690197824 submission_runner.py:408] Time since start: 8092.86s, 	Step: 9039, 	{'train/ctc_loss': Array(0.39555684, dtype=float32), 'train/wer': 0.13521204284994728, 'validation/ctc_loss': Array(0.7158556, dtype=float32), 'validation/wer': 0.20704403487260686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42413357, dtype=float32), 'test/wer': 0.13653443828326528, 'test/num_examples': 2472, 'score': 7218.460584163666, 'total_duration': 8092.8622970581055, 'accumulated_submission_time': 7218.460584163666, 'accumulated_eval_time': 873.7335147857666, 'accumulated_logging_time': 0.26636743545532227}
I0213 05:08:57.879033 140280856385280 logging_writer.py:48] [9039] accumulated_eval_time=873.733515, accumulated_logging_time=0.266367, accumulated_submission_time=7218.460584, global_step=9039, preemption_count=0, score=7218.460584, test/ctc_loss=0.4241335690021515, test/num_examples=2472, test/wer=0.136534, total_duration=8092.862297, train/ctc_loss=0.39555683732032776, train/wer=0.135212, validation/ctc_loss=0.715855598449707, validation/num_examples=5348, validation/wer=0.207044
I0213 05:09:44.596036 140280847992576 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.9375030994415283, loss=1.711432933807373
I0213 05:10:59.899765 140280856385280 logging_writer.py:48] [9200] global_step=9200, grad_norm=5.120303630828857, loss=1.631774663925171
I0213 05:12:19.839204 140280856385280 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.4055356979370117, loss=1.6098064184188843
I0213 05:13:34.981971 140280847992576 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.645508050918579, loss=1.675625205039978
I0213 05:14:50.169416 140280856385280 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.735189437866211, loss=1.6059095859527588
I0213 05:16:05.554856 140280847992576 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.3249998092651367, loss=1.6001471281051636
I0213 05:17:22.418494 140280856385280 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.4795587062835693, loss=1.5887770652770996
I0213 05:18:47.036238 140280847992576 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.670123815536499, loss=1.7111016511917114
I0213 05:20:11.055156 140280856385280 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.4844608306884766, loss=1.584964632987976
I0213 05:21:35.208079 140280847992576 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.8771820068359375, loss=1.634095311164856
I0213 05:22:59.236866 140280856385280 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.210787296295166, loss=1.5429083108901978
I0213 05:24:23.613576 140280847992576 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.8558199405670166, loss=1.584594488143921
I0213 05:25:51.767960 140280856385280 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.5963985919952393, loss=1.5835494995117188
I0213 05:27:07.121201 140280847992576 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.371849775314331, loss=1.5309194326400757
I0213 05:28:22.271978 140280856385280 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.831416130065918, loss=1.6079403162002563
I0213 05:29:37.293431 140280847992576 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.2849133014678955, loss=1.603284478187561
I0213 05:30:53.581733 140280856385280 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.092313289642334, loss=1.6355208158493042
I0213 05:32:17.361804 140280847992576 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.085448980331421, loss=1.5547279119491577
I0213 05:32:58.076565 140437690197824 spec.py:321] Evaluating on the training split.
I0213 05:33:56.825556 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 05:34:48.941798 140437690197824 spec.py:349] Evaluating on the test split.
I0213 05:35:15.716436 140437690197824 submission_runner.py:408] Time since start: 9670.74s, 	Step: 10850, 	{'train/ctc_loss': Array(0.39360082, dtype=float32), 'train/wer': 0.12606838743305127, 'validation/ctc_loss': Array(0.68587214, dtype=float32), 'validation/wer': 0.1970128503432229, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40153503, dtype=float32), 'test/wer': 0.13015660227895923, 'test/num_examples': 2472, 'score': 8658.568603992462, 'total_duration': 9670.741458892822, 'accumulated_submission_time': 8658.568603992462, 'accumulated_eval_time': 1011.367288351059, 'accumulated_logging_time': 0.3219921588897705}
I0213 05:35:15.754542 140280856385280 logging_writer.py:48] [10850] accumulated_eval_time=1011.367288, accumulated_logging_time=0.321992, accumulated_submission_time=8658.568604, global_step=10850, preemption_count=0, score=8658.568604, test/ctc_loss=0.4015350341796875, test/num_examples=2472, test/wer=0.130157, total_duration=9670.741459, train/ctc_loss=0.39360082149505615, train/wer=0.126068, validation/ctc_loss=0.6858721375465393, validation/num_examples=5348, validation/wer=0.197013
I0213 05:35:54.147551 140280847992576 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.2374300956726074, loss=1.6339291334152222
I0213 05:37:09.189347 140280856385280 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.749056339263916, loss=1.6056686639785767
I0213 05:38:24.357503 140280847992576 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.05869197845459, loss=1.6007513999938965
I0213 05:39:40.683083 140280856385280 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.770240068435669, loss=1.5279446840286255
I0213 05:41:04.457376 140280847992576 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.394212245941162, loss=1.614983081817627
I0213 05:42:26.478270 140280856385280 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.146020412445068, loss=1.5347179174423218
I0213 05:43:41.608076 140280847992576 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.8892714977264404, loss=1.559150218963623
I0213 05:44:56.830656 140280856385280 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.761070966720581, loss=1.5501885414123535
I0213 05:46:11.674587 140280847992576 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.312319278717041, loss=1.6545883417129517
I0213 05:47:30.296837 140280856385280 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.8104166984558105, loss=1.60206937789917
I0213 05:48:54.964558 140280847992576 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.337468147277832, loss=1.5453860759735107
I0213 05:50:19.055705 140280856385280 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.283036231994629, loss=1.5584181547164917
I0213 05:51:43.907524 140280847992576 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.7636077404022217, loss=1.5394853353500366
I0213 05:53:09.000172 140280856385280 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.990751028060913, loss=1.6159576177597046
I0213 05:54:34.323415 140280847992576 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.693883419036865, loss=1.6395760774612427
I0213 05:55:58.947300 140280856385280 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.713618516921997, loss=1.5754529237747192
I0213 05:57:14.027675 140280847992576 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.246223211288452, loss=1.5377336740493774
I0213 05:58:29.058625 140280856385280 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.2066526412963867, loss=1.552154541015625
I0213 05:59:16.302012 140437690197824 spec.py:321] Evaluating on the training split.
I0213 06:00:15.147888 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 06:01:07.234735 140437690197824 spec.py:349] Evaluating on the test split.
I0213 06:01:34.022789 140437690197824 submission_runner.py:408] Time since start: 11249.05s, 	Step: 12664, 	{'train/ctc_loss': Array(0.30456778, dtype=float32), 'train/wer': 0.10658186522114081, 'validation/ctc_loss': Array(0.66065574, dtype=float32), 'validation/wer': 0.1916158992826593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38939542, dtype=float32), 'test/wer': 0.12715048849349014, 'test/num_examples': 2472, 'score': 10099.024131059647, 'total_duration': 11249.047943592072, 'accumulated_submission_time': 10099.024131059647, 'accumulated_eval_time': 1149.0821301937103, 'accumulated_logging_time': 0.3768749237060547}
I0213 06:01:34.057005 140280856385280 logging_writer.py:48] [12664] accumulated_eval_time=1149.082130, accumulated_logging_time=0.376875, accumulated_submission_time=10099.024131, global_step=12664, preemption_count=0, score=10099.024131, test/ctc_loss=0.38939541578292847, test/num_examples=2472, test/wer=0.127150, total_duration=11249.047944, train/ctc_loss=0.30456778407096863, train/wer=0.106582, validation/ctc_loss=0.6606557369232178, validation/num_examples=5348, validation/wer=0.191616
I0213 06:02:01.862339 140280847992576 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.4963314533233643, loss=1.5559157133102417
I0213 06:03:16.884128 140280856385280 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.567302942276001, loss=1.6351510286331177
I0213 06:04:31.928947 140280847992576 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.7367799282073975, loss=1.5770561695098877
I0213 06:05:48.118838 140280856385280 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.9820289611816406, loss=1.5504611730575562
I0213 06:07:11.567417 140280847992576 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.2482025623321533, loss=1.501710295677185
I0213 06:08:35.876877 140280856385280 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.9714226722717285, loss=1.5704270601272583
I0213 06:09:59.248025 140280847992576 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.5520899295806885, loss=1.556405782699585
I0213 06:11:26.885594 140280856385280 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.7931718826293945, loss=1.560250997543335
I0213 06:12:41.783785 140280847992576 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.687378406524658, loss=1.5539679527282715
I0213 06:13:56.855212 140280856385280 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.3085274696350098, loss=1.5724201202392578
I0213 06:15:11.857474 140280847992576 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.4236276149749756, loss=1.4687093496322632
I0213 06:16:26.593963 140280856385280 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.9649534225463867, loss=1.4536018371582031
I0213 06:17:46.887267 140280847992576 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.4902114868164062, loss=1.5556294918060303
I0213 06:19:11.622329 140280856385280 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.7686758041381836, loss=1.597171425819397
I0213 06:20:35.861400 140280847992576 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.537937164306641, loss=1.5164636373519897
I0213 06:22:00.480497 140280856385280 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.421799421310425, loss=1.5826789140701294
I0213 06:23:25.404603 140280847992576 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.0410070419311523, loss=1.5296273231506348
I0213 06:24:50.288344 140280856385280 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.1058926582336426, loss=1.5730732679367065
I0213 06:25:34.574158 140437690197824 spec.py:321] Evaluating on the training split.
I0213 06:26:33.329486 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 06:27:26.676154 140437690197824 spec.py:349] Evaluating on the test split.
I0213 06:27:53.825967 140437690197824 submission_runner.py:408] Time since start: 12828.85s, 	Step: 14453, 	{'train/ctc_loss': Array(0.29636636, dtype=float32), 'train/wer': 0.10252902500936291, 'validation/ctc_loss': Array(0.63199, dtype=float32), 'validation/wer': 0.181507477528795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37232095, dtype=float32), 'test/wer': 0.1210773261836573, 'test/num_examples': 2472, 'score': 11539.4461581707, 'total_duration': 12828.851173877716, 'accumulated_submission_time': 11539.4461581707, 'accumulated_eval_time': 1288.3281433582306, 'accumulated_logging_time': 0.43149304389953613}
I0213 06:27:53.856901 140280856385280 logging_writer.py:48] [14453] accumulated_eval_time=1288.328143, accumulated_logging_time=0.431493, accumulated_submission_time=11539.446158, global_step=14453, preemption_count=0, score=11539.446158, test/ctc_loss=0.3723209500312805, test/num_examples=2472, test/wer=0.121077, total_duration=12828.851174, train/ctc_loss=0.2963663637638092, train/wer=0.102529, validation/ctc_loss=0.6319900155067444, validation/num_examples=5348, validation/wer=0.181507
I0213 06:28:29.902000 140280847992576 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.930854320526123, loss=1.513952612876892
I0213 06:29:45.049767 140280856385280 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.1230931282043457, loss=1.5381940603256226
I0213 06:31:00.149739 140280847992576 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.712639093399048, loss=1.4771504402160645
I0213 06:32:15.038071 140280856385280 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.5284218788146973, loss=1.5265878438949585
I0213 06:33:30.256482 140280847992576 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.7178208827972412, loss=1.537361979484558
I0213 06:34:47.397418 140280856385280 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.487664222717285, loss=1.538953423500061
I0213 06:36:11.437443 140280847992576 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.280357837677002, loss=1.5741764307022095
I0213 06:37:36.592012 140280856385280 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.678060293197632, loss=1.53000009059906
I0213 06:39:00.267522 140280847992576 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.0252444744110107, loss=1.5487648248672485
I0213 06:40:25.533816 140280856385280 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.426159381866455, loss=1.5386333465576172
I0213 06:41:49.671360 140280856385280 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.9904453754425049, loss=1.5261174440383911
I0213 06:43:04.793339 140280847992576 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.1034984588623047, loss=1.5391963720321655
I0213 06:44:19.953333 140280856385280 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.4457151889801025, loss=1.5509164333343506
I0213 06:45:34.899368 140280847992576 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.2624101638793945, loss=1.5115883350372314
I0213 06:46:52.146342 140280856385280 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.497704267501831, loss=1.4783716201782227
I0213 06:48:16.556416 140280847992576 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.1500258445739746, loss=1.4993010759353638
I0213 06:49:41.039381 140280856385280 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.947705864906311, loss=1.448380708694458
I0213 06:51:05.858141 140280847992576 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.4484734535217285, loss=1.5024051666259766
I0213 06:51:54.957023 140437690197824 spec.py:321] Evaluating on the training split.
I0213 06:52:51.158859 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 06:53:44.076262 140437690197824 spec.py:349] Evaluating on the test split.
I0213 06:54:11.059717 140437690197824 submission_runner.py:408] Time since start: 14406.08s, 	Step: 16259, 	{'train/ctc_loss': Array(0.4332551, dtype=float32), 'train/wer': 0.14540633904309272, 'validation/ctc_loss': Array(0.6278105, dtype=float32), 'validation/wer': 0.18295567548780134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36600688, dtype=float32), 'test/wer': 0.11886336400381858, 'test/num_examples': 2472, 'score': 12980.456796169281, 'total_duration': 14406.084884166718, 'accumulated_submission_time': 12980.456796169281, 'accumulated_eval_time': 1424.4248707294464, 'accumulated_logging_time': 0.47845959663391113}
I0213 06:54:11.092219 140280856385280 logging_writer.py:48] [16259] accumulated_eval_time=1424.424871, accumulated_logging_time=0.478460, accumulated_submission_time=12980.456796, global_step=16259, preemption_count=0, score=12980.456796, test/ctc_loss=0.36600688099861145, test/num_examples=2472, test/wer=0.118863, total_duration=14406.084884, train/ctc_loss=0.4332551062107086, train/wer=0.145406, validation/ctc_loss=0.6278104782104492, validation/num_examples=5348, validation/wer=0.182956
I0213 06:54:42.602582 140280847992576 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.9445178508758545, loss=1.5561269521713257
I0213 06:55:57.875662 140280856385280 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.5814001560211182, loss=1.4474384784698486
I0213 06:57:17.459310 140280856385280 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.7600560188293457, loss=1.4817993640899658
I0213 06:58:32.396530 140280847992576 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.359947443008423, loss=1.5187897682189941
I0213 06:59:47.540340 140280856385280 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.7402589321136475, loss=1.4478036165237427
I0213 07:01:02.425489 140280847992576 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.7217319011688232, loss=1.5317696332931519
I0213 07:02:19.498361 140280856385280 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.836108684539795, loss=1.448189616203308
I0213 07:03:43.903669 140280847992576 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.635164499282837, loss=1.490532636642456
I0213 07:05:09.466760 140280856385280 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.6458637714385986, loss=1.411963701248169
I0213 07:06:34.139260 140280847992576 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.255067825317383, loss=1.3982568979263306
I0213 07:07:58.461865 140280856385280 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.8983148336410522, loss=1.4439479112625122
I0213 07:09:23.553951 140280847992576 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.767821788787842, loss=1.5742772817611694
I0213 07:10:49.037567 140280856385280 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.756018877029419, loss=1.5363953113555908
I0213 07:12:08.946089 140280856385280 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.526146411895752, loss=1.469935417175293
I0213 07:13:24.188349 140280847992576 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.3296902179718018, loss=1.4236198663711548
I0213 07:14:39.434784 140280856385280 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.3310303688049316, loss=1.5257112979888916
I0213 07:15:54.743833 140280847992576 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.1868765354156494, loss=1.5336540937423706
I0213 07:17:14.477561 140280856385280 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.016803741455078, loss=1.4413775205612183
I0213 07:18:11.484822 140437690197824 spec.py:321] Evaluating on the training split.
I0213 07:19:08.736559 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 07:20:01.859801 140437690197824 spec.py:349] Evaluating on the test split.
I0213 07:20:29.058788 140437690197824 submission_runner.py:408] Time since start: 15984.08s, 	Step: 18069, 	{'train/ctc_loss': Array(0.41025263, dtype=float32), 'train/wer': 0.13652797420221108, 'validation/ctc_loss': Array(0.59172356, dtype=float32), 'validation/wer': 0.17229693850951466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33499104, dtype=float32), 'test/wer': 0.1091341173603071, 'test/num_examples': 2472, 'score': 14420.758267641068, 'total_duration': 15984.083220481873, 'accumulated_submission_time': 14420.758267641068, 'accumulated_eval_time': 1561.9921565055847, 'accumulated_logging_time': 0.5266251564025879}
I0213 07:20:29.091533 140280856385280 logging_writer.py:48] [18069] accumulated_eval_time=1561.992157, accumulated_logging_time=0.526625, accumulated_submission_time=14420.758268, global_step=18069, preemption_count=0, score=14420.758268, test/ctc_loss=0.3349910378456116, test/num_examples=2472, test/wer=0.109134, total_duration=15984.083220, train/ctc_loss=0.4102526307106018, train/wer=0.136528, validation/ctc_loss=0.5917235612869263, validation/num_examples=5348, validation/wer=0.172297
I0213 07:20:53.205354 140280847992576 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.9702212810516357, loss=1.3913066387176514
I0213 07:22:08.333147 140280856385280 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.784217119216919, loss=1.4186984300613403
I0213 07:23:23.911732 140280847992576 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.971073031425476, loss=1.4713313579559326
I0213 07:24:39.115217 140280856385280 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.776698112487793, loss=1.4715826511383057
I0213 07:26:03.084319 140280847992576 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.937526702880859, loss=1.4421638250350952
I0213 07:27:25.899657 140280856385280 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.0166075229644775, loss=1.4692299365997314
I0213 07:28:41.210686 140280847992576 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.9725112915039062, loss=1.4542357921600342
I0213 07:29:56.319186 140280856385280 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.5208864212036133, loss=1.3854260444641113
I0213 07:31:11.202198 140280847992576 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.0971574783325195, loss=1.467791199684143
I0213 07:32:29.255501 140280856385280 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.584860324859619, loss=1.437281847000122
I0213 07:33:52.624217 140280847992576 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.367778778076172, loss=1.4360021352767944
I0213 07:35:16.470608 140280856385280 logging_writer.py:48] [19200] global_step=19200, grad_norm=4.759363174438477, loss=1.4234055280685425
I0213 07:36:40.855255 140280847992576 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.7758657932281494, loss=1.4758484363555908
I0213 07:38:04.915538 140280856385280 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.211293935775757, loss=1.4525794982910156
I0213 07:39:29.429455 140280847992576 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.5564565658569336, loss=1.4108792543411255
I0213 07:40:55.519082 140280856385280 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.5925612449645996, loss=1.4623849391937256
I0213 07:42:10.688633 140280847992576 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.286165714263916, loss=1.3994085788726807
I0213 07:43:25.706099 140280856385280 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.1565539836883545, loss=1.4936468601226807
I0213 07:44:29.244322 140437690197824 spec.py:321] Evaluating on the training split.
I0213 07:45:24.130039 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 07:46:16.720577 140437690197824 spec.py:349] Evaluating on the test split.
I0213 07:46:44.081305 140437690197824 submission_runner.py:408] Time since start: 17559.11s, 	Step: 19886, 	{'train/ctc_loss': Array(0.4899871, dtype=float32), 'train/wer': 0.15885338040272198, 'validation/ctc_loss': Array(0.57561785, dtype=float32), 'validation/wer': 0.16756615851009393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32893935, dtype=float32), 'test/wer': 0.10752950256941482, 'test/num_examples': 2472, 'score': 15860.820094823837, 'total_duration': 17559.10546183586, 'accumulated_submission_time': 15860.820094823837, 'accumulated_eval_time': 1696.8221898078918, 'accumulated_logging_time': 0.5745553970336914}
I0213 07:46:44.112547 140280856385280 logging_writer.py:48] [19886] accumulated_eval_time=1696.822190, accumulated_logging_time=0.574555, accumulated_submission_time=15860.820095, global_step=19886, preemption_count=0, score=15860.820095, test/ctc_loss=0.3289393484592438, test/num_examples=2472, test/wer=0.107530, total_duration=17559.105462, train/ctc_loss=0.4899871051311493, train/wer=0.158853, validation/ctc_loss=0.5756178498268127, validation/num_examples=5348, validation/wer=0.167566
I0213 07:46:55.479912 140280847992576 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.1022818088531494, loss=1.4503834247589111
I0213 07:48:10.651305 140280856385280 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.5608062744140625, loss=1.4695017337799072
I0213 07:49:25.736993 140280847992576 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9420045614242554, loss=1.472557544708252
I0213 07:50:40.672314 140280856385280 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.4107632637023926, loss=1.4462283849716187
I0213 07:52:04.867225 140280847992576 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.052617073059082, loss=1.4316576719284058
I0213 07:53:28.972391 140280856385280 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.8914318084716797, loss=1.4396072626113892
I0213 07:54:53.964411 140280847992576 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.7139930725097656, loss=1.488484501838684
I0213 07:56:22.526423 140280856385280 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.448800802230835, loss=1.421045184135437
I0213 07:57:37.584057 140280847992576 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.5313873291015625, loss=1.4053211212158203
I0213 07:58:52.619402 140280856385280 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.7470719814300537, loss=1.4350193738937378
I0213 08:00:07.607412 140280847992576 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.060333490371704, loss=1.4666885137557983
I0213 08:01:22.834186 140280856385280 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.5020521879196167, loss=1.4187710285186768
I0213 08:02:45.811809 140280847992576 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.223428964614868, loss=1.4537508487701416
I0213 08:04:10.349959 140280856385280 logging_writer.py:48] [21200] global_step=21200, grad_norm=4.116686820983887, loss=1.468788743019104
I0213 08:05:34.790590 140280847992576 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.528088092803955, loss=1.4515758752822876
I0213 08:06:59.613484 140280856385280 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.687650442123413, loss=1.4397634267807007
I0213 08:08:24.933124 140280847992576 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.815967082977295, loss=1.4690922498703003
I0213 08:09:50.026963 140280856385280 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.56302547454834, loss=1.3909175395965576
I0213 08:10:44.125204 140437690197824 spec.py:321] Evaluating on the training split.
I0213 08:11:38.387386 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 08:12:31.426285 140437690197824 spec.py:349] Evaluating on the test split.
I0213 08:12:57.895855 140437690197824 submission_runner.py:408] Time since start: 19132.92s, 	Step: 21663, 	{'train/ctc_loss': Array(0.38680267, dtype=float32), 'train/wer': 0.12805462666323691, 'validation/ctc_loss': Array(0.5477699, dtype=float32), 'validation/wer': 0.16106857700068547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31936333, dtype=float32), 'test/wer': 0.10389372981536774, 'test/num_examples': 2472, 'score': 17300.74575161934, 'total_duration': 19132.921291828156, 'accumulated_submission_time': 17300.74575161934, 'accumulated_eval_time': 1830.5872704982758, 'accumulated_logging_time': 0.6198842525482178}
I0213 08:12:57.934241 140280856385280 logging_writer.py:48] [21663] accumulated_eval_time=1830.587270, accumulated_logging_time=0.619884, accumulated_submission_time=17300.745752, global_step=21663, preemption_count=0, score=17300.745752, test/ctc_loss=0.3193633258342743, test/num_examples=2472, test/wer=0.103894, total_duration=19132.921292, train/ctc_loss=0.38680267333984375, train/wer=0.128055, validation/ctc_loss=0.5477699041366577, validation/num_examples=5348, validation/wer=0.161069
I0213 08:13:26.748548 140280847992576 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.073709011077881, loss=1.34842050075531
I0213 08:14:42.058069 140280856385280 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.8492183685302734, loss=1.4717222452163696
I0213 08:15:57.251245 140280847992576 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.6209189891815186, loss=1.4271389245986938
I0213 08:17:12.427417 140280856385280 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.207718849182129, loss=1.3820608854293823
I0213 08:18:27.745144 140280847992576 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.3195934295654297, loss=1.3174172639846802
I0213 08:19:42.632834 140280856385280 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.277043342590332, loss=1.3367197513580322
I0213 08:21:06.633882 140280847992576 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.4207093715667725, loss=1.4050942659378052
I0213 08:22:31.371431 140280856385280 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.134631395339966, loss=1.4450643062591553
I0213 08:23:56.460762 140280847992576 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.4989535808563232, loss=1.4562424421310425
I0213 08:25:21.455152 140280856385280 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.2246193885803223, loss=1.4411615133285522
I0213 08:26:45.803681 140280856385280 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.218648672103882, loss=1.4081803560256958
I0213 08:28:00.832791 140280847992576 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.729447364807129, loss=1.3984405994415283
I0213 08:29:15.921053 140280856385280 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.950321674346924, loss=1.438536524772644
I0213 08:30:30.992497 140280847992576 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.1580581665039062, loss=1.417937994003296
I0213 08:31:46.234875 140280856385280 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.8190292119979858, loss=1.4084330797195435
I0213 08:33:09.529606 140280847992576 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.411206007003784, loss=1.455834984779358
I0213 08:34:33.702637 140280856385280 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.0607376098632812, loss=1.4343534708023071
I0213 08:35:58.198689 140280847992576 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.7336922883987427, loss=1.3748772144317627
I0213 08:36:58.685879 140437690197824 spec.py:321] Evaluating on the training split.
I0213 08:37:54.211193 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 08:38:46.831549 140437690197824 spec.py:349] Evaluating on the test split.
I0213 08:39:13.220303 140437690197824 submission_runner.py:408] Time since start: 20708.25s, 	Step: 23473, 	{'train/ctc_loss': Array(0.39550456, dtype=float32), 'train/wer': 0.1330114534639051, 'validation/ctc_loss': Array(0.54725, dtype=float32), 'validation/wer': 0.15923419291927746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3084308, dtype=float32), 'test/wer': 0.09995328336684745, 'test/num_examples': 2472, 'score': 18741.406412363052, 'total_duration': 20708.245043039322, 'accumulated_submission_time': 18741.406412363052, 'accumulated_eval_time': 1965.1153333187103, 'accumulated_logging_time': 0.6744070053100586}
I0213 08:39:13.258423 140280856385280 logging_writer.py:48] [23473] accumulated_eval_time=1965.115333, accumulated_logging_time=0.674407, accumulated_submission_time=18741.406412, global_step=23473, preemption_count=0, score=18741.406412, test/ctc_loss=0.3084307909011841, test/num_examples=2472, test/wer=0.099953, total_duration=20708.245043, train/ctc_loss=0.39550456404685974, train/wer=0.133011, validation/ctc_loss=0.547249972820282, validation/num_examples=5348, validation/wer=0.159234
I0213 08:39:34.381030 140280847992576 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.867396116256714, loss=1.3878929615020752
I0213 08:40:49.843947 140280856385280 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.097153425216675, loss=1.457946538925171
I0213 08:42:08.723446 140280856385280 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.029802083969116, loss=1.3835539817810059
I0213 08:43:23.926363 140280847992576 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.8576687574386597, loss=1.4693025350570679
I0213 08:44:38.957711 140280856385280 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.4856419563293457, loss=1.4370033740997314
I0213 08:45:54.235288 140280847992576 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.2901952266693115, loss=1.4326504468917847
I0213 08:47:10.149725 140280856385280 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.1242291927337646, loss=1.3810127973556519
I0213 08:48:33.584632 140280847992576 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.322134256362915, loss=1.373904824256897
I0213 08:49:57.725852 140280856385280 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.3550679683685303, loss=1.3916311264038086
I0213 08:51:22.438010 140280847992576 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.488227367401123, loss=1.3921067714691162
I0213 08:52:47.579292 140280856385280 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.5843312740325928, loss=1.3776841163635254
I0213 08:54:13.199935 140280847992576 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.7301909923553467, loss=1.3748270273208618
I0213 08:55:37.848038 140280856385280 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.5497336387634277, loss=1.3345364332199097
I0213 08:56:58.357118 140280856385280 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.3819289207458496, loss=1.3076791763305664
I0213 08:58:13.347921 140280847992576 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.893085241317749, loss=1.3976119756698608
I0213 08:59:28.349212 140280856385280 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.156223773956299, loss=1.3467285633087158
I0213 09:00:43.349446 140280847992576 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.8757331371307373, loss=1.3534866571426392
I0213 09:02:02.046869 140280856385280 logging_writer.py:48] [25200] global_step=25200, grad_norm=4.847418308258057, loss=1.463463544845581
I0213 09:03:13.759290 140437690197824 spec.py:321] Evaluating on the training split.
I0213 09:04:11.141146 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 09:05:02.931697 140437690197824 spec.py:349] Evaluating on the test split.
I0213 09:05:29.131560 140437690197824 submission_runner.py:408] Time since start: 22284.16s, 	Step: 25286, 	{'train/ctc_loss': Array(0.3043809, dtype=float32), 'train/wer': 0.105151034241601, 'validation/ctc_loss': Array(0.5312797, dtype=float32), 'validation/wer': 0.15423308263417554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2982347, dtype=float32), 'test/wer': 0.09806430646111348, 'test/num_examples': 2472, 'score': 20181.81382036209, 'total_duration': 22284.15694141388, 'accumulated_submission_time': 20181.81382036209, 'accumulated_eval_time': 2100.4818663597107, 'accumulated_logging_time': 0.7313663959503174}
I0213 09:05:29.166494 140280856385280 logging_writer.py:48] [25286] accumulated_eval_time=2100.481866, accumulated_logging_time=0.731366, accumulated_submission_time=20181.813820, global_step=25286, preemption_count=0, score=20181.813820, test/ctc_loss=0.2982347011566162, test/num_examples=2472, test/wer=0.098064, total_duration=22284.156941, train/ctc_loss=0.3043808937072754, train/wer=0.105151, validation/ctc_loss=0.5312796831130981, validation/num_examples=5348, validation/wer=0.154233
I0213 09:05:40.542364 140280847992576 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.90539288520813, loss=1.4153952598571777
I0213 09:06:55.396110 140280856385280 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.3251466751098633, loss=1.3716652393341064
I0213 09:08:10.283172 140280847992576 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.0185344219207764, loss=1.3727343082427979
I0213 09:09:26.017737 140280856385280 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.4301633834838867, loss=1.3740975856781006
I0213 09:10:49.736062 140280847992576 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.2475790977478027, loss=1.4104928970336914
I0213 09:12:13.697767 140280856385280 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.262340545654297, loss=1.375311017036438
I0213 09:13:28.723386 140280847992576 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.3838582038879395, loss=1.3907166719436646
I0213 09:14:43.867306 140280856385280 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.1275815963745117, loss=1.4050318002700806
I0213 09:15:59.242177 140280847992576 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.0767056941986084, loss=1.337369441986084
I0213 09:17:17.209174 140280856385280 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.87021803855896, loss=1.3998994827270508
I0213 09:18:41.451910 140280847992576 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.243196964263916, loss=1.3961212635040283
I0213 09:20:07.376004 140280856385280 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.0791001319885254, loss=1.3138267993927002
I0213 09:21:32.927152 140280847992576 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.344662666320801, loss=1.379564881324768
I0213 09:22:59.398643 140280856385280 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.2726945877075195, loss=1.38076913356781
I0213 09:24:25.458288 140280847992576 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.1071741580963135, loss=1.3564426898956299
I0213 09:25:52.705466 140280856385280 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.585691452026367, loss=1.3435574769973755
I0213 09:27:07.580707 140280847992576 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.368549346923828, loss=1.3511558771133423
I0213 09:28:22.930743 140280856385280 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.6901514530181885, loss=1.282161831855774
I0213 09:29:29.437570 140437690197824 spec.py:321] Evaluating on the training split.
I0213 09:30:26.257188 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 09:31:18.767026 140437690197824 spec.py:349] Evaluating on the test split.
I0213 09:31:45.342482 140437690197824 submission_runner.py:408] Time since start: 23860.37s, 	Step: 27090, 	{'train/ctc_loss': Array(0.3554216, dtype=float32), 'train/wer': 0.12076760555671108, 'validation/ctc_loss': Array(0.51525754, dtype=float32), 'validation/wer': 0.14956988520617512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28729317, dtype=float32), 'test/wer': 0.09205207889017529, 'test/num_examples': 2472, 'score': 21621.993543624878, 'total_duration': 23860.367542266846, 'accumulated_submission_time': 21621.993543624878, 'accumulated_eval_time': 2236.380726337433, 'accumulated_logging_time': 0.7822191715240479}
I0213 09:31:45.382003 140280856385280 logging_writer.py:48] [27090] accumulated_eval_time=2236.380726, accumulated_logging_time=0.782219, accumulated_submission_time=21621.993544, global_step=27090, preemption_count=0, score=21621.993544, test/ctc_loss=0.2872931659221649, test/num_examples=2472, test/wer=0.092052, total_duration=23860.367542, train/ctc_loss=0.35542160272598267, train/wer=0.120768, validation/ctc_loss=0.5152575373649597, validation/num_examples=5348, validation/wer=0.149570
I0213 09:31:53.694161 140280847992576 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.6100530624389648, loss=1.3470913171768188
I0213 09:33:08.765876 140280856385280 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.0828418731689453, loss=1.3547667264938354
I0213 09:34:23.733911 140280847992576 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.1616995334625244, loss=1.276090145111084
I0213 09:35:38.820260 140280856385280 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.494222640991211, loss=1.3647500276565552
I0213 09:37:01.655705 140280847992576 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.4217381477355957, loss=1.305485486984253
I0213 09:38:25.603689 140280856385280 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.3235154151916504, loss=1.2727818489074707
I0213 09:39:49.381349 140280847992576 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.3359856605529785, loss=1.3167448043823242
I0213 09:41:14.689661 140280856385280 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.8568811416625977, loss=1.3618106842041016
I0213 09:42:34.562794 140280856385280 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.125981569290161, loss=1.333809733390808
I0213 09:43:49.698879 140280847992576 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.4142253398895264, loss=1.3161985874176025
I0213 09:45:04.944037 140280856385280 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.806156873703003, loss=1.3831512928009033
I0213 09:46:20.113002 140280847992576 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.781376600265503, loss=1.3440773487091064
I0213 09:47:39.526568 140280856385280 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.873364210128784, loss=1.307195782661438
I0213 09:49:04.137240 140280847992576 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.497099757194519, loss=1.3811427354812622
I0213 09:50:29.079641 140280856385280 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.205963373184204, loss=1.2981327772140503
I0213 09:51:54.264977 140280847992576 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.5109057426452637, loss=1.3387571573257446
I0213 09:53:18.762763 140280856385280 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.123774528503418, loss=1.4040013551712036
I0213 09:54:43.863701 140280847992576 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.2232868671417236, loss=1.3756757974624634
I0213 09:55:45.688934 140437690197824 spec.py:321] Evaluating on the training split.
I0213 09:56:43.672491 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 09:57:35.804091 140437690197824 spec.py:349] Evaluating on the test split.
I0213 09:58:02.419347 140437690197824 submission_runner.py:408] Time since start: 25437.44s, 	Step: 28873, 	{'train/ctc_loss': Array(0.3024444, dtype=float32), 'train/wer': 0.10361523046092184, 'validation/ctc_loss': Array(0.49139738, dtype=float32), 'validation/wer': 0.14322677814572735, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27378377, dtype=float32), 'test/wer': 0.08788820506570796, 'test/num_examples': 2472, 'score': 23062.2101726532, 'total_duration': 25437.443744897842, 'accumulated_submission_time': 23062.2101726532, 'accumulated_eval_time': 2373.104640483856, 'accumulated_logging_time': 0.8374857902526855}
I0213 09:58:02.461781 140280856385280 logging_writer.py:48] [28873] accumulated_eval_time=2373.104640, accumulated_logging_time=0.837486, accumulated_submission_time=23062.210173, global_step=28873, preemption_count=0, score=23062.210173, test/ctc_loss=0.27378377318382263, test/num_examples=2472, test/wer=0.087888, total_duration=25437.443745, train/ctc_loss=0.3024443984031677, train/wer=0.103615, validation/ctc_loss=0.4913973808288574, validation/num_examples=5348, validation/wer=0.143227
I0213 09:58:23.605742 140280847992576 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.333491086959839, loss=1.2787442207336426
I0213 09:59:38.757256 140280856385280 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.0928139686584473, loss=1.2834746837615967
I0213 10:00:53.933036 140280847992576 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.6718437671661377, loss=1.294448971748352
I0213 10:02:09.102345 140280856385280 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.586564540863037, loss=1.319645881652832
I0213 10:03:24.390310 140280847992576 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.7925372123718262, loss=1.2974321842193604
I0213 10:04:39.773198 140280856385280 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.152974843978882, loss=1.3595622777938843
I0213 10:06:03.852717 140280847992576 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.350440502166748, loss=1.3158704042434692
I0213 10:07:29.251802 140280856385280 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.673801898956299, loss=1.33652663230896
I0213 10:08:53.681389 140280847992576 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.9303892850875854, loss=1.3053864240646362
I0213 10:10:18.206033 140280856385280 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.15524959564209, loss=1.293910264968872
I0213 10:11:43.258014 140280856385280 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.1156110763549805, loss=1.2435895204544067
I0213 10:12:58.428062 140280847992576 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.0795626640319824, loss=1.240665078163147
I0213 10:14:13.729079 140280856385280 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.6078741550445557, loss=1.2398251295089722
I0213 10:15:28.734532 140280847992576 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.5065600872039795, loss=1.3216089010238647
I0213 10:16:44.050622 140280856385280 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.167051076889038, loss=1.2506078481674194
I0213 10:18:09.216401 140280847992576 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.3586528301239014, loss=1.3223811388015747
I0213 10:19:35.062763 140280856385280 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.2406105995178223, loss=1.3123414516448975
I0213 10:20:59.934933 140280847992576 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.266063928604126, loss=1.3525077104568481
I0213 10:22:03.006812 140437690197824 spec.py:321] Evaluating on the training split.
I0213 10:22:59.349308 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 10:23:51.048690 140437690197824 spec.py:349] Evaluating on the test split.
I0213 10:24:17.323529 140437690197824 submission_runner.py:408] Time since start: 27012.35s, 	Step: 30676, 	{'train/ctc_loss': Array(0.2924098, dtype=float32), 'train/wer': 0.10257953941445107, 'validation/ctc_loss': Array(0.48229715, dtype=float32), 'validation/wer': 0.1403014182685345, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26265836, dtype=float32), 'test/wer': 0.08536956919139602, 'test/num_examples': 2472, 'score': 24502.66463828087, 'total_duration': 27012.347834587097, 'accumulated_submission_time': 24502.66463828087, 'accumulated_eval_time': 2507.414543867111, 'accumulated_logging_time': 0.8959517478942871}
I0213 10:24:17.361151 140280856385280 logging_writer.py:48] [30676] accumulated_eval_time=2507.414544, accumulated_logging_time=0.895952, accumulated_submission_time=24502.664638, global_step=30676, preemption_count=0, score=24502.664638, test/ctc_loss=0.26265835762023926, test/num_examples=2472, test/wer=0.085370, total_duration=27012.347835, train/ctc_loss=0.2924098074436188, train/wer=0.102580, validation/ctc_loss=0.4822971522808075, validation/num_examples=5348, validation/wer=0.140301
I0213 10:24:36.272340 140280847992576 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.775785207748413, loss=1.2985801696777344
I0213 10:25:51.143274 140280856385280 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.5525243282318115, loss=1.3007731437683105
I0213 10:27:09.991804 140280856385280 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.093958616256714, loss=1.3110177516937256
I0213 10:28:25.342115 140280847992576 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.0686700344085693, loss=1.297287940979004
I0213 10:29:40.624045 140280856385280 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.41672682762146, loss=1.3135038614273071
I0213 10:30:55.832211 140280847992576 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.1063406467437744, loss=1.2709627151489258
I0213 10:32:10.782625 140280856385280 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.423433542251587, loss=1.3269048929214478
I0213 10:33:33.353537 140280847992576 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.7163280248641968, loss=1.2722394466400146
I0213 10:34:58.349663 140280856385280 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.0752716064453125, loss=1.2893872261047363
I0213 10:36:22.937776 140280847992576 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.153278350830078, loss=1.2631003856658936
I0213 10:37:48.152734 140280856385280 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.6406517028808594, loss=1.261412501335144
I0213 10:39:13.097532 140280847992576 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.322540283203125, loss=1.3502748012542725
I0213 10:40:37.789507 140280856385280 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.9404568672180176, loss=1.2716178894042969
I0213 10:41:58.721057 140280856385280 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.402252674102783, loss=1.2881474494934082
I0213 10:43:13.666881 140280847992576 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.9171442985534668, loss=1.238560438156128
I0213 10:44:28.641851 140280856385280 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.5052099227905273, loss=1.296951413154602
I0213 10:45:43.848524 140280847992576 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.750112533569336, loss=1.2600809335708618
I0213 10:47:01.384262 140280856385280 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.516526699066162, loss=1.2394211292266846
I0213 10:48:17.513147 140437690197824 spec.py:321] Evaluating on the training split.
I0213 10:49:14.032317 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 10:50:05.730058 140437690197824 spec.py:349] Evaluating on the test split.
I0213 10:50:32.572484 140437690197824 submission_runner.py:408] Time since start: 28587.60s, 	Step: 32491, 	{'train/ctc_loss': Array(0.2727315, dtype=float32), 'train/wer': 0.09430362496593077, 'validation/ctc_loss': Array(0.4702428, dtype=float32), 'validation/wer': 0.13723123859544106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2568106, dtype=float32), 'test/wer': 0.08285093331708407, 'test/num_examples': 2472, 'score': 25942.722469568253, 'total_duration': 28587.597064971924, 'accumulated_submission_time': 25942.722469568253, 'accumulated_eval_time': 2642.467351436615, 'accumulated_logging_time': 0.9536893367767334}
I0213 10:50:32.617038 140280856385280 logging_writer.py:48] [32491] accumulated_eval_time=2642.467351, accumulated_logging_time=0.953689, accumulated_submission_time=25942.722470, global_step=32491, preemption_count=0, score=25942.722470, test/ctc_loss=0.25681060552597046, test/num_examples=2472, test/wer=0.082851, total_duration=28587.597065, train/ctc_loss=0.2727315127849579, train/wer=0.094304, validation/ctc_loss=0.47024279832839966, validation/num_examples=5348, validation/wer=0.137231
I0213 10:50:40.165446 140280847992576 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.7421784400939941, loss=1.2719814777374268
I0213 10:51:55.307028 140280856385280 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.069303274154663, loss=1.2769523859024048
I0213 10:53:10.374157 140280847992576 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.083170175552368, loss=1.289300560951233
I0213 10:54:25.523679 140280856385280 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.137782573699951, loss=1.2904419898986816
I0213 10:55:47.327674 140280847992576 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.8482577800750732, loss=1.2500015497207642
I0213 10:57:11.831766 140280856385280 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.9075303077697754, loss=1.245444416999817
I0213 10:58:27.111902 140280847992576 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.668707847595215, loss=1.2650203704833984
I0213 10:59:42.356990 140280856385280 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.5440165996551514, loss=1.2787261009216309
I0213 11:00:57.361969 140280847992576 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.482788562774658, loss=1.2539563179016113
I0213 11:02:12.371801 140280856385280 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.7888727188110352, loss=1.2327907085418701
I0213 11:03:35.559346 140280847992576 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.3226401805877686, loss=1.3369594812393188
I0213 11:05:00.404924 140280856385280 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.0689516067504883, loss=1.2418758869171143
I0213 11:06:26.176606 140280847992576 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.3384339809417725, loss=1.2355340719223022
I0213 11:07:50.994339 140280856385280 logging_writer.py:48] [33800] global_step=33800, grad_norm=4.17094612121582, loss=1.2148922681808472
I0213 11:09:16.454799 140280847992576 logging_writer.py:48] [33900] global_step=33900, grad_norm=6.718029022216797, loss=1.218358039855957
I0213 11:10:44.949634 140280856385280 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.4469385147094727, loss=1.2518788576126099
I0213 11:12:00.078828 140280847992576 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.3663558959960938, loss=1.2293920516967773
I0213 11:13:15.386053 140280856385280 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.898301124572754, loss=1.219063639640808
I0213 11:14:30.501351 140280847992576 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.474104642868042, loss=1.2184913158416748
I0213 11:14:32.638755 140437690197824 spec.py:321] Evaluating on the training split.
I0213 11:15:29.542616 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 11:16:21.846294 140437690197824 spec.py:349] Evaluating on the test split.
I0213 11:16:48.653109 140437690197824 submission_runner.py:408] Time since start: 30163.68s, 	Step: 34304, 	{'train/ctc_loss': Array(0.26884595, dtype=float32), 'train/wer': 0.09494766574412591, 'validation/ctc_loss': Array(0.46222648, dtype=float32), 'validation/wer': 0.13335006806530408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24790536, dtype=float32), 'test/wer': 0.08086039851319238, 'test/num_examples': 2472, 'score': 27382.652527332306, 'total_duration': 30163.677755355835, 'accumulated_submission_time': 27382.652527332306, 'accumulated_eval_time': 2778.475238084793, 'accumulated_logging_time': 1.014662265777588}
I0213 11:16:48.691106 140280856385280 logging_writer.py:48] [34304] accumulated_eval_time=2778.475238, accumulated_logging_time=1.014662, accumulated_submission_time=27382.652527, global_step=34304, preemption_count=0, score=27382.652527, test/ctc_loss=0.24790535867214203, test/num_examples=2472, test/wer=0.080860, total_duration=30163.677755, train/ctc_loss=0.26884594559669495, train/wer=0.094948, validation/ctc_loss=0.4622264802455902, validation/num_examples=5348, validation/wer=0.133350
I0213 11:18:01.694448 140280847992576 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.9038941860198975, loss=1.2127821445465088
I0213 11:19:17.183773 140280856385280 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.6963798999786377, loss=1.223005771636963
I0213 11:20:32.452385 140280847992576 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.9302774667739868, loss=1.2112637758255005
I0213 11:21:54.010328 140280856385280 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.554431676864624, loss=1.2596839666366577
I0213 11:23:18.459755 140280847992576 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.3439860343933105, loss=1.2517225742340088
I0213 11:24:43.360660 140280856385280 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.3759894371032715, loss=1.2422199249267578
I0213 11:26:08.572533 140280847992576 logging_writer.py:48] [35000] global_step=35000, grad_norm=6.5027618408203125, loss=1.256475806236267
I0213 11:27:29.445123 140280856385280 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.8244973421096802, loss=1.2448893785476685
I0213 11:28:44.391358 140280847992576 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.540276288986206, loss=1.1748743057250977
I0213 11:29:59.353798 140280856385280 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.3627636432647705, loss=1.240127682685852
I0213 11:31:14.267008 140280847992576 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.4092116355895996, loss=1.2425230741500854
I0213 11:32:34.156274 140280856385280 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.6997175216674805, loss=1.2319700717926025
I0213 11:33:58.971863 140280847992576 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.216111898422241, loss=1.197563648223877
I0213 11:35:24.402654 140280856385280 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.1115782260894775, loss=1.2318148612976074
I0213 11:36:49.158074 140280847992576 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.830179452896118, loss=1.2217934131622314
I0213 11:38:14.479340 140280856385280 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.531229019165039, loss=1.2437198162078857
I0213 11:39:39.535205 140280847992576 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.8043290376663208, loss=1.2129510641098022
I0213 11:40:49.043677 140437690197824 spec.py:321] Evaluating on the training split.
I0213 11:41:43.786485 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 11:42:35.558799 140437690197824 spec.py:349] Evaluating on the test split.
I0213 11:43:02.039482 140437690197824 submission_runner.py:408] Time since start: 31737.06s, 	Step: 36084, 	{'train/ctc_loss': Array(0.2914037, dtype=float32), 'train/wer': 0.09721227850366923, 'validation/ctc_loss': Array(0.44839144, dtype=float32), 'validation/wer': 0.12929511378008632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23915242, dtype=float32), 'test/wer': 0.07775272682956554, 'test/num_examples': 2472, 'score': 28822.911303281784, 'total_duration': 31737.064566612244, 'accumulated_submission_time': 28822.911303281784, 'accumulated_eval_time': 2911.465180158615, 'accumulated_logging_time': 1.0729358196258545}
I0213 11:43:02.079931 140280856385280 logging_writer.py:48] [36084] accumulated_eval_time=2911.465180, accumulated_logging_time=1.072936, accumulated_submission_time=28822.911303, global_step=36084, preemption_count=0, score=28822.911303, test/ctc_loss=0.23915241658687592, test/num_examples=2472, test/wer=0.077753, total_duration=31737.064567, train/ctc_loss=0.29140371084213257, train/wer=0.097212, validation/ctc_loss=0.4483914375305176, validation/num_examples=5348, validation/wer=0.129295
I0213 11:43:15.085900 140280847992576 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.229625701904297, loss=1.1976902484893799
I0213 11:44:30.237233 140280856385280 logging_writer.py:48] [36200] global_step=36200, grad_norm=4.30623722076416, loss=1.1780405044555664
I0213 11:45:45.345534 140280847992576 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.098052978515625, loss=1.2188053131103516
I0213 11:47:00.416859 140280856385280 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.1002907752990723, loss=1.2328907251358032
I0213 11:48:15.795782 140280847992576 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.3214163780212402, loss=1.2598916292190552
I0213 11:49:31.024104 140280856385280 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.952744722366333, loss=1.214044213294983
I0213 11:50:54.070326 140280847992576 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.0681772232055664, loss=1.1702892780303955
I0213 11:52:19.051993 140280856385280 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.8610281944274902, loss=1.1516062021255493
I0213 11:53:43.591328 140280847992576 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.299405813217163, loss=1.2024357318878174
I0213 11:55:08.499345 140280856385280 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.9713940620422363, loss=1.1668363809585571
I0213 11:56:35.039151 140280856385280 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.3341143131256104, loss=1.1784369945526123
I0213 11:57:50.020053 140280847992576 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.7743117809295654, loss=1.2089855670928955
I0213 11:59:04.930686 140280856385280 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.2393434047698975, loss=1.2424237728118896
I0213 12:00:19.995224 140280847992576 logging_writer.py:48] [37400] global_step=37400, grad_norm=6.451042175292969, loss=1.1947762966156006
I0213 12:01:36.122455 140280856385280 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.608530282974243, loss=1.1549211740493774
I0213 12:02:59.610705 140280847992576 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.4514381885528564, loss=1.1887485980987549
I0213 12:04:24.705895 140280856385280 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.447699546813965, loss=1.2339720726013184
I0213 12:05:50.482087 140280847992576 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.984967589378357, loss=1.2117778062820435
I0213 12:07:02.151065 140437690197824 spec.py:321] Evaluating on the training split.
I0213 12:07:57.485296 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 12:08:48.781362 140437690197824 spec.py:349] Evaluating on the test split.
I0213 12:09:15.083869 140437690197824 submission_runner.py:408] Time since start: 33310.11s, 	Step: 37885, 	{'train/ctc_loss': Array(0.22805688, dtype=float32), 'train/wer': 0.0809899755595646, 'validation/ctc_loss': Array(0.43772352, dtype=float32), 'validation/wer': 0.1265725016171544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23163031, dtype=float32), 'test/wer': 0.07436069303109703, 'test/num_examples': 2472, 'score': 30262.89229130745, 'total_duration': 33310.108652830124, 'accumulated_submission_time': 30262.89229130745, 'accumulated_eval_time': 3044.3916516304016, 'accumulated_logging_time': 1.1292433738708496}
I0213 12:09:15.126677 140280856385280 logging_writer.py:48] [37885] accumulated_eval_time=3044.391652, accumulated_logging_time=1.129243, accumulated_submission_time=30262.892291, global_step=37885, preemption_count=0, score=30262.892291, test/ctc_loss=0.23163031041622162, test/num_examples=2472, test/wer=0.074361, total_duration=33310.108653, train/ctc_loss=0.2280568778514862, train/wer=0.080990, validation/ctc_loss=0.4377235174179077, validation/num_examples=5348, validation/wer=0.126573
I0213 12:09:27.234563 140280847992576 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.95689058303833, loss=1.1939815282821655
I0213 12:10:42.298585 140280856385280 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.4863152503967285, loss=1.2085061073303223
I0213 12:11:57.243331 140280847992576 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.6776103973388672, loss=1.147190809249878
I0213 12:13:16.292029 140280856385280 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.0697293281555176, loss=1.2160221338272095
I0213 12:14:31.796333 140280847992576 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.1022937297821045, loss=1.1867680549621582
I0213 12:15:47.136324 140280856385280 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.6945852041244507, loss=1.190521240234375
I0213 12:17:02.747584 140280847992576 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.7321245670318604, loss=1.136843204498291
I0213 12:18:24.908358 140280856385280 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.2123100757598877, loss=1.161987066268921
I0213 12:19:50.229858 140280847992576 logging_writer.py:48] [38700] global_step=38700, grad_norm=4.2510986328125, loss=1.165217638015747
I0213 12:21:16.303468 140280856385280 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.383653402328491, loss=1.2368381023406982
I0213 12:22:41.416767 140280847992576 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.3629956245422363, loss=1.2032634019851685
I0213 12:24:06.977564 140280856385280 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.180276393890381, loss=1.204144835472107
I0213 12:25:32.012594 140280847992576 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.4155994653701782, loss=1.2168387174606323
I0213 12:26:54.901599 140280856385280 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.150784730911255, loss=1.1953125
I0213 12:28:09.953633 140280847992576 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.335432767868042, loss=1.1309529542922974
I0213 12:29:25.111308 140280856385280 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.5472960472106934, loss=1.125673532485962
I0213 12:30:40.154084 140280847992576 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.414185047149658, loss=1.1672585010528564
I0213 12:31:57.410598 140280856385280 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.831179141998291, loss=1.2049305438995361
I0213 12:33:16.051048 140437690197824 spec.py:321] Evaluating on the training split.
I0213 12:34:11.041895 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 12:35:02.263222 140437690197824 spec.py:349] Evaluating on the test split.
I0213 12:35:28.687988 140437690197824 submission_runner.py:408] Time since start: 34883.71s, 	Step: 39694, 	{'train/ctc_loss': Array(0.23828937, dtype=float32), 'train/wer': 0.0854084474355999, 'validation/ctc_loss': Array(0.4246287, dtype=float32), 'validation/wer': 0.1229906253318787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22602047, dtype=float32), 'test/wer': 0.07239046980683687, 'test/num_examples': 2472, 'score': 31703.72644138336, 'total_duration': 34883.71180319786, 'accumulated_submission_time': 31703.72644138336, 'accumulated_eval_time': 3177.021288871765, 'accumulated_logging_time': 1.1870660781860352}
I0213 12:35:28.729002 140280856385280 logging_writer.py:48] [39694] accumulated_eval_time=3177.021289, accumulated_logging_time=1.187066, accumulated_submission_time=31703.726441, global_step=39694, preemption_count=0, score=31703.726441, test/ctc_loss=0.2260204702615738, test/num_examples=2472, test/wer=0.072390, total_duration=34883.711803, train/ctc_loss=0.23828937113285065, train/wer=0.085408, validation/ctc_loss=0.42462870478630066, validation/num_examples=5348, validation/wer=0.122991
I0213 12:35:34.214752 140280847992576 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.758452892303467, loss=1.179443120956421
I0213 12:36:49.639093 140280856385280 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.854180335998535, loss=1.2104426622390747
I0213 12:38:04.870165 140280847992576 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.198060989379883, loss=1.1810976266860962
I0213 12:39:19.779992 140280856385280 logging_writer.py:48] [40000] global_step=40000, grad_norm=4.481441497802734, loss=1.119221806526184
I0213 12:40:43.325884 140280847992576 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.2793595790863037, loss=1.1145285367965698
I0213 12:42:09.705671 140280856385280 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.720292568206787, loss=1.1484942436218262
I0213 12:43:24.800464 140280847992576 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.3472206592559814, loss=1.1127305030822754
I0213 12:44:39.630619 140280856385280 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.6182010173797607, loss=1.2071478366851807
I0213 12:45:54.652581 140280847992576 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.730363130569458, loss=1.1987521648406982
I0213 12:47:11.556017 140280856385280 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.02453351020813, loss=1.2014037370681763
I0213 12:48:35.762925 140280847992576 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.977831482887268, loss=1.1347126960754395
I0213 12:50:00.519815 140280856385280 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.3716628551483154, loss=1.12293541431427
I0213 12:51:25.694862 140280847992576 logging_writer.py:48] [40900] global_step=40900, grad_norm=4.829181671142578, loss=1.215623378753662
I0213 12:52:50.537499 140280856385280 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.228070020675659, loss=1.1504955291748047
I0213 12:54:16.055861 140280847992576 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.0795042514801025, loss=1.1426481008529663
I0213 12:55:45.003838 140280856385280 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.1254987716674805, loss=1.1448622941970825
I0213 12:56:59.969533 140280847992576 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.0023698806762695, loss=1.172156810760498
I0213 12:58:15.299319 140280856385280 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.4532792568206787, loss=1.1859140396118164
I0213 12:59:29.419073 140437690197824 spec.py:321] Evaluating on the training split.
I0213 13:00:25.334674 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 13:01:17.353840 140437690197824 spec.py:349] Evaluating on the test split.
I0213 13:01:43.874284 140437690197824 submission_runner.py:408] Time since start: 36458.90s, 	Step: 41500, 	{'train/ctc_loss': Array(0.20024322, dtype=float32), 'train/wer': 0.07069968842901182, 'validation/ctc_loss': Array(0.42071438, dtype=float32), 'validation/wer': 0.12050937949544784, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22364078, dtype=float32), 'test/wer': 0.0725732740235208, 'test/num_examples': 2472, 'score': 33144.32629656792, 'total_duration': 36458.89913678169, 'accumulated_submission_time': 33144.32629656792, 'accumulated_eval_time': 3311.4702532291412, 'accumulated_logging_time': 1.2442855834960938}
I0213 13:01:43.916723 140280856385280 logging_writer.py:48] [41500] accumulated_eval_time=3311.470253, accumulated_logging_time=1.244286, accumulated_submission_time=33144.326297, global_step=41500, preemption_count=0, score=33144.326297, test/ctc_loss=0.2236407846212387, test/num_examples=2472, test/wer=0.072573, total_duration=36458.899137, train/ctc_loss=0.20024321973323822, train/wer=0.070700, validation/ctc_loss=0.4207143783569336, validation/num_examples=5348, validation/wer=0.120509
I0213 13:01:44.799295 140280847992576 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.0420241355895996, loss=1.1042178869247437
I0213 13:02:59.946766 140280856385280 logging_writer.py:48] [41600] global_step=41600, grad_norm=5.275314807891846, loss=1.1546990871429443
I0213 13:04:15.052495 140280847992576 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.4014017581939697, loss=1.1129271984100342
I0213 13:05:30.074319 140280856385280 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.3865981101989746, loss=1.1539239883422852
I0213 13:06:52.588645 140280847992576 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.864710569381714, loss=1.1390225887298584
I0213 13:08:17.113250 140280856385280 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.368710994720459, loss=1.1978733539581299
I0213 13:09:41.442687 140280847992576 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.218445301055908, loss=1.1714117527008057
I0213 13:11:06.305954 140280856385280 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.6727207899093628, loss=1.1560347080230713
I0213 13:12:27.800050 140280856385280 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.8810088634490967, loss=1.1461005210876465
I0213 13:13:42.976722 140280847992576 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.3580493927001953, loss=1.130164384841919
I0213 13:14:58.250229 140280856385280 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.7303504943847656, loss=1.1230920553207397
I0213 13:16:13.605014 140280847992576 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.842867851257324, loss=1.158815622329712
I0213 13:17:31.624588 140280856385280 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.088505268096924, loss=1.1408109664916992
I0213 13:18:56.113678 140280847992576 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.474154472351074, loss=1.1785356998443604
I0213 13:20:21.134505 140280856385280 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.2462551593780518, loss=1.1846592426300049
I0213 13:21:46.115989 140280847992576 logging_writer.py:48] [43000] global_step=43000, grad_norm=8.090483665466309, loss=1.1317166090011597
I0213 13:23:11.882879 140280856385280 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.5066909790039062, loss=1.110395908355713
I0213 13:24:36.861265 140280847992576 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.542767286300659, loss=1.117275357246399
I0213 13:25:44.318498 140437690197824 spec.py:321] Evaluating on the training split.
I0213 13:26:39.877078 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 13:27:31.970712 140437690197824 spec.py:349] Evaluating on the test split.
I0213 13:27:58.335861 140437690197824 submission_runner.py:408] Time since start: 38033.36s, 	Step: 43278, 	{'train/ctc_loss': Array(0.21650901, dtype=float32), 'train/wer': 0.0750982776662236, 'validation/ctc_loss': Array(0.41551012, dtype=float32), 'validation/wer': 0.11895498035278103, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21895356, dtype=float32), 'test/wer': 0.07017650762699815, 'test/num_examples': 2472, 'score': 34584.63703107834, 'total_duration': 38033.359548807144, 'accumulated_submission_time': 34584.63703107834, 'accumulated_eval_time': 3445.4803664684296, 'accumulated_logging_time': 1.304053783416748}
I0213 13:27:58.381174 140280856385280 logging_writer.py:48] [43278] accumulated_eval_time=3445.480366, accumulated_logging_time=1.304054, accumulated_submission_time=34584.637031, global_step=43278, preemption_count=0, score=34584.637031, test/ctc_loss=0.21895356476306915, test/num_examples=2472, test/wer=0.070177, total_duration=38033.359549, train/ctc_loss=0.21650901436805725, train/wer=0.075098, validation/ctc_loss=0.4155101180076599, validation/num_examples=5348, validation/wer=0.118955
I0213 13:28:15.765127 140280847992576 logging_writer.py:48] [43300] global_step=43300, grad_norm=4.159616947174072, loss=1.1331080198287964
I0213 13:29:30.979453 140280856385280 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.8250491619110107, loss=1.1284117698669434
I0213 13:30:46.281991 140280847992576 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.9370803833007812, loss=1.146972894668579
I0213 13:32:01.582021 140280856385280 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.4191830158233643, loss=1.1243393421173096
I0213 13:33:16.826126 140280847992576 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.5737907886505127, loss=1.1630665063858032
I0213 13:34:32.061793 140280856385280 logging_writer.py:48] [43800] global_step=43800, grad_norm=4.096756935119629, loss=1.14708411693573
I0213 13:35:55.597602 140280847992576 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.8727691173553467, loss=1.1652953624725342
I0213 13:37:20.686167 140280856385280 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.746699333190918, loss=1.1382365226745605
I0213 13:38:46.346367 140280847992576 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.3706324100494385, loss=1.1184345483779907
I0213 13:40:11.128765 140280856385280 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.2225966453552246, loss=1.0985510349273682
I0213 13:41:38.513946 140280856385280 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.36295485496521, loss=1.1500385999679565
I0213 13:42:53.725339 140280847992576 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.234400510787964, loss=1.1274667978286743
I0213 13:44:08.907346 140280856385280 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.5951831340789795, loss=1.13149094581604
I0213 13:45:23.961795 140280847992576 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.1159465312957764, loss=1.1532732248306274
I0213 13:46:39.150915 140280856385280 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.8833730220794678, loss=1.1209598779678345
I0213 13:48:01.971886 140280847992576 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.0590245723724365, loss=1.1032226085662842
I0213 13:49:27.047390 140280856385280 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.6462199687957764, loss=1.1005972623825073
I0213 13:50:52.662896 140280847992576 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.594377279281616, loss=1.1482220888137817
I0213 13:51:58.901645 140437690197824 spec.py:321] Evaluating on the training split.
I0213 13:52:55.435238 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 13:53:47.540339 140437690197824 spec.py:349] Evaluating on the test split.
I0213 13:54:14.476897 140437690197824 submission_runner.py:408] Time since start: 39609.50s, 	Step: 45080, 	{'train/ctc_loss': Array(0.21757558, dtype=float32), 'train/wer': 0.07707054560348604, 'validation/ctc_loss': Array(0.41123518, dtype=float32), 'validation/wer': 0.11877154194464022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21681073, dtype=float32), 'test/wer': 0.06930310970284159, 'test/num_examples': 2472, 'score': 36025.066982507706, 'total_duration': 39609.50177645683, 'accumulated_submission_time': 36025.066982507706, 'accumulated_eval_time': 3581.0494186878204, 'accumulated_logging_time': 1.3660883903503418}
I0213 13:54:14.521703 140280856385280 logging_writer.py:48] [45080] accumulated_eval_time=3581.049419, accumulated_logging_time=1.366088, accumulated_submission_time=36025.066983, global_step=45080, preemption_count=0, score=36025.066983, test/ctc_loss=0.21681073307991028, test/num_examples=2472, test/wer=0.069303, total_duration=39609.501776, train/ctc_loss=0.2175755798816681, train/wer=0.077071, validation/ctc_loss=0.41123518347740173, validation/num_examples=5348, validation/wer=0.118772
I0213 13:54:30.362032 140280847992576 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.185412645339966, loss=1.1027305126190186
I0213 13:55:45.455428 140280856385280 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.572598695755005, loss=1.0975407361984253
I0213 13:57:00.481106 140280847992576 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.61767840385437, loss=1.07901930809021
I0213 13:58:19.603733 140280856385280 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.4625742435455322, loss=1.1117942333221436
I0213 13:59:34.780268 140280847992576 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.062217950820923, loss=1.134345293045044
I0213 14:00:49.870011 140280856385280 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.338364601135254, loss=1.1100503206253052
I0213 14:02:05.111714 140280847992576 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.1118240356445312, loss=1.127634882926941
I0213 14:03:26.694414 140280856385280 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.2954487800598145, loss=1.1137877702713013
I0213 14:04:52.585318 140280847992576 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.593566656112671, loss=1.1000827550888062
I0213 14:06:17.815129 140280856385280 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.183093309402466, loss=1.1353641748428345
I0213 14:07:42.218381 140280847992576 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.5510997772216797, loss=1.1224292516708374
I0213 14:09:07.940562 140280856385280 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.4739437103271484, loss=1.183002233505249
I0213 14:10:33.981381 140280847992576 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.261836528778076, loss=1.1101975440979004
I0213 14:11:57.611862 140280856385280 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.30778169631958, loss=1.1046799421310425
I0213 14:13:12.755671 140280847992576 logging_writer.py:48] [46500] global_step=46500, grad_norm=4.147683143615723, loss=1.1165083646774292
I0213 14:14:27.823617 140280856385280 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.4203622341156006, loss=1.1131770610809326
I0213 14:15:42.981436 140280847992576 logging_writer.py:48] [46700] global_step=46700, grad_norm=4.206149578094482, loss=1.1637042760849
I0213 14:17:01.881004 140280856385280 logging_writer.py:48] [46800] global_step=46800, grad_norm=4.59770393371582, loss=1.1317293643951416
I0213 14:18:14.813582 140437690197824 spec.py:321] Evaluating on the training split.
I0213 14:19:11.787335 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 14:20:04.215903 140437690197824 spec.py:349] Evaluating on the test split.
I0213 14:20:31.178484 140437690197824 submission_runner.py:408] Time since start: 41186.20s, 	Step: 46887, 	{'train/ctc_loss': Array(0.22279812, dtype=float32), 'train/wer': 0.07826722659513653, 'validation/ctc_loss': Array(0.4108359, dtype=float32), 'validation/wer': 0.11850121165895904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21612823, dtype=float32), 'test/wer': 0.06962809497694636, 'test/num_examples': 2472, 'score': 37465.26635026932, 'total_duration': 41186.20255231857, 'accumulated_submission_time': 37465.26635026932, 'accumulated_eval_time': 3717.407273054123, 'accumulated_logging_time': 1.4297056198120117}
I0213 14:20:31.219119 140280856385280 logging_writer.py:48] [46887] accumulated_eval_time=3717.407273, accumulated_logging_time=1.429706, accumulated_submission_time=37465.266350, global_step=46887, preemption_count=0, score=37465.266350, test/ctc_loss=0.21612823009490967, test/num_examples=2472, test/wer=0.069628, total_duration=41186.202552, train/ctc_loss=0.22279812395572662, train/wer=0.078267, validation/ctc_loss=0.4108358919620514, validation/num_examples=5348, validation/wer=0.118501
I0213 14:20:41.803828 140280847992576 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.8308279514312744, loss=1.0826280117034912
I0213 14:21:57.032472 140280856385280 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.2020866870880127, loss=1.0925554037094116
I0213 14:23:12.209137 140280847992576 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.7898786067962646, loss=1.130433440208435
I0213 14:24:27.429645 140280856385280 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.925095558166504, loss=1.0904872417449951
I0213 14:25:50.772766 140280847992576 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8802117109298706, loss=1.1643259525299072
I0213 14:27:17.549970 140280856385280 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.08211350440979, loss=1.169534683227539
I0213 14:28:32.732784 140280847992576 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.04998254776001, loss=1.1640493869781494
I0213 14:29:47.835982 140280856385280 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.362596273422241, loss=1.1132065057754517
I0213 14:31:03.173442 140280847992576 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.2837090492248535, loss=1.14268159866333
I0213 14:32:19.635539 140280856385280 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.4964706897735596, loss=1.1461286544799805
I0213 14:33:43.903318 140280847992576 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.7416272163391113, loss=1.0958765745162964
I0213 14:35:07.665718 140437690197824 spec.py:321] Evaluating on the training split.
I0213 14:36:02.803619 140437690197824 spec.py:333] Evaluating on the validation split.
I0213 14:36:54.960240 140437690197824 spec.py:349] Evaluating on the test split.
I0213 14:37:21.475039 140437690197824 submission_runner.py:408] Time since start: 42196.50s, 	Step: 48000, 	{'train/ctc_loss': Array(0.18652456, dtype=float32), 'train/wer': 0.06758433938846892, 'validation/ctc_loss': Array(0.41070828, dtype=float32), 'validation/wer': 0.11837570116917849, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21605648, dtype=float32), 'test/wer': 0.06948591391952552, 'test/num_examples': 2472, 'score': 38341.64616441727, 'total_duration': 42196.50052070618, 'accumulated_submission_time': 38341.64616441727, 'accumulated_eval_time': 3851.2109656333923, 'accumulated_logging_time': 1.4912919998168945}
I0213 14:37:21.513791 140280856385280 logging_writer.py:48] [48000] accumulated_eval_time=3851.210966, accumulated_logging_time=1.491292, accumulated_submission_time=38341.646164, global_step=48000, preemption_count=0, score=38341.646164, test/ctc_loss=0.2160564810037613, test/num_examples=2472, test/wer=0.069486, total_duration=42196.500521, train/ctc_loss=0.18652455508708954, train/wer=0.067584, validation/ctc_loss=0.4107082784175873, validation/num_examples=5348, validation/wer=0.118376
I0213 14:37:21.544052 140280847992576 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=38341.646164
I0213 14:37:21.706125 140437690197824 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 14:37:22.628504 140437690197824 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_5/checkpoint_48000
I0213 14:37:22.648365 140437690197824 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/librispeech_deepspeech_jax/trial_5/checkpoint_48000.
I0213 14:37:24.115839 140437690197824 submission_runner.py:583] Tuning trial 5/5
I0213 14:37:24.116086 140437690197824 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0213 14:37:24.129952 140437690197824 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.380108, dtype=float32), 'train/wer': 3.862109067393991, 'validation/ctc_loss': Array(30.891895, dtype=float32), 'validation/wer': 3.324869420817363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.942286, dtype=float32), 'test/wer': 3.698819897223407, 'test/num_examples': 2472, 'score': 16.74648356437683, 'total_duration': 220.1540470123291, 'accumulated_submission_time': 16.74648356437683, 'accumulated_eval_time': 203.40749049186707, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1803, {'train/ctc_loss': Array(3.9911628, dtype=float32), 'train/wer': 0.7952104345989333, 'validation/ctc_loss': Array(4.117805, dtype=float32), 'validation/wer': 0.7905519565154426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.7096946, dtype=float32), 'test/wer': 0.7506753600227489, 'test/num_examples': 2472, 'score': 1457.3477289676666, 'total_duration': 1782.0162966251373, 'accumulated_submission_time': 1457.3477289676666, 'accumulated_eval_time': 324.55981945991516, 'accumulated_logging_time': 0.031755924224853516, 'global_step': 1803, 'preemption_count': 0}), (3624, {'train/ctc_loss': Array(0.5607584, dtype=float32), 'train/wer': 0.18567327632882813, 'validation/ctc_loss': Array(0.92604756, dtype=float32), 'validation/wer': 0.2639678693146162, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.59962285, dtype=float32), 'test/wer': 0.19145694960697093, 'test/num_examples': 2472, 'score': 2897.5168826580048, 'total_duration': 3360.348862886429, 'accumulated_submission_time': 2897.5168826580048, 'accumulated_eval_time': 462.58443236351013, 'accumulated_logging_time': 0.08944153785705566, 'global_step': 3624, 'preemption_count': 0}), (5448, {'train/ctc_loss': Array(0.46810666, dtype=float32), 'train/wer': 0.1586667026822614, 'validation/ctc_loss': Array(0.79226905, dtype=float32), 'validation/wer': 0.23071724417583053, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.493212, dtype=float32), 'test/wer': 0.15786159689639062, 'test/num_examples': 2472, 'score': 4337.565103292465, 'total_duration': 4936.6641318798065, 'accumulated_submission_time': 4337.565103292465, 'accumulated_eval_time': 598.7094995975494, 'accumulated_logging_time': 0.14923548698425293, 'global_step': 5448, 'preemption_count': 0}), (7230, {'train/ctc_loss': Array(0.40930718, dtype=float32), 'train/wer': 0.13781296003709956, 'validation/ctc_loss': Array(0.7724912, dtype=float32), 'validation/wer': 0.2218735819728318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47449654, dtype=float32), 'test/wer': 0.15371803465155484, 'test/num_examples': 2472, 'score': 5777.571934938431, 'total_duration': 6514.106091976166, 'accumulated_submission_time': 5777.571934938431, 'accumulated_eval_time': 736.0015232563019, 'accumulated_logging_time': 0.21187639236450195, 'global_step': 7230, 'preemption_count': 0}), (9039, {'train/ctc_loss': Array(0.39555684, dtype=float32), 'train/wer': 0.13521204284994728, 'validation/ctc_loss': Array(0.7158556, dtype=float32), 'validation/wer': 0.20704403487260686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42413357, dtype=float32), 'test/wer': 0.13653443828326528, 'test/num_examples': 2472, 'score': 7218.460584163666, 'total_duration': 8092.8622970581055, 'accumulated_submission_time': 7218.460584163666, 'accumulated_eval_time': 873.7335147857666, 'accumulated_logging_time': 0.26636743545532227, 'global_step': 9039, 'preemption_count': 0}), (10850, {'train/ctc_loss': Array(0.39360082, dtype=float32), 'train/wer': 0.12606838743305127, 'validation/ctc_loss': Array(0.68587214, dtype=float32), 'validation/wer': 0.1970128503432229, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40153503, dtype=float32), 'test/wer': 0.13015660227895923, 'test/num_examples': 2472, 'score': 8658.568603992462, 'total_duration': 9670.741458892822, 'accumulated_submission_time': 8658.568603992462, 'accumulated_eval_time': 1011.367288351059, 'accumulated_logging_time': 0.3219921588897705, 'global_step': 10850, 'preemption_count': 0}), (12664, {'train/ctc_loss': Array(0.30456778, dtype=float32), 'train/wer': 0.10658186522114081, 'validation/ctc_loss': Array(0.66065574, dtype=float32), 'validation/wer': 0.1916158992826593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38939542, dtype=float32), 'test/wer': 0.12715048849349014, 'test/num_examples': 2472, 'score': 10099.024131059647, 'total_duration': 11249.047943592072, 'accumulated_submission_time': 10099.024131059647, 'accumulated_eval_time': 1149.0821301937103, 'accumulated_logging_time': 0.3768749237060547, 'global_step': 12664, 'preemption_count': 0}), (14453, {'train/ctc_loss': Array(0.29636636, dtype=float32), 'train/wer': 0.10252902500936291, 'validation/ctc_loss': Array(0.63199, dtype=float32), 'validation/wer': 0.181507477528795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37232095, dtype=float32), 'test/wer': 0.1210773261836573, 'test/num_examples': 2472, 'score': 11539.4461581707, 'total_duration': 12828.851173877716, 'accumulated_submission_time': 11539.4461581707, 'accumulated_eval_time': 1288.3281433582306, 'accumulated_logging_time': 0.43149304389953613, 'global_step': 14453, 'preemption_count': 0}), (16259, {'train/ctc_loss': Array(0.4332551, dtype=float32), 'train/wer': 0.14540633904309272, 'validation/ctc_loss': Array(0.6278105, dtype=float32), 'validation/wer': 0.18295567548780134, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36600688, dtype=float32), 'test/wer': 0.11886336400381858, 'test/num_examples': 2472, 'score': 12980.456796169281, 'total_duration': 14406.084884166718, 'accumulated_submission_time': 12980.456796169281, 'accumulated_eval_time': 1424.4248707294464, 'accumulated_logging_time': 0.47845959663391113, 'global_step': 16259, 'preemption_count': 0}), (18069, {'train/ctc_loss': Array(0.41025263, dtype=float32), 'train/wer': 0.13652797420221108, 'validation/ctc_loss': Array(0.59172356, dtype=float32), 'validation/wer': 0.17229693850951466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33499104, dtype=float32), 'test/wer': 0.1091341173603071, 'test/num_examples': 2472, 'score': 14420.758267641068, 'total_duration': 15984.083220481873, 'accumulated_submission_time': 14420.758267641068, 'accumulated_eval_time': 1561.9921565055847, 'accumulated_logging_time': 0.5266251564025879, 'global_step': 18069, 'preemption_count': 0}), (19886, {'train/ctc_loss': Array(0.4899871, dtype=float32), 'train/wer': 0.15885338040272198, 'validation/ctc_loss': Array(0.57561785, dtype=float32), 'validation/wer': 0.16756615851009393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32893935, dtype=float32), 'test/wer': 0.10752950256941482, 'test/num_examples': 2472, 'score': 15860.820094823837, 'total_duration': 17559.10546183586, 'accumulated_submission_time': 15860.820094823837, 'accumulated_eval_time': 1696.8221898078918, 'accumulated_logging_time': 0.5745553970336914, 'global_step': 19886, 'preemption_count': 0}), (21663, {'train/ctc_loss': Array(0.38680267, dtype=float32), 'train/wer': 0.12805462666323691, 'validation/ctc_loss': Array(0.5477699, dtype=float32), 'validation/wer': 0.16106857700068547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31936333, dtype=float32), 'test/wer': 0.10389372981536774, 'test/num_examples': 2472, 'score': 17300.74575161934, 'total_duration': 19132.921291828156, 'accumulated_submission_time': 17300.74575161934, 'accumulated_eval_time': 1830.5872704982758, 'accumulated_logging_time': 0.6198842525482178, 'global_step': 21663, 'preemption_count': 0}), (23473, {'train/ctc_loss': Array(0.39550456, dtype=float32), 'train/wer': 0.1330114534639051, 'validation/ctc_loss': Array(0.54725, dtype=float32), 'validation/wer': 0.15923419291927746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3084308, dtype=float32), 'test/wer': 0.09995328336684745, 'test/num_examples': 2472, 'score': 18741.406412363052, 'total_duration': 20708.245043039322, 'accumulated_submission_time': 18741.406412363052, 'accumulated_eval_time': 1965.1153333187103, 'accumulated_logging_time': 0.6744070053100586, 'global_step': 23473, 'preemption_count': 0}), (25286, {'train/ctc_loss': Array(0.3043809, dtype=float32), 'train/wer': 0.105151034241601, 'validation/ctc_loss': Array(0.5312797, dtype=float32), 'validation/wer': 0.15423308263417554, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2982347, dtype=float32), 'test/wer': 0.09806430646111348, 'test/num_examples': 2472, 'score': 20181.81382036209, 'total_duration': 22284.15694141388, 'accumulated_submission_time': 20181.81382036209, 'accumulated_eval_time': 2100.4818663597107, 'accumulated_logging_time': 0.7313663959503174, 'global_step': 25286, 'preemption_count': 0}), (27090, {'train/ctc_loss': Array(0.3554216, dtype=float32), 'train/wer': 0.12076760555671108, 'validation/ctc_loss': Array(0.51525754, dtype=float32), 'validation/wer': 0.14956988520617512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28729317, dtype=float32), 'test/wer': 0.09205207889017529, 'test/num_examples': 2472, 'score': 21621.993543624878, 'total_duration': 23860.367542266846, 'accumulated_submission_time': 21621.993543624878, 'accumulated_eval_time': 2236.380726337433, 'accumulated_logging_time': 0.7822191715240479, 'global_step': 27090, 'preemption_count': 0}), (28873, {'train/ctc_loss': Array(0.3024444, dtype=float32), 'train/wer': 0.10361523046092184, 'validation/ctc_loss': Array(0.49139738, dtype=float32), 'validation/wer': 0.14322677814572735, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27378377, dtype=float32), 'test/wer': 0.08788820506570796, 'test/num_examples': 2472, 'score': 23062.2101726532, 'total_duration': 25437.443744897842, 'accumulated_submission_time': 23062.2101726532, 'accumulated_eval_time': 2373.104640483856, 'accumulated_logging_time': 0.8374857902526855, 'global_step': 28873, 'preemption_count': 0}), (30676, {'train/ctc_loss': Array(0.2924098, dtype=float32), 'train/wer': 0.10257953941445107, 'validation/ctc_loss': Array(0.48229715, dtype=float32), 'validation/wer': 0.1403014182685345, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26265836, dtype=float32), 'test/wer': 0.08536956919139602, 'test/num_examples': 2472, 'score': 24502.66463828087, 'total_duration': 27012.347834587097, 'accumulated_submission_time': 24502.66463828087, 'accumulated_eval_time': 2507.414543867111, 'accumulated_logging_time': 0.8959517478942871, 'global_step': 30676, 'preemption_count': 0}), (32491, {'train/ctc_loss': Array(0.2727315, dtype=float32), 'train/wer': 0.09430362496593077, 'validation/ctc_loss': Array(0.4702428, dtype=float32), 'validation/wer': 0.13723123859544106, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2568106, dtype=float32), 'test/wer': 0.08285093331708407, 'test/num_examples': 2472, 'score': 25942.722469568253, 'total_duration': 28587.597064971924, 'accumulated_submission_time': 25942.722469568253, 'accumulated_eval_time': 2642.467351436615, 'accumulated_logging_time': 0.9536893367767334, 'global_step': 32491, 'preemption_count': 0}), (34304, {'train/ctc_loss': Array(0.26884595, dtype=float32), 'train/wer': 0.09494766574412591, 'validation/ctc_loss': Array(0.46222648, dtype=float32), 'validation/wer': 0.13335006806530408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24790536, dtype=float32), 'test/wer': 0.08086039851319238, 'test/num_examples': 2472, 'score': 27382.652527332306, 'total_duration': 30163.677755355835, 'accumulated_submission_time': 27382.652527332306, 'accumulated_eval_time': 2778.475238084793, 'accumulated_logging_time': 1.014662265777588, 'global_step': 34304, 'preemption_count': 0}), (36084, {'train/ctc_loss': Array(0.2914037, dtype=float32), 'train/wer': 0.09721227850366923, 'validation/ctc_loss': Array(0.44839144, dtype=float32), 'validation/wer': 0.12929511378008632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23915242, dtype=float32), 'test/wer': 0.07775272682956554, 'test/num_examples': 2472, 'score': 28822.911303281784, 'total_duration': 31737.064566612244, 'accumulated_submission_time': 28822.911303281784, 'accumulated_eval_time': 2911.465180158615, 'accumulated_logging_time': 1.0729358196258545, 'global_step': 36084, 'preemption_count': 0}), (37885, {'train/ctc_loss': Array(0.22805688, dtype=float32), 'train/wer': 0.0809899755595646, 'validation/ctc_loss': Array(0.43772352, dtype=float32), 'validation/wer': 0.1265725016171544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23163031, dtype=float32), 'test/wer': 0.07436069303109703, 'test/num_examples': 2472, 'score': 30262.89229130745, 'total_duration': 33310.108652830124, 'accumulated_submission_time': 30262.89229130745, 'accumulated_eval_time': 3044.3916516304016, 'accumulated_logging_time': 1.1292433738708496, 'global_step': 37885, 'preemption_count': 0}), (39694, {'train/ctc_loss': Array(0.23828937, dtype=float32), 'train/wer': 0.0854084474355999, 'validation/ctc_loss': Array(0.4246287, dtype=float32), 'validation/wer': 0.1229906253318787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22602047, dtype=float32), 'test/wer': 0.07239046980683687, 'test/num_examples': 2472, 'score': 31703.72644138336, 'total_duration': 34883.71180319786, 'accumulated_submission_time': 31703.72644138336, 'accumulated_eval_time': 3177.021288871765, 'accumulated_logging_time': 1.1870660781860352, 'global_step': 39694, 'preemption_count': 0}), (41500, {'train/ctc_loss': Array(0.20024322, dtype=float32), 'train/wer': 0.07069968842901182, 'validation/ctc_loss': Array(0.42071438, dtype=float32), 'validation/wer': 0.12050937949544784, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22364078, dtype=float32), 'test/wer': 0.0725732740235208, 'test/num_examples': 2472, 'score': 33144.32629656792, 'total_duration': 36458.89913678169, 'accumulated_submission_time': 33144.32629656792, 'accumulated_eval_time': 3311.4702532291412, 'accumulated_logging_time': 1.2442855834960938, 'global_step': 41500, 'preemption_count': 0}), (43278, {'train/ctc_loss': Array(0.21650901, dtype=float32), 'train/wer': 0.0750982776662236, 'validation/ctc_loss': Array(0.41551012, dtype=float32), 'validation/wer': 0.11895498035278103, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21895356, dtype=float32), 'test/wer': 0.07017650762699815, 'test/num_examples': 2472, 'score': 34584.63703107834, 'total_duration': 38033.359548807144, 'accumulated_submission_time': 34584.63703107834, 'accumulated_eval_time': 3445.4803664684296, 'accumulated_logging_time': 1.304053783416748, 'global_step': 43278, 'preemption_count': 0}), (45080, {'train/ctc_loss': Array(0.21757558, dtype=float32), 'train/wer': 0.07707054560348604, 'validation/ctc_loss': Array(0.41123518, dtype=float32), 'validation/wer': 0.11877154194464022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21681073, dtype=float32), 'test/wer': 0.06930310970284159, 'test/num_examples': 2472, 'score': 36025.066982507706, 'total_duration': 39609.50177645683, 'accumulated_submission_time': 36025.066982507706, 'accumulated_eval_time': 3581.0494186878204, 'accumulated_logging_time': 1.3660883903503418, 'global_step': 45080, 'preemption_count': 0}), (46887, {'train/ctc_loss': Array(0.22279812, dtype=float32), 'train/wer': 0.07826722659513653, 'validation/ctc_loss': Array(0.4108359, dtype=float32), 'validation/wer': 0.11850121165895904, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21612823, dtype=float32), 'test/wer': 0.06962809497694636, 'test/num_examples': 2472, 'score': 37465.26635026932, 'total_duration': 41186.20255231857, 'accumulated_submission_time': 37465.26635026932, 'accumulated_eval_time': 3717.407273054123, 'accumulated_logging_time': 1.4297056198120117, 'global_step': 46887, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.18652456, dtype=float32), 'train/wer': 0.06758433938846892, 'validation/ctc_loss': Array(0.41070828, dtype=float32), 'validation/wer': 0.11837570116917849, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21605648, dtype=float32), 'test/wer': 0.06948591391952552, 'test/num_examples': 2472, 'score': 38341.64616441727, 'total_duration': 42196.50052070618, 'accumulated_submission_time': 38341.64616441727, 'accumulated_eval_time': 3851.2109656333923, 'accumulated_logging_time': 1.4912919998168945, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 14:37:24.130196 140437690197824 submission_runner.py:586] Timing: 38341.64616441727
I0213 14:37:24.130267 140437690197824 submission_runner.py:588] Total number of evals: 28
I0213 14:37:24.130317 140437690197824 submission_runner.py:589] ====================
I0213 14:37:24.164594 140437690197824 submission_runner.py:673] Final librispeech_deepspeech score: 37935.802810907364
